sem no . with tem you can see the atomic structure but only very local . it does not give you the idea of the whole sample . besides , tem is a much complex technique than xrd and involves a specimen preparation which may be very ( very ! ) difficult and alter you sample . powder xrd is a simple technique , most of the times non destructive . you do not need to do a fourier transform . . . powder xrd gives you a diffraction pattern with diffraction peaks . in a general way , sharper peaks means a more " crystalline " sample and well organized ( closer to a single crystal ) . however this is not always straightforward ( it depends on type of sample , material , size of particles . . . ) . it would be helpful if you could say what type of sample are you trying to analyse .
any optical assembly , whether a telescope or a camera or whatever , will generally have a minimum distance at which it can focus . that said , the scale for distances is set by the focal lengths of the optics . as you point out , they are $360~\mathrm{mm}$ and either $6~\mathrm{mm}$ or $20~\mathrm{mm}$ . ( the $360~\mathrm{mm}$ is the really important one . ) distances long compared to these are all much the same to your telescope , so the building half a kilometer away and the stars thousands of light years away are all " at infinity " as far as it is concerned . the $360~\mathrm{mm}$ actually means that parallel rays " from infinity " need to travel $360~\mathrm{mm}$ beyond the lens to converge to a point . diverging rays from " closer than infinity " objects need to travel further . thus the minimum distance at which you can focus is set by how much you can extend the optical path , and the maximum distance you can focus is set by how short the path can be made . any telescope , whether for terrestrial or astronomical use , should be able to focus at infinity - that is after all where its targets of interest are . note though that i mentioned changing the optical path . in order to focus , assuming you do not have some multimillion dollar deformable optics on hand , you need to turn some knob somewhere to move something . while there are systems built that are fixed to focus at a single distance , i am guessing that is not what you have , since it seems designed for changing eyepieces ( doing so requires refocusing ) and personal use ( while contacts are fine , people who wear glasses are best served by removing them and changing the focus to correct for nearsightedness , since you generally need to get your eye close to the eyepiece ) .
first , a correction . the first formula is the probability , not probability amplitude . and it is computed at the leading order only , " linearized " in a sense , so of course it is only a good approximation for $p_{f\leftarrow i}\ll 1$ . when the probability becomes comparable to one , subleading and higher-order corrections become important because one must also study how the newly created coefficients in front of other states – states absent in the initial state – change by the time evolution . the perturbation theory always becomes inadequate when the perturbation , in this case the matrix element $\langle f |v|i\rangle$ , is too large . but one must properly understand what " too large " means . and it means $p_{f\leftarrow i} \geq o ( 1 ) $ which is equivalent to $\langle f |v|i\rangle \cdot \delta t \geq o ( \hbar ) $ . for transitions at $\omega_{fi}\to 0$ , the requirement for " how small the perturbation matrix element has to be " simply gets tougher , the upper limit becomes smaller . one more equivalent way to say it : for the perturbation theory to be ok , you need to have $\delta t\ll \hbar / \langle f |v|i\rangle$ . however , your treatment has one more problem . well , one of two problems . if you consider the transition to a discrete final state that just happens to have a finite energy , you are dealing with degenerate perturbation theory and you should first rediagonalize $h_0+v$ in this hilbert subspace , to find out that the actual energy eigenstates differ from the original initial state and their energies actually differ . if you consider a transition to a final state that belongs to a continuum , then you are interested in the integrated probability over $\omega_f$ , anyway , and in that case , $\sin^2 y / y^2$ may be approximated by a multiple of the delta-function which imposes the " naive " energy conservation law . see e.g. this document for some intro to the method . my inequality appears as ( 11.40 ) on page 104 .
great question ; i remember being so confused by this when i first took analytic mechanics . the components of the angular velocity " in the body frame " are not zero because when one writes these components , one is not referring to measurements of the motions of the particles in the body frame ( because , of course , the particles are stationary in this frame ) . instead , one is referring to angular velocity as measured in an inertial frame but whose components have simply been written with respect to a time-varying basis that is rotating with the body . in practice , we make measurements of the positions $\mathbf x_i ( t ) = ( x_i ( t ) , y_i ( t ) , z_i ( t ) ) $ of the particles in an inertial frame . then , we note that for a rigid body ( let 's consider pure rotation for simplicity ) , the position of each particle $i$ satisfies \begin{align} \mathbf x_i ( t ) = r ( t ) \mathbf x_i ( 0 ) \end{align} for some time-dependent rotation $r ( t ) $ . then we compute $\boldsymbol\omega ( t ) = ( \omega^x ( t ) , \omega^y ( t ) , \omega^z ( t ) ) $ in the standard way in terms of $r ( t ) $ . to see how this is done in detail , see , for example stackexchange-url once we have $\boldsymbol\omega$ , we can write its components with respect to any basis we like . if we write it in the standard ordered basis $\{\mathbf e_i\}$ , then we will just get $\omega_x ( t ) $ as its components . if we write it in some basis $\{\mathbf e_{i , b} ( t ) \}$ that is rotating with the body ( like one that points along the principal axes of the body ) then we get different components $\omega^i_b ( t ) $ , and these are the body components . main point reiterated . angular velocity is being measured with respect to an inertial frame , but its components can be taken with respect to any basis we wish such as one rotating with the body .
i think you just forgot that the $\int_a^b f\ , dl$ is not a scalar expression . rather it should be written in a form $\int_a^b \vec{f}\cdot d\vec{l}$ . then it comes to the sign of the scalar product : $$\vec{f}\cdot d\vec{l}=f\ , dl\ , \cos\theta$$ where the angle $\theta$ is taken between the vector $\vec{f}$ and the direction of the tangent to the integration path from $a$ to $b$ . then , in your first example , $w_{a \to b}=\int_{r_a}^{r_b} f ( r ) dr = \int_{r_a}^{r_b} \left ( -\frac{gmm}{r^2} \right ) dr$ the path could go with any slope , but the gravity is always directed downwards , along the $r$ axis . that means , we can always take $ ( \pi-\theta ) $ as the angle between the vector $d\vec{l}$ and the $r$ axis , that is $$dl\ , \cos ( \pi-\theta ) =dr$$ but $\cos ( \pi-\theta ) =-\cos\theta$ and thus we have $$\vec{f}\cdot d\vec{l}=-f\ , dr=-\frac{gmm}{r^2}dr$$ for your second example : . . . we also should change the sign , because the gravitational force is always a force of attraction . what the authors actually mean is that : the coulomb 's and newton 's forces have exactly the same expressions , but the sign conventions for them are different . the newton 's force is defined that if all the quantities ( $m$ , $m$ and $r$ ) are positive , then the vector of the force is directed towards the other body . but for the coulomb 's force , if all the quantities ( $q_1$ , $q_2$ and $r$ ) are positive , then the vector of the force is directed away from the other charge . that becomes manifest if we take the vector expressions for these forces : $$\vec{f}_n=-\frac{gmm\ , \vec{r}}{r^3}\qquad\vec{f}_c=\frac{q_1q_2\ , \vec{r}}{4\pi\epsilon_0\ , r^3}$$ now the different signs are clearly seen . " . . . from point $a$ to point $b$ . . . " - . . . as i understand it - the work that i must do is always $u_b-u_a$ . however the work that the force that is being created by the field do_es_ is always $u_a-u_b$ , am i correct ? yes this is correct . the mnemonic rule is very simple : $u$ is like the height of the slope . when you go up , $u_b&gt ; u_a$ , and it is you who does the work . but when you go down , $u_a&gt ; u_b$ , and it is the field force who does the work .
scale bars may be helpful . also , spr 's decay exponentially . if you google " surface plasmon image , " you might notice that there is a lack of images and mostly cartoons . imaging a surface plasmon does not really make sense , as the light is traped along the interface between a metal and a dielectric . and while you may be able to image light near a spr , it would be really hard , your objective lens would have to be ridiculously close to the metal , most likely less than a micron if your objective is in air . imaging the light that is coupled out of a spr makes more sense . to get outcoupling of light you need to use a type of grating , or create some kind of rough patch for the light to escape via . in order to get this kind of out coupling , you need to do angled imaging , which it seems you perform . so maybe what you have is just a bump in your metal film ? the wavelengths of light that are emitted from film of course are affected by the permittivity/refractive index of your surounding materials , but they are also very much modified by the size/shape/geometry of your nanostructure . this is very important if your looking to gain insight about what you are seeing . so in order to gain some kind of quantitative insight from this blimp in your image , it sounds like you are going to have to make that same identical rough patch . or actually create a meaningful outcoupling grating . as for really knowing if it is a spr , i am not sure what to tell you . you had probably need a high intensity laser to create that kind of resonance , since you are in the uv ( blue color ) . you would need something higher than the color that is being emitted , as a great deal of loss is typically seen with spr 's . i am not sure what you imaged with ( you sounded like the tem imaging and the other imaging were separate ? ) , and i can not speak for tem ( not familiar ) . i would suggest googling to see if you can find tem 's producing spr 's . from a quick google search , i think they can , but it looks like advanced filtering might be needed . . . good luck
no , op 's calculation is correct . in more detail , the paper states on page 323 ( apparently assuming that $\mu$ and $\nu$ are real numbers ) , that the result is $$ \mu^2 + 2 \nu^2 ~=~ 2\mu^2 -1~ . $$ the first expression is correct , and corresponds to op 's $3\mu^2-2$ . the second expression is wrong . in other words , the paper makes a mistake in the very last step while reducing with $\nu^2=\mu^2-1$ .
the electric field from your potential is : $$e ( r ) = {2\over r^3}$$ using gauss 's law , the total charge in a sphere of radius r is : $$q ( r ) = \oint e \cdot ds = 4\pi r^2 {2\over r^3} = {8\pi\over r}$$ the total charge is decreasing with r , so there is a negative charge cloud of density $$ \rho ( r ) = {1\over 4\pi r^2} {dq\over dr} = - {4\over r^4}$$ but the total charge at infinity is zero , so there is a positive charge at the origin , cancelling the negative charge cloud , of a divergent magnitude . if you assume this charge is a sphere of infinitesimal radius $\epsilon$ , the positive charge at the origin is $$q_0 = \int_\epsilon^\infty 4\pi r^2 {4\over r^4} = {16\pi \over \epsilon}$$ this is not a distribution in the mathematical sense , but it is certainly ok to work with , so long as you keep the $\epsilon$ around and take the limit $\epsilon$ goes to zero at the end of the day . mathematicians have not had the last word on the class of appropriate generalized solutions yet .
you are absolutely right . in fact , the uncertainty relations are a direct consequence of this wave nature and the " conjugated " variables being $p \leftrightarrow x$ and $e \leftrightarrow t$ . i will attempt a simple explanation . remember that the wavefunction gives you a probability amplitude for finding the particle at some location $x$ at some time $t$ ? the probability to find the particle at some point $x$ at time $t$ is given by $|\psi ( x , t ) |^2$ . now here 's the catch : if you put your plane wave into that equation , you will find that $|\psi ( x , t ) |^2 = a$ , independent of $x$ and $t$ . so for your plane wave , the particle has equal probability to be anywhere . in terms of uncertainty , then , we could say that the uncertainty in position is at a maximum , and so is the uncertainty in time . what about momentum and energy ? well , your plane wave has a definite momentum of $p = \hbar k$ and a definite energy of $e = \hbar \omega$ , so their uncertainty is zero . this is an extreme case of the uncertainty relation : certainty in one variable means maximum uncertainty in the other variable . any other possible form of the wave function can always be written as a sum of plane waves ( that is the fourier theorem ) : $$\psi ( x , t ) = \int de \int dk a ( k , e ) \exp ( i ( kx-e/\hbar t ) $$ you can think of $\psi ( x , t ) $ as the wavefunction in position and time domain and $a ( k , e ) $ as the wavefunction in momentum and frequency domain , and the conversion between those two is achieved via the fourier transform . you can now mathematically show how if $\psi ( x , t ) $ is " narrow " ( i.e. . position has low uncertainty ) then $a ( k , e ) $ must be broad and vice versa . the intuition behind that : you need to add up a lot of plain waves with lots of different momenta to go from the broad plain wave distribution to a narrow wave packet , but this then implies that you have high uncertainty in momentum .
general remarks . let $\delta w$ denote the differential work done by a system , so $\delta w$ is postive when the system does work on something else and negative when work is done by something else on the system . for a given process taking place over a path $\gamma$ in thermodynamic state space , the systematic way of determining whether work was done by or on the system is to determine the sign of $w$ , the total work done by the system , which is given by $$ w = \int_\gamma\delta w $$ this can be computed in various ways depending on the system at hand , and the process it undergoes . the trick is to attempt to find an expression for $\delta w$ that allows for the efficient calculation of the integral for $w$ . example - adiabatic compression . suppose , for example , that we want to determine the work done by the gas during process $1$ of your diagram . recall that the first law of thermodynamics in differential form can be written as follows : $$ de = \delta q - \delta w $$ the sign convention here is that $\delta q$ denotes the heat transferred to the system , and $\delta w$ , again , denotes the work done by the system . since process $1$ is adiabatic , we have $\delta q = 0$ by definition . it follows that $$ w = -\int_\gamma de = -\delta_\gamma e $$ where $\delta_\gamma e$ denotes the total change in energy along the path $\gamma$ . let process $1$ start at point $a$ and end at point $b$ , then we can write this result as $$ w = - ( e_b - e_a ) = e_a-e_b $$ so to determine the sign of the work done , we simply need to know whether or not the internal energy of the gas increased ( in which case $w&lt ; 0$ so that work was done on the gas ) or decreased ( in which case $w&gt ; 0$ so that work was done by the gas ) . how to we figure this out for this adiabatic process ? well take , for example , a monatomic ideal gas , and recall that for such a process , we have $$ t_av_a^{\gamma-1} = t_bv_b^{\gamma-1} , \qquad \gamma = \frac{5}{3} $$ then we see that since $v_b&lt ; v_a$ , we have $t_b&gt ; t_a$ ; the temperature of the gas increased . but for a monatomic ideal gas , the internal energy can be written purely as a function of temperature and number of particles ; $$ e = \frac{3}{2}nkt $$ so assuming the number of particles is fixed , the internal energy also increase , and therefore , $w&lt ; 0$ , so work was done on the gas .
the hopping term is given by $$ t_{ij}=\int\limits_{\mathbf{r}}d\mathbf{r}\phi^*\left ( \mathbf{r}_i\right ) \left [ -\frac{\hbar^2}{2m}\nabla^2+u ( \mathbf{r} ) \right ] \phi\left ( \mathbf{r}_j\right ) $$ where $i$ and $j$ are the sites whose hopping you want to find , $\phi\left ( \mathbf{r}_j\right ) $ are the atomic orbitals , and $u ( \mathbf{r} ) $ is the potential of the crystal lattice . so the sign depends on your choice for $u ( \mathbf{r} ) $ . if $u ( \mathbf{r} ) $ is taken as negative ( coulomb potential ) you are more likely to end up with a negative $t$ which can be a nuisance , so people just redefine it as $-|t|$ .
the diffusion is determined from the broadening of the usual elastic scattering peaks . this is known as quasielastic scattering . there is no requirement for the neutron speed to be related to the diffusion speed . have a look at this book for an introduction to the subject .
when two black holes collide , they coalesce to form a larger black hole . the event will produce gravitational waves that will have a particular signature that depends on the details of the collision . black hole merger events are one of the main gravitational wave signals that are being sought at ligo . there are a few misconceptions in your question : 1 ) black holes do not have infinite mass . they in fact have a finite mass . the orbit of the earth around a black hole with the same mass as the sun 's would in fact be identical to the earth 's current orbit . so , the idea of black holes sucking up the space around them is inaccurate . 2 ) the gravity at the event horizon of a black hole is finite and ( generally ) inversely proprortional to the mass of the black hole--a sufficiently large black hole would not exert any detectable local strains on objects at all ! 3 ) the geometry of spacetime will be well-defined everywhere except at the central singularity of the black hole . every observer will report only one direction of time . this direction , in an absolute sense , will be observer dependent , just as it is in special relativity ( the rate of aging will depend on speed and position , but will always be well-defined and knowable ) .
when you write the five dimensional kaluza-klein metric tensor as $$ g_{mn} = \left ( \begin{array}{cc} g_{\mu\nu} and g_{\mu 5} \\ g_{5\nu} and g_{55}\\ \end{array} \right ) $$ where $g_{\mu\nu}$ corresponds to the ordinary four dimensional metric and $ g_{\mu 5}$ is the ordinary four dimensional vector potetial , $g_{55}$ appears as an additional scalar field . this new scalar field , called a dilaton field , is physically meaningful , since it defines the size of the 5th additional dimension in kaluza-klein theory . they are natural in every theory that hase compactified dimensions . even though such fields have up to now not been experimentally confirmed it is wrong to call such a field " unphysical " . " unphysical " are in some cases fields introduced to rewrite the transformation determinant in calculations of certain generating functionals , or the additional fields needed to make an action local , which may have conversely to such dilaton field , no well defined physical meaning .
you are not " replacing a mathematical proof " . what the statements you are referring to mean is that in tensor notation , the proof is immediate , so that nothing needs to be written down . this is because if you have a tensor equation as above , in order to prove lorentz invariance , do a lorentz transformation and go to another set of coordinates $x^{\mu'}$ . then using the usual transformation laws we get that ${\partial_{\mu}} = \lambda^{\mu'}_{\mu}\partial_{\mu'}$ and $f_{\mu\nu} = \lambda^{\mu'}_{\mu}\lambda^{\nu'}_{\nu}f_{\mu'\nu'} $ , we can write the maxwell equation in terms of the new coordinates to become $\lambda^{\mu'}_{\mu}\lambda^{\nu'}_{\nu}\lambda^{\sigma'}_{\sigma} ( f_{\mu'\nu ' , \sigma'}+f_{\nu'\sigma ' , \mu'}+f_{\sigma'\mu ' , \nu'} ) =0 . $ however , this can only hold if the thing inside the brackets is zero itself . namely maxwell 's equation in the primed coordinate system also holds . more succintly , what a " tensor equation " means is that there was nothing special about the coordinate system in which the equations were derived . you could have equally well chosen another system and derived the same equations . thus invariance under coordinate change is immediate .
a bar magnet , current loop , electron etc has a magnetic moment vector associated with it . when placed in a magnetic field , the torque $\tau$ tending to align the magnetic moment $m$ with the applied magnetic field $b$ is given by $\tau = m\times b$
i think $\cos ( 90^\circ-\theta ) =\frac{g}{a}$ is wrong . angle between $\vec{g}$ and $\vec{a}$ is that , but that is not the same thing . think about what happens when $\theta=0$ in the acceleration infinite ? to solve this problem you need to find the projection of $\vec{g}$ along the incline . see picture below that might help you .
it is not wrong , all three normalization conditions are natural and they do not contradict each other because , in fact , the first equation is nothing else than the product of the following two equations ! just substitute your formula for $\psi$ , $\psi = r y$ , to the first equation . the only mistake you have to fix to show that the first equation becomes the product of the other two is to replace the wrong ( and really nonsensical , one can not integrate over same $r$ " twice " in one integral ) $drdr$ in the first equation by the correct $r^2 dr$ . the first integral splits to the product because some factors in the integrand only depend on the radial coordinate $r$ so they can be taken out of the integral and integrated over $r$ separately while the remaining factors only depend on the angles $\theta , \phi$ . as kyle said , the general technique is called the " separation of variables " .
original paper on dft is published in 1964 . it was definitely formulated as an approximate solution of quantum many-body problem . i suspect that ' its application to classical problems ' is not what is really discussed in the first paper . for me , though , these claims seem to be an incorrect pr . manybody problems in classical mechanics are far simpler than in quantum mechanics , i see no reason why one should call this " classical dft " and not " statistical mechanics " which is basically turning classical manybody problem into distributions of parameters in space .
there does not appear to be anything wrong with user35915 's calculation . however , in order to get the desired answer , the canonical momenta needs to be different . starting from user35915 's action , $$ s=\int d^{4} x\left ( -\frac{1}{4}f_{\mu\lambda}f^{\mu\lambda}-\frac{1}{2}a^{\mu}_{ , \mu}a^{\lambda}_{ , \lambda}\right ) $$ change the second term by integrating by parts and chuck the surface term away to get , $$ s=-\frac{1}{4}\int d^{4} x ( f_{\mu\lambda}f^{\mu\lambda}-2a^{\mu}a^{\lambda}_{ , \lambda\mu} ) \ . $$ now expand the electromagnetic field tensor $f_{\mu\lambda}=a_{\lambda , \mu}-a_{\lambda , \mu}$ and do a bit of swopping dummy indices to get , $$ s=-\frac{1}{2}\int d^{4}x \eta^{\mu\rho}\eta^{\lambda\sigma} ( a_{\lambda , \mu}a_{\sigma , \rho}-a_{\lambda , \mu}a_{\rho , \sigma}-a_{\rho}a_{\lambda , \sigma\mu} ) \ . $$ the last two terms can be combined into a surface integral which vanishes at infinity and the final form of the action is only the first term in the last line . the lagrangian is now , $$ l=-\frac{1}{2}\int d^{3}x \eta^{\mu\rho}\eta^{\lambda\sigma}a_{\lambda , \mu}a_{\sigma , \rho}=-\frac{1}{2}\int d^{3}x a_{\mu , \lambda}a^{\mu , \lambda}\ . $$ the reason for getting the lagrangian in this form is because it looks like the lagrangian for four scalar fields . the canonical momenta are now , $$ \pi^{\mu}=-a^{\mu}_{ , 0}=-\frac{\partial a^{\mu}}{\partial t} $$ which look like the momenta for four scalar fields . now , it is straightforward to go over to the desired hamiltonian , $$ h=-\frac{1}{2}\int d^{3}x ( \pi^{\mu}\pi_{\mu}+a^{\mu}_{ , r}a_{\mu , r} ) $$
your map fails to be completely positive . if you apply it to half of a maximally entangled state $ ( |0\rangle|0\rangle+|1\rangle|1\rangle ) /\sqrt{2}$ , you can easily see that $\phi ( \rho ) =\rho$ and $\phi ( \rho' ) =\rho'$ imply that $\phi ( |0\rangle\langle1| ) = \alpha |0\rangle\langle1|$ and $\phi ( |1\rangle\langle0| ) = \alpha^* |1\rangle\langle0|$ for the resulting state to be positive ( with $|\alpha|\le1$ ) . however , this is incompatible with the last condition .
i ) for a general lagrangian $l ( q , v , t ) $ , the legendre transformation may be singular , i.e. the velocities $v^i$ in the momentum relations $$\tag{1} p_i~:=~\frac{\partial l ( q , v , t ) }{\partial v^i}$$ cannot be isolated . how to perform a singular legendre transformation to achieve the corresponding hamiltonian formulation goes under the name dirac-bergmann analysis , cf . refs . 1 and 2 . ii ) example . op is evidently considering hopfields 's em model with polarization also studied in this phys . se post . its lagrangian density$^1$ $$\tag{2} {\cal l} ( a_{\mu} , {\bf p} ) ~=~-\frac{1}{16\pi}f_{\mu\nu}f^{\mu\nu} +a_{\mu} j^{\mu}_b +\frac{1}{2\beta}\left ( \frac{1}{\omega_0^2}\dot{\bf p}^2 -{\bf p}^2\right ) $$ leads to a singular legendre transformation . the momentum $$\tag{3} \pi^0~:=~\frac{\partial {\cal l}}{\partial \dot{a}_0}~=~0$$ corresponding to the $a_0$ field vanishes ! eq . ( 3 ) is a primary constraint in dirac 's terminology . one may show that there also is a secondary constraint , namely gauss 's law $$\tag{4} {\bf \nabla}\cdot {\bf d}~=~0 , $$ where ${\bf d}={\bf e}+4\pi{\bf p}$ . ( there are no free charges in this model . ) the momentum ${\bf \pi}=-\frac{1}{4\pi}{\bf e}$ for the magnetic vector potential ${\bf a}$ is essentially the electric field ${\bf e}$ . let ${\bf \pi}$ be the momentum for the polarization ${\bf p}$ . one may show that the hamiltonian density becomes $$\tag{5} {\cal h} ( a_{\mu} , {\bf e} , {\bf p} , {\bf \pi} ) ~=~ \frac{1}{8\pi} ( {\bf e}^2+{\bf b}^2 ) +\frac{1}{4\pi}a_0 {\bf \nabla}\cdot {\bf d} +\frac{\beta\omega_0^2}{2} ( {\bf \pi}-{\bf a} ) ^2 +\frac{1}{2\beta}{\bf p}^2 , $$ after dropping a total divergence term and eliminating the $\pi^0$ field . iii ) technically , what op writes in his second equation is not the hamiltonian density ${\cal h}$ but the lagrangian energy density function $$ \tag{6} {\cal h} ( a_{\mu} , \dot{\bf a} , {\bf p} , \dot{\bf p} ) ~:=~\dot{a}_{\mu} \frac{\partial {\cal l}}{\partial \dot{a}_{\mu}} + \dot{\bf p}\cdot \frac{\partial {\cal l}}{\partial \dot{\bf p}}-{\cal l} . $$ iv ) more generally , the point is that the lagrangian energy function $h$ depends on velocities $v$ , while the hamiltonian $h$ depends on momenta $p$ . if the lagrangian is of the form $$ \tag{7} l~=~\sum_{n=0}^{2}l_n , $$ where $l_n$ is homogeneous in velocities $v$ with weight $n$ ( i.e. . the lagrangian ( 7 ) depends on the velocities up to quadratic order ) , then the lagrangian energy function is $$\tag{8} h~:=~~\left ( v^i\frac{\partial}{\partial v^i}-1\right ) l ~=~\sum_{n=0}^{2} ( n-1 ) l_n ~=~ l_2 - l_0 . $$ in words : the quadratic terms $l_2$ are preserved , the linear terms $l_1$ disappear , and the constant terms $l_0$ flip signs . references : p.a.m. dirac , lectures on quantum mechanics , 1964 . m . henneaux and c . teitelboim , quantization of gauge systems , 1994 . -- $^1$ in this answer we work with cgs units where the speed of light in vacuum is $c=1$ , and minkowski signature $ ( - , + , + , + ) $ , cf . this phys . se answer .
water vapor is invisible . i think you mean fog - fine water droplets condensed from water vapor . pressurized planes fly with an 8000 foot equivalent altitude and humidity in the cabin is low . but , at 35,000 feet ( called flight level 35 ) it is likely one would get a brief fog . if loss of pressure is fast , you would only get to watch for a few seconds . at those pressures , oxygen actually rapidly diffuses out of the blood in the lungs and unconsciousness comes much sooner than one would expect . there have been plenty of accidents dating back to the wwii fighters that cruised at 40,000 feet where a pilot thought he could manage briefly without his oxygen . something like this likely happened to the missing malaysian aircraft .
surely someone has mulled over why the universe might exhibit such a non-intuitive and thus interesting asymmetry ? oh yes , definitely . i have for one ( though i have not made a significant contribution to the question ) ! : ) there are a number of " left-right symmetric " models out there which usually involve a group like $su ( 2 ) _l \times su ( 2 ) _r$ where the $su ( 2 ) _r$ gets spontaneously broken by a higgs mechanism . you will find a number of highly cited papers in inspirehep . i have always thought these models sounded very interesting but have not yet had the opportunity to work on them ! if you find anything good let me know . : )
i ) let there be given a local action functional $$\tag{1} s [ \phi ] ~=~\int_v \mathrm{d}^nx ~{\cal l} , $$ with the lagrangian density $$\tag{2} {\cal l} ( \phi ( x ) , \partial\phi ( x ) , x ) . $$ [ we leave it to the reader to extend to higher-derivative theories . ] ii ) assume that a variation of $s$ for arbitrary $x$-dependent infinitesimal $\epsilon ( x ) $ takes the form $$\tag{3} \delta s ~=~ \int_v \mathrm{d}^n x \left ( \epsilon ~ k + j^{\mu} ~ d_{\mu} \epsilon \right ) $$ ( up to possible boundary terms ) for some structure functions $$\tag{4} k ( \phi ( x ) , \partial\phi ( x ) , \partial^2\phi ( x ) , x ) $$ and $$\tag{5} j^\mu ( \phi ( x ) , \partial\phi ( x ) , x ) . $$ [ one may show that some terms in the $k$ structure function ( 4 ) are proportional to eoms , which are typically of second order . ] iii ) next we assume that the action has a quasisymmetry$^1$ for $x$-independent infinitesimal $\epsilon$ . then eq . ( 3 ) reduces to $$\tag{6} 0~=~ \epsilon\int_v \mathrm{d}^n x~ k $$ ( up to possible boundary terms ) . iv ) now let us return to op 's question . due to the fact that eq . ( 6 ) holds for all off-shell field configurations , we may show that eq . ( 6 ) is only possible if $$\tag{7} k ~=~ d_{\mu}k^{\mu} $$ is a total divergence . ( here the words on-shell and off-shell refer to whether the eoms are satisfied or not . ) in more detail , there are two possibilities : if we know that eq . ( 6 ) holds for every integration region $v$ , we can deduce eq . ( 7 ) by localization . if we only know that eq . ( 6 ) holds for a single fixed integration region $v$ , then the reason for eq . ( 7 ) is that the euler-lagrange derivatives of the functional $k [ \phi ] :=\int_v \mathrm{d}^n x~ k$ must be identically zero . therefore $k$ itself must be a total divergence , due to an algebraic poincare lemma of the so-called bi-variational complex , see e.g. ref . 1 . [ note that there could in principle be topological obstructions in field configuration space which ruin this proof of eg . ( 7 ) . ] see also this phys . se answer . v ) next define the bare and the full noether currents , $j^{\mu}$ and $$\tag{8} j^{\mu}~:=~j^{\mu}-k^{\mu} , $$ respectively . on-shell , after an integration by part , eq . ( 3 ) becomes $$\tag{9} 0~\approx~ -\delta s ~=~\int_v \mathrm{d}^n x ~ \epsilon~ d_{\mu} j^{\mu} $$ ( up to possible boundary terms ) for arbitrary $x$-dependent infinitesimal $\epsilon ( x ) $ . [ the $\approx$ sign means equality modulo eom . ] vi ) equation ( 9 ) implies by the fundamental lemma of calculus of variations , the conservation law $$\tag{6} d_{\mu}j^{\mu}~\approx~0 . $$ references : g . barnich , f . brandt and m . henneaux , local brst cohomology in gauge theories , phys . rep . 338 ( 2000 ) 439 , arxiv:hep-th/0002245 . -- $^1$ a quasisymmetry of a local action $s=\int_v d^dx ~{\cal l}$ means that the infinitesimal change $\delta s$ is a boundary term under the quasisymmetry transformation .
for a static situation ( i.e. . no charges moving or current flowing ) the net electric field is always zero inside of a conductor . where by ' inside ' i mean actually inside the material itself , not a region of free space that is simply enclosed by the material . the reason for this is , say there is an electric field $\vec{e}_0$ , applied to the conductor- by definition there is an essentially infinite well of free charge that can move . this charge will move in response to the applied electric field and set up its own induced field , $\vec{e}_{ind}$ . this field will oppose the orignal field and charge will keeping moving until it reaches equilibrium such that $\vec{e}_0+\vec{e}_{ind}= \vec{e}_{net} = 0 $ . in practice this happens almost instantaneously . above i have not mentioned anything about shapes or charges outside the conductor so this holds for all static electric field configurations , including the one you describe above .
a translation by $x^\nu \to x^\nu - \epsilon^\nu$ corresponds to an infinitesimal transformation of the fields , by $$\phi \to \phi + \epsilon^\nu \partial_\nu \phi$$ as we are performing an active rather than passive transformation . the lagrangian transforms as , $$\mathcal{l}\to \mathcal{l}+\epsilon^\nu \partial_\nu \mathcal{l}$$ by substituting $\phi$ into the lagrangian . notice the change is up to a total derivative , and hence noether 's theorem is applicable to the symmetry . the conserved current density is given by , $$j^\mu = \frac{\partial\mathcal{l}}{\partial ( \partial_\mu\phi ) }x ( \phi ) -f^\mu ( \phi ) $$ where $x=\delta\phi$ and $f^\mu$ is such that $\partial_\mu f^\mu=\delta \mathcal{l}$ infinitesimally . for our case , we obtain the symmetric stress-energy tensor ( analogous to that of general relativity ) , $$t^\mu_\nu=\frac{\partial \mathcal{l}}{\partial ( \partial_\mu \phi ) } \partial_\nu \phi - \delta^\mu_\nu \mathcal{l}$$ where the kronecker delta is raised with the minkowski metric . the current satisfies , $\partial_\mu t^{\mu}_\nu = 0$ , and the corresponding noether charge , $$e=\int \mathrm{d}^3 x \ , t^{00}$$ is the total energy of the system , whereas , $$p^i = \int \mathrm{d}^3 x \ , t^{0i}$$ is the $i$th component of the total momentum of the field , where $i= ( x , y , z ) $ only . a caveat : the stress-energy tensor derived by noether 's theorem is not always symmetric , and may require the addition of a term which satisfies the continuity equation , and ensures symmetry in the indices . alternate method recall to obtain the einstein field equations in general relativity , we may vary the einstein-hilbert action , $$s\sim \int \mathrm{d}^4 x \ , \sqrt{-g} \ , \left ( r + \mathcal{l}\right ) $$ similarly , in quantum field theory , we may promote our minkowski metric to a generic metric tensor , thereby replacing the kinetic term of the lagrangian with covariant derivatives . up to some constants , the stress-energy tensor is given by $$t^{\mu\nu} \sim \frac{1}{\sqrt{-g}} \frac{\partial ( \sqrt{-g}\mathcal{l} ) }{\partial g^{\mu\nu}}$$ evaluated at $g_{\mu\nu}=\eta_{\mu\nu}$ , which is precisely the definition we implement when obtaining the einstein field equations for general relativity .
what is colloquially called ''empty space'' is not really empty - it is filled by the electromagnetic field and the gravitational field ; it is called empty only because it does not contain ( nonzero ) matter fields . the electromagnetic is the medium that carries electromagnetic waves , such as the air density field ( colloquially just called ''air'' ) carries sound waves and the water density field ( colloquially just called ''water'' ) carries water waves . indeed , electromagnetic waves are nothing else than propagating high-frequency oscillations in the electric fields , in precisely the same way as sound waves are propagating ohigh-frequency scillations in the pressure field of air ( or any other mechanical medium ) , and water waves are propagating low frequency oscillations in the mass density field of water .
virtual particles influence physics at every point of space , whether or not there is a nearby atomic nucleus or orbital . all electrons in an atom receive energy shifts analogous to the lamb shift ( from virtual photons ) , aside from other quantum corrections . in fact , the influence of the virtual particles only becomes truly measurable if there are some nearby particles that feel the effect . there is a counterpart of the pauli principle for virtual fermions : one may get some cancellation between feynman diagrams for various special quantities . however , one should not interpret the pauli exclusion principle for virtual particles in the same way as for real particles because virtual particles are not real particles .
the most general lorentz transformation that is connected to the identity is given by the conjugation by $\exp ( -a ) $ where $$ a = \frac 12 \omega_{\mu\nu} \gamma^\mu \gamma^\nu $$ and $\omega_{\mu\nu}$ is an antisymmetric tensor containing $d ( d-1 ) /2$ parameters . the group of all such transformations is isomorphic to $spin ( d-1,1 ) $ . if $\omega$ only contains one component $0\mu$ , then it is a boost , and the nonzero numerical value of $\omega$ is the rapidity - the " hyperbolic angle " $\eta$ such that $v/c=\tanh\eta$ . if only one doubly spatial component of $\omega$ is nonzero , then this component $\omega_{\mu\nu} = -\omega_{\nu\mu}$ is obviously the angle itself . note that the spatial-spatial terms in $a$ are anti-hermitean , producing unitary transformations ; the mixed temporal-spatial terms in $a$ are hermitean and they do not product unitary transformations on the 4-component space of spinors ( but they become unitary if they are promoted to transformations of the full hilbert space of quantum field theory ) . in 4 dimensions , a general antisymmetric matrix $4\times 4$ contains 6 independent parameters and has eigenvalues $\pm i a , \pm i b$ , so in 3+1 dimensions , one can always represent a general lorentz transformation as a rotation around an axis in the 4-dimensional space followed by a boost in the complementary transverse 2-plane . this is the counterpart of the statement that any $su ( 2 ) $ rotations in 3 dimensions is a rotation around a particular axis by an angle . if you allowed $a$ to contain something else than $\gamma^{\mu\nu}$ matrices which generate the lorentz group , you could get other groups . only for a properly chosen subset of allowed values of $\omega$ , you would get a closed group from the resulting exponentials ( under multiplication ) . in particular , if you allowed $a$ to be an arbitrary complex combination of any products of gamma matrices , well , then you would allow $a$ to be any complex $4\times 4$ matrix , and its exponentials would produce the full group $gl ( 4 , c ) $ - surprising , carl ? ; - ) it is not a terribly useful groups in physics because actions are usually not invariant under this " full group " , are they ? also , there are not too many groups in between $spin ( 3,1 ) $ and $gl ( 4 , c ) $ - i guess that there is no proper group of $gl ( 4 , c ) $ that has a proper $spin ( 3,1 ) $ subgroup . obviously , there are many subgroups of $spin ( 3,1 ) $ - such as $spin ( 3 ) $ , $spin ( 1,1 ) \times spin ( 2 ) $ , and others .
this is exactly the point of the symmetry factor . let 's call the term in $z$ that we are considering $t$ . without considering the symmetric exchanges that produce the symmetry factor , the contribution of each diagram to $t$ is simply its associated term without any numerical factor in front ( a factor of 1 ) . this is because when we count every possible exchange of vertices , propagators , derivatives , etc . that leaves the feynman diagram invariant , this number neatly cancels out the factorials in the taylor expansion and our choice of 1/6 and 1/2 in the field lagrangian . if the symmetry factor for a diagram is 1 , each of these exchanges gives rise to an identical term in the $t$ . when a diagram has a symmetry factor that is not 1 , some of these exchanges mentioned above no give rise to additional terms . hence the contribution of that particular diagram must be divide by the symmetry factor $s$ . this is a confusing topic , i wrote a note specifically on this kind of counting here
aluminium is paramagnetic , so external magnets will induce a magnetic field in it . however the magnetic susceptibility is only 2.2 $\times$ 10$^{-5}$ so i would be surprised if the induced magnetism would be high enough to be noticable . normally you can only measure paramagnetism in the lab with sensitive equipment . does the aluminium bar remain magnetic when you remove the magnets ? if not that would suggest it is induced magnetism , but if so there must be some ferromagnetic material present . either way a noticable magnetic field suggests your aluminium is not pure ( as anna suggests in a comment ) .
the point of ted bunn 's answer to the question you link is that when physicists call something a law , e.g. newton 's laws or the coulomb law , we mean that it is an approximate law . for example newton 's laws are a low speed , low density approximation , and in situations where speeds and densities are high we have to use an improved description i.e. general relativity . if you wind time backwards the universe gets denser and hotter ( hotter basically means things move faster ) so laws that are low speed and low density approximations will not apply . however this does not mean that the laws of physics have changed . it just means that some low energy approximations are not valid . we expect that there is some ( probably very complicated ) law that applies right back to the big bang , and this law of physics does not change with time , not even at the planck era . however , we have little idea what this ultimate law looks like , so neither we nor the physicist you were talking to can do more than guess at what happens .
in a $d$-dimensional euclidean space ( with positive definite norm ) , one has $$ \vec{\nabla} \cdot \frac{\vec{r}}{r^d} ~=~{\rm vol} ( s^{d-1} ) ~\delta^d ( \vec{r} ) , $$ cf . the divergence theorem and arguments involving either test functions and integration by part , or $\epsilon$-regularization , similar to methods applied in this phys . se answer . here ${\rm vol} ( s^{d-1} ) $ is the surface area of the $ ( d-1 ) $-dimensional units sphere $s^{d-1}$ . by similar arguments one may show that the identity $$ \vec{\nabla} ( r^{2-d} ) ~=~ ( 2-d ) \frac{\vec{r}}{r^d} , \qquad d\neq 2 , $$ contains no distributional contributions in $d$-dimensional euclidean space . for the related questions in minkowski space , one suggestion is to introduce an $\epsilon$-regularization in the euclidean formulation , and then perform a wick rotation , and at the end of the calculation , let $\epsilon\to 0^+$ .
heat of vaporization is related to enthalpy change , while dew point is related to free energy change , i.e. enthalpy plus entropy . that is why they are very different concerning relative humidity . the enthalpy of a gas is more-or-less independent of pressure or partial pressure , because gas molecules do not really interact with each other . at insanely-high pressures there would be some effect on enthalpy of course , but the effect at everyday pressures is very low . pressure mainly affects a gas via entropy not enthalpy . the enthalpy of a liquid is somewhat dependent on total pressure : a high pressure will push the molecules closer together and therefore change their interaction energies . but obviously the enthalpy of the liquid does not depend on what the gas partial pressures are , it can only depend on the liquid 's own total internal pressure . so the answer is : heat of vaporization , being related to enthalpy not entropy , has essentially no dependence on relative humidity . ( given a constant total air pressure ) -- update -- oops , whenever i wrote " enthalpy " i should have said " enthalpy per molecule " or " enthalpy per mole " [ "molar enthalpy" ] . you can check for yourself that the enthalpy per molecule of an ideal gas is independent of pressure or partial pressure . for a real-world gas , it is approximately independent . the " per mole " quantities are what matter for dew point etc .
your problem lies in assuming that $$ \nabla^2 = \frac{\partial^2}{\partial r^2} + \cdots $$ this is not the case , you need to use $$ \nabla^2 = \frac{1}{r^2}\frac{\partial}{\partial r}\left ( r^2\frac{\partial}{\partial r}\right ) +\cdots $$ then will you obtain the correct answer of $-1/2$ .
yes . yes . yes . see below . the falkenhagen relation ( nb : paywall , but ( a ) it is on the first page of the " look inside " option and ( b ) your university 's library might have a copy ) suggests that $$\frac{\eta_s}{\eta_0}=1+a\sqrt{c}$$ where $\eta_s$ is the solution viscosity , $\eta_0$ the solvent viscosity , $a$ a constant that depends on the electrostatic forces on the ions , and $c$ the concentration of the solute . there are other approximations , e.g. ones that go to higher order $c$ , that account for larger concentrations , so the above may not be exactly what you need for whatever purposes you have .
for simplicity of notation say $p = \frac{x - n}{x + n}$ given $\delta x$ is the uncertainty in x and $\delta n$ is the uncertainty in n then $\delta ( x - n ) $ = $\delta ( x + n ) = \sqrt {\delta ^2x + \delta ^2n}$ and therefore : $\delta p = p \sqrt{ ( \frac{\sqrt {\delta ^2x + \delta ^2n}}{x - n} ) ^2 + ( \frac{\sqrt {\delta ^2x + \delta ^2n}}{x + n} ) ^2}$ this is based upon equations 1b and 2b of the following reference : http://www.rit.edu/~w-uphysi/uncertainties/uncertaintiespart2.html
try to look at introduction to the replica theory of disordered statistical systems by v . dotsenko . in the following , i have written a possible answer to your question : \begin{equation} f=-\lim_{n\rightarrow\infty}\frac{1}{\beta n}\mathbb{e}\left [ \ln z_{j}\right ] \end{equation} where : $\mathbb{e}\left [ \mathcal{o}\right ] =\left ( \prod_{\left\{ i , j\right\} }\int dj_{ij}\right ) p\left [ j\right ] \mathcal{o} $ $z_{j}=\sum_{\sigma}e^{-\beta h\left [ j , \sigma\right ] } $ then labelling with $a$ the replicas : \begin{equation} z_{j}^{n}=\left ( \prod_{a=1}^{n}\sum_{\sigma^{a}}\right ) e^{-\beta\sum_{a=1}^{n}h\left [ j , \sigma_{a}\right ] } \end{equation} thus , remember that $\ln x=\lim_{n\rightarrow0}\frac{1}{n}\left ( x^{n}-1\right ) $: \begin{equation} f=-\lim_{n\rightarrow\infty}\frac{1}{\beta n}\mathbb{e}\left [ \ln\left ( z_{j}\right ) \right ] =-\lim_{n\rightarrow\infty}\lim_{n\rightarrow0}\frac{1}{\beta n}\mathbb{e}\left [ \frac{\left ( z_{j}^{n}-1\right ) }{n}\right ] =-\lim_{n\rightarrow\infty}\lim_{n\rightarrow0}\frac{1}{\beta nn}\mathbb{e}\left [ z_{j}^{n}\right ] \end{equation} but in general there are many issues concerning the commutation of the two limits .
yes , the expression seems correct . to compute the probability , you have to compute $|\langle 1|u^{ ( 1 ) }|0\rangle|^2$ . as $|0\rangle$ and $|1\rangle$ do not depend on time , you can just pull them inside the integral , and integrate the (1,0) component of the 2 x 2 matrix $v ( t_1 ) $ , denoted below by $v_{10} ( t_1 ) $ . so \begin{align}p and =\left|\frac{1}{i\hbar}\int^t_0 dt_1e^{-ih_f ( t-t_1 ) /\hbar} \langle 1|v ( t_1 ) |0\rangle e^{-ih_it_1/\hbar}\right|^2\\ and =\frac{1}{\hbar^2}\left| \int^t_0 dt_1e^{-ih_f ( t-t_1 ) /\hbar} v_{10} ( t_1 ) e^{-ih_it_1/\hbar} \right|^2\\ and =\frac{1}{\hbar^2}\left| \int^t_0 dt_1e^{+ih_f t_1/\hbar} v_{10} ( t_1 ) e^{-ih_it_1/\hbar} \right|^2\\ and =\frac{1}{\hbar^2}\left| \int^t_0 dt_1e^{+i ( h_f-h_i ) t_1/\hbar} v_{10} ( t_1 ) \right|^2\\ and =\frac{1}{\hbar^2}\left| \int^t_0 dt_1e^{+2i \mu b_0 t_1/\hbar} v_{10} ( t_1 ) \right|^2 \end{align} hope this helps .
first of all , this is just a change of basis , which is up to us to make . furthermore we should always choose a basis that makes our calculations easier , and hopefully makes things more intuitive . for a simpler example - just try finding the volume of a sphere in cartesian coordinates , its just a bad choice . second of all , you do not have to use a fourier basis , to my knowledge everything -loops renormalization etc can be done in a position basis . now as to why the fourier basis is a convenient choice : ( 1 ) it simplifies derivative terms in the lagrangian - as usual the fourier basis turns derivative expressions into algebraic ones , which are much easier to manipulate . ( 2 ) it it more intuitive - written in terms of a fourier basis the feynman rules are in terms of momentum . so for example at the vertices momentum is conserved - its just a nice tidy way to think about whats happening at the vertex . ( 3 ) even if you start in position space , one method for doing the integrals you will encounter when writing for your loop expressions will be going to momentum space - so you sort of cut this step out from the outset . ( 4 ) ( following up on vibert 's comment ) plane waves are the basis we do the experiment in . that is , we send in wave packets highly localized in p space , i.e. this is the exact solution we perturb around .
up and anti-up . or down and anti-down . funny thing is , both of those have the exact same quantum numbers - parity , spin , baryon number and the rest . so a neutral pion can be a mixture of ( u + anti-u ) and ( d + anti-d ) . there actually result two types of neutral " pion " that decay differently . one is actually heavier , and we call it the eta meson . oops i did not mention yet the strange and anti-strange quark combination , which also gets tangled into the mixes . . . but it is not important to the neutral pion .
notice that $$\frac{l_{\bigodot}}{\pi r_{\bigodot}^2} = \sigma t_{\bigodot}^4$$ so that , dividing through the relation for an arbitrary star and that for the sun gives : $$\frac{l/l_{\bigodot}}{r^2/r_{\bigodot}^2} = t^4/t_{\bigodot}^4$$ using the other relations $$\frac{ ( m/m_{\bigodot} ) ^{3.5}}{m/m_{\bigodot}} = ( t/t_{\bigodot} ) ^4$$ or $$\left ( \frac{m}{m_{\bigodot}} \right ) ^{2.5} = \left ( \frac{t}{t_{\bigodot}}\right ) ^4$$
from math and the power rule : $\dfrac{d ( x^2 ) }{dx} = 2x$ and we assume that l is a function of time : $\vec{l} = \vec{l ( t ) }$ . to refresh you on the chain rule : if x were a function of time , then $\dfrac{d ( f ( x ) ) }{dt} = \dfrac{d ( f ( x ) ) }{dx} * \dfrac{dx}{dt}$ . back to math : $\dfrac{d ( x^2 ) }{dx}$ is actually $2x\dfrac{dx}{dx}$ if you apply said chain rule . $\dfrac{dx}{dx}$ is commonly held to be equal to $1$ , so it is left out . as such : $\dfrac{d ( \vec{l}^2 ) }{dt} = 2\vec{l}\dfrac{d\vec{l}}{dt}$ . this is all assuming that we are operating element-wise on your vector $\vec{l}$ . that means it is the same as a normal ( scalar ) equation , but there is one scalar equation for each dimension of your vector . additionally , the notation $\vec{l}^2$ for a vector should be avoided . use dot product or cross product . this equation should be written as : $$2\vec{l}\cdot\dfrac{d\vec{l}}{dt} = \dfrac{d ( \vec{l}\cdot\vec{l} ) }{dt}$$ this equation is not true if $l^2$ were to be interpreted as a cross product ( $\vec{l}\times \vec{l} = 0$ ) of a vector with itself . as for the difference between single and double bars , both mean magnitude , but for vectors we like to use double bars to remove any similarity to the absolute value of a number : $$|-3| = 3$$ $$||\vec{\langle 3 , 4\rangle}|| = 5$$
molten solder has a low contact angle on ( clean ) copper . so if you looked at a cross section of the pipe joint as the solder was flowing in you had see something like : the solder is drawn into the joint in exactly the same way as water rises in a capillary tube . both are correctly described as capillary action .
the symbol $k_\textrm{b}$ pretty much invariably denotes boltzmann 's constant . apart from that , your question is asking about the statistical mechanics fact that for a canonical ensemble ( i.e. . a physical system in contact with a heat bath at some temperature $t$ ) the probability for the system to have energy $e$ is equal to $$p = \frac{1}{z}e^{-e/k_\textrm{b}t}$$ where $z$ is a normalization factor known as a partition function .
the square brackets mean antisymmetrization . that is : $$ x_{ [ a_1a_2\dots a_n ] } = \frac{1}{n ! }\sum_{p\in s ( n ) } \text{sign} ( p ) x_{a_{p ( 1 ) }a_{p ( 2 ) }\dots a_{p ( n}} $$ where $s ( n ) $ is the set of permutations of $n$ elements , and $\text{sign} ( p ) $ is the sign of the permutation $p$ , that is , $\text{sign} ( p ) =-1$ if you need an odd number of element exchanges , and $\text{sign} ( p ) =+1$ if you need an even number of element exchanges . in particular , $r_{ [ abc ] }{}^d = \frac{1}{6}\left ( r_{abc}{}^d+r_{bca}{}^d+r_{cab}{}^d-r_{bac}{}^d-r_{acb}{}^d-r_{cba}{}^d\right ) $ $\nabla_{ [ a}r_{bc ] d}{}^{e} = \frac{1}{6}\left ( \nabla_{a}r_{bcd}{}^{e}+\nabla_{b}r_{cad}{}^{e}+\nabla_{c}r_{abd}{}^{e}-\nabla_{b}r_{ac d}{}^{e}-\nabla_{a}r_{cbd}{}^{e}-\nabla_{c}r_{bad}{}^{e}\right ) $ you can " move " indices up and down using the metric tensor . that is , $$r_{abcd} = g_{de}r_{abc}{}^e , \quad r_{abc}{}^d = g^{de}r_{abce} . $$ the square brackets just affects the indices ; the $r$ is inside because the antisymmetrization affects indices both from $\nabla$ and from $r$ .
i was also wondering this for a while and found an not entirely complete derivation of the formula ( starting from page 14 ) . in which the following equation is used , $$ \ddot{\vec{r}}+\underbrace{\frac{\mu_i}{\|\vec{r}\|^3}\vec{r}}_{-a_i}=\underbrace{-\mu_j\left ( \frac{\vec{d}}{\|\vec{d}\|^3}+\frac{\vec{\rho}}{\|\vec{\rho}\|^3}\right ) }_{p_j} , $$ where $\vec{r}$ is the vector between the centers of gravity of a spacecraft and the celestial body with gravitational parameter $\mu_i$ , $\vec{d}$ is the vector between the centers of gravity of a spacecraft and the celestial body with gravitational parameter $\mu_j$ and $\vec{\rho}$ is the vector between the centers of gravity of celestial body $\mu_i$ and $\mu_j$ . and looking at the spacecraft from an accelerated reference frame of a celestial body than $a$ is defined as the primary gravitational acceleration and $p$ as the perturbation acceleration due to the other celestial body . and the soi is defined due to laplace as the surface along which the following equation satisfies , $$ \frac{p_j}{a_i}=\frac{p_i}{a_j} , $$ so $$ \frac{\mu_j\left ( \frac{\vec{d}}{\|\vec{d}\|^3}+\frac{\vec{\rho}}{\|\vec{\rho}\|^3}\right ) }{\mu_i\frac{\vec{r}}{\|\vec{r}\|^3}}=\frac{\mu_i\left ( \frac{\vec{r}}{\|\vec{r}\|^3}+\frac{\vec{\rho}}{\|\vec{\rho}\|^3}\right ) }{\mu_j\frac{\vec{d}}{\|\vec{d}\|^3}} . $$ this will not return a spherical surface , but it can be approximated by one when $\mu_i &lt ; &lt ; \mu_j$ , who is radius is equal to $$ \|\vec{r}\|\approx r_{soi}=\|\vec{\rho}\|\left ( \frac{\mu_i}{\mu_j}\right ) ^{\frac{2}{5}} . $$ this is where the slides of the lecture stop and i will try to to fill in the rest . when $\mu_i &lt ; &lt ; \mu_j$ than the soi will be relatively close to $\mu_i$ so $$ \|\vec{\rho}\|\approx\|\vec{d}\| , $$ and if you look at the figure on page 14 of the lecture sheets you can see that $\vec{d}$ and $\vec{\rho}$ almost point in opposite direction and form a triangle with $\vec{r}$ such that $$ \vec{\rho}+\vec{d}=\vec{r} . $$ by rewriting the definition of the surface using the approximation you get $$ \mu_j^2\frac{\vec{d}}{\|\vec{\rho}\|^6}=\mu_i^2\frac{1}{\|\vec{r}\|^3}\left ( \frac{\vec{r}}{\|\vec{r}\|^3}+\frac{\vec{\rho}}{\|\vec{\rho}\|^3}\right ) $$ the other approximation which has to be made is that $\|\vec{r}\|&lt ; &lt ; \|\vec{\rho}\|$ so that $$ \frac{\vec{r}}{\|\vec{r}\|^3}+\frac{\vec{\rho}}{\|\vec{\rho}\|^3}\approx\frac{\vec{r}}{\|\vec{r}\|^3} . $$ now the equation can be reduced to $$ \mu_j^2\frac{\vec{d}}{\|\vec{\rho}\|^6}=\mu_i^2\frac{\vec{r}}{\|\vec{r}\|^6} . $$ by generalizing $\vec{r}$ as a constant radius can make this problem one dimensional , so $\vec{r}=\|\vec{r}\|$ , which returns to final equation $$ \mu_j^2\|\vec{r}\|^5=\mu_i^2\|\vec{\rho}\|^5\longrightarrow\|\vec{r}\|=\|\vec{\rho}\|\left ( \frac{\mu_i}{\mu_j}\right ) ^{\frac{2}{5}} . $$
it is a standard terminology – and set of insights – not only in string theory but in quantum field theories or anything that can be approximated by ( other ) quantum field theories at . . . low energies . such a low-energy action becomes very accurate for the calculation of interaction of particles ( quanta of the fields ) of low energies , in this case $e\ll m_{\rm string}$ . equivalently , the frequencies of the quanta must be much smaller than the characteristic frequency of string theory . the previous sentence may also be applied in the classical theory : the low-energy effective action becomes accurate for calculations of interactions of waves whose frequency is much lower than the stringy frequency or , equivalently , whose wavelength is much longer than the string scale , $\lambda\gg l_{\rm string}$ . low-energy effective actions may completely neglect particles whose mass is ( equal to or ) higher than the characteristic energy scale , in this case $m_{\rm string}$ , because such heavy particles can not be produced by the scattering of low-energy particles at all – so they may be consistently removed from the spectrum in this approximation . the scattering of the light and massless particles that are kept may be approximately calculated from the low-energy effective action and this approximation only creates errors that are proportional to positive powers of $ ( e/m_{\rm string} ) $ so these errors may be ignored for $e\ll m_{\rm string}$ . you may imagine that there are corrections in the action proportional to $\alpha'$ or its higher powers that would make the effective action more accurate at higher energies but become negligible for low-energy processes . there are lots of insights – conceptual ones as well as calculations – surrounding similar approximations and they are a part of the " renormalization group " pioneered mainly by ken wilson in the 1970s . in particular , by " low-energy effective actions " , we usually mean the wilsonian effective actions . but they are pretty much interchangeable concepts to the 1pi ( one-particle-irreducible ) effective actions , up to a different treatment of massless particles . it is impossible to teach everything about the renormalization group and effective theories in a single stack exchange answer . this is a topic for numerous chapters of quantum field theory textbooks – and for whole graduate courses . so i just conclude with a sentence relevant for your stringy example : string theory may be approximated by quantum field theories for all processes in which only particles much lighter than the string mass are participating and in which they have energies much smaller than the string scale , too . if that is the case , predictions of string theory for the amplitudes are equal to the predictions of a quantum field theory , the low-energy effective field theory , up to corrections proportional to powers $ ( e/e_{\rm string} ) $ .
netwon 's law of cooling i think is exactly what you want to look at . roughly , heat exchange is porportional to the difference in temperature between the beer and the cooling apparatus ( be it water or something else ) and also to the contact area between the two . a secondary effect is the mixing of the beer . if the cold beer sits at the outer walls , the temperature difference is reduced and the middle will never cool . the reverse goes for the cooling liquid .
neutrinos are leptons , they have leptons number just like the charged leptons ( electron , muon , tau ) . weak interaction conserve not only the global lepton number , but the lepton flavor numbers as well . and that is how we identify their flavors : electron neutrinos participate in reactions that involve electrons and muon neutrinos participate in reaction that involve muons . we know that they are not the same because we have intense sources of muon neutrinos and muon anti-neutrinos ( from cosmic rays and accellerator created muons and anti-muons ) and we have intense sources of electron anti-neutrinos ( reactors ) . and when we put detectors in front of these sources the two flavor behave differently ; the most diagnostic interaction ( and a common one ) is quasi-elastic scattering in which a charged lepton out---a muon or an electron depending on the flavor of the beam . we do not have any intense sources for tau neutrinos , but they have been identified in the oscillated input to both opera ( which was designed for that measurement ) and icecube .
the whystringtheory page is written in a popularized style that makes it impossible to tell what they really have in mind . their statement does not make sense if interpreted according to the standard technical definitions of the terms . gr does not describe gravity as a force . in the system of units normally used in gr , with g=c=1 , force and power are unitless , so there is also no natural motivation for defining something like a planck force by analogy with the planck length , etc . possibly their " force " really means curvature , in which case this could be interpreted as a correct statement that gr breaks down when the riemann tensor corresponds to a radius of curvature comparable to the planck length .
ionization of this sort is not caused by positive or negative charges per say , but by an electric field . we can view the ionization as the electric field trying to accelerate the electrons and nucleus of an atom in opposite directions and ripping them apart . ( in reality , this is a minor effect . most ionization is caused by a chain reaction as free electrons are accelerated and collide with other molecules , ionizing them ) . therefore to get ionization we need to setup a large electric field . to achieve this generally a positive and negative charge are used . for example in a glow discharge . i guess if you had a single positive or negative charge in space it could generate a big enough electric field near it to cause ionization . this would be somewhat similar to what happens in lightning . you can also have ionization due to other sources such as thermal ionization in stars but i do not think that is what is asked about here .
studies have been done at a few popular frequencies , but in general this is hard to do with rf . you can get a feel for how it was done at 2ghz and 5ghz from this article http://www.ko4bb.com/manuals/05%29_gps_timing/e10589_propagation_losses_2_and_5ghz.pdf they also publish tables of their results which you might be able to scale to other frequencies as a starting point .
to calculate fuel consumption , you can typically use the tsiolkovsky rocket equation shown here ( without taking relativity into account ) : $$ \delta v = v_e * ln ( \frac{m_0}{m_1} ) $$ $m_0$ is the initial total mass , including propellant , $m_1$ is the final total mass , $v_e$ is the effective exhaust velocity , and $\delta v$ is the maximum change in the speed of the vehicle ( with no external forces ) the $\delta v$ from earth 's surface to leo from kennedy space center is $9.3 - 10\ ; km/s$ , and leo ( kss ) to geo is $4.24\ ; km/s$ . source : delta-v budgets , earth-moon space , high thrust assuming an exhaust velocity of 4.5 km/s , single stage rocket , we get $$ \frac{m_0}{m_1} = e^{\delta v/v_e} = e^{9.3/4.5} = 7.90 $$ $$ m_{propellant} = ( 1 - \frac{m_1}{m_0} ) m_0 = ( 1 - \frac{1}{7.90} ) m_0 = ( 1 - 12.66\% ) m_0 = 87.3\%\ ; m_0 $$ just getting the rocket to leo takes 87.3% of your initial mass . now let 's try this again for leo -> geo : $$ \frac{m_1}{m_2} = e^{\delta v/v_e} = e^{4.24/4.5} = 2.57 $$ $$ m_{propellant} = ( 1 - \frac{m_2}{m_1} ) m_1 = ( 1 - \frac{1}{2.57} ) m_1 = ( 1 - 38.98\% ) m_1 = 61.02\%\ ; m_1 $$ however , this is a percentage of the initial mass at leo , which is 12.66% of the original . let $m_1$ be the new initial mass for the rocket equation achieved after reaching leo ( from above ) and $m_2$ be the final mass after reaching geo . the fraction of launch mass $m_0$ would be $$ \frac{m_2}{m_0} = \frac{m_2}{m_1}*\frac{m_1}{m_0} = \frac{1}{2.57}*\frac{1}{7.90} = . 3898 * . 1266 = 4.93\% $$ $$ m_{propellant} = ( \frac{m_1}{m_0} - \frac{m_2}{m_0} ) m_0 = ( 12.66\% - 4.93\% ) m_0 = 7.73\%\ ; m_0 $$ thus , getting from earth 's surface -> leo takes about 87.3% of the mass of whatever you launch ejected at 4.5 km/s , where leo -> geo takes only 7.73% of of that mass at 4.5 km/s , leaving you in geo with 4.93% of what you started with . note : because of the difficulty from earth-> leo , we typically use multi-staged rockets which can ease the fuel required . because you said ' a rocket ' , i took that literally and did the calculation with a single-stage-to-orbit configuration .
strictly speaking , tension is not the same as force , although it is sometimes described as the magnitude of the ' pulling force ' experienced by an element ( such as a rope ) . the important thing to remember when resolving forces in classical mechanics and to understand tension is to apply newton 's three laws of motion . they are : 1st law : an object with no external force will not change velocity 2nd law : force = mass x acceleration 3rd law : every applied force ( action ) has an equal and opposite force ( reaction ) . so for the one dimensional cases you have given , think of the ' tension ' of the rope as the magnitude of any pulling force it would be experiencing , bearing in mind that this tension is not actually a force ( it has no direction ) , whereas the force whose magnitude it has , would be appear to be pulling the rope in opposite directions ( as per newton 's 3rd law ) . $t\leftarrow\rightarrow t$ so , back to your questions : 1 - when you pull on a rope tied to an immovable object , applying a force $f$ , it reacts with force $-f$ ( newton 's 3rd law ) and the ' tension ' in the rope is the magnitude of this force $f$ . $f\leftarrow\rightarrow f$ 2 - if you pull on a rope which is tied a mass $m$ ( initially at rest and free to move ) it will accelerate towards you ( newton 's second law ) . if you keep keep pulling the rope , keeping it taut by applying a constant force $f$ for a time $t$ and then remove the force thereby slackening the rope ( no tension ) , the final velocity of the mass will be $v=at$ ( neglecting friction ) . you can determine the force applied by $f=mv/t$ . 3 - if you apply a force of $x$ newtons pulling a rope tied to a mass $m$ which i am holding , the tension on the rope is $x$ as long as the mass is not moving . if i increase my pulling force to $y$ , the resultant force , $f=y-x$ will pull you along with the mass , towards me . note that we subtract the forces because they are acting in opposite directions . the resultant force $f$ will accelerate both you and the mass towards me at a rate $a=f/ ( m+m ) $ , where $m$ is your mass ( assuming the mass of the rope is negligible ) . the tension on the rope will be equal to the magnitude of resultant force on the rope , which is $t =\lvert x-ma\rvert = \lvert x-m\times \frac{f}{m+m} \rvert= \lvert x-\frac{ ( y-x ) m}{m+m}\rvert$ . note that if your mass , $m$ is negligible , the tension of the rope becomes $x$ , whereas if the mass of the body $m$ is negligible , the tension of the rope becomes $y$ . if your mass is equal to the mass of the body $ ( m=m ) $ then the tension on the rope is $ ( y-x ) /2 = f/2$ . if i apply a pushing force $y$ directly to the body of mass $m$ , while you pull on the rope tied to it by applying a force $x$ , the resultant force on the mass will be $f=x+y$ ( in your direction ) . the two forces are added not subtracted ( since they are applied in the same direction towards you ) . the body will therefore accelerate in your direction ( newton 's second law ) under the total force $a=f/m$ and the tension on the rope will be equal to the magnitude of the resultant force , $ ( f-y ) =x$ . note in this instance , your mass is irrelevant , because the rope does not transmit my pushing force $y$ to you ( a rope does not work under compression ! ) . 4 - if two bodies of mass $m$ are tied together with a rope and are moving in opposite directions at a speed $v$ , they will each have momentum with magnitude $mv$ but in opposite directions . since neither mass is experiencing a force , they will continue to move at at constant velocities in opposite directions ( newton 's 1st law ) , until the rope between them becomes taut . at that point , they will quickly decelerate and travel back towards each other . the rate of deceleration and subsequent speed at which they will travel towards each other will depend upon the ' elasticity ' of the rope as well as the amount of ' friction ' in the rope . in the case of an ' inextensible ' rope with no friction , the rope will have a non-zero ' impulse ' tension only at the instant it is taut . the two bodies will then move towards each other with the same velocity as they were previously moving away from each other ( due to conservation of momentum ) . until you realise that tension is not the same as force , you may experience a little tension yourself as you grapple with the concept ! as an aside , you may come across some textbooks on engineering mechanics or materials which describe tension as a type of pressure or stress ( force per unit area ) as in ' tensile stress ' applied to a truss member . if we define the area as a vector whose magnitude is the cross sectional area of the material under stress and whose direction is normal ( perpendicular ) to the cross sectional area , then the resulting force is the product of stress and area . in the most general sense , since the tension may have a different effect in different directions ( anisotropic ) , the resulting force is not necessarily in the same direction as the area . in a three-dimensional euclidean space , the tension is a tensor of rank 2 . this is a linear transformation ( mapping ) with $3^{2}$ co-ordinates , something like a ( 3x3 ) matrix , which when ' multiplied ' by the " area vector " produces the resultant " force vector " ( not necessarily in the same direction ) . however , since your examples are all dealing with forces in 1 dimension only , we can treat tension as a scalar ( that is , a tensor of rank 0 ) whose magnitude is that of the force exerted by the rope under tension .
at low velocities like this you can ignore special relativity and simply add the two velocities . this is really easy to see if you imagine yourself standing still and the earth moving under you . relative to you the gun should fire just like you were standing still . this is called an inertial frame of reference . you see the bullet leave at $400\: \mathrm{m/s}$ ( relative to you ) and the earth sees the bullet leave at $800\: \mathrm{m/s}$ .
presuming that there are not nonlocal constraints , a differential operator that is polynomial in differential operators is local , it does not have to be quadratic . my understanding is that irrational or transcendental functions of differential operators are generally nonlocal ( though that is perhaps a question for math . se ) . a given space of solutions implies a particular nonlocal choice of boundary conditions , unless the equations are on a compact manifold ( which , however , is itself a nonlocal structure ) . there is always an element of nonlocality when we discuss solutions in contrast to equations . [ for the anti -locality of the operator $ ( -\nabla^2+m^2 ) ^\lambda$ for odd dimension and non-integer $\lambda$ , one can see i.e. segal , r.w. goodman , j . math . mech . 14 ( 1965 ) 629 ( for a review of this paper , see here ) . ] edit : sorry , i should have gone straight to hegerfeldt 's theorem . schrodinger 's equation is enough like the heat equation to be nonlocal in hegerfeldt 's sense . there are two theorems , from 1974 in prd and from 1994 in prl , but in arxiv:quant-ph/9809030 we have , of course with references to the originals , theorem 1 . consider a free relativistic particle of positive or zero mass and arbitrary spin . assume that at time $t=0$ the particle is localized with probability 1 in a bounded region v . then there is a nonzero probability of finding the particle arbitrarily far away at any later time . theorem 2 . let the operator $h$ be self-adjoint and bounded from below . let $\mathcal{o}$ be any operator satisfying $$0\le \mathcal{o} \le \mathrm{const . }$$ let $\psi_0$ be any vector and define $$\psi_t \equiv \mathrm{e}^{-\mathrm{i}ht}\psi_0 . $$ then one of the following two alternatives holds . ( i ) $\left&lt ; \psi_t , \mathcal{o}\psi_t\right&gt ; \not=0$ for almost all $t$ ( and the set of such t 's is dense and open ) ( ii ) $\left&lt ; \psi_t , \mathcal{o}\psi_t\right&gt ; \equiv 0$ for all $t$ . exactly how to understand hegerfeldt 's theorem is another question . it seems almost as if it is not mentioned because it is so inconvenient ( the second theorem , in particular , has a rather simple statement with rather general conditions ) , but a lot depends on how we define local and nonlocal . i usually take hegerfeldt 's theorem to be a non-relativistic cognate of the reeh-schlieder theorem in axiomatic qft , although that is perhaps heterodox , where microcausality is close to the only definition of local . microcausality is one of the axioms that leads to the reeh-schlieder theorem , so , no nonlocality .
spinor algebra is very helpful in sorting out spinorial equations . in chiral representation the ( four-component ) wave function of the fermion field $\psi$ is considered as a formal sum of first rank spinor and first rank co-spinor fields : $\psi{} ( x ) =\left\{\xi{} , \ \dot{\eta{}}\right\}=\ \left\{\xi{} , \ 0\right\}+\left\{0 , \ \dot{\eta{}}\right\}=\ \left ( {\begin{array}{ cc} {\xi{}}^1 \\ {\xi{}}^2\\ {\eta{}}_{\dot{1}} \\ {\eta{}}_{\dot{2}} \end{array}}\right ) $ any quantities transforming like the products ${\xi{}}^{\mu{}}{\xi{}}^{\nu{}}$ , ${\eta{}}_{\dot{\mu{}}}{\eta{}}_{\dot{\nu{}}}$ , ${\xi{}}^{\mu{}}{\eta{}}_{\dot{\nu{}}}$ are called second rank spinors and denoted by $a^{\mu{}\nu{}}$ , $b_{\dot{\mu{}}\dot{\nu{}}}$ , $c_{\dot{\nu{}}}^{\mu{}}$ correspondingly . analogously one can define the spinors of higher ranks . there are 3 lorentz-invariant constant second rank spinors ${\epsilon{}}_{\mu{}\nu{}}$ , ${\epsilon{}}^{\dot{\mu{}}\dot{\nu{}}}$ and $c_{\mu{}\dot{\nu{}}}$ that play important role in spinor algebra . 1 . transition from subscript to superscript spinor indices spinors ${\epsilon{}}_{\mu{}\nu{}}$ and ${\epsilon{}}^{\dot{\mu{}}\dot{\nu{}}}$ are written as $ {\epsilon{}}_{\mu{}\nu{}}=\left [ \begin{array}{ cc} 0 and +1 \\ -1 and 0 \end{array}\right ] \hspace{10mm} {\epsilon{}}^{\dot{\mu{}}\dot{\nu{}}}=\left [ \begin{array}{ cc} 0 and -1 \\ +1 and 0 \end{array}\right ] $ transition from subscript to superscript spinor indices is established by means of spinors ${\epsilon{}}_{\mu{}\nu{}}$ and ${\epsilon{}}^{\dot{\mu{}}\dot{\nu{}}}$: $ {\xi{}}_{\mu{}}=\ {\epsilon{}}_{\mu{}\nu{}}{\xi{}}^{\nu{}} , \ \hspace{10mm} {\eta{}}^{\dot{\mu{}}}=\ {\epsilon{}}^{\dot{\mu{}}\dot{\nu{}}}{\eta{}}_{\dot{\nu{}}} $ 2 . complex conjugated spinors complex conjugates of spinors transform as co-spinors , and vice versa , so that we can denote \begin{array}{ cc} {\xi{}}_{\dot{\mu{}}}=\bar{{\xi{}}}_{\mu{}} \\ \\ {\eta{}}_{\nu{}}=\bar{{\eta}}_{\dot\nu} \end{array} 3 . charge conjugation second rank spinor $c_{\mu{}\dot{\nu{}}}$ ( often denoted as $i\sigma_2$ ) has the form $ c_{\mu{}\dot{\nu{}}}=\left [ \begin{array}{ cc} 0 and +1 \\ -1 and 0 \end{array}\right ] $ and can be used to transform first rank spinors to co-spinors and vice versa ( charge conjugation ) : $ {\chi{}}_{\dot{\mu{}}}=\ c_{\nu{}\dot{\mu{}}}{\xi{}}^{\nu{}} $ 4 . majorana condition majorana ( or neutrality ) condition is a lorentz-invariant property of electrically neutral spinors . it is expressed in the following form : $ {\eta{}}_{\dot{\mu{}}}=\ c_{\nu{}\dot{\mu{}}}{\xi{}}^{\nu{}} $ i.e. co-spinor $\dot{\eta{}}$ is charge conjugated to spinor $\xi{}$ . in spinor components this condition is written as follows : \begin{array}{c} {\xi{}}^1=- \ \bar{{\eta}}_{\dot 2} \\ \\ {\xi{}}^2=+ \ \bar{{\eta}}_{\dot 1} \end{array} 5 . mass term " mass term " in spinorial equations appears when there is " mixing " of spinor and co-spinor components . for instance , the free dirac equation in spinorial form can be written as \begin{array}{columns} {\partial{}}^{\mu{}\dot{\nu{}}} {\eta{}}_{\dot{\nu{}}}\ =\ -im \ {\xi{}}^{\mu{}} \\ \\ {\partial{}}_{\mu{}\dot{\nu{}}} {\xi{}}^{\mu{}}=\ -im \ {\eta{}}_{\dot{\nu{}}} \end{array} the most general form of manifestly lorentz-invariant spinorial equation with " mixing " is as follows : \begin{array}{cols} {\partial{}}^{\mu{}\dot{\nu{}}}{\eta{}}_{\dot{\nu{}}}=\ \ f_{\nu{}}^{\mu{}} \ {\xi{}}^{\nu{}} \\ \\ {\partial{}}_{\mu{}\dot{\nu{}}}{\xi{}}^{\mu{}}=\ \ {\dot{f}}_{\dot{\nu{}}}^{\dot{\mu{}}} \ {\eta{}}_{\dot{\mu{}}} \end{array} where $f_{\nu{}}^{\mu{}}$ and $\dot{f}^{\dot \mu}_{\dot \nu}$ are second rank spinor and second rank co-spinor correspondingly . let us demonstrate that free dirac equation is just a special case of the more general equation presented above . the most general form of spinorial equation can be made similar to free dirac equation , if we require that spinor $\xi{}$ and co-spinor $\dot{\eta{}}$ are eigenvectors of second rank spinor matrices $f_{\nu{}}^{\mu{}}$ and $\dot{f}^{\dot \mu}_{\dot \nu}$: \begin{array}{ ccc} f_{\nu{}}^{\mu{}} \ {\xi{}}^{\nu{}}=\ \lambda{}\ {\xi{}}^{\mu{}} \\ \\ \\ {\dot{f}}_{\dot{\nu{}}}^{\dot{\mu{}}} \ {\eta{}}_{\dot{\mu{}}}= {\lambda{}}\ {\eta{}}_{\dot{\nu{}}} \end{array} here $\lambda{}$ is an eigenvalue . it is important to note that this " eigenvector " condition is lorentz-invariant . now our spinorial equation has the form : \begin{array}{cols} {\partial{}}^{\mu{}\dot{\nu{}}}{\eta{}}_{\dot{\nu{}}}= \lambda{}\ {\xi{}}^{\mu{}}\\ \\ {\partial{}}_{\mu{}\dot{\nu{}}}{\xi{}}^{\mu{}}=\ {\lambda{}}\ {\eta{}}_{\dot{\nu{}}} \end{array} which is very similar to free dirac equation . now the " type " of equation ( i.e. . dirac , majorana or weyl ) will only depend on the special choice of second rank spinor matrices $f_{\nu{}}^{\mu{}}$ and $\dot{f}^{\dot \mu}_{\dot \nu}$ . in particular , we can choose $f_{\nu{}}^{\mu{}}$ and $\dot{f}^{\dot \mu}_{\dot \nu}$ as $f_{\nu{}}^{\mu{}}=\left [ \begin{array}{cc} 0 and m \\ -m and 0 \end{array}\right ] $ $\dot{f}^{\dot \mu}_{\dot \nu}=\left [ \begin{array}{cc} 0 and m \\ -m and 0 \end{array}\right ] $ the eigenvectors corresponding to the eigenvalue ( $\lambda = - im$ ) will be : $ \xi_d = \left [ \begin{array}{c} 1 \\ \\ -i \end{array}\right ] \phi ( x ) $ $ \dot\eta_d = \left [ \begin{array}{c} 1 \\ \\ -i \end{array}\right ] \phi ( x ) $ this case corresponds to dirac equation . alternatively , we can choose $f_{\nu{}}^{\mu{}}$ and $\dot{f}^{\dot \mu}_{\dot \nu}$ as $ f_{\nu{}}^{\mu{}}=\left [ \begin{array}{cc} im and 0 \\ 0 and -im \end{array}\right ] $ $ \dot{f}^{\dot \mu}_{\dot \nu}=\left [ \begin{array}{cc} -im and 0 \\ 0 and im \end{array}\right ] $ and the eigenvectors corresponding to the eigenvalue ( $\lambda = - im$ ) will be : $ \xi_m = \left [ \begin{array}{c} 0 \\ \\ 1 \end{array}\right ] \phi ( x ) $ $ \dot\eta_m = \left [ \begin{array}{c} 1 \\ \\ 0 \end{array}\right ] \phi ( x ) $ it is easy to check that spinors $\xi_m$ and $\dot\eta_m$ automatically satisfy majorana condition . hence , majorana equation is also the special case of the most general spinorial equation . both dirac and majorana spinors belong to $\left ( \frac{1}{2} , 0\right ) + \left ( 0 , \frac{1}{2}\right ) $ representations of $sl ( 2 , c ) $ , but they are only subspaces in the entire space of $\left ( \frac{1}{2} , 0\right ) + \left ( 0 , \frac{1}{2}\right ) $ representation . here you can read more about the most general form of spinorial equation . you will see that it can be used to develop the concept of electromagnetic mass and charge . to read more about spinors and spinorial algebra , you can read : laporte , o . and g . e . uhlenbeck , phys . rev . 37 , 1380 ( 1931 ) rumer , yu . b . and a.i. fet . group theory and quantum fields . moscow , ussr : nauka publisher , 1977
well wavefunctions are technically rays , so what you wrote is acceptable , although usually one would normalize the wavefunction again . once you have measured $\hat o$ and obtained $5$ , you are past the point of asking about probabilities . before measurement you have probabilities but it is like rolling dice . once the dice is rolled and you got $6$ , you do not ask what the probability is of having a $4$ because it is clearly zero . put another way , probability in quantum mechanics means that if you have $n$ identically prepared systems , and you perform the measurement on each one of them , then the fraction of times you get a result $x$ is given by some probability function $p ( x ) $ as $n\to \infty$ . if you only perform the measurement once , or have already performed the measurement , then the probability is simply not defined . btw , all this wavefunction collapse business is just an idealization , because all our experiments point to the fact that everything is ruled by quantum mechanics , and unitarity is a central tenet of quantum mechanics . wave function collapse is not a unitary process , decoherence and einselection is . cheers .
your book is incorrect . since $p$ is a length , $pv$ and $l$ cannot be equal by dimensional analysis alone . the specific angular momentum , however , does equal $pv$ , though it is very misleading to use the letter $l$ for it . a body 's specific angular momentum is its angular momentum divided by its mass , i.e. its angular momentum per unit mass . it captures the interesting kinematics of angular momentum ( i.e. . how bodies move in space through time ) but it is quite useless when it comes to dynamics ( i.e. . how bodies interact ) .
first thing , for a rotating ball , $i=\frac{2}{5}mr^2$ . you also need to be clear on what $\omega$ you are talking about . the kinetic energy of a rotating ball is $\frac12 i_{cm}\omega_{cm}^2 + \frac12 mv_{cm}^2$ . here , $v_{cm}=v$ . but , $\omega_{cm}=v_{cm}\times \frac{r}{r}$ . since $r&lt ; &lt ; r$ , we can take the net kinetic energy to be just $\frac12 mv^2$ ; the $\frac12 i_{cm}\omega_{cm}^2$ term becomes too small to matter . the main thing is is that you need to remember that the formula " kinetic energy=rotational energy + translational energy " works only when you consider all rotations about center of mass . you cannot just keep tacking on terms for each motion you see . even though the ball is revolving around the center of the loop , we still classify this as translational motion . if you do not do this , you can easily get confused while building the expression for ke . basically , for a ball of center of mass moment of inertia $i$ , mass $m$ , radius $r$ , rotating about itself with $\omega_cm$ , revolving in a circle of radius $r$ with $\omega'$ , the energy is not $\frac12 i\omega^2+ \frac12 ( i+mr^2 ) \omega'^2+\frac12 mv^2$ , it is $\frac12 i\omega^2+ \frac12 mv^2=\frac12 i\omega^2+ \frac m ( \omega'r ) ^2$ .
i will give it a shot . spoiler : i did this in the body frame so that the moment of inertia is time independent , before you get excited . . . starting with euler 's equations : $$ i_i\dot{\omega}_i+ ( i_j - i_k ) \omega_j \omega_k = 0 $$ and taking cyclic permutations of $i , j , k$ to get the three of them ; and in the absence of torques ( i ignore air friction ) . it is a symmetric top so $i=i_1=i_2 \neq i_3$ so write $$ \dot{\omega}_1 = -\frac{ ( i_3-i ) }{i}\omega_2 \omega_3 $$ $$ \dot{\omega}_2 = -\frac{ ( i - i_3 ) }{i}\omega_1\omega_3 $$ $$ \dot{\omega}_3=0 \implies \omega_3=k_1 $$ now for this problem the coin is spinning about one of the first two symmetric axies . i chose 1 . then consider small variations on the other two angular velocities from zero : $\omega_2 = \delta\omega_2$ , $\omega_3 = \delta\omega_3$ , and $\omega_1 \rightarrow \omega_1$ . so we make small changes in how the coin is rotating about a line through its center perpendicular to the coin , and about the other symmetric axis . in other words , it was spinning ideally like a coin would , then we changed the ideal to a little weird spinning . making the changes , and ignoring second order in perturbations : $$ \dot{\omega}_1=0 \implies \omega_1 = k_1 $$ $$ \frac{d}{dt} ( \delta\omega_2 ) =-\frac{ ( i-i_3 ) }{i}\omega_1 ( \delta\omega_3 ) $$ $$ \frac{d}{dt} ( \delta\omega_3 ) =0 \implies \delta\omega_3 = k_2 $$ then we can write $$ \frac{d}{dt} ( \delta\omega_2 ) =-\frac{ ( i-i_3 ) }{i}k_1 k_2 $$ everything on the r.h. s is a number so $$ \delta\omega_2 = -\left ( \frac{ ( i-i_3 ) }{i}k_1 k_2 \right ) t $$ so depending on how big $i$ is compared to $i_3$ will determine how $\delta\omega_2$ changes during the flip . if one uses a radius of $r=0.014$ m and $h=0.0015$ m for the hight of the coin , one gets a moment of inertia tensor like the following : $$ i=m ( 0.0000491875 ) \quad i_3 = m ( 0.000098 ) $$ which tells me that the variations are unstable . . . which i do not really believe since i have seen a coin in real life . so look this over . but i can not find anything wrong so i am going with it , and thinking that i can not really see a coin in real life up close while it is spinning . . . hope this helps .
in principle , it is possible , using , e.g. , high-current relativistic electron beams - please see , e.g. , the review http://arxiv.org/abs/physics/0409157 . @john rennie offers reasonable arguments , but the very real problems he mentions can be overcome - i do not have time to describe the specific mechanisms ( see the review ) . in experiments , propagation length in the atmosphere of at least 20 m was achieved ( 30 years ago ) . let me emphasize that the relevant research is done mostly for weapons development , not for power transmission , and the results are often classified , so i cannot be sure about the maximum propagation length currently achieved . let me also note that it may be easier to use electromagnetic radiation ( visible or , say , microwave ) for power transmission in the atmosphere . it is for you to decide if this is relevant to your question .
in the frame of reference of the body , is the centripetal force felt or is only the centrifugal force felt ? it depends on what you mean exactly . consider , for example , the amusement park ride dumbo at disneyland : . on this ride , passengers sit in mini dumbo replicas and are swung around in a circle . what forces do they feel ? well , firstly , they feel a centrifugal force radially outward . but this is not all . if that were the only force they felt , then in the frame that is stationary with respect to dumbo , they would accelerate radially outward . instead , they also feel a normal force of dumbo pushing them inward that is precisely equal to the centrifugal force , and as a result , as measured in the dumbo frame , they remain stationary with respect to dumbo . now , we know that if we were to analyze the same situation from the frame of reference of a person watching the ride from the ground , then we would say that there is only one force on the passengers , namely the normal force of dumbo on them , and this force causes the passengers to accelerate , namely to move in a circle . as a result , the convention is to call the normal force the " centripetal " force . i personally think this is terrible terminology that confuses students because it leads them to believe that " centripetal force " is somehow an independent thing that does not need to be comprised of real physical interactions with objects . . . by anywho . now , going back to the accelerated frame , we had noticed that there were two forces acting on the passengers , the ( fictitious ) centrifugal force , and the normal force . would you now call the normal force a " centripetal " ? if we are doing the analysis in the accelerating frame , then that would be extremely non-standard because in that frame , no circular motion is occurring . does a body only feel the effect of pseudo forces in an accelerated reference frame ? no ! just look at the above example ! the passengers feel the centrifugal force , but they also feel a normal force due to their interaction with dumbo ! in general , there can be all sorts of forces that an object feels in an accelerated frame that are not pseudo forces like friction , gravitational forces , electromagnetic forces etc .
firstly , note that they postulate those commutation relations in the beginning of section 3.5 in order to show that they are wrong , which they demonstrate in the ensuing pages . the ultimate point is to show that one needs to impose anti-commutation relations on fermionic fields . in fact , the correct relations are postulated in equation 3.96 ; \begin{align} \{\psi_a ( \mathbf x ) , \psi_b^\dagger ( \mathbf y ) \} and = \delta^{ ( 3 ) } ( \mathbf x - \mathbf y ) \delta_{ab} \end{align} you could then ask , are these equivalent to the anti-commutation relations of the mode operators that they write in ( 3.97 ) ? namely , \begin{align} \{a^r_\mathbf p , {a^s_\mathbf q}^\dagger\} = \{b^r_\mathbf p , {b^s_\mathbf q}^\dagger\} = ( 2\pi ) ^3\delta^{ ( 3 ) } ( \mathbf p - \mathbf q ) \delta^{rs} \end{align} and the answer is yes . to show that the second set implies the first , write the fields in their integral mode expansions , compute the anti-commutator of these integral expressions , and apply the anti-commutators between modes . to show that the first set implies the second , invert the integral expressions for the fields in terms of the modes to obtain integral expressions for the modes in terms of the fields , and do the analogous thing . main point . the commutators/anti-commutators between fields are equivalent to the commutators/anti-commutators between modes .
the tl ; dr version : even if we could form a synthetic event horizon , it would not help us learn about the black hole interior . the long version : the phenomena you describe where light essentially orbits a black hole is called the " photon sphere " and it does not happen at the event horizon . the radius of a black hole , $r$ is where the event horizon is and the photon sphere where photons can orbit ( unstable orbits ) is $\frac{3}{2} r$ . you do not actually have to form an event horizon to form the photon sphere although i am pretty sure the density needed to form the photon sphere is greater than the quark degeneracy pressure and so you had still get a collapse into a black hole . unfortunately there are not any magic tricks and everything we know about general relativity and quantum mechanics says that even if we were infinitely advanced technologically we would not be able to learn anything about what happens beyond the event horizon . other than the possibility of a firewall at the horizon , there is nothing special or interesting about either the photon sphere or horizon . the only way we are going to learn about what is inside of a black hole is with a good theory of quantum gravity . no amount of making one to run experiments will help us .
you can analyze this by imagining a tiny tiny gap between the two masses . physically we have exactly that , as the electrons at the surface of the first block are certainly not in contact with the electrons at the surface of the second block . then we have a series of two collisions : the first is the initial impulse , the second is the collision between the two objects . the answer will differ , of course , depending on the degree of elasticity of the collision .
the presence or absence of inversion symmetry in a medium has a direct impact on the types of nonlinear interactions that it can support ; specifically , media which do have inversion symmetry cannot support nonlinear effects of even order . the reason for this is that adding an even harmonic to the fundamental will yield an asymmetric dependence of the electric field , and this is only possible if the medium itself is asymmetric . " inversion symmetry " is the property that the material remain the same when you change the position $\mathbf r_j$ of each particle $j$ to its ' inverse ' , $-\mathbf r_j$ . because we can typically move materials around , this is equivalent to saying that the medium is identical to its mirror image . this is true , for example , for a gas ( if the $\mathbf r_j$ are random , then the $-\mathbf r_j$ will also be random ) , or for crystals like body-centred cubic lattices : on the other hand , certain lattices do not have this symmetry , like you get when you displace the atom in the centre towards one of the faces of the cubic unit cell : here the symmetry is broken , and if you invert all the coordinates with respect to ( say ) the middle atom , you no longer recover the original lattice . it is fairly easy to see why this asymmetry is necessary for second-harmonic generation . suppose that the second harmonic has a maximum in the $+z$ direction at the same time as the fundamental , so that they add constructively . if you wait for half a period of the fundamental , its electric field will be in the $-z$ direction , but the second harmonic will have undergone a full period and will be pointing towards $+z$ , so that the two interfere destructively . this means that the maximum of the total field is stronger in the $+z$ direction than it is in the $-z$ direction . this is actually quite remarkable ! in particular , the medium itself needs to be asymmetric to " know " which direction the stronger fields need to go towards . if the medium has inversion symmetry , then the $+z$ and $-z$ directions are equivalent , and an asymmetric output like this is impossible . consider , on the other hand , a process with odd order like third harmonic generation . here a half-integral period of the fundamental is also a half-integral period of the harmonic , which means that they add in the same direction in each half-cycle , and the output is symmetric . in fact , this selection rule - the forbiddenness of even harmonics in inversion-symmetric media - goes all the way up the harmonic scale , including phenomena where the field is strong enough to break out of the perturbative treatment in david 's answer . the nicest example is high-harmonic generation , which you get in gas jets when the driving laser is intense enough that the laser 's electric field roughly equals the internal electric fields of the atom . in that case , you get a reasonable response at very high harmonic orders ( the record is around 5000 ( doi ) ) , and you get a very flat plateau in which the response does not really drop with the harmonic order : note , in particular , that all the even harmonics are missing . in here i am plotting the response of a single , symmetrical atom , which means that even orders cannot appear . for inversion-symmetric media , then , this relation holds all the way up the even-integers scale .
you are missing the concept of critical mass . in a small amount of uranium , like a single pellet , some fraction of the atoms will decay spontaneously every second . when enough of this is put in proximity , then the emissions of some of the decaying atoms kick other atoms just right so that they fission too . as a result , more atoms fission than you would predict from just the probability of a single atom doing so . the main difference between a small pile of uranium and a large one is that in a large one you get a chain reaction . power plant reactors rely on this chain reaction mechanism to get the large amounts of output power . the control rods control how much the emissions of fissioning atoms can hit other atoms , thereby controlling the overall reaction rate . in reality , this description is over simplified . there can also be moderators envolved that sortof convert some of the fission results into stuff that can kick other atoms to fission when without the moderator they would not . however , that is a aside to this question .
the index of refraction of a material can be less than 1 at high frequency , this is called " anomalous dispersion " and it happens as you cross an energy level of certain materials . it means that the phase velocity of light of a certain frequency is higher than c . if the index of refraction is constant , as it is for long wavelengths , n has to be bigger than 1 to avoid superlumimal communication . the principle of energy conservation in a static environment forbids a frequency shift for a photon , since this would add energy or take away energy , and nothing in the medium is changing with the right frequency to do that . but light entering a moving medium shifts frequency . photons can combine to make one of double the frequency in a strong light beam in a nonlinear medium , and this corresponds to making higher harmonics of the classical field .
yes , at least for functional enough lens or mirrors . as long as the answer is " yes " , the lenses or mirrors will produce an image that is totally sharp . if the answer were " no " , a point-like source of light would always look like a fuzzy disk . the lenses and mirrors and telescopes may be optimized at least for a whole " two-dimensional locus " where the light source may be located to produce sharp images . the shape of the lenses may be constructed so that this condition is satisfied exactly : a real condition ( the light ray gets to the right point ) has to be satisfied for each distance $y$ from the axis - but one has at least one variable , $x ( y ) $ ( the thickness of the lens ) , to adjust for each $y$ , too . in the first subleading approximation , the shape is always the same : as a function of the vertical coordinate $y$ , the thickness of the glass in the horizontal direction goes like $a+by^2$ . that approximates a circle , parabola , hyperbola , or anything else , up to errors of order $o ( y^4 ) $ . if you want to totally neglect those terms , it is like neglecting $o ( \theta^4 ) $ terms , quartic in the angle . in this approximation , the angle between the light ray and the horizontal line that is needed for convergence is small and linearly depends on $y$ . the linear dependence of the angles on $y$ - how much the direction of the light ray is changed when switching from the air to the glass or back - is equivalent to the quadratic shape of the mirror sketched above ( the angle change is a derivative of the shape because this derivative determines the slope of the glass at a given point ) . this explains why it is not a " miraculous conspiracy": the change of the angle is a linear function of $y$ so the shape of the glass must be a quadratic function of $y$ . in reality , where the $o ( \theta^4 ) $ terms in the shape can not be neglected , the light sources may be away from the plane where the convergence was guaranteed , and in that case , the answer will be " no " and the image will inevitably be fuzzy . however , it is important to note that e.g. telescopes that observe stars " at infinity " may always be adjusted so that all the images are sharp . because the shape of the lens actually has two functions , $x_{left} ( y ) $ and $x_{right} ( y ) $ to be adjusted , one may actually guarantee that the condition " light rays converge " is exactly satisfied in a whole region of 3d space , at least in some situations .
protons and their spin 1/2 were measured and used in experiments long before quarks were a gleam in theorists ' eyes . spin was studied with the stern gerlach experiment the stern–gerlach experiment involves sending a beam of particles through an inhomogeneous magnetic field and observing their deflection . the results show that particles possess an intrinsic angular momentum that is closely analogous to the angular momentum of a classically spinning object , but that takes only certain quantized values . polarized protons are protons in a beam with the spins oriented in one direction , up for example . there are experiments that try to explore how the spin of the proton is built up by the spins of the constituent quarks and gluons , and these need polarized proton beams .
one possibility is that your approach is hugely sensitive to measurement uncertainty : integrating noisy signals can be a huge problem . you might think about ways to average the measurements over time , so that they are ( hopefully ! ) more stable . another possibility is that your instrument does not output the data in quite the format that you think . have you verified that your inputs are sensible ?
well , you end up with integrals but those are very , very easy to solve for the harmonic oscillator ! since your problem is already formulated in terms of the raising and lowering operators $a_+$ and $a_-$ . recall that $$a_+ | n \rangle = \sqrt{n+1} | n+1 \rangle$$ $$a_- | n \rangle = \sqrt{n} | n - 1 \rangle$$ $$ a_+ a_- | n \rangle = n | n \rangle$$ where $|n\rangle$ is short-hand for $|\psi_n^0 \rangle$ . these relations make it almost trivial to compute matrix elements involving eigenstates of the harmonic oscillator and those operators . just as an example , let 's prove that all eigenstates have zero expectation value for $x$: we know $x$ is proportional to $a_+ + a_-$ . inserting that into the matrix element gives us $$\langle n | x | n \rangle \propto \langle n | a_+ | n \rangle + \langle n | a_- | n \rangle = \sqrt{n+1} \langle n | n+1\rangle + \sqrt{n} \langle n | n- 1 \rangle = 0$$ because eigenstates are orthogonal . edit : continuing with the derivation where you left off , you see that you get a non-zero contribution only if $m = n+1$ or $m = n-1$ . so in the infinite sum over all states $m \not= n$ , only two terms will contribute , making it possible to easily carry out that sum : just add those two non-zero terms .
your argument is correct . indeed ( in $d$ dimensions ) when $c = 0$ , unitarity implies that $t_{\mu \nu} \equiv 0$ , but by assumption any local cft must have a stress tensor $t_{\mu \nu}$ satisfying $$ t_{\mu \nu} ( x ) \times \mathcal{o} ( y ) \sim \delta \mathcal{o} ( y ) $$ ( a ward identity ) where $\delta$ is the dimension $\mathcal{o} . $ so unless $\delta = 0$ ( the unit operator ) , you fail to satisfy this condition .
white light does not have a single frequency , it is a mixture of all of the colors . similarly pink does not have a single frequency it is a mixture of red with white i.e. white with extra red . if the color does not appear in the color spectrum ( rainbow ) then it is not a single frequency but a mixture of different frequencies . in an led light is produced by an electron dropping from an excited state to a relaxed energy state which produces a light of a given energy ( frequency ) however all of the light is going to be of the same color since the bandgap ( energy difference ) is going to be the same for all of the electrons . it is possible ( see here ) to put three different leds on one chip corresponding to red green and blue . this produces the effect of a white light . but is not truly white since it only contains red green and blue and no orange yellow or violet / purple . your computer screen cannot produce true whites either for the same reason . if you see yellow on your computer screen , there is not light of the frequency yellow only the right amount of red green and blue to convince your eyes nerves to fire the same as if it were a true yellow frequency .
the assumption of constant velocity throughout the tube arises from the following : assuming constant density conservation of mass in addition to some other stipulations that should be found in the problem description ( like the fact that the tube has constant area ) , and a few other hydrodynamics simplifying assumptions ( like assuming the velocity is constant over the cross sectional area , or an equivalent assumption ) . one way to argue for the constant velocity assumption is to employ the common $\dot{m}=\rho v a$ equation , which gives flow rate given density , velocity , and area , assuming constant velocity over the cross sectional area . out of these , area is constant from the basic specifications of the tube , and density can be written to have two dependencies , pressure and temperature , $\rho=\rho ( p , t ) $ . both the pressure and temperature change over the length of the tube , but it is small enough that if you are working under ordinary " bathtub " like conditions they do not matter . the fact that $\dot{m}$ is constant over the length of the tube can be established by simply noting that the system is at steady state , and conservation of mass dictates that the mass flow rate going in is the same mass flow rate going out , since the tube has a constant amount of fluid in it . to do a little bit more overkill , $d\rho/dp$ is a constant that plays a role in determining the speed of sound in the fluid , but is very small for " incompressible " fluids , although incompressibility would mean that value is zero . temperature can have a significant effect , as $d\rho/dt$ is very much nonzero , but the frictional heating over the pipe just is not much .
you are on the right track with integration by parts , but you need to get the right differential equation to integrate and then apply integration by parts to . to do this , first take the partial of the original differential equation with respect to $\lambda$ . then combine the resulting equation with the original differential equation to obtain a differential equation where $\lambda$ is not explicitly present as a multiplicative factor anywhere . at this point , you should be able to manipulate the equation that results to obtain $$ \psi ( \partial_r^2\partial_\lambda\psi ) + \frac{2m}{\hbar^2} \psi u \psi - \partial_r^2\psi\partial_\lambda\psi =0 $$ i will let you do the rest . by the way , are you sure there is no multiplicative factor of $\frac{\hbar^2}{2m}$ on the right hand side of the result you want to prove ? hope that helps ! cheers !
antenna gain is often expressed in the following form , $g = \frac{4\pi a_{e}}{\lambda^{2}}$ , where $a_{e}$ is the effective area of the antenna and $\lambda$ is the operating wavelength . however , using the antenna equation , the effective area can be expressed in terms of the main beam width ( 3db width ) $\omega$ , $a_{e} = \frac{\lambda^{2}}{\omega}$ . assuming both antennas operate at the same wavelength , the following is true , $g_{b} = g_{a}\frac{\omega_{b}}{\omega_{a}}$ . i hope this is helpful . p.s. this essentially follows from the definition of gain .
the drag of a fluid acting on an object inside is the flow of momentum through the boundary of the object . the momentum conservation law is the entire content of the navier stokes equation , which can be written in integral form : $$ {\partial\over \partial t} \int_r \rho v^i = - \int_{\partial r} \rho v^i v\cdot \hat{n} + \int_{\partial r} ( p \hat{n} + \nu ( \rho ) \nabla v^i ) \cdot \hat{n} $$ where $\hat{n}$ is the normal to the boundary of $r$ , $p$ is the pressure , $\nu$ is the viscosity ( as a function of the density $\rho$ ) , and v is the velocity . the left hand side says that you are looking at the flow of total i-component of momentum out of region r . the first term on the right is the physical amount of momentum flowing out of the boundary of r by the flow of the fluid . the last term is the flow of momentum through the boundary of r due to forces at the edge . using the divergence theorem , you learn that $$ \int_r {\partial\over\partial t} ( \rho v^i ) + \partial_j ( \rho v^i v^j ) - \partial_i p - \nabla\cdot ( \nu \nabla v^i ) d^dx = 0$$ and you conclude that the ns equations are satisfied . $$ {\partial\over\partial t} ( \rho v^i ) + \partial_j ( \rho v^i v^j ) - \partial_i p -\nabla \cdot ( \nu \nabla v^i ) $$ if you expand this out , and use the continuity equation , you will recover the more standard forms , but this is the form in which it is most transparently a continuity equation for the momentum flow . so you see that the flow of the i-component of momentum into any region r due to the fluid , which is the i-th component of the force exerted by the fluid on whatever is inside r , is given by the boundary integral $$ f^i_r = - \int_{\partial r} \rho v^i v\cdot \hat{n} + \int_{\partial r} ( p\hat{n} + \nu \nabla v^i ) \cdot \hat{n}$$ for the case where you have a solid object that the fluid cannot penetrate , the velocity is perpendicular to the object 's surface , and the first term is zero ( obviously-- the first term describes the momentum carried along with the fluid , and this is not entering r ) . so the drag is the integral of two terms across the surface , the pressure across the object , which tells you how much the object is pushing to get the water to go around , and the gradient of the velocity , which describes how the viscosity pulls the object . for a moving object , this works at one instant to tell you how much momentum is entering or leaving the object , which is the instantaneous drag force .
short answer : give up now . conservation of baryon number , lepton number , strangeness , charm , bottomness , and most importantly charge are making your job hard . long answer : i am ignoring charge conservation issues from time to time ; as its no fun if you take that into account well , not baryons or leptons . and we have conservation of charge in the way as well . baryons we have conservation of baryon number $b=\frac{1}{3} ( \text{number of quarks-number of antiquarks} ) $ , which has opposite values for particle-antiparticle pairs . the only way to convert a baryon to its antibaryon would be to bombard it with a different antibaryon with ( negative ) twice the baryon number ( and twice the negative charge as well ) . and that would require energy ( to create the antibaryon ) . leptons for leptons , we have conservation of lepton numbers $l_e , l_{\mu} , l_{\tau}$ . i doubt any reaction exists where a lepton becomes an antilepton , as leptons can only have lepton number $\pm1$ , and particle reactions involve two reactants ( iirc ) . i am not considering neutrino oscillations here ; they make it possible to do stuff like this . the only naturally abundant neutrino is the electron one ( maybe not naturally abundant , but easy to generate passively ) ; unfortunately , it is got a very low energy . and the entire neutrino oscillation thing is still debated . anyways , your neutrino source would be a beta-decaying substance ; and there are different , easier ways to get energy from that . mesons it is easier for mesons , though there still are restrictions ( conservation of charmness , topness , bottomness ) . due to these restrictions , the only conversions possible would be for these mesons ( and their antimesons ) : $u\bar{d} ( \pi^\pm ) , d\bar{t} , u\bar{t}$ . the last one is anyways in a superposition with its antiparticle , so we get a total of two pairs of particle-antiparticle conversion reactions . guage bosons ( i am not sure if the gauge bosons can do such reactions ) analysis of feasability of mesons anyways , the antimatter=limitless energy is something rather overhyped . over here , we have two possible candidates . $d\bar{t}$ is not an everyday particle ( dunno if it is even been synthesized ; top quarks are pretty hard to create , and wikipedia has no data in its list of mesons ) , and anyways is pretty unstable . you had have to pump in a bunch of energy to create it , and that defeats the purpose . feasibility of pi-meson $u\bar{d}/\bar{u}d ( \pi^\pm ) $ is interesting , as it is a common particle in nuclei . but it is bound inside ( not exactly--its part of the virtual particle " sea " , but that makes it worse ) , and decays pretty fast . so you had have to break apart the atom ( yes , you had get energy from that , but not if you separate it into nucleons ) , " catch " a pi meson , convert it to an antiparticle by means of the " quantum switch " , and collide it with another pi meson ( alternatively , without the " quantum switch " , you can just find an opposite pi-meson ) . and that will give you a tiny amount of energy as compared to your efforts . you would also need to supply some oppositely-charged particle to conserve charge . making it more complicated . conclusion so nope , it is not a good energy source . it does not work for protons/neutrons/electrons ; it only will work for two particles ( one more if we consider neutrino oscillations ) . neither of them is feasible . stick to fission-fusion .
you are in free field theory , right ? in this case there is a discrete $\mathbb{z}_2$ global symmetry which takes $\phi\to -\phi$ . applying this to your path integral gives that the three point function is equal to its opposite , so it must be zero . this also holds for interacting theories as long as each term in the interaction potential contains an even amount of $\phi$ 's , e.g. $v ( \phi ) =\phi^4$ . p.s. you forgot the path integral weight $\exp ( is ) $ in your formula .
if you think about it , " vertical " is generally defined as the direction in which things fall - that is , parallel to the local gravitational field . so by that definition of " vertical , " no , it is not possible . but if you were to use a different definition of " vertical " and " horizontal , " sure , it might be possible .
temperature is the measurement of kinetic energy per unit particle mass . since you have added the same amount of heat energy to each object , the finite object will have a higher temperature because its heat energy is distributed across a smaller collection of mass . taking something 's temperature is indeed a meaningful measurement ; )
yes , it is possible , but not in the way you suggest . it is totally possible for poisson 's equation with non-continuous electric field ; the charge density ( and hence the second derivative of potential ) could be non-finite . this is less esoteric than it seems ; the electric field of a point charge is discontinuous at the location of the point charge itself . there is another way to show that poisson 's equation is not enough , though . consider an electric field which is constant , ie $\mathbf{e} = \mathbf{e_0}$ for some constant $\mathbf{e_0}$ . we can deduce that the charge density is $0$ everywhere , but this is independent of $\mathbf{e_0}$ ! so if we are given that $\rho=0$ to start with and nothing else , there is no unique solution for the electric field . this shows that in addition to poisson 's equation , we must have boundary conditions . in most cases it is implicitly assumed that the boundary conditions are that $e \to 0$ at large distances .
i will go mostly with chad 's argumentation . larger equals slower , which should excite relatively more low frequency sound . also note that the larger the blast ( hopefully ) the further away the observer is . and air is not a perfectly elastic acoustic medium , some energy is lost , and the higher frequencies attenuate quicker than shorter , so distance will selectively filter out the higher frequencies . also , note , explosion usually means detonation . a detonation is an exothermic reaction which spreads by the compression ( adiabatic ) heating from the shockwave , and the chemical energy maintains the shockwave . a shockwave is essentially a highly nonlinear soundwave , and as the overpressure decays with distance from the source is will grade into a soundwave . fireworks ( pyrotechnics ) are not explosives , but are the ( relatively ) slow reaction of chemicals ( combustables , and an oxidizer ) due to heat . fireworks may generate shockwaves in air , if the package ruptures at sufficiently high pressure . likewise volcanic blasts are not detonations , but shockwaves formed by the escape of high pressure gas . if an explosion is fast compared to the sound frequencies the detector is sensitive to ( probably human ears in your case ) , then we might be able to model the explosion as a delta-function in time . a delta function should equally excite all frequencies , so it should be a simple matter of distance attenuation of sound waves . an explosion close to a solid surface , creates an amazing effect i have heard called a mach-stem ( although wikipedia does not produce anything useful for this term ) . in any case , at fixed distance the near surface shock wave is much stronger than the shockwave at height above ground . in essence the ground effect part of the shockwave weakens roughly only as 1/r rather that the 1/r**2 one would expect for a free air spherical blastwave . i do not know what effect this has on the sound spectrum ( pitch ) , but you need to be at a much larger standoff distance from a near groundblast than from a samesized high altitude blast because of it .
the line you wrote 105,000 $\frac{kg}{m*s^2}$ * 330 k * 287 $\frac{m^2}{s^2*k}$ has to read in fact $\frac{105,000 \frac{kg}{m*s^2} }{ 330 k * 287 \frac{m^2}{s^2*k}} = 1.11 \frac{kg}{m^3}$ . this comes from the gas law $p=\rho \ r \ t $ where $p$ is the air pressure and $\rho$ is the air density . solving for $\rho$ you get $\rho =\frac{p}{r t} $ from which the numerical solution follows .
you define the density auto-correlation function as $$s_{\rho\rho} = \langle \delta \rho ( \mathbf{x}_1 ) \delta \rho ( \mathbf{x}_2 ) \rangle$$ where $\delta \rho ( \mathbf{x} ) = \rho ( \mathbf{x} ) - \langle \rho ( \mathbf{x} ) \rangle$ is deviation from the local mean value . the fourier transform of $s_{\rho\rho}$ is related to the structure-factor $$s ( \mathbf{q} ) = \langle \rho \rangle^2 ( 2\pi ) ^d \delta ( \mathbf{q} ) + \frac{1}{v}\int d^d x_1 d^d x_2 e^{-i \mathbf{q} \cdot ( \mathbf{x}_1-\mathbf{x}_2 ) } s_{\rho\rho}$$ where $\langle \rho \rangle$ is the average density of the whole system , i.e. , $v \langle \rho \rangle = \int d^d \mathbf{x} \ , \rho ( \mathbf{x} ) $ . the structure factor $s ( \mathbf{q} ) $ is related to the pair-correlation function $g ( \mathbf{x} ) $ via $$s ( \mathbf{q} ) = \langle \rho \rangle \big [ 1 + \langle \rho \rangle \int d^d \mathbf{x} \ , g ( \mathbf{x} ) e^{-i \mathbf{q} \cdot \mathbf{x}} \big ] $$ if the system is isotropic , then $g ( \mathbf{x} ) = g ( |\mathbf{x}| ) $ is called the radial-distribution function . most of these relations are already in the wikipedia page linked in the question .
if you need to set up a field of magnitude $e$ in a dielectric medium of relative permittivity $\epsilon_r$ then you need to originally supply an electric field of $\epsilon_r e$ , now since a larger field would be provided with more charge , and bringing new charge to an existing charge configuration would require energy , definitely more energy is required to set up an electric field in presense of a dielectric medium than it would be in perfect vacuum .
consider two $h$ atoms $rotating^{ ( 1 ) }$ about their centre of mass , now both the atoms are electrically neutral and far apart so thay neither strong nor weak nuclear force comes into consideration . if they were to stay in this state only then they would never combine and form $h_2$ but what happens is that when the electrons of each atom are moving during various instances dipoles are formed and london dispersion forces come into play due to these interactions the 2 atoms move towards each other and at a certain distance acheive a stable equilibrium , and are therefore bonded to each other . on the other hand lets suppose you somehow got 2 balls of $h$ in elemental form and placed them near each other , the atoms of both ball will still get induced dipoles but since all the dipoles are randomly oriented , their would not be any significant overall dipole moment which may force the two balls to come close and join . this was highly simplified version of what happens with iron and other elements and compounds , on thd atomic scale considerable dipole moments are developed even in neutral atoms/compounds , these motivate the further bonding to form other objects such as lattices , crystals , sheets and so on . on the other hand at the macro scale , no considerable attraction forcd develops between two neutral objects which may motivate them to join/bond . but indeed if you have to oppoisitely charged objects they may join together . also to join objects on macroscales we have different proceses such as different types of welding etc $ ( 1 ) $ i said rotating about centre of mass to avoid considering attraction due to gravitational interaction between the two atoms .
let me focus on the context of rigorous equilibrium statistical physics ( see , e.g. , georgii 's book " gibbs measures and phase transitions" ) . there , one works with probability measures on infinite systems , often on a lattice ; let me assume it is $\mathbb{z}^d$ . in this context , a macroscopic observable is defined as a function $o:\omega\to\mathbb{r}$ ( where $\omega$ is the set of all configurations $ ( \omega_i ) _{i\in\mathbb{z}^d}$ ) which does not depend on the values of any finite sets of spins $\omega_i$ ( technically , one says that such an observable is measurable with respect to the tail $\sigma$-field ) . let me give you some examples of such observables , in the simple case of ising-type systems , i.e. , with $\omega=\{-1,1\}^{\mathbb{z}^d}$ . let $\sigma_i$ denote the spin at $i$ , $\sigma_i ( \omega ) =\omega_i$ . $\circ$ averages of local observables , e.g. , $$ \lim_{\lambda\uparrow\mathbb{z}^d} \frac1{|\lambda|}\sum_{i\in\lambda} \sigma_i\ ; . $$ $\circ$ events such as " there are no infinite connected components of $-1$-spins " . in both cases , changing a finite number of spins does not modify the value of the observable $o$ . one nice thing about this definition is that one can prove very generally that such observables take deterministic values ( i.e. . , are almost surely constant ) with respect to any pure phase ( extremal gibbs measure ) . in other words , they do not fluctuate ( remember one deals here with infinite systems ) .
unfortunately i can not add comments yet . how much knowledge theoretical physics do you know , before i make too many assumptions ( or lack thereof ! ) ? i do not think that this question can have an acceptable answer . this will depend greatly on what model for the dark energy ( de ) you are using . there are dozens , and new ones continuily being created , and we do not know which ( if any ) are correct . the standard cosmology ( $\lambda$cdm ) has a cosmological constant , $\lambda$ , which acts as the dark evergy to cause accelerating cosmic expansion today . this $\lambda$ has to be added in by hand however , which is not very pretty . theorists like to have everything be for a reason . ' most ' models treat the de as a scalar field , $\phi ( x ) $ , say , which could have any of dozens of different properties in order to give a specific de model . i do not know too much about all of them . usually an expansion of the potential for this field $\phi ( x ) $ will yield a constant term , which becomes our '$\lambda$' . the one that i do know something about , ( and i do not want to be accidently plugging my own here , my apologies ) is one in which , long story short , the interaction coupling scales inversely with the density of matter nearby . thus you are question of ' how far apart must two test masses be ' is not relevant . the question becomes a question of ' how large must the nearby density be to screen out the $\phi ( x ) $ interaction . you could have two such test masses in our earth atmosphere which are only of the order of 1mm apart and the interaction may be neglible . alternatively in the vast nothingness of space and time you could have two test masses many mpc apart and the $\phi ( x ) $ interaction could be large . even still , we do not know for sure , because we can only set bounds on these sorts of questions using current experimental results . it may be the case that there is a ' fifth interaction ' , or it may not . we do not know yet for certain . typically the scales involved for the galaxy clusters that we measure to be moving away from us at an acceleration rate are on the order of 100 mpc or more .
instead of using the chain rule ( although it of course gives the same answer ) expand the square of the $i^\mathrm{th}$ term in the sum parentheses to obtain $$ \alpha_i^2 - 2\alpha_i\alpha_{i+1}+\alpha_{i+1}^2 $$ differentiating this with respect to $\alpha_i$ gives $$ 2\alpha_1 - 2\alpha_{i+1} $$ now , from the $ ( i-1 ) ^\mathrm{th}$ term $$ \alpha_{i-1}^2 - 2\alpha_{i-1}\alpha_i + \alpha_i^2 $$ you get an additional $$ -2\alpha_{i-1} + 2\alpha_i $$ when you take the $\alpha_i$ derivative . putting these results together gives the answer in the book .
geant is a framework---which means that you use it to build applications that simulate the detector and physics you are interested in . the simulation can include all of physics and the complete detector including electronics and trigger ( i.e. . you can write your simulation so that it output a data file that looks just like the one you are going to get from the experiment 1 ) . 2 the various parts of geant are validated by being able to correctly predict the outcomes of experiments . particular models are tuned on well known physics early in the analysis of the data . this allows you to get simulated optical properties , detector gains and so on correctly matched to the actual instrument . geant is also heavily documented . read the introduction and the first two chapters of the user 's guide for application developers , which will give you the basics . after that you can delve into the hairy details in the physics and software references . there is much , much too much to cover in a stack exchange answer . ( i mean literally . . . . if i tried i would end up overrunning the 32k characters per post limit . ) it helps to know that geant4 derives from geant3 and earlier efforts . this thing has a history that goes back for decades and has been tested in thousands of experiments large and small . the use in the higgs search goes something like this we have a theory--the standard model--which tells us what coupling to expect for the particle we hope to detect we write ( and test ) a geant physics module implementing those physics . maybe more than one . we may need to write a new event generator or tweak an existing one in parallel to this effort . you construct a geant simulation of your detector . you include a simulation of the electronics , trigger and so on . 3 you simulate a lot of data from the desired channel and from possible interfering channels ( including detector noise and backgrounds ) . you are going to use a cluster or a grid for this , because it is a big problem you combine this simulated data . you run your analysis on the simulated data . 4 you extract from these results an " expected " signal . actually , you did all of the above at lower precision several times during the design and funding phase and used those result to determine how much data you would have to collect , what kinds of instrumentation densities you needed , what data rate you had to be able to support and so on ad nauseum . once you have got the data , you start by showing that : you can detect lots of well known physics in your detector ( to validate the detector and find unexpected problems ) 5 that your model correctly represents the detector response to that well known physics ( to let you debug and tune your model ) then you may need to re-run some of the " expected " processing . only then can you try to compare data to expectation . 6 1 indeed the data format is often thrashed out and debugged from the mc before the experiment is even built . 2 for big , complicated experiments like those at the lhc geant is usually paired with one or more external event generators . in the neutrino experiments i am currently working on that means genie and cry . not sure what the collider guys are using right now . 3 for speed reasons we often simulate the electronics and trigger outside of geant proper , but this decision is made on a case by case basis . 4 indeed the analyzer is often programmed and debugged from the mc output before there is real data . 5 this is also where most of the actual repetition of results in the particle physics world comes from . you will not get funding to repeat bigexper 's measurement of the wingding sum rule , but if your proposed nextgen spectrometer can do that as well as your spiffy new physics ( tm ) it helps your case with the funding agencies . 6 many of these steps will be done by more than one person/group in the collaboration to provide copious cross-checks and protection against embarrassing mistakes . ( see also , opera 's little issue last year . . . )
this choice is closest to the the correct one . i am tempted to shrug of the entire particle exchange as a mere numerical convenience ; a discretization of the maxwell equations perhaps . i am reluctant to say " virtual particle " because i suspect that term means something different to what i think it means . and virtual exchange is a correct description , because during the interaction the exchanged particle is not on mass shell . keep in mind that in the microcosm of particles nature is quantum mechanical . the particle scattering on another particle and the momentum and energy and quantum number exchanges between them are all described by one wave function , one mathematical formula that gives the probability for the interaction to take place in the way it has been ( will be ) observed . . thus it is not a matter for " knowing " but a matter of " being " . the feynman diagrams that give rise to the " particle exchange " framework are just a mathematical algorithm for the calculations and help in understanding how to proceed with them . to see how classical fields are built up by the substructure of quantum mechanics see the essay here .
my incomplete understanding is that the width of pulsars generally seems to depend on their periods and the angle between their magnetic axes and rotational axes . the general trend is for shorter period pulsars to have pulse widths that are a larger fraction of their periods . see on the pulse-width statistics in radio pulsars for some more detail . it looks like there are pulse half widths as narrow as a few degrees ( ~1/100 of a period ) , and some that are as wide as ninety degrees ( 1/4 of a period , meaning that if we were able to see both pulsing sides , the pulsar is on ~1/2 the time ) . given that level of variation , i am not sure if there is a very general sort of plot of intensity versus phase . some are nicely gaussian pulses , with a nice interpulse at 180 degree phase separation , whereas others have much more structure . take a look at some of the figure in multi-frequency integrated profiles of pulsars ( there are a total of 34 pulsar profiles plotted , and it should be available to all as it was posted to arxiv ) .
the proposed partition of physics into thermodynamics , classical mechanics , and quantum mechanics is quite arbitrary . to take just one conspicuous example , statistical mechanics does not fit , as it is the discipline that mediates between these three areas of physics . the physics and astronomy classification scheme ( pacs ) http://www.aip.org/pacs/pacs2010/individuals/pacs2010_regular_edition/index.html , ''an internationally adopted , hierarchical subject classification scheme , designed by the american institute of physics ( aip ) '' , partitions physics instead into the physics of elementary particles and fields nuclear physics atomic and molecular physics electromagnetism , optics , acoustics , heat transfer , classical mechanics , and fluid dynamics physics of gases , plasmas , and electric discharges condensed matter : structural , mechanical and thermal properties condensed matter : electronic structure , electrical , magnetical , and optical properties interdisciplinary physics and related areas of science and technology geophysics , astronomy , and astrophysics . it would be quite meaningless to put each of these general containers under the hood of either thermodynamics , classical mechanics , or quantum mechanics . in many cases , there is an interplay between thermodynamical , classical , and/or quantum aspects that bear on a given physical problem . but let me respond to the challenge by proposing a systematic view of physics not by its phenomena but by classifying it in terms of 7 orthogonal criteria . the first criterion is methodological , and distinguishes between applied physics ( ap ) , didactical physics ( dp ) , experimental physics ( ep ) , theoretical physics ( tp ) , and mathematical physics ( mp ) . the other six criteria are defined in terms of the six limits that play an important role in physics : the classical limit ( $\hbar\to 0$ ) distinguishes between classical physics ( cl ) , in which $\hbar$ is negligible , and quantum physics ( qu ) where it is not . the nonrelativistic limit ( $c\to \infty$ ) distinguishes between nonrelativistic physics ( nr ) , in which $c^{-1}$ is negligible , and relativistic physics ( re ) where it is not . the thermodynamic limit ( $n\to\infty$ ) distinguishes between macroscopic physics ( ma ) , in which microscopic details are negligible , and microscopic physics ( mi ) where they are not . the eternal limit ( $t\to\infty$ ) distinguishes between stationary physics ( st ) , in which time is negligible , and nonequilibrium physics ( ne ) where it is not . the cold limit ( $t\to 0$ ) distinguishes between conservative physics ( co ) , in which entropy is negligible , and thermal physics ( th ) where it is not . the flat limit ( $g\to 0$ ) distinguishes between physics in flat space-time ( fl ) , in which curvature is negligible , and general relativistic physics ( gr ) where it is not . a particular subfield is characterized by a signature consisting of choices of labels ( or double arrows between labels ) in some categories . a few examples : thermodynamics : ma , th equilibrium thermodynamics : ma , th , st classical mechanics : cl , co classical field theory : cl , co , ma general relativity : cl , re , ma , gr quantum mechanics : qu , nr relativistic quantum field theory : tp , qu , re , mi statistical mechanics : tp , mi$&lt ; -> $ma , th precision tests of the standard model : tp$&lt ; -> $ep , qu , re , mi , st , co the empty signature is simply the field of physics itself . in each category , one can choose no label , a single label , or an arrow between two labels , giving $1+5+5*4/2=16$ cases for the first category , and $1+2+1=4$ cases in the six other categories . thus the classification splits physics hierarchically into $16*4^6=65536$ potential subfields with different signatures , of which of course only the most important ones carry conventional names . let me give what i think is a particularly useful subhierarchy of the complete hierarchy . this subhierarchy splits the whole physics recursively into quadrangles of subfields . on the highest first level , we split physics according to the cold limit and the flat limit . this gives a quadrangle of first level theories of thermal physics in curved spacetime ( th cu ) thermal physics in flat spacetime ( th fl ) conservative physics in curved spacetime ( co cu ) conservative physics in flat spacetime ( co fl ) together with two first level interface theories statistical physics ( th&lt ; -> co ) geometrization of physics ( cu&lt ; -> fl ) these first level theories describe very general principles on the theoretically most fundamental level of physics . on the second level , we split each first level theory according to the eternal limit and the thermodynamic limit . this gives in each case a quadrangle of theories of nonequilibrium particle physics ( ne mi ) nonequilibrium thermodynamics ( ne ma ) physics of bound states and scattering ( st mi ) equilibrium thermodynamics ( st ma ) together with two second level interface theories long time asymptotics ( ne&lt ; -> st ) thermodynamic limits ( ma&lt ; -> mi ) these second level theories describe physics on a level already close to many applications , especially outside physics , though still lacking detail . on the third , lowest level , we split each second level theory according to the nonrelativistic limit and the classical limit . this gives in each case a quadrangle of theories of relativistic quantum physics ( re qu ) relativistic classical physics ( re cl ) nonrelativistic quantum physics ( nr qu ) nonrelativistic classical physics ( nr cl ) together with two third level interface theories nonrelativistic limit ( re&lt ; -> nr ) quantization and classical limit ; quantum-classical systems ( qu&lt ; -> cl ) these third level theories describe physics on the usual textbook and research level . ( maybe someone who likes to do graphics can illustrate this hierarchy with appropriate diagrams . )
the earliest stars did not have planets primarily due to a lack of metals . metals in this sense is an element ( with some extra properties that are not relevant in this context ) heavier than helium . the very article that you linked to references this . this leads to the following : stars without metals tend to not last very long . metals in a star act to slow down the reaction speed of the fusion . without metals , the stars quickly get to a state where they will explode . short time scales do not allow for enough time to form planets . metals seem to be the initial building block of planets . this wikipedia article discusses the current leading theories for rocky and gas planets . basically , they both start with a rock forming that is big enough , leading to a chain effect which ends up to be a planet . rocks can not form from hydrogen and helium , making planet formation difficult .
malicious counter example the desired object is a sphere of radius $r$ and mass $m$ with uniform density $\rho = \frac{m}{v} = \frac{3}{4} \frac{m}{\pi r^3}$ and moment of inertia $i = \frac{2}{5} m r^2 = \frac{8}{15} \rho \pi r^5$ . now , we design a false object , also spherically symmetric but consisting of three regions of differing density $$ \rho_f ( r ) = \left\{ \begin{array}{l l} 2\rho\ , and r \in [ 0 , r_1 ) \\ \frac{1}{2}\rho\ , and r \in [ r_1 , r_2 ) \\ 2\rho\ , and r \in [ r_2 , r ) \\ \end{array} \right . $$ we have two constraints ( total mass and total moment of inertia ) and two unknowns ( $r_1$ and $r_2$ ) , so we can find a solution which perfectly mimics our desired object .
they talk about contact voltages because it affects “stop voltage” as measured by their instruments , but it doesn’t affect $\delta u_{\rm stop} / \delta\nu$ since aforementioned contact voltages are assumed independent of illumination conditions . because a photovoltaic cell made of a homogeneous piece of material won’t work . a piece of conductor will not produce any voltage , whereas you will be unable to extract the current from a dielectric . no , it doesn’t . you can also read an interesting discussion at ambiguity on the notion of potential in electrical circuits ? . btw , where does wikipedia tell you about the contact voltage ? https://en.wikipedia.org/wiki/volta_potential , or… ? a posting that vaguely refers to “books and wikipedia” without any specificity demonstrates a shortage in internet communication skills , especially given that the people refers to several different things as to contact voltages . i don’t known ( i’m not an english speaker , usually ) .
from your comment it seems you know $x_0$ , $x_1$ , $x_2$ ( in your question you state you want to know $x_2$ ) , $t_1$ and $t_2$ . now as stated in my comment $$x_1=x_0+v_0t_1+\frac 1 2 a t_1^2$$ and $$x_2=x_1+\frac 1 2 a ( t_2-t_1 ) ^2 . $$ solving for $a$ gives $$a=\frac{2 ( x_2-x_1 ) }{ ( t_2-t_1 ) ^2} . $$ thus , $$x_1=x_0+v_0t_1+\frac{ ( x_2-x_1 ) }{ ( t_2-t_1 ) ^2} t_1^2 . $$ solving for $v_0$ is all that can be done now . all this can be adapted to whatever exactly is given in your problem .
first of all , earthquakes are not necessarily transverse waves . both transverse and longitudinal waves are there in seismic waves . these waves depend upon both modulus of elasticity and density of medium . longitudinal p-waves ( primary ) have properties similar to that of sound and due to their compressive and rarefactive wave motion , they reach us faster than transverse s-waves ( secondary ) . these are the before-socks as what we call . it is the effect of s-waves which cause the shear fracture of the rocks ( due to high amplitudes ) . they result of rapid sideways movement of faults thereby causing rocks to shake randomly around their hypo-center . they typically travel up to 60% of the velocity of p-waves . they necessarily require the shear-modulus of the medium . but , they are the most destructive type of all . all the answers for your questions are yes . ' cause they have already been a done-deal . . ! refer wiki for a more detailed description regarding the topic . . .
yes easily . fiber-fed 5-10kw nd-yag lasers are commonly used for cutting metal in machine shops . fibers are so transparent , especially when designed for a single wavelength laser , that the power loss and so heating in the fiber is very small . it is generally less than an optically fed laser where dirt accumulates on the lenses . many systems have a thin fiber wrapped around the main power delivery fiber with a small low power laser diode shining through it . if there is a fault in the main fiber , the high power laser beam will leak out and cut the thinner fiber . the loss of light in the wrapping fiber signals the controller to kill the power to the main laser .
the velocity distribution is related to the temperature by the maxwell-boltzmann distribution . you cannot find the disordered kinetic energy of each particle because they are randomly distributed , however you can use the maxwell-boltzmann distribution to calculate what that random distribution looks like .
no . the world we observe with our five senses is three dimensional . two independent measurements are enough to calculate the three dimensional position of everything , which is what our brain does with the input of two eyes . more eyes would only over constrain the solution , and might help in low lighting or long distance estimates when the errors are large .
the wok is a design that gets the maximum efficient use of energy for cooking and needs a minimum of heat source . a flat bottom equivalent pan transfers the heat to the food from the bottom plane , and the walls are just containers radiating most heat away , since the food is at the bottom . a wok focuses the heat from the walls to the food being cooked ( think of a parabola and its focal point ) . the walls partake in the cooking process effectively giving more area contact to the food than the flat bottom one .
assume horizontal rod sum of forces equals acceleration of center of gravity $$ t - m g = m \ddot{y}_c $$ sum of moments about center of gravity equals mass moment of inertia times angular acceleration $$ t \frac{l}{2} = i \ddot{\theta} $$ as the bar rotates by a small angle $\theta$ the center of gravity height is found by $y = -\frac{l}{2} \theta$ or by differentiating twice , $\ddot{y} = -\frac{l}{2} \ddot{\theta}$ together it all comes as $$ \begin{aligned} t and = m g - m \frac{l}{2} \ddot\theta \\ t \frac{l}{2} and = \left ( \frac{m}{12} l^2 \right ) \ddot \theta \end{aligned} $$ solve the above for $t$ and $\ddot{\theta}$ . hint the value does not have to be more than $\frac{m g}{2}$ .
the block is accelerating , but that is due to the pseudo force , not the frictional force , which is zero . you can not just look at the horizontal forces on the block to determine whether or not static friction will hold . you have to consider what is going on with the other surface as well . here 's a simpler situation to consider first in an inertial frame . imagine two identical blocks stacked on top of each other . there is friction between the two blocks , but there is no friction between the lower block and the horizontal surface . now push on the upper block while holding the lower block in place . it will slide if $f_\text{applied} &gt ; \mu m g$ , as you describe . but if you instead push on each block with identical forces , they will move together and there will be no frictional force ( because alone they would have identical motions ) . here , the top block is accelerating even though the frictional force is zero ; it is the applied force that is causing it to accelerate . same thing with your situation . the block and surface alone would have the same motion as each other in the non-inertial frame , so there will be no frictional force . yes , the block is accelerating , but that is due to the pseudo force , not the frictional force , which is zero .
this is a tricky question and in my opinion some elder physicists are deliberately try to confuse students by using euphemisms when characterizing phase transitions . roughly speaking ( there might be counter examples , please comment . i am interested of finding all of them ) : first order phase transition : finite correlation length scales as e.g. $k^\alpha , \alpha = 2$ ( short or finite range interaction ) in fourier space ( 1d ) second order phase transition : infinite correlation length scales as e.g. $k^\alpha , \alpha = 1$ ( long or infite range interaction ) in fourier space ( 1d ) scale invariance ( i think soc goes here . . . ) note that there is a continuous transition with exponent $\alpha$ that escapes my mind . also $\alpha$ is dependent of dimension and probably something else ( see e.g. anne tanguy et al . from individual to collective pinning : effect of long range interactions , pre 1998 ) . also daniel fisher has a nice paper , collective transport in random media : from superconductors to earthquakes . also i just stumbled upon : http://www.tcm.phy.cam.ac.uk/~bds10/phase/introduction.pdf which has a nice overview . in general the purpose of these correlation lengths , roughness exponents and orders of phase transitions is just to find universality classes . the goal is to group the phenomena together and say " look , all these systems have properties of x , y and z . this simple model has the same properties . so by explaining the simple mode , i explain all these systems . " simple characterization of phase transitions can be found at : http://link.aps.org/doi/10.1103/revmodphys.76.663
the first way to consider the problem , as we have a simple rigid body is to consider two points on it , say one end ( the one that will touch the table first ) and the center of mass ( or the other end ) . the embedding configuration space is thus $r^2 \times r^2$ ( the three coordinates of the end of the rod and the three coordinates of the other ) if we consider that the rod can only move in the vertical plane including a line on the table ( i hope this is the actual question . . . ) . the action of the surface lead to the [ holonomic ] constraint equation that can be written $f ( x , y ) = 0$ which is the one of a surface ( here a curve ) in the configuration space $r^2$ . in addition to this constraint equation we have the newton 's equation which is $m \frac{d^2}{dt^2} \vec{r} = \vec{f}+\vec{n}$ ( where $n$ is the force of constraint . this is for one end . for the other ( or for the center of mass ) you have two similar equations , with the constraint being that the other end ( or the center of mass ) is at a given distance to the other end . in that view , we can consider the points as being free and we have 4 newtonian equations and 2 constraint equations thus reducing the degrees of freedom to 2 . we can also consider that the constraints do not work and that they are perpendicular to the plane of the table : $\vec{n} = \lambda \vec{\nabla}f$ where $f$ is the one of the constraint equation . newton 's equation can be rewritten $\langle m \frac{d^2}{dt^2} \vec{r} - \vec{f} , \vec{\xi} \rangle = 0$ where $\xi$ is a tangent vector defined as $\langle \vec{\xi} , \vec{\nabla} f \rangle= 0$ ** ( 1 ) ** . this is d'alembert principle . as the components of $\vec{xi}$ are constrained by ( 1 ) , this lead to 2n ( newton ) - 2 ( constraints ) independant relations . the next step is to choose a system of coordinates where ( 1 ) is automatically satisfied : these are the generalized coordinates . the rod is modeled when in addition to specifying the coordinate of the end that is on the plane you also specify the angle that the rod makes with the surface : we can choose to independent generalized coordinates : $x$ and $\theta$ . the motion of the system ( now the system is not free anymore ) will take place in the manifold $m$ where $x$ and $\theta$ are independent coordinates . these $ ( x , \theta ) $ are usually written $q = ( q_1 , q_2 ) $ . this manifold is the direct product of $r$ and $s^1$ ( the angles take values in a 1-torus , a segment whose ends are identified ) . it can then be derived that the motion in these coordinates follow lagrange 's equations . the lagrangian is this case as the natural $l ( x , \theta , \dot{x} , \dot{\theta} ) =t-v$ form . for your case ( b ) this problem is simplified : x is fixed and you should only care about $\theta$ , you have $l = \frac{m}{2} l^2 \dot{\theta}^2 - l m g sin\theta$ where $l$ is the distance between one end and the center of mass . from lagrange 's equation you obtain $m l \ddot{\theta} = -m g cos\theta$ from that you can obtain the complete motion of the rod . for the case ( a ) you have to consider a more complete lagrangian : $l = \frac{m}{2} ( l^2 \dot{\theta}^2 - 2 l \dot{x} sin\theta \dot{\theta} + \dot{x}^2 ) - l m g sin\theta$ this lagrangian lead to a more complicated motion ; if i did the calculation correctly we have $\ddot{\theta} = - \frac{g}{l} cos\theta$ $\ddot{x} = l sin \theta \ddot{\theta}$ . to obtain the constraint , you can solve this and then you go back to newton 's equations . in the case ( b ) you have in addition to force $\dot{x} = \ddot{x} = 0$ to obtain the tangential constraint .
you are right ; hamiltonian for center of mass of hydrogen atom should be : $h=\displaystyle\frac{p^2}{2 ( m+m_e ) }$ where $p$ is momentum of hydrogen atom ( please check what is $p$ in your book ) . now you can also write it as : $h=\displaystyle\frac{p^2}{2m} ( \frac {m}{m+m_e} ) $ $=\displaystyle\frac{p^2}{2m} ( \frac { ( m+m_e ) -m_e}{m+m_e} ) $ $=\displaystyle\frac{p^2}{2m} ( 1-\frac {m_e}{m+m_e} ) $ $=\displaystyle\frac{p^2}{2m}-\frac {m_e}{2m ( m+m_e ) }p^2$ so there must be some typos in your book .
super novae were known a long time ago . but they were not understood as a the death throes of a star . in spite of the apparent immutability of the heavens , chinese astronomers were aware that new stars could appear . in 185 ad , they were the first to observe and write about a supernova , now known as the sn 185 . the brightest stellar event in recorded history was the sn 1006 supernova , which was observed in 1006 and written about by the egyptian astronomer ali ibn ridwan and several chinese astronomers . the sn 1054 supernova , which gave birth to the crab nebula , was also observed by chinese and islamic astronomers . but it was not until later we understood they had a life-cycle . the greek philosopher aristotle even proposed that the stars were made of a special element , not found on earth , that never changes . the chinese might have been the first to suggest the idea as they took careful note of " guest stars " which suddenly appeared among the fixed stars . it would seem that another person who suggested the idea was probably tycho brahe ( 14 december 1546 – 24 october 1601 ) as he coined the term nova meaning new star . and likely with this new mind set births brings deaths . he also is famous for realizing stars are very far away ( due to parallax ) . in 1572 he witnessed a super nova and in 1573 he published a small book , " de nova stella " ( the new star ) based on the super nova he saw . ( most super novae were assumed to be new stars , not dieing stars ) . the event of understanding stars die probably just fell out of understanding what stars are . i am not sure you can point to one event or person in history that could prove to know stars die prior to understanding stars themselves .
the beta function beyond 1-loop is scheme dependent , but the physical quantities you can extract from it are scheme independent ( at least if you can compute the beta function at all order ) . for instance , even though the fixed point position is scheme dependent , the critical exponent are not . on the other hand , if you stop at a given order in the loop expansion , it is possible that physical quantities depends on the scheme . in the case of the functional rg ( such as wilson-polchinski or wetterich 's version ) , the physical quantities should be regulator independent , but as you make approximations , a spurious dependence on the regulator appears ( for instance , when you change a parameter b of the regulator , the critical exponent eta depends on b ) . a way to cope with this , it to use the pms ( principle of minimum sensitivity ) , which tells you that when the real value of eta is given by the extremum of eta ( b ) . one last point : when one does a loop expansion ( say a 4-epsilon expansion ) , one needs to resum the series , which involve some ( non-physical ) parameters . the final result should be independent of the resummation techniques , but because one knows only a finite number of terms , one gets again a spurious dependence on the resummation parameters .
dimensional analysis is arguably the most important technique in any physicist 's bag of tricks . it is used regularly throughout physics . variations of the principle are also used in mathematics , biology and probably in other fields as well . sticking to physics , here is an example from classical mechanics : derive kepler 's third law . the independent variables for a planet are its mass $m$ , its distance $r$ from the sun , its period $t$ , and the force $f$ acting on it . all other variables , like the velocity or acceleration , are functions of those . the only dimensionless number we can construct from the variables is $$ \frac{f t^2}{m r} \ , , $$ so this number must be the same for any planet in our solar system ( assuming that there is some functional relation between the variables ) . we also have newton 's law of gravitation , $$ f = \frac{g m m_s}{r^2} \ , . $$ together , we see that $$ \frac{t^2}{r^3} = const \ , . $$ most fields outside the sort-of fluid mechanics described above are easy enough to calculate without dimensional analysis . i encourage you to pick up polchinski 's book on string theory and work through the first few chapters . then review that statement .
yes , you have taken the analogy too far : electrons do not actually move through the wire in the way that fluids flow through a pipe . hence , there is no reason why an analog of bernoulli 's principle should apply .
after a really brief cursory review of the literature , i think that a dalitz decay is a meson decay that involves two leptons in the final state , plus a photon . a double dalitz decay has four leptons in the final state : see this paper and this paper for examples of the usage . the dalitz decay is when a virtual photon from 2 photon decay of $\pi_0$ internally converts to a real lepton pair before it gets too far , and analogous thing for other meson or higgs processes ( two electrons from an internal photon conversion , plus a neutral object ) . i guess that the usage comes from the kinematic decay product phase space is described by a dalitz plot , hence the name . i do not think it is anything deep .
if the neutron decayed to a two body state ( any two body state ) the energy spectrum of the products in the neutrons rest frame would be single valued ( this is required by the conservation of energy and momentum ) . it is not . instead the electron energy spectrum is a continuum that runs from that roughly the two-body limit down to as near zero as our instruments can measure . to grab an image from the wikipedia : so , a third particle is required . that third particle is known to be uncharged ( because our detectors are sensitive to charged particles and do not see it ) . it is also known to be of very low mass because the end-point of the electron energy spectrum is almost exactly what you would expect from the two body decay . the lifetime of the neutron suggests that the interaction that is responsible for it is decay is very weak ( and going on a little further in history it obeys the principle of weak universality suggesting that it is the same interaction responsible for the decay of strange hadrons ) . the sum of these requirements constrain the properties of the third particle quite a lot , and much observation since then has shown quite conclusively that neutrinos exist .
no , because not all the fundamental particles ( in the sense of the standard model ) are stable . in particular , electrons and photons are stable ; muons and tau leptons are not , and will decay into lighter leptons , e.g. $\mu^-\to e^- \bar\nu_e \nu_\mu$ . neutrinos are kind of funny because , while they are prevented from decaying in the traditional sense by conservation of energy , momentum , and lepton family number , they do oscillate - so if you start with an electron neutrino , for example , it will turn into something that can be observed as any flavor of neutrino , and in this way you can find yourself measuring something like $\nu_e\to\nu_\tau$ . but then you can just as well measure $\nu_\tau\to\nu_e$ , and if you have two ( actually three ) particles which can all decay into each other , does it even make sense to call it a decay ? quarks try to be funny but actually wind up just being annoying , because they are never found in isolation so nobody is exactly sure how an isolated quark would behave if you could put one in a universe by itself . that being said , heavy quarks ( charm , bottom , theoretically top ) are routinely observed to decay in high-energy collisions , where asymptotic freedom presumably applies , so it is not much of a stretch to imagine that an isolated heavy quark would decay into lighter quarks plus a pion or leptons . likewise , the weak gauge bosons decay all the time in collisions , so if you managed to create a universe that contained only a $w^\pm$ or $z^0$ and nothing else , it would presumably decay quickly into a lepton and antineutrino or into some combination of hadrons . same goes for the higgs , except with different possible decay products .
the first assumption is that whatever vev the higgs picks up is constant in space , because this has less energy than one that increases the kinetic term in the lagrangian . so we can do one global transformation to make the vev be in the second component only . you can imagine doing this prior to symmetry breaking , if you know what it is going to be ahead of time , and since the other fields are invariant , bob 's yer uncle . stated differently , the pre-symmetry-breaking electrons and neutrinos are not the ones we observe , so we just label whatever remains as electrons and neutrinos . " without loss of generality " , we work in an electron-neutrino ( global ) basis in which the higgs starts out with only the second component of the vev being nonzero and real . if you buy that part , then it is just a matter of showing that you can perform a gauge transformation that gauges away all the other components of the higgs except the real part of the second component . this gauge transformation will of course mix $\nu$ and $e$ spatially , but you can say that when we perform the path integral we have a gauge redundancy , and so we only integrate along a slice that obeys some gauge fixing condition . the components of $l$ might as well be labeled $l_1$ and $l_2$ . it is only after we have chosen a gauge that we decide , hey , let 's name them $\nu$ and $e$ .
i am not sure i understand your question correctly , but let me know if the below helps . let \begin{array}$ n^2 ( \omega ) and = 1 - \frac{\omega_p^2}{\omega^2} - i\epsilon\\ and =-\omega-i\epsilon\end{array} where $\omega:=-\left ( 1 - \frac{\omega_p^2}{\omega^2}\right ) &gt ; 0$ by assumption . then , expanding $n ( \omega ) $ for small $\epsilon$ we find ( using some algebra-software : is this ok with you ? ) $$\sqrt{-\omega-i\epsilon} = -i\cdot\mathrm{csgn} ( i ( -\omega-i\epsilon ) ) \sqrt{\omega}+\frac{1}{2}\frac{\mathrm{csgn} ( i ( -\omega-i\epsilon ) ) }{\sqrt{\omega}}\epsilon+\mathcal{o} ( \epsilon^2 ) $$ where the csgn is defined on this page as : in our case we get : $$\sqrt{-\omega-i\epsilon} = -i\sqrt{\omega}+\frac{1}{2}\frac{1}{\sqrt{\omega}}\epsilon +\mathcal{o} ( \epsilon^2 ) $$ which lies in the fourth quadrant . does this answer your question ?
no . momentum is conserved . since momentum is mass times the velocity of the center of mass , if the momentum is zero , the center of mass can not move . alternately , if the center of mass is already moving , it will keep moving indefinitely in a straight line when there are no external forces . however , in curved spacetime the above may not hold . see http://dspace.mit.edu/handle/1721.1/6706
mobile phones transmit at microwave frequencies so they can induce currents in metals and other conductors . the energy dissipates as heat . the principle is the same used by microwave ovens . the phone would not itself get hot because ( presumably ) the microwave radiation would be directed away from metal components in the phone . your key must have been close to the antenna and was perhaps aligned with it in a way that maximized the effect , or perhaps its length was just right for a resonance . here is a paper that seems relevant http://www.springer.com/about+springer/media/springer+select?sgwid=0-11001-6-1296921-0 edit : let me add a small calculation . suppose a phone were outputting 1 watt and 50% was absorbed by the key for 3 minutes . this puts 1w*180s*0.5 = 90 j of heat into the key . if the key weighs 10 g and is made of iron with a heat capacity of 450 j/kg/k then the temperature goes up by 90j/0.01kg/450j/kg/k = 20k so this could raise the temperature of the key from 25c to 45c which would make it feel quite hot to the touch . actual result depends on how fast the heat is conducted away and how much of the power is absorbed but it is not beyond the bounds of possibility that enough heating is possible to account for the observation . i would not blame anyone for being skeptical though . it would perhaps be unusual for the phone to transmit on full power for that long .
you need the units because though $x$ kelvin is the same as $x$ celcius it is not the same as $x$ fahrenheit . you can treat $\delta t$ as a temperature . a temperature scale has a fixed zero point ( absolute zero for the kelvin scale and the freezing point of water for the centigrade scale ) and an interval defining 1 degree . to make $\delta t$ a temperature you are just specifying that the fixed zero point is $t_1$ , that is $\delta t = 0$ when the temperature is $t_1$ . this may seem pedantic , but you would no doubt claim your height is six feet ( or whatever it is ) . i am sure you would not respond to a query by saying instead that the difference in height between the top of your head and the bottom of your feet is six feet .
it seems to me that " symmetric fission " refers to any fission process where the end products are symmetric about some point . specifically , where the end products are symmetric in their atomic mass . this website explains it quite well , but for completeness i am quoting the relevant paragraph in the article below . it is thought to be helpful for a better understanding to take the atomic numbers into consideration . the atomic numbers of the above-mentioned elements are : ru = 44 , rh = 45 , pd = 46 , ag = 47 , cd = 48 , in = 49 and sn = 50 , respectively . by noticing that the atomic number 46 of palladium is just half that of 92 of uranium , it is supposed that one uranium atom splits into two palladium isotopes . when rhodium ( atomic number 45 ) is produced with some probability ( cross section ) , silver ( atomic number 47 ) is the counter fragment . in the same way , ruthenium ( atomic number 44 ) and cadmium ( atomic number 48 ) are the pairing fragments . thus , the nuclear fission observed by nishina and kimura is highly symmetric .
a group $g$ by itself is not a group of linear transformations , it is an abstract algebraic object . only its representations map its elements ( injectively if the representation is faithful ) to elements $\mathrm{aut} ( v ) $ of some vector space $v$ . now , physics seems to have no need of such abstract language at first . our " vector space " is pretty much our spacetime , and its pretty much $\mathbb{r}^4$ , so your symmetries are really just matrices on that spacetime . the lorentz symmetry is just $\mathrm{so} ( 1,3 ) $ in its fundamental representation on minkowski space $\mathbb{r}^{1,3}$ , right ? or non-relativistic , rotational symmetry is just $\mathrm{so} ( 3 ) $ on $\mathbb{r}^3$ , right ? . . . and then there is angular momentum and spin . if you solve the schrödinger equation for the energy levels of a hydrogen atom , you find that the energy levels are characterized by " quantum numbers " $ ( n , l , m , s ) $ . now $n$ is boring . but $l$ and $m$ are eigenvalues of the spherical laplacian , and lead to the beloved spherical harmonics $y^l_m$ as independent solutions . turns out , if you rotate the system in space , these harmonics behave differently depending on their $l$ ! formally , the space $$h_l := \{\sum_m c_m y^l_m | m \in \{-l , -l+1 , \dots , l\}\wedge c_m \in \mathbb{r}\}$$ is a vector space , and it carries a representation of the rotation group $\mathrm{so} ( 3 ) $ ! but not the fundamental one , if $l &gt ; 1$ . so there is your non-fundamental representation arising solely by solving the equations describing a physical system . it gets even weirder for these rotation groups , since it also turns out that there are objects , the fermions , which do not transform in a representation of $\mathrm{so} ( 1,3 ) $ or $\mathrm{so} ( 3 ) $ , but in a representation of their universal covers , $\mathrm{spin} ( 1,3 ) $ or $\mathrm{su} ( 2 ) $ , respectively . you have no chance to describe the kinds of phenomena you observe for fermions without accepting that they transform that way . and that is not the end of the story . if you build a gauge theory with gauge group $g$ , you will find that the associated field strength of the gauge field must transform as an element of the adjoint representation of $g$ . non-fundmental representations pervade many aspects of ( quantum ) field theory in that way .
at tree level , the conditions for the electroweak-symmetry breaking vacuum of the mssm can be found in any of the standard review articles and are : $$ \sin ( 2\beta ) = \frac{2b_\mu}{2|\mu|^2 + m_{h_u}^2 + m_{h_d}^2} $$ $$ \frac{1}{2} m_z^2 = -|\mu|^2 - \frac{m_{h_u}^2 \tan^2\beta - m_{h_d}^2}{\tan^2\beta - 1} $$ now , the right logical way to think about this is that a top-down theory determines the values of $\mu$ and the soft-breaking parameters $m_{h_u}^2$ , $m_{h_d}^2$ , and $b_\mu$ . these are really the inputs . on the other hand , we only want to consider the slice of parameter space on which ewsb is realized as in our world , with a particular value of $m_z$ . one can choose other useful coordinates on this slice , like $\tan \beta$ , as software inputs to choose only those points on this slice of parameter space , rather than specifying the high-scale input , which in general will fail to realize correct ewsb .
air has a specific heat capacity of slightly more than 1kj/kgk at room temperature so it takes a 1kw heater 1 second to heat 1kg ( roughly 1 m^3 ) of air 1 deg c fairly humid air ( say 60% rh at 20c ) will contain around 10g/m^3 of water vapour with a specific heat capacity of 1.8 kj/kgk - so it takes almost twice as much energy ( per unit mass ) to heat the water in the air than the dry air itself . but , only 10g in every 1kg of air is water vapour ( ie 1% ) so you only have to do twice as much work to that 1% . in other words - no you will not see any measurable difference .
you have the answer in the edit - the dwarf stars orbits will be unstable . gravitational systems with more than two bodies are inherently chaotic ( n-body problem ) . even the solar system is unstable over the long term . the heavier the bodies and the closer they are , the more gravitational energy can be exchanged . you will find stars being ejected all over the place . there is a " very far apart " limit : for example multiple star systems like a trinary system where two stars are very close and the third one is far out orbiting the central pair like a planet , or two pairs widely separated so that the other pair is approximately a point source . the record so far is 5 stars . to do that with a dozen stars would take a lot more space than the orbit of mercury . if you tried , you would find singles and pairs being ejected at random and maybe even get some spectacular dwarf collisions .
$k=\frac1{4\pi\epsilon_0}=98.9\times 10^9 \:\mathrm{n m^2 c^{-2}}$ this is coulomb 's constant , part of coulomb 's law : $$\vec f=\frac{kq_1q_2\hat{r}}{r^2}=\frac{q_1q_2\hat{r}}{4\pi\epsilon_0r^2}$$ ( $\epsilon_0$ is the permittivity of free space , and is another constant ) in a dielectric medium , you replace $\epsilon_0$ with $\epsilon_r\epsilon_0$ ( the permittivity in that medium ) in this equation , since the induced charges decrease the force . in case of a vacuum ( and to an approximation , air ) , $\epsilon_r=1$
the square modulus of the transmission coefficient ( $|t|^2$ ) is the transmission probability , and you have the data to calculate that for your wave-packets ( i.e. . it is just $|\psi|^2$ as you surmised , if you normalized your wavefunction ) . you can compare that to the expected transmission coefficient for plane waves with the same average energy as your wave-packet . i do not know how well the comparison will be since you have uncertain energy , but you can use wider wave-packets and at least see if it seems to be converging to the plane wave solution . incidentally it is a good idea to check the norm of your wave function , it can help determine if anything is going wrong with your numerical simulation . ps : this http://arxiv.org/abs/quant-ph/0301114 might be of some value . it is an explicit solution for a wave-packet on a square barrier ( i.e. . like your problem , but only one ' edge ' . )
no , einstein 's relativity has not been proved wrong by anyone up to now . anyone who did would get a nobel prize .
ok , eventually i figured out the answer . $$\mathcal{u}=e^{-i\phi a^{\dagger}a}$$
as you said , the equation you give works only in " wavefunctionspace " . as long as you deal with wave functions , you can always decompose the object your derivative acts on as $$\int \frac{\mathrm d\omega }{\sqrt{2\pi}} \tilde f ( \omega ) e^{i \omega t}$$ so your derivative will draw a factor of $i \omega$ from the exponential . the general shape still is a superposition of plane waves and therefore still in $\mathbf f$ , and the operator $i\hbar\frac{\mathrm d}{\mathrm dt}$ is self-adjoint .
there are different eigenvalues $h_n$ with different probabilities ( lets call them $\phi_n$ ) . your expectation value is then $\sum_n h_n\cdot \phi_n$ if you have only one eigenvalue it is obviously identical to the expectation value
can it sustain a reaction for more than a few seconds ? yes , at jet . lifetime of the plasma : 20–60 s at iter : it will operate over a wide range of iter plasma scenarios , from short plasma pulses ( a few hundred seconds ) with enlarged fusion power ( 700 mw ) to long plasma pulses of 3,000 s are these devices still huge or have they been made on a smaller scale ? they have to be large in order to give out more energy than they consume . this is the iter link , the one planned and already being built , to get the size idea .
congratulations , you made me look into this for the last hour ! and , unfortunately , i believe the answer is : nope we are looking for a ricci-flat riemannian symmetric space , since your isometry group is a lie group . i spent some time trying to construct the ricci-flat manifold from the irreducible symmetric spaces given there , but could not figure out a good way to actually calculate the ricci tensor with the data given . then i found this paper , and proposition 3.3 says [ 1 ] $$\text{any locally riemannian symmetric space that is ricci-flat is flat}$$ and that is the death of our endeavour , since our riemannian symmetric space is so certainly locally , but flat symmetric spaces are just the euclidean ones ( see classification again ) , which do not have the desired isometry group . thus , ricci-flatness is too harsh a requirement to get interesting isometry groups . [ 1 ] unfortunately , they give no proof , but refer to some russian ( what else ! ) result from " alekseevskii dv and kimelfeld bn 1975 structure of homogeneous riemannian spaces with zero ricci curvature ( russian ) funkcional . anal . i prilozen . 9 2 5-11"
quantum mechanics and quantum mechanical theories are totally independent of the classical ones . the classical theories may appear and often do appear as limits of the quantum theories . this is the case of all " textbook " theories - because the classical limit was known before the full quantum theory , and the quantum theory was actually " guessed " by adding the hats to the classical one . in a category of cases , the full quantum theory may be " reverse engineered " from the classical limit . however , one must realize that this situation is just an artifact of the history of physics on earth and it is not generally true . there are classical theories that can not be quantized - e.g. field theories with gauge anomalies - and there are quantum theories that have no classical limits - e.g. the six-dimensional $ ( 2,0 ) $ superconformal field theory in the unbroken phase . moreover , it is typical that the quantum versions of classical theories lead to new ordering ambiguities ( the identity of all $o ( \hbar^k ) $ terms in the hamiltonian is undetermined by the classical limit in which all choices of this form vanish , anyway ) , divergences , and new parameters and renormalization of them that have to be applied . also , the predictions of quantum mechanics do not need any classical crutches . quantum mechanics works independently of its classical limits , and the classical behavior may be deduced from quantum mechanics and nothing else in the required limit . historically , people discussed quantum mechanics as a tool to describe the microscopic world only , assuming that the large objects followed the classical logic . the copenhagen folks divided the world in these two subworlds , in an ad hoc way , and that simplified their reasoning because they did not need to study quantum physics of the macroscopic measurement devices etc . but these days , we fully understand the actual physical mechanism - decoherence - that is responsible for the emergence of the classical logic in the right limits . because of decoherence , which is a mechanism that only depends on the rules of quantum mechqnics , we know that quantum mechanics applies to small as well as large objects , to all objects in the world , and the classical behavior is an approximate consequence , an emergent law . to know the evolution in time , one needs to know the hamiltonian - or something equivalent that determines the dynamics . the previous sentence is true both in classical physics and quantum mechanics , for similar reasons , but independently . if a classical theory is a limit of a quantum theory , it of course also means that its classical hamiltonian may be derived as a limit of the quantum hamiltonian . of course , if you do not know the hamiltonian operator , you will not be able to determine the dynamics and evolution with time . guessing the quantum hamiltonian from its classical limit is one frequent , but in no way " universally inevitable " , way to find a quantum hamiltonian of a quantum theory .
david , as you can see from lubos 's comment there is a serious flaw in the formulation of this question , so i will use this answer simply to explain some aspects of the topic . first the flaw : the energy density of a schwarzchild black hole the schwarzchild solution is a solution of the vacuum einstein equations ie $r^{u , v}=0$ and so $t^{u , v}=0$ ie the stress energy tensor is zero throughout the schwarzchild solution ( removing the origin from this manifold avoids the undefinedness there ) . with zero stress-energy there are no fluids or local energy densities to measure or examine . what is meant in the question ( as it arises from the earlier stack questions ) has some relationships with the hoop conjecture as edward points out , but just for clarification i shall add more . let us not consider any more an einstein vacuum and assume that some form of matter ( or radiation - i will just say matter below ) must have been present " originally " . this matter is part of a non-vacuum solution of the einstein equations , and so there will be a corresponding non-zero stress-energy tensor whose matter is destined to become the black hole . so now the question begins to make sense . so the question is really about what determines whether a given non-vacuum solution of einstein 's equations forms a black hole and whether this fact is measurable locally . the sentence that asks this is : what would be the " signature " of the black hole in the components of $t^{u , v}$ ? $ i dont believe that this answer is known , partly because the space of all solutions of einstein 's equations is not yet known . if one considers the points made below , one might also conclude that gr alone was inadequate to predict the bh formation - matter properties are central too . there are the classical hawking-penrose theorems which give a topological-geometric answer to this question by positing the existence of " closed trapped surfaces " , along with certain properties of $t^{u , v}$ . in that sense there is an answer , but it doesnt tell us when the closed trapped surfaces will form ( generically ) . black holes arose as physically plausible solutions to einstein 's equations because of the early work of oppenheimer et al . here there are two metrics combined to form the matter leading to a black hole : friedmann dust ( interior ) + schwarzchild ( exterior ) ( a clever trick to consider " massless dust " allows one to use only the friedmann dust solution ) . these two solutions need to be " glued together " to form the surface of the star . the $t^{u , v}$ ( in the comoving frame and its translation into other frames ) for this was given in edward 's earlier answer , and is non-zero in the star interior . what causes another layer of complication in discussing the formation of black holes and event horizons is the teleological nature of their formation in general relativity . this arises because " time " is just a parameter in the theory and the formation of the black hole is determined by the overall solution ( thus in a time independent way ) . now it has been concluded that for stellar objects of mass > tov limit a black hole will form . but as remarked above translating this condition into a condition on the stress-energy tensor alone may not be possible . physically one might expect that there is some local condition despite all these issues , such as the hoop conjecture which includes the object 's mass in its formulation . there are several subtleties connected with this unproven conjecture and one problem here is that " mass " is not a local property in gr ( because mass = energy and the gravitational field contributes energy too , not just the stress-energy tensor - hence we again may need the full solution of $g^{u , v}=t^{u , v}$ . ) i shall add this link from willie wong for anyone interested in the latest on the hoop conjecture . finally my answer to the linked question might be of interest .
the correct option is really option 3 . most of the time when a physicist says a theory is renormalizable , they mean that the theory is a relevant deformation of some conformal field theory . this is a non-perturbative definition . it contains the physically meaningful content that the other more technical definitions about counterterms in perturbation theory are attempting to capture . indeed , it implies them . ( however , the reverse implication is not always true . for example , perturbative qed is renormalizable in the sense of option 2 , but there is no underlying non-perturbative qed , so one can not even ask about option 3 . ) it is good practice to always try to think about the non-perturbative meaning of the physical formalism you are studying . perturbation theory is a sometimes useful tool for computations , but it can obscure the physics in a cloud of virtual technicalities . so what does it mean for a theory to be a relevant deformation of a cft ? it means that there is a cft whose observables are essentially the same as the observables in your qft , and that you can compute any correlation function in the qft as $\langle \mathcal{o} \mathcal{o}' . . . \rangle_{qft} = \langle \mathcal{o} \mathcal{o}' . . . e^{\sum_i g_i \int\mathcal{o}_i} \rangle_{cft}$ where the $\mathcal{o}_i$ are relevant operators in the cft and $g_i$ are ( dimensionful ) coupling constants . knowing that your qft is near a cft in this sense is what allows you to study the behavior of the qft 's expectation values under changes of scale , which is the heart of the renormalization group analysis . edit : first , an easy example : the free scalar field theory is a conformal field theory . this theory is basically described by $\langle \mathcal{o} \rangle_{cft} = \int \mathcal{o} ( \phi ) e^{i\int |d\phi|^2} \mathcal{d}\phi$ . in this theory , in dim > = 3 , the operator $\phi^2 ( x ) $ is relevant , so we can deform with this term and get a non-conformal field theory . the expectation value in this qft is then described by $ \langle \mathcal{o} \rangle_{qft} = \langle \mathcal{o} e^{i m^2 \int \phi^2 ( x ) dx}\rangle_{cft} = \int \mathcal{o} ( \phi ) e^{i\int [ |d\phi ( x ) |^2 + m^2\phi^2 ( x ) ] dx} \mathcal{d}\phi . $ so , not surprisingly , the theory we get by deforming the free scalar cft with a mass term is the massive free scalar . second , a subtlety that i should point out . the first equality above is not exact in most situations . the problem is the deformations we want may not be integrable with respect to the cft 's path integral measure , thanks to uv singularities . this is dealt with by regularizing . so , in most qfts , what we get is a family of approximations $\langle \mathcal{o} \mathcal{o}' . . . \rangle_{qft} \simeq \langle \mathcal{o} \mathcal{o}' . . . e^{\sum_i g_i ( \lambda ) \int\mathcal{o}_i ( \lambda ) } \rangle_{cft}$ where the relevant operators and the coupling constants depend on a cutoff scale $\lambda$ and the errors vanish as $\lambda \to \infty$ .
i do not know why they should violate thermodynamics either , but they do not exist because they are static . they cannot be created at any finite time - they must have existed since the beginning of time and will exist forever . the physically realistic schwarzschild solution is created from collaps and does not have the second asymptotic region .
i do not understand question 1: where does he equate a speed to a position ? as far as question 2 is concerned , it is basically what davephd said , but maybe i can extend it a bit more saying something about the conservation of linear momentum : along the x-direction , there is no external force ( because gravity points downwards only , assuming a flat surface ) so the linear momentum of the projectile is conserved . since $p_x = mv_x$ , $v_x$ is constant .
verifying this in its entirety is tedious but good practice , so here 's the skeleton of what you need to do without giving it all away : recall the fundamental structure relation \begin{align} \{\gamma^\mu , \gamma^\nu\} = 2g^{\mu\nu} \end{align} where , as usual , there is an identity matrix implicit on the right hand side . the expressions you really want to compare are the expression on the first line which reads \begin{align} -ig_{\nu\rho} ( -ie\gamma^\nu ) i ( k_\alpha\gamma^\alpha + m ) \gamma^u i ( k_\beta \gamma^\beta+m ) ( -ie\gamma^\rho ) \end{align} and the expression on the second line which reads \begin{align} 2ie^2 ( k_\alpha\gamma^\alpha\gamma^\mu\gamma^\beta k'_\beta -2m ( k+k' ) ^\mu+ m^2\gamma^\mu ) \end{align} it is useful to match the stuff in each line that does not depend on $k$ and $k'$ first , and then match the stuff that does depend on $k$ and $k'$ . for example , the term on the first line that does not have $k$ and $k'$ in it is \begin{align} -ie^2g_{\nu\rho}\gamma^\nu\gamma^\mu\gamma^\rho m^2 \end{align} while the stuff on the second line that does not have $k$ and $k'$ in it is \begin{align} 2ie^2m^2\gamma^\mu \end{align} these things are the same since \begin{align} g_{\nu\rho}\gamma^\nu\gamma^\mu\gamma^\rho and = g_{\nu\rho}\gamma^\nu ( \{\gamma^\mu , \gamma^\rho\} - \gamma^\rho\gamma^\mu ) \\ and = g_{\nu\rho}\gamma^\nu ( 2g^{\mu\rho}-\gamma^\rho\gamma^\mu ) \\ and = 2\gamma^\mu-g_{\nu\rho}\gamma^\nu\gamma^\rho\gamma^\mu \\ and = 2\gamma^\mu-\frac{1}{2} ( g_{\nu\rho}\gamma^\nu\gamma^\rho + g_{\rho\nu}\gamma^\rho\gamma^\nu ) \gamma^\mu \\ and = 2\gamma^\mu - \frac{1}{2}g_{\nu\rho}\{\gamma^\nu , \gamma^\rho\}\gamma^\mu \\ and = 2\gamma^\mu - \frac{1}{2}g_{\nu\rho} ( 2g^{\nu\rho} ) \gamma^\mu \\ and = 2\gamma^\mu-4\gamma^\mu \\ and = -2\gamma^\mu \end{align} do a similar ( but more tedious ) thing for the stuff that depends on $k$ and $k'$ .
hawking radiation is a very slow process of the black hole losing energy and shrinking . if you counter this by supplying a little bit of matter or energy falling into the black hole you can easily overcome it and sustain the black hole . other than hawking radiation i do not think there is any known process for black holes to shrink . the area theorem in classical general relativity states that the area of black hole horizon always increases in any physical process . so at least classically there is no way for black holes to die , or even shrink a little . hawking radiation evades that because it is a quantum process ( which is also why it is a slow process ) . as for the final stage of the evaporation , i think the honest answer is nobody knows . the logical possibilities are either the black hole shrinks to nothing and disappears , or it leaves behind some long-lived " remnant " . either one of this possibilities has its strong and weak points , but ultimately you had need to know more about a quantum theory of gravity to know for sure .
suppose that two molecules are at distance $b$ and have zero kinetic energy . there is a lower potential energy position in $c$ and therefore the molecules will attract . they will convert $\epsilon$ potential energy into kinetic energy and reach $c$ . now , the law of inertia states , and the fact that they have positive kinetic energy indicates , that they will maintain their state of motion at $c$ towards $a$ . going towards $a$ they will gain potential energy by converting kinetic energy into it ( in other words , slowing down ) . once at the distance $a$ , they will have gained exactly $\epsilon$ potential energy and will have therefore zero kinetic energy . at this point the cycle will repeat inverted , they will move towards a distance of $c$ because it is lower energy , surpass it and reach $b$ with zero kinetic energy , which will make the cycle repeat from the start .
when first coming into contact with the water , it is conduction . the skin feels the water colder than air because water is a better conductor of heat than air . so the skin cools faster in water than in air . for longer intervals convection will enhance the effect bringing cooler water next to the skin and removing the water already heated by the skin . the difference will persist plotting water temperatures ( equal with air temperature ) up to the temperature the skin raises the water when in contact with it . after that , the water is felt as warm .
it depends on what you mean by " propagation of energy . " usually when we think of energy propagating , there is an non-constant distribution of energy in space , and so you can follow features of that distribution to see which way the energy is moving . for example , in an electromagnetic wave , the energy density as a function of position takes a sinusoidal form , $u\sim\sin^2 kx$ , and you can follow the peaks and troughs in this distribution over time to see the movement of the wave . but if you really think about it , energy is not a vector , so it does not have an inherent direction , and so in general there may not be any propagation to speak of . without some feature in the energy distribution to follow , the whole idea of propagation becomes essentially inapplicable . in that case , all you can say about energy is how the amount of it in some particular region changes with time . that quantity is related to the divergence of the poynting vector , $$\frac{\partial u}{\partial t} = -\nabla\cdot\vec{s}$$ ( in empty space ) , so as far as the energy distribution is concerned , it is only the divergence of $\vec{s}$ that matters , not its value . from wikipedia : the poynting vector is usually interpreted as an energy flux , but this is only strictly correct for electromagnetic radiation . the more general case is described by poynting 's theorem above , where it occurs as a divergence , which means that it can only describe the change of energy density in space , rather than the flow . however , the poynting vector does have a different interpretation as the momentum density of the em field . momentum is a vector , so it does have a direction , and you can meaningfully talk about a " momentum propagation " ( loosely speaking ) . when you calculate that $\vec{s} \neq 0$ , what you are finding is that the em field of this configuration does have a nonzero momentum density . however , the momentum normally has no effect because it never gets transferred to anything . if you put something that interacts with the em field in its way , though , the object will experience radiation pressure .
nacl melts at around 800°c . molten nacl has a density of about $1.556 \frac{g}{cm^3}$ [ 1 ] , at room temperature ( solid ) it has one of $2.71\frac{g}{cm^3}$ [ 2 ] . sadly i could not find a value for the density at barely underneath melting point but i strongly assume that the density is a strictly monotonously falling function of temperature . therefore solid nacl will probably sink in liquid salt . the case that ice floats on liquid water is special and known as the anomaly of water .
patents are for inventions , which are feats of engineering , not advances in physics . edison was a great businessman as well as a great inventor : he took understood principles of physics and turned them into useful machines . these machines are codified in the patents . he did not , however , contribute to the understanding of the laws of physics . now , admittedly , tesla did not advance our understanding much either . he , too , was predominantly an engineer/inventor . however , by experimenting with high-voltage current , x-rays and radio waves , he indirectly helped our understanding of electromagnetic radiation and the electromagnetic force .
the reason you encountered higher and higher pressure at the center of the rod as you cut it into more pieces is that you were essentially approximating an integral , but the integral diverges ( "is infinity " colloquially ) . when the rod has zero thickness , but still has mass , the density of the matter is infinite , and this leads to infinitely strong gravitational forces . to answer this question , we will imagine the cylinder has some small , finite radius $r$ . we want to find the force between the two halves of the cylinder . we will let one half just sit stationary in space . it will create a gravitational potential . then we will grab the other half and pull it away to some distance $d$ . the gravitational potential energy is a function of $d$ . the force between the two halves of the cylinder is the derivative of the gravitational potential energy with respect to $d$ when $d=0$ . the problem described above is too hard . it is quite difficult to calculate the gravitational potential of a cylinder at an arbitrary point . the gravitational potential of a point mass is just $-gm/r$ , but for a cylinder that extends out in three dimensions , we need to replace $m$ with the density $\rho$ and then integrate over the mass of the entire cylinder . the expression for $r$ , the distance from an arbitrary point outside the cylinder to a point inside it , is not very tractable . however , at a point on the axis of the cylinder , the gravitational potential is more accessible due to the extra symmetry . if we set up cylindrical coordinates with the axis of the cylinder along the z-axis , and then integrate over the bottom half of the cylinder , we get $v ( z ) = \int_{z&#39 ; =0}^{-l/2}\int_{r=0}^{r}\int_{\theta=0}^{2\pi} \frac{g\rho}{\sqrt{ ( z-z&#39 ; ) ^2+r^2}} r\textrm{d}\theta\textrm{d}r\textrm{d}z&#39 ; $ and doing the integral of $\theta$ it is $v ( z ) = 2\pi g\rho\int_{z&#39 ; =0}^{-l/2}\int_{r=0}^{r} \frac{1}{\sqrt{ ( z-z&#39 ; ) ^2+r^2}} r\textrm{d}r\textrm{d}z&#39 ; $ . this allows us to make an approximation . although the half of the cylinder we use to calculate the potential must have finite width , we can calculate the potential energy by assuming that the other half of the cylinder is located perfectly along the axis . as long as the radius of the cylinder is very small compared to the length , this is a valid approximation . so the potential energy comes from integrating the previous expression for $v$ along the $z$-axis for the length of the cylinder . we do not actually want the potential energy , but the derivative of the potential energy . so we imagine moving the top half up the cylinder up a little bit $dz$ , and ask how the potential energy changes . moving the entire top half of the cylinder up by $dz$ is equivalent to taking a piece of thickness $dz$ and slicing it off the bottom and moving it to the top . so we really just need to find the difference in the potential between the top and bottom of the top half of the cylinder and multiply by the mass-per-unit-length of the cylinder . the force between the two halves of the cylinder is $\frac{m}{l} [ v ( l/2 ) - v ( 0 ) ] $ that still leaves two integrals to evaluate . $v ( l/2 ) $ is easy , because it is far away from the half of the cylinder providing the gravitational potential ( compared to $r$ ) . that lets us approximate $v ( l/2 ) = \frac{-gm}{l} \int_{-l/2}^0 \frac{1}{l/2-x}dx = -\frac{gm}{l}\ln 2$ . the integral for $v ( 0 ) $ is trickier , so i put it in mathematica and got $v ( 0 ) = -\frac{gm}{l}\textrm{arcsinh}\left ( \frac{l}{2r}\right ) $ . in the regime we are interested in ( $r$ small compared to $l$ ) the $\sinh ( x ) $ is just $e^x/2$ , so this simplifies to $v ( 0 ) = -\frac{gm}{l} \ln\left ( \frac{l}{r}\right ) $ this gives a final answer for the force $f = \frac{m^2g}{l^2}\ln\left ( \frac{l}{2r}\right ) $
the einstein field equations , describing the gravitational field are given by $$r_{\mu\nu}-\frac12 g_{\mu\nu}r = -\frac{8\pi g}{c^4}t_{\mu\nu}\ . $$ they relate in a complicated manner the gravitational field that can be seen as the metric $g_{\mu\nu}$ itself to the stress-energy tensor $t_{\mu\nu}$ that might also depend on $g$ . newtonian gravitation if now velocities are small compared to the speed of light $c$ , the stress-energy tensor approximately only consists of its time-component , $$t_{tt} \approx \rho c^2$$ and the metric is flat with the exception of $$g_{tt} \approx 1 + \frac{2 u}{c^2}$$ where we can find , directly from the einstein equations , that the newtonian $$\delta u = 4\pi g \rho$$ holds . this is a static approach and we see that there is no dependence on any flux term $j_i \propto t_{ti}$ . velocity matters : rotating disc of dust considering the whole theory , we find that of course the metric will depend on contributions of all components of $t$ but only have a meaningful effect if associated characteristic velocities are approaching the speed of light . a prominent analytic solution is that of a rigidly rotating disc of dust . taking this solution , you can get an idea of how relativistic effects are important for the theory calculating the multipole moments $q_n$ with respect to some relativistic parameter $\mu$ ( corresponding also to the angular frequency $\omega$ of the disc ) . in the following picture you can see that the kerr-spacetime is approached ( from above ! ) for all moments $q_n ( \mu ) $ for $\mu \rightarrow \infty$ . this means that there is some $\mu$ where the effects of rotation dominate those of the mass itself . ( picture taken from here . ) so , to conclude , there will only be some measurable time change for a person living on a rotating planet if it is extremely fastly rotating . it is hence needless to say that this person would have some other difficulties than to measure this deviation from an almost flat metric . sincerely
if two capacitors are in series the charge on them must be the same . this is because there is no source or sink for charge in between the two capacitors : that means $q_1 = q_2$ . you know $q_2$ so you now know $q_1$ and you can calculate the voltages $v_1$ and $v_2$ and the total voltage across both , $v_{12}$ . because $c_3$ is parallel with $c_1 + c_2$ you know $v_3 = v_1 + v_2$ and from this you can calculate $q_3$ . finally , if you calculate the combined capacitance of $c_1$ , $c_2$ and $c_3$ and you know the voltage across this combined capacitor $v_{123}$ you can calculate a combined charge $q_{123}$ , and because $c_4$ in series $q_4 = q_{123}$ .
when it comes to fundamental charges , the ( left-handed ) up-type quarks actually have either the same values of the charge as the down-type quarks , or exactly the opposite ones . it just happens that the electric charge is not a fundamental charge in this sense . let me be more specific . all the quarks carry a color – red , green , or blue – the charge of the strong nuclear force associated with the $su ( 3 ) $ gauge group . there is a perfect uniformity among all quarks of all types . the quarks also carry the hypercharge , the charge under the $u ( 1 ) $ gauge group of the electroweak force . both the up-quarks and the down-quarks ( well , their left-handed components ) carry $y=+1/6$ charge under this group . the right-handed components carry different values of $y$ but i will not discuss those because that would make the story less pretty . ; - ) finally , there is the $su ( 2 ) $ group of the electroweak force . the left-handed parts of the up-quarks and down-quarks carry $t_3=+1/2$ and $t_3=-1/2$ , exactly opposite values , and there is a perfect symmetry between them . ( the right-handed components carry $t_3=0$ . ) it just happens that neither $y$ nor $t_3$ but only their sum , $$ q = y+ t_3$$ known as the electric charge , is conserved . the individual symmetries generated by $y$ and $t_3$ are " spontaneously broken " due to the higgs mechanism for which the latest physics nobel prize was given . the symmetry is broken because a field , the higgs field $h$ , prefers – in order to lower its energy – a nonzero value of the field . more precisely , one component is nonzero , and this component has $y\neq 0$ , $t_3\neq 0$ but $q=0$ . so the first two symmetries are broken but the last one , the $u ( 1 ) $ of electromagnetism generated by the electric charge , is preserved . it is often the case that every symmetry that is " imaginable " and " pretty " is indeed a symmetry of the fundamental laws but at low energies , due to various dynamical mechanisms , some of these symmetries are broken . the laws of physics may still be seen to respect the symmetry at some level but the vacuum state is not invariant under it , and the effective low-energy laws therefore break the symmetry , too .
i think the last line does not follow from the previous steps . it is used to show how $\gamma$ comes in place , so i extrapolated a bit and show the next few steps : since $$ \frac{c_v}{nk_b} = \frac{c_v}{c_p-c_v} = \frac{\frac{c_v}{c_v}}{\frac{c_p}{c_v}-\frac{c_v}{c_v}}=\frac{1}{\gamma-1} $$ therefore , $$ \frac{c_v}{nk_b} ( pdv+vdp ) = \frac{1}{\gamma-1} ( pdv+vdp ) = -pdv $$ dividing both sides with $pdv$: $$ \frac{1}{\gamma-1} ( 1+\frac{v}{p}\frac{dp}{dv} ) =-1 $$ continue to simplify the expressions and you will reach your result of $pv^\gamma$is constant .
first , the critical dimension . there are many ways ( seemingly inequivalent ways but ultimately bound to give the same result ) to calculate $d=10$ for the superstring that mirror the methods to calculate $d=26$ for the bosonic string . for the bosonic string , one may use a conformally invariant world sheet theory . because of the residual conformal symmetry , it has to have $bc$ ghosts . the central charge of the $bc$ system is $c=1-3k^2$ where $k=2j-1$ where $j$ is the dimension of the $b$ antighost , in this case $j=2$ . you see that my formulae imply $k=3$ and $c=1-27=-26$ so one has to add 26 bosons , i.e. 26 dimensions of spacetime , to get $c=0$ in total . now , for the superstring , the local symmetries on the world sheet are enhanced from the ordinary conformal group to the $n=1$ superconformal group . one needs to add the $\beta\gamma$ ( bosonic ) ghosts for the new ( fermionic ) generators . their dimension is $j=3/2$ , different from $j=2$ of $bc$ by $1/2$ , as usual for the spin difference of things related by supersymmetry . you see that $k=2j-1=2$ and $3k^2-1=12-1=11$ . now , the central charge of $\beta\gamma$ is $3k^2-1$ and not $1-3k^2$ , the sign is the opposite one , because they are bosons . so the $bc$ and $\beta\gamma$ have $c=-26+11=-15$ . this minus fifteen must be compensated by 10 bosonic fields and 10 fermionic fields ( whose $c=1/2$ per dimension : note that a fermion is half a boson ) and $10+10/2=15$ so that the total $c=0$ . if some of the steps are not understandable above , it is almost certainly because the reader is not familiar with basics of conformal field theory and it is not possible to explain conformal field theory without conformal field theory . it is a whole subject , not something that should be written as one answer on this server . in this formalism with the new world sheet fermions $\psi^\mu$ transforming as spacetime vectors , one has to protect the spin-statistics relationship . vector-like fermions violate it so they are only allowed in pairs . this is achieved by the gso projection – well , there are actually two gso projections , one separate for left-movers and one for right-movers . only 1/4 of the states are kept in the spectrum . the projection is a flip side of having four sectors – the left-moving and right-moving fermions may independently be periodic or antiperiodic . i wrote about the gso projection a month ago : http://motls.blogspot.com/2012/11/david-ian-olive-1937-2012.html?m=1 again , if anything is incomprehensible and incomplete , it is because it is not really one isolated insight that a layman may understand from one sentence . it is one of many technical results that follows from a large subject – string theory – that has to be systematically studied if one wants to understand it .
the duel gets tenser and tenser . $\epsilon^{xyab}\partial_a\partial_b= ( -\epsilon^{xyba} ) \partial_a\partial_b= ( -\epsilon^{xyba} ) \partial_b\partial_a= ( -\epsilon^{xycd} ) \partial_c\partial_d=-\epsilon^{xyab}\partial_a\partial_b$ $\longrightarrow\ \ \epsilon^{xyab}\partial_a\partial_b=0$ more abstractly , if $a^{ab}=-a^{ba}$ and $s_{ab}=s_{ba}$ , then $a^{ab}s_{ab}=0$ .
from your comment , it seems you want to know how to determine the direction of the electric field of a continuous charge distribution at points not in the charge distribution when you are applying the integral form of gauss 's law to obtain the electric field . the procedure commonly used to obtain the electric field using gauss 's law for continuous charge distributions relies on the fact that the distribution possesses some symmetry . in particular , the procedure is commonly applied to distributions that are either spherically symmetric , cylindrically symmetric , or possess 2-dimensional planar symmetry . in each of these cases , it is possible to determine the direction of the electric field before attempting to apply the integral form of gauss 's law to determine the magnitude of the field . in fact , there is a theorem on can prove which shows that if the charge density possess a certain symmetry , then the electric field must also respect that symmetry . it follows , for example , that in the case of spherically symmetry , the electric field must point radially because only radially pointing vector fields are invariant under all rotations just as the charge density is . similar statements hold for charge distributions enjoying other symmetries .
good question , actually . the way you thought to do the problem at first is fine conceptually , but you used the wrong equation : $e=\frac{hc}{\lambda}$ applies only to photons or other massless particles . the actual equation for computing de broglie wavelength is $$\lambda = \frac{h}{p}$$ where $p$ is momentum . you can then put that in terms of energy using $e = pc$ for a photon ( which will give you $\lambda = \frac{hc}{e}$ ) , or $e = mc^2 + \frac{p^2}{2m}$ for a nonrelativistic particle ( like a slow-moving electron ) , or $e = \sqrt{m^2c^4 + p^2c^2}$ for anything in general . if you are curious , you can derive the equation the book uses from $\lambda = \frac{h}{p}$ by using the expression for a nonrelativistic particle . just remember that $k$ is kinetic energy , so $e = mc^2 + k$ . also , you will have to drop a relatively small term .
the aharonov-bohm effect arrises in a hypothetical situation when the magnetic field $b=\nabla\times a=0$ whereas the vector potential $a$ may not be null . this situation in particular arrise when we choose the gauge in such a way that $a'=0\leftrightarrow a=-\nabla\chi$ , which indeed verifies $b=-\nabla\times\nabla\chi=0$ . aharonov and bohm gave the particular example of a two-slit interference setup , when a magnetic flux is enclosed inside the interfering paths . this can be done using a infinitely long solenoid , which do produce a magnetic flux , but without magnetic field outside the solenoid . now , one can integrate the gauge choice along a given path such that $$ a=-\nabla\chi\rightarrow\int_{a}^{b}a\cdot dl=-\int_{a}^{b}\nabla\chi\cdot dl=\chi\left ( a\right ) -\chi\left ( b\right ) $$ for a path from $a$ to $b$ . then , the phase shift along a path is given by the circulation of the vector potential along the same path in the chosen gauge . now , as conventional for a two-slit experiment , one can separate the total wave function $\psi$ as a superposition of the wave function $\psi_{\uparrow}$ passing on the upper slit and the wave function $\psi_{\downarrow}$ passing throw the lower slit . suppose we choose to separate the $\psi$ wave function at the point $a$ and recollect it at the point $b$ . then , one has reads $$ \left\vert \psi\left ( b\right ) \right\vert ^{2}=\left\vert \psi_{\uparrow}\left ( b\right ) \right\vert ^{2}+\left\vert \psi_{\downarrow}\left ( b\right ) \right\vert ^{2}+2\text{re}\left [ \psi_{\uparrow}\left ( b\right ) \psi_{\downarrow}\left ( b\right ) ^{*}e^{\mathbf{i}q\left ( \chi_{\uparrow}\left ( b\right ) -\chi_{\downarrow}\left ( b\right ) \right ) /\hslash}\right ] $$ and , using the above definition for the phase drop along a path $$ \chi_{\uparrow}\left ( b\right ) -\chi_{\downarrow}\left ( b\right ) =\int_{a\rightarrow b ; \downarrow}a\cdot dl-\int_{a\rightarrow b ; \uparrow}a\cdot dl $$ where the two integral paths are from $a$ to $b$ . the first integral follows this path using the bottom slit , whereas the second integral follows the upper slit . thus , one has , for the probability to find the particle after its passage though the system $$ \int_{a\rightarrow b ; \downarrow}a\cdot dl-\int_{a\rightarrow b ; \uparrow}a\cdot dl=\left ( \int_{a\rightarrow b ; \downarrow}+\int_{b\rightarrow a ; \uparrow}\right ) a\cdot dl=\int_{a\circlearrowleft a}a\cdot dl $$ and thus corresponds to the integral along the closed contour made by the two interfering paths . using that $$ \oint a\cdot dl=\iint b\cdot ds=\phi $$ with $\phi$ the flux enclosed in between the two interfering paths , one finally obtain that the total probability amplitude $\left\vert \psi\left ( b\right ) \right\vert ^{2}$ is $$ \left\vert \psi\left ( b\right ) \right\vert ^{2}=\left\vert \psi_{\uparrow}\left ( b\right ) \right\vert ^{2}+\left\vert \psi_{\downarrow}\left ( b\right ) \right\vert ^{2}+2 {\re}\left [ \psi_{\uparrow}\psi_{\downarrow}^{*}\right ] \cos\frac{2\pi\phi}{\phi_{0}}+2 {\im} \left [ \psi_{\uparrow}\psi_{\downarrow}^{*}\right ] \sin\frac{2\pi\phi}{\phi_{0}} $$ with $\phi_{0}=2\pi\hslash/q$ is called the flux quantum . in a superconductor , the basic charge is $2e$ , thus the flux quantum becomes $\phi_{0}=\pi\hslash/e$ which is a fundamental constant of quantum circuitry .
free energy perturbation is a free energy method , i.e. it allows one to calculate the difference in free energy between two states a and b , during a molecular simulation . other , and possibly better-known free energy methods , include thermodynamic integration and umbrella sampling . the idea behind the free energy perturbation method is that , if your states a and b are close enough , i.e. if they represent well-overlapping regions of phase space , you can get information about their free energy difference from one single molecular simulation in one of the states . the big gain of using a small perturbation is that you do not need to actually sample both a and b , because they are close enough .
the primary reason is that surface tension arises from attractive interactions between the molecules of a liquid a . in the bulk there are other molecules of liquid a all around so the interactions are balanced . at the interface however , there is a lack of molecules a on one side which results in a net force that pulls the surface molecules in the direction of the bulk . since this is a pulling action of the liquid itself arising from the attractive forces ( not a pushing action of the other liquid ) it is termed a tension .
yes they do , and this is a great way to introduce perturbation theory . exact solutions to n-body problems where n is greater than 2 are hard to find . it has been accomplished in the case of 3 and 4 bodies , and a general solution where bodies do not collide has also been found , but these problems are extremely complex . one way to get around this complexity is to start with a two body problem ( like mercury orbiting the sun ) and then perturb the solution to reflect the effects of other celestial bodies . this is possible especially since the mass of the sun is so much greater than the mass of all the other planets in orbit . update : to modify to reflect ron 's comments , a good way to understand how other planets effect each others orbits is to look at the anomalous precession of mercury which can be visualized here . each planet causes a perturbation on the orbit of the planet , which can be understood as a slight lateral acceleration from the intended path if there were only two bodies . the anomalous precession of mercury was found after the effects of all the other planets were subtracted . one of the successes of general relativity is that it can explain the anomalous precession , which appears to be non-conservative until one accounts for the full stress-energy-momentum pseudotensor . as far as the rotation of the earth , the moon , sun , jupiter and other planets do have small calculable effects on the tides on earth . tidal forces arise because objects like earth have spatial dimension , and the gravitational force from another body has greater influence on the side of the planet closer to the force . differentiating the gravitational force allows one to determine the tidal force . the ocean tides caused by tidal forces do effect the rotation of the planet , so although the effect of other planets beyond the moon , sun and jupiter is negligible , it is calculable , and does have some non-zero effect on the rotation of the planet .
there are a few things that keep saturn 's rings roughly the way they are . first , saturn 's d ring actually is " raining " down on saturn currently . but , the phenomenon of shepherd moons prevents the vast majority of material from leaving the other rings : " the gravity of shepherd moons serves to maintain a sharply defined edge to the ring ; material that drifts closer to the shepherd moon 's orbit is either deflected back into the body of the ring , ejected from the system , or accreted onto the moon itself . " ( quote from wikipedia ) besides this , the majority of the particles within the ring system have almost no motion towards or away from saturn ; no motion towards the planet prevents them from being lost . second , saturn 's rings cannot clump into " full-fledged " moons , but they can clump into moonlets up to several hundred meters to a few kilometers across . at last count , i think there were over 200 that had been found , and they also come out of numerical simulations . beyond these larger moonlets , quasi-stable clumps and clusters of ring particles form with great frequency the farther you get from saturn . these clusters of particles are constantly changing size , trading material , etc . , and so there is no time for them to become solid and cohesive . this gets into the idea of the roche limit and hill spheres . the basic idea of the roche limit is that the closer you are to a massive object , the more tidal forces are going to tear you apart ( or prevent you from forming to begin with ) . hill spheres are related , where the idea is at what point you are gravitationally bound to one object or another . if you are within saturn 's hill sphere versus a moon 's hill sphere , you are going to be pulled to saturn . with both concepts , you will need to have a moon forming farther away from saturn than its rings are now to actually be stable . you can see the effects of these by looking at n-body dynamical simulations of the rings . this was my research for a year and a half , and it culminated in over a hundred simulations , many of which i made movies of , and then i posted them on one of my personal websites . if you go to it , scroll down and take a look at one of the c ring simulations , b ring simulations , and a ring simulations ( warning - the movies are a bit big ) . you should choose ones with a large &tau ; value and &rho ; of 0.85 because those will show clumping better . what you will see is that , in the c ring , almost no clumping occurs . go farther from saturn into the b ring and you will see a spider web start to happen of strands of clumps of particles . then if you go to the farther away a ring the strands are fragmented more into clusters . ( note on the movies : the " l " value next to each one is how large the simulation cell is on a side , in meters . so you are just looking at a very small region of the ring . it is set so that the center of the cell does not move , so you had imagine that whole thing orbiting around saturn . )
magnetic fields do no work since $$ \left ( \frac{d{\bf{r}}}{dt} \times {\bf{b}} \right ) \bullet d{\bf{r}} = 0 , $$ so they cannot change a charged particle 's speed ( they can , however , change the direction of its velocity ) .
different boundary conditions represent different models of cooling . the first one states that you have a constant temperature at the boundary . this can be considered as a model of an ideal cooler in a good contact having infinitely large thermal conductivity the second one states that we have a constant heat flux at the boundary . if the flux is equal zero , the boundary conditions describe the ideal heat insulator with the heat diffusion . robin boundary conditions are the mathematical formulation of the newton 's law of cooling where the heat transfer coefficient $\alpha$ is utilized . the heat transfer coefficient is determined by details of the interface structure ( sharpness , geometry ) between two media . this law describes quite well the boundary between metals and gas and is good for the convective heat transfer . http://www.ugrad.math.ubc.ca/coursedoc/math100/notes/diffeqs/cool.html the last one reflects the stefan-boltzman law and is good for describing the heat transfer due to radiation in vacuum
if i understand correctly , then i believe you’re right . we are considering a situation where the drill pipe in the ground gets stuck , but where all the rest of the equipment is functioning properly , right ? in particular , we are assuming that the blocks are not jammed or anything . the proper way to define mechanical advantage $m$ is as the ratio between the distance $d$ traveled by the driving force and the distance $l$ traveled by the load . then $d$ is just the length of cable that passes through the driving winch at the base during a certain amount time , and $l$ is the height that the drill pipe moves during the same time . this ratio is equal to the inverse of the ratio of the tension $t_d$ on the cable at the winch and the tension $t_l$ on the cable attached to the drill pipe : $m = \frac{d}{l} = \frac{t_l}{t_d}$ this ensures that the amount of work done by the winch , $t_d d$ , is equal to the work done on the pipe , $t_l l$ . if the drill pipe gets stuck , then it will require a much larger tension to pull it from the ground , right ? this means $t_l$ will shoot up . but the mechanical advantage $m$ is the same , so the tension $t_d$ at the winch will likewise shoot up . this doesn’t mean the mechanical advantage is lost . to summarize : the ratio between $d$ and $l$ is not affected by the pipe jamming ; it’s just that it becomes more difficult to get them to move at all . therefore , the mechanical advantage $m=d/l$ doesn’t change i’m not sure that this is feasible , but you could confirm this by measuring the tensions at the winch and at the pipe individually .
in the einstein convention , pairs of equal indices to be summed over may appear at the same tensor . for example , the formula ${a_k}^k=tr~a$ is perfectly legitimate . but your formula looks strange , as one usually sums over a lower index and an upper index , whereas you sum over lower indices only , which does not make sense in differential geometry unless your metric is flat and euclidean ( and then higher order tensors are very unlikely to occur ) .
it would be an extremely cumbersome and inefficient way to do it . already one uses the acceleration of ions in ion propulsion systems in space : an ion thruster is a form of electric propulsion used for spacecraft propulsion that creates thrust by accelerating ions . ion thrusters are categorized by how they accelerate the ions , using either electrostatic or electromagnetic force . electrostatic ion thrusters use the coulomb force and accelerate the ions in the direction of the electric field . electromagnetic ion thrusters use the lorentz force to accelerate the ions . the term " ion thruster " by itself usually denotes the electrostatic or gridded ion thrusters . [ citation needed ] reply to the edit : of course , the lhc cant accelerate 1 kg of protons in an hour , but maybe a derivative of it could and would be the basis of space propulsion system . contemplate the lhc system , http://en.wikipedia.org/wiki/lhc . the large circle is 27kimometers diameter . thousands of magnets and cryogenic support give us a few horsepower in energy . how can this be amplified to the science fiction numbers you propose ? scientists are not crazy to use kilometers of land and enormous power were they able to get the same energy result in miniature . the technology yes , as the link shows , is usable , but multiplying the energies by enormous factors does not belong to the present technology or available energies . in addition how would one produce megawatts in space ? if one can , one does not need an intermediate wasteful step of such magnitude .
fermionic generators of course do not act just geometrically on a bosonic space ; all differential operators acting on bosonic coordinates are bosonic . at most , you could consider a superspace extension of $ads_5 \times s^5$ but superspaces are not too useful if there are too many supercharges ( they have too many components ) . so it is a kind of misguided approach to ask about the action of the supercharges on the spacetime only ; one should learn what is the action of the supergroup at the hilbert space – the whole actual theory – and it is pretty straightforward if you define the $n=4$ gauge theory . witten – when he mentions that the group acting on the ads-space times the sphere is the supergroup – really wants to say that $psu ( 2,2|4 ) $ is the ( or " a" ) maximal supergroup of symmetries that a theory defined on $ads_5\times s^5$ may have . but he surely does not mean that all the generators - the fermionic ones in particular - may be defined as differential operators acting on the 10 bosonic coordinates of this spacetime only . for the terminology of superalgebras ( and the same supergroups ) , look at page 58 of this kac 's review ( pdf ) : http://projecteuclid.org/dpubs?service=uiversion=1.0verb=displayhandle=euclid.cmp/1103900590 finally , the extra $p$ in $psu$ means that one eliminates a block-diagonal " hypercharge-like " generator from $su$ . it is similar to the embedding of $su ( 3 ) \times su ( 2 ) \times u ( 1 ) $ in $su ( 5 ) $ in grand unification ; in the superalgebra case , one can consistently eliminate the $u ( 1 ) $ here , at least if the number of bosonic entries and fermionic entries ( dimensions of the fundamental representation ) are equal . and it is equal , both are 4 for $psu ( 2,2|4 ) $ . if you check a se question about an $su ( 5 ) $ decomposition here introduction to physical content from adjoint representations the equal dimensions allow us to set the " hypercharges " of the off-block-diagonal entries ( all the fermionic generators ) which were $\pm 5/6$ above to zero and eliminate the " hypercharge " $u ( 1 ) $ which was just shown to become a center ( generator commuting with all others ) completely . without the $p$ which stands for " projective " , the bosonic subgroup of $su ( 2,2|4 ) $ would really be $su ( 2,2 ) \times su ( 4 ) \times u ( 1 ) $ with the extra last factor that actually gets eliminated in $psu$ .
without doing the analysis , i would think that a cooling system is more effective at extracting heat from a warm container than from a cold one . for a fridge , the effectiveness ( or coefficient of performance ) is $eff=q_c/w$ is the ratio of the heat removed from the cold source ( the fridge ) to the energy used for the purpose . it increases with the temperature of the cold source . this is actually the prime factor to be considered in the analysis . if we assume that , for a given current temperature of its contents , and thus for a given coefficient of performance , the cooling capacity of the fridge ( heat removed per second ) is limited only by the power of its cooling engine ( i do not know whether that is the case ) , then the heat pump will pump more heat per second when the fridge is warm . hence it is better to put all bottles at once and get the fridge warmer to have a maximum heat pumpimg capacity from the heat pump . heat sharing rate within the fridge may also be an important issue , but there are no data available to measure how important . if it is really low , thus leaving an important temperature gradient in the fridge , it may be useful to exchange the position of bottles , so as to have the warmer part of the load near the heat pump and have work with highest possible coefficient of performance . precise figures about the load do not matter very much . however , a load with large heat capacity will take longer to cool and will thus allow more time for heat sharing . in the second part below , we prove formally that all bottles should be cooled at once , and we use the understanding to discuss the heat sharing issue in some more depth . the variability of the coefficient of performance with temperature is central to this analysis . formal statement and proof a refrigerator is a carnot machine functionning as a heat exchanger , where we are interested in removing heat from the low-temperature reservoir , using work from an engine that provides compression . the effectiveness , or coefficient of performance , noted here $z$ , is defined as $z=q_c/w$ where $w$ is the work provided and $q_c$ is the amount of heat extracted from the cold source ( the refrigerator ) with that work . if we note $q_h$ the amount of heat delivered to the hot source ( outside the refrigerator ) , we have the equality $q_h=w+q_c$ . for an ideal carnot cycle , we have $z_{ideal}=q_c/w=q_c/ ( q_h-q_c ) =t_c/ ( t_h-t_c ) $ where $t_h$ and $t_c$ are the temperatures of the hot and cold source . ( see http://en.wikipedia.org/wiki/coefficient_of_performance ) . of course , the actual coefficient of performance $z$ is less that the carnot ideal . short of knowing its specific , we will only assume that , like the ideal coefficient , it depends monotonically on the temperature $t_c$ of the cold source , the hot source ( outside the refrigerator ) being considered at constant temperature . hence we only assume that the coefficient of performance $z$ is a strictly increasing function of ( cold source ) temperature , i.e. , such that $t_1&lt ; t_2\ \rightarrow\ z ( t_1 ) &lt ; z ( t_2 ) $ we also assume that the mechanical power available for compression is invariant , i.e. does not depend on the temperature of the sources , at least within the range of temperatures considered . finally , we also assume that the heat capacity of the refrigerator itself is negligible , and that heat sharing within the refrigerator takes negligible time compared to cooling time so that the content may be considered to have uniform temperature . these later two assumptions will be discussed afterwards . with the above assumptions , given two masses $m_1$ and $m_2$ to be cooled in the cold source , it is faster to cool booth simultaneously than to try to cool one first and later add the second one . it also consumes less energy . proof cooling a mass $m$ actually , the formulae above are about heat and work increments . this is necessary since $z$ is temperature dependant , and temperature may vary . also , since we intend to analyze the system from the point of view of the cold source , the heat increments are actually removed from that source and must be countd as negative . so we can write $z= -dq_c/dw$ , or $dq_c/dw=-z$ . the power provided by the compressor is a constant $p=dw/dt$ . hence $dq_c/dt= ( dq_c/dw ) \times ( dw/dt ) =-z\times p$ . on the other hand we know that removing heat reduces the temperature according to the formula $\delta q=-cm \delta t$ , where $m$ is the mass being colled and $c$ is the specific heat for the substance constituting that mass . hence we have $dq_c/dt= cm ( dt_c/dt ) $ . combining the two formulae , we get $dt_c/dt=-zp/cm$ . but we cannot resolve this equation since $z$ is an unknown function of $t_c$ . what we know is that $z ( t_c ) $ , $p$ , $c$ and $m$ are strictly positive values . so $dt_c/dt$ is strictly negative . hence $t_c$ will decrease with time . since the function $z ( t_c ) $ is a strictly increasing function , its value will also decrease with time , and hence the absolute value of the derivative $dt_c/dt$ will also decrease with time . hence the graphical representation of the evolution of the temperature will look like the red curve in the figure , where $t_0$ is the initial temperature at time $t_0$ . cooling the same mass $m$ in two steps if we consider cooling independently ( in an identical refrigerator ) another mass $m_1$ , smaller than $m$ , with initial temperature $t_0$ we get another curve , like the curve in blue in the left part of the figure up to point c , corresponding to the equation $dt_c/dt=-zp/cm_1$ . it is below the red curve because the smaller mass $m_1$ cools faster than $m$ . formally , if we draw a horizontal line like the line cutting both curves in a and b , this corresponds to the same temperature for both curves , hence to a common value of the coefficient of performance $z$ . then $m_1&lt ; m\ \rightarrow\ ( dt_c/dt ) _a&lt ; ( dt_c/dt ) _b$ . since this is true for any value of the temperature $t_c$ , it confirms that the blue curve for $m_1$ decreases faster than the red curve for $m$ . suppose now that at time $t_2$ the mass $m_1$ has been cooled to temperature $t_2$ corresponding to point c below the red curve . we add to $m_1$ another mass $m_2$ such that $m=m_1+m_2$ , the mass $m_2$ being at the initial temperature $t_0$ . the mass $m_2$ being warmer than $m_1$ will share its heat with $m_1$ ( in negligible time according to our hypothesis ) so that both reach the temperature $t_1$ corresponding to point d , and pursue cooling . at any time between $t_0$ and $t_2$ , the temperature of $m_1$ ( blue ) is less than the temperature of $m$ ( red ) . hence the refrigerator works with a lower coefficient of performance $z$ for $m_1$ than for $m$ , and less heat has been removed from the refrigerator containing $m_1$ than from the refrigerator containing $m$ at time $t_2$ . when we introduce the mass $m_2$ with $m_1$ , the total heat introduced in the refrigerator is that of $m_1+m_2$ at temperature $t_0$ . this is exactly the same as the heat introduced in the refrigerator containing $m$ . since less heat was removed from the $m_1+m_2$ refrigerator at time $t_2$ , it is at a higher temperature that the $m$ refrigerator . hence the point d is above the red curve . the $m_1+m_2$ refrigerator now contains the same mass as the $m$ refrigerator . hence it will follow an identical curve . but it is at temperature $t_1$ that was attained earlier , at time $t_1$ by the $m$ refrigerator . so the right part of the blue cooling curve for $m_1+m_2$ , starting at point d is the same as the right part of the red coling curve for $m$ starting at point b , translated by a duration $\delta t=t_2-t_1$ . conclusion the masses $m_1+m_2$ will always reach any temperature with a delay $\delta t$ after the mass $m$ has reached it . actual figures would be required to be more precise . given the problem , the cooling engine will be working at maximum power to get the fastest possible cooling in both cases . then it is obvious that the faster solution is also the most economical energetically . this assumes either that the cooling is started just at the right time , or that the cooling power is reduced once the right temperature has been attained . these results are based exclusively on our assumptions , independently of any actual figures . we will now discuss some of these assumptions . discussion heat capacity of the refrigerator we have assumed that the heat capacity of the refrigerator itself is negligible . we should however analyse its effect . we first note that the objective is to extract heat from a given number of bootles to bring them from $t_0=30^{\circ}c$ to $t_f=6^{\circ}c$ . that corresponds to a precise amount $q$ of heat to be removed , independently of the process used for that purpose . if the refrigerator itself is initially at temperature $t_f$ , it will at the start share heat with the mass to be cooled , thus warming up and cooling the bootles . but then it will have to be cooled down to $t_f$ again , so that it net contribution to the cooling process will be null , and the same amount $q$ of heat has to be removed by the heat pump . however , by sharing heat at the beginning , it induces an early cooling , thus making the whole heat removal process operate at a lower temperature , hence with a lower coefficient of performance $z$ . the net effect of the heat capacity of the refrigerator is thus to provide some early cooling , which can sometimes be considered an advantage , but at the expense of a lower effective coefficient of performance $z$ . these effects increase with the heat capacity of the refrigerator . note that if the initial temperature of the refrigerator is below the targeted final temperature $ t_f$ , the difference multiplied by the heat capacity is a net contribution to the refrigeration process , though the loss on the coefficient of performance remains . hence it is better not to have anything else in the refrigerator , even already cooled , unless it is cooled to a much lower temperature than the final temperature $ t_f$ intended for the bottles . heat sharing rate as we have seen from the previous discussion , the main objective is to remove a given amount $q$ of heat , and the effectiveness of the removal decreases as temperature is lowered . if the rate of heat sharing within the refrigerator is small , the volume near the cooling system will cool faster , thus reducing the coefficient of performance , i.e. the rate of heat removal . hence , ensuring the best possible heat sharing can help in all circumstances . it should be noted that direct sharing between cylindrical bottles will be reduced to a minimum : just one line of contact . so , if space allows , it is probably preferable to help air circulate between the bottles . keeping the bottles in vertical position will help if the refrigerator has grid shelves that let air through , rather than glass shelves . and , of course , the bottles must be unpacked . opening the door to quickly exchange bottles so that the warmer ones are placed near the cooling system will improve the coefficient of performance and reduce cooling time . it may have some cost in warming the refrigerator , but that is less important if the heat capacity of the load to be cooled is large ( actual measurements would be useful ) . exchanging bootles will also avoid having to cool those close to the cooling system below the required temperature $ t_f$ in order to have all the bottles temperature at least as low as $ t_f$ .
the dynamical degrees of freedom are in the schroedinger state , not in the quantum field operators ( which would require a heisenberg picture with a fixed state ) . the squeezed states at time $t$ are created from the vacuum by multiplication with a unitary matrix $e^{ix ( t ) }$ where $x ( t ) $ is an appropriate hermitian expression quadratic in the creation and annihilation operators in momentum space , with coefficients depending on the time $t$ .
current physics formulation has two frameworks . one is the macroscopic one , in dimensions comensurate with our physical faculties of observation , larger than micrometer sizes . this is the classical framework which was studied and formulated mathematically until the beginning of the twentieth century . since then we have found out that the classical framework emerges from a more fundamental one , the quantum mechanical framework that describes the microscopic world , molecules , atoms , nuclei etc . there is wave–particle duality . according to this theory , light is a wave and a particle at once . the statement on wave particle duality is a statement on the quantum mechanical framework . in that framework yes light can behave like a billiard ball ( particle ) or like a wave ( with a sine/cosine wave like behavior in the possibility of detecting it ) . what about magnetic field ? can it be so , the magnetic field is a macroscopic concept described by the classical electromagnetic theory . that it is also a wave and particle , but this particle has not yet been discovered ? so no , it is not a wave and a particle in the quantum mechanical sense . the magnetic field emerges from the underlying quantum mechanical framework in a strict mathematical way , that you might learn if you continue your physics studies , but has no description as a quantum mechanical/wave-particle . as a handwaving explanation , the magnetic field emerges from the virtual coherent overlapping of innumerable photons , which photons are the particle/wave form of the electromagnetic force in the quantum mechanical framework . virtual because of the mathematics involved which do not allow them to light up as a light source : ) but remain tied up to the atoms and molecules creating the classical field . non virtual photons do obey the wave particle duality , depending on the way they are detected . is magnetic field discrete ? it is discrete only in the sense that it is generated by individual atoms that carry a magnetic field , not in the sense of the particle/wave duality .
building an action : if you know the field content ( which i assume means you know the gauge group and reps of all the fields ) then : write down every term that is lorentz scalar ( so combinations like $\partial_\mu a^\mu$ , $\bar{\psi}\gamma^\mu \partial_\mu\psi$ allowed but not things like $\vec{n}\cdot\nabla \phi$ where $\vec{n}$ is some random 3-vector ) . stop at terms with dimension greater than the spacetime dimension ( 4 in 4d , so include terms like $\phi^4$ and $\phi\bar{\psi}\psi$ but not higher dimension terms like $\phi \partial_\mu \phi \bar{\psi} \gamma^\mu \psi$ ) . cross out terms that are not gauge invariant . this means that gauge fields can only appear through covariant derivatives and field strength tensors , and matter fields must appear in singlet combinations . cross out terms that violate any global symmetries you want to impose ( though be warned - if these symmetries are anomalous you can not drop these terms consistently ) . in susy theories you need to impose relations between coupling constants as well . after you have done this you can probably use field redefinitions ( orthogonal/unitary rotations in flavour space ) to simplify some of the coupling constants . an example would be the standard model where the lepton yukawas can be diagonalised and the quark yukawas can be diagonalised leaving just the physical ckm matrix . it is really a lot like lego - the fields are the building blocks and symmetries and gauge invariance tell you what you can put together .
let 's say your target is a film $10^{-2}$ mm thick . nuclei are about $10^{-14}$ m in diamater at most . this means that the alignment of the beam with the target would have to be $10^{-9}$ radians , which is not possible with realistic beam optics . even if your beam optics were that perfect , the perfect parallelism would be destroyed by scattering once the beam entered the target .
i believe there is such a representation , as follows : you need only consider operators which can be written as $\omega = \sum_k \alpha_k \gamma_k$ , where $\gamma_k = \sum_\ell p_\ell \big ( \sigma_{k_1} \otimes \cdots \otimes \sigma_{k_n} \big ) p_\ell^\dagger$ with $p_\ell$ being the operator corresponding to the $\ell^{\text{th}}$ permutation of qubits . note that $\gamma_k$ is defined on the whole space , and is essentially a generalization of the number operator . $\gamma_k$ can be uniquely identified by each of the number of each kind of pauli matrix it contains , and so is polynomial in the number of qubits . the same works for the case of qudits , though the degree of the polynomial will scale with the dimensionality of the local systems . for qubits , we have 3 relevant numbers : 1 ) $n_x$ the number of sites where the operator acts as $\sigma_x$ , 2 ) $n_y$ the number of sites where the operator acts as $\sigma_y$ and 3 ) $n_z$ , the number of sites where the operator acts as $\sigma_z$ , subject to the constraint that $n_x + n_y + n_z \leq n$ . so instead , we can relabel the $\gamma$ matrices as $\gamma_{n_x , n_y , n_z}$ for the two dimensional case . notice that the set of possible $\omega$ form a group under multiplication , and that each element of the group has a polynomial description ( up to approximation of the complex coefficients ) . thus : since the $\gamma$ operators form a basis for hermitian matrices which are invariant under permutation of the local hilbert spaces , they satisfy your first criterion . any stabilizing operator $\omega$ can be described by a set of real numbers ( or approximations there of ) , the number of which is polynomial in the number of subsystems and exponential in their local dimension , thus satisfying your second criterion . symmetric local unitaries can also be written in terms of a sum of $\gamma$ matrices ( albeit it with an additional restriction on the values of $\alpha_k$ ) , and thus by the group structure , the outcome can still be represented within this framework . when a unitary is applied , the stabilizers transform as $u \omega u^\dagger$ , which is efficiently computable if $u$ is symmetric given the reduced basis with which both $u$ and $\omega$ can be expressed , satisfying your last criterion .
i would like to morph the question somewhat : what happens when a tank alternatively sucks in and expels fluid through the same nozzle ? the surprising answer is : it will move in the direction it would move if only fluid would be expelled . this phenomenon is known as machian propulsion . machian propulsion is utilized in so-called pop-pop ( or putt-putt ) boats : toy boats that deploy an ultra-simple steam engine . pop-pop ( putt-putt ) boat design the abstract to this paper summarizes the physics : " many experimenters , starting with ernst mach in 1883 , have reported that if a device alternately sucks in and then expels a surrounding fluid , it moves in the same direction as if it only expelled fluid . this surprising phenomenon , which we call " machian propulsion , " is explained by conservation of momentum : the outflow efficiently transfers momentum away from the device and into the surrounding medium , while the inflow can do so only by viscous diffusion . however , many previous theoretical discussions have focused instead on the difference in the shapes of the outflow and the inflow . whereas the argument based on conservation is straightforward and complete , the analysis of the shapes of the flows is more subtle and requires conservation in the first place . our discussion covers three devices that have usually been treated separately : the reverse sprinkler ( also called the inverse , or feynman sprinkler ) , the putt-putt boat , and the aspirating cantilever . we then briefly mention some applications of machian propulsion , ranging from microengineering to astrophysics . " pop-pop boat engine working cycle video on how to build a pop-pop boat : http://youtu.be/0ki9kta8g14 the answer to the question posed above should now be clear : when sucking in air into an empty tank , the tank will briefly tremble but not show any sustained movement . this is even the case when the tank can move without friction : the air would rush in and the tank will start to move in opposite direction , however , the inflowing air will transfer its momentum to the tank , and the tank will come to a perfect stand still .
faraday 's cage is known to block static and non-static electric fields . the mechanism of blocking depends on whether the electric field is static or non-static ( em field ) . i suppose you question is about how the cage works in non-electrostatic case . in em case ( time changing field ) , two scenarios could happen . the first is electric discharge where the current flows from a distant electrode to the cage . the second is an em wave with high power propagating toward the cage generating its current locally within the conductor . i will explain how the cage works for both cases . with respect to the first case , it can be mathematically described by charge continuity equation ( equation 3 in this link ) . this equation basically relates the current flowing through a conductor to the charge accumulating in it . what happens in the first scenario is that the external current ( being moving charges ) coming from the electrode accumulates at the point where it ( the spark or the streamer ) hit the cage . because the cage is a conductor the charge continuity equation tells us that the local accumulation of charge where the spark hit the cage will cause current to flow within the conductor to remove that accumulation . the characteristic time required to remove the accumulation is called the relaxation time . it can be derived from charge continuity equation . for the derivation have a look at pages 57-59 of this document . i think that is taken from a book called elements of electromagnetics chapter 5 . if the conductor is made from a material with infinite conductivity , the relaxation time is zero . that means the current will keep flowing though the cage without any problem and that the electric field in the conductor is always zero . in other words , the electrostatic point of view holds even for non-electrostatic case if the conductivity is infinite . that is a direct consequence from charge continuity equation . for non infinite conductivity cases , the electric field within the conductor will survive within the conductor with a time scale related directly to relaxation time of that conductor . i hope it is clear now with respect to first case . the second case is related to em waves where they generate their currents locally within the conductor , that is where the skin effect comes into play . an em wave penetrates into a conductor the skin effect occurs . in general , em waves when they penetrate a conductor they are attenuated until their fields become almost zero . a characteristic depth of penetration is called skin depth . the skin depth is the distance it takes an em wave to be attenuated to certain value . this skin depth depends on many factors such as conductivity and frequency , the following figure taken from wikipedia shows the skin depth of different materials for different frequencies : for the cage to protect from em waves , it is thickness has to be larger than multiples of skin depth at the particular frequency of interest . so briefly with respect to the second scenario , the skin depth becomes relevant when we speak about shielding from electromagnetic waves rather than discharge current . the first and the second scenarios can be put together in frequency spectrum , the first scenario describes why the cage protects current in low frequencies while the second scenario describes why it protects from both current and radiation at high frequencies . i think the cage in the picture shows scenario 1 . you can clearly see the distant electrodes and the point at which the spark hits the cage i hope that answered your question
general relativity complicates things a bit , but in special relativity an inertial frame is a time independent co-ordinate system that is homogenous and isotropic . the time independence means that everything in that frame is at rest wrt everything else , and the homogenous and isotropic bit means the co-ordinate system has no curvature . take your question 1: you could define a rotating frame that rotates along with your rod , but this frame would not be isotropic ( except at the pivot point ) because there is an acceleration acting towards the pivot point and this picks out a particular direction . it is also not homogeneous because the acceleration varies with position . incidentally , your example of the rod is basically the same as the rotating disk i mentioned in my answer to your other question . you can treat the rod as a radial strip in the disk . re your other questions , can you ask these separately . as it is , the answer is heading towards essay size !
all stars have some motion relative to us called the proper motion , though only nearby stars or fast moving stars have a big enough proper motion for it to be easily measured . lots of stars move at several arc seconds per year so 30 arc seconds would take only a few years ( is 30 arc seconds really " noticable " ? - i doubt i would notice it ! ) . when a change to a constellation becomes noticable is i would guess a matter of judgement . this site shows changes in the better known constellations after 50,000 years . i am sure googling will find many similar sites .
this is an extremely comprehensive review of electronic properties in two-dimensional electron systems ( 2dess ) : http://rmp.aps.org/abstract/rmp/v54/i2/p437_1 but , as you can imagine , it covers almost everything there is to cover in 2dess . for areas ( in transport ) you are focusing on you will find only sections iv c and d useful ; it involves computation of relaxation times in certain regimes . the research/review articles that i have come across so far do not talk comprehensively about transport ; most of them are limited to certain cases . general transport theory is covered in many excellent textbooks . one such example is this book by lundstrom : http://www.amazon.com/fundamentals-carrier-transport-mark-lundstrom/dp/0521631343 chapter 2 of the about book discusses computing relaxation times due to various scattering mechanisms : electron-impurity , electron-phonon ( optical and acoustic ) , electron-electron , as well as various types of scattering : inter- and intra-valley scattering etc . the rest of the characteristic scales can easily be determined from the relaxation time ( except the phase coherence length ) . the core of the book , however , makes use of the boltzmann transport equation ( bte ) , discussed in chapter 4 , which is inherently classical . however , since we often use band theory ( quantum ) alongside the bte , this is called semiclassical transport . this is why i said that you can compute all characteristic scales except for the phase coherence length . you need a fully quantum treatment to determine the phase coherence length . such a treatment is possible using non-equilibrium green 's function ( negf ) formalism . this book by datta : http://www.amazon.com/quantum-transport-transistor-supriyo-datta/dp/0521631459 gives you a very good intuition for negf since it approaches negf from the landauer formalism . if you are already familiar with the landauer formalism then all you need to do is take a look at the comparison between negf and landauer on page 27 , and then skip to chapters 8-10 . even towards the end of lundstrom 's book , i.e. chapter 8 and 9 , you will start seeing crossovers from semiclassical to quantum , diffusive to ballistic etc . the two books by datta and lundstrom are mutually complementary , and serve as an extremely valuable resource on carrier transport . if you are overwhelmed by the content in these books ( not surprising if true ) and then you can always visit the website http://nanohub.org/ to watch video lectures given by both these authors . this is an excellent class taught by datta : http://nanohub.org/resources/6172 and follows the same textbook i listed above .
you are right about the stopping time , if you continuously apply a constant force , this will indeed be true . the stopping distance will probably not be the same , as the pingpong-ball is moving much faster initially ( why ? ) . can you determine the velocity of the ball as a function of time ? how will you use this velocity for determining the stopping distance ?
the temperature of the gas that is sprayed goes down because it adiabatically expands . this is simply because there is no heat transferred to or from the gas as it is sprayed , for the process is too fast . ( see this wikipedia article for more details on adiabatic processes . ) the mathematical explanation goes as follows : let the volume of the gas in the container be $v_i$ , and its temperature $t_i$ . after the gas is sprayed it occupies volume $v_f$ and has temperature $t_f$ . in an adiabatic process $tv^{\ , \gamma-1}=\text{constant}$ ( $\gamma$ is a number bigger than one ) , and so $$ t_iv_i^{\ , \gamma-1}=t_fv_f^{\ , \gamma-1} , $$ or $$ t_f=t_i\left ( \frac{v_i}{v_f}\right ) ^{\gamma-1} . $$ since $\gamma&gt ; 1$ and , clearly , $v_f&gt ; v_i$ ( the volume available to the gas after it is sprayed is much bigger than the one in the container ) , we get that $t_f&lt ; t_i$ , i.e. the gas cools down when it is sprayed . by the way , adiabatic expansion is the reason why you are able to blow both hot and cold air from your mouth . when you want to blow hot air you open your mouth wide , but when you want to blow cold air you tighten your lips and force the air through a small hole . that way the air goes from a small volume to the big volume around you , and cools down according to the equations above .
there are two ways of getting electromagnetic radiation from matter . matter is usually neutral , the electrons and protons are equal in number to each other and any fields are spill over giving rise to van der waals forces which bind neutral atoms into molecules etc . at this micro level nature is quantum mechanical . that means that all electrons are in energy levels some of which energy levels are practically a continuum , i.e. the difference between them is very small . this means that vibrations of the atoms and molecules in their solid structure , as an example , will excite by kinematics these levels and fall back by the emission of a photon ( de-excitation ) ; the ensemble of these photons gives rise to black body radiation . when the temperature is high the corresponding energy levels have larger gaps , and the photons are of higher energy . a filament lamp has high enough temperature to emit visible light . liquids have similar behavior , gases only have molecular energy levels and vibrations but the process is the same . kinetic energy from temperature is transformed into photons from de-excitations . the bulk of light we see comes from this mechanisms , even the light from the sun . there are the led lights , again a quantum mechanical effect , but of different origin : " when electrons cross the junction from the n- to the p-type material , the electron-hole recombination process produces some photons in the ir or visible in a process called electroluminescence . " the second way of getting light is how the other answers state , by accelerating charges , ions and electrons , as in sparks and lightning , plasma etc .
i have both very good news and very bad news for you . the good news : the original quality ( at least the higher quality i have been able to find ) seems to be here : go to http://videolectures.net/mit801f99_physics_classical_mechanics/ then click on one of the lectures , e . g http://videolectures.net/mit801f99_lewin_lec04/ then look at the right side of the playing video ; there are several links : download mit801f99_lewin_lec04_01 . flv ( video 148.2 mb ) download video download mit801f99_lewin_lec04_01 . m4v ( video 113.4 mb ) download video download mit801f99_lewin_lec04_01 . rm ( video 113.4 mb ) download video download mit801f99_lewin_lec04_01 . wmv ( video 466.0 mb ) download subtitles download subtitles : tt/xml , rt , srt as you see , the . wmv version weights much more , and it is actually in a much better resolution . now , the bad news : there seems to be no way to massively download all the . wmv files with the lectures . more than that , even if you decide to visit each single lecture page to download them one by one , you have to do it with your navigator , not with kget / wget or any other download manager . additionally , the . wmv ( windows media video ) format is not 100% compatible with my codecs in ubuntu . i cannot play the videos with vlc , but rather have to use other programs , and the aspect ratio has to be manually set every time . . . so please , if you find another possibility to download a higher resolution version of these funny lectures , tell it here , or e-mail to ngc6720 . at . gmail . com ( note that there is a couple of isolated higher resolution lectures in youtube , but not the complete collection ) i hope this helps .
one possible explanation is delayed phase transformation in a nickel sulphide inclusion in toughened glass ( http://www.glassonweb.com/articles/article/330/ ) , which glass is sometimes used for cooking ware . as noted in a comment to the referenced article , this process can be accelerated by temperature variations . judging by the description , the pattern of failure seems typical for toughened ( tempered ) glass . again , i cannot be sure that this mechanism was indeed present in the case described in the question .
now , what does it mean for a to be negative ? it is not negative in general . the mistake you are doing is to assume that the logarithm is positive . but $\ln z$ may be both positive and negative depending on whether $z$ is greater than one or smaller than one . both options are possible because $z=\sum \exp ( -e_i/kt ) $ and if $e_i$ is smaller than $kt$ ( and smaller by an extra margin that compensates the degeneracy ) , you will get $z\lt 1$ and $a\gt 0$ . after all , the quantity $a$ has the same additive shift ambiguity as any form of energy ( any thermodynamic potential ) , $e\to e+\delta e$ does not change anything about non-gravitational physics , so whether $a$ is smaller or greater than a particular ad hoc threshold ( although it is denoted " zero" ) is a physically inconsequential question . perhaps more importantly , how does one simply go about obtaining work from a system with some $a$ ( a practical question ) ? there is obviously no " mechanical recipe " to construct clever and/or efficient engines . the value of $-a$ nevertheless encodes the maximum amount of useful work that may be extracted from the system whose helmholtz free energy is $a$ . if one wants not to lose energy , one tries to make the individual stages of the engine as reversible as possible , like in the carnot cycle . so one should better be sure that the heat is transferred between bodies of ( almost ) the same temperature etc . or , perhaps even more importantly , is it this requirement that there be a second final state , seemingly of lower free energy , that makes $a$ itself not so significant , but rather $\delta a$ ? the claim that $-a$ measures the maximum amount of useful work that may be extracted holds assuming that $e=0$ is the minimum non-thermal energy where one can get . so yes , the additive shift ambiguity is always there and the maximum extraction of the useful work really means that we are interested in the value of $\delta a$ , the changes of energy or thermodynamic potentials , and not the values themselves . the final state 's having $e=0$ is what de facto identifies $e_{initial}$ and $-\delta e$ and similarly for other quantities . and if so , what happens to the intuition about a system with only one state having exactly its energy as free-energy ? if there is only one state , there is no interesting thermodynamics or statistical physics . the state has energy $e=a$ and the maximum amount of useful work that may be extracted is whatever the mechanical properties of the state allow . if it is not specified otherwise , the assumption is that $e=0$ which also means $a=0$ in this case is the minimum value one can get to .
$$ \frac{1}{\sqrt{2}} \begin{vmatrix} \chi_{\mathbf{p}_1} ( \mathbf{x}_1 ) and \chi_{\mathbf{p}_2} ( \mathbf{x}_1 ) \\ \chi_{\mathbf{p}_1} ( \mathbf{x}_2 ) and \chi_{\mathbf{p}_2} ( \mathbf{x}_2 ) \end{vmatrix} $$ corresponds to $a_{\mathbf{p}_1 s_1}^{\dagger} a_{\mathbf{p}_2 s_2}^{\dagger} |0\rangle$ . here $s$ is the spin for the fermions . in the slater determinant $\mathbf{x}$ includes both spatial and spin parts .
the general issue is that you cannot plug your equations of motion into the lagrangian and naively expect to get the same equations of motion back out again . why not ? let us look at your specific example . for the usual story we start with $$ l = \frac12 m ( \dot r^2 + r^2\dot\theta^2 ) - v ( r ) . $$ we find that the angular momentum , defined by $\ell=m r^2\dot\theta$ , is conserved so the equation of motion for the radial coordinate is $$ m \ddot r - \frac{\ell^2}{m r^3} + \frac{\partial v}{\partial r} = 0 . $$ now , you want to plug $\ell$ back into the lagrangian . if we do that we have $$ l = \frac12 m \left ( \dot r^2 + \frac{\ell^2}{m^2 r^2} \right ) - v ( r ) . $$ naively , if we calculate the equation of motion from this lagrangian that we will get the opposite sign for the $\ell^2/m r^3$ term . this is not correct ! recall that when we call $\ell$ a conserved quantity we mean it is a constant in time , that is $\dot\ell=0$ . explicitly writing out the euler-lagrange equations we have $$ \frac{\mathrm{d}}{\mathrm{d}t}\left [ \left ( \frac{\partial l}{\partial\dot r} \right ) _{r , \theta , \dot\theta} \right ] - \left ( \frac{\partial l}{\partial r} \right ) _{\dot r , \theta , \dot\theta} = 0 . $$ here i have included the reminder that when we take partial derivatives we mean that " everything else " is held constant and what that " everything else " is . for the problem at hand note that $$ \frac{\partial\ell}{\partial r} = \frac{2\ell}{r} \ne 0 $$ so it is not a general constant . keeping this in mind , we do get the correct equation of motion ( as we must ) .
suppose you have some linear algebra background . the most important thing you need to know is that the inner product has the same meaning of what you have learnt in linear algebra class . the inner product $$\left\langle \phi|\psi\right\rangle =\int\phi^{*} ( x ) \psi ( x ) dx$$ has the meaning related to a projection of one vector onto another vector ( for true projection , the wavefunctions needed to be normalized ) . it is similar to the projection of a three dimensional vector $\mathbf{\vec{v}}=a\hat{\mathbf{x}}+b\hat{\mathbf{y}}+c\hat{\mathbf{z}}$ onto another unit vector $\mathbf{\hat{x}}$ which gives you the results $\mathbf{\vec{v}}\cdot\mathbf{\hat{x}}=a$ . first , the inner product can give you the " length square " of the wavefunction : $$\left\langle \psi|\psi\right\rangle =\int\psi^{*} ( x ) \psi ( x ) dx =\int|\psi ( x ) |^2dx$$ similar to the $\mathbf{\vec{v}}\cdot\mathbf{\vec{v}}=a^{2}+b^{2}+c^{2}$ , so you can normalize your wavefunction by the condition $\left\langle \psi|\psi\right\rangle =1$ . second , it allows you to show that two wavefunctions are orthogonal to each other , given by the condition that the inner product evaluated to zero $\left\langle \phi|\psi\right\rangle =0$ which is the analog to the $\mathbf{\hat{x}}\cdot\mathbf{\hat{y}}=0$ . third , if we write the wavefunction $\psi ( x ) $ as a linear combination of orthonormal wavefunctions $\psi_{n} ( x ) $: $$\psi ( x ) =\sum_{n}c_{n}\psi_{n} ( x ) $$ similar to a general vector in linear algebra , then we will have the inner product $\left\langle \psi_{n}|\psi\right\rangle =c_{n}$ . the meaning of $c_{n}$ is the probability amplitude and it is a complex number in general . so the probability $p_{n}$ of the wavefunction $\psi$ having the component $\psi_{n}$ is given by $p_{n}=|c_{n}|^{2}=|\left\langle \psi_{n}|\psi\right\rangle |^{2}$ . the meaning here is very important when you learn how to preform measurement . lastly , you should add two wavefunctions amplitude together before you take the square , similar to adding the amplitude of two water waves . more precisely , if the new wavefunction is $\psi ( x ) = a [ \psi_{a} ( x ) +\psi_{b} ( x ) ] $ , then the probability density at position $x$ is $a^2|\psi_{a} ( x ) +\psi_{b} ( x ) |^{2}$ . note that $a$ is the normalization constant given by the condition $\left\langle \psi|\psi\right\rangle=1$ . it is where the quantum effect arise . dont take the square and then add them together .
to close the loop , andrew , the answer to your newest question is : the best and most famous reference about the electrodynamics of moving bodies is einstein , albert ( 1905-06-30 ) . " zur elektrodynamik bewegter körper " . annalen der physik 17: 891–921 . see also a digitized version at wikilivres:zur elektrodynamik bewegter körper . the english translation , " on the electrodynamics of moving bodies " , is here : http://www.fourmilab.ch/etexts/einstein/specrel/www/ the content of this paper became known as the special theory of relativity . i am just partly joking because for uniformly moving media , the lorentz boost to the rest frame is still the most natural way to proceed .
there is really no such thing as a semi-classical particle in the sense you are thinking of it . everything follows quantum laws , even large , massive objects . however , when you have a large number of particles and/or a large number of interactions with the environment , the different parts of the system are not in a coherent superposition . so you can take a statistical average of sorts , and the quantum laws simplify to classical laws . perhaps this is better explained with an example : consider trying to send a car through a double-slit apparatus . in principle , you could do it , if you managed to create a coherent superposition of ( all the car 's atoms going through the left slit ) + ( all the car 's atoms going through the right slit ) + ( all the car 's atoms crash into the wall together ) and maintain that superposition throughout the experiment . but in practice , that is not going to happen . the car is going to interact with its surroundings through radiation , vibrations , etc . and also different parts of the car interact among themselves , and that serves to collapse the superposition of left+right long before the car ever makes it to the slits .
first of all , according to earnshaw 's theorem one can not create a potential well in the ball . second , the function should fit laplace 's equation : $$ \delta \varphi = 0 . $$ in fact the second point is enough to prove the first one . so the answer is : no , this can not be made for every continuous function . yes , this follows from gauss 's law .
there is a difference between finding a solution and recognizing a solution . oracle can recognize the solution or solve a particular instance of the problem but cannot give you the solution for complete problem . or in other words , oracles gives you a part of solution and you may need to consult oracle a number of times to get the complete solution . oracle also may be thought as a library function ( as in programming languages ) which will give you solution for one instance of a problem , and the real cost of computation is measured by how many times you call the function and not the inherent complexity of the function itself which is taken as black box . for example , lets say we have a oracle for a function $f ( x ) = x^2$ , on presenting this oracle with a pair $ ( a , b ) $ it will tell whether $b^2 = a$ or not . in this case time complexity is taken as how many time you need to consult the oracle to get the desired result . more concrete example can be taken from oracle for verifying if the number if prime . lets say we want to find the first prime number $p : a &lt ; p &lt ; b , a &lt ; b \in z^+$ . the problem has different complexity when you are given access to the oracle and when you are not given . physical example of an oracle : lets say our problem is to determine the angle between the floor and the wall which may not be necessarily $90^\circ$ to it . all you can do is throw a ball which will go elastic collision on the wall and return back . you have control of the angle you throw and you can note the angle it came back . each throwing of a ball can be compared with the calling of oracle function and the constraint on the angle of the returning ball ( reflection , which gives you a hint on the orientation of the wall ) can be considered an oracle . the number of times you need to repeat the throwing of ball to get the orientation with desired accuracy may be considered as the complexity of the problem relative to the oracle .
when we say that object a is moving at speed v relative to an object b it means that there is a reference from where b is at rest and a is moving at speed v in that reference frame . if a is a photon then it moves at the speed of light c in all reference frames so if b is is us then in our reference frame it is moving at c , so it makes sense to say that the photon moves at speed c relative to us , but is it ok to say it the other way round ? if a and b are both objects that have mass so that they move at less than the speed of light , and if a is moving at speed v relative to an object b then in the frame where a is at rest b will be moving at speed v relative to a in the opposite direction . so for speeds less than the speed of light , the speed of a relative to b equals the speed pf b relative to a . it is tempting to extrapolate this to the case where a is a photon and conclude that therefore b ( us ) is also moving at speed c relative to the photon . however this would mean that we were moving at speed c in a reference frame where the photon is at rest . this is not possible because we cannot move at speed c in any reference frame and a photon cannot be at rest in any reference frame . so the answer is " no " , it is not correct to say that we move at speed c relative to the photon .
1 ) this varies by textbook . a common format you will see is h=6.62606957 ( 29 ) ×10−34 ( from wikipedia : planck constant ) . the digits in parentheses indicate they are uncertain . hence , you had expect that h is known to at least 0.00000001/6.6260957 ( pretty well known . ) other references will explicitly state what the error bars are , or may simply cite the sources . you had expect the error bars to be in the references in the latter case . 2 ) this is a fine way to determine your accuracy . i am presuming , of course , that your error bars are much larger than the uncertainty in the boiling point of water . do not be so sure that is the case ! water boiling point changes with air pressure , humidity , saline content . . .
for a high degree of accuracy you would have to probe the particle with a high energy ( short wavelength ) photon so there is plenty of energy that can go into vibrational excitation . after such hard hit the particle will be smeared across a wide range of states $$\psi=a_0*\psi_0+a_1*\psi_1+ . . . $$this is not an entanglement but a simple superposition of eigenstates . the expectation value of the particle 's energy $\bar{e}=\sum_{i}a_i^2e_i$ should not be equal to the energy of any particular state and the second measurement will yield $e_i$ with $a_i^2$ probability . so , the extra energy comes from an interaction with a probe particle and it does not have to be precisely equal to the energy of a certain vibrational state .
peltier elements available in trade are commonly rated for operating down to to vicinity of -50 °c this is still much higher than ln2 . while the electric properties should not change much ( though the efficiency drops ) , such thermal stress may simply damage them mechanically . their primary profit in cooling applications is to raise the temperature threshold versus the ambient level : it is much easier to get a heatsink down from 120°c to 100°c in room temperature , than from 60°c to 40°c so while the heatsink runs really hot , the cpu enjoys nice , low temperature despite the additional heat produced by the element . now , applying ln2 to the heatsink completely negates these benefits . your peltier element is just a fancy heater vulnerable to low temperatures . i suggest you get a normal resistive heater instead for regulation . they are vastly more cold-resistant and you have such a surplus of " cold " that you really do not need the option to go " even cooler " . edit : the thermal coefficient of unpowered ( and shorted , as it will power itself up with seebeck voltage even unconnected ) peltier module is around 2w/ ( m*k ) ( 2 ) with a typical 36x36x4mm module that would give heat transfer of about 130 watt at temperature difference 200k ( assuming -196°c of ln2 , +4°c of cpu ) this is also about the limit of heat the module of this size is able to transfer when powered to vmax , which would mean this temperature difference ( 200k ) would be about what you had be able to maintain between the heatsink and the cpu while powering up the module for blocking heat transfer , and so the whole contraption would work just right under the dubious assumption that the module will perform correctly . why dubious ? the modules are rated for operating completely under temperatures of above -50°c and with δт not exceeding 100°c they will be operating with one side in temperature -196°c and δт in vicinity of 200°c you exceed both the minimum temperature rating and the temperature difference between the two sides by 100% . while the semiconductors should not be affected , the plastic that binds them is not nearly so frost-resistant . the most likely outcome is that the module will come apart at its seams , the binding plastic will shatter and crumble , massive condensation in dehermetized inside will short the circuit and the whole device will fail completely .
in general , both iqhe and fqhe are rigid quantum states , whose rigidness is protected by the finite energy gap ( $h\omega$ for iqhe ) between the ground state ( s ) and the exited states . finite temperature can support excitations to overcome the gap , which destroys the rigidness of the state . under finite temperature , the quantization of the hall conductivity is no longer exact . if the energy scale of temperature $k_bt$ is greater than the gap , the rigidness will be completely destroyed , and the quantization effect can not be observed anymore . in particular for the iqhe you considered , the landau level is still quantized under any temperature , but the fermion occupation is not . the fermion occupation follows the fermi-dirac distribution . under finite temperature , the fermion surface is no longer sharp , so the fermions can not integer fill the landau level , as a result , the quantized transport will be smeared out by the temperature .
if you look at this problem in 2d you have the following parameters at some instant which describe your trajectory ( position and velocity ) around a celestial body with gravitational parameter $\mu$: radius $r$ , radial velocity $\dot{r}$ and angular velocity $\omega$ . there are also a few others , but these do not really matter in this problem , due to symmetry . you can calculate the radius of your periapsis by using conservation of specific angular momentum and orbital energy . $$ \epsilon=\frac{\omega^2r^2+\dot{r}^2}{2}-\frac{\mu}{r}=\frac{\omega_p^2r_p^2}{2}-\frac{\mu}{r_p} $$ $$ h=\omega r^2=\omega_pr_p^2 $$ from here you can derive an expression for the radius of the periapsis $r_p$: $$ \omega_p=\frac{h}{r_p^2} $$ $$ \epsilon=\frac{\left ( \frac{h}{r_p^2}\right ) ^2r_p^2}{2}-\frac{\mu}{r_p}=\frac{h^2}{2r_p^2}-\frac{\mu}{r_p} $$ $$ \epsilon r_p^2=\frac{h^2}{2}-\mu r_p\rightarrow \epsilon r_p^2+\mu r_p-\frac{h^2}{2}=0 $$ $$ r_p=\frac{-\mu\pm\sqrt{\mu^2+2\epsilon h^2}}{2\epsilon} $$ these two solutions correspond with apoapsis and periapsis ( the to point in the orbit at which the radial velocity is zero ) . and you might suspect that the ' plus ' solution might correspond with apoapsis and ' minus ' with periapsis . however the opposite is true , since for an elliptical orbit $\epsilon$ is negative ( it will also hold for trajectories with higher eccentricity ) , so : $$ r_p=\frac{\sqrt{\mu^2+2\epsilon h^2}-\mu}{2\epsilon}=\frac{\sqrt{\mu^2+\left ( \omega^2r^2+\dot{r}^2-\frac{2\mu}{r}\right ) \omega^2r^4}-\mu}{\omega^2r^2+\dot{r}^2-\frac{2\mu}{r}} $$ to find what would be the best angle to burn to lower your periapsis the most you could use a fixed amount of $\delta v$ and see which angle $\phi$ would lower the periapsis the most ( $\frac{\delta}{\delta\phi}\delta r_p=0$ ) : $$ \delta r_p=\frac{\sqrt{\mu^2+\left ( \omega^2r^2+\dot{r}^2-\frac{2\mu}{r}\right ) \omega^2r^4}-\mu}{\omega^2r^2+\dot{r}^2-\frac{2\mu}{r}} - \frac{\sqrt{\mu^2+\left ( ( \omega r+\delta v\sin{\phi} ) ^2+ ( \dot{r}+\delta v\cos{\phi} ) ^2-\frac{2\mu}{r}\right ) ( \omega r+\delta v\sin{\phi} ) ^2r^2}-\mu}{ ( \omega r+\delta v\sin{\phi} ) ^2+ ( \dot{r}+\delta v\cos{\phi} ) ^2-\frac{2\mu}{r}} $$ i will leave deriving this equation to you . but i suspect that lowering the specific angular momentum will have the greatest impact , so burning in the radial direction , especially when are far away . edit : to answer your second question in your ( first ) comment . adding the radial part to $\omega^2r^2$ can be done using the fact that the radial velocity component ca be expressed as $v_\theta=\omega r$ , so : $$ v_\theta+\delta{v_\theta}=\omega r+\delta{v}\sin{\phi}\rightarrow \left ( v_\theta+\delta{v_\theta}\right ) ^2=\left ( \omega r+\delta{v}\sin{\phi}\right ) ^2 $$ however your equation can be simplified to this as well : $$ \left ( \omega+\frac{\delta{v}\sin{\phi}}{r}\right ) ^2 r^2=\left ( \left ( \omega+\frac{\delta{v}\sin{\phi}}{r}\right ) r\right ) ^2=\left ( \omega+\frac{\delta{v}\sin{\phi}}{r}\right ) ^2 r^2 $$
the short answer is that if you have coefficients for all the terms , you have two independent exact fake scale invariance for the field $\phi$ and $m$ which just rescales the fields and the coefficients appropriately to keep the hamiltonian exactly the same . this is not a real invariance of the action , since it changes the parameters of the action , it is best thought of as choosing the dimensional scale of the two fields . you usually do this by fixing the terms " l " and " k " , but you get a different scaling if you fix the " t " and " l " terms , which is physical in different limits . i should point out that this model is exactly solvable , there are no real interactions in this model , so the renormalization group analysis is just dimensional analysis in disguise . there are two rotated q-modes mixing $m ( q ) $ and $\phi ( q ) $ which are completely free .
i have turned my comment into an answer so the question can be closed . a very similar question was asked and answered here .
jem-euso is designed to look for air showers caused by extremely high energy cosmic rays ( > $10^{19}$ ev ) . according to the website , it will be attached to the international space station in 2016 .
the use of the chemical potential $\mu$ as state variable is useful in situations where composition $n$ is variable and/or cannot be easily controlled . from an experimental point of view the chemical potential is fixed when the system is in contact with energy and particle reservoirs . at equilibrium , the chemical potential of the system equals that of the reservoir $\mu = \mu_\mathrm{res}$ . thus by modifying the parameters of the reservoir you can control the chemical potential of the system . a typical example is when the system is a layer of molecules adsorbed in a surface . this is an open system and composition is variable --and generally unknown-- . by using a gas of the same molecules as reservoir you can fix the value of the chemical potential of the open system . the chemical potential $\mu_\mathrm{res}$ of the gas can be obtained from evaluating the fundamental equation of the gas or , if this is not available , from integrating the gibbs duhem relation if the equation of state of the gas is known .
charging and discharging a capacitor periodically surely creates electromagnetic waves , much like any oscillating electromagnetic system . the frequency of these electromagnetic waves is equal to the frequency at which the capacitors get charged and discharged . that means that if you have just dc , the frequency is de facto zero and the resulting electromagnetic waves will be pretty invisible . for the frequency to be that of the visible light , the circuit would have to be as small as an atom . ideally , it would have to be an atom because atoms are the " circuits " that naturally emit visible light .
the poynting vector shows that the energy is not transmitted through the wires ; it is transmitted through the surrounding space , as in this picture : to simplify the picture , most of the heat in this circuit is produced by the blue light bulb on the right side . the energy flows from the battery " through the air " to the light bulb . the energy flows are given by the white arrows in red ellipses . the electric field is not just parallel to the wire ; because the current ( charges ) must ultimately get back somewhere in space , the electric field goes " mostly " from a piece of wire to another piece of wire which is oriented oppositely . on the picture above , the electric field is given by the thin red arrows ( from the top to the bottom ) . similarly , the thin green lines are magnetic field lines ( around the wires , as you probably expected ) . it was taken from this page : http://www.furryelephant.com/content/electricity/visualizing-electric-current/surface-charges-poynting-vector/ it is important to realize that many of the flows indicated by the poynting vector are partially fictitious . in fact , you can have just a static electric field induced by an electric charge at point $x_{\rm el}$ on top of a static magnetic field from an end point of a long bar magnet located at $x_{\rm mg}$ . and the poynting vector will tell you that energy is running in loops around the axis connecting $x_{\rm el}$ with $x_{\rm mg}$ , in the surrounding vacuum . this is not a problem . one may also define the poynting vector ( and the whole stress-energy tensor ) differently so that the flows will be different . however , it is equally important to realize that the energy is locally conserved with the poynting vector being the flux . in particular , in the vacuum , the energy just " flows " through the vacuum and whenever the poynting lines gets denser , they must become longer , and vice versa . there are no sinks or sources of energy in the vacuum . this conservation law $$ \frac{\partial \rho_{\rm energy}}{\partial t} + {\rm div}\ , \ , \vec s =0 $$ can be proved by multiplying maxwell 's equations by fields and combining the equations appropriately . in the wire , there is an extra term from the heat creation in the conductor etc . again , in these situations , one should not overreact . one should not imagine energy as " some kind of liquid " that pushes all things . energy is just an abstract quantity that is conserved . when it just runs in some loops in the vacuum , it is not a problem . whether such an energy flow does some work on charges etc . depends on the detailed equations , the lorentz force , maxwell 's equations , and so on : one should not try to guess such influences from the flow of energy only .
that really depends on how you are going to deflect the motion . this is the best case : you apply a radial force . for instance , you could attach a fixed string to your mass , effectively making it a pendulum ( if that helps your imagination ) . what is important is that the mass will be moved in a circular path , because the taut string provides a fixed radius . the radial force will never do any work ( since force and movement are always perpendicular to each other ) . if you release the string after the 90° turn has been completed , you will not have put in any work into the system . it could be much worse though ( and will in reality be somewhat worse ) . for instance , you could also slow the mass to rest and then accelerate it to the terminal velocity in the new direction . this would require you to put in an amount of work equal two twice $e_{kin}=mv^2/2$ . given your quantities , that would be $w=1\ , \text{j}$ . but in theory , your initial energy is the same as the final energy , because kinetic energy is a scalar quantity and does not depend on the direction of travel . therefore , if you do it cleverly , you do not need any energy ( or work ) at all .
analyzing one moving clock from the perspective of one stationary person will be inadequate to derive special relativity from . with just that set-up , you are not actually using the key fact that the speed of light is the same for all observers – all you are actually using is just the fact that the speed of light is finite . with just taking into account that the speed of light is finite , all you will arrive at is the non-relativistic doppler effect , which is different from time dilation .
an energy eigenstate is just an eigenstate of the hamiltonian . so , given a particular hamiltonian operator $h$ , the energy eigenstates $\lvert n\rangle$ satisfy $$h\lvert n\rangle = e_n\lvert n\rangle$$ where $e_n$ is just a number . the reason energy eigenstates are useful is that according to the schroedinger equation , they remain unchanged ( except for a phase factor ) over time . suppose $\lvert\psi ( 0 ) \rangle$ is the initial state of some system with a hamiltonian $h$ . if $\lvert\psi ( 0 ) \rangle$ is the $n$th eigenstate of $h$ , namely if $\lvert\psi ( 0 ) \rangle = \lvert n\rangle$ , the system 's state at a later time $t$ will be $$\lvert\psi ( t ) \rangle = e^{ie_nt}\lvert n\rangle = e^{ie_nt}\lvert \psi ( 0 ) \rangle$$ and since the schroedinger equation is linear , if the initial state is a linear combination of energy eigenstates , $\lvert\psi ( 0 ) \rangle = \sum_n \alpha_n\lvert n\rangle$ , the same holds for each of the eigenstates in the sum . essentially you can distribute the time evolution over the sum . accordingly , this lets you easily write down an expression for the state of the system at time $t$: $$\lvert\psi ( t ) \rangle = \sum_n\alpha_n e^{ie_nt}\lvert n\rangle$$ so if you can express the initial state as a sum of coefficients times the energy eigenstates , it makes it pretty trivial to express the state at any later time . that is where the inner products come in . it is often the case that eigenstates of $h$ form a complete orthonormal basis , and when you have an orthonormal basis , the way you decompose an arbitrary state into that basis is by taking inner products , $\alpha_n = \langle n\vert\phi ( 0 ) \rangle$ . none of this has anything to do with the uncertainty principle .
no , from experimentation point of view ( check emilio 's comments ) . yes , from the " practical"-theoretical point of view . no , from the rigorous theoretical point of view . ( edited paragraph ) . let 's assume a planet with mass $m_p$ , and a object in which will make a slingshot , of mass $m$ . planet has speed $v_p$ . mass $m$ has speed $v$ . after slingshot , mass $m$ has speed $2v_p + v$ . linear momentum must conserve . so , initially $p_i$ and after slingshot $p_f$ . $$ p_i = mv + m_p v_p = m ( v + 2v_p ) + m_p v_p ' = p_f $$ where $v_p'$ is the final velocity of the planet . we can isolate it : $$ v_p ' = \frac{mv + m_p v_p - m ( v + 2v_p ) }{m_p} = \frac{m_p v_p - 2mv_p}{m_p} = v_p - \frac{2m}{m_p}v_p $$ therefore , the variation of planet speed $\delta v_p = v_p ' - v_p$ is : $$ \delta v_p = -\frac{2m}{m_p}v_p $$ now we can throw up $n$ times a mass $m$ object to peform slingshot , which means , $n$ slingshots . if $m_p &gt ; &gt ; m$ ( which is of course true since you will not slingshot a planet in another planet ) it is valid the approximation such that $\delta v_p \approx dv_p$ and then we integrate over $n$ slingshots . $$ \frac{dv_p}{v_p} = -\frac{2m}{m_p}dn \quad\longrightarrow\quad \int \frac{dv_p}{v_p}dn = -\int \frac{2m}{m_p}dn $$ $$ \ln v_p = -\frac{2m}{m_p}n + c $$ we can find out the integral constant such that be in function of $v_0$ , where $v_0$ is the initial speed of the planet . then we get a function $v_p ( n ) $ , which means , velocity of the planet is dependent from the amount $n$ of slingshots performed . we end up with : $$ v_p ( n ) = v_0 \exp\left ( -\frac{2m}{m_p}n\right ) $$ where $n$ is the number of slingshots . so , you can see each slingshot the planet speed drops exponentially , and therefore , rigorously never reaches zero . but , it will be close enough to zero after a lot slingshots . a nice observation , from here we can notice that after a slingshot , the planet speed is independent from the initial speed of mass $m$ object . only depends on mass $m$ of the object .
how should i notate my bounds of integration ? i guess you mean for this integral , passing to gauge pressure \begin{align} \int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_s ( x , t ) +b ) \ dp_s ( x , t ) . \end{align} since $p_s=p_g+p_a$ , all you need to do is that change of variables . besides , $dp_s=dp_g$ because $p_a$ is a constant . also , there is no need to say $dp_s ( x , t ) $ , since these are dummy variables . \begin{align} \int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_s+b ) \ dp_s and =\int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_g+p_a+b ) dp_s\\ and =\int_{p_s ( o , t ) }^{p_s ( l , t ) } ( p_g+p_a ) dp_s+ ( p_{s , l}-p_{s , o} ) b\\ and =\int_{p_s ( o , t ) }^{p_s ( l , t ) }p_g dp_s+ ( p_{s , l}-p_{s , o} ) ( b+p_a ) \\ and =\int_{p_g ( o , t ) }^{0}p_g dp_g-p_g ( b+p_a ) \\ and =-\frac{p_g^2}{2}-p_g ( b+p_a ) =-\frac{p_g}{2} ( p_g+2p_a+2b ) , \\ \end{align} where in the last line $p_g$ is $p_{g , o} ( t ) $ . note that $p_{s , l}-p_{s , o}=-p_g$ . how does dividing atmospheres by absolute pressure give you gauge pressure ? here i think you are confused . psi , psig , atm , and pa are all units of pressure . that is why you can add gauge pressure to atmospheric pressure in the first place . the problem is that pressures expressed in these units are not proportional since they have different zero , which is the same problem when working between the different temperature scales ( farenheit , celsius , and kelvin ) . my advice is to make every algebraic and mathematical manipulation without units , or using a single system like si , and only translate to more practical units at the end .
for the reasons that you already mentioned in the question , it would not be possible to modify stereoscopic depth without adding artifacts . so instead i would argue if the depth is actually increased . it probably is not . we have gotten used to watching monoscopic ( regular ) photographs with both our eyes . theoretically the perceived scale of the scenes on those photographs should be huge . but we have gotten used to it . so , the depth effect of an stereoscopic image that is captured in the range of monoscopic ( 0 camera distance ) up to a regular human eye-distance will be perceived natural . a small amount of depth is already obvious to the brain . if you would increase the camera distance beyond human eye distance , then your brain would get an unusual stimulus hence it might conclude that the scene is a miniature . also note that the viewing angle ( field of view ) of a photograph or mobile screen is also significantly smaller than the captured angle . not that it would compensate for reduced depth , but just another example of our tolerance in accepting the illusion of a photograph . if you are shooting images for a more immersive viewing experience ( eg . 3d cinema ) , then the tolerances are probably much smaller .
when you mix colors using watercolors , then they mix as " subtractive colors " . however , light itself mixes as " additive colors " . even though it might seem strange why the inherently same thing works so differently , it makes sense if you think about watercolors , etc . as absorbing everything but that specific color .
the classical equations of motion are not affected by changing the lagrangian $$l \qquad \longrightarrow \qquad l&#39 ; = l+ \frac{df}{dt}$$ by a total time derivative . put $f= -q_1 q_2$ . then $$l&#39 ; = -\frac{1}{2} ( q_1^2 + q_2^2 ) . $$ this lagrangian $l&#39 ; $ does not contain time derivatives , and thus there are no dynamics . the classical equations of motion are $$ q_1=0 \qquad \mathrm{and}\qquad q_2=0 , $$ in conflict with what is said in the original question formulation ( v1 ) .
just compare the resolution of the two : prism depending on n , there is no good material n> 1.7 ( besides diamond ) depending on base length if you use a equilateral triangle have to use more than one to overcome this prism absorb light , you have got scattering ( stray light ) too now a grating : optimize it for your wavelength choose lines per milimeter resolution depending on the number of lines that are illuminated compact device just transmission gratings have got absorption , you can do your measurement in reflection with a blazed grating design your blazed grating to get the most light in e.g. 2nd order quantitatively prism : $\frac{\lambda}{\delta \lambda} = t \frac{dn}{d\lambda}$ grating : $\frac{\lambda}{\delta \lambda} = \frac{zd}{g}=zn$ where t is your base length , z . . . order of spectrum , g . . . grating constant , d . . . entrance beam diameter , n . . . number of illuminated lines so just use a grating , nowadays they can be fabricated in excellent quality . on my university learning the pros of a diffraction grating is part of the 1st year laboratory exercises .
we are talking just newtonian gravity here . you should probably know that the orbit in this case is just a conic section . depending on the initial angular momentum and energy you can compute the closest point to the star on the orbit . the object will fall into star if and only if this point is inside the star . now , supposing you already determined that the object will fall into the star , you can solve for the point of crossing star 's surface ( this is just geometry , intersection of the conic section and the circle ) and then solving for the time of arrival into that point . all of this is simplified by noting that you only need to know the $r$-coordinate which is simply the radius of the star . so this all boils down to finding a solution to your equation . you can do that just by taking a square root , separating variables and computing the integral . that will give you dependence of $t = t ( r ) $ . so then just plug in the radius of the star and you are done . this means that the problem is reduced to finding integrals . try some software for that ( because the integral does not look easy ) like wolfram mathematica integrator . if you have any problems with this i suggest you ask what to do next at math . se to get better answers .
general approach first recall that euler-lagrange equations are conditions for the vanishing of the variation of action $s$ . for a scalar field $\phi$ with lagrangian density $\mathcal l$ on some open subset u we have $$s [ \phi ] = \int_u {\mathcal l} ( \phi ( x ) , \partial^{\mu}\phi ( x ) ) {\rm d}^4 x$$ consider a variation of the field in direction $\chi$ and compute $$s [ \phi + \varepsilon \chi ] = \int_m {\mathcal l} ( \phi ( x ) + \varepsilon \chi ( x ) , \partial^{\mu} ( \phi ( x ) + \varepsilon \chi ( x ) ) ) {\rm d}^4 x$$ then using taylor expansion $$s [ \phi + \varepsilon \chi ] - s [ \phi ] = \int_u \left [ \varepsilon \chi ( x ) {\partial{\mathcal l} \over \partial \phi} ( \phi ( x ) , \partial^{\mu}\phi ( x ) ) + \varepsilon ( \partial^{\mu} \chi ( x ) ) {\partial{\mathcal l} \over \partial ( \partial^{\mu} \phi ) } ( \phi ( x ) , \partial^{\mu}\phi ( x ) ) + o ( \varepsilon^2 ) \right ] {\rm d}^4 x$$ using integration by parts on the second term ( assuming $\chi$ vanishes on $\partial u$ ) , diving by $\varepsilon$ on both sides and letting $\varepsilon \to 0$ this becomes a variation in direction $\chi$ $$\delta s [ \phi ] [ \chi ] = \int_u \chi ( x ) \left [ {\partial{\mathcal l} \over \partial \phi} ( \phi ( x ) , \partial^{\mu}\phi ( x ) ) - \partial^{\mu}\left ( {\partial{\mathcal l} \over \partial ( \partial^{\mu} \phi ) } ( \phi ( x ) , \partial^{\mu}\phi ( x ) ) \right ) \right ] {\rm d}^4 x$$ by requiring variations in all directions equal zero we obtain $$ {\partial{\mathcal l} \over \partial \phi} - \partial^{\mu}\left ( {\partial{\mathcal l} \over \partial ( \partial^{\mu} \phi ) }\right ) = 0 $$ ( arguments the same as always , so omitted ) . massive scalar field example consider lagrangian density $${\mathcal l} = {1 \over 2}\eta_{\mu \nu} \partial^{\mu} \phi \partial^{\nu} \phi - {1 \over 2} m^2 \phi^2 $$ by using the e-l equations we have just derived we obtain klein-gordon equation . $$ \eta_{\mu \nu} \partial^{\mu} \partial^{\nu} \phi + m^2 \phi = \square \phi + m^2 \phi = 0$$
in the case of a real , scalar field , we start from the action $$s=\int d^4x\ \eta^{\mu\nu}\partial_\mu\phi\partial_\nu\phi+m^2\phi^2$$ the equation of motion is the klein-gordon equation $$ ( -\eta^{\mu\nu}\partial_\mu\partial^\nu+m^2 ) \phi=\ddot{\phi}-\nabla^2 \phi+m^2\phi=0$$ now , we introduce the fourier modes $\phi_k$: $$\phi=\int \frac{d^3k}{ ( 2\pi ) ^3}e^{i\vec{k}\cdot\vec{x}}\phi_k $$ in terms of the fourier modes , the equation of motion becomes $$\ddot{\phi}_k +\omega^2\phi_k=0 , \hspace{2cm}\omega\equiv \sqrt{k^2+m^2}$$ as you see , this is just an independent harmonic oscillator equation for each value of $k$ . this is what it means to say that the different fourier modes are independent . when we introduce an interaction term proportional to $\phi^n$ where $n\geq 3$ ( e . g . the canonical $\lambda \phi^4$ ) , we will have terms proportional to $\phi^{n-1}$ ( $4\lambda\phi^3$ in my example ) in the equation of motion . when going to fourier space , these powers of $\phi$ all have ' their own label ' , so we obtain a term something like $\phi_{k_1}\phi_{k_2}\dots\phi_{k_{n-1}}$ . as you see , the equation of motion no longer depends only on a single fourier mode ! the fourier modes are now coupled .
it does not sound exactly like a chord , but this type of technique was widely used in the 8-bit and 16-bit computer game eras , when the number of sound channels available was limited . it has a very distinctive retro video game sound , but the ear is able to identify the chord that it is supposed to be . here is a youtube video explaining how to achieve the effect using synthesis software - your situation is different , but you can probably adapt some of the advice . you can experiment with the speed of modulation , but the guy in the video sets it to around 30 hz , which sounds good . actually , i think you should be able to achieve the effect of a chord through a different method . to play a note at a frequency of $f_1$ at the same time as a note with frequency $f_2$ , you just need to time the sparks so that there are sparks at times $0 , \frac{1}{f_1} , \frac{2}{f_1} , \frac{3}{f_1} , \dots$ and also at times $\frac{1}{f_2} , \frac{2}{f_2} , \frac{3}{f_2} , \dots$ . this will sound to the ear exactly like the two notes being played at once , because that is essentially what it is .
if m1 has a positive velocity and m2 has a negative velocity , you get (m1.velocity - m2.velocity) = (positive - negative) = positive . on the other hand if the signs are switched , you will get a negative result because the particles are moving away from each other .
comment to the question ( v1 ) : it seems op is conflating , on one hand , a gauge transformation $$ \tilde{a}_{\mu} ~=~ a_{\mu} +d_{\mu}\lambda $$ with , on the other hand , a gauge-fixing condition , i.e. choosing a gauge , such e.g. , lorenz gauge , coulomb gauge , axial gauge , temporal gauge , etc . a gauge transformation can e.g. go between two gauge-fixing conditions . more generally , gauge transformations run along gauge orbits . ideally a gauge-fixing condition intersects all gauge orbits exactly once . mathematically , depending on the topology of spacetime , it is often a non-trivial issue whether such a gauge-fixing condition is globally well-defined and uniquely specifies the gauge-field , cf . e.g. the gribov problem . existence and uniqueness of solutions to gauge-fixing conditions is the topic of several phys . se posts , see e.g. this and this phys . se posts .
the thing to understand is how we tag neutrinos for flavor in the first place . neutrinos are created and destroyed in reactions that also involve a charged lepton ( electron , muon or tau ) . at vertex level these are $$ w^\pm \to l^\pm + \nu_l $$ and various rotations . the flavor of a neutrino is defined as coincident with that of the charged lepton produced . ( i have neglected $z \to \nu + \bar{\nu}$ reactions here , but these are far off-shell at the energies available in the core of the sun , so they do not contribute . ) the reactions in the sun are fusion reaction that do not have enough excess energy to create a heavy lepton , so the neutrinos must ( by definition ) be electron-type . this rule for neutrino flavors can be tested at accelerator where we can generate beams of known neutrino content ( because we can count the number and type of hadrons decaying to charged leptons ) , and when the beam is directed onto a detector very near-by the number of charged-current neutrino interactions of each flavor that we detect agrees with the flavor content of the beam . but there is more , we can predict the energy spectrum of the solar neutrinos assuming that they are electron-type , and that is the spectrum that we detect . by conservation of energy neutrinos created in concert with heavy leptons ( that is , other flavors ) would have a different energy spectrum . now , the evidence that the theoretical oscillation framework we use is correct is pretty diverse , but some of the very best " one plot " evidence comes from kamland , where we plot the flux of electron-type anti-neutrinos from japanese power reactors as a function of observed energy and compare to the expected $\sin \left ( \frac{l}{e} \right ) $ behavior ( appropriately convoluted to account for the many different distances to reactors ) . ( image from http://kamland.lbl.gov/ ) . full disclosure : i was a member of the kamland collaboration for about 4 years and am named as an author on the paper from which that figure is drawn .
i give the answer with the general method even though a much straightforward way would be to guess the result because it is simple . the unit of $h$ is the inverse of time denoted by $ [ \mathrm t^{-1} ] $ . the dimension of a temperature is denoted by $ [ \theta ] $ . to find the numerical value of $t$ in kelvin , one should find a combination of $c : [ \mathrm{lt^{-1}} ] $ , $\hbar= [ \mathrm{ml^2t^{-1}} ] $ , $\mathcal g : [ \mathrm{m^{-1}l^3t^{-2}} ] $ and $k_{\mathrm b}: [ \mathrm{ml^2t^{-2}\theta^{-1}} ] $ ( $ [ \mathrm l ] $ and $ [ \mathrm m ] $ represent length and mass respectively ) such that $$ c^x \ ; \hbar^y \ ; \mathcal g ^z\ ; k_{\mathrm b}^u\times h$$ has the dimension of a temperature ( $x$ , $y$ , $z$ and $u$ are unknown ) . this gives the system $$\left\{\begin{array}{rcc} y-z+u and =0 and \quad [ \mathrm m ] \\ x+2y+3z+2u and =0 and \quad [ \mathrm l ] \\ -x-y-2z-2u and =1 and \quad [ \mathrm t ] \\ -u and =1 and \quad [ \theta ] \end{array}\right . $$ the solution is $$\left\{\begin{array}{cl} x and =0\\ y and =1\\ z and =0\\ u and =-1 \end{array}\right . $$ we obtain thus $$t=\frac{\hbar}{2\pi k_{\mathrm b}}h . $$ the value of $h$ is $h=67.8\ , \mathrm{km . s^{-1} . mpc^{-1}}=2.194\times 10^{-18}\ , \mathrm{m . s^{-1}}$ . we find a temperature of $t=2.67\times10^{-30}\ , \mathrm k$ .
after stating the solution , i will try to give some physical insights to the best of my knowledge and some more references . the dimension of the required state space is given by the verlinde formula , having the following form for a general compact semisimple lie group $g$ on a riemann surface with genus $g$ corresponding to the level $k$: $$ \mathrm{dim} v_{g , k} = ( c ( k+h ) ^r ) ^{g-1} \sum_{\lambda \in \lambda_k}\prod_{\alpha \in \delta} ( 1-e^{i\frac{\alpha . ( \lambda+\rho ) }{k+h}} ) ^{ ( 1-g ) }$$ ( please see blau and thompson equation 1.2 . ) . here , $c$ is the order of the center , $h$ is dual coxeter invariant , $\rho$ is half the sum of the positive roots , and $r$ is the rank of $g$ . $g$ is the genus , $\delta$ is the set of roots and $\lambda_k$ is the set of integrable highest weights of the kac-moody algebra $g_k$ . for the torus ( $g=1$ ) , this formula simplifies to : $$ \mathrm{dim} v_{\mathrm{torus} , k} = \# \lambda_k $$ i.e. , the dimension is equal to the number of integrable highest weights of the kac-moody algebra $g_k$ . the integrable highest weights of a level-$k$ kac-moody algebra are given by the following constraints : $$ \lambda - \mathrm{dominant} , 0 \leq \sum_{i=1}^r \frac{2 \lambda . \alpha^{ ( i ) }}{ \alpha^{ ( i ) } . \alpha^{ ( i ) }}\leq k$$ where $ \alpha^{ ( i ) }$ are the simple roots , please see , for example , the following review by fuchs on kac-moody algebras . ( my favorite reference for the representation theory of kac-moody algebras is the goddard and olive review which seems not available on line ) for example for $su ( 3 ) _k$ whose dominant weights are $2$-tuples of nonnegative numbers $ ( n_1 , n_2 ) $ , the above condition reduces to : $$\mathrm{dim} v^{su ( 3 ) }_{\mathrm{torus} , k} = \# ( n_1\geq 0 , n_2\geq 0 , 0\leq n_1 + n_2 \leq k ) = \frac{ ( k+1 ) ( k+2 ) }{2}$$ to perform the computations for the more general cases , one can use the seminal review by slansky . the verlinde formula was discovered before the chern-simons theory came into the world . originally it is the dimension of the space of conformal blocks for the wzw model . this formula has been derived in a large variety of ways , please , see footnote 26 in the fuchs review . it is still an active research topic , please see for example a new derivation in this recent article by gukov . the chern-simons theory may be the most sophisticated example in which the dirac quantization postulates can be carried out in spirit . ( more precisely their generalization in geometric quantization ) . i mean starting from a phase space and utilizing a specified set of rules to associate a hilbert space to it . in the case of the chern-simons theory , the phase space is the set of solutions of the classical equations of motion . the classical equations of motion require the field strength to vanish , in other words the connection to be flat . this phase space ( the moduli space of flat connections ) is finite dimensional , it has a kähler structure and it can geometrically quantized as a kähler manifold , just like the case of the harmonic oscillator . thus the problem can be reduced in principle to a problem in quantum mechanics . the case of the torus is the easiest because everything can be carried out explicitly in the abelian and the non-abelian case , please see the following explicit construction by bos and nair , ( a more concise treatment appears in dunne 's review ) . in the case of the torus , the moduli space of flat connections in the abelian case is also a torus and in the non-abelian case it is : $$\mathcal{m} = \frac{t \times t}{w}$$ where $t$ is the maximal torus of $g$ . basically , a fock quantization can be carried away , but there is a further restriction on the admissible wave functions coming from the invariance requirement under the large gauge transformations ( please see for example , the dunne 's review ) . the invariant wave functions are called non-abelian theta functions and they are just in a one to one correspondence with the kac-moody algebra integrable highest weights . ( in the abelian case , the wave functions are the jacobi theta functions ) . in the higher genus case , although the quantization program leading to the verlinde formula can be carried out in principle , few explicit results are known , please see the following article by lisa jeffrey ( and also the following lecture notes ) . the dimension of these moduli spaces is known . in addition . witten in an ingenious work computed their symplectic volumes and their cohomology ring in some cases . witten 's idea is that as in the case of a simple spin , the dimension of the hilbert space in the semiclassical limit ( $k \rightarrow \infty$ ) becomes proportional to the volume and the leading exponent of $k$ is the complex dimension of the moduli space ( please observe for example , that in the case of $su ( 3 ) $ on the torus , the leading exponent is $2$ which is the rank of $su ( 3 ) $ which is the dimension of the maximal torus $t$ ) .
the short answer is that we do not know why the world is this way . there might eventually be theories which explain this , rather than the current ones which simply take it as axiomatic . maybe these future theories will relate to what we currently call the holographic principle , for example . there is also the apparently partially related fact of the quantization of elementary phenomena , e.g. that the measured spin of an elementary particle always is measured in integer or half integer values . we also do not know why the world is this way . if we try to unify these two , the essential statistical aspect of quantum phenomena and the quantization of the phenomena themselves , the beginnings of a new theory start to emerge . see papers by tomasz paterek , borivoje dakic , caslav brukner , anton zeilinger , and others for details . http://arxiv.org/abs/0804.1423 and http://www.univie.ac.at/qfp/publications3/pdffiles/paterek_logical%20independence%20and%20quantum%20randomness.pdf beginning with zeilinger 's ( 1999 ) http://www.springerlink.com/content/jt342534x711542g/ , also online free here these papers present phenomenological ( preliminary ) theories in which logical propositions about elementary phenomena somehow can only carry 1 or a few bits of information . thanks for asking this question . it was a pleasure to find these papers .
no . when you hit the wall , the bicycle rotates around the front axis . the angular momentum l that you create for an arbitrary number of mass particles is $$l=\sigma_i ( r_i \times m_iv_i ) . $$ if you split location r=r+r_i and v=v+v_i with r and v being center of mass location and velocity , respectively , and r_i and v_i deviation from it , then it can be shown that l does not change when the center of mass does not change . so , the wood block on wheels should work ( in theory ) .
as john says , the mohs criterion is useful because it may be immediately applied . one may try to rob the two materials with any force but the magnitude of the force really does not matter because once the force exceeds a certain threshold , the materials ' atoms or molecules start to rearrange . scratches – whatever is their exact definition – will begin to develop and the force you exerted gets reduced . the point of the mohs scale is that when the materials start to get modified – develop scratches – to relieve the external pressure , it is far more likely that the softer material according to this scale is the one that will " surrender " first and get damaged by the scratches . this is no exact law . the harder material may sometimes gets damaged , too . but the quantitative difference between the amount of scratches is huge and very sensitive on the scale . for every temperature , you should in principle quantify the hardness again , from scratch , to use the word again . so materials will surely get softer near the melting point . if they start to " self-repair " because they are partly liquids , this is certainly a proof of their being less hard , not harder ! liquids would have the lowest rating on this scale . a melted piece of the material may not fit the definition of a " scratch " but it is " at least as bad " as a scratch . a material that is really without scratches has individual atoms or molecules sitting tightly at the prescribed points of the lattice or another structure . freely moving molecules of a liquid violate this rule . the extended mohs scale assigns hardness 1 to all liquids , see http://www.rockroost.com/mohs-hardness-scale-tips.shtml
firstly , you can not just assert that $v=ex$ . absolute potential is not defined here , since there is an infinite increase of potential energy when going from $x=-\infty$ to $x=+\infty$ . we can only use absolute potential when the assertion that absolute potential is $0$ at infinity holds . however , this was not really your issue here , it would have worked regardless . you basically forgot a negative sign while calculating v : $$\delta v=\color{red}{-}\int\vec e\cdot d\vec l$$ so , we use : $$\delta ke+\delta pe=0$$ and , $$\delta pe= ( -e ) \delta v= ( -e ) ( -\int\vec e\cdot d\vec l ) =ee\delta x$$ $$\frac12m0^2-\frac12mv_0^2+ee\delta x=0$$ $$\implies eex=\frac12mv_0^2\implies x=\frac{mv_0^2}{2e}$$ which solves the problem .
$$ \newcommand{\ket} [ 1 ] {|{#1}\rangle} \newcommand{\bra} [ 1 ] {\langle{#1}|} \newcommand{\braket} [ 2 ] {\langle{#1}\ , |\ , {#2}\rangle} \newcommand{\bracket} [ 3 ] {\langle{#1}\ , |\ , {#2}\ , |\ , {#3}\rangle} $$ well , i think by specifying mass $m$ and charge $q$ you simply define your system , a single electron , and then its complete non-relativistic description is indeed given by a wave-function $\psi ( \vec{r} , m_s , t ) $ as you mentioned . wave-function of the form $\psi ( \vec{r} , m_s , t ) $ , also known as spin-orbital , arises as follows . there is a postulate of qm which says that the state space $h$ of a system , composed of $n$ subsystems each associated with its own space $h_{i}$ , is the tensor product of this spaces , \begin{equation} h = h_{1} \otimes h_{1} \otimes \dotsm \otimes h_{n} \ , . \end{equation} the notion of a system composed of subsystems in the postulate above is not to be take literally : the state space can be written as a tensor product of state spaces which are not even associated with real physical systems . for instance , we can subdivide electron with position in space and spin system into 2 subsystems : electron only with position in space and electron only with spin . taking spin into account the state space for a particle is the tensor product of \begin{equation} h = h_{\text{space}} \otimes h_{\text{spin}} \ , , \end{equation} where $h_{\text{space}}$ is a state space spanned by eigenvectors $\ket{r}$ of the position operator \begin{equation} \widehat{\vec{r}} \ket{\vec{r}} = \vec{r} \ket{\vec{r}} \ , , \end{equation} and $h_{\text{spin}}$ is a state space spanned by eigenvectors $\ket{m_{s}}$ of a spin component operator conventionally chosen to be $\widehat{s}_{z}$ \begin{equation} \widehat{s}_{z} \ket{m_{s}} = m_{s} \hbar \ket{m_{s}} \ , . \end{equation} the resulting space $h$ is spanned then by $\ket{\vec{r} , m_{s}} = \ket{\vec{r}} \otimes \ket{m_{s}}$ by the property of a tensor product space and the expansion of a state vector $\ket{\psi} \in h$ then takes the following form \begin{equation} \ket{\psi ( t ) } = \sum_{m_s=-s}^{s} \ , \int\limits_{\mathrm{r}^{3n}} \psi_{m_s} ( \vec{r} , t ) \ket{\vec{r} , m_s} \mathrm{d}^3 \vec{r} \ , , \end{equation} where \begin{equation} \psi_{m_s} ( \vec{r} , t ) = \braket{\vec{r} , m_s}{\psi} \ , . \end{equation} it should be clear now that coefficients in the expansion of a state vector $\ket{\psi}$ over the basis $\ket{\vec{r} , m_{s}}$ are given by $2s + 1$ functions $\psi_{m_s} ( \vec{r} , t ) $ and that all them are required for the complete description of a state . thus , for instance , the state of an electron can be represented by a two-row vector \begin{equation} \ket{\psi ( t ) } \longleftrightarrow \begin{pmatrix} \psi_{+1/2} ( \vec{r} , t ) \\ \psi_{-1/2} ( \vec{r} , t ) \end{pmatrix} \ , , \end{equation} known as the two-component wavefunction . alternatively , $\psi_{+1/2} ( \vec{r} , t ) $ and $\psi_{-1/2} ( \vec{r} , t ) $ can be combined into single piecewise function $\psi ( \vec{r} , m_s , t ) $ which can also be used as the description of the state of an electron \begin{equation} \ket{\psi ( t ) } \longleftrightarrow \psi ( \vec{r} , m_s , t ) = \begin{cases} \psi_{+1/2} ( \vec{r} , t ) , and m_s = +1/2 \\ \psi_{-1/2} ( \vec{r} , t ) , and m_s = -1/2 \end{cases} \ , . \end{equation} another equivalent representation of the same idea is one which is usually used in quantum chemistry , where the spin dependence is separated out by introducing the " spin up " and " spin down " spin functions \begin{equation} \alpha ( m_s ) = \begin{cases} 1 , and m_s = +1/2 \\ 0 , and m_s = -1/2 \end{cases} \ , , \quad \quad \quad \beta ( m_s ) = \begin{cases} 0 , and m_s = +1/2 \\ 1 , and m_s = -1/2 \end{cases} \ , , \end{equation} and writing down $\psi ( \vec{r} , m_s , t ) $ in the following way \begin{equation} \psi ( \vec{r} , m_s , t ) = \psi_{+1/2} ( \vec{r} , t ) \alpha ( m_s ) + \psi_{-1/2} ( \vec{r} , t ) \beta ( m_s ) \ , . \end{equation} and since spin is relativistic phenomenon , such description is , in a sense , already relativistic , though , partly . fully relativistic description is different . the wave function is four-component , rather than two-component , and although , i could not tell you how it arises , the role of two additional components is to describe the " spin-up " and " spin-down " state of associated positron .
the short answer is that physicists/astronomers want to avoid fine tuning wherever possible . creating a universe where the temperature everywhere was essentially the same requires exceptional fine tuning . creating a universe where the temperatures were random in different parts of space and had an opportunity to come in thermal equilibrium before going out of causal contact ( as a result of inflation ) is more natural . if this was all that inflation solved , it maybe would not be considered as likely to have occurred as it is . it also solves another fine tuning problem though , in that the universe is very nearly flat ( if not exactly so ) , and inflation naturally would produce such a universe as well . edit : i should also mention that inflation naturally explains the absence of observations of magnetic monopoles as well . this problem was actually the primary motivation of alan guth , who first developed the idea of inflation . this issue is not so much one of fine tuning though , unlike the flatness problem and the homogeneity problem .
$\def\d{{\ , \prime}}$ $\def\nr{{\mathbb{r}}}$ $\def\l{\left}\def\r{\right}$ $\def\vf{{\vec{f}}}$ $\def\vc{{\vec{c}}}$ $\def\vr{{\vec{r}}}$ $\def\ve{{\vec{e}}}$ $\def\vf{{\vec{f}}}$ $\def\rmf{{\mathrm{f}}}$ $\def\rmc{{\mathrm{c}}}$ $\def\rmn{{\mathrm{n}}}$ $\def\rmt{{\mathrm{t}}}$ $\def\nn{{\mathbb{n}}}$ what your teacher tried to explain is most easily described with the help of the principle of virtual work . the sphere poses a constraint to the chain . it defines the shape of the chain . the chain itself implicitly represents also a constraint ( it does not lengthen under pull forces ) . the sphere does not move therefore it does not exert any work on the chain ( we neglect friction here ) . let the curve $\vf:\nr\rightarrow\nr^3$ describe the shape of the constraint in dependence on the curve length . we have extended the domain of this function to full $\nr$ to avoid problems when the chain moves . for our example we can use $$ \vf ( s ) = \begin{cases} ( r , s ) and @ s &lt ; 0\\ \l ( r\cos\l ( \frac sr\r ) , r\sin\l ( \frac sr\r ) \r ) and @ s\in\l [ 0 , \frac\pi2r\r ] \\ \l ( \frac{\pi}2r-s , r\r ) and @ s &gt ; \frac\pi2r \end{cases} $$ note , that we do not really need the actual representation . it is just added that we know about what we speak . the curve $\vf$ is represented with red color in the following picture together with its parameterization through $s$ . for simplicity we do not care about the bending of the chain if it slides off the sphere on the thread side just as if it were also supported by some shelf there . if $s$ is the positioning parameter of the chain in space and $\xi\in [ 0 , l ] $ is the actual position of the chain element on the chain the chain can be described by $$ \vr ( s , \xi ) = \vf ( s-\xi ) . $$ for $\xi=0$ the point $\vr ( s , 0 ) =\vf ( s ) $ is the beginning of the chain . for $\xi=l$ the point $\vr ( s , l ) =\vf ( s-l ) $ is the end of the chain . values of $\xi$ in between of $0$ and $l$ address other points $\vr ( s , \xi ) $ on the chain . if we enlarge $s$ the chain glides up along the curve $\vf$ if we reduce $s$ the chain glides down along the curve $\vf$ . to see this you can follow the chain beginning at $\vr ( s , 0 ) =\vf ( s ) $ . the placement of the chain from your picture results for $s=\frac\pi2 r$ . excursion into mechanics with constraints ( principle of work ) : at first consider a single particle constraint to some curve . in reality the particle could be a very small and heavy locomotive on a curved rail track in the mountains ( up and down ) . the curve is given by a smooth function $\vf:\nr\rightarrow\nr^3$ in dependence of some parameter $s$ ( e . g . the path length in one direction starting from some predefined point as for our above example ) . the particle is affected by some free force $\vf_\rmf$ which would also exists if the constraint would not be there . furthermore , the particle is affected by the constraint force $\vf_\rmc$ which keeps the particle on the curve . the constraint force counteracts the component $\vf_\rmn$ of the free force the normal direction of the curve . the resulting force is the tangent component $\vf_\rmt$ of the free force . the particle is in equilibrium if the resulting tangent force $\vf_\rmt=\vf_\rmf+\vf_\rmc$ is zero . the nice thing of the principle of virtual work is that we do not need to calculate explicitly the normal force but we can restrict the equations to the tangent direction . the tangent direction of the curve at some location $s$ is the derivative $\vf\ , ' ( s ) $ of $\vf ( s ) $ . we can scale the tangent direction by some parameter $\delta s$ . in this way we get a straight line parameterized by $\delta s$ which is tangent to the curve at the point $s$ . note , that close to $s$ the curve and the tangent line look very similar ( inclusively the scale division for $s$ and $\delta s$ ) . for each given value $\delta s$ the vector $\delta\vr:=\vf\ , ' ( s ) \cdot \delta s$ is a tangent vector on the curve at $s$ . the tangent quantities such as $\delta \vr$ and $\delta s$ are often denoted as virtual in physics . and if physicans talk about small deviations $\delta s$ they actually mean such tangent quantities . the force balance in the equilibrium point is $$ \vec0 = \vf_\rmf + \vf_\rmc $$ if we multiply this equation by a tangent vector at the location $s$ of the particle we get $$ 0 = \delta\vr\cdot ( \vf_\rmf + \vf_\rmc ) = \delta\vr\cdot\vf_\rmf + \underbrace{\delta\vr\cdot\vf_\rmc}_{=0} $$ whereby the second term $\delta\vr\cdot\vf_\rmc$ is zero because the tangent vector $\delta\vr$ and the constraint force $\vf_\rmc$ are perpendicular to each other . so our resulting equilibrium equation is $$ 0=\delta\vr\cdot \vf_\rmf = \delta s \vf\ , ' ( s ) \cdot \vf_\rmf . $$ we are only considering here the case of one degree of freedom $s$ where we can just set $\delta s=1$ . ( note by the way , that if we had more parameters , e.g. , $ s= ( s_1 , s_2 ) $ , we would have to test for all possible tangent directions , e.g. $\delta s= ( 1,0 ) $ and $\delta s= ( 0,1 ) $ . ) for $\delta s=1$ our equilibrium equation reads as $$ 0= \vf\ , ' ( s ) \cdot \vf_\rmf . $$ which is one scalar equation which can be solved for the scalar curve parameter $s$ . as i already mentioned above . the nice thing about our first application of the principle of virtual displacement is that we have eliminated the unknown constraint forces in the equation to be solved . now we consider a system of $n$ coupled particles with one degree of freedom $s$ . all particles are constraint to $n$ curves $\vr_i = \vf_i ( s ) $ with $i=1 , \ldots , n$ . now , there must be additional internal constraint forces $\vf_{ij}$ between the particles else they would run on the curves $\vf_i$ independently and their positions could not be parameterized by the same parameter $s$ . thereby , $\vf_{ij}$ is the force exerted by particle $j$ on particle $i$ . because of actio=reactio we have $\vf_{ij} = -\vf_{ji}$ . therefore we only need the forces $\vf_{ij}$ with $i&gt ; j$ in our formulas . inclusively the internal constraint forces the equilibrium force balance for the $i$-th particle reads as $$ 0=\sum_{j=1}^{i-1} \vf_{ij} - \sum_{j=i+1}^n \vf_{ji} + \vf_{\rmc i} + \vf_{\rmf i} . $$ the $\vf_{\rmc i}$ are the constraint forces from the environment and $\vf_{\rmf i}$ are the free forces on the $i$-th particle as for our one-particle problem . what are examples of internal constraint and how do the internal constraints work ? a simple internal constraint would be a light staff with a heavy particle attached to each end . to show that this is not the only kind of internal constraint we can extend this example . we do not attach the particles directly but put them on two light wheels which are attached rotary to the ends of the staff such that they work together like a mechanism with one degree of freedom such that the motion of the particles is coupled and can be expressed by one parameter $s$ . in a difference coordinate system $\vr_j-\vr_i$ the point $\vr_j$ is constrained to the curve $\vf_j ( s ) -\vf_i ( s ) $ and we have the same situation as for the constrained single particle . the constraint force $\vf_{ji}$ is perpendicular to the tangent of the difference curve $$ ( \delta\vr_j-\delta\vr_i ) \perp \vf_{ji}\quad\rightarrow\quad ( \delta\vr_j-\delta\vr_i ) \cdot \vf_{ji} = 0 . $$ from the single-particle example we have some hope to eliminate the constraint forces by exploiting this orthogonality . but , in the balance formula $0=\sum_{j=1}^{i-1} \vf_{ij} - \sum_{j=i+1}^n \vf_{ji} + \vf_{\rmc i} + \vf_{\rmf i}$ for the force on the $i$-th particle all internal constraint forces are mixed in . furthermore , since we only have one parameter $s$ to determine the position of all particles we only need one formula . for that reason let us try the sum over all the scalar products : $$ 0 = \sum_{i=1}^n \delta \vr_i \cdot \l ( \sum_{j=1}^{i-1} \vf_{ij}- \sum_{j=i+1}^n \vf_{ji} + \vf_{\rmc i} + \vf_{\rmf i}\r ) $$ $$ 0= \sum_{i=1}^n\sum_{j=1}^{i-1} \delta \vr_i \cdot\vf_{ij}- \sum_{i=1}^n\sum_{j=i+1}^n \delta \vr_i \cdot\vf_{ji} +\sum_{i=1}^n \delta \vr_i \cdot \vf_{\rmc i} + \sum_{i=1}^n \delta \vr_i \cdot\vf_{\rmf i} $$ from the single-particle problem we already know $\delta\vr_i\cdot \vf_{\rmc i}=0$ and there remains only the balance equation $$ 0= \sum_{i=1}^n\sum_{j=1}^{i-1} \delta \vr_i \cdot\vf_{ij}- \sum_{i=1}^n\sum_{j=i+1}^n \delta \vr_i \cdot\vf_{ji} + \sum_{i=1}^n \delta \vr_i \cdot\vf_{\rmf i} $$ let us have a closer look onto the first two sums with the internal constraint forces $\vf_{ij}$ $$ \sum_{i=1}^n\sum_{j=1}^{i-1} \delta \vr_i \cdot\vf_{ij}- \sum_{i=1}^n\sum_{j=i+1}^n \delta \vr_i \cdot\vf_{ji} $$ for easier comparison of the terms in the two sums we exchange the names of the indexes in the second one : $$ \sum_{i=1}^n\sum_{j=1}^{i-1} \delta \vr_i \cdot\vf_{ij}- \sum_{j=1}^n \sum_{i=j+1}^n \delta \vr_j \cdot\vf_{ij} $$ that is just re-naming and does not change the value of the sum . let us have a look onto the set of index pairs over which we have to sum in the first term and in the second one . we can do this graphically in the following diagram . there mark the locations $ ( i , j ) $ of the index pairs which are included in the sum with points . i just draw some of these points and put a gray triangle onto the region where we would have to mark all grid points . if we do the same for the second term we see that we obtain exactly the same triangle and therefore also the same set of index pairs . if you prefer we can also check this in set-notation . the set of index pairs for the first term is : $$ \{ ( i , j ) \in\nn^2 \mid 1\leq i\wedge i\leq n \wedge 1 \leq j\wedge j \leq i-1\} $$ just transforming the last inequality : $$ =\{ ( i , j ) \in\nn^2 \mid 1\leq i\wedge i\leq n \wedge 1 \leq j\wedge j+1 \leq i\} $$ just reordering of the inequalities : $$ =\{ ( i , j ) \in\nn^2 \mid 1 \leq j \wedge 1\leq i \wedge j+1 \leq i\wedge i\leq n\} $$ the inequalities $1\leq j$ and $j+1\leq i$ already imply $1\leq i$ therefore we can drop this inequality . furthermore , $j+1\leq i$ implies the inequality $j\leq n$ which we can add without danger . this gives the set $$ =\{ ( i , j ) \in\nn^2 \mid 1 \leq j \wedge j\leq n \wedge j+1 \leq i\wedge i\leq n\} $$ of index pairs for the second term . since the sets of index pairs for the sums are the same we can combine the summands under the same sum sign $$ \sum_{i=1}^n\sum_{j=1}^{i-1} \delta \vr_i \cdot\vf_{ij}- \sum_{j=1}^n \sum_{i=j+1}^n \delta \vr_j \cdot\vf_{ij} =\sum_{i=1}^n\sum_{j=1}^{i-1} \l ( \delta \vr_i \cdot\vf_{ij}-\delta \vr_j \cdot\vf_{ij}\r ) $$ and factor out the internal constraint forces $$ =\sum_{i=1}^n\sum_{j=1}^{i-1} \underbrace{\l ( \delta \vr_i -\delta \vr_j \r ) \cdot\vf_{ij}}_{=0} = 0 $$ eventually the sum becomes zero with the orthogonality condition for the internal constraint forces $\l ( \delta \vr_i -\delta \vr_j \r ) \cdot\vf_{ij}=0$ which we discussed further above . exploiting this our balance equation $$ 0= \sum_{i=1}^n\sum_{j=1}^{i-1} \delta \vr_i \cdot\vf_{ij}- \sum_{i=1}^n\sum_{j=i+1}^n \delta \vr_i \cdot\vf_{ji} + \sum_{i=1}^n \delta \vr_i \cdot\vf_{\rmf i} $$ becomes just $$ 0 = \sum_{i=1}^n \delta \vr_i \cdot\vf_{\rmf i} . $$ the sum over the products of the virtual displacements $\delta\vr_i$ with the free forces $\vf_{\rmf i}$ is zero . this formula is called the principle of virtual work . just to get a formula usable for calculation we substitute $\delta\vr_i=\vf_i\ , ' ( s ) \cdot\delta s$ $$ 0 = \sum_{i=1}^n \delta s \vf_i\ , ' ( s ) \cdot\vf_{\rmf i} $$ and set $\delta s=1$ which gives $$ 0 = \sum_{i=1}^n \vf_i\ , ' ( s ) \cdot\vf_{\rmf i} . $$ this is one scalar formula for our one degree of freedom $s$ in the system . if our system of particles is actually a continuum instead of discrete particles the index $i\in \{1 , \ldots , n\}$ for $\vr_i ( s ) $ is replaced by a continuous parameter $\xi\in [ 0 , l ] $ with some length $l&gt ; 0$ and we write $\vr ( s , \xi ) $ instead of $\vr_i ( s ) $ . beside discrete forces $\vf_{\rmf i}$ which operate at certain places $\xi_{\rmf i}$ $i=1 , \ldots , n_{\rmf}$ of the constrained continuum there can also act a distributed force per length $\vf_{\rmf}^\d ( \xi , \vr ) $ on the continuum . to get a transition from the principle of virtual work for discrete particle systems to continuous particle systems one can start with a with a discretization of the continuum . we dissect the continuum into $k$ sections $j=1 , \ldots , k$ with length $\frac lk$ . the parameter value for the $j$-th section runs from $\xi=\xi_{j-1}:= ( j-1 ) \frac lk$ to $\xi_j:=j\frac{l}{k}$ . furthermore , we address each of the sections by some intermediate parameter $\xi^*_j$ with $\xi_{j-1}&lt ; \xi^*_j&lt ; \xi_j$ . for an instance $\vr ( s , \xi_j^* ) $ can be the center of mass of the $j$-th section . if the pieces are small enough , i.e. , the number of pieces $k$ is high enough , a discrete particle system will be a good approximation of the system of pieces . we consider the points $\vr ( s , \xi_j^* ) $ at the intermediate parameter values $\xi_j^*$ as location of the $j$-th particle . the free force caused by the distributed force per length on the $j$-th piece will be : $$ \int_{\xi_{j-1}}^{\xi_j} \vf_\rmf^\d ( \vr ( s , \xi ) ) d\xi\approx \vf_\rmf^\d ( \vr ( s , \xi^*_j ) ) \cdot ( \xi_j-\xi_{j-1} ) $$ now we apply the principle of virtual work to this approximative particle system and obtain $$ 0 = \sum_{j=1}^{k}\delta \vr ( s , \xi_j^* ) \cdot \vf_\rmf^\d ( \vr ( s , \xi^*_j ) ) \cdot ( \xi_j-\xi_{j-1} ) + \sum_{i=1}^{n_{\rmf}} \delta \vr ( s , \xi_{\rmf i} ) \cdot \vf_{\rmf i} $$ we expect the approximation to get better with higher with larger $k$ . for $k=1,2 , \ldots$ we obtain a sequence of riemann sums for the first sum which is converging to the integral $\int_0^l \delta\vr ( s , \xi ) \cdot\vf_{\rmf}^\d ( s , \vr ( s , \xi ) ) d\xi$ for $k\rightarrow\infty$ . this leads us to the principle of virtual work for a 1d-continuum with one degree of freedom $s$: $$ 0 = \int_0^l\delta\vr ( s , \xi ) \cdot\vf_{\rmf}^\d ( s , \vr ( s , \xi ) ) d\xi + \sum_{i=1}^{n_{\rmf}} \delta \vr ( s , \xi_{\rmf i} ) \cdot \vf_{\rmf i} $$ thereby , the integral accounts for the distributed free force acting on the continuum and the sum accounts for the discrete free forces acting on it . the application of the principle of virtual work delivers $$ f_s\delta s + \int_{\xi=0}^l \delta\vr ( s , \xi ) \cdot \vf_{\rm g}^\d d \xi = 0 $$ where $f_s$ is the supporting force of the chain , i.e. , the thread force , $\vf_{\rm g}^\d = -\lambda g\ve_z$ is the weight force per length and $\delta\vr ( \xi , s ) = \frac{\partial \vr}{\partial s} ( s , \xi ) \delta s$ is the virtual vectorial displacement of the string . $$ \l ( f_s + \int_{\xi=0}^l \frac{\partial \vr}{\partial s} ( s , \xi ) \cdot \vf^\d_{\rm g} d \xi\r ) \delta s = 0 $$ and if this should hold for virtual chain displacements $\delta s\neq 0$ we get the condition $$ f_s = -\int_{\xi=0}^l \frac{\partial\vr}{\partial s} ( s , \xi ) \cdot \vf^\d_{\rm g} d \xi $$ note , that $\frac{\partial \vr}{\partial s}$ is just the unit vector in tangential direction of the chain . so , because of the constraint we are just integrating the tangential component $\frac{\partial \vr}{\partial s} ( s , \xi ) \cdot \vf^\d_{\rm g}$ of the weight force . you could say that the constraint force of the sphere deviates the pull force in the chain . the problem with your approach is that you are missing the deviation force of the chain in the calculation of the normal force $dn$ . if you cut out a ( finite ) piece of chain then the ends of this piece will not have the same tangent direction because of the curvature of the sphere . if some force ( the weight of the remainder of the chain ) acts tangentially on the lower end of the piece it cannot be fully compensated by the tangent force at the higher end ( the tangent vectors are linearly independent ) . the force difference must be compensated by a normal force on the shpere . we can also just use another approach to check the result . assumption : no friction , no damping . potential energy of chain depending on the start angle $\theta$ on the sphere $$ e_{\rm pot} ( \theta ) = g\lambda\underbrace{ ( l-r\theta ) }_{\small\mbox{length}}\underbrace{\left ( -\frac{l-r\theta}2\right ) }_{\small\mbox{height}}+\int_0^\theta g \underbrace{r\sin ( \bar\theta ) }_{\mbox{height}}\underbrace{\lambda r d\bar\theta}_{d m} = g\lambda\left ( -\frac{ ( l-r\theta ) ^2}2 +r^2 ( 1-\cos ( \theta ) ) \right ) $$ the cut force in the thread on the chain-side is $$ f=\frac1{r}\frac{d e_{\rm pot}}{d\theta} = \frac{g\lambda}{r}\left ( -r ( r\theta-l ) + r^2\sin ( \theta ) \right ) $$ which gives at $\theta=\frac{\pi}2$ $$ f=g\lambda\left ( l-r\frac{\pi}2+r\right ) $$ hopefully , without any mistakes . but , you should check . so , in the end this calculation delivers also solution a . note , a very good book on classical mechanics ( with constraints ) is arnold 's " mathematical methods of classical mechanics " . this is a graduate text but the first chapters should already be readable for advanced students from high-school .
this can be resolved by being clear about what surface you are integrating over . in the first equation , $$ \oint {\overrightarrow{b} . \overrightarrow{da}} = 0 , $$ you are integrating over any closed surface , i.e. a surface without a hole in it , such as a sphere . the equation says that the magnetic flux coming in must equal the magnetic flux going out . but in the second equation , $$ e = \frac{d}{dt} \int_\sigma {\overrightarrow{b} . \overrightarrow{da}} , $$ you are integrating over a surface with a hole in it , where the hole is a loop of wire , as shown in this diagram from wikipedia : this equation says that the rate of change of flux passing through the surface must equal the emf in the wire loop . you can imagine it as coming in through the hole , and out through the surface . ( or the other way round or a bit of both . ) you can see a connection between the two equations if you imagine making the wire loop smaller and smaller until the hole closes completely . then you are back at a closed surface again , where the first equation applies - and this infinitely small loop of wire can never experience an emf .
$e_0 = m_0 c^2$ is only the equation for the " rest energy " of a particle/object . the full equation for the kinetic energy of a moving particle is actually : $$e-e_0 = \gamma m_0c^2 - m_0c^2 , $$ where $\gamma$ is defined as $\gamma = \frac{1}{\sqrt{1 - ( v/c ) ^2}} , $ where $v$ is the relative velocity of the particle . an " intuitive " answer to the question can be seen by noticing that the particle 's energy approaches $\infty$ as its velocity approaches the speed of light . thus , in order for the particle to move faster than the speed of light would require it to attain infinite kinetic energy , which can not happen .
any physics equations you write down could be wrong , so you need to verify them experimentally , or you are just doing math . i can imagine a universe in which the gauss 's law does not work for moving charges ; and i have to test to see if we live in such a universe . in that sense , there is no non-experimental way to verify it . on the other hand , if you mean to ask if there is a mathematical way to prove it from the experimentally verified equations of electromagnetism , then sure ! but you need to be able to calculate the electric field of a moving charge . the derivation is a bit complicated , and i will leave it to you to find your favorite version of it , but the answer for a single charge moving at constant velocity is \begin{equation} \vec{e} = \frac{k\ , q\ , \vec{r}}{r^3}\ , \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}} \end{equation} it is enough to show this for just one charge because the integral is linear , so you can just add up contributions from different charges . also , we can take the integral over a sphere centered on this charge , since the divergence theorem tells us that moving the surface of integration will not change anything ( as long as we keep the charge inside ) . so , we do \begin{align} \oint \vec{e} \cdot d\vec{a} and = \oint \frac{k\ , q\ , \vec{r} \cdot \hat{r}}{r^3}\ , \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}}\ , r^2\ , \sin \theta\ , d\theta\ , d\phi \\ and = k\ , q\ , \int_0^{2\pi} \int_0^\pi \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}}\ , \sin \theta\ , d\theta\ , d\phi\\ and = 2\pi\ , k\ , q\ , \int_0^\pi \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}}\ , \sin \theta\ , d\theta \\ and = 4\pi\ , k\ , q~ , \end{align} which is just the usual gauss 's law . this , of course , is only for constant velocities , as peter kravchuk pointed out below . to see it more generally , it would probably be easier to go over to the differential form of gauss 's law , which is one of maxwell 's equations . then , you can note that maxwell 's equations are relativistically covariant .
there are many natural examples of quantities with fractional non-rational dimensions . these are ubiquitous in macroscopic physics , because the scaling laws of nature are emergent properties that do not usually care about differentiability . it is only in cases where you demand smoothness that you restrict to integer dimensions . the phenomenon discussed below are discussed at much greater length in mandelbrot 's " the fractal geometry of nature " , and in his publications . levy diffusion suppose a particle is undergoing levy diffusion , a process by which it hops from place to place with a jump whose distribution has a tail , meaning that the probability density of a hop of length l is : $$\rho ( l ) \propto {1\over l^{1+\beta}}$$ where $0&lt ; \beta&lt ; 2$ . then , if you take the limit of many steps , the analog of the central limit theorem guarantees that the probability of finding the particle at position x obeys a levy diffusion equation , which is easiest to write in a spatial fourier transform : $${\partial \rho ( k ) \over \partial t} = - d k^\beta \rho ( k ) $$ the constant d is the levy analog of the diffusion constant , and it has dimensions {l^\beta\over t} . the interpretation of d is that , when multiplied by t and extracting the $\beta$ root , it gives the typical displacement scale after time t . it is a natural quantity with fractional dimension . processes which undergo levy diffusion are relatively common . for example , advection of a dust particle by a turbulent flow ( see this experimental article:http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.152.6792 ) hausdorff measures when you try to define the " length " of a fractal shape , you run into the problem that the length is infinite . the way around this is to define a notion of length per length-scale , and to ask exactly how the length blows up . in mathematics , the most general way to do this , which works for essentially any set , is called the hausdorff measure . in practice , you can define a simpler version in terms of box-counting scaling which is sufficient for ordinary fractals . consider the coastline of england , and ask : given a boxes of length \epsilon , how many do i need to cover it up ? the minimum number n diverges as a power of \epsilon : $$ n ( \epsilon ) \approx {a\over \epsilon^\alpha}$$ the coefficient of the divergence , a , is the hausdorff measure , and a has units of $l^{\alpha}$ , and $\alpha$ can be anything between 1 and 2 for a coastline , and between 0 and 3 for a physical shape , like a cloud boundary , or a diffusion-limited-aggregation cluster . three dimensional quantum fields consider a scalar field in 2+1 dimensions , with a quartic self-interaction . at short distances , the correlation function of the field blows up . the scale dimension of the field is defined by this power : $$\langle \phi ( x ) \phi ( y ) \rangle = {1\over |x-y|^{1+2\nu}}$$ this might seem surprising if you are used to fields having canonical dimensions . the scalar in 2d has dimension 1/2 right ? wrong . the canonical dimension 1/2 only describe a free scalar theory or a cut-off theory with no short-distance self-interactions . after a successful continuum-limit by renormalization , the fields get new dimensions , which do not involve the dimensional quantity $\epsilon$ , which has gone to exactly zero . then the fields must be defined so that the left side and the right side of the equation above are dimensionally consistent . the exponent $\beta$ is very close to 0 , the correction is quadratic in " d-4" ( i hate this traditional name for the expansion parameter of the wilson-fisher point ) , but it is nonzero , and if you had to take bets , it will certainly be irrational . in four dimensions , ordinary local quantum fields only get logarithmic corrections to their classical scaling values , so the scaling of the field at short distances is either canonical ( like in qcd ) , or nonsensical ( like in qed ) . but in 3 dimensions , you get good anomalous dimensions , and fields have a true scale-invariant continuum limit . other fields if you are willing to leave physics , the quantities of finance often have levy behavior . if you consider the dollar a unit of nature , you can find fractional exponents there too .
i recommend using octave ( or matlab , which is much more user-friendly but you will need a license ) . for every quantity that you mentioned there is a command in octave and it is as simple as a=mean ( y ) or v=cov ( x , y ) . importing and exporting data is also very easy .
if by large density you mean large baryon density , then i believe one of the fundamental large $n_c$ results is that at densities of order nuclear densities , but below the density where the baryons have dissolved into quarks , baryonic matter forms a crystalline structure . this has been analyzed in the skyrme model . i think this paper by klebanov was one of the first to point this out : " nuclear matter in the skyrme model , " igor r . klebanov , nucl . phys . b262:133,1985 . ads/qcd models can also be used to study qcd at large baryon chemical potential and low temperatures ( and necessarily at large $n_c$ ) . an instability to the condensation of vector mesons resulting from chern-simons couplings dictated by anomaly matching was found in a paper by me and s . domokos , arxiv:0704.1604 . i am not aware of any evidence that this phenomenon happens in the real world , and even its existence at large $n_c$ should be regarded as conjectural . i do not know whether this instability is related to the dgr instability or not .
in the situation you gave , it is immediately clear what is meant , and there is no possibility for misinterpretation , so yes , it is perfectly acceptable . ( remember that torque is mathematically defined as a vector for convenience , but the direction of that vector is not really physical . ) the only issue i can see with that is that as you leave the simple two-dimensional world and start looking at more complex problems , you have quickly need to abandon that way of describing things , since " clockwise " and " counter-clockwise " are a lot harder to keep track of .
1 ) why does length contraction only occur in the direction of travel , ( not in all directions ) when approaching the speed of light ? there are a number of ways to answer that vary in the level of sophistication . but first , do understand that the lorentz transformation gives length contraction only in the direction of motion . so , if you accept the lorentz transformation as the correct coordinate transformation between relatively moving reference frames , you accept that there is no transverse contraction . also remember , the lorentz transformation was originally derived so that the measured speed of light would always be c in any inertial reference frame . but , assume for the sake of argument that transverse length contraction does occur . we almost immediately arrive at a contradiction . consider a wall upon which two parallel horizontal stripes have been painted . the vertical distance between the stripes is exactly 1 meter which can be confirmed by placing an ideal meter stick vertically on the wall and finding that the ends of the meter stick touch the top and bottom stripes . now , imagine that the same vertical meter stick moves horizontally with respect to the wall . if there is vertical contraction then , according to someone at rest with respect to the wall , the meter stick no longer reaches between both lines . since the stick is vertically contracted , the spacing between the lines is greater than the length of the meter stick . however , to someone at rest with respect to the meter stick , it is the wall that is moving and it is the wall that is vertically contracted . to this person , the meter stick overlaps the two lines ; the spacing between the lines is less than the length of the meter stick . but this is a contradiction ! it cannot be the case that the top end of the meter stick is both below and above the top stripe and similarly for the bottom end . we conclude that transverse contraction leads to a paradox . therefore an outside observer would then see the car expanded by a factor of 2 not contracted . no , that is not the correct conclusion . first , the phrase " outside observer " is ambiguous . what we should specify here is whether the observer is at rest with respect to the road or with respect to the car . for an observer at rest with respect to the road , your car is contracted and fits in between the markers . for an observer at rest with respect to your car , the distance between the markers is contracted and your car is longer than the markers . this is not a contradiction because , and this is crucial , of the relativity of simultaneity ; relatively moving observers do not agree on whether spatially separated events , along the axis of motion , are simultaneous . in order to tell if the car fits between the markers , we must determine the location of the rear and front of the car at the same time according to spatially separated clocks synchronized in our reference frame . so , imagine that there is a clock at the front of the car and at the rear of the car and that they are synchronized by einstein synchronization according to the observer at rest with respect to the car . then , at some time , the location of the clocks are recorded . according to the observer at rest with respect to the car , the recording of the location of the two clocks occurred at the same time . thus , the difference in the location of the clocks is the length of the car . however , according to the observer at rest with respect to the road , the locations were not recorded at the same time , i.e. , the clocks on the car are not synchronized . thus , the two observers do not agree on the length of the car .
for a $d=1\mu$m solid particle of density $1\text{g/cm}^3$ , the very approximate terminal velocity according to the drag equation is ( in mathematica ) : 0.499546 foot/second this is on the order of the speed of the air currents in a typical room of a house . edit : as kevinkostlan mentioned , using kinematic drag is completely incorrect in this size regime , and viscous drag is actually the dominant force . using the terminal velocity relation given on the wikipedia page for stoke 's law gives 27.4742 micro meter/second thus , air currents will completely dominate the motion of the particle , and the mass is mostly irrelevant in this size regime .
i am not an expert on the history , but wiki has a page on the history of entropy that may be helpful ( i could not tell you if it is accurate ) . note that the early work in thermodynamics was almost entirely experimental , with people working on the efficiency of heat engines and so on . so it was in that context that entropy emerged as a useful quantity , without any special " microscopic " interpretation attached to it . in any case the interpretation of entropy as " disorder " makes no sense before statistical mechanics , and it is innacurate and sometimes misleading even with statistical mechanics .
great and important question ; i hope this response is illuminating and encourages you and others to explore lie groups , lie algebras , and their representations . when you want to rotate a vector $\mathbf v$ in three dimensions , then you act on that vector with a rotation matrix $r$ to obtain a rotated vector $\mathbf v'$ related to the original vector by $$ \mathbf v ' = r\mathbf v $$ it is a mathematical fact that every rotation ( special orthogonal transformation ) $r$ is a rotation by some angle $\theta$ about some axis defined by a unit vector $\mathbf n$ , and that each such rotation can be written as the matrix exponential of a particular linear combination of certain 3-by-3 matrices $j_i$ called rotation generators . explicitly $$ r ( \theta , \mathbf n ) = e^{-i\theta n_i j_i} , \qquad ( j_i ) _{jk} = i\epsilon_{ijk} $$ so we can write the rotation of a vector as $$ \mathbf v ' = e^{-i\theta n_i j_i} \mathbf v $$ it turns out , that we can define the rotation of spinors in an analogous way . instead of the rotation generators $j_i$ which are 3-by-3 matrices , we choose the pauli matrices which are 2-by-2 , and for a given spinor $\xi$ , we define a rotated spinor by $$ \chi ' = e^{-i\frac{\theta}{2} n_i\sigma_i}\chi $$ notice how this is basically the same as rotating a vector in 3 dimensions , it is just that we have represented rotations acting on the vector space of spinors in a different way . the math behind all of this is called representation theory . in particular , when we are talking about spin , the representation theory is related to that of the lie groups $\mathrm{so} ( 3 ) $ and $\mathrm{su} ( 2 ) $ and their so-called lie algebras and their representations .
sure , it is no problem to do this . the thing that has to change is that $i$ should index over all possible configurations of the $n$ elements , and the energy in the boltzmann distribution has to be the total energy of the system . so if $m=10$ , $n=3$ and $d=2$ then , for example , $$ p ( [ 1,0,0,1,0,0,1,0,0,0 ] ) = \frac{1}{z}e^{-\frac{\epsilon_1 + \epsilon_4 + \epsilon_7}{kt}} , $$ but $$ p ( [ 1,0,1,0,0,0,1,0,0,0 ] ) = 0 $$ because it is not allowed by the constraint . to calculate the normalising factor ( or " partition function" ) $z$ , you have to sum over all allowed configurations of the system . it is not immediately obvious ( to me ) how to do that analytically in this case , but you are the mathematician so i am sure you can find an elegant way . incidentally , you should be able to see that if there are no interactions between the $m$ positions then this reduces to the formula you originally quoted .
feeling silly now . just equating the component of velocities along the wall : $$1/2 sin\theta=cos\theta$$ we get $$\tan\theta =2$$ so , $$e=1/4$$
i see four reasons to make this assumption : the first one is simply that we do not know really how to deal with the non linear problem in general cases and therefore we linearize but i agree that it is not a good justification although this is most of the time the hidden reason why people use it the poisson-boltzmann equation is mostly used in aqueaous solutions and therefore you have a factor 80 ( owing to the dielectric constant of bulk water ) that appears in all your potentials . . . this more or less ensures that a monovalent ion does not generate a so high potential energy with other monovalent ions . if you have free ions in solution , it means that the thermal energy was enough to begin with to unbind them from the groups there were bound to it can be rigorously shown that at large distances from an ion at any valency , the generated potential decays exponentially with the distance ( or rather like a yukawa potential ) because of the screening owing to the mobile charges in solution ( this result holds in the non linear poisson-boltzmann case but also beyond the poisson-boltzmann theory ) . incindentally , a yukawa potential is what you can expect from the field generated by an isolated ion in solution within the linearized poisson-boltzmann theory . also , if you are a bit familiar with liquid theory , a particular closure of the orstein-zernike equation leads to an infinite summation over tree-like diagrams that leads to a total correlation function between two point ions in solutions that decays as a yukawa as well which tells you more or less that although you linearize , you still encapsulate important many body effects because of the last point , there is a huge litterature on charge renormalization of charged particles in solution as a function of their valency , size and salt concentration . in the context of the poisson-boltzmann theory , charge renormalization refers to a mapping from the non linear theory to the linearized one via the use of effective charge paramaters
first of all you should recall that schroedinger equation is an eigenvalue equation . if you are unfamiliar with eigenvalue equations you should consult any math book or course as soon as possible . answer 1 ( my apologies , i will use my own notation , as this is mainly copy-paste from my old notes ) : first define constants \begin{equation} x_0 = \sqrt{\frac{\hbar}{m\omega}} , \end{equation} \begin{equation} p_0 = \frac{\hbar}{x_0} = \sqrt{\hbar m \omega} , \end{equation} and dimensionless operators \begin{equation} \hat{x} = \frac{1}{x_0} \hat{x} , \end{equation} and \begin{equation} \hat{p} = \frac{1}{p_0} \hat{p} . \end{equation} their commutation relation then is \begin{equation} \left [ \hat{x} , \hat{p} \right ] = \left [ \frac{1}{x_0}\hat{x} , \frac{1}{p_0}\hat{p} \right ] = \frac{1}{x_0 p_0} \left ( \hat{x} \hat{p} - \hat{p} \hat{x} \right ) = \frac{1}{x_0 p_0} \left [ \hat{x} , \hat{p} \right ] = \frac{i\hbar}{x_0 p_0} = i , \end{equation} as \begin{equation} x_0 p_0 = \sqrt{\frac{\hbar}{m\omega}} \sqrt{\hbar m\omega} = \hbar . \end{equation} now write hamiltonian in terms of $\hat{x}$ and $\hat{p}$ . start with \begin{equation} \hat{h} = \frac{p_0 ^2}{2m} \hat{p} ^2 + \frac{1}{2} m\omega^2 x_0 ^2 \hat{x}^2 . \end{equation} notice that \begin{equation} p_0 ^2 = \hbar m \omega \end{equation} and \begin{equation} x_0 ^2 = \frac{\hbar}{m \omega} , \end{equation} hence \begin{equation} \hat{h} = \frac{\hbar \omega}{2} \hat{p}^2 + \frac{\hbar \omega}{2} \hat{x}^2 = \frac{\hbar \omega}{2} \left ( \hat{x}^2 + \hat{p}^2 \right ) . \end{equation} up to the commutation relation we can write \begin{equation} \left ( x^2 + p^2 \right ) = \left ( x - ip \right ) \left ( x + ip \right ) . \end{equation} on the other hand , for operators this is not quite allowed , as \begin{align} \left ( \hat{x} - i\hat{p} \right ) \left ( \hat{x} + i\hat{p} \right ) and = \hat{x}^2 + i\hat{x}\hat{p} - i\hat{p}\hat{x} + \hat{p}^2 \nonumber \\ and = \hat{x}^2 +i \left ( \hat{x}\hat{p} - \hat{p}\hat{x}\right ) + \hat{p}^2 \\ and = \hat{x}^2 + i \left [ \hat{x} , \hat{p} \right ] + \hat{p}^2 = \hat{x}^2 + \hat{p}^2 - 1 \nonumber , \end{align} so one has \begin{equation} \left ( \hat{x}^2 + \hat{p}^2 \right ) = \left ( \hat{x} - i\hat{p} \right ) \left ( \hat{x} + i\hat{p} \right ) + 1 . \end{equation} now we can define \begin{equation} \hat{a} = \frac{1}{\sqrt{2}} \left ( \hat{x} + i\hat{p} \right ) , \end{equation} and \begin{equation} \hat{a}^\dagger = \frac{1}{\sqrt{2}} \left ( \hat{x} - i\hat{p} \right ) , \end{equation} calling this creation operator and $\hat{a}$ - annihilation operator . notice that we can now express hamiltonian in terms of creation and annihilation operators : \begin{equation} \hat{h} = \frac{\hbar\omega}{2} \left ( \sqrt{2} \hat{a} ^\dagger \sqrt{2} \hat{a} + 1 \right ) = \hbar \omega \left ( \hat{a} ^\dagger \hat{a} + \frac{1}{2} \right ) . \end{equation} but we can also define the number operator , $\hat{n} = \hat{a} ^\dagger \hat{a}$ , so finally get \begin{equation} \hat{h} = \hbar \omega \left ( \hat{n} + \frac{1}{2} \right ) . \end{equation} now go aside a bit and consider creation and annihilation operators . by definition , \begin{equation} \hat{a}^\dagger \left| n \right\rangle = \sqrt{n + 1} \left| n + 1 \right\rangle , \end{equation} \begin{equation} \hat{a} \left| n \right\rangle = \sqrt{n} \left| n - 1 \right\rangle , \end{equation} where $\left| n \right\rangle$ is the eigenstate of creation and annihilation operators , as well as of the hamiltonian ( due to the fact that they commute - homework to prove ) . now \begin{equation} \hat{a}^\dagger\hat{a} \left| n \right\rangle = \hat{a}^\dagger \sqrt{n} \left| n - 1 \right\rangle = \sqrt{n} \sqrt{n} \left| n \right\rangle = n \left| n \right\rangle , \end{equation} so conclude that the eigenvalue of a number operator , $\hat{n}$ , is just $n$ , so if we now apply hamiltonian in the schroedinger equation , get \begin{equation} \hat{h} \psi = e \psi , \end{equation} \begin{equation} e_n = \hbar \omega \left ( n + \frac{1}{2} \right ) , \end{equation} which is exactly the result you were looking for . answer 2: first of all you should remember that the general aim of solving an eigenvalue problem is to find a set of eigenvectors , but not a single eigenvector . in your case , equation should be modified to \begin{equation} \frac{d^2 \psi_n}{dx^2} + \left [ ( 2n + 1 ) - \varepsilon^2\right ] \psi_n = 0 , \end{equation} where $\psi_n$ are eigenvectors ( eigenfunctions ) that correspond to eigenvalues $e_n$ . try to think a little bit and explain physical meaning of having many energy eigenvalues in quantum mechanics . now return to the general theory of eigenvalue equations . although i have never met the equation you wrote , i cannot find any place it can be wrong apart from the one just pointed out . though , i do not see how far can you go from it . answer 3: hermite polynomials are usually beyond standard quantum mechanics courses . if you know legendre , chebyshev and/or other polynomials , you may guess that hermite polynomials are derived as solution to some differential equation , and this does not contradict to the definition of $\psi$ . as i have already mentioned , hermite polynomials are usually beyond standard quantum mechanics courses . usually you are not supposed to derive them at this level . however , if you are still interested , you may want to consult with google or ask another question here . hope your questions have now been answered in full . however , should you need any further comment - you are welcome .
you are right that a finite universe , if flat , would necessarily have a center . however , an infinite universe has no center . an infinite muffin in 3d has a divergent volume at any point in its history , and so any point you choose will have equal ( equally infinite , speaking loosely ) amounts of stuff in every direction from it . all we can say is that the universe has expanded by some factor between time $t_1$ and time $t_2$ . alternatively , a finite universe can also have no center if it is curved . in particular , if it has positive curvature 1 everywhere , it loops back on itself . the analogy here is of the surface of a balloon . this is a finite 2d surface , and it has a perfectly well defined area . still , all points are equal and none can be said to be the center . even if the balloon is expanding , the " center " is not a part of the " universe " in this model . the balloon analogy is used a lot in explaining cosmology , but in fact the infinite muffin describes our actual universe better . as best we can tell , our actual universe is flat and infinite . 1 uniform positive curvature is characteristic of a ( hyper ) sphere . uniform negative curvature in 2d yields a saddle .
the question op is proposing is linked to the question of the mass formulas . here , what really matters is if the mass of the u quark is indeed very near zero and if one has some compelling theoretical reason to believe this . the strong cp problem could not be of much help here as pointed out in the dine 's review . the reason is quite simple : if one should have a $\theta$ term into qcd lagrangian , the neutron would have a measurable electric dipole . from experiments we know that is not the case and a lower bound is fixed . but the electric dipole of the neutron does not depend only from the mass of the quark u and so , having $m_u\approx 0$ is a sufficient condition but not necessarily the right one . from a theoretical stand point , from qcd sum rules a lower bound for the masses of u and d quarks can be estimated . the main reference is s . narison , qcd as a theory of hadrons ( cambridge university press , 2007 ) . i report here the estimation given in this book for the sake of completeness ( chapter 53 in the book ) : $$ ( m_u+m_d ) ( 2\ gev ) &gt ; 7\ mev . $$ this grants a small but yet finite mass and whatever mass formula should satisfy this bound . of course , this is consistent with $m_u\approx 0$ . but a more recent review ( see here ) gives $m_u\approx 3\ mev$ that is not so small but it is on the strong interaction scale . smallness of $m_u$ and $m_d$ masses makes chiral symmetry a very good yet approximate symmetry .
i think you might try approaching this in the heisenberg picture . the time derivative of the position operator is : $$\dfrac{d \hat x}{dt} = \dfrac{i}{\hbar} [ \hat h , \hat x ] $$ which is a reasonable velocity operator . the time derivative of the velocity operator is then : $$\dfrac{d^2 \hat x}{dt^2} = \dfrac{i}{\hbar} [ \hat h , \dfrac{d \hat x}{dt} ] $$ for example , consider a free particle so that $\hat h = \frac{\hat p^2}{2m}$ . the velocity operator would then be $\frac{\hat p}{m}$ . this certainly looks reasonable as it is of the form of the classical $\vec v = \frac{\vec p}{m}$ relationship . but , note that the velocity operator commutes with this hamiltonian so the commutator in the definition of the acceleration operator is 0 . but that is what it must be since we are assuming the hamiltonian of a free particle which means there is no force acting on it . now , consider a particle in a potential so that $\hat h = \frac{\hat p^2}{2m} + \hat u$ . the velocity operator , for this system , is then $\frac{\hat p}{m} + \frac{i}{\hbar} [ \hat u , \hat x ] $ . assuming the potential is not a function of momentum , the commutator is zero and the velocity operator is the same as for the free particle . the acceleration operator is then $\dfrac{i}{\hbar} [ \hat u , \frac{\hat p}{m} ] $ . in the position basis , this operator is just $\frac{-\nabla u ( \vec x ) }{m} $ which looks like the acceleration of a classical particle of mass m in a potential given by $u ( \vec x ) $ .
i think ( although i cannot find any source for this that is even remotely reliable ) : the cracking is due to a sudden temperature gradient being exerted on the ice , because compared to the ice , the wine is usually pretty warm . this sudden rise in temperature just on the outside causes the ice to fracture . this is accompanied by a cracking sound which is due to the sudden displacement ( release of energy ) of different layers of the cube during this fracture . i am actually pretty sure this is at least part of the reason , because often when you throw in the ice , you can see fractures appearing in the ice at the same moment you hear the sound . the increase in bubbling is due to there being more seed locations for bubble formation . basically , this is true for most things you would throw in ; bubbles tend to form best in places where there is an in-homogeneity of some sort ; that is why they often form on particular points on the surface of the glass , and not just " somewhere in the middle " of the fluid . throwing an ice cube in will drastically increase the possible number of formation sites , because ice cubes are usually pretty rough , microscopically speaking .
it could be any vector from bc to oa . let 's assume that $r$ is the vector perpendicular to bc to the point oa . any vector from bc to oa ( whether or not it is perpendicular to bc ) has the form $r + s$ , where $s$ is some vector parallel to bc . of course , you have told us that $\omega$ is also parallel to bc , so we could also write that as $r+\alpha\ , \omega$ , for some number $\alpha$ . so let 's take the cross product : \begin{equation} \omega \times ( r+\alpha\ , \omega ) = \omega \times r + \omega \times ( \alpha\ , \omega ) = \omega \times r + \alpha\ , \omega \times \omega = \omega \times r~ , \end{equation} since $\omega \times \omega = 0$ . so $\omega$ cross any vector from bc to oa will equal $\omega$ cross the perpendicular vector . so you do not need to specifically find the perpendicular .
the question speed of light originating from a star with gravitational pull close to black-hole strength ? is very nearly , but not quite , a duplicate . however my answer to that question applies to your question as well . when you ask : how would light going away from a black hole react to the gravity ? you have to extend your question to indicate what observers you are asking about . for an observer hovering next to the atom emitting the light the photon will be travelling at $c$ . for an observer far from the black hole the speed of the photon will be less than $c$ and given by : $$ v = c\left ( 1 - \frac{2gm}{c^2r}\right ) $$ where $m$ is the mass of the black hole and $r$ is the distance from the centre of the black hole . it is common in general relativity that diffrent observers will observe different behaviour , which is one of the ( many ! ) things that makes gr confusing for beginners . in addition , as brandon and frederic have said , the light will lose energy and red shift as it moves away from the black hole . if $\nu_r$ is the original frequency when the light is emitted at a distance $r$ then the frequency at infinity is given by : $$ \nu_\infty = \nu_r\sqrt{1 - \frac{2gm}{c^2r}} $$
john , that is my blog and antenna design . fractal designs are used to fit a larger antenna in a smaller space . the self similar pattern has been found to reduce loss of gain when needing to compress the size of an antenna . the wikipedia page on fractal antennas might be a useful read for you . http://en.wikipedia.org/wiki/fractal_antenna
there is an expression for the lorentz force on a charge in a magnetic field . this expression is based on experimental facts , and the order in the vector product of the charge velocity and the magnetic field is fixed .
you have exact equations for the solution in the related question time it takes for temperature change . here i would add a few comments . it is actually easier if container is thick ! then suppose that all water is at same temperature $t$ and all the air in the frizer is at the same temperature $t_e$ . $t_e$ is constant . if that is so , you can use only fourier 's law to describe how heat $q$ leaves the container $$\frac{\text{d}q}{\text{d}t} = \frac{\lambda a}{d} ( t-t_e ) . $$ $d$ is thickness , $a$ area and $\lambda$ thermal conductivity of the container . knowing that water cools as heat is leaving the container $$\text{d}q = m c \text{d}t , $$ where $m$ is mass and $c$ is specific heat capacity of the liquid , you get rather simple differential equation $$m c \frac{\text{d}t}{\text{d}t} = \frac{\lambda a}{d} ( t-t_e ) , $$ $$\frac{\text{d}t}{ ( t-t_e ) } = \frac{\lambda a}{d m c } \text{d}t = k \text{d}t , $$ which has exponential solution : $$k t = \ln \left ( \frac{t-t_e}{t_0-t_e}\right ) , $$ $$t = t_e + ( t_0-t_e ) e^{-kt} . $$
in short , i think the answers are : 1 ) yes , the approximation $ \langle s_i s_j \rangle \approx \langle s_i \rangle \langle s_j \rangle $ gives you the correct behavior for a spin system with homogenous spin values , but 2 ) there is more to mean field theory than this level of calculation the approximation $$ \langle s_i s_j \rangle \approx \langle s_i \rangle \langle s_j \rangle $$ provides an approximation for determining what the mean spin value is within the ising model , but is insufficient to actually calculate $ \langle s_i s_j \rangle$ , as you have noted . the result of this approximation is a free energy in terms of the mean field $\langle s \rangle = m$ . to get the two-point correlation function , we have to determine the energetic cost for having non-uniform $m ( {\bf r} ) $ . one natural way of doing this is to use the landau expansion , i.e. we write ( see , e.g. chaikin and lubensky chapter 4 ) $$ f = \int d^d x f + \int d^d x \frac{c}{2} |\nabla m |^2 $$ where $f ( x ) = \frac{1}{2} r m^2 + u m^4 + \cdots $ . the free energy from the first term is something that you can get from making the approximation $\langle s_i s_j \rangle \approx \langle s_i \rangle \langle s_j \rangle$ . however , this does not get you the value $c$ , which is essentially a phenomenological term ( you can relate it to the effective line tension between domains in the ising model ) . whenever you see a description of the correlation function in mft , some term like this has been included . there is also an equivalent mft scheme in field theory where the mft can be derived by a saddle-point approximation ( see kardar 's statistical physics of fields , for instance ) . however , i do not remember offhand how to get from an ising model to the appropriate field theory . . . i think this is done with a hubbard-stratonovich transformation , but i do not remember the details .
no . there are two different types of an angular momentum . first is connected with the coordinate representation , so it can be interpreted from classical mechanics point of view . second is not connected with coordinate representation , but it exist in every particle of the free field ( i.e. . , is an own angular momentum ) which you have tested . they are the principal different types of an angular momentum . from the qm position , they both are the eigenvalues of the representations of 3-rotation generator , but first refers to reducible , and second - to irreducible representations . maybe it is more convenient for you to compare the spin and electrical charge . mainly we do not ask about origin of charge and do not interpret it as the result of other quantity . it is independ quantity , and it is existence leads to electromagnetical interaction . also , the existence of spin ( it is value ) leads to some spin interaction ( simplistically can imagine as result of fermi-dirac or bose-einstein statistics . also , we can make an analogy between the quantum spin of particle and an own angular momentum ( classical spin ) of the system of particles . first and second are not connected with motion the particle ( or system ) as whole .
actually , in the first situation ( elastic impact ) , the block a will stand still after the impact and the block b will continue at 5m/s . you have to make the summary of momentum ( quantité de mouvement ) and kinetic energy to solve those problems . for the spring deflection , the maximum deflection will occur at speed 0 , 0 kinetic energy , as all the energy will be store in the spring . you need to calculate the kinetic energy of the system then calculate how much deflection you need to achieve that amount of energy . the energy stored in a spring is define e= . 5*k*x hope it helps .
presumably you are thinking that when c and d annihilate they turn into massless photons so their gravitational field must disappear , hence the problem of what happens to the gravitational energy that a and b acquired in moving away from c and d . the answer is that the gravitational field of c and d is not generated by their mass , but by their energy density . energy gravitates in exactly the same way that mass does . the equivalence between the two is just the formula we have all see a hundred times : $e = mc^2$ . life is actually a bit more complicated than this , because the spacetime curvature that creates gravity is actually proportional to an object called the stress-energy tensor . however the mass/energy density is usually the most important part of this object so at the instant that c and d annihilate , the other two particles feel no change in the gravitational field and their gravitational potntial energy does not change . what happens next is more complicated because the photons created by the annihilation fly off at the speed of light so of course the gravitational field felt by a and b changes . calculating the gravitational field created by a photon is not a trivial business , though googling should find you papers on the subject .
why do you assume that in the case of torques , the torques must cancel out , in my opinion the best way to deal with this problem is your method 1 , but you can solve by taking torques as follows : when you are balancing along y-axis ( calculating x ) $6a × g × 4 + 29a × g × x = 35a × g × 2.5$ here a is mass per unit area , 4 is the x coordinate of centre of mass of small cut off block , x is the coordinate of centre of mass of left over block and 2.5 is the coordinate of original block we have equate the combined torque of cut off and left off pieces with that of original piece , you can do similar operation for y coordinate $6a × g × 5.5 + 29a × g × y = 35a × g × 3.5$ these operations give the same results as those obtained by method 1 , so there is no error .
for resistors $r_1 , r_2 , \dots , r_n$ in parallel , the equivalent resistance $r_e$ is given by $$ \frac{1}{r_e} = \frac{1}{r_1} + \frac{1}{r_2} + \cdots + \frac{1}{r_n} $$ if two resistors with equal resistance $r_1 = r_2 = r$ are in parallel , then this gives $$ r_e^{ ( 2 ) } = \frac{r_1r_2}{r_1+r_2} = \frac{r^2}{2r} = \frac{r}{2} $$ if three identical resistors $r_1 = r_2 = r_3 = r$ are in parallel , then the equivalent resistance is $$ r_e^{ ( 3 ) } = \frac{r_1r_2r_3}{r_1r_2 + r_2r_3 + r_3r_1} = \frac{r^3}{3r^2} = \frac{r}{3} $$ in fact , for $n$ identical resistors one has $$ \frac{1}{r_3^{ ( n ) }} = \frac{n}{r} $$ so that $$ r_e^{ ( n ) } = \frac{r}{n} $$ and therefore the resistance decreases with the addition of each successive resistor in parallel .
actually , this is the basic stuff every mechanics textbook should have . center of mass is the most basic it needs just newton laws : second : $m_i\ddot{\vec{r}_i}= \vec{f}_i$ and third : $\sum_i\vec{f}_i = 0$ summing over i one obtains : $\frac{d^2}{dt^2}\left ( \sum_i m_ir_i\right ) = \sum_i\vec{f}_i = 0$ for the total energy you need those forces to be potential and independent on time : $f_i = -\frac{d}{d\vec{r}_i}u ( \vec{r}_1 , \vec{r}_2 , . . . ) $ then just take the total energy : $e = \sum_i \frac{m|\vec{r}_i|^2}{2}+u$ and differentiate with respect to time : $\frac{de}{dt} = \sum_i m_i ( \ddot{\vec{r}}_i\dot{\vec{r}}_i ) +\sum_{i} \frac{du}{d\vec{ r}_i}\dot{\vec{r}}_i = 0$ finally , for the angular momentum $u$ must be rotational invariant : $u ( r\vec{r}_1 , r\vec{r}_2 , r\vec{r}_3 , . . . ) = u ( \vec{r}_1 , \vec{r}_2 , \vec{r}_3 , . . . ) $ where r is the rotation matrix . consider now very small ( infinitesimal ) rotation : $r\vec{r}_i = \vec{r}_i+\left [ \delta\vec\phi\times\dot{\vec{r}}_i\right ] $ substituting and expanding , one can get : $u+\delta\vec\phi\sum_i\left [ \frac{du}{d\vec{r}_i}\times\vec{r}_i\right ] =u$ which works for every angle $\delta\phi$ , so the following must hold $\sum_i\left [ \frac{du}{d\vec{r}_i}\times\vec{r}_i\right ] =0$ &nbsp ; finally take the angular momentum : $\vec{m} = \sum_i m_i \left [ \vec{r}_i\times\dot{\vec{r}}_i\right ] $ and differentiate with respect to time : $\frac{d\vec{m}}{dt} = \sum_i \left [ \vec{r}_i\times m_i\ddot{\vec{r}}_i\right ] + \sum_i m_i \left [ \dot{\vec{r}}_i\times \dot{\vec{r}}_i\right ] = \sum_i\left [ \frac{du}{d\vec{r}_i}\times\vec{r}_i\right ] =0$ phew . . . you can see that i never used the " pairwise " interactions . and i only needed the basic assumptions about the potential energy . concerning " extra " things that you cans say for your systems -- check the virial theorem
the x-ray diffraction pattern is the fourier transform of whatever is doing the diffracting . if you had an infinite plane of atoms then the spots ( rings in a powder pattern ) would be infinitely sharp because the fourier transform of an infinite wave is a delta function . however a real crystal is the product of an infinite plane with an envelope function , where the envelope is the size of the crystal , so the spot is the convolution of a delta function with the fourier transform of the envelope function . in a powder pattern we have many crystals of differing sizes , so the average fourier transform of the crystal size ends up looking something like a gaussian and the spots have a roughly gaussian profile . re your formula , suppose the average crystal ends up looking like a sphere , i.e. a disk in profile , then it is fourier transform is going to be an airy disk ( but blurred out by the variation in particle size ) . the half angle subtended by the airy disk , $\beta$ , is ( for small angles ) : $$ \beta \propto \frac{\lambda}{d} $$ which is the scherrer equation for small $\theta$ ( i would have to go away and look at the derivation of the scherrer equation to remember why there is a factor of $\cos\theta$ , but for small $\theta$ this factor is approximately $1$ anyway ) . response to comment : the incoming x-rays are scattered by the atoms in the crystal , so each atom acts as an x-ray source . we get the diffraction pattern by summing up the x-rays emitted ( i.e. . scattered ) by all the atoms in the crystal . if you start with one atom then the scattered x-rays will be just be a spherical wave . add a second atom and now the pattern will be the same as the young 's slits experiment . as you add more and more atoms the pattern will tend towards the pattern we expect from a large crystal , however to get an infinitely sharp spot would require an infinite number of atoms . when we have a finite number of atoms the spot will have a finite width . it is a bit like a fourier synthesis ( which is where we came in ) . each atom adds a term to the fourier sum , but to get a perfect transform of the lattice requires contributions from an infinite number of atoms . with a finite number of scatterers the diffraction pattern will only be an approximation to the ft of the lattice .
arriving at the same answer as quantum mechanics for one particular scenario by making a bunch of ad hoc assumptions ( for example - the calculation did not work , so we will make the orbital planes perpendicular ) is not useful . qm allows you to calculate much more than the ground states of atoms . any competing theory - and that paper does not contain anything which could be described as a theory - would have to have the same breadth of applicability as qm
in a word , no . the drift velocity is always very small in common circuits ( ~$10^{-4}cm/s$ ) . in this scenario , when the plates are connected , the electrons still travel slow but all of the electrons along the wire start moving almost at the same time , so even though they move slowly , the electric charge on the plates will vanish quickly . the electrons on the negative plate move into the wire and the positive plate is filled with electrons that were previously in the wire right next to it . this is the difference between the speed of current and the speed of the electrons . current travels very fast ( it is like the speed of sound in the free electron sea ) . electrons drift very slow .
i do not know of any research to find out if skin sunburns faster when wet , though someone did a comparable experiment to find out if plants can be burnt by sunlight focussed through drops of water after the plants have been watered . you need to be clear what is being measured here . the total amount of sunlight hitting you , and a plant , is unaffected by whether you are wet or not . the question is whether water droplets can focus the sunlight onto intense patches causing small local burns . the answer is that under most circumstances water droplets do not cause burning because unless the contact angle is very high they do not focus the sunlight onto the skin . burning ( of the plants ) could happen if the droplets were held above the leaf surface by hairs , or when the water droplets were replaced by glass spheres ( with an effective contact angle of 180º . my observation of water droplets on my own skin is that the contact angles are less than 90º , so from the plant experiments these droplets would not cause local burning . the answer to your question is ( probably ) that wet skin does not burn faster . i would agree with will that the cooling effect of water on the skin may make you unaware that you are being burnt , and this may lead to the common belief that wet skin accelerates burning .
yes , the resistance to the magnetic field is called reluctance . as the magnet moves through a copper coil ( consider circular ) , the change in magnetic field induces current in the coil . due to the current in the coil , another magnetic field is produced in the opposite direction to the magnet moving through the copper coil . also , the magnetic strength of both the field are same . but , the directions are exact opposite .
matt strassler goes into detail with lhc data here : http://profmattstrassler.com/articles-and-posts/largehadroncolliderfaq/whats-a-proton-anyway/checking-whats-inside-a-proton/
here is an explanation by bo danforth : shown in the picture above is a segment of a wurlitzer jukebox bubble tube . in the tube , the bubbles rise as expected , but as they approach the top , odd things begin to occur . instead of remaining the same size as one might think , or increasing slightly from a minute reduction in pressure , they instead decrease in size , in some cases disappearing entirely . the reason for this bewildering sight is that the bubbles do not actually contain air , but are pockets of heated vapor . the bubble tube is a sealed system where the air has been completely removed by a pump , creating a partial vacuum , which causes the liquid to fill the remaining space with vapor . the system is at a pressure where it will change states near room temperature . when this sealed tube is placed next to a heat source at the bottom ( the light bulb ) the liquid near it reaches a boiling point , according to the equation of pv=nrt . as the bubbles of vapor float upwards along the exposed glass and away from the heat source , they slowly shrink as the gas cools down , condensing back into a liquid state .
let 's explain the principle of rotation here from the scratch . say you have the circuit with 1t magnetic field perpendicular to it but the battery is intially switched off . obviously the wheel will not rotate now as there is no moving charge and hence no lorentz force . now when you switch on the battery , electrons will flow through the spokes from outer rim to inner rim . now due to this motion of the electrons , the wheel will start to rotate clockwisely ( just apply lorentz force $\vec{f}=-e ( \vec{v}\times \vec{b} ) $ , where e is positive ) . now the electrons have another velocity due to motion of the wheel in addition to the radial velocity due to the battery . remember this velocity starts from zero as the wheel was initially not rotating . due to this new velocity ( which is increasing from zero ) , the electrons in the spoke will feel a force along the outward direction ( again apply lorentz force due to this new velocity ) . see as the wheel rotational speed increases this force increases , which reduces the velocity of the electrons flowing from outer rim to inner rim . after some time a situation comes when there is no flow of electrons from outer rim to inner rim , which you have described by $\ u_0=u_i$ . at this situation there will be no lorentz force perpendicular the spokes . as there is perpendicular force on the spokes , if you consider torque ( = $\vec{x}\times \vec{f}$ ) , then that will be zero . hence angular momentum $\vec{l} ( =i\vec{\omega}$ ) is constant . " but elsewhere i was told that kirchhoff 's rules do not apply in systems with changing magnetic fields . " but before answering this , at first let me answer the following : " also i am not sure if there still would be an emf induced in stationary conditions since the flow would not change anymore then . " obviously , there will be induced emf in the stationary condition . see lorentz force is still present along radially out on the electrons due to the wheel rotation . see , i do not know why you want to use kvl or something . you have some induced emf in the spoke which is opposing the battery so that no current can flow in the stationary state . nothing else .
start with $$ m{dv\over dt}=b_1 ( v_1-v ) -b_2v . $$ move everything involving $v$ to one side of the equation , and everything involving $t$ ( in this case , just $dt$ ) to the other side . integrate both sides . one side will be just $\int dt$ , or $t+c$ . the other side will be some function of $v$ . algebraically solve the result to get $v$ in terms of $t$ .
in short , yes it completely makes sense to keep searching for dark matter using earth-based direct-detection equipment . even in the case that there is no significant amount of dark matter in the solar vicinity , that is the only area we are currently able to search using direct-detection . so it makes sense that if we search for dark matter ( and we should search for dark matter because finding would be immensely good for mostly all of physics ) we should search for it in the only place we are able to . but that does not mean we should not switch to space-based searching right ? yes and no . no doubt , a space-based detector would have a greater range of testing locations and less ground-based interference sources . but that is about all the benefits . remember that we expect dark matter to be very weakly interacting if it interacts at all . this is why the earth-based detectors are deep underground and/or heavily shielded ( much like neutrino detectors when you think about it ) . in space , the amount of shielding a detector can have is limited because shielding is heavy and heavy is expensive . plus there is a lot more radiation and sources of interference in space than on earth . plus , typically , space-based sensors never begin to rival the sensitivity of earth-based ones ( that is , the technology does not rival it , the sensitivity could be better if space produces less noise in the data , but it will not in this case ) . also , and again , it is bloody expensive to send stuff into space and retrieve the data . for the price of a low sensitivity space-craft , we could build a much better detector on earth . and the amount of dark matter within the areas in space that we could position such a detector is probably about the same as the amount here on earth . but , and this is strictly for the record , let 's assume that there is some small isolated pocket of dark matter that we expect to find in earth 's l2 point around the sun . should we build space-craft and equip detectors and send them there ? yes ! should we stop building or even using detectors on earth ? no . why would we ? what if we get no detection from that pocket ? or what if we do not want to constantly send out expensive new equipment ? we can do both , build space detectors and use earth detectors . so even if space-based detectors might give us a higher chance of detection , it would still make sense to keep using earth-based detectors . so in short , yes it is completely sane ; it makes sense to continue searching for dark matter on earth . we can not position a direct-detector very far from earth . we need a direct-detection eventually , so we should keep looking . and space-based sensors are more expensive , less shielded , harder to maintain , and have more sources for false positives than earth-based sensors . the science may say that we should not expect to or it is not very likely that we will find dark matter around here . but until we have any better options ( and given that we have to search ) , it still makes sense for us to search on earth .
suppose you have some collection of matter that is so dense it has an event horizon where the escape velocity is greater than the speed of light . the escape velocity is obviously due to the strong gravitational field of the matter inside the event horizon , and equally obviously that matter is also pulled by it is own gravity towards it is centre of mass . also obvious is that because the surface of your collection of matter is nearer to the centre of mass than the horizon is , the gravitational pull on it must be even stronger than the gravity at the event horizon i.e. the ( hypothetical ) escape velocity would be even faster than the speed of light . the reason why this situation is not stable is that the matter making up your object cannot resist the force of it is own gravity and is irrestably pulled inwards until it forms a singularity . at that point we have a standard black hole with an event horizon and a singularity at the centre . to understand why the matter within the event horizon cannot avoid being pulled down into a singularity you have to do some maths . if you are interested my answer to why is a black hole black ? gives a hopefully not too scary explanation of the maths . i think there is a semi-plausible way to explain why the matter can not avoid collapsing into a singularity , but do not take this too literally . i have mentioned above that if the escape velocity at the event horizon is the speed of light , the escape velocity inside the event horizon must be faster than light . but all forces , e.g. the electrostatic forces that hold you in shape , propagate at the speed of light . that means inside the event horizon the electrostatic force can hold matter in shape because it can not propagate outwards fast enough . this also applies to the weak and strong forces , and the end result is that no force is able to resist the inwards fall of the matter into a singularity .
for the complete quantum processors , there is not much progress . in 2001 , we can factorize the number $15$ using nmr implementation . now , eleven years later , we can factorize the number $21$ . surely , the first experiment is to demonstrate the practicality of the quantum computer so it uses only the simplest implementation that is not necessary scalable . the development after that are more focus on different components that can be scalable when combined together . there are many different implementations and small variations can change the performance pretty much so it is hard to classify them . the review article of quantum computers has listed the requirements of quantum computer and the pros and crons for each implementation . the following shows the current data for coherence time , number of qubits and storage time . it should be noticed that the 1 . coherence time the table 1 in the review articles " quantum computers " ( 2010 ) gives the following values : $t_2\approx 25 s$ for nmr ( longest one ) $t_2\approx 15 s$ for trapped ions $t_2\approx 0.1 ms$ for infrared photon where $t_2$ is a measure of decoherent . 2 . scale of quantum computing the table 1 in the review articles " introduction to quantum algorithms for physics and chemistry " ( 2012 ) shows that the scale of two implementations : 6 qubits for trapped ions 4 qubits for nmr 2 qubits for quantum optics , but 6 quantum walks steps note that quantum walks is alternative implementation of quantum computer . 3 . storage time in quantum memory the table 1 in the review articles " quantum memories " ( 2010 ) shows storage time and the progress of different implementations of quantum memory .
you are right that the question is making a very crude model of a tennis ball . however , it does capture some important qualitative features of the classical limit that are worth understanding . the intuition that you are supposed to get is that the classical limit corresponds to large $n$ . the key thing to look at is $\delta e_n/e_n$ , the fractional difference in energy between two adjacent energy levels : \begin{equation} \frac{\delta e_n}{e_n} = \frac{ ( n+1 ) ^2-n^2}{n^2} = \frac{2n +1}{n^2} \end{equation} when $n$ is small , the jump to the next energy level is relatively large , and you notice that the tennis ball is quantum mechanical . the tennis ball is not free to move freely because the energy levels of this bound system are discrete , and it can not bounce around however it likes . when $n$ is huge , the jump to the next energy level is very small ( it scales like $2/n$ ) . in that case you can approximate the energy levels as continuous , and you do not notice quantum behavior at all . based on all of this , you should notice that $n=2$ is probably not a good guess for the energy level of a tennis ball . indeed , taking $a=10 {\rm cm}$ and $m=10 {\rm g}$ , i find \begin{equation} e_1 = \frac{\hbar^2 \pi^2}{2 m a^2} \approx 10^{-64} {\rm j} \end{equation} which is muuuuuch less than the average kinetic energy of a tennis ball ! and $e_2$ is only a factor of 4 bigger than this , which is really no better some parting thoughts : ( 1 ) if it bothers you that a tennis ball is a system with many internal degrees of freedom , but here we are modeling it as a single quantum particle with no structure , well congrats because that is definitely a very crude model . however we can say we are only looking at the center of mass of the tennis ball : quantum mechanics allows us to separate out the center of mass for special treatment in a similar way as occurs in classical mechanics . ( 2 ) i said the limit $n\rightarrow \infty$ is the classical limit . we can also phrase it as $\hbar\rightarrow 0$ . to see this , note that $e_1\rightarrow 0$ as $\hbar\rightarrow 0$ , so any particle with finite energy must have $n\rightarrow \infty$ to compensate .
you may simply replace the arguments $ x \to x+vt $ in all of those commutators . the way to prove that is to go to momentum space , where $\rho ( k , t ) = \rho ( k , 0 ) e^{i k v t} $ . proving that about the commutator $ [ \rho ( x , t ) , \psi ( x&#39 ; , t&#39 ; ) ] $ is a little trickier , because we do not know the explicit form of $\psi ( t ) $ , but the way i did it was to use $$ [ \rho ( x , t ) , \psi ( x&#39 ; , t&#39 ; ) ] = u^\dagger ( t&#39 ; ) [ \rho ( x , t-t&#39 ; ) , \psi ( x&#39 ; , 0 ) ] u ( t&#39 ; ) $$ where u is the time evolution operator . this way only the time evolution of the density is needed .
there are two possibilities . water is not leaking up between the log and the dam and flowing over the dam . in this case , the pressure on the bottom of the log depends only on the depth , and the amount of force needed to balance the water is the same as the weight of a log with the density of water , with an extra corner added ; i.e. , the red area in the crude picture below . this would be the weight of water in a volume $l ( \frac{3}{4} \ ! \pi r^2 + r^2 ) $ , where $l$ is the length of the log and $r$ is the radius of the log . ( i expect this was not the way that your teacher intended you to solve the problem . also note that this log is denser than water , which means that it is very difficult to see how it could have reached this position . ) water is leaking , and flowing up between the log and the dam . in this case , you have a very complicated problem in hydrodynamics which you cannot solve without more information . ,
if i recall correctly , most of the material in kittel ( or a solid state introductory lecture ) is about new concepts , e.g. lattices , reciprocal space , band structure , and does not build heavily on quantum mechanics . of course , there is qm beneath it , but you do not need much rigorous qm . basically just the wave mechanics part - solving differential equations , finding a wave function for a given potential , etc . , and a basic understanding of how quantum particles behave . for the later chapters , the advanced maths of a qm ii course could be helpful if you want to follow the derivations , e.g. calculating complex integrals , residue theorem and so on . but i am not sure how much of that is actually in the book and how much was in the lectures i heard . the best would be to just grab the book from a library ( or an ebook ) and browse through it - you will notice pretty soon if there are any new concepts/notations you have to learn about first .
do not forget that the polarization tensors depend on the gauge choice via reference vectors ( call them $q$ , $q'$ ) now you have to check what happen when you chance the reference vectors from $q , q'$ to some new vectors $r , r'$ . the change of the vectors will lead to the new polarization vectors aquire a term proportional to its momentum $p$ . $$\epsilon ( p , r ) ^\mu \sim \epsilon ( p , q ) ^\mu+p^\mu$$ the contraction of the last term with $m_{\mu\nu}$ vanishes , i.e. you have shown gauge invariance . do you also have to show that $m_{\mu\nu}$ contracted into one of its momenta vanishes ?
eamann , i know this is a physics site , and you are looking for a physics answer ; but i noticed that you are in portland , oregon . it seems amazing to me that you can not find a site reasonably close . if nothing is closer , it seems to me that one of your beautiful beaches would make a nice launch site . that is ( iirc ) more than an hour away , but you could combine it with a camping trip . my high school biology professor used to take us out to the gingko petrified forest in eastern wa . along the way we would stop and jump down the sand dunes on the wa side of the columbia river , not too far ( a few hours ) from where you are . there is a lot of desert in eastern wa and eastern or . can not imagine not being able to find a launch site out there . the other thing i would do is network through your own organization , bsa , to find out where other nearby scout troops are going . i hope this helps .
all of the ice core methods of measuring temperature in past millenia from gas trapped in bubbles in ice , and measuring concentrations , depend on the fact that permeability is small even in imperfect containers , as the ice in the glaciers . so the answer to " how long " would depend on the exact materials and geometry and temperature , and it will be in any case the gas will disappear if you wait long enough , and long is more than a few million years . there is of course the sublimation of the molecules of the flask which will eventually make holes no matter how perfect the material , but also the quantum mechanical " tunneling effect " will come to play a role if time is long enough .
as anna pointed out in her comment , it really depends on the system . in general , there is not much you can say except that the minimum of kinetic energy corresponds to the minimum speed and the maximum of kinetic energy corresponds to the maximum speed . there are many systems in which the minimum speed ( and thus the minimum kinetic energy ) is zero , and in those systems , finding the times at which the kinetic energy is a minimum tells you when the object stops . mathematically , if you actually take the derivative of kinetic energy with respect to time , you get $$\frac{\mathrm{d}k}{\mathrm{d}t} = \frac{\mathrm{d}}{\mathrm{d}t}\biggl [ \frac{1}{2}mv^2\biggr ] = m\vec{v}\cdot\vec{a}$$ ( using the nonrelativistic expression for $k$ ) . there are three ways this can be equal to zero : $v = 0$: the object is not moving . this corresponds to a minimum of kinetic energy . $a = 0$: the object is not accelerating ( and by $f = ma$ is also experiencing no net force ) . this corresponds to either constant velocity motion , or a maximum of kinetic energy , in a simple harmonic oscillator for example . $\vec{v}\perp\vec{a}$: the object is experiencing pure centripetal acceleration , which means it is moving with a momentarily constant radius of curvature . this can be either a minimum or a maximum of kinetic energy , in an orbit for example . using the relativistic expression , you get $$\frac{\mathrm{d}k}{\mathrm{d}t} = \frac{\mathrm{d}}{\mathrm{d}t}\bigl [ ( \gamma - 1 ) mc^2\bigr ] = \frac{m\vec{v}\cdot\vec{a}}{\bigl ( 1-\frac{v^2}{c^2}\bigr ) ^{\frac{3}{2}}}$$ from which the same conclusions follow .
inertia does not suddenly " break " in the sense that the axis will remain fixed until some force threshold is reached , and move thereafter ( for that matter , an ice skater cannot change direction by any clever combination of heel-toe maneuvering ) . in reality , any change in the mass distribution of the earth will move the orientation of the axis . small changes in the mass distribution will result in small changes in the orientation of the axis , but a small change is still a change . so , in a technical sense , moving some gravel from a quary to your driveway will change the axis . even walking around will change the axis . however , for a measurable change the mass redistribution would need to be immense . practically this is impossible , but only in the engineering sense , not the theoretical .
it is not clear what you mean by " the r-charge . " if you have a u ( 1 ) r-symmetry , you can make linear combinations of its charges and those of a non-r u ( 1 ) symmetry and get a new r-symmetry . so " values greater than 1/2" are not , generically , special . at a superconformal fixed point , there is a special u ( 1 ) r-symmetry that is part of the superconformal algebra . in this case , r-charges are related to operator dimensions and so are constrained by unitarity bounds . maybe you have this in mind . the literature on $a$-maximization might be the sort of thing you are looking for .
you need to be careful what you mean by a force in general relativity . the usual definition of a force is that you get a non-zero reading on an accelerometer you are holding , but this can lead to some surprising conclusions . to illustrate this suppose you are falling towards some massive body ( with no atmosphere to complicate the issue ) . does the gravity of the massive body create a force ? the answer has to be no , because you are in free fall so you are weightless and any accelerometer you were carrying would read zero . suppose know we give you a harness and tie you to some support fixed wrt the massive body . now you feel a force , and your accelerometer reads non-zero . but this force is due to the fact you have been restricted ( by the harness ) from following the geodesic you would otherwise follow . it is the harness that exerts the force on you ( and you on it ) because it is pulling you away from geodesic motion and therefore imparting a non-zero four acceleration . the force is not due to gravity , it is due to the harness and without the harness there will be no force . incidentally , although it is peripheral to this issue there is a nice calculation of the four acceleration and force in twistor59 's aswer to what is the weight equation through general relativity ? . let 's go back to expanding spacetime . hopefully you will now see why we say that the expansion of spacetime does not create a force . suppose we place you and me at some distance apart in an frw universe and constant comoving position and we wait to see what happens . we will each have an accelerometer so we can tell if we are accelerating . if we now wait the expansion of spacetime will increase the proper distance between us - that is , we will move apart . however because both of us are at constant comoving position we are moving along a geodesic and experience no acceleration . our accelerometers will read zero , which means we feel no force . in this sense the expansion of spacetime does not produce a force . this is exactly analogous to the claim i started out with , that the gravity of a massive body does not create a force either . now suppose we tie ourselves together with a rope . once we have done this we cannot remain at constant comoving position and this means we must be accelerating . our accelerometers would now register a non-zero acceleration towards each other and we had feel a force . any objects we drop will fall away from us . this is exactly analogous to using a harness to support yourself against the gravity of a massive body . it is the rope between us that generates a force not the expansion of spacetime . by now you are probably thinking that this is all a bit of a swindle and i have just redefined what is meant by force to make it zero . well , yes , but this is key to understanding general relativity . we do not often talk about force , but four acceleration is precisely defined in gr and can be calculated as described in the question i linked . when a general relativist talks about force they implicitly mean four acceleration . this does mean they are using the term in a different way to the general public - hence the confusion .
for the drawn reference direction , $i_2 ( ma ) = \frac{6 - v_a}{2}$ , i.e. , you have a sign error .
what force particle mediates electric fields and magnetic fields ? photons , as you have suggested . 1 ) would not that mean that a charged particle ( e . g . an electron or even a polarized h2o molecule ) would constantly be losing energy from sending out photons ? you must describe this process in a quantum field theory . virtual photons emitted by charged particles are reabsorbed in a time consistent with uncertainty principle . hence over some finite amount of time , energy is conserved . 2 ) would not that mean that an electric field is inseparable from a magnetic field , as photons have both - and that one can not have one without the other ? you can already show this in classical electromagnetism - see maxwell 's equations . 3 ) would it be possible , then , to determine the wavelength of magnetic-field-mediating photons ? if so , what is the wavelength - is it random or constant ? the wavelength of a photon is related to it is energy , which is again related to the uncertainty principle . the longer the time borrowed from the vacuum , the lower the energy of the photon , so it has a longer wavelength . hence the wavelength of virtual photons at large distances from the em source is much longer than at short distances . 4 ) how can a photon ( which has momentum ) from one electrically charged particle to an oppositely charged particle cause these particles to be pulled toward each other - or how can a magnetic field cause an electrically charged moving particle to experience a force perpendicular to the source of the magnetic field if a particle with a non-zero mass moving between the two is the mediator of that force ? this become less intuitive depending on your background . richard feynman introduced a trick which offers a way to imagine the process . imagine the photon is emitted between opposite-charge particles in the future and travels ' backward in time'- therefore its momentum minus minus what it really is . this is explained in good detail here . if " virtual photons " are involved , please explain why they work differently from regular photons unlike ' real ' photons ( which have transverse polarisation ) , virtual photons have both transverse and longitudinal polarisations . the energy momentum four vector of the virtual photons , and generally all virtual particles , is not necessarily 0: virtual particles are off mass shell . this means that virtual photons may have non-zero mass - which means that they also have a longitudinal polarisation state . it is important to consider the extra polarisation in your calculations .
before thinking about circuits , let 's think about two conducting spheres of charge that i connect by a wire . before i connect the wire , sphere 1 is at voltage $v_1$ and sphere 2 at voltage $v_2$ , let 's say $v_1&gt ; v_2$ . i find it useful , in terms of thinking about what is going on , to notice that if the spheres are the same size then saying the spheres have different potentials is equivalent to saying that the 2 spheres have different charges residing on their surfaces ( you can justify this by noting that the capacitance of a sphere is determined by its radius ) . now let 's connect the spheres . what will happen ? well , a current will flow in the wire . this will take positive charge off of sphere 1 and deposit it on sphere 2 [ strictly speaking if you want electrons to be charge carriers , then negative charge is flowing from 2 to 1 ; but in terms of thinking about what is going on it is easier to imagine , and mathematically equivalent to say , that positive charges are going from 1 to 2 ] . this in turn changes the voltages on the two spheres ; $v_1$ decreases and $v_2$ increases . the process stops when $v_1=v_2$ . again , if the spheres are the same size this condition is equivalent to the charges on both spheres being equal . ok , now imagine a battery hooked to a resistor and a switch , the simplest circuit imaginable . before we close the switch , terminal 1 is at $v_1$ and terminal 2 is at $v_2$ . at this point , it makes perfect sense to think of each terminal of the battery as being a sphere of charge . then we close the switch , this is like connecting our spheres with a wire . based on our silly model of a battery , you would the voltage between the two terminals of the battery ( ie , $v_1-v_2$ ) to decrease until eventually it reached equilibrium with $v_1=v_2$ . clearly , a battery does not behave like two spheres of charge after the circuit is closed . the whole point of a battery is that it maintains the potential difference between its two terminals . after we close the switch , a little bit of positive charge flows from terminal 1 to terminal 2 by going through the circuit . naively this means that terminal 1 has less positive charge and terminal 2 has more positive charge , so terminal 1 's voltage decreases while terminal 2 's voltage increases . inside the battery , some process takes place to to take the excess positive charge on terminal 2 and put it back onto terminal 1 . whatever this process is , it cannot be electrostatic , because positive charges following the electric field can only ever move from terminal 1 to terminal 2 [ positive charges move from high voltage to low voltage , if the only force is electrostatic ] . the details of what the battery does to maintain the potential difference varies depending of the kind of battery . a conceptually simply example of a battery is a van de graaff generator . in a van de graaff generator , you have a conveyer belt that literally carries the excess positive charge on terminal 2 and deposits it back on terminal 1 , undoing the naive ' equilization process . ' most useful batteries rely on some chemical process to maintain the potential difference . for example , one can use oxidation reactions to do this . the details involve some chemistry ( there is a wikipedia summary at http://en.wikipedia.org/wiki/electrochemical_cell ) , but essentially you put each terminal in a bath of ions , and the chemical energy of the reactions at each terminal [ balancing oxidation and reduction ] forces ionized atoms to carry electrons from terminal 2 to terminal 1 .
in order to understand asymptotic freedom , you need to be aware of the concept of renormalization . since you want a qualitative description , just think of renormalization a modification of the coupling strengths and masses of particles at high energies . this is roughly like pushing a ball through the water ; the harder you push , the more the water sticks around it and the harder it is to move . this can be modeled with newton 's 2nd law $f=ma$ by replacing the mass with a slightly larger mass $m+\delta m$ , and this $\delta m$ depends on the velocity of the ball in the water . ( that discussion can be found in section 3.2 of connes and marcolli , " noncommutative geometry , quantum fields and motives" ) once you have the concept of renormalization , asymptotic freedom is a property the strong force has as you scale the coupling constant to high energy . rather then the coupling getting stronger , it gets weaker . this has major consequences for confinement - that is , bound quarks . at low energies , quarks in bound states are forever bound - it becomes harder and harder to pull them apart the further apart you pull them . at high enough energies ( say , colliding two protons at 7 tev like the lhc ) the quark coupling gets small and quarks are essentially free and unbound . it should be easy to see how this would change the cross section . as a sidenote , only the strong force is asymptotically free . the e/m and weak force become stronger as the energy gets higher . in addition , it is important to realize that we cannot solve problems involving the strong force at low energies ( if you could , the clay mathematics institute would give you $1 million ! ) . once they are at high energies , the strong coupling is weak so qcd acts quite a bit like qed .
from the relativistic covariance of the dirac equation ( see section 2.1.3 in the qft book of itzykson and zuber for a derivation . i also more or less follow their notation . ) , you know how a dirac spinor transforms . one has $$\psi' ( x' ) =s ( \lambda ) \ \psi ( x ) $$ under the lorentz transformation $$x'^\mu= {\lambda^\mu}_\nu\ x^\nu= {\exp ( \lambda ) ^\mu}_\nu\ x^\nu= ( i + {\lambda^\mu}_\nu+\cdots ) \ x^\nu\ . $$ explicitly , one has $s ( \lambda ) =\exp\left ( \tfrac{1}8 [ \gamma_\mu , \gamma_\nu ] \ \lambda^{\mu\nu}\right ) $ . to show reducibility , all you need is to find a basis for the gamma matrices ( as well as dirac spinors ) such that $ [ \gamma_\mu , \gamma_\nu ] $ is block diagonal with two $2\times 2$ blocks . once this is shown , it proves the reducibility of dirac spinors under lorentz transformations since $s ( \lambda ) $ is also block diagonal . such a basis is called the chiral basis . it is also important to note that a mass term in the dirac term mixes the weyl spinors in the dirac equation but that is not an issue for reducibility . while this derivation does not directly use representation theory of the lorentz group , it does use the lorentz covariance of the dirac equation . i do not know if this is what you wanted . ( i am not interested in your bounty -- please do not award me anything . )
quantum mechanics can be reconciled with special relativity to make quantum field theory , but there are some awkward things going on in that marriage . sr treats time symmetrically with position , but in quantum mechanics , position is an operator and time is not . baez at ucr has a nice discussion of that here : http://math.ucr.edu/home/baez/uncertainty.html
yes , the velocity field inside the ellipse is really zero . to convince myself of it i have run a few numeric evaluations of the integral for various $ ( a , b ) $ and $ ( x , y ) $ . here is how can we obtain this result analytically . first , we introduce the elliptical coordinate system with coordinates $\chi$ , $\theta$: $$x = c \cosh \chi \cos \theta , \qquad y = c \sinh \chi \sin \theta , $$ so that the ellipse in question corresponds to a fixed value of $\chi$: $$a = c \cosh \chi_0 , \qquad b = c \sinh \chi_0 , $$ we also assume $a&gt ; b$ . second , we compute the vorticity field . we start with single vortex $$ \mathop{\mathrm{curl}} \left ( \frac{\gamma}{2\pi |\mathbf{x}-\mathbf{x}_0|} \mathbf{e}_\perp \right ) = \mathbf{e}_z \gamma \delta ( \mathbf{x}-\mathbf{x}_0 ) =\mathbf{e}_z \gamma \delta ( x-x_0 ) \cdot\delta ( y-y_0 ) , $$ and integrate the expression over the ellipse : \begin{multline} \mathop{\mathrm{curl}} \mathbf{u} = \mathbf{e}_z \int_0^{2\pi} \delta ( \mathbf{x}-\mathbf{x}_0 ( \theta' ) ) d \theta ' = \mathbf{e}_z \int_0^{2\pi} \delta ( x - a \cos \theta' ) \delta ( y-b \sin \theta' ) d\theta ' = \\= \mathbf{e}_z \int_0^{2\pi} c^{-2} ( \cosh^2\chi-\cos ^2 \theta ) ^{-1} \delta ( \chi-\chi_0 ) \delta ( \theta - \theta' ) d\theta ' = \\ = \mathbf{e}_z c^{-2} ( \cosh^2\chi-\cos ^2 \theta ) ^{-1} \delta ( \chi - \chi_0 ) , \end{multline} where we used the expression for area element in elliptical coordinates : $da = c^2 ( \cosh^2\chi-\cos ^2 \theta ) d\chi\ , d\theta$ . finally , we calculate the stream function $\psi$ with vorticity as a source : $$\delta \psi = - \omega \equiv - \mathbf{e}_z \cdot \mathop{\mathrm{curl}} \mathbf{u} . $$ in elliptical coordinates this reads as : $$ c^{-2} ( \cosh^2\chi-\cos ^2 \theta ) ^{-1} \left ( \frac{\partial^2 \psi}{\partial \chi^2}+\frac{\partial^2 \psi}{\partial \theta^2}\right ) = - c^{-2} ( \cosh^2\chi-\cos ^2 \theta ) ^{-1} \delta ( \chi - \chi_0 ) . $$ notice , that the factors on both sides are equal . so we could look for the solution depending only on $\chi$ , compatible with $\mathbf{u} ( \mathbf{0} ) =\mathbf{0}$: $$ \psi = - h ( \chi - \chi_0 ) \cdot ( \chi - \chi_0 ) , $$ wher $h ( x ) $ is heaviside function . this shows , that inside the ellipse velocity field is indeed zero . it also enables us to calculate the velocity field outside the ellipse , right away we see that the streamlines will be confocal ellipses .
$\hat{p}$ is hermitian and hermitian operators $o$ satisfy , by definition , $$\hat{o} = \hat{o}^\dagger$$ adjoint is not synonym for complex . $\hat{p} = -i\hbar \nabla \rightarrow +i\hbar \nabla^\dagger \rightarrow -i\hbar \nabla =\hat{p}^\dagger$ , but $\hat{p} \neq \hat{p}^*$ .
if the energy difference between two sites separated by $\mathbf{r}$ , then the effective electric field $\mathbf{e}$ between those two sites is given by \begin{equation} \mathbf{e}\cdot\mathbf{r}=\frac{de}{e} , \end{equation} where $e$ is the electron charge .
there are some very interesting subtleties here . let 's analyze the situation very carefully . let 's choose our system to consist of the block , spring , and earth . by choosing the earth and block to be in our system , we will have a change in gravitational potential energy . in the beginning , the ( massless ) spring hangs vertically with a block of mass $m$ attached at the bottom . we could calculate how much the spring is stretched by equating the gravitational and spring forces ( $kx_1=mg$ ) but we will not need this . now , during the pulling process you describe , it is important to note that you are doing positive work on the system , which means that the energy in the system increases . it is tempting to say that the change in energy is zero , but this is not the case for the system we have chosen . let 's use the work-energy theorem to answer your question of where the gravitational potential energy " goes . " $$\underbrace{w_\text{net , external}}_\text{positive}=\delta e_\text{tot}=\underbrace{\delta u_\text{grav}}_\text{negative}+\underbrace{\delta u_\text{elastic}}_\text{positive}$$ yes , the gravitational potential energy decreases . where does it go ? well , the only other term that could ( mathematically ) compensate for this decrease in gravitational potential energy is the increase in elastic potential energy . but be careful with wording here . the spring is not storing gravitational potential energy ; rather , gravitational potential energy was converted to elastic potential energy . as a side note , since the left-hand side of the equation above is positive , the absolute value of $\delta u_\text{elastic}$ is greater than that of $\delta u_\text{grav}$ . so , not only was the gravitational potential energy converted to elastic potential energy , the positive work you did on the system also adds to the increase in elastic potential energy .
in that case , by symmetry , if you take any loop of the coil , for every field line that goes through it , if it goes to the negative pole , there must be another symmetrical that comes from the positive , so the total flux will be $0$ , and constant , so there will be no current induced .
the notation means \begin{align} \sum^3_{\alpha , \beta = 0}{\lambda^\mu_\alpha \lambda^\nu_\beta f^{\alpha \beta}} and = \sum_{\alpha=0}^3\sum_{\beta = 0}^3\lambda^\mu_\alpha \lambda^\nu_\beta f^{\alpha \beta} \\ and = \lambda^\mu_0 \lambda^\nu_0 f^{00}+\lambda^\mu_0 \lambda^\nu_1 f^{0 1}+\lambda^\mu_0 \lambda^\nu_2 f^{0 2}+\lambda^\mu_0 \lambda^\nu_3 f^{0 3}+\text{12 more terms} \end{align} it is not just the " diagonal " terms with $\alpha = \beta$ .
looks like what you are trying to simulate is a catenary . and this comes under statics - a part of classical mechanics . this paper ( so does the wikipedia article ) introduces to the basics of catenary analysis . also see a similar question on mathoverflow .
update 2: it might also be useful add the relation to usual galilean physics . this will probably only make sense after reading update 1 . recall that one can parametrize boosts in ( 1+1 ) by $ [ \cosh ( \eta ) , \sinh ( \eta ) ] $ . as it so happens this equals to $ [ \gamma , \gamma {v \over c} ] $ . the classical physics correspond to the asymptotic case $c \to \infty$ ( i.e. . no limit on the speed of light ) . so this reduces to $\gamma \to 1$ , ${v \over c} \to 0$ and that means $\eta \to 0$ . so in galilean case all boosts degenerate to trivial transformation and this is why time and space separate and addition of velocities starts to work . update 1: having seen kennytm 's strange answer i decided to add some notes to this answer . first , there exists a concept of rapidity . this is a natural variable for parametrization of boosts . first consider a circle . why circle ? because it has to do with rotations and rotations are very similar to boosts . circle is an object given by an equation $x^2 + y^2 = 1$ . you can decide that you will parametrize ( part of ) it by coordinates $ [ x , \sqrt{1 - x^2} ] $ . now what if you want to rotate the circle by some angle ? can you figure out how will the parametrization change ? maybe you can but i assure you this is not pretty . but there is a nicer way . let 's try parametrizing the circle by an angle $\phi$ so that it would become set of points $ [ \cos ( \phi ) , \sin ( \phi ) ] $ and now the rotation by angle $\psi$ corresponds to parametrization $ [ \cos ( \phi + \psi ) , \sin ( \phi + \psi ) ] $ . so our parameter is additive ! you will not find any better parametrization of the circle than this . okay , so we understand circles a little better now . but as already said , rotation in two space dimensions is almost the same thing as boost in ( 1+1 ) space-time dimensions . in the same way that rotations ( around origin ) preserve circles , boosts preserve hyperbolas . so instead of working with $\sin$ and $\cos$ you will work with hyperbolic functions $\sinh$ and $\cosh$ and instead of $\phi$ you will obtain rapidity $\eta$ . now , this only works this nicely in ( 1+1 ) -dimensions . in ( 3+1 ) you will have many more interesting effects ( similarly to like rotations are strange beasts in 3 dimensions as opposed in 2 ) . but it is still true that like the general 3-dimensional rotations are nicely parametrized by an axis and rotation angle , the boosts are nicely parametrized by the direction of boost and rapidity . so if you perform two rotations about the same axis it is the same as rotation by a sum of the angles and if you perform two boosts in one direction it is the same as doing a boost with added rapidities . i will assume you are talking about galilean principle of relativity whereby the velocities transform by pure addition . this concept breaks down when the speeds one is dealing with are too large . speed of light is an extreme case of such speed . then one has to use special relativity and instead consider four-vectors transforming by lorentz transformation . now , this transformations preserve the minkowski length of four-vectors ( in the same way that rotations preserve length of usual vectors ) . the point is that velocity of light corresponds to zero minkowski length and so light moves at the speed of light in every inertial frame . this is the famous einstein 's postulate .
a good analogy for the difference between the two can be given in terms of two other examples of anomalies , that are possibly more familiar . consider a field theory with a global symmetry , take $u ( 1 ) $ for simplicity . at the classical level , the equations of motion lead to the existence of a conserved current ( noether 's theorem ) . at the quantum level , the conservation of the current is valid as an operator equation , namely it is valid in correlators at separated points . the two effects , related but very different in nature , that are referred to as anomalies , are : 1 ) there can exist contact terms in correlators ( i.e. . terms that are non-zero only when two or more of the operators in the correlator are evaluated at the same point ) that do not respect the operator equation . in 4d field theory this typically happens in correlators of three current operators . this is what sometimes is referred to as an ' t hooft anomaly . it does not represent a breaking of the symmetry , because the conservation of the current operator is still valid at separated points , and one still gets a conserved charge . however , it leads to interesting constraints ( the coefficients of such contact terms must match between the uv and ir , if the symmetry is not broken along the rg flow ) . 2 ) there can be quantum effects ( you can think about them as loop corrections , assuming we are in a perturbative setting ) that violate the operator equation even at separated points . in this case the symmetry is broken , much like if you add a term in the lagrangian that does not respect the symmetry . there is no conserved charge any more . the relation between 1 ) and 2 ) can be explained in a slightly refined example . take the global symmetry to be $u ( 1 ) ^2$ . than you could have an anomaly of type 1 ) in a correlator involving one current of the first $u ( 1 ) $ , and two currents of the second $u ( 1 ) $ . now suppose modifying the theory by gauging the second $u ( 1 ) $ , i.e. coupling the current of the second $u ( 1 ) $ to dynamical gauge fields . in the new gauged theory , the first $u ( 1 ) $ is broken by an anomaly of type 2 ) . the divergence of its current is now non-zero , and given by the pontryagin density of the gauge fields of the second $u ( 1 ) $ . the first example of trace-anomaly that you discuss is the analogue of 1 ) , while the second is the analogue of 2 ) , when instead of a global $u ( 1 ) $ we consider the dilatation symmetry . the first example do not represent a violation of the symmetry , it is just the statement that certain contact terms in the correlators with multiple insertions of the energy-momentum tensor are not compatible with the traceless-ness condition . the second example instead is a genuine violation of the symmetry . the analogy with the $u ( 1 ) $ symmetry does not go through when we try to relate 1 ) with 2 ) , because the equivalent of " coupling the current to gauge field " would be introducing dynamical gravity , which brings us away from the domain of quantum field theory . this analogy becomes very concrete in supersymmetric theories . there , the energy-momentum tensor belongs to the same multiplet of the current associated to the so-called r-symmetry . supersymmetry relates the ' t hooft anomaly of this current to the first kind of trace-anomaly that you discuss ( i.e. . they have the same coefficient ) . moreover , when dilatation symmetry is broken by a gauge coupling via the trace anomaly of second type that you discuss , then the current has an anomaly of type 2 ) . again , the trace anomaly and the current anomaly have the same coefficient by supersymmetry .
you are correct , but probably not for the reason you think . at low frequencies the inertia of the electrons is small enough to be ignored , that is we assume that when we apply a voltage the electrons instantaneously accelerate to their steady state velocity . as you increase the frequency of the applied voltage the inertia of the electrons starts to become significant , and above a frequency called the plasma frequency the electrons can no longer move fast enough to respond . for most metals the plasma frequency is around or slightly below that of visible light .
your definitions are in fact those for proper , orthochronous lorentz transformation , not for general lorentz transformations , that is why you are having trouble telling the difference ! ( if it makes you feel any better , yesterday a collegue and i were trying to debug his test setup and two hours of complex testing passed before we two geniusses realised we had not switched the power to a key bit of kit on ! ) a general lorentz transformation is defined by criterion 1 ) alone - it is simply any linear transformation that preserves the quadratic form $t^2 - x^2 - y^2 - z^2$ . the proper , orthochronous transformations are those that belong to the identity connected component $so^+ ( 1 , \ , 3 ) $ of the full lorentz group $o ( 1 , \ , 3 ) $ . that is , the proper , orthochronous transformations are those that can be reached from the $4\times4$ identity matrix by following a continuous path through the lorentz group . equivalently , they are the matrices that are on paths through the lorentz group defined by the differential equation : $$\begin{array}{lcl}\mathrm{d}_s l and = and ( a_x ( s ) \ , j_x + a_2 ( s ) \ , j_y+a_z ( s ) \ , j_z + b_x ( s ) \ , k_x + b_y ( s ) \ , k_y+b_z ( s ) \ , k_z ) \ , l\\l ( 0 ) and = and \mathrm{id}\end{array}\tag{1}$$ where $\mathrm{id}$ is the $4\times 4$ identity , $a_j ( s ) , \ , b ( s ) $ are continuous functions of the parameter $s$ and the $j_j , \ , k_j$ are six matrices $4\times 4$ that span the lie algebra of the lorentz group , i.e. the real vector space of all possible " tangents to the identity " , i.e. all possible values of $\mathrm{d}_s l|_{s=0}$ . one possible set is : $$\begin{array}{lcllcllcl}j_x and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and -1\\0 and 0 and 1 and 0\end{array}\right ) and j_y and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 0\\0 and 0 and 0 and 1\\0 and 0 and 0 and 0\\0 and -1 and 0 and 0\end{array}\right ) and j_z and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 0\\0 and 0 and -1 and 0\\0 and 1 and 0 and 0\\0 and 0 and 0 and 0\end{array}\right ) \\k_x and = and \left ( \begin{array}{cccc}0 and 10 and 0 and 0\\1 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\end{array}\right ) and k_y and = and \left ( \begin{array}{cccc}0 and 0 and 1 and 0\\0 and 0 and 0 and 0\\1 and 0 and 0 and 0\\0 and 0 and 0 and 0\end{array}\right ) and k_z and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 1\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\1 and 0 and 0 and 0\end{array}\right ) \end{array}\tag{2}$$ ( see how the $j_j$ are skew-hermitian , thus have pure imaginary eigenvalues , so that $\exp ( a_j\ , j_j ) $ has stuff like $\sin , \ , \cos$ of an angle and is a rotation matrix , whereas the $k_j$ are hermitian , with purely real eigenvalues , so that $\exp ( b_j\ , k_j ) $ has stuff like $\sinh , \ , \cosh$ of a rapidity and is a pure boost matrix ) . an intuitive description : imagine you are sitting at the console of your spaceship 's " hyperdrive": it has two track balls each with their own levers marked " spin " and " boost " and a set of accelerometers - linear and rotational . your spaceship is initially moving inertially . you roll the trackballs around to set the axis of rotation and direction of boost respectively . when you pull on the levers , the spin lever accelerates the angular speed about the rotation axis , the boost lever accelerates the linear velocity in the boost direction . otherwise put , the " rotate " trackball and its lever set the superposition weights $a_j ( s ) $ of the $j_j$ in ( 1 ) when we use the definitions in ( 2 ) and the " boost " trackball and its lever set the weights $b_j$ of the $k_k$ . you go through a control sequence , ending so that your accelerometers read nought , so that now a set of $x , \ , y , \ , z$ axes attached to your spaceship is moving inertially relative to the beginning frame . the proper , orthochronous transformations are precisely every transformation between the beginning frame and an inertial frame that you can reach with your controls . however , there are other transformations possible that preserve the quadratic form $t^2 - x^2 - y^2 - z^2$ that do not fulfill your criteria 2 . and 3 . but they follow only a " simple " pattern that makes them " not much different " from the identity connected component . a discrete subgroup of the full lorentz group is $\{\mathrm{id} , \ , p , \ , t , \ , p\ , t\}$ with $$p=\text{"parity flipper"} = \mathrm{diag} [ 1 , \ , -1 , \ , -1 , \ , -1 ] ; \\t=\text{''time flipper''} = \mathrm{diag} [ -1 , \ , 1 , \ , 1 , \ , 1 ] $$ with the exception of $\mathrm{id}$ , none of these can be reached from the identity by paths fulfilling ( 1 ) . they belong to different connected components from the identity component $so^+ ( 1 , \ , 3 ) $ . indeed , the identity connected component is a normal subgroup of the full lorentz group $so ( 1 , \ , 3 ) $ and the quotient $o ( 1 , \ , 3 ) / so^+ ( 1 , \ , 3 ) $ is the little group $\{\mathrm{id} , \ , p , \ , t , \ , p\ , t\}$ . so any full lorentz transformation can be represented as a proper orthochronous transformation followed by one of $p , \ , t$ or $p\ , t$ . there are four separate connected components to the full lorentz group . ( an aside : $\{\mathrm{id} , \ , p , \ , t , \ , p\ , t\}$ is the klein " fourgroup": the only possible group of four elements aside from $\mathbb{z}_4$ ) . to sniff out a non-proper or non-orthochronous transformation , you do one of two things : compute the matrix 's determinant . if it is -1 , then you know it has to include one of $p$ or $t$ , so it is not proper or not orthochronous . you can further differentiate the $p$ and $t$ cosets by looking at the $l_0^0$ component of the transformation : the $t$ coset has $l_0^0&lt ; 0$ , since such a transformation swaps the roles of the " future " and " past " ( actually reflects minkowsky vector space in the $t=0$ plane ) . if the determinant is $+1$ , then it may belong to the $p\ , t$ coset of $o ( 1 , \ , 3 ) $ . as in point 1 , the $t$ coset and the $p\ , t$ coset can be recognised as transformations with $l_0^0&lt ; 0$
first of all , the question you are asking is very important and you may master it completely . dimensionful constants are those that have units - like $c , \hbar , g$ , or even $k_{\rm boltzmann}$ or $\epsilon_0$ in si . the units - such as meter ; kilogram ; second ; ampere ; kelvin - have been chosen partially arbitrarily . they are results of random cultural accidents in the history of mankind . a second was original chosen as 1/86,400 of a solar day , one meter as 1/40,000,000 of the average meridian , one kilogram as the mass of 1/1,000 cubic meters ( liter ) of water or later the mass of a randomly chosen prototype , one ampere so that $4\pi \epsilon_0 c^2$ is a simple power of 10 in si units , one kelvin as 1/100 of the difference between the melting and boiling points of water . clearly , the circumference of the earth , the solar day , a platinum prototype brick in a french castle , or phase transitions of water are not among the most " fundamental " features of the universe . there are lots of other ways how the units could be chosen . someone could choose 1.75 meters - an average man 's height - to be his unit of length ( some weird people in the history have even used their feet to measure distances ) and he could still call it " one meter " . it would be his meter . in those units , the numerical values of the speed of light would be different . exactly the products or ratios of powers of fundamental constants that are dimensionless are those that do not have any units , by definition , which means that they are independent of all the random cultural choices of the units . so all civilizations in the universe - despite the absence of any interactions between them in the past - will agree about the numerical value of the proton-electron mass ratio - which is about $6\pi^5=1836.15$ ( the formula is just a teaser i noticed when i was 10 ! ) - and about the fine-structure constant , $\alpha\sim 1/137.036$ , and so on . in the standard model of particle physics , there are about 19 such dimensionless parameters that " really " determine the character of physics ; all other constants such as $\hbar , c , g , k_{\rm boltzmann} , \epsilon_0$ depend on the choice of units , and the number of independent units ( meter , kilogram , second , ampere , kelvin ) is actually exactly large enough that all those constants , $\hbar , c , g , k_{\rm boltzmann} , \epsilon_0$ , may be set equal to one which simplifies all fundamental equations in physics where these fundamental constants appear frequently . by changing the value of $c$ , one only changes social conventions ( what the units mean ) , not the laws of physics . the units where all these constants are numerically equal to 1 are called the planck units or natural units , and max planck understood that this was the most natural choice already 100 years ago . $c=1$ is being set in any " mature " analysis that involves special relativity ; $\hbar=1$ is used everywhere in " adult " quantum mechanics ; $g=1$ or $8\pi g=1$ is sometimes used in the research of gravity ; $k_{\rm boltzmann}=1$ is used whenever thermal phenomena are studied microscopically , at a professional level ; $4\pi\epsilon_0$ is just an annoying factor that may be set to one ( and in gaussian 19th century units , such things are actually set to one , with a different treatment of the $4\pi$ factor ) ; instead of one mole in chemistry , physicists ( researchers in a more fundamental discipline ) simply count the molecules or atoms and they know that a mole is just a package of $6.022\times 10^{23}$ atoms or molecules . the 19 ( or 20 ? ) actual dimensionless parameters of the standard model may be classified as the three fine-structure constants $g_1 , g_2 , g_3$ of the $u ( 1 ) \times su ( 2 ) \times su ( 3 ) $ gauge group ; higgs vev divided by the planck mass ( the only thing that brings a mass scale , and this mass scale only distinguishes different theories once we also take gravity into account ) ; the yukawa couplings with the higgs that determine the quarks and fermion masses and their mixing . one should also consider the strong cp-angle of qcd and a few others . once you choose a modified standard model that appreciates that the neutrinos are massive and oscillate , 19 is lifted to about 30 . new physics of course inflates the number . susy described by soft susy breaking has about 105 parameters in the minimal model . the original 19 parameters of the standard model may be expressed in terms of more " fundamental " parameters . for example , $\alpha$ of electromagnetism is not terribly fundamental in high-energy physics because electromagnetism and weak interactions get unified at higher energies , so it is more natural to calculate $\alpha$ from $g_1 , g_2$ of the $u ( 1 ) \times su ( 2 ) $ gauge group . also , these couplings $g_1 , g_2$ and $g_3$ run - depend on the energy scale approximately logarithmically . the values such as $1/137$ for the fine-structure constant are the low-energy values , but the high-energy values are actually more fundamental because the fundamental laws of physics are those that describe very short-distance physics while long-distance ( low-energy ) physics is derived from that . i mentioned that the number of dimensionless parameters increases if you add new physics such as susy with soft breaking . however , more complete , unifying theories - such as grand unified theories and especially string theory - also imply various relations between the previously independent constants , so they reduce the number of independent dimensionless parameters of the universe . grand unified theories basically set $g_1=g_2=g_3$ ( with the right factor of $\sqrt{3/5}$ added to $g_1$ ) at their characteristic " gut " energy scale ; they may also relate certain yukawa couplings . string theory is perfectionist in this job . in principle , all dimensionless continuous constants may be calculated from any stabilized string vacuum - so all continuous uncertainty may be removed by string theory ; one may actually prove that it is the case . there is nothing to continuously adjust in string theory . however , string theory comes with a large discrete class of stabilized vacua - which is at most countable and possibly finite but large . still , if there are $10^{500}$ stabilized semi-realistic stringy vacua , there are only 500 digits to adjust ( and then you may predict everything with any accuracy , in principle ) - while the standard model with its 19 continuous parameters has 19 times infinity of digits to adjust according to experiments .
first impressions based on a quick read of the preprint : i am out of my depth on this ! i could not tell you if their derivation is correct , but assuming that it is : they do not treat real qcd . they study su ( 2 ) ym without quarks . the authors claim they can do real qcd and get the same result , but this is not demonstrated in the paper ( they defer this to a later publication ) . they derive the gauge invariant infrared finite effective action of this theory at one loop . this is surely an impressive achievement , and an important milestone if it is true , but is probably still far from what a mathematician would accept as a " proof . " with the above caveats they show that monopole-antimonopole condensation is responsible for confinement , and that the tachyons appearing in previous calculations are unphysical . edit : user1504 mentions the millenium prize , which involves pure ym ( with an arbitrary gauge group though ) . this paper definitely does not satisfy the prize conditions : it uses the regular not entirely rigorous ( i.e. . , not axiomatically formulated ) definition of yang-mills theory used by physicists , and it does not prove a mass gap . you need a calculation to all orders to do that to a mathematician 's satisfaction .
equation ( 7a ) describes a system of differential equations : you have one such equation for each value of $j$ , i.e. for each nuclear state $\chi_j$ . and i do not like the way author ( s ? ) write the first term on the left side , i would better write it as $ [ t_{n} + v_{nn} ] \ , \chi_j$ . anyway , the point is that equations in the system ( 7a ) are as we say coupled in a sense that solution of $j$-th equation ( $\chi_j$ ) enters all other equations through this $\lambda_{ji} \ , \chi_i$ terms . that is why we call equations ( 7a ) coupled channel equations and terms $\lambda_{ji} \ , \chi_i$ coupling terms . later to simplify the system we decouple the equations by introducing what is called the adiabatic approximation , in which we neglect all off-diagonal coupling terms . and thus , we call these terms non-adiabatic . why do we call the approximation in which we neglect $\lambda_{ji} \ , \chi_i$ for $j \neq i$ adiabatic ? the term " adiabatic " from ancient greek αδιαβατος ( α - " not " , δια - " through " , βατος - " passable" ) literally means the situation when something " is not passing through " something else . in thermodynamics adiabatic process is a process occurring without exchange of heat of a system with its environment , i.e. a process in which heat is not passing through system enclosure . in quantum mechanics adiabatic refers to a process in which no abrupt transition from one state to another with respect to continuous changes of some parameters happens and thus energy of a system changes continuously with respect to continuous changes of these parameters . and that exactly the picture we have here because if you take a look at ( 7b ) then $\lambda_{ji} \ , \chi_i = 0$ for $j \neq i$ can be interpreted as follows : for some particular $i$-th nuclear state when varying nuclear coordinates no transition from corresponding $i$-th electronic state to any other $j$-th electronic states can happen . p . s . you are reading quite advanced book on the subject . since you do not know the meaning of the word " non-adiabatic " , you would better start from something simpler .
the activity of a radioactive source is measured ( si units ) in bq - becquerels . one bq = 1 disintegration per second . frequently you will see the curie ( ci ) which is $37 \cdot 10^9 bq$ . the energy of radiation depends on the decay scheme . for example , for cs-137 you find ( source : http://upload.wikimedia.org/wikipedia/commons/6/66/caesium-137_decay_scheme-de.svg ) here you see that there are two different ways for the cs-137 to decay : one gives rise to ba-137 with the emission of a $\beta -$ ( electron ) with energy up to 1.17 mev , while the other goes through an intermediate ( metastable ) 137-ba which subsequently decays to stable 137-ba with the emission of a gamma ray with 662 kev of energy . if you want to include the beta energy in your " intensity " calculation you will find that extremely hard since there is a lot of self-absorption ( betas do not travel very far in matter ) . if you are only interested in the gamma radiation , then you can find the total energy per second ( emitted into $4\pi$ steradians ) as $$energy = activity ( bq ) * 0.95 * 662 * 10^3 * 1.6 * 10^{-19} j$$ at a given distance $r$ , the total area is of course $a = 4\pi r^2$ so you can divide energy by area and get intensity . it would be unusual to express this in $w/m^2$ . when you look at radiation damage , you actually do use a measure of energy . the gray is the si unit , expressed as $j/kg$ - in other words , the amount of energy deposited per kg of material absorbing . that depends not only on the radiation emitted , but also on the material receiving . materials with higher z will typically stop more energy per unit mass and thus have higher values for radiation dose . note that the gray is used for non-living materials . for biological materials , the sievert ( sv ) is preferred since it represents " damage " and not just " absorbed energy " . for a more complete explanation see for example http://en.wikipedia.org/wiki/gray_(unit)
i made a very simple mistake of plotting specific heat and not specific heat per spin . it is specific heat per spin that scales as $l^{\alpha/\nu}$ . and hence , the actual value from the data of my previous simulations is $2.44-2 = 0.44$ . using system sizes $10$ and $12$ , one in fact gets a value $0.3$ .
in general , quantum numbers are labels of irreducible representations of the relevant symmetry group , not primarily eigenvalues of an otherwise simply defined operator . but for every label that has a meaningful numerical value in every irreducible representation , one can define a hermitian operator having it as an eigenvalue , simply by defining it as the sum of the projections to the irreducible subspaces multiplied by the label of this representation . it is not clear whether such an operator has any practical use . this also holds for the spin . however , one can define the spin in a representation independent way , though not via eigenvalues . the spin of an irreducible positive energy representation of the poincare group is $s= ( n-1 ) /2$ , where $s$ is the smallest integer such that the representation occurs as part of the foldy representation in $l^2 ( r^3 , c^n ) $ with inner product defined by $~~~\langle \phi|\psi \rangle:= \displaystyle \int \frac{dp}{\sqrt{p^2+m^2}} \phi ( p ) ^*\psi ( p ) $ . the poincare algebra is generated by $p_0 , p , j , k$ and acts on this space as follows . $p$ is multiplication by $p$ , $~~~p_0 := \sqrt{m^2+p^2}$ , $~~~j := q \times p + s$ , $~~~k := \frac{1}{2} ( p_0 q + q p_0 ) + \displaystyle\frac{p \times s}{m+p_0}$ , with the position operator $q := i \hbar \partial_p$ and the spin vector $s$ in a unitary irreducible representation of $so ( 3 ) $ on the vector space $c^n$ of complex vectors of length $n$ , with the same commutation relations as the angular momentum vector . the poincare algebra is generated by $p_0 , p , j , k$ and acts on this space irreducibly if $m&gt ; 0$ ( thus givning the spin $s$ representation ) , while it is reducible for $m=0$ . indeed , in the massless case , the helicity $~~~\lambda := \displaystyle\frac{p\cdot s}{p_0}$ , is central in the universal envelope of the lie algebra , and the possible eigenvalues of the helicity are $s , s-1 , . . . , -s$ , where $s= ( n-1 ) /2$ . therefore , the eigenspaces of the helicity operator carry by restriction unitary representations of the poincare algebra ( of spin $s , s-1 , . . . , 0$ ) , which are easily seen to be irreducible . the foldy representation also exhibits the massless limit of the massive representations . edit : in the massless limit , the formerly irreducible representation becomes reducible . in a gauge theory , the form of the interaction ( multiplication by a conserved current ) ensures that only the irreducible representation with the highest helicity couples to the other degrees of freedom , so that the lower helicity parts have no influence on the dynamics , are therefore unobservable , and are therefore ignored .
these are chunks of rock that existed as part of the crust of mars but were ejected into interplanetary space by a very powerful impact and then eventually impacted the earth . it was not until we had sent probes to mars and began to understand the composition of martian minerals and atmosphere that we started realizing some of the meteorites we had already found were from mars . the clincher has been the analysis of trapped gasses within meteorites . nearly 100 martian meteorites are known to have been found . interestingly , most martian meteorites fall into only 3 mineralogical categories ( shergottites , nakhlites , and chassignites , or snc meteorites ) and had been identified as being unusual as meteorites even before they were confirmed to have originated on mars . similarly , there are likely some number of earth meteorites on mars .
if the uniformity were somehow perfectly smooth , all we would have is a very tenuous nebula of mostly hydrogen , with some helium and a little bit of lithium . without the irregularities to start off star formation or any of those activities , that is all we had have . i am not about to run a back of the envelope calculation , but i would think that the overall density would not be much greater than current interstellar space . going further back , we would possibly be in a state of having no baryonic matter at all . while delving into areas we do not really know that much about ( but we are exploring ) , one school of thought says that because of the irregularities , somehow regular matter won out over anti-matter . as to the massive black hole idea , it would most likely not be the case because of cosmic inflation . the space would still be expanding so that a collapse would not be feasible in that epoch of the universe . and the question is pretty nonsensical in that nothing is perfect . an irregularity is pretty much a certainty given we are dealing with particles that have mass .
how does an electrical field really work ? there are two formulations that describe the known data on electric and magnetic fields . a ) the classical electromagnetic theory ruled by maxwell 's equations . this works well in describing the macroscopic data , of which the electric field is a component . b ) the quantum mechanical formulation that leads to an explanation of how fields are built up , which is necessary to explain effects like the " photelectric effect " , the behavior of atoms and molecules , the internals of atoms and molecules . for a ) the electric field is a fundamental component of the behavior of matter . for b ) the electric field is built up coherently by innumerable virtual particle exchanges , mainly virtual photons , between the generators of the field and the detectors of its existence , so it is not fundamental . charge is fundamental in this framework , and charge is quantized ( +/-1/3 , +/-2/3 , +/-1 ) in absolute value electron charge units . that is why it is a quantized theory of the world . how come that an electron can exert a force on another electron without physical contact ? for a ) it is an action at a distance , the field of the electron exerts a force on other charged matter ; similar to classical newtonian gravity , where the masses exert a force on each other . what is it in an electron that creates the field , where does the energy for doing the work come from ? it is the charge of the electron . when we are talking of electrons we are really in the realm of b ) , quantum mechanics , because its size is of the size where quantum mechanics has to be used to understand the data . in qm language the electron , when looked at individually , is continually exchanging virtual photons with the boundaries of its containment . virtual means that energy and momentum are not conserved because nothing real is exchanged with the other electrons/ions except an information " i am here " . when many electrons are involved , the surface of a charged metal sphere for example , the collective electric field is built up out of those exchanges . the energy was supplied in this case by the experimenter who provided work to separate the electrons from the rest of the molecules , turning them into ions . either by the triboelectric effect or the classical generators of electricity , using magnetic fields and providing a current of electrons in metals . ultimately it is kinetic energy turned into electric energy . ( actually sun energy stored in fuels or water works , turned into kinetic energy . . . ) now magnetic fields , used to generate most of our electricity , are a bit of a different story , but similar and again needing quantum mechanics to be understood . in scenario a ) they also are fundamental .
i am guessing you really just want the appropriate equations rather than an in depth treatment of relativistic acceleration . in that case read john baez 's article on the relativistic rocket . in particular , the velocity measured by an observer watching the rocket is : $$ v = \frac{at} {\sqrt{1 + ( at/c ) ^2}} $$ and the distance the rocket has travelled is : $$ d = \frac{c^2}{a} \left ( \sqrt{ ( 1 + ( at/c ) ^2} - 1\right ) $$ john baez does not give velocity as a function of distance , but it should not be hard to use the second equation to substitute for $t$ in the first equation . that would give you velocity as a function of distance . incidentally , as monster truck points out , the equation for distance is $v^2 = 2ad$ not $v = 2ad$ , though i am guessing that is just a typo . note that the variable $t$ in these equations is the time measured by the stationary observer watching the rocket . because of the time dilation that happens at speeds near the speed of light , the time measured by the stationary observer and the the time measured by the crew of the rocket will not be the same . because of this the equations give the velocity and distance of the accelerating rocket as measured by the stationary observer . the experience of the crew inside the rocket would be different . in the unlikely event you feel the urge to find out how these equations are derived , borrow a copy of gravitation by misner , thorne and wheeler . the equations for the relativistic rocket are derived in chapter 6 . however be warned that it is not easy going .
say your friend harry stands on the left end of the boxcar . harry says " when the light hit the right side of the boxcar , i was just scratching my nose . at that moment , exactly $t'$ minutes had elapsed " . you ( standing on the platform ) are going to say that it took $t_1$ minutes till harry scratched his nose and $t_2$ minutes till the light hit the right end of the boxcar . but $t_1$ will not equal $t_2$ . that is because unlike harry , you do not believe that the nose-scratch was simultaneous with the light hitting the right end of the boxcar . the factor $\gamma$ is not supposed to measure $t'/t_2$ . it is supposed to measure $t'/t_1$ . this , i think , is what is misleading you . if you stare long enough at the space-time diagram , things like this always become clear : the vertical axis is your worldline ; the blue lines are the worldlines of the left and right sides of the boxcar ; the red lines are lines of simultaneity for the denizens of the boxcar . ( these red lines are supposed to be equally spaced ; sorry if my artistry is imperfect on this point . ) the gold lines are the path of the light ray . the distances along the axes are measured taking the speed of light to be $1$ . the green point is where harry scratches his nose . if you drew a horizontal line from here over the the vertical axis , it would hit at point $t_1$ . the yellow point is where the light ray hits the far end of the boxcar . i have drawn a horizontal line from here to the vertical axis which hits at point $t_2$ . you can see that $d'/d=t'/t_1$ , as it should be . this is the ratio you are calling $\gamma$ . but $t'/t_2$ can not be equal to $\gamma$ ( as you have noted ) , which is okay because it is not supposed to be . ( ps--i hope the labels are clear . i have marked distances rather than points , so that the $t_2$ on the left , for example , is the distance from the origin to the black dot above it , so that black dot is at the point $t_2$ . )
dear robert , the answer to your question is trivial and your statement holds pretty much by definition . you know , the green 's functions contain terms such as $$g ( \omega ) = \frac{k}{\omega-\omega_0+i\epsilon}$$ where $\epsilon$ is an infinitesimal real positive number . the imaginary part of it is $$-2\im ( g ) = 2\pi \delta ( \omega-\omega_0 ) $$ so it is the dirac delta-function located at the same point $\omega$ which determines the frequency or energy of the particle species . at $\omega_0$ , that is where the spectrum is localized in my case . if there are many possible objects , the $g$ and its imaginary part will be sums of many terms . this delta-function was for a particle of a well-defined mass ( or frequency - i omitted the momenta ) . if the particle is unstable , or otherwise quasi- , the sharp delta-function peak will become a smoother bump , but there is still a bump . because you did not describe what you mean by " peak " more accurately , i can not do it , either . it is a qualitative question and i gave you a qualitative answer . cheers lm
they basically measure the intensity of the infrared blackbody radiation in some wavelength region and calculate the temperature needed to give that intensity according to planck 's law .
first we need to discuss what is meant by a boundary . consider a rectangular sheet of paper . the boundary of the whole sheet is a rectangle , the edge of the paper . it is where the paper ends . now draw a circle on the sheet of paper . the circle is the boundary of a disk-shaped region of paper inside the circle , but the paper does not stop there . this boundary has an outside as well as an inside . the firewall , if it existed , would be like the boundary of the circle . but holographic duality involves the edge of the paper . the dual field theory describes everything on the sheet of paper , not just what happens inside the circle .
$$ \int_0^\infty \text{d}r\frac{r^{ ( 2a+d-1 ) }}{ ( r^2+d ) ^b}= \frac{1}{2}\int_0^\infty \text{d}x\frac{x^{ ( a+d/2-1 ) }}{ ( x+d ) ^b}= \frac{1}{2}\int_0^\infty \text{d}y\frac{y^{ ( a+d/2-1 ) }}{ ( y+1 ) ^b}d^{a-b+d/2} , $$ with $x=r^2$ and $y=x/d$ . this integral is a beta function ( see eq . ( 22 ) of wolfram mathworld ) : $$ \int_0^\infty \text{d}y\frac{y^{ ( a+d/2-1 ) }}{ ( y+1 ) ^b} = b ( a+d/2 , b-a-d/2 ) . $$ i do not know where the factor $ ( 4\pi ) ^{d/2}$ comes from , you will have to check that .
general remarks . in general , you cannot " derive " a representation of a given group $g$ on the objects you are considering , but there are some really standard definitions of certain group representations which are given special names like " scalar , " " vector , " and so on . however , given the representation of a lie group $g$ , this induces a representation of its lie algebra $\mathfrak g$ , and determining an explicit formula for this lie algebra representation is precisely what we do when we find the so-called " infinitesimal generators " of the corresponding group representation . an example . $\mathrm{so} ( 2 ) $ let $c^\infty ( \mathbb r^2 ) $ denote the vector space of smooth functions on the plane $\mathbb r^2$ . the scalar representation $\rho$ of $\mathrm{so} ( 2 ) $ acting on $c^\infty ( \mathbb r^2 ) $ is defined as \begin{align} ( \rho_0 ( r ) \phi ) ( \mathbf x ) = \phi ( r^{-1}\mathbf x ) . \end{align} for each $\phi\in c^\infty ( \mathbb r^2 ) $ and for each $r\in\mathrm{so} ( 2 ) $ . what the heck is going on here ? well , notice that this can also be written as follows : \begin{align} ( \rho_0 ( r ) \phi ) ( r\mathbf x ) = \phi ( \mathbf x ) \end{align} so this definition encapsulates the intuitive idea that the transformed field $\rho ( r ) \phi$ evaluated at the transformed point $r\mathbf x$ agrees with the untransformed field $\phi$ evaluated at the untransformed point $\mathbf x$ . in physics , it is common to see " primed " notations for the transformed field and transformed point ; \begin{align} \rho_0 ( r ) \phi = \phi ' , \qquad r\mathbf x = \mathbf x ' \end{align} in which case the definition of the scalar representation can be written as \begin{align} \phi' ( \mathbf x' ) = \phi ( \mathbf x ) \end{align} this probably looks familiar . so basically the " invariance " that is happening is that the value of the field does not change provided the transformed field is evaluated at the transformed point . infinitesimal generators . to find the infinitesimal generators of a given representation , we are really just trying to find a certain representation of the lie algebra of the group . this lie group representation $\rho$ naturally induces a lie algebra representation $\bar \rho$ as follows : \begin{align} \bar \rho ( x ) = \frac{d}{dt}\rho ( e^{tx} ) \big|_{t=0} \end{align} so , for the $\mathrm{so} ( 2 ) $ example , we know that the lie algebra $\mathfrak{so} ( 2 ) $ is generated by the single element \begin{align} j = \begin{pmatrix} 0 and -1 \\ 1 and 0 \\ \end{pmatrix} , \end{align} and we can determine how this element is represented in representation induced by the scalar representation defined above as follows : \begin{align} ( \bar\rho_0 ( j ) \phi ) ( \mathbf x ) and = \frac{d}{dt}\phi ( e^{-tj}\mathbf x ) \big|_{t=0} \\ and = \frac{d}{dt}\phi ( x-ty , y+tx ) \big|_{t=0} \\ and = -y\partial_x\phi ( x , y ) + x\partial_y\phi ( x , y ) \\ and = ( -y\partial_x + x\partial_y ) \phi ( \mathbf x ) \end{align} in other words , in the scalar representation , the generator of rotations on the plane is represented by a differential operator ; \begin{align} \bar\phi_0 ( j ) = -y\partial_x + x\partial_y . \end{align} this same procedure can be extended to find infinitesimal generators of other representations as well , like the vector representation $\rho_1$ of $\mathrm{so} ( 2 ) $ which is defined to act on vector fields $\mathbf v$ on the plane as follows : \begin{align} ( \rho_1 ( r ) \mathbf v ) ( \mathbf x ) = r\mathbf v ( r^{-1}\mathbf x ) \end{align} by the way , you might find the following links interesting and/or helpful as well : tensor operators representations of lie algebras in physics differential realizations of certain algebras generators of poincare groups idea of covering group unitary spacetime translation operator rigorous underpinnings of infinitesimals in physics
your understanding that a change in rotational velocity requires a net torque ( calculated around any convenient axis ) is correct . the only forces acting are gravity , the normal force from the ramp , and the force of friction between the ramp and the block . choosing an axis across the ramp through the center of mass of the block makes life simpler ; the force of gravity ( all of it ) acts through this point , and thus exerts no torque . next , consider the friction force , that is given by $mg \sin \theta$ . this acts up the ramp , at the point of contact of the ramp and block , and thus exerts a torque around the center of mass depending on the dimensions of the block . ( tall blocks are more tippy than squat blocks ) in this case , the torque is a clockwise one . in order for there to be no rotation , there must be a counter-clockwise torque about the cofm . this means that the diagram is wrong ; the normal force no longer is distributed uniformly along the base , and cannot be ignored as acting through the cofm . in fact , the normal force shifts towards the front of the block as the ramp slope increases , and when it reaches the front corner , the rear corner lifts and the block tumbles . . .
i hope you know that intensity $ ( i ) $ of light at any point on the screen due to interference in the young 's double slit experiment can be given as $$a^2=i=a_1^2+a_2^2+2a_1a_2\cos{\phi}$$ where $a_1 , a_2$ are the amplitudes of the light waves with constant phase difference of $\phi$ , $a$ is the amplitude of the resultant displaement at the point on the screen . for simplicity , we can assume that intensity of light to be equal to square of the amplitude as given above . thus , $$i_{max}=a_1^2+a_2^2+2a_1a_2 ( 1 ) = ( a_1+a_2 ) ^2$$ $$i_{min}=a_1^2+a_2^2+2a_1a_2 ( -1 ) = ( a_1-a_2 ) ^2$$ therefore , $\frac{i_max}{i_min}=\frac{ ( a_1+a_2 ) ^2}{ ( a_1-a_2 ) ^2}=\frac{25}{9}$ thus , $a_1+a_2=5 , a_1-a_2=3$ $a_1+ ( a_1-3 ) =5=2a_1-3$ thus , $a_1=8/2=4 , a_2=1$ the intensity of light due to a slit ( source of light ) is directly proportional to width of the slit . therefore , if $w_1$ and $w_2$ are widths of the tow slits $s_1$ and $s_2$ ; $i_1$ and $i_2$ are intensities of light due to the respective slits on the screen , then $$\frac{w_1}{w_2}=\frac{i_1}{i_2}=\frac{a_1^2}{a_2^2}=\frac{4^2}{1^2}=16$$
in some sense this question is silly . you have made a measurement . the value you measured was $l \pm \delta l$ . you found $l=15.67\textrm{ mm}$ and $\delta l = . 01 \textrm{ mm}$ . but really you should be thinking of $l$ and $\delta l$ as lengths , independent of any representation in terms of a specific choice of units . now thinking of it this way , since $\delta l = 0.01 \textrm{ mm}$ , it necessarily follows that $\delta l = 0.00001 \textrm{ m}$ . this is just two different ways of representing the same length using different units . so i say that this question is a little " silly " because you do not need to know anything about uncertainties to know that $0.01 \textrm{ mm} = 0.00001 \textrm{ m}$ . the one caveat is that it is preferable to make a consistent choice of units when giving the value of $l\pm \delta l$ . so for example it is ok to say $15.67 \pm . 01 \textrm{ mm}$ or $0.01567 \pm 0.00001 \textrm{ m}$ , but writing $ 15.67\textrm{ mm} \pm 0.00001 \textrm{ m}$ or $0.01567 \textrm{ m} \pm 0.01 \textrm{ mm}$ is considered a sub-optimal way of giving the value . i think if you read it , you will consider it to be more confusing as well . however , it is still logically consistent .
this problem originated with passengers using electronics ( they call them ped 's - portable electronic devices ) during flight . while all consumer electronics have to be qualified by a regulatory body ( fcc , etc . ) to prove they do not emit harmful interference , this does not mean they emit no interference especially to high gain sensitive navigation equipment . " the first national committee that investigated interference by passenger-carried peds was created in the early 1960s . its activities were initiated by a report that a passenger-operated portable fm broadcast receiver caused an airplane navigation system to indicate that the airplane was off course by more than 10 deg . the airplane was actually on course and , when the portable receiver was turned off , the malfunction ceased . a final report from this committee , rtca do-119 , was issued in 1963 and resulted in the revision of the faa federal aviation regulations ( far ) by establishing a new rule ( far 91.19 , now 91.21 ) , which states that the responsibility for ensuring that peds will not cause interference with airplane navigation or communication systems remained with the operator of the airplane . " -- from boeing this reference also has some incidents which i hightlight here : 1995 , 737 a passenger laptop computer was reported to cause autopilot disconnects during cruise . 1996/1997 , 767 over a period of eight months , boeing received five reports on interference with various navigation equipment ( uncommanded rolls , displays blanking , flight management computer [ fmc ] / autopilot/standby altimeter inoperative , and autopilot disconnects ) caused by passenger operation of a popular handheld electronic game device . 1998 , 747 a passenger’s palmtop computer was reported to cause the airplane to initiate a shallow bank turn . one minute after turning the ped off , the airplane returned to " on course . " now this " all electronics off " rule has become legacy and broadly applied . there are a mix of opinions about how relevant it is today . but the fact remains that while manufacturers do a decent job of limiting rfi ( radio frequency interference ) there is still a good chance that the very sensitive radios on board aircrafts could have a problem with one of the rfi emissions . and due to the nature of the aircraft radios it is almost impossible to test every possible interference scenario . these radios do a lot of frequency mixing , amplification and filtering . if you have ever done a detailed spurious analysis on a mixer , you will know that each time you mix a single you create a tremendous number of possible interference problems . there has been recent studies showing rfi issues but with a very low probability ( 1:1,000,000 ) of harmful interference . ( sorry , you have to pay $63 to read the report ) . so for everyone 's safety the " all electronics off " rule during critical takeoff/landing has remained in effect . for a good overview , i suggest you read the first link in my answer from boeing .
dust . the light in this band originates from un-resolved stars and other material that lie within the galactic plane . dark regions within the band , such as the great rift and the coalsack , correspond to areas where light from distant stars is blocked by interstellar dust . cosmic dust consists of small particles of matter from the periodic table which have the usual properties of matter and can be detected in various ways , as the article linked extensively discusses .
this is a pretty vague question , but i take it that you are groping for some " physical significance " . the clearest one is that the logarithm is the inverse of the exponential function $x\mapsto e^x$ which itself arises whenever the rate of quantity 's variation is equal to or proportional to that quantity , a fairly common statement describing physical processes . for example : rates of chemical reactions , radio active decays , attenuation of light or other em radiation through mediums all follow such laws . given this " physical definition " it follows then that the inverse function is simply that given by $x\mapsto \int_1^x \frac{\mathrm{d}z}{z}$ and then this definition is broadened into the punctured complex plane $\mathbb{c}\sim \{0\}$ by analytic continuation . moreover the functions $\exp$ and $\log$ defined in this way have particularly simple taylor series ( the former is universally convergent , the latter convergent in an open unit radius circle about $z=1$ ) that make their definitions relatively easy to broaden to objects other than numbers such as matrices , operators and so forth . the idea of a rate of a quantity 's variation being proportional to that quantity is further generalized in operator equations and , in particular , in the theory of lie groups , where $\exp$ and its inverse $\log$ play central roles in mapping neighbourhoods of the group 's identity to and from the " lie algebra " , i.e. the space of the linear transformations that play the role of generalized " rates of change " - these can now be complex numbers , quaternions or in general square matrices ( for the lie algebra they can always be thought of as square matrices - ado 's theorem - but this is not always so for the lie group ) . again , it is the natural base $e$ logarithm that falls from the definitions by dint of its taylor expansion around the identity . the theory of lie groups , with its fundamental reliance on $\exp$ and $\log$ , plays many important roles in physics and the sciences in general . in an even more generalized setting , the schrödinger equation is also a generalized " rate of change proportional to the quantity " equation , as are the descriptions of flows and the exponential map defining geodesics in differential geometry . lastly , since you ask about thermodynamics and the formula graven on boltzmann 's headstone , the logarithm is the grounding of the natural encoding of the idea that numbers of possibilities ( volumes of phase spaces ) multiply , whereas intuitively the corresponding " entropies " , as extensive protperties of thermodynamic systems should add . whilst it should be clear that the logarithm 's base does not matter for this definition ( indeed information theorists choose base 2 logarithms to write informational entropies in bi nary digi ts or bits ) , one could argue that the natural base $e$ logarithm that is the " prototypical " isomorphism ( which is what boltzmann 's intuitive idea is all about ) between the group of reals and addition and the group of strictly positive reals and multiplication that arises from the lie theoretical idea of mapping the lie group $ ( \mathbb{r}^+\sim\{0\} , \ , \times ) $ onto its lie algebra $ ( \mathbb{r} , \ , + ) $ what is the probability of all this happening ? it is precisely equal to unity : for the above ideas are how we define the natural logarithm ( i.e. . as the ones defined above as opposed to logarithms with another base or even indeed other functions altogether ) .
i am not quite sure what is your doubt so i will try to describe this as precise as possible what i think the doubt you might have . q . momentum == trajectory . a . yes . . . kind-of ! momentum does describe the trajectory of the free-particle if you knew the starting position in transverse plane ( say $ ( x_o , y_o ) $ ) . as a matter of fact ( in technical words ) momentum is generator of position . if you are okay with this then no need to read forward you could just let this go , but if you are new to quantum then i would suggest that you read further as it might give a clear pathway to understanding of the problem itself which get blurred by weird orthodox interpretation still taught in classrooms . in simple words when you try to find the position of photon 's position in xy-plane ( assuming that light is traveling along z-axis ) , then it is momentum becomes uncertain " in xy-direction only " ( well from energy conservation you might argue that z-momentum as well becomes uncertain , well yes but the effect is more pronounced in x-/y-direction as prior to slit the momentum was exactly zero ) . the momentum ( hence the trajectory in space ) was perfectly known in before photon encounters slit . later the photon get dispersed after it has passed through slit purely due to the quantum effects . following is just mathematical details of " what happens " from " before " to " after " . lets say you start with plane wave , $|\psi_{before}&gt ; = |\vec{p}&gt ; $ . now when photon are passing through the slit measurement is being carried by the atoms making the slit and they are making observation in position basis so ( orthodox interpretation suggest ) we better express the state-ket in terms of position basis $|x&gt ; $ . since the slit is only making the measurement on x and y position of photons we expand the state in $|x&gt ; $ and $|y&gt ; $ . $$ |\psi_{before}&gt ; = \frac{1}{2\pi \hbar^2} \int dp_x dp_y e^{i ( p_x x + p_y y ) /\hbar} |x , y&gt ; $$ it basically means that we have a superposition state i.e. prior to measurement by slit the transverse position-wise incoming photon is dispersed though-out the measuring plane . say the state of photon ( after it has passed through slit ) is $|\psi_{after}&gt ; = |x_\circ , y_\circ&gt ; $ . now we have to evolve it to screen in unitary fashion . with known hamiltonian , $ h = \frac{p^2}{2m} $ it can be seen easily that $ &lt ; x , y|u|x_\circ , y_\circ&gt ; \neq 0 \rightarrow $ the beam spreads . also trajectory is just a colloquial term it sometime helps connecting the quantum mechanics to classical notions , but you might have realised that it is quite confusing to use . that is why you should proceed with more concrete way of expressing the quantum-ideas .
meteoroids come in a very large range of sizes , from specks of dust to many-kilometer-wide boulders . explosions like that of the chelyabinsk meteor are only found meteors that are larger than a few meters in size but smaller than a kilometer . though the details are argued endlessly by those who study such phenomena ( it is very hard to get good data when you do not know when/where the next meteor will occur ) , the following qualitative description gets much of the important ideas across . the basic idea is that the enormous entry velocity into the atmosphere ( on the order of $15\ \mathrm{km/s}$ ) places the object under quite a lot of stress . the headwind places a very large pressure in front of it , with comparatively little pressure behind or to the sides . if the pressure builds up too much , the meteor will fragment , with pieces distributing themselves laterally . this is known as the " pancake effect . " as a result , the collection of smaller pieces has a larger front-facing surface area , causing even more stresses to build up . in very short order , a runaway fragmentation cascade disintegrates the meteor , depositing much of its kinetic energy into the air all at once . this is discussed in [ 1 ] in relation to the tunguska event . that paper also gives some important equations governing this process . in particular , the drag force has magnitude $$ f_\mathrm{drag} = \frac{1}{2} c_\mathrm{d} \rho_\mathrm{air} a v^2 , $$ where $c_\mathrm{d} \sim 1$ is the geometric drag coefficient , $\rho_\mathrm{air}$ is the density of air , $a$ is the meteor 's cross-sectional area , and $v$ is its velocity . also , the change in mass due to ablation is $$ \dot{m}_\text{ablation} = -\frac{1}{2q} c_\mathrm{h} \rho_\mathrm{air} a v^3 , $$ where $q$ is the heat of ablation ( similar to the heat of vaporization ) of the material and $c_\mathrm{h}$ is the heat transfer coefficient . since the mass-loss rate scales as $a \sim m^{2/3}$ , sublinearly with mass , smaller objects will entirely ablate faster , setting a lower limit on the size of a meteor that can undergo catastrophic fragmentation before being calmly ablated . meteors that are too big , on the other hand , will cross the depth of the atmosphere and crash into the ground before a pressure wave ( traveling at the speed of sound in the solid ) can even get from the front to the back of the object . there simply is not time for pressure-induced fragmentation of the entire object to occur , meaning the kinetic energy is not dissipated until the entire body slams into earth . [ 1 ] chyba et al . 1993 . " the 1908 tunguska explosion : atmospheric disruption of a stony asteroid . " ( link , pdf )
is it possible to focus the sun in such way ? yes , as others have pointed out , all of the ideas in your sketch are already used in existing designs - perhaps excepting the shutter ( which actually performs no useful purpose so far as i can see ) . as chris white commented - " this exact design ( with the shutter permanently open ) is a schmidt-cassegrain telescope , probably the most popular high-end consumer scope these days . " is it possible to increase the power of the beam by making it bounce between the mirrors no , focusing a beam of light , or reflecting it , does not increase the power . the amount of power is the amount of light energy entering the system per second . that is limited by the diameter of the entry pupil . energy is conserved .
it does not quite work like that . for one thing , stars ' orbital speeds , while reasonably fast by human standards ( often hundreds of km/s ) , are ( in most cases ) incredibly slow by relativistic standards - in other words , they are miniscule fractions of the speed of light . so the difference between the " relativistic mass " ( or energy , as i would call it ) and the rest mass is entirely negligible for all except perhaps a few stars in any given galaxy . certainly there is no way it could account for the missing mass attributed to dark matter . besides , it is not the case that the stars ' orbital speeds steadily increase over time in response to increased gravity . instead , there is going to be some equilibrium at which the effects of the increased orbital speed balance out the effects of the increased gravity . the stars will quickly reach that equilibrium during the galaxy 's formation , and then , simply speaking , they will remain at that speed , so the entire galaxy exists in a steady state of orbital motion . there would be no further correlations of orbital speed with age beyond this point . and anyway , the models that people used to determine the presence of dark matter do take this effect into account ( in the sense that they have determined that it has no noticeable effect on the calculations ) .
as you have in the commutation relations , $\sigma_i \sigma_j= \sigma_j\sigma_i$ e.g. spin operators on different sites commute , so there is no minus sign to pick up .
as was stated in the comments , your question can not be answered precisely . here 's my reactions , i hope it could help you : from a biological ( evolution ) point of view , a creature can be considered as " good " or " bad " only in a given environment . it means that there can not be a " best creature " , because you will always find environments in which the " best " becomes " weak " . from a physical point of view , you also need to precise the task that you want to evaluate . strength and agility is very vague . in my opinion , the human body can be outclassed by other species in any precise physical task . so maybe your question was more about the impression that the human body is the best design when you consider all the activities of human beings ? or a selection of these activities , like in olympic games ? in this case , it is not a surprise because : that is how evolution works ( the design was selected based on the activities necessary to survive , so we are not bad for these activities in our environment ) and because human beings prefer doing things that their body enables them to do ( that seems stupid i know , but would you add ' flying ' or ' digging ' without tools in the olympic games ? )
as you described , we substitute $y=0$ and $x=r$ into the trajectory equation : $$0=h+r\tan{\theta}-r^2\frac{g}{2u^2}\sec^2\theta . \tag{1}$$ then , differentiating with respect to $\theta$ and setting $\frac{dr}{d\theta}=0$: $$0=r_{max}\sec^2\theta-r_{max}^2\frac{g}{2u^2}2\sec^2\theta\tan\theta , $$ which simplifies to $$r_{max}=\frac{u^2}{g}\cot\theta . \tag{2}$$ solving $ ( 1 ) $ and $ ( 2 ) $ will yield the desired expressions for $\theta$ and $r_{max}$ .
ostwald ripening is not a " law " in any usual sense of the word . in general a colloid is in dynamic equilibrium with a finite concentration of the solute . small particles/droplets have a higher surface energy to mass ratio simply because their surface area to volume ration is higher , so it is energetically favourable to transfer material through the solution to the larger particles . this means large particles/droplets tend to grow while the smaller ones shrink . this is the phenomenon of ostwald ripening . i suppose this is sort of similar to black holes . if you maintain a bath of background radiation between the temperature of small and large black holes the small ones will shrink , heating the background , and the heated background will then enlarge the big black holes . however it is not clear there is anything useful to be gained from looking at black holes in this way .
no physical experiment can disprove the existance of god . let 's get that out of the way so we can concentrate on the interesting stuff . if you consider some area of intergalactic space far from anything , then this is a pretty good definition of zero energy because there is nothing there . quantum mechanics complicates this a bit , but for now lets ignore that and just take our vacuum as zero energy . if we have some test mass , e.g. a baseball , and let it fall towards a planet then as it falls it picks up speed and therefore it has kinetic energy . but we believe in the conservation of energy , and we started with zero energy . if our baseball acquires ( positive ) kinetic energy as it falls into a gravity well then there must be an equal negative energy that balances it out , so the total energy stays at zero . this is why we say the energy of the gravity well is negative . now go back to our patch of vacuum . suppose we want to create a baseball from nothing . this costs energy because even a stationary baseball has energy $e = mc^2$ ( from einstein 's famous formula where $m$ is the mass of the baseball ) . but suppose at the same time we create the baseball we create a gravity well with a matching negative energy $-mc^2$ . that means the total energy is still zero so we have created something from nothing , but without violating any physical laws . creating a baseball from nothing may seem an unreasonable thing to do , but quantum mechanics allows this sort of thing . well , do not take my example too literally since you have probably noticed that baseballs do not pop into being every day . the point is that when you start thinking about the creation of the whole universe you can make the sums work . the positive energy of the mass in universe can be balanced out by the negative gravitational energy that all that mass creates . but i think a health warning is in order here . all these ideas are speculative since we have no firm theory to describe how the universe started , just lots of interesting ideas .
a simple model that explains the frequency dependency of the resistivity of metals reasonably well is the drude model ( http://en.wikipedia.org/wiki/drude_model ) . there we have frequency dependency because the electrons in a plasma are not moving arbitrarily fast , which is consistent with xurtio 's explanation . the cutoff frequencies are usually in the optical domain . for dielectrics similar models exist , which are often a sum of lorentzian resonances . these have their origin in resonant absorption which is a quantum physical effect . the imaginary part of the permittivity is related to the conductivity . this can be seen as follows : amperes law is $\nabla \times \mathbf{h} = \mathbf{j} +i \omega \epsilon_r \epsilon_0 \mathbf e$ and insert ohms law in differential form $\mathbf{j} = \sigma \mathbf{e}$ then you get $\nabla \times \mathbf{h} = i \omega ( \epsilon_r \epsilon_0 -i \sigma/\omega ) \mathbf e$ which is just of the same form of as original form of amperes law but without the explicit $\mathbf{j}$ term . in conclusion ohms law can be integrated in free space maxwells equations ( without the source terms ) when the relative permittivity $\epsilon_r$ is taken as a complex value ( $\widetilde\epsilon_r = \epsilon_r - i \sigma/ ( \omega \epsilon_0 ) $ ) , where an imaginary part is added related to the conductivity . this essentially models the effect of moving charges under the influence of an oscillating field ( light ) . so the relation between polarization ( $\mathbf d = \widetilde{\epsilon}_r \epsilon_0 \mathbf e = \mathbf p + \epsilon_0 \mathbf e$ ) and conductivity $\sigma$ is given as $\mathbf{p} = \epsilon_0 ( \epsilon_r - i \sigma/\omega - 1 ) \mathbf e$ . since the real part of the permittivity is frequency dependent , so is the conductivity . this is because of the kramers-kronig relations which follow from a causality relation .
you are on the right track in calculating the uncertainty in momentum using the uncertainty principle . the new position will be $$x_2 = x_1 + \frac{p}{m} t$$ there is a well known technique of error propagation which works like $\delta ( f ( x_1 , x_2 , x_i , . . . ) = \sqrt{\sigma\left ( \frac{\partial f }{\partial x_i} \delta x_i\right ) ^2 } $ , where $\delta x_i$ means the uncertainty in $x_i$ , which is an independent coordinate ( including momenta and times ) of the motion . you sum over every measurement that has an uncertainty . this comes from the taylor series . applying this , you will get $$\delta x_2 = \sqrt{\delta x_1^2 + \left ( \frac{t}{m} \delta p\right ) ^2}$$ edit - i thought about this a little more and i think that the addition in quadrature is not so appropriate here . usually you use this for measurement uncertainties , where you look for one-sigma intervals , but for quantum mechanics , where you look for complete uncertainty , it might be more correct to add the components directly . $$\delta x_2 = \delta x_1 + \left ( \frac{t}{m} \delta p\right ) $$
in theory , yes , this could be done . pretty much exactly as much gravitational energy is lost by the water coming down as is gained by the water going up , so you could then supply the water while hardly using any energy at all . ( just enough to offset the heat generated by friction in the pipes . ) one way in which it can be done in theory is simply to connect two gear pumps with a solid axel . water from the down pipe will force the axel to turn , which then drives the up pump . water can be made to flow by applying just a little bit of extra torque to the axel . however , in practice i do not think this would be done . i imagine there would be a lot of practical issues involved in passing waste water through a pump - it would at least have to be filtered first - and as energynumbers points out in a comment , the energy needed to pump water up 20 floors is pretty small in comparision to ( for example ) heating the apartments .
i finally found some prior art . this object has been introduced as the " husimi matrix " by harriman " some properties of the husimi function " harriman , john e . , the journal of chemical physics , 88 , 6399-6408 ( 1988 ) , http://dx.doi.org/10.1063/1.454477 and briefly referred to by morrison and parr " approximate density matrices and husimi functions using the maximum entropy formulation with constraints " morrison , robert c . and parr , robert g . , international journal of quantum chemistry , 39: 823–837 http://dx.doi.org/10.1002/qua.560390607 the treatment was fairly basic . from what i can tell , harriman primarily introduced the husimi matrix to highlight an analogy with density matrices ( since you can use it as the kernel of an integral operator ) . morrison and parr use it for something related to calculating a density matrix as a maximum entropy husimi function , but i do not really understand . i do not believe anyone has explored the relationship to decoherence .
there are two phenomena present diffusion , which happens due to inhomogeneity in concentration . particles " want to " go from areas of higher concentration to the lower ones . one can write this in the form of diffusion current $$j_{diff} ( x ) = - d \nabla \rho ( x ) $$ where $\rho ( x ) $ is the concentration . this expression is known as fick 's law but it is actually just the standard linear response to inhomogeneities . drift , which is the terminal velocity particles attain due to presence of some force . e.g. the drift one can observe for balls falling in viscous liquid . one can write $$j_{drift} = \rho ( x ) v ( x ) = \rho ( x ) b f ( x ) = -b \rho ( x ) \nabla u ( x ) $$ from the requirement of equilibrium we have that $j_{diff} + j_{drift} = 0$ and from boltzmann statistics we can obtain the concentration $\rho ( x ) \sim \exp ( -{u ( x ) \over k_b t} ) $ . putting it all together we get $$0 = - d \nabla \rho ( x ) - b \rho ( x ) \nabla u ( x ) = - \nabla u ( x ) \rho ( x ) ( -{d \over k_b t} + b ) $$ and we can see the required relation in the last term .
i do not know a good answer to your first question ( i would be interested in a good text for that myself ) , but i can answer the second . it is easier to explain if we temporarily imagine $\phi$ represents the concentration of some dye made up of little particles suspended in the fluid . the convective term ( aka advective term ) is transport of $\phi$ due to the fact that the fluid is moving : a single " particle " of $\phi$ will tend move around according to the velocity of the fluid around it . the diffusive term , on the other hand , represents the fact that the dye tends to spread out , regardless of the motion of the fluid , because each particle is undergoing brownian motion . so if you were moving along at the same velocity as the fluid you would see a small spot of dye tend to become more and more blurred over time . for quantities like energy and momentum the diffusion happens for a slightly different reason ( transfer of the quantity between fluid molecules when they collide ) but the principle is the same . the property is transported along with the fluid 's bulk velocity ( convective term ) but also tends to spread out and become blurred of its own accord ( diffusive term ) .
if two particles are close to each other , there is more space for the rest of the particles to move . this gives rise to an effective entropic attraction between the particles because when looking at two particles for different separations while " tracing out " over the degrees of freedom of the rest of the system , the entropy of the rest is higher when the two tagged particles you are looking at are close to each other . in fact at high density , you should also observe oscillations in the g ( r ) and not a single peack . the width of the bumps in these oscillations is related to the particle size .
your question mentions ' fusing ' cracks . i think you may be asking whether there are actual examples of what is sometimes referred to as " reversible crack growth . " this is often the ideal case discussed when students are learning crack mechanics theory . the closest approximation to reversible crack growth demonstrated in the laboratory that i am aware of are experiments performed using cleavage cracks in mica . the cracks must be made under a vacuum to avoid contamination of the surfaces . when the applied stress is removed the crack can ' heal . ' it is never perfectly reversible , because of reasons that can include surface contamination and plastic deformation near the crack tip . another strain mechanism that might be approximated ( under simple conditions ) is possibly stress induced twinning . if the applied stresses are removed and the twinned crystal is heated , the twins might reverse . ' removing ' a dislocation from a crystal lattice means moving the dislocation to a point where it is ' annihilated ' by reaction with other defects or a surface . a dislocation may move to a free surface , and out of the lattice . a dislocation loop may shrink down to no radius . two dislocation segments of exactly opposite burgers vector may move towards each other cancel out . dislocation motion means plastic strain . it would be rare that all the strains would cancel out exactly , so in most real cases annealing probably result in at least a little residual strain .
this is the way in which all physical theories get formulated--- you first acquire certainty regarding the behavior of many special cases you have some experimental data or theoretical insight about , then you try to formulate a precise theory which extends these heuristic laws to a precise understanding , and when you succeed in matching the heuristic laws ( when they apply ) and you can predict everything consistently and correctly , you are done . in fundamental physics , one has a pretty solid understanding of phenomena which are not quantum gravitational , because we have a precise fundamental theory of relativistic quantum fields . so the most significant things that are not fully understood at the precise level are the class of insights deriving from hawking radiation and black hole classical behavior . these are semi heuristic , because there are puzzles that are not yet fully resolved within string theory . this class includes : the near horizon behavior of semiclassical black holes : an observer falling into a black hole sees nothing special when crossing the horizon , and this has not been rigorously demonstrated in string theory , it is only rigorously true classically . now some people claim that it can not work in quantum gravity , that black holes are " firewalls " . i read the arguments , and i find them uncompelling , because they mix assumptions about the semiclassical behavior at late times with full quantum observations on the hawking radiation which are restricted when you measure the semiclassical state at late times , so the argument smells fishy and does not stop smelling fishy even with later clarifications , although understanding what is going wrong is important , and this is at the heuristic level , because we can not reconstruct black hole interiors completely from quantum gravity scattering data . the related holographic principle and black hole complementarity : this is also heuristic for semiclassical thermal black holes , although it is precise in ads/cft , for certain extremal black holes . cosmological horizon entropy : the entropy of the cosmological horizon is not even in-principle understood in string theory , and it is a major clue to understanding how to do quantum gravity in desitter space , because it is understood at the heuristic level--- it is the same as black hole entropy . rindler horizons : the behavior of strings on a rindler background is nontrivial , even though this is just minkowski space . quantum fields are completely understood on rindler , but string theory on rindler is harder , because you do not have an s-matrix ( everything falls into the rindler horizon ) . for string theory , we only have heuristic guidelines regarding compactification and susy breaking . these heuristics are summarized in guidelines about what topologies and matter configurations give rise to which gauge fields and representations , and where one should expect to find the standard model . these are relatively well understood , since there are many explored vacua , but there are always surprises . in cases where we know the fundamental laws without serious doubt , there are still cases where we understand things only at the heuristic level . the following are in qcd : confinement and regge theory : we know the qcd confines to regge trajectories for mesons and topological soliton baryons , but this formulation is not mathematically linked to qcd by any rigorous path . pomerons : this is related--- we know that high energy scattering is dominated by pomeron exchange , and this is heuristic only , because we can not relate this to qcd high energy diffractive scattering except in certain regimes which are not completely diffractive . chiral perturbation theory : again related , this is the low-energy approximation to qcd , and the parameters are from assumptions on low energy condensates . i suppose i should not include this , because you can extract chiral data from lattice data in principle exactly , but i had something else in mind for what constitutes a full understanding--- it would mean that any chiral configuration can be mapped to a qcd configuration , and you could do the path integral for qcd in two steps--- as a path integral over long wavelength chiral configurations plus an additional path integeral over qcd in the given chiral background . nobody did this , although it should not be hard . instanton fluid : there is a class of semiheuristic models that give the qcd vacuum as a dense instanton fluid , and this is not 100% understood . it is essentially the same problem as before--- condensates and confinement . quark condensates : there are condensate models where we can semi-heuristically calculate the effects on hadrons using the svz sum-rules ( qcd sum rules ) , but the condensate values again are not derived from qcd , although they can be measured from experiment . in principle , lattice gives them , but this is not enough--- you want to know the values and effects with more insight . there are no doubts qcd resolves these questions , but the exact best way to make these heuristic things precise is unclear . there are condensed matter systems where the understanding is heuristic in the same way--- the most famous and the one i like best is probably : hightc : the condensate state of the hightc superconductor can be described phenomenologically using a d-wave superconducting condensate . getting this d-wave from the fundamental interactions has not been done in a universally convincing way , although there is no doubt that the fundamental theory will do it . there are many other such things , this is usually all the open problems people work on . historically , if you look at any phenomenon that was understood , it was understood heuristically before it was understood precisely , and learning the heuristic stuff is an important prerequisite for getting certainty about the final explanation , because physical theories are evolved by common sense , they do not emerge fully formed from nothing .
the stress tensor is defined in such a way that taking its divergence brings you back to the force density ( apart from the cross product term which corresponds to the time derivative of the poynting vector ) . in the case of $\mathbf{b}=\mathbf{0}$ , we take the divergence in index notation , i.e. $$f_j=\partial_i\sigma_{ij}=\epsilon_0\left ( \partial_i ( e_ie_j ) -\frac12\delta_{ij}\partial_i ( e_ke_k ) \right ) =\epsilon_0\left ( \partial_ie_ie_j+e_i\partial_ie_j-\frac12\partial_j ( e_ke_k ) \right ) . $$ in order to see that this expression corresponds to the first term in your expression 3 . , we recast this in index-free notation : $$\mathbf{f}=\epsilon_0\left ( ( \boldsymbol{\nabla}\cdot\mathbf{e} ) \mathbf{e}+ ( \mathbf{e}\cdot\boldsymbol{\nabla} ) \mathbf{e}-\frac12\boldsymbol{\nabla} ( \mathbf{e}\cdot\mathbf{e} ) \right ) . $$ making use of the identity $$\frac{1}{2} \boldsymbol{\nabla} \left ( \mathbf{a}\cdot\mathbf{a} \right ) = \mathbf{a} \times ( \boldsymbol{\nabla} \times \mathbf{a} ) + ( \mathbf{a} \cdot \boldsymbol{\nabla} ) \mathbf{a}$$ and the maxwell equation ( for $\mathbf{b}=\mathbf{0}$ ) $$\boldsymbol{\nabla}\times\mathbf{e}=\mathbf{0} , $$ we find that $$\mathbf{f}=\epsilon_0 ( \boldsymbol{\nabla}\cdot\mathbf{e} ) \mathbf{e} . $$ from this we can see that it does not matter at which stage of the derivation the magnetic field is omitted .
recall that $\left|\psi\left ( x\right ) \right\rangle =\int\left|\varphi_{p}\left ( x\right ) \right\rangle \left\langle \varphi_{p}\left ( x\right ) |\psi\left ( x\right ) \right\rangle dp$ ( for continuous p ) where $\left\langle \varphi_{p}\left ( x\right ) |\psi\left ( x\right ) \right\rangle=\text{φ}\left ( p\right ) $ , which is the amplitude of momentum measurement $p$ then $\text{φ}\left ( p\right ) =\left\langle \varphi_{p}\left ( x\right ) |\psi\left ( x\right ) \right\rangle =\intop_{-\infty}^{\infty}\bar{\varphi_{p}}\left ( x\right ) \psi\left ( x\right ) dx$ =$\frac{1}{\sqrt{2\pi a\bar{h}}}\int_{-\infty}^{\infty}e^{\frac{-\left|x\right|}{a}+ix\left ( k-\frac{p}{\bar{h}}\right ) }dx$ by solving the integral , you get $\frac{1}{\sqrt{2\pi a\bar{h}}}\left ( -\frac{1}{\frac{-1}{a}+i\left ( k-\frac{p}{\bar{h}}\right ) }+\frac{1}{\frac{1}{a}+i\left ( k-\frac{p}{\bar{h}}\right ) }\right ) $ $=\frac{1}{\sqrt{2\pi a\bar{h}}}\left ( \frac{2a}{1+a^{2}\left ( k-\frac{p}{\bar{h}}\right ) ^{2}}\right ) $ which can then be simplified to obtain the answer provided
in a ( classical ) lagrangian field theory , the configuration space $\mathcal c$ of the system is a space of field configurations . a field configuration ( or just " field " for short ) is usually taken to be a function $\phi:\mathcal m\to t$ where $m$ is a manifold and $t$ is some set , often a manifold or vector space or both , called the target space of the field . the configuration space $\mathcal c$ is then taken to be some sufficiently smooth ( when a notion of smoothness can be defined ) subset of the set of all possible fields . the lagrangian is then a function $l:\mathcal c\to\mathcal c$ ; \begin{align} \phi\mapsto l [ \phi ] \end{align} namely , the lagrangian maps a particular field configuration , to another field configuration . often , one considers a field theory for which the lagrangian can be written as local local density , but this is not strictly speaking necessary . the action of the theory can then be defined as the integral of $l [ \phi ] $ over $m$ ; \begin{align} s [ \phi ] = \int_m \ , d^dx\ , l [ \phi ] ( x ) . \end{align} note . my terminology and notation here are a bit non-standard in some contexts . for example , in relativistic physics ( field theory ) the lagrangian will usually map a field configuration $\phi$ to a function $l [ \phi ] $ of time , and then this function of time will be integrated to yield the action . it is not hard in practice to translate between conventions . one can then define what it means for the action to possess symmetry . in particular , given a mapping $f:\mathcal c\to \mathcal c$ of the manifold on which field configurations are defined to itself , one says that the action is invariant under $f$ provided \begin{align} s [ f ( \phi ) ] = s [ \phi ] \end{align} for all $\phi\in\mathcal c$ . for " continuous " transformations , one can also define notions of symmetry that do not involve full invariance , but let 's keep the discussion simple at this point . example . a common toy theory considered as the first example in most relativistic field theory texts is that of a single , free , real lorentz scalar defined on minkowski space ( i will use metric signature $+ - - -$ ) . in this case , we have \begin{align} m = \mathbb r^{3,1} , \qquad t = \mathbb r \end{align} and $\mathcal c$ is a space of sufficiently smooth functions $\phi:\mathbb r^{3,1}\to\mathbb r$ that satisfy certain desired boundary conditions . the lagrangian of such a theory is \begin{align} l [ \phi ] ( x ) = \mathscr l ( \phi ( x ) , \partial_0\phi ( x ) , \dots \partial_3\phi ( x ) ) \end{align} where $\mathscr l$ is the lagrangian density defined by \begin{align} \mathscr l ( \phi , \partial_0\phi , \dots , \partial_3\phi ) and = \frac{1}{2}\partial_\mu\phi\partial^\mu\phi - \frac{1}{2}m^2\phi^2 . \end{align} given any lorentz transformation $\lambda$ , one can define a transformation $f_\lambda:\mathcal c\to\mathcal c$ as follows : \begin{align} f_\lambda ( \phi ) ( x ) = \phi ( \lambda^{-1} x ) . \end{align} a short computation then shows that the lagrangian is a lorentz scalar under this transformation , namely \begin{align} l [ f_\lambda ( \phi ) ] ( x ) = l [ \phi ] ( \lambda^{-1}x ) . \end{align} in fact , this is essentially done for you on page 36 of peskin . it follows from this that the action is invariant under $f$ ; \begin{align} s [ f_\lambda ( \phi ) ] = \int_{\mathbb r^{3,1}} d^4x\ , l [ \phi ] ( \lambda^{-1}x ) = \int_{\mathbb r^{3,1}} d^4x\ , l [ \phi ] ( x ) = s [ \phi ] . \end{align} since the measure $d^4 x$ is lorentz-invariant . notice , in particular , that the fact that the lagrangian transformed as a lorentz scalar ( namely precise in the same way as the scalar field $\phi$ was defined to transform ) , immediately led to invariance of the action . furthermore , suppose that $\phi$ is a field configuration that leads to stationary action , then we can also show that a lorentz-transformed field leads to a stationary action using lorentz invariance of the action . to see this , recall that the variational derivative in the direction of a field configuration $\eta$ is defined as follows : \begin{align} \delta_\eta s [ \phi ] = \frac{d}{d\epsilon}s [ \phi+\epsilon\eta ] \big|_{\epsilon=0} \end{align} now , suppose that $\phi$ is a stationary point of the action , namely that $\delta_\eta s [ \phi ] = 0$ for all admissible $\eta$ , then for all such $\eta$ we have \begin{align} \delta_{f_\lambda ( \eta ) } s [ f_\lambda ( \phi ) ] and = \frac{d}{d\epsilon} s [ f_\lambda ( \phi ) + \epsilon f_\lambda ( \eta ) ] \big|_{\epsilon = 0} \\ and = \frac{d}{d\epsilon} s [ f_\lambda ( \phi+\epsilon_\eta ) ] \big|_{\epsilon = 0} \\ and = \frac{d}{d\epsilon} s [ \phi+\epsilon_\eta ] \big|_{\epsilon = 0} \\ and = 0 \end{align} now set $\eta = f_\lambda^{-1} ( \xi ) $ , then the computation we just performed shows that \begin{align} \delta_{\xi} s [ f_\lambda ( \phi ) ] =0 . \end{align} for all admissible field configurations $\xi$ . in other words , the lorentz transformed scalar is also a stationary point of the action . notice that this demonstration holds for any lorentz transformation , not just boosts . addendum . as pointed out in the comments , the argument at the end about variational derivatives hinges on linearity of $f_\lambda$ . this can be demonstrated as follows : \begin{align} f_\lambda ( a\phi+b\psi ) ( x ) and = ( a\phi+b\psi ) ( \lambda^{-1}x ) \\ and = a\phi ( \lambda^{-1}x ) + b\psi ( \lambda^{-1}x ) \\ and = af_\lambda ( \phi ) ( x ) + bf_\lambda ( \psi ) ( x ) . \end{align} let me make some remarks about the mapping $f:\mathcal c\to \mathcal c$ ; a symmetry of the action . if there exists a mapping $f_t:t\to t$ on the target space that induces this mapping , namely \begin{align} f ( \phi ) ( x ) = f_t ( \phi ( x ) ) , \end{align} then $f$ is called an internal symmetry . on the other hand , if there is a mapping $f_m:m\to m$ on the base manifold $m$ that induces this mapping , namely \begin{align} f ( \phi ) ( x ) = \phi ( f_m ( x ) ) , \end{align} then $f$ is called a base manifold symmetry ( or more commonly a spacetime symmetry since in the context of relativistic field theory , the base manifold is a spacetime like minkowski space . ) furthermore , the mapping $f:\mathcal c \to\mathcal c$ on the field configuration space is often , as in the scalar field example , a group action of some group $g$ on $\mathcal c$ . this means that to each $g\in g$ , we associate a mapping $f_g:\mathcal c\to \mathcal c$ such that the mapping $g\mapsto f_g$ is a homomorphism of the group $g$ . in practice , the group $g$ is sometimes a group of symmetries that naturally acts on the base manifold , and sometimes $g$ a group of symmetries that naturally acts on the target space ( or even both when the $m=t$ ) . in any event , this group action is usually obtained by composing a target space group action $ ( f_t ) _g:t\to t$ with a base manifold group action $ ( f_m ) _g:m\to m$ . more explicitly , for each $g\in g$ , we can define mappings $ ( f_t ) _g:\mathcal c\to \mathcal c$ and $ ( f_m ) _g:\mathcal c\to\mathcal c$ as follows : \begin{align} ( f_t ) _g ( \phi ) ( x ) = ( f_t ) _g ( \phi ( x ) ) , \qquad ( f_m ) _g ( \phi ) ( x ) = \phi ( ( f_m ) _g ( x ) ) \end{align} and then the full group action $f_g:\mathcal c\to\mathcal c$ is defined by the composition of these two ; \begin{align} f_g = ( f_t ) _g\circ ( f_m ) _g \end{align} or more explicitly \begin{align} f_g ( \phi ) ( x ) = ( f_t ) _g ( \phi ( ( f_m ) _g ( x ) ) ) . \end{align} now , this is all a bit abstract , so let 's write out what all of these objects would be for the scalar field example : \begin{align} g and = \mathrm{so} ( 3,1 ) \\ g and = \lambda\\ ( f_t ) _g ( \phi ( x ) ) and = \phi ( x ) \\ ( f_m ) _g ( x ) and = \lambda^{-1}x \\ ( f_t ) _g ( \phi ) ( x ) and = \phi ( x ) \\ ( f_m ) _g ( \phi ) ( x ) and = \phi ( \lambda^{-1}x ) \\ f_g ( \phi ) ( x ) and = \phi ( \lambda^{-1}x ) \end{align} notice that $f_t$ is simply the identity mapping on the target space . this is precisely what we mean when we say that the scalar field is a lorentz scalar . on the other hand , for a lorentz vector , the target space itself would be minkowski space $\mathbb r^{3,1}$ , and the target space group action would be \begin{align} ( f_t ) _\lambda ( a ( x ) ) = \lambda a ( x ) , \end{align} namely , there is an internal symmetry in which the vector indices on the field transform non-trivially . in components , which is how you will see this written in peskin for example , the right hand side would be written as $\lambda^\mu_{\phantom\mu\nu} a^\mu ( x ) $ .
because it is structure displays translational symmetry in 2d . atoms themeselves are 3d as in other materials , but their are placed on a 2d flat plane . compare to 1d fullerenes .
while example 1 can be read as " a 10n force is applied to an object through a displacement of 5.0m . . . " , example 2 is quite oddly phrased . the solution given is correct only if it is the case that the acceleration of the puck is constant over the distances given , i.e. , it is being pushed by a constant force . but , a hockey puck being " shot " does not sound like that at all . the image in my mind is of the puck receiving an impulse that imparts a momentum that is then constant ( assuming no loss to friction of any type ) . in other words , i imagine that the work is done in the initial moment of the " shot " and none thereafter . without any more context than is given , i would conclude that the textbook author ( s ) need remedial physics classes .
best description of the problem of those i found is given by nobel winner vitaly ginzburg . see , e.g. http://ufn.ru/en/articles/1973/3/k/ or ( even better ) vl ginzburg , theoretical physics and astrophysics ( pergamon , oxford , 1979 ) ; other editions with somewhat different titles also available .
comment to the question ( v2 ) : p and s is using the notation of a ' same-spacetime ' functional derivative . to illustrate this notation , let us for simplicity stay within first variations , and leave it to the reader to generalize to higher-order variations . i ) first of all , functional/variational derivatives should not be confused with partial derivatives . in practice , from an operational point of view ( if we are not worried about mathematical details about existence and boundary terms ) , all we need to know is the following rules : the formula $$\tag{a} \frac{\delta \phi^{\beta} ( y ) }{\delta\phi^{\alpha} ( x ) } ~=~\delta^{\beta}_{\alpha}~\delta^n ( x-y ) , $$ where $n$ is the spacetime dimension . appropriate generalizations of elementary rules in calculus , such as , e.g. , the chain rule , integration by parts , commutativity of derivatives , and the dirac delta distribution . for instance , by these rules 1 and 2 , we have that $$ \frac{\delta}{\delta\phi^{\beta} ( y ) } \frac{\partial}{\partial x^{\mu_1}}\ldots \frac{\partial}{\partial x^{\mu_r}}\phi^{\alpha} ( x ) ~=~ \frac{\partial}{\partial x^{\mu_1}}\ldots \frac{\partial}{\partial x^{\mu_r}} \frac{\delta}{\delta\phi^{\beta} ( y ) }\phi^{\alpha} ( x ) $$ $$\tag{b}~\stackrel{ ( a ) }{=}~\delta_{\beta}^{\alpha}~\frac{\partial}{\partial x^{\mu_1}}\ldots \frac{\partial}{\partial x^{\mu_r}}\delta^n ( x-y ) . $$ similarly , by rules 1 and 2 , we can deduce that the action $$\tag{c}s~=~\int d^nx ~{\cal l} ( x ) , \qquad {\cal l} ( x ) \equiv {\cal l} ( \phi ( x ) , \partial \phi ( x ) , \ldots , x ) , $$ has the euler-lagrange expression as its functional derivative $$ \tag{d}\frac{\delta s}{\delta\phi^{\alpha} ( x ) }~=~ \frac{\partial{\cal l} ( x ) }{\partial\phi^{\alpha} ( x ) } - d_{\mu} \left ( \frac{\partial{\cal l} ( x ) }{\partial\partial_{\mu}\phi^{\alpha} ( x ) } \right ) +\ldots . $$ the ellipsis $\ldots$ in eqs . ( c ) and ( d ) denotes possible contributions from higher-order spacetime derivatives . ii ) from formula ( a ) it becomes clear that it does not makes sense to consider the functional derivative $\frac{\delta {\cal l} ( x ) }{\delta\phi^{\alpha} ( x ) }$ wrt . the same spacetime argument $x$ , because that would lead to infinities , cf . $\delta^n ( 0 ) =\infty$ . nevertheless , it is tempting to introduce the notation of a ' same-spacetime ' functional derivative $$\tag{e}\frac{\delta {\cal l} ( x ) }{\delta\phi^{\alpha} ( x ) }~:=~ \frac{\partial{\cal l} ( x ) }{\partial\phi^{\alpha} ( x ) } - d_{\mu} \left ( \frac{\partial{\cal l} ( x ) }{\partial\partial_{\mu}\phi^{\alpha} ( x ) } \right ) +\ldots . $$ we stress that eq . ( e ) is only a notational definition . it becomes meaningless if we try to interpret the lhs . of eq . ( e ) using the above rules 1 and 2 . iii ) similarly , p and s talk about second-order ' same-spacetime ' functional derivative $$\tag{f}\frac{\delta^2 {\cal l} ( x ) }{\delta\phi^{\alpha} ( x ) \delta\phi^{\beta} ( x ) } . $$ we recommend to first work out the ordinary second-order functional derivative $$\tag{g}\frac{\delta^2 s}{\delta\phi^{\alpha} ( x ) \delta\phi^{\beta} ( y ) }$$ using rules 1 and 2 . then it should be fairly straightforward to translate ( g ) into the ' same-spacetime ' functional derivative language ( f ) , if needed . [ in particluar , eq . ( g ) contains a $\delta^n ( x-y ) $ while eq . ( f ) does not . ] iv ) finally we should mention that in field theory one often suppresses the spacetime indices $x , y , \ldots$ , by using dewitt 's condensed notation .
this is exactly the approach taken in bernard shutz 's note " gravitational waves on the back of an envelope " ( am . j . phys . 50 vol 5 pp 412 ) . the abstract reads : using only newtonian gravity and a little special relativity we calculate most of the important effects of gravitational radiation , with results very close to the predictions of full general relativity theory . used with care , this approach gives helpful back‐of‐the‐envelope derivations of important equations and estimates , and it can help to teach gravitational wave phenomena to undergraduates and others not expert in general relativity . we use it to derive the following : the quadrupole approximation for the amplitude h of gravitational waves ; a simple upper bound on h in terms of the newtonian gravitational field of the source ; the energy flux in the waves , the luminosity of the source ( called the ‘‘quadrupole formula’’ ) , and the radiation reaction in the source ; order‐of‐magnitude estimates for radiation from supernovae and binary star systems ; and the rate of change of the orbital period of the binary pulsar system . where our simple results differ from those of general relativity we quote the relativistic ones as well . we finish with a derivation of the principles of detecting gravitational waves , and we discuss the principal types of detectors under construction and the major limitations on their sensitivity . ( if you do not have access to am j phys , this talk seems to recapitulate the details . ) a major difference in this newtonian scalar theory from the real gr theory of gravitational waves is in the effect of the waves on an inertial test particle . this newtonian theory predicts that the waves would appear as an oscillating force along the direction between the source and the test particle . by contrast , gr predicts an oscillating differential tidal effect in the plane perpendicular to the line connecting the source and the test mass . as a result , while i think ligo would still detect the " newtonian " form of gravitational waves , the antenna pattern of the detector would be different . the l-shaped ligo detector has optimal sensitivity to a source located directly overhead in the gr case ( allowing the gravitational wave to stretch one arm while it is compressing the other ) . there would be no sensitivity to a " newtonian " source directly overhead . however , you could detect it if the " newtonian " source were aligned with either arm . by the way , " newtonian noise " ( the near-field action of newtonian gravity arising from density waves in the material near the detector ) is a real concern for terrestrial gravitational wave detectors ! p.s. to be pedantic , it is best to avoid the term " gravity wave " ( as opposed to " gravitational wave" ) , since a " gravity wave " ( "newtonian gravity wave " even ! ) is something completely different .
start with the two equations : $$ x ' = \gamma ( x - vt ) $$ $$ x = \gamma ( x ' + vt' ) $$ note the sign change of $v$ because going from $x'$ to $x$ is in the opposite direction to going from $x$ to $x'$ . rearrange equation 2 to give : $$ t ' = \frac{1}{v} ( \frac{x}{\gamma} - x' ) $$ now use equation 1 to substitute for $x'$: $$ t ' = \frac{1}{v} ( \frac{x}{\gamma} - \gamma ( x - vt ) ) $$ and rearrange to give : $$ t ' = \gamma \left ( \frac{x}{v} \frac{1}{\gamma^2} - \frac{x}{v} + t\right ) $$ and $1/\gamma^2$ is $1 - v^2/c^2$ so : $$ t ' = \gamma \left ( \frac{x}{v} - \frac{xv}{c^2} - \frac{x}{v} + t\right ) $$ $$ t ' = \gamma \left ( t - \frac{xv}{c^2}\right ) $$ and likewise to get the expression for $t$ in terms of $t'$ and $x'$ .
the answers given so far are fine , but to my surprise nobody 's mentioned the most important point : in modern terminology , we generally do not say that the mass of an object increases with speed . " relativistic mass increase " is outdated terminology , not used by most physicists anymore . in general , nowadays , " mass " means " rest mass " and is independent of velocity . igor ivanov 's answer to this question says it all . i have not read the article by lev okun that he refers to , but i like the term " pedagogical virus " for this notion .
so this was the answer given in the key :
do not worry , i did research in surface plasmons and even then i was more than a year into it before i truly understood , on an intuitive level , how the light gets a ' kick ' from the grating . you are correct that it is diffraction at a 90 degree angle to the normal , but there is an easier way to think about it . you say you have never taken a formal course in optics so i will talk a little bit about diffraction gratings in general . you might have come across one before and know that if a beam of light hits it , it is diffracted into several different beams . transmissive diffraction gratings are what one usually encounters in high school physics so i will illustrate one below : the numbers at the end of each beam are known as the order $\nu$ of that beam . the grating equation is $d ( \sin\theta_i + \sin\theta_o ) = \nu\lambda$ , where d is the distance between lines of the grating , $\lambda$ is the wavelength of the light , $\theta_i$ is the angle of incidence , and $\theta_o$ is the angle of the outgoing beam . in the above illustration , $\theta_i$ is zero . next we consider a reflective grating ( for example a piece of metal with 1d periodic grooves ) , as in the following illustration : the same mathematics govern this situation as well . you will notice the $\nu=+2$ order being very close to grazing the grating surface . adjusting the angle of incidence a little bit would cause it to do so . in that case , it would have the required wave vector to launch a surface plasmon , which is the phase matching condition that you started out with . you get the $\beta = k\sin\theta\pm\nu g$ when you convert the grating formula to wave vectors ( reciprocal space ) which i am too lazy to do right now . i suppose you could technically say that the light got a momentum ' kick ' from the $\nu=-2$ order being launched in the opposite direction , but thinking of it as the light getting a ' kick ' is really misleading in my experience .
in space you do not just " go somewhere " . you have to match orbits , while not wasting too much fuel . if you are in a low circular orbit , and you want to get to a high circular orbit , it takes two tangential burns , one to elongate your orbit into an ellipse , and another at the high point of the ellipse to make it circular again . this is called a hohman transfer . you may have to do this multiple times , depending on how much thrust you have . if your orbit is in a different plane from the orbit of the space station , you have to wait until you reach the plane of the other orbit , then do a lateral burn . you may have to do this several times to change your orbit is angle sufficiently , each time having to wait another half-orbit . edit : to give some perspective on this , if your orbit crosses the plane of the other orbit at an angle of 10 degrees , that means you are crossing that plane at about one mile per second . ( orbit velocity times sin ( 10 degrees ) . ) if your rocket motor generates 1g of thrust , you need to run it around 2.5 minutes to get aligned with that plane . ( 5280/32/60 ) revised : if you are in the same orbit as your destination , but some distance behind it ( say ) , the way you catch up is by getting into a lower orbit by a hohman transfer , with greater angular velocity , and then another such transfer to get back to the original orbit . this is called orbit phasing . if you just accelerate toward the object , that would put you in an orbit that rises above the target , and then eventually falls further behind because it is a higher orbit .
it actually gets a bit complicated , since several effects are involved : evaporating water does require heat , which comes primarily from the hot stones . so throwing water on the stones does cool them down . ( this is where the claim one occasionally hears , that " throwing water on the stones makes the sauna colder " , comes from . technically it is true , if one considers the total heat content of the sauna as a whole . but since most of that heat is in the stones , and since you do not sit on the stones , that is pretty much completely irrelevant to how hot the part of the sauna that you do sit in gets , or feels . ) on the other hand , throwing water on the stones also significantly increases the heat transfer rate from the stones to the air : the evaporation produces a lot of hot steam , which will rise and mix with the ambient air in the sauna . so it is possible for the air temperature in the sauna to increase , even as the stones are cooled down . also , the introduction of steam obviously increases the humidity of the air , which will increase the rate of water precipitation on skin , and/or decrease the rate of sweat evaporation . ( the relative importance of these two effects will depend on the baseline humidity of the air , which can vary quite a lot . my gut feeling , based on experience , is that in all but the driest of saunas condensation probably dominates , simply because human skin is so much cooler than the air . ) in either case , the effect will be to transfer more heat to the skin , and thus to make the air feel hotter . finally , as the hot steam rises off the stones , it will push hot air around the sauna in front of it . while this increase in air movement is slight and transient , it probably does have a noticeable effect : as the hot air flows past the people in the sauna , it will act to disperse the layer of cooler air that forms over the skin , and thus increases heat transfer to the skin . ( if you do not believe me , try blowing some air over your skin in a sufficiently hot sauna . it burns . ) the upshot is that throwing water on the stones increases heat transfer , both from the stones to the air and from the air to your skin . as long as the stones stay hot enough to supply that heat , the net effect will be that you feel hotter . however , if you throw too much water on the stones , it is possible to " kill the stones " by cooling them close to or even below the boiling point of water . at that point , throwing more water is useless , and all you can do is add more firewood or turn up the thermostat and wait for the stones to heat up again . or , if you manage to do this in a smoke sauna , go wash yourself and get dressed up because the sauna is over for the night .
your soda was in a supercooled state . being in the freezer , it was at a temperature below its freezing point however it remained as a liquid as the glass was too smooth to allow ice crystals to start to form ( in technical terms , the phase transition requires a nucleation site ) . when you removed it from the freezer , you gave it the disturbance necessary to catalyse the transition from supercooled liquid into a block of ice . no violation of the 2nd law occurred .
ok , i think there is a mistake here : a general tensor $\varphi^i$ transforms as : $$\varphi^i\rightarrow u^i_{\phantom{1}j}\varphi^j$$ whereas $\varphi_i$ transforms as : $$\varphi_i\rightarrow ( u^\boldsymbol{\ast} ) _i^{\phantom{1}j}\varphi_j$$ where did you find these equations ? the unitary matrix element in the second line should not be a complex conjugate . i do not remember giorgi 's conventions but the customary notation i am used to is this one : $$u_i^{\phantom{i}j}=u_{ij} , \quad \varphi_i\rightarrow u_i^{\phantom{1}j}\varphi_j\\ u^i_{\phantom{i}j}=u^\ast_{ij} , \quad \varphi^\ast_i\equiv \varphi^i\rightarrow u^i_{\phantom{1}j}\varphi^j\equiv ( u_i^{\phantom{i}j}\varphi_j ) ^\ast . $$ hence , in your equations i would understand : $$ ( u^\ast ) _i^{\phantom{i}j}\equiv u^\ast_{ij}=u^i_{\phantom{i}j}$$ and it does not provide the right transformation law for $\varphi_i$ . edit : well , provided the previous comments , let me clarify some issues with the notation , that may led to confuse the meaning of these transformation laws . let us choose the convention to denote $su ( n ) $ transformations , that is $n\times n$ unitary matrices with unit determinant , with uppercase letters , like $u$ , and base states ( scalars , vectors and tensors ) with lowercase greek letters , $\psi\in \mathbb{c}^n$ . for example vector states transform as : $$\psi\to u\psi , \quad \psi_i\to u_{ij}\psi_j\equiv u_i^{\ j}\psi_j$$ note that here i followed the convention of writing base states of the fundamental or vector representation with lower indices , as georgi does and as you can find here . this is the convention i am used to , but nothing stops you to do the contrary , choosing upper indices ! note also that $u\psi$ represents the ordinary product of an $n\times n$ matrix by a vector $\psi= ( \psi_1 , \ldots , \psi_n ) ^t$ , and produce a vector of the same type . in the notation $u_{ij}$ the index $i$ represents the rows whereas the second index $j$ represents the columns . it is customary to write it like $u_i^{\ j}$ to distinguish rows and columns . $\psi_i$ is a column vector and $i$ counts its rows . you can define the conjugate representation by means of the conjugate vectors $\psi_i^\ast$ , whose transformation law is $$\psi^\ast\to ( u\psi ) ^\ast=\psi^\ast u^\ast , \quad \psi_i^\ast\to ( u^\ast ) _{ij}\psi_j^\ast=\psi_j^\ast ( u^\dagger ) _{ji}$$ since these conjugate vectors transform in a different way with respect to $\psi_i$ , it is useful to introduce upper indices to distinguish them : $$\psi^i\equiv \psi_i^\ast \to u^\ast_{ij}\psi_j^\ast\equiv u^i_{\ \ j} \psi^j . $$ as you can see , now indices are " summed on the bottom-right " . the extension to any arbitrary $ ( p , q ) $-tensor is trivial , their transformation law are those of the direct ( diagonal ) product of $p$ type $\psi^i$ vectors and $q$ type $\psi_i$ vectors : $$\psi^{i_1\ldots i_p}_{j_1\ldots j_q}\to \big ( u_{j_1}^{\ \ j'_1}\cdot\ldots\cdot u_{j_q}^{\ \ j'_q}\big ) \big ( u^{i_1}_{\ \ i'_1}\cdot\ldots\cdot u^{i_p}_{\ \ i'_p}\big ) \psi^{i'_1\ldots i'_p}_{j'_1\ldots j'_q} . $$ since upper and lower indices represents different objects it has no sense mixing them .
while there are more mathematical definitions available , an appropriate working definition of locally asymptotically $\textrm{ads}_3$ is that the line-element in gaussian normal coordinates for $\rho\to\infty$ must take the form \begin{equation} ds^2 = d\rho^2 + ( \gamma_{ij}^{ ( 0 ) } e^{2\rho} + {\rm subleading}_{ij} ) dx^i dx^j \end{equation} where $\gamma_{ij}^{ ( 0 ) }$ is the 2-dimensional minkowski metric and " subleading " means " not diverging as fast as $e^{2\rho}$" . you can easily convince yourself that the btz black hole ( and any other asymptotic ads$_3$ space-time you might have encountered ) can be written in this form . in 3-dimensional vacuum einstein gravity the asymptotic solutions to the equations of motion imply that the subleading terms must fall off as in the brown-henneaux boundary conditions ( bh bc 's ) provided in olof 's answer . however , it is not true that bh bc 's are necessary for asymptotically $\textrm{ads}_3$ behavior . you can obtain as asymptotic symmetry algebra ( asa ) two copies of the virasoro algebra even for weaker boundary conditions . the question of whether or not such boundary conditions are useful depends on the theory under consideration . permit me to given an example that i know quite well : in topologically massive gravity at a certain critical point one should allow logarithmic violations of the bh bc 's in order to accommodate the full normalizable spectrum of linearized fluctuations around $\textrm{ads}_3$ and to avoid the elimination of otherwise valid classical solutions . see http://arxiv.org/abs/arxiv:0808.2575, in particular eq . ( 8 ) , which displays violations of the bh bc 's in two entries of the metric . nevertheless , one can show that one has two copies of the virasoro algebra as asa , and that the asymptotic charges are finite and conserved . quite generally , a reasonable strategy to find the " right " boundary conditions is to weaken them as much as possible , but without creating inconsistencies , like infinite or non-conserved charges . this can be sometimes a bit of an art and may require physical input .
there is two important differences between air and water : air is compressible , and the densities are about a factor of ~1000 apart - 1 kg/m³ vs 1 t/m³ ! for most concerns where you use propellers , compression plays no role because the pressure diferences are very low . the densities , however play a large role . the thrust can be described as $f = \dot m * \delta v$ , with $\dot m$ beeing the mass flow - kg and /s or such - and $\delta v$ the difference in velocity a volumeelement of fluid is accelerated . so , to achieve a similiar thrust , the same propeller would have to move 1000 times more air than water by volume . hence the often larger and faster spinning propellers for planes . on the other hand , in a heavier medium each wing of the propeller is subject to stronger torque ( all else beeing equal ) : $$q = \rho v_{a}^2 d^3 f_q ( \frac{nd}{v_a} ) $$ ( source ) $\rho$ is density , $v_a$ rate of advance ( how much the propeller moves forward per revolution ) , d is diameter and n number of revolutions . without going into the math it can probably be shown that in a heavier medium the propeller will experience somewhat more torque for the same thrust - i am to lazy to try now . the propeller will be built with more robust ( and possibly heavier ) material than would be the case id it is an only air propeller . that said , i believe that a propeller for both media is entirely possible , though challenging . however , a propeller for a both media will need a drivetrain that can accomodate speed roughly a factor 1000 apart ( that is not trivial ) . one other reason why do not see a propeller for both media is that there is no vehicle that could make use of one .
there are lots of different types of rock , and water can vary in it is acidity , so there are a few variables to consider . one obvious example is the formation of caves . these are usually in limestone , which is calcium carbonate . if carbon dioxide is present , calcium carbonate will dissolve in water to form calcium bicarbonate . so pure water ( ok with a bit of co2 :- ) will happily dissolve limestone . another example is in forming gorges like the grand canyon . if the surrounding rock is limestone then the gorge forms by dissolution just like a cave ; in fact many gorges are actually collapsed caves i.e. a cave forms first then the roof collapses . in the case of the grand canyon the rock contains some limestone layers but there is also a lot of sandstone . sandstone is silica ( silica grains loosely bonded ) so it is not soluble in water and would not be eroded just by solution . in this case the erosion is mostly by abrasion of rocks carried in the water , though any limestone present will dissolve and undercut sandstone layers above it . sandstone is quite soft and easily abraded . if the rock is granite this is very insoluble and very hard so it erodes only very slowly . i would guess the main water based ersion method would be freeze thaw cracking . you expanded your title to ask about any erosion of a hard material by a soft material , but rock ersion by water is not a bad place to start as it is quite varied . martin beckett mentioned water jets , and this is another good example if a bit specialised . if you fire the jet of water fast enough it is momentum means it will break off bits from a hard material . i can not think of any example where this is important in nature .
objects , defined as things with mass , do not move at the speed of light . the time dilation factor is $$\gamma = \frac{1}{\sqrt{1 - v^2/c^2}}$$ and it has no limit - it diverges at $v\to c$ . for speeds very close to the speed of light , we could define $\epsilon = \frac{c - v}{c}$ , then we had have $\gamma \sim \frac{1}{\sqrt{2\epsilon}}$ this shows how much time slows down for speeds very near the speed of light . here 's a picture as gamma shoots up to infinity , the time dilation factor becomes arbitrarily large . if you want the clock to go 1/100 as fast , or one millionth as fast , or one quadrillionth , that can be done by going very close to the speed of light ( just solve for $\epsilon$ in the above ) .
there exists a basic misunderstanding in this question concerning mass and energy . the way special relativity works there can be massless particles , of which the photon is a prime example . even though the individual photon is massles , two photons have an invariant mass , the measure of the sum of their four vectors . proof is the two gamma decay of the pi0 . in the big bang model , after all the universe " exploded " from a point , there was only energy which particle physics posits was carried by elementary particles with zero masses since the symmetries of the standard model were not broken in the first moments . once one has elementary particles , even with zero mass each , the ensemble will have an invariant mass which will be the measure of the sum of the four vectors of the particles in the ensemble .
the values of $e_{\infty}$ were probably calculated numerically . they explain in the paper that they rewrote the equation for the energy to contain only $a$ . so what will have done is input a range of realistic values for $a$ and plot the energy in that range . they found a minimum and calculated the value of $e$ in that minimum ( that is not too hard ) . the reason for writing $e_{\infty}$ will be because there is apparently an attractor point in configuration space to which the oscillon tends . this means that after a long time the oscillon will come infinitesimally close to that point . it will ' settle down ' there . the energy corresponding to that point is therefore a good approximation for the energy after a long time ( $t\rightarrow\infty$ ) . that is most likely why this energy is subscripted with an infinity symbol .
show me a distribution of remote mass that would provide the behavior we see for both jupiter in orbit around the sun the many moons in orbit around jupiter which both appear to be $1/r^2$ forces . now try to generalize to support all the moons and planets in the solar system . you can not do it because the system is highly over-constrained .
you have to realize that depending on the dimensions of the variables under consideration the laws of physics change . when the dimensions are of order of h_bar , the planck constant , it is the framework of quantum mechanics , and this is the underlying framework from which all other physics frameworks emerge . quantum mechanics sees elementary particles , molecules and structures less than nanometers , although its results can manifest macroscopically , as in the crystal lattice order , or in superconductors and superfluids which obey large dimensions quantum mechanical equations . in addition to quantum mechanics at the particle level special relativity is also the law of nature . nature and physics which describes nature mathematically have no discontinuities . as dimensions grow statistical mechanics is the framework to describe ensembles of particles/atoms/molecules , in the beginning quantum statistical mechanics emerges as the next framework and then classical statistical mechanics where the velocities are such the newtonian mechanics emerges from special relativity . when instead of looking at the particulate nature of matter , numbers become very large macroscopically , order of 10^23 particle per mole , thermodynamics emerges as a framework to describe the macroscopic behaviors . the laws of thermodynamics were discovered long before statistical mechanics . they seemed absolute and were for this reason called " laws " because they were the postulates on which the mathematics of thermodynamics was developed . but there is continuity , and in the framework on which thermodynamics emerges , statistical mechanics , the law of entropy is shown to be probabilistic , and holds with very very high probability as long as the numbers are of order 10^23 . a common use of statistical mechanics is in explaining the thermodynamic behavior of large systems . microscopic mechanical laws do not contain concepts such as temperature , heat , or entropy , however statistical mechanics shows how these concepts arise from the natural uncertainty that arises about the state of a system when that system is prepared in practice . the benefit of using statistical mechanics is that it provides exact methods to connect thermodynamic quantities ( such as heat capacity ) to microscopic behaviour , whereas in classical thermodynamics the only available option would be to just measure and tabulate such quantities for various materials . statistical mechanics also makes it possible to extend the laws of thermodynamics to cases which are not considered in classical thermodynamics , for example microscopic systems and other mechanical systems with few degrees of freedom . one can define entropy within statistical mechanics considering the probabilities of the microsystems contained in the macrosystem , for example the gibbs entropy : the quantity k_{b} is a physical constant known as boltzmann 's constant , which , like the entropy , has units of heat capacity . the logarithm is dimensionless . so what is a law in one scale of physics is a value dependent on probabilities derived from the microstates . it is very very improbable for the atoms of the floor to align and kick the ball .
my question is , whether this is caused by an modification of the frequency/wavelength or simply by my eye combining the two incoming lights . short answer no so your second picture is accurate . frequency is not modified , the two different waves are added up by your eye to produce the light that you perceive . most computer screens operate on a rgb colour model , i.e. they only have red green and blue lights . so the " yellow " light you see coming from your screen contains zero photons of yellow frequency but the exact right proportion of red green and blue to trick your eye into firing neurons the same way it would if photons of the yellow spectrum were striking it . when " adding " light colours you start with a white wall , that is un-illuminated ( black ) . and then you add red green and blue ( the additive primary colours ) lights to get any colour you want . however when " subtracting " colours , if you start with a white sheet of paper illuminated with white light ( white ) and start adding paints ( subtracting colour ) you can make any colour by using the tree primary colours red yellow and blue ( the subtract-ative primary colours ) , or more accurately as referred to in the printing industry cmyk , cyan magenta yellow and key ( black ) , black is needed because adding enough coloured paint to make an image black is inefficient and expensive . if you are allowed to add and subtract colours you can mix ( almost ) any three different colours to generate the illusion of any desired colour . ie you get to choose any three colours as your primary colours . exactly three colours is required since there are three different types of cones in your eye ( three degrees of freedom ) . also note that the srgb does not encapsulate all different colours , so watching a movie in cinema with a reel film may display hues that cannot be rendered on a computer monitor . so far i have only dealt with the eye and completely ignored the interaction of the light with the interface . in general light is absorbed , reflected or transmitted through the medium . reflected light can be coherent light in a mirror or diffuse like a painted wall . however there are a large number of non-linear optical effects where the photons do interact with each other and may combine . there are also interactions with the medium such as those seen with a black light . links are to the respective wikipedia pages .
there are some missing data in your question . what voltage does the batteries have , i am going to assume 12v since it is common . battery capacity , you typed it as 150a but i guess it was 150ah . please note that a normal car battery on a car like a vw golf has approx 60ah so 150ah is a quite big battery . output power , you state that you have 1000v ( volts ) out , but i guess you are talking about 1000w ( watts ) . and that should be maximum power . output voltage should be either 230v ( or 110v ) . i will assume 230v . the first thing is that a device connected to the ups and is using 1a , that is 1a at 230v or $230v * 1a = 230w . $ then we go inside the ups and have a look . we still has the 230w but since it is now a 12v system , we need to reverse the conversion into amps again and get something like $\frac{230w}{12v}=19a$ . and then we divide the battery capacity with this current and get something like , $\frac{ ( 150*2 ) ah}{19a}=15h$ . please note that those 15 hours is the best value you can expect to see , but since there is losses in the conversion from 12v to 230v we probably loose some 10-20% of the energy , and that translate directly into a shorter time . let 's say 80% of 15h would be approx 12h . then i must add that fortunato does have a point . batteries degrade over time and can not hold the same charge , so make sure you have some margin and check/service your ups on yearly basis .
the affine galilean structure is assigned by the first principle of newtonian dynamics , i.e. by giving the class of inertial reference frames in the spacetime $g^4$ . on the one hand it assigns the structure of an affine space to the spacetime , on the other hand it selects a subclass of permitted transformations between reference frames . a reference frame is a bijective map $g^4 \ni p \mapsto ( t ( p ) , {\bf x} ( p ) ) \in \mathbb r \times \mathbb r^3$ the class of allowed coordinate transformations is that including all of the transformations : $$t ' = t + c \quad ( 1 ) $$ $${\bf x}' = r{\bf x} + t {\bf v} + {\bf c}\: , \quad ( 2 ) $$ where $c\in \mathbb r$ is constant , ${\bf v}$ and ${\bf c}$ are constants in $\mathbb r^3$ and $r \in o ( 3 ) $ . these are the transformation of coordinate between inertial reference frames . in this way a foliation $\{\sigma_t\}_{t \in \mathbb r}$ of $g^4$ turns out to be defined , made of 3-dimensional euclidean spaces . also a surjective map $t : g^4 \to \mathbb r$ is consequently defined , that labels the class of those manifods . the function $t$ ( absolute time ) coincides , up to an additive constant , with the coordinate $t$ of every inertial reference frame , so that intervals of time are absolute . the coordinates ${\bf x}$ of every inertial frame range in each absolute 3-dimensional space $\sigma_t$ that is in common with each of these observers , including the metrical structures that are invariant changing reference frame , in view of ( 2 ) . concerning special relativity . yes , you can completely characterize minkowski spacetime $m^4$ as a real affine 4d space equipped with a pseudo distance with signature $ ( 1 , -1 , -1 , -1 ) $ and a time orientation . the inertial reference frames are a subclass of the affine coordinate systems on $m^4$ where the pseudo distance assumes the canonical form you have written in your question . the transformations between these coordinate frames are the so called poincaré orthochronous transformations . addendum to answer the last question : the relativistic corresponding of galileo group is poincaré orthochronous group and not the lorentz group . orthochronous poincaré transformations are the most general transformations preserving the structure of minkowski spacetime including time orientation . these are , obviously , affine transformations as galilean transformations are .
something moving such that it " measures the proper time " is simply something moving from point 1 at time 1 , to point 2 at time 2 , as seen in s ( you have the data ) . recall the basic definition of velocity .
well i ought to be studying for a physics exam , but i will consider answering this to be my studying . newton 's third law states that for every action , there is an equal an opposite reaction . in this case , the jetpack is ejecting water at high velocity toward the ground . this is generating a significant force downward . the resulting opposite force pushes upward , elevating the rider . the actual pump for the unit is located on a floating watercraft attached to but not elevated with the jetpack . this is how there is enough force for the unit to fly - otherwise , the mass of the pump would be so great that it would be difficult to maintain enough pressure to keep both the pump and rider aloft . i do not know the specifics of how it orients itself , though i imagine that by making small variations in the water pressure and orientation of each nozzle , one can cause an imbalance in the forces acting on the rider , casing them to move one way or the other .
we can prove it in perturbative string theory but it is probably valid beyond it . in perturbative string theory , any ( continuous ) global symmetry has to be associated with a conserved charge which , because of the locality of the physics on the world sheet , implies the existence of a world sheet current $j$ or $\bar j$ or both ( left movers vs right movers ) whose left/right dimension is $ ( 1,0 ) $ or $ ( 0,1 ) $ or both ( because its integral has to be a conformally invariant charge ) . typically , such a symmetry might be something like the isometry of the target ( spacetime ) manifold or something on equal footing with it . it follows that one may also construct operators $j\exp ( ik\cdot x ) \bar\partial x^\mu$ or $\bar j \exp ( ik\cdot x ) \partial x^\mu$ or both with a null vector $k$ which have the dimension $ ( 1,1 ) $ , transform as spacetime vectors , and therefore belong to the spectrum of vertex operators of physical states which moreover transform as spacetime vectors , i.e. they are the gauge bosons ( the $x$ only go over the large spacetime coordinates ) . one would need to prove that the multiplication of the operators does not spoil their tensor character but it usually holds . consequently , any would-be global symmetry may automatically be shown to be a gauge symmetry as well . the argument above only holds for the gauge symmetries that transform things nontrivially in the bulk of the world sheet . but even symmetries acting on the boundary degrees of freedom , i.e. the chan-paton factors , obey the same requirement because one may also construct ( open string ) vertex operators for the corresponding group that transform properly . we do not have a universal non-perturbative definition of string theory but it is likely that the conclusion holds non-perturbatively , too . in some moral sense , it holds for discrete symmetries as well even though discrete symmetries do not allow gauge bosons .
from the point of view of people standing on the earth , no effect whatsoever . that is because the so-called " relativistic mass " is an effect of different frames of reference . ( it is also pretty trivial from the pov of someone at rest with respect to the sun , surpressed by factors of order $\frac{30000\text{ m/s}}{300000000\text{ m/s}} = 10^{-4}$ . ) for many purposes working scientists have almost entirely stopped using the phrases " relativistic mass " and " rest mass " , finding the former concept to be of little practical use .
the big change in understanding that happened with quantum physics is the idea that the universe is random , rather than clockwork , at its lowest level . the double slit experiment the classic example is that if i fire a particle ( e . g . an atom ) at board with 2 slits in it , the particle could go through slit a , or slit b , before hitting a plate at the back . it turns out , however , that the particle behaves like a wave , and goes through both at the same time , and you see an interference pattern on the plate at the back . but , if i put a measuring device on one of the slits to find out whether it really goes through both , it stops behaving like a wave , and switches to behaving like a particle instead , going through one slit or the other - no interference pattern . it changes if you watch it what seems to be happening here is that the atom encounters the 2 slits and has a certain probability of going through either slit . but since nothing ( like a detector ) is forcing it to reveal which one it went through , it continues as though it went through both . when it hits the plate at the back , it is forced to reveal its position , so it randomly picks a position to be in . but when you put the detector next to one slit , the particle has to reveal its position earlier , so the possibilities are different , and we get different behaviour and a different pattern on the back plate . consciousness often people use this to suggest that it is because we ' look ' at the particle that its behaviour changes . this is rubbish . the difference appears when you put a detector there , not when you , as a human , look at the result . if you put the detector in place and then refused to view the result , it would not change the result . so what prof jim was saying is that when we measure things like the position of an atom , we unavoidably change its behaviour , because we force it to decide on an answer , rather than carrying on as it was .
to formalize michael brown 's comment : suppose we want to change the units that we are working in to ones that are naturally set by the problem . we can write the dimensional variables $x$ and $t$ as $x = s_x \bar{x}$ and $t = s_t \bar{t}$ where $s_x$ and $s_t$ are the length and time scales respectively . these scales carry the dimensions of the quantity and the variables with bars are dimensionless variables , ie just pure numbers . obviously , in the si system $s_x$ is 1 meter and $s_t$ is 1 second . let us substitute our expressions for $x$ and $t$ into the equation that you found : $$mv^2+kx^2 = m \big ( \frac{dx}{dt}\big ) ^2 +k x^2 = m \frac{s_x^2}{s_t^2} \big ( \frac{d \bar{x}}{d \bar{t}}\big ) ^2 + k s_x^2 \bar{x}^2 = m \frac{s_x^2}{s_t^2} \bar{v}^2 + k s_x^2 \bar{x}^2 . $$ we are free to use whatever unit system that we like , so let 's choose $s_x = \lambda$ and $s_t = \sqrt{m/k}$ . substituting this above gives $ k \lambda^2 \bar{v}^2 + k \lambda^2 \bar{x}^2 = k \lambda^2 $ or $$ \bar{v}^2 + \bar{x}^2 = 1 , $$ which is just the equation of a circle of radius 1 . so , just by changing the units we have moved the distance between the foci to 0 . this implies that the distance between the foci itself can not be a physical quantity because we set it to 0 just by rescaling . we cannot ever recover the original answer in the si unit system from this one . however , the distance between the foci can be related to a physical quantity . if the semimajor axis of the ellipse is labeled $a$ and the semiminor axis labeled $b$ then the area enclosed by the ellipse is $\pi a b$ . in your equation in the original units ( assuming without loss of generality that $k/m &lt ; 1$ ) $a = \lambda$ and $b = \lambda \sqrt{k/m}$ so the area is $ \pi \lambda^2 \sqrt{\frac{k}{m}}$ . this area is known as the symplectic area , and it is a conserved quantity for hamiltonian systems . interestingly , even for hamiltonians that are time dependent , the symplectic area is still conserved . we can write the area of an ellipse in terms of the distance between the foci . if the distance between the foci is $c$ , then the semiminor axis is defined to be $b^2 \equiv a^2 - c^2$ , so the area can be written as $\pi a b = \pi a \sqrt{a^2 - c^2}$ . you can check that by using the scales we considered earlier , you can convert the symplectic area for the original case to the symplectic area for the rescaled case ( the circle ) meaning that the symplectic area is a perfectly physical quantity . if you are looking for something you can measure , then the period of motion , $t$ , of the object along the curve can also be found from the symplectic area . $ t = \frac{da}{de}$ where $a$ is the symplectic area and $e$ is the energy . in short , the locations of the foci and the distance between them are themselves not physical quantities . however , they can be related to some physical quantities , namely the symplectic area and the period of motion .
starting with your given equation , we add $p^2 c^2$ to both sides to get $$ e^2=m^2 c^4 + p^2 c^2$$ now using the definition of relativistic momentum $p=\gamma m v$ we substitute that in above to get $$e^2 = m^2 c^4 + ( \gamma m v ) ^2 c^2=m^2 c^4 +\gamma^2 m^2 v^2 c^2$$ now , factoring out a common $m^2 c^4$ from both terms on the rhs in anticipation of the answer we get $$e^2=m^2 c^4 ( 1+\frac{v^2}{c^2}\gamma^2 ) $$ now using the definition of $\gamma$ as $$\gamma=\frac{1}{\sqrt{1-\frac{v^2}{c^2}}}$$ and substituting this in for $\gamma$ we get $$e^2=m^2 c^4 \left ( 1+\frac{\frac{v^2}{c^2}}{1-\frac{v^2}{c^2}}\right ) $$ and making a common denominator for the item in parenthesis we get $$e^2=m^2 c^4 \left ( \frac{1}{1-\frac{v^2}{c^2}} \right ) =m^2 c^4 \gamma^2$$ taking the square root of both sides gives $$e=\pm \gamma mc^2$$ hope this helps .
the volumes of phase spaces have different dimensions ( units ) depending on the number of decay products but this is no problem because there exists a natural mass scale , namely $m$ , the mass of the unstable particle at the beginning . if the phase space over which the integrand is comparable to its maximum is of order $m^k$ where $k$ is an exponent determined by the dimensional analysis ( i.e. . by the number of final particles ) , we deal with the generic situation . if it is smaller ( or much smaller ) than $m^k$ , the decay rate is phase-space-suppressed . the neutron decay is a textbook example of the phase-space suppression because the electron 's and antineutrino 's energies can not go up to $o ( m ) $ where $m$ is the neutron mass ; instead , these energies have to be thousands of times smaller because $m_{n}\approx m_{p}$ and the proton already takes almost all the energy . the " generic " lifetime of a strongly as well as weakly interacting , weakly decaying particle of this mass would be a tiny fraction of a second but because of the phase space suppression , neutrons live for 15 minutes or so ( mean lifetime ) .
an object is not necessarily heated to a plasma when it falls into a black hole . with quasars matter spirals down towards the event horizon so both it is speed and density increases , but this does not heat it directly . it is because matter interacts with the other matter around the event horizon that you get collisions and heating and the spectacular x-ray emission . by contrast the black hole at the centre of our galaxy is thought to be fairly quiet because it is already gobbled up all the matter in it is vicinity . if you jumped into it you had probably make it through the event horizon unharmed and it would only be near the singularity that tidal forces squished you . you need to bear in mind that once matter has passed the event horizon it is fall to the singularity is very quick , so there is not very much matter within the event horizon that has not already fallen into the singularity . what happens to the matter at the singularity no-one knows . response to comment : my point is that an acretion disk is not a feature of all black holes . accretion disks only form where a black hole is actively swallowing matter . also accretion disks will form around any heavy object . arguably saturn 's rings are a form of accretion disk with the matter in them eventually falling into saturn . my other point is that for matter falling into a black hole nothing special happens when it crosses the event horizon . if you were falling into a black hole then you would not be able to detect when you had crossed the event horizon . so if you had been heated up by friction in an accretion disk before you hit the event horizon you had be in pretty much the same state after you had crossed it . the state you are in , whether it is plasma or not , is dependant on how much you got heated up as you fell through the accretion disk ( if an accretion disk is present ) and is not anything specifically related to the presence of an event horizon .
when the finite speed of light – the delay of the rays from the source – is taken into account , one encounters many optical effects aside from " how the world is in relativity effects " such as lorentz contraction . flat lines look like arcs or cicles , one may see " behind himself " , and there is of course the doppler shift of the frequencies depending on the relative speed . also , streetcars going from left to right are rotated around a vertical axis , and so on . you may download real time relativity as a great " relativistic 3d game " . for a link and other comments on relativistic optical effects , see http://motls.blogspot.com/2009/02/relativistic-optical-effects.html?m=1
a superfield is a function of the bosonic coordinates $x^\mu$ as well as the supercoordinates $\theta^a$ and $\theta^{\dot c}$ . it may be taylor-expanded in the latter . the taylor expansion in terms of anticommuting , grassmann variables is inevitably finite because the square of each grassmann variable is already zero , e.g. $\theta^1\theta^1=0$ . consequently , the expansions of the superfields have a finite number of terms such as $$ \phi ( x , \theta ) = h ( x ) + \theta^a \psi_a + \theta^{\dot c}\psi_{\dot c} + \theta^a\theta^b \epsilon_{ab} h +\dots $$ note that both sides of the equation above are grassmann-even , bosonic fields , which implies that the coefficients in the terms on the right hand side with an even number of $\theta$-like factors are bosonic , i.e. grassmann-even , for example $h , h$ , while those with an odd number such as $\psi_a$ are grassmann-odd i.e. fermionic . if you act on the expression above by $\partial / \partial \theta^a$ , it erases the $\theta^a$ factor ( and if there is none , the derivative of the term vanishes ) . consequently , the $\theta$-derivative of e.g. $\phi ( x , \theta ) $ produces terms like $\psi_a$ without any $\theta$ factor . so if the original superfield is varied by a multiple of the $\theta$-derivative of itself , $$\delta\phi = \epsilon^a \psi_a+\dots $$ you see that the term proportional to $\psi_a$ has the same $\theta$-structure ( namely no $\theta$ factors ) as the $h$ fields had at the beginning . so the variation of $h$ ( a bosonic field ) is proportional to $\psi_a$ ( a fermionic field ) – $\delta h \sim \epsilon_a \psi^a $ – which must of course be multiplied by another fermionic object , namely $\epsilon^a$ , a grassmann parameter of the supertransformation ( you should view this as a pure number , an angle , even though it is anticommuting : it is not a field ) . this " variation of boson is proportional to a fermion and vice versa " is what we mean by " susy transforms bosons to fermions and vice versa " . of course , all the other terms in $q^a$ ( and when acting on all the other terms in $\phi$ or any other superfield ) have the same effect of exchanging bosons and fermions . it boils down to the statistics or grading .
this is a famous period doubling experiment which is reprinted in cvitanovic 's " universality in chaos " , along with other foundational papers on the period doubling route to chaos . the paper you read is probably due to libchaber and maurer " a rayleigh bernard experiment : helium in a small box " proceedings of the nato advanced studies institute on nonlinear phenomena at phase transitions p . 259 ( 1982 ) . or else giglio , musazzi , perini " transition to chaotic behavior via a reproducible sequence of period doubling bifurcations " prl 47 , 243 ( 1981 ) or else libchaber , laroche , fauve " period doubling cascade in mercury , a quantitative measurement " j . phys . lett 43 l211 ( 1982 ) i just copied these from cvitanovic 's table of contents , i read this paper you are talking about and it is one of these . i should point out that it is not surprising theoretically that the period doubling cascade is the same as any other one-dimensional map , because this is the most likely way for a system to make a period doubling cascade--- to have two directions unstable at once is measure zero , so you have a one-dimensional instability from a fixed point , and this is modelled by feigenbaum , and it is universal , so universally applicable .
the standard book is introduction to solid state physics ( 8th ed . 2005 , isbn 0-471-41526-x ) by charles kittel . your question should be answered in chapter 13 . all that is said there should in principle be applicable to semiconductors . but since their bandgap is lower than in dielectrics you might have a problem measuring this ( they might just be too conductive an the effect is lost ) . on the other hand in your prototypical gaas crystals there is a rather strong effect in the ( 111 ) direction due to the stacking of ga and as layers and their asymmetric response to stress . . . note though that this has nothing to do with electron/hole pairs . this is strictly a polarisation phenomenon .
if you consider a standard differential operator $b$ working on functions defined in $\mathbb r^n$ , like $\partial/\partial x_i$ or a polynomial of partial derivatives , and pick out a sufficiently smooth function $f$ vanishing in a neighbourhood $\omega$ , you see that also $bf$ vanishes therein . this is the relevant notion of locality for operators . in the rhs of the equation you wrote down an operator shows up which does not fulfil locality in the sense i said . that equation is , in fact , the equation satisfied by the positive energy solutions of klein-gordon equation . the operator in the rhs cannot be defined by formal taylor expansion ( it works only formally ) , but one has to use spectral theory . in the considered case it is equivalent to translate that equation in fourier transform . non locality arises here due to a known property of the operator $a:= \sqrt{-\delta + ai}$ and , more generally , for $ ( \delta + ai ) ^\nu$ with $\nu \not \in \mathbb z$ . this property is called anti locality ( i.e. . segal , r.w. goodman , j . math . mech . 14 ( 1965 ) 629 ) and is related to the famous reeh and schlieder property in qft . anti locality means that if both $f$ and $af$ vanish in a bounded region $\omega \subset \mathbb r^3$ then $f$ is everywhere zero . if $f$ has support included in a bounded open set $\omega$ , then , remarkably and very differently from what happens for standard differential operators , $af$ does not identically vanish outside $\omega$ otherwise $f$ would be the everywhere zero function .
the first image shows an object traveling at mach 1 ( $v=c$ ) . the second one shows the object traveling at some supersonic velocity ( $v&gt ; c$ ) . for both the cases , the longitudinal pressure waves pile up . say the observer is standing in the ground and the object is traveling at $c$ . the observer can not hear the pitch of sound because , the waves reach him all at once and hence , he had hear a loud " bash " . the most necessary thing is that he had to wait until the source arrives . when the source is directly overhead , he hears the shock waves . when the object breaks the sound barrier ( supersonic ) , it is somewhat worse . the same loud " thump " is produced here . but , the observer would notice a delay in sound ( i.e. ) he has to wait for the shock waves to reach him . there is also this mach cone produced by these waves since the waves group so fast behind the object . and so , there is a region of high pressure at first followed by a low pressure zone . thus , if the object passes by in some comparable distance , it makes a lot of disturbance , " breaking things " , etc . . . the comic thing is , for someone inside the aircraft , he can still speak with his partner , can hear the bump of a ball on the plane , etc . the problem is only for the distant observer who suffers . . .
there are two that i know of in the context of state estimation . the first is for estimating the mean of $p$ and is a metropolis-hasting mcmc algorithm here : optimal , reliable estimation of quantum states . the second is also mainly for computing the mean ( but can do other functions -- including the characteristic function of the region you are interested in ) . it is a sequential monte carlo algorithm and is here : adaptive bayesian quantum tomography .
the calculation is described in detail in the wikipedia article on recombination . if you consider the ionisation of hydrogen as a reaction : $$ p + e \rightarrow h + \gamma $$ then you can write down an expression for the equilibrium constant as a function of temperature using the saha equation : $$ \frac{n_pn_e}{n_h} = \left ( \frac{m_ek_bt}{2\pi\hbar^2} \right ) ^{3/2} \exp \left ( \frac{-e_i}{k_bt} \right ) $$ if you take 50% ionisation you can work out the corresponding temperature and it turns out to be about 4,000k . so now it is just a matter of relating the temperature of the universe to the time after the big bang . once we are past the various phase transitions that happened in the first few instants after the big bang the temperature is inversely proportional to the scale factor . sadly there is not a simple equation to give the scale factor as a function of time , however it is a straightforward numerical calculation , and the result is that the temperature was 4,000k about 380,000 years after the big bang . that is how the figure of 380,000 years is calculated .
sorry , i was not being very clear . what i mean is , why do photons interact with electrons ? what we have discovered up to now with our studies in physics is that there exist 4 fundamental interactions of elementary particles . both the photon and the electron are elementary particles and interact with the electromagnetic interaction . now the electrons can be free , as for example in an accelerator beam , or bound with the electromagnetic interaction in an atom , as in the hydrogen atom . if they are free , a photon hitting them will scatter elastically , or might give up part of its energy to the electron and go away with a smaller energy/frequency ( $e=h\nu$ ) . if bound , it is in an energy level about the nucleus which has a unique , quantized energy , $e_1$ . over it will be unoccupied energy levels . an incoming photon , if it has an energy that corresponds to the difference between an empty energy level $e_2$ , i.e. it has energy $e_2-e_1$ can transfer its energy to the electron kicking it up and disappearing as an individual photon . the electron will probably decay from that energy level emitting a photon of energy $e_2-e_1$ but it will be a different photon . in nuclei with large $z$ there may be cascades of photons if the energy of the initializing photon is large and there exist intermediate energy levels . now if we go to second quantization , the photon interacts with the electron because it is the carrier of the electromagnetic force . this covers both the bound and the unbound state of the electron , except that in the bound case still the energy has to be $e_2-e_1$ to give a large enough probability of interaction .
the no-cloning theorem states that it is not possible to have a quantum state $|\psi&gt ; $ evolve into two separable ( non-entangled ) copies described by the tensor product state $|\psi&gt ; |\psi&gt ; $ . the proof boils down to the simple observation that when expressing $|\psi&gt ; $ in some basis ${|0&gt ; , |1&gt ; , |2&gt ; , . . . }$: $$|\psi&gt ; = \alpha_0 |0&gt ; + \alpha_1 |1&gt ; + \alpha_2 |2&gt ; + . . . $$ the cloning operation would be a unitary evolution of the form : $$u ( \alpha_0 |0&gt ; + \alpha_1 |1&gt ; + . . . ) = \alpha_0^2 |0&gt ; |0&gt ; + \alpha_0 \alpha_1 |0&gt ; |1&gt ; + \alpha_1 \alpha_0 |1&gt ; |0&gt ; + \alpha_1^2 |1&gt ; |1&gt ; . . . $$ this leads to a contradiction , as the unitary operator $u ( . . ) $ is linear and can never create amplitudes like $\alpha_0^2$ and $\alpha_0\alpha_1$ that are quadratic functions of the $\alpha_i$ . so the linearity the author is referring to is the linearity of the unitary evolution . in quantum physics evolution is described by unitary operators which transform incoming states into outgoing states that are a linear combination of the ingoing states .
cygnus x-1 is a binary system , and the radius you cite ( taken presumably from wikipedia article ) is the radius of the binary system not the radius of the black hole . the wikipedia article is highly misleading in this respect . at least , i think it is the radius of the binary system . 20-22 r☉ is about 0.1 au which i think is about the suggested spacing between the two stars .
the external field causes the magnetic dipole moments $\mathbf m$ of the atoms in the material to align with the applied field $\mathbf b$ . if one now imagines summing up the fields due to all of the tiny little dipole moments that are now aligned with the external field , then one will find that the net effect is that the field inside is augmented .
( note : this is not a definitive answer and is largely guesswork , but it might be useful for further answerers . ) at first i suspected it was some sort of weak spring or magnetic device which applied an opposing torque to the arm , so as to make the center reading unique . searching google , i found this explanation of the theory of a triple-beam balance offered by ohaus , and it mentions that there is a magnetic dampener that couples to the free end of the arm that is used to stop the oscillation of the beam ( presumably by the generation of opposing electrical eddy currents ) : unfortunately , the slide in question also specifically notes that " this resistance is to movement and not an attractive force thus no added torque is applied " , so this is not the correct reason . i searched through the rest of the slides to see if there was any mention of an angle-dependent opposing force , but there did not seem to be any . however , it is obvious that there has to be an opposing torque that occurs when the arm moves up and down , as otherwise if the two arms were balanced , there would be a wide range of accessible angles which the arm would be stable in , whereas having worked with these sort of scales before i can say that there is a restoring force that depends on angle . possible leads of investigation : according to the same set of slides , the weighing platform makes contact with the balancing arm using a knife-edge bearing : as a result , if the weighing arm angle is $\theta$ , the torque $t_\theta$ provided by the weighing platform will " correctly " be $$t_\theta=t_0\cos ( \theta ) . $$ otherwise , using a different sort of bearing could cause the load to be applied closer or further down the arm , destroying the $\cos ( \theta ) $ dependence . however , there is no mention of how the sliding weights make contact with the arm . so i would try to find out how the sliding weights make contact with the arm , as this could potentially be used to ensure that there is a net angular difference on how the two sides react to changes in angle . for example , if the right edge of the sliding weight makes contact with the balancing arm when the sliding-weight section is raised higher , it would cause the torque to decrease slower than the normal $\cos ( \theta ) $ dependence that the left-hand side experiences , and thus there would be a restoring force that drives the sliding-weight section downwards . or maybe the post which is connected to the weighing platform and extends downwards into the hole on the top of the balance is actually connected to a spring : unfortunately i do not have access to a mechanical scale at the moment ( all mine are digital ) . honestly the easiest way would be to ask one of their engineers , but that'd sort of be cheating : ) .
buying directly from the shop ensures you will get collimated binocular instead of buying it online . but there are some companies such as garret opticals from us . they ship with great care and the binocular comes with excellent collimation . i ordered their binocular gemini 15x70 it came with excellent collimation . so if you are in us then you can order from them . yea the difference between the optical clarity depends on the type of prism used .
ice and snow reflect light quite a bit , so you might be better off focusing the light onto something black and then using the radiant heat from that to melt the snow . but in case you want to do it directly , i would suggest pointing the axis of the parabola at the sun . you do not need a complete parabola ( whatever that means ! ) , as any rays that hit it parallel to the axis will end up at the focal point . a parabola with equation $x = \frac{y^2}{4a}$ ( opens rightward ) has a focus at $x=a$ . so assuming the light comes in along the $x$ axis , the ground would some line passing through the point $ ( x=a , y=0 ) $ sloping downward . i.e. , $y= \tan\theta\ , ( x-a ) $ . you only need the part of the parabola that is to the right of that line ( i.e. . , above ground )
the significance of the metric : $$ d\tau^2 = dt^2 - dx^2 $$ is that $d\tau^2$ is an invarient i.e. every observer in every frame , even accelerated frames , will agree on the value of $d\tau^2$ . in contrast $dt$ and $dx$ are coordinate dependant and different observers will disagree about the relative values of $dt$ and $dx$ . so while it is certainly true that : $$ dt^2 = d\tau^2 + dx^2 $$ this is not ( usually ) a useful equation because $dt^2$ is frame dependant .
oneat , yes , and i understood what you meant " i mean this one " . ; - ) you meant that it is the frequency $\nu$ ( nu ) that appears in the formula $e=h\nu$ for the photon energy , as opposed to $\omega$ that appears in $e=\hbar\omega$ and that differs by a factor of $2\pi$ . whenever you hear "$\mbox{hz}$" or any multiple of it , it means that the frequency is defined in the same convention as the frequency in $e=h\nu=hf$ , without the slashed $\hbar$ ( hbar ) . if someone wanted to express the angular frequency $\omega=2\pi\nu$ , he would have to write the unit as $\mbox{s}^{-1}$ instead of $\mbox{hz}$ . dimensionally , $\mbox{hz}$ is the same thing as $\mbox{s}^{-1}$ . however , people deliberately use both units so that the inverse second is reserved for the angular frequency $\omega$ and the hertz is reserved for the old-fashioned frequency $\nu$ . $\nu=89 \mbox{mhz}$ is the same thing as $\omega=5.6\times 10^{8} \mbox{s}^{-1}$ .
the gas will not ignite as long as the concentration is higher than uel/ufl ( upper explosive/flammability limit ) . so as long as there is no hole , you should be fine . that said , i would not play with hydrogen if i can use helium instead !
there are not mechanical oscillators that have the same frequencies as visible light . the most common oscillators with the same frequencies as visible light are the electrons orbiting atoms , which is why atomic transitions emit and absorb visible light . if you are willing to accept a powered system , you could absorb all the light into an image-processing system and then project that image in another direction , possibly with much more light intensity than the original image had . for instance , folks with telescopes sometimes use video cameras to observe objects too faint to see naked-eye in real time .
to show that this measure is lorentz invariant you first need to explicitly write your integral as an integral over mass shell in 4d k-space . this could be done by inserting dirac delta function $\delta [ k^\mu k_\mu-m^2 ] $ and integrating over the whole 4d space . then you could apply the following transformations : \begin{align} \theta ( k_0 ) \cdot\delta [ k^\mu k_\mu-m^2 ] and = \theta ( k_0 ) \cdot\delta [ k_0^2-|\mathbf{k}|^2-m^2 ] \\ and =\theta ( k_0 ) \cdot\delta\left [ ( k_0-\sqrt{|\mathbf{k}|^2+m^2} ) ( k_0+\sqrt{|\mathbf{k}|^2+m^2} ) \right ] \\ and =\frac{\delta\left [ k_0-\sqrt{|\mathbf{k}|^2+m^2}\right ] }{2\ , k_0} , \end{align} where heaviside function $\theta ( k_0 ) $ is used to select only future part of the mass shell .
i ) the proofs of both the first ( algebraic ) bianchi identity and the second ( differential ) bianchi identity crucially use that the connection $\nabla$ is torsionfree , so they are not entirely consequences of the jacobi identity . proofs of the bianchi identities are e.g. given in ref . 1 . ii ) the second bianchi identity may be formulated not only for a tangent bundle connection but also for vector bundle connections . iii ) the lie bracket in the pertinent jacobi identities is the commutator bracket $ [ a , b ] :=a\circ b -b\circ a$ . the jacobi identity follows because operator composition "$\circ$" is associative . iv ) in the context of yang-mills theory and em , the second bianchi identity follows because the gauge potential $a_{\mu}$ and the field strength $f_{\mu\nu}$ may be viewed as ( part of ) a covariant derivative and corresponding curvature tensor , respectively . references : m . nakahara , geometry , topology and physics , section 7.4 .
you are asking the following question : suppose a particle is travelling to the left . if you have a magical reverser , which reverses it is momentum , you can make it go back to where it started . suppose you measure the position of the particle along the way , and then apply the reverser . then the particle will not necessarily go back to where is started . you are asking why this is , why there is an unavoidable disturbance to the state as a result of measurement . first , it must be said that a measurement does not necessarily disturb the particle . if you have a particle travelling in a wavepacket which is a gaussian times a plane-wave , you can suddenly apply an external co-moving potential which is an appropriately sized harmonic oscillator , and check if the particle is in the n-th energy state for n not zero . if you get a null result , you have confirmed that the particle is in the ground state , and then you can release the potential , and let the particle continue on its motion . if you do all this quickly enough so that there would be no significant wavepacket spreading during this time interval for the free particle , you have confirmed the wavefunction is a moving gaussian , and if you use your reverser , you will reverse the motion . this is a simple non-demolition measurement . the issue with measurement in quantum mechanics is simply that when you make a measurement , different states give different macroscopic outcomes . the remaining state is the one consistent with the macroscopic outcome you observe , and this state behaves differently than the original . although you can choose a measurement which will not affect any one given state , for a general state , a measurement will always kick the state into one of the special directions which give a definite result for the measurement . there is no answer to this question which does not involve learning quantum mechanics , unfortunately , since this effect is central to the description .
the assumption here is that the water is more incompressible than a balloon , so that the density variations in the water are less than the density variation in a balloon . this is true for a balloon , but false for a submarine made of steel . the compressibility of steel is roughly 80 times less than the compressibility of water , so if the submarine has thick steel walls , it will make a stable depth equilibrium using bouyancy alone . you can maintain a depth with a submarine by adjusting the density very slightly to the one appropriate for water at a given depth . for the balloon , the equilibrium is unstable , but for there to be an equilibrium at all requires that the balloon skin be made of a material significantly denser than water , and the depth where there is balance , the air will be compressed enormously . edit : submarine compressibility ( in response to the comments of zassounotsukushi ) when a submarine is submerged in water , the compressibility is not determined by the bulk compressibility of steel , because the pressure stress has to travel through the much thinner hull to get from one side of the submarine to the other . the actual stress in the hull is increased . to see how much it is increased , roughly , consider a model submarine which is a square-box-cylinder , with walls of thickness w and side-length l . the pressure is a flow of momentum per unit length equal to pl from one side ( times the length ) of the box to the other , through the sides of the wall , and if the thing were perfectly solid , the compressibility would be roughly 80 times less than water . but the momentum must flow from right to left through a region of width 2w instead of the full width l , as it would be in a solid steel cylinder , so the actual momentum current in the side of the box is increased by a factor of l/2w . for a round cylinder , up to a factor of order unity ( it will be between . 5 and 2 ) , the analogous factor is r/w . for a typical submarine , i found a hull width w of 30 cm , while a spatious radius is 6m . the ratio is 1/20 , so the submarine is about 20 times more compressible than bulk steel . but this still makes the submarine 4 times less compressible than water , and makes the equilibrium stable . steel balloon in order to make a hollow steel balloon neutrally bouyant , ignoring the density of air , the ratio of unoccupied volume to occupied volume is as the ratio of the density of steel to air volume is as the ratio of the density of steel to water , about 8 to 1 . since this ratio is much smaller than the bulk modulus ratio of 80 to 1 , a neutrally bouyant steel balloon will acheive a stable equilibrium . this was the model i originally had in mind for a submarine , and for this model , the walls are extremely thick , and the bulk compressibility is only changed by a factor of order 10 at most , not 80 . but this model is nonsense : it is assuming an empty submarine ! the accurate description is above .
the space station could shoot itself , but it is extremely unlikely to happen by accident . assuming your space station is in a circular orbit you can calculate it is position in polar co-ordinates as a function of time $ ( r ( t ) , \theta ( t ) ) $ very easily since $r$ is constant and $\theta = 2\pi t/\tau$ where $\tau$ is the orbital period . when you fire the cannon the shell is in a different orbit , and specifically it is in an elliptical orbit $ ( r' ( t ) , \theta' ( t ) ) $ . for the station to shoot itself the two orbits must intersect , i.e. at some time $t$ you have simultaneously : $$r ( t ) = r' ( t ) $$ $$\theta ( t ) = \theta' ( t ) $$ the problem is that for a generic elliptical orbit the expressions for $r' ( t ) $ and $\theta' ( t ) $ are not at all simple so there is no easy way to solve the above simultaneous equations and work out at what time , if ever , they intersect . there is certainly no obvious reason to suppose they should intersect . you can see that it is possible for the spaceship to shoot itself . if you fire the cannon radially outwards the shell will fall behind the space station . if you fire in the direction of motion the shell will move ahead of the space station . so there must be some angle in between where the shell hits the space station . however this will be the exception rather than the rule . i am aware this is not a great answer since i can not solve the equations of motion and give you a rigorous answer . if anyone else can do this i would be very interested to see the calculation .
the pairs of lines are the same phase and at the same voltage - they are really just a single thick wire split into two thinner ones . it is easier to install two smaller wires to double the current capacity than a single thicker wire . it is easier to handle the lighter cable and you can stock just a single gauge of wire and handling equipment . it also provides some redundancy if one wire fails . there is an effect with ac electricity that the current mostly flows near the surface of the conductor . a number of thinner wires have more area-near-the-surface and so a larger effective cross section area than a single thick one . at the 50/60hz frequencies used by ac transmission this only affects wires more than a cm thick .
permittivity $\varepsilon$ is what characterizes the amount of polarization $\mathbf{p}$ which occurs when an external electric field $\mathbf{e}$ is applied to a certain dielectric medium . the relation of the three quantities is given by $$\mathbf{p}=\varepsilon\mathbf{e} , $$ where permittivity can also be a ( rank-two ) tensor : this is the case in an anisotropic material . but what does it mean for a medium to be polarized ? it means that there are electric dipoles , that units of both negative and positive charge exist . but this already gives us an answer to the original question : there are no opposite charges in gravitation , there is only one kind , namely mass , which can only be positive . therefore there are no dipoles and no concept of polarizability . thus , there is also no permittivity in gravitation .
in a sense yes , if you are very careful about what you are holding constant . stating that a variable is proportional to another variable implies that all other relevant quantities are being held constant . for example , there is a simple relation $d=vt$ that describes the distance $d$ something travels in a time $t$ when traveling at speed $v$ . one might say that $d$ is proportional to $t$ . however , this relation is only valid if the speed $v$ is constant throughout the interval $t . $ in your example , one could say that mass is proportional to displacement if the $k$ and $a$ are constant , but you will need to do some work to figure out what physical system ( s ) meet such requirements .
the accelerations of k and l will be different from one another and also from the particle a . to solve this consider $a_p$ to be the acceleration of the pulley j and $a_r$ be the relative accelerations of the particles k and l relative to the frame of reference of the pulley j . then the net acceleration of the particle k will be $a_p-a_r$ and that of particle l will be $a_p+a_l$ . now apply the equations for the particles k and l and the acceleration term wont cancel out . ( you can interchange the accelerations of both the particles and the only change it will bring is the sign of the answer and that will tell you the actual direction of the particles k and l ) for k $$s-12g=12 ( a_p-a_r ) $$ and for l $$s-9g=9 ( a_p+a_r ) $$ . solve this and you will get your answer .
loosely speaking , the gradient of a scalar field ( such as the electrostatic potential ) points in the direction of that field 's greatest change . since no change occurs in the field when you go along the surface , the gradient should not have a component in that direction . here is another intuitive explanation : imagine for a moment that the electric field was not perpendicular to the surface . that means it has a component along the surface . now , electric fields exert a force on charges , so now we have a force on the charges in the conductor along the surface of the conductor . this force is not balanced by anything else , so it will then move the charge around . but that means that our system was not yet in equilibrium , since charge was moving around . in equilibrium , the charges must be at rest , and that can only be the case when there is no electric force along the surface , i.e. , when it is perpendicular to it . note : you say that $\varphi = 0$ inside and on the surface of the conductor . that is not true . $\varphi$ is constant inside and on the surface of the conductor .
the detector that took that image--super kamiokande ( super-k for short ) --is a water cerenkov device . it detects neutrinos by imaging the cerenkov cone produced by the reaction products of the neutrinos . mostly elastic scattering off of electrons : $$ \nu + e \to \nu + e \ , , $$ but also quasi-elastic reactions like $$ \nu + n \to l + p \ , , $$ where the neutron comes from the oxygen and $l$ means a charged lepton corresponding to the flavor of the neutrino ( for energy reasons always an electron from solar neutrinos , but they also get muons from atmospheric and accelerator neutrinos---super-k is the far detector for t2k ) . then you reconstruct the direction in which the lepton was moving ( which is correlated with but not identical to the direction the neutrino was going ) . this indirect pointing method accounts for the very poor angular resolution of the image .
no , this is talking about correlations between s random particles . the s-particle distribution function is a 2*d*s ( so 6s in 3 dimensional space ) dimensional pdf that statistically describes s particles . for s=1 , this is just the normal density in phase space . for s=2 , this might show , for example , that more often than not two particles are traveling away from each other ( maybe they just collided ) . a good source for this is ch . 2 in " the statistical physics of particles " by mehran kardar .
the fraction of baryonic matter to dark matter is not deduced only from galactic dynamics . it is also derived from big bang nucleosynthesis and from the higher multipole acoustic peaks in the cmb spectrum . i would say that the element abundance is a far more important indicator of the fraction between baryonic and dark matter . big-bang nucleosynthesis theoretical overview of cosmic microwave background anisotropy : 1.2 . results
i am going to assume that omer is specifically asking why the centre of mass is at the focus ( well , one of the foci ) of the orbits . omer , if this is not what you meant please ignore what follows because it is completely irrelevant ! if you have a body moving in a central field ( i.e. . the force is always pointing towards the centre ) , and the field is inversely proportional to the square of the distance from body to the centre , then the orbit is an ellipse with the centre at one of the foci . for now let 's just assume this and we can come back to prove it later . so if we can show that both of the bodies feel a central inverse square force , with the com at the centre , this guarantees the orbits will be ellipses with a focus at the com . given that the force is due to the two bodies attracting each other , and that both bodies are orbiting around , it may seem a bit odd that each body just feels a central inverse square force , but actually this is easy to show . the picture shows the two masses and the com . i have not shown the velocities because it does not matter what they are . for now let 's just consider $m_1$ and calculate the force on it . by newton 's law this is simply : $$ f_1 = \frac{gm_1m_2}{ ( r_1 + r_2 ) ^2}$$ first is this force central ? we know the centre of mass does not move . for two bodies this seems obvious to me , but in any case dmckee proved it in his answer . if the com does not move it must lie on the line joining the two mases , otherwise there would be a net force on it . so the force $f_1$ must always point towards the com i.e. the force is central . second is this an inverse square law force i.e. is $f_1 \propto 1/r_1^2$ ? well the definition of the centre of mass is that : $$m_1r_1 = m_2r_2 $$ or $$ r_2 = r_1 \frac{m_1}{m_2} $$ if we substitute for $r_2$ in the expression for $f_1$ we get : $$ f_1 = \frac{gm_1m_2}{ ( r_1 + r_1 ( m_1/m_2 ) ) ^2}$$ or with a quick rearrangement : $$ f_1 = \frac{1}{ ( 1 + m_1/m_2 ) ^2} \frac{gm_1m_2}{r_1^2}$$ and this shows that $f_1$ is inversely proportional to $r_1^2$ . i will not work through it , but it should be fairly obvious that exactly the same reasoning applies to $f_2$ so : $$ f_2 = \frac{1}{ ( 1 + m_2/m_1 ) ^2} \frac{gm_1m_2}{r_2^2}$$ this is the key result . even though the two bodies are whizzing around each other , each body just behaves as if it were in a static gravity field , but the strength of the field is reduced by a factor of $ ( 1 + m_1/m_2 ) ^2$ for $m_1$ or $ ( 1 + m_2/m_1 ) ^2$ for $m_2$ . this applies to all two body systems , even such unequal ones as the sun and the earth ( ignoring perturbations from jupiter etc ) . i did start by assuming that a body in a central gravity field orbits in an ellipse with the foci at the centre , but i am going to wimp out of proving this since it would double the length of this answer and you had all go to sleep . the proof is easily googled . nb this only applies to two body systems . for three or more body systems the orbits are generally not ellipses with the centre of mass at the focus .
actually it is not that difficult ( but a neat problem ) , there is only one crucial step in the development that i will show you , but let 's start from a bit earlier . let 's first write out the two forces on interest here ( in terms of magnitudes , as we know already they are on parallel trajectories ) : the coulomb repulsion between the charges : $$f_c = \frac{q^2}{4\pi \epsilon_0 r^2} $$ now since we have moving charges , ( hence a current ) , each charge will experience a lorentz force in the magnetic field induced by the other charge , using biot-savart 's law , we have for the magnetic field $b$ : $$b = \frac{\mu_0 i dl}{4\pi r^2}$$ which should be interpreted as ( by analogy to a electrical circuit ) " the magnetic field felt at a distance $r$ , induced by a wire length $dl$ carrying current $i$" . note $\mu_0$ is the permeability of vacuum , $\epsilon_0$ is the corresponding permittivity . the lorentz force : $$f_l = qbv = \frac{\mu_0 q^2 v^2}{4\pi r^2}$$ where the $i dl$ term in b field is replaced using : $i=q/dt$ and $dl/dt = v$ , which gives finally $$i dl=qv . $$ the two forces together ( with $f_c$ the repulsive force here ) : $$\sum f =\frac{q^2}{4\pi \epsilon_0 r^2} -\frac{\mu_0 q^2 v^2}{4\pi r^2}$$ it is clear that the resulting effect of the two forces depends on the strength of the $b$ field which in turn then depends on the velocity of the two charges , if they are slow , the repulsion dominates . so in order to be able to compare their relative strengths , we need a slight rearrangement for the $\sum f$: $$f_{tot} =\frac{q^2}{4\pi \epsilon_0 r^2} \left ( 1-v^2 \epsilon_0 \mu_0\right ) $$ where $$v^2 \epsilon_0 \mu_0=\frac{f_l}{f_c}$$ but we know that $\epsilon_0 \mu_0 = c^{-2}$ the inverse squared of speed of light in vacuum , substituted in the total force expression : $$f_{tot}= f_c \left ( 1-\frac{v^2}{c^2}\right ) = \frac{f_c}{\gamma^2} $$ where $\gamma$ is the relativistic factor $ ( 1-v^2/c^2 ) ^{-1/2}$ or often called lorentz factor , which relates the measurements performed in different inertial frames moving at $v$ with respect to one another . last note : if the charges are not in vacuum , the relative permittivity $\epsilon$ and permeability $\mu$ are included in the expressions .
is it because the acceleration is too weak ? it is too weak with respect to the four forces we measure . the fact that the four known forces are so much stronger means that agglomerates of particles , up to the scale of galaxies are not internally affected , they keep their structure intact , like the famous raisins in the rising bread . it is only at the level of clusters of galaxies that the expansion and the acceleration can be observed . and if there is a small apparent force , what direction is it in ? a cluster of galaxies sees expansion in all three space dimensions . the balloon surface analogy , blowing up the balloon with a gnat on it , might give an insight . the gnat sees the surface expanding away from it in both surface directions .
the quantum fields in the interaction picture evolve according to the free field equations – the evolution is given just by the quadratic part of the hamiltonian , without the interactions – so green 's functions constructed from these interaction-picture field operators would be those of the free field theory , too . it would not be terribly interesting . we would only " learn " the wick 's theorem and things about the free field theory . the normal green 's ( $n$-point ) functions are supposed to include all the interactions given by feynman 's vertices etc . , so they need to be evaluated from the operators in the normal picture , i.e. the heisenberg picture . the interaction picture is just a " fudged " compromise between the heisenberg picture and the schrödinger picture – a compromise that is useful and convenient but in no way fundamental . it is the heisenberg picture and correlators in it that reduce to classical physics in the $\hbar\to 0$ classical limit .
your process will be reversible only if it is a ) quasi-static and b ) non-dissipative . it will be quasi-static if it is carried out infinitely slowly in such a manner that the pressure on either sides of the piston varies only infinitesimally . it will be non-dissipative if the piston is frictionless and there is no viscous heating of the gas as it expands .
the word comes from the ancient greek meaning " pertaining to man ; " ' man ' here means human . the etymologyonline dictionary is helpful . the anthropic principle is so-named because it is fundamentally based on the fact of a human observer .
kugo and ojima 's work was one of the major breakthroughs in understanding the role of brst in the quantization of gauge theories . historically brst was discovered in the path integral formalism . the understanding of this theory as a cohomology theory started from the kugo and ojima 's work . now , the action is brst invariant with and without the gaussian integration over the auxiliary field $b^a$ ( called the lautrup-nakanishi multipliers ) . they are introduced in order to not have explicit dependence on the gauge parameter in the ward identities ( please see a recent review by becchi ) . the brst invariance ward identities is a crucial step in the unitarity proof . kugo and ojima actually solved the brst cohomology problem of the yang-mills theory . they actually identified the physical and unphysical states of the theory ( in terms of the brst operator ) as follows : the physical states correspond to the states annihilated by the brst operator and in addition of positive norm . the unphysical states are arranged in degenerate quartets . this is called the kugo - ojima quartet mechanism . one quartet corresponds to a ghost , anti-ghost , longitudinal and temporal gluons . in their formalism these states can be generated from the vacuum by the action of the ghost antighost operators as well as by the field $b^a$ and its conjugate momentum . they also conjectured that since colored quark operators and transversal gluon operators belong to the quartet sector , then these states must be confined .
welcome to physics . se how are decays related to forces , what is meant by particle x decays through the , say , strong force ? it means that particle x was bound by the strong force to particle y to form particle z for a delta ( t ) and then it parts company . the way i understand forces is by how they change the acceleration of particles with the right charge ( mass , electric etc ) , through f=ma , how does it cause one particle to turn into other ? your understanding is about the classical domain of forces . elementary particles necessarily belong to the quantum domain , one uses relativistic four vectors to describe them and forces are mediated by other elementary particles , graphically shown with feynman diagrams , which are a short hand for the way one can calculate the probability of the decay . one can think of the mediating particle as a carrier of delta ( p ) , where p is the four momentum , which transfers momentum and energy to the decay products from the binding energy of the original particle . from a teacher site here is the feynman diagram of the decay of a lambda baryon to a proton and a pion , mediated by the weak interaction : lambda contains a strange quark which is not stable under weak interactions . for a time the three quarks are bound together by the strong force , with gluon exchanges ( not shown ) but then the s quark decays into an up quark which is bound into a proton , and a w- weak boson off mass shell goes into an anti up and a down quark making a pi- . how is it determined which force is responsible for which decays ? it has been determined experimentally and theoretically encoded into the standard model can a particle decay through the gravitational interaction ? we have not found elementary particles or resonances bound by the gravitational interaction so the answer is that experimentally it cannot . the reason is that the gravitational force is many orders of magnitude weaker than the other three forces that reign in the particle domain .
yes - there are places . most large accelerator complexes like brookhaven , slac , ornl , etc , have groups dedicated to development of software for accelerator controls and/or data analysis . you can find the relevant job descriptions at the different lab websites and see where you might be a good fit .
use the directions up the ramp and perpendicular to the ramp as the simplest coordinate system . find the component of the vertical force of gravity that acts down the ramp . find the component of the horizontal force f that acts up the ramp . find an expression for the net force up the ramp , and equate to the mass of the cart times the acceleration of the cart up the ramp . solve for f .
focus on the integral $$ i_{ij} ( k ) = \int k_i k_j\ \mathrm{d}\omega_k . $$ this is a rank 2 symmetric tensor which can only depend on $\vec{k}$ through its magnitude $k^2$ , since the direction has been integrated over . so the only possibility is that $i_{ij}$ is proportional to the unit tensor ( kronecker delta ) : $$ i_{ij} ( k ) = f ( k^2 ) \delta_{ij} , $$ where $f ( k^2 ) $ is some function to be determined . we can find $f ( k^2 ) $ by taking the trace ( using the summation convention for repeated indices ) : $$\begin{array}{ll} \delta_{ij} i_{ij} ( k ) and = \delta_{ij} f ( k^2 ) \delta_{ij} \\ and = 3 f ( k^2 ) \\ and = \delta_{ij} \int k_i k_j\ \mathrm{d}\omega_k \\ and = \int k^2\ \mathrm{d}\omega_k \\ and = k^2 4\pi . \end{array}$$ thus $$ i_{ij} ( k ) = \frac{4\pi}{3} k^2 \delta_{ij} . $$
the expectation value of the energy stays the same after the doubling of size but it does not mean that the spectrum is the same . for a normalized $\psi$ , the expectation value of the energy is simply $$ \int_{-l}^{+l}dx\ , \psi^* \left ( -\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2} + v ( x ) \right ) \psi $$ because the integral may be reduced to the interval as the wave function vanishes outside the interval . now , immediately when you double the size of the well , the value of $\psi ( x ) $ remains the same so it still vanishes outside the interval $ ( -l , l ) $ and makes the integrand vanish as well ( even though the second derivative could refuse to vanish ) . that is why the integral above may still be rewritten as $$ \int_{-2l}^{+2l}dx\ , \psi^* \left ( -\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2} + v ( x ) \right ) \psi $$ without any change . it is the expectation value of the new hamiltonian . note that $v ( x ) =0$ wherever $\psi ( x ) \neq 0$ so the potential term may be omitted . you are right that there is some probability that in the larger well , the particle sits at a lower-than-the-initial-energy-eigenvalue value of energy . however , there is some probability that the energy is raised as well – the wave packet is unnecessarily squeezed in a small part of the well which adds more kinetic energy than the minimum possible one . these positive and negative changes cancel in the expectation value of the energy : the calculation above showed that it stayed constant . the expectation value of the energy stays constant when the particle evolves according to the larger-well hamiltonian , too . the probabilities for each energy eigenvalue are constant for all $t&lt ; 0$ and then for $t&gt ; 0$ but there is a discontinuity at $t=0$ . however , as the simple calculation above shows , in the expectation value of the energy itself , the change of the spectrum etc . at $t=0$ cancels when it comes to the expectation value of the energy .
assuming non-relativistic velocities , the power radiated by a charge accelerating at constant acceleration $a$ is given by the larmor formula : $$p = \frac{e^2 a^2}{6\pi \epsilon_0 c^3} $$ to do the calculation properly is surprisingly complicated , but it is easy show that the effect of the radiation on the electrons fall is negligible . if the electron falls a distance $h$ then the time it takes is given by : $$ h = \frac{1}{2}gt^2 $$ so : $$ t = \sqrt{\frac{2h}{g}} $$ if we assume the electron is accelerating at a constant rate of $g$ , the total energy radiated is just power times time or : $$ e_{rad} = \frac{e^2 g^2}{6\pi \epsilon_0 c^3} \sqrt{\frac{2h}{g}} $$ in your question $h$ is 1000m , so : $$ e_{rad} = 7.83 \times 10^{-51}j $$ the potential energy change is , as you say , just $mgh$: $$ e_{pot} = m_e g h = 8.94 \times 10^{-27} j $$ so the ratio of the radiated energy to the potential energy is about $10^{-24}$ , and therefore the effect of the radiation on the electron 's fall is entirely negligible . response to comment : the power radiated from the electron produces a force that opposes the acceleration due to gravity . assume we can ignore the deviations from accelerating at a constant rate $g$ , then in a small time $dt$ the energy radiated is $pdt$ . the energy is force times distance ( $dx$ ) so to get the force we divide by the distance : $$ f = p\frac{dt}{dx} = \frac{p}{v} = \frac{p}{\sqrt{2gh}} $$ using $v^2 = 2as$ . the acceleration produced by this force is just $f/m_e$ , so the net acceleration on the electron is : $$ a_{net} = g - \frac{p}{m_e \sqrt{2gh}} $$ so the electron does accelerate slightly more slowly than $g$ , but the difference between the acceleration and $g$ is inversely proportional to distance fallen so it gets increasingly negligible the further the electron falls . you have probably spotted that the above equation says the force should be infinite at the moment you release the particle . that is because as you approach the moment of release it is no longer safe to make the approximation that you can ignore the change in the acceleration due to radiation .
when a particle is deflected by gravity the gravitational field will also be modified by the particle . to form a conservation law for momentum you need to take into account the momentum in the gravitational field as well as the particle . this can be done e.g. using pseudo-tensor methods . this works but remember that momentum is a relative concept . even in newtonian dynamics it depends on the velocity of your reference frame . in general relativity it also depends on the frame but a much wider class of frames is valid . this means that momentum conservation depends on the choice of co-ordinates . locally you can pick an inertial reference frame but over extended regions there is no inertial frame . a momentum in one location cannot be simply added to a momentum vector in another location . nevertheless , momentum conservation laws over extended regions do work correctly in general relativity . for an extended description of the formalism and why it works see my article at http://vixra.org/abs/1305.0034 edit : in the comments below mwt p457 has been cited to support the idea that energy and momentum are only conserved in specific cases . i am adding this to directly refute what has been said there . mwt begin by saying that there is no such thing as energy or momentum for a closed universe because " to weigh something one needs a platform on which to stand to do the weighing " this is pure wheeler rhetoric of the type for which he is greatly admired , but in this case it is simply misleading . weight is a newtonian term with no useful counterpart in general relativity except in the specific case of an isolated system in an asymptotically flat spacetime . for other situations such as the closed cosmology energy and momentum conservation take a different but equally valid form . they go on to say that in a closed universe total energy or momentum or charge is " trivially zero " they justify that it is zero because you can use gauss 's divergence theorem to write the charge , energy or momenta as a boundary integral . for a closed universe the boundary disappears making the result zero . this is of course correct , but they give no justification for calling this answer trivial . energy and momenta are initially defined as a sum of volume integral contributions from each physical field including electromagnetic fields , fermionic fields , gravitational field etc . it is in this sense that we understand that conserved quantities can move and can transform from one form to another but the total remains constant . it is a property of gauge fields that when the dynamical field equations are used the conserved quantity is the divergence of a flux from just the gauge field so that it can be integrated over a volume and be calculated as a boundary surface integral . this gives charge/energy/momenta a holographic nature where they can be considered either as a volume integral over contributions from different fields of a surface integral over the gauge field flux . the important thing to understand is that to go from the volume to the surface form the field equations must be used . this means that the total charge/energy/momenta in a closed universe is zero but that this is not in any sense a trivial result . if you calculate total energy as a volume integral for a configuration of fields that do not satisfy the equations of motion the answer will not necessarily be zero . stating that it is zero is therefore making a non-trivial assertion about the dynamics . this is what conservation laws are all about . mwt go on to explain why it would make no sense to have an energy-momentum 4-vector globally . the invalid assumption they are making is that energy and momenta need to form a 4-vector . a 4-vector is a representation of the poincare group and is the natural form that energy-momentum takes in special relativity where poincare invariance is the global spacetime symmetry . in general relativity the global spacetime symmetry is diffeomorphism invariance so the correct expectation is that all quantities should take the form of a representation of the diffeomorphism group for the manifold . this is what happens . if you demand an energy-momentum 4-vector then of course you will only get an answer locally , and also for an asymptotically flat spacetime where poincare symmetry is valid at spatial infinity , but demanding such a 4-vector is simply the wrong thing to do in general relativity . in the fully general case of any spacetime we can apply noether 's theorem using invariance under diffeomorphism generated by any contravariant transport vector field ( observe that it is invariance of the equations that is required , not invariance of the solutions . some people like to confuse the two ) the result is a conserved current with a linear dependence on the transport vector field . this is the correct form for a representation of the diffeomorphism group . i refer to my cited paper for the mathematical details . this current gives conservation laws for energy , and momenta including generalizations of angular momenta as well as linear momenta depending on the transport vector field chosen . if it transports space-like hypersurfaces in a timelike direction it will give an energy conservation law and if it transports in spacelike directions it gives momenta conservation laws . these energy and momenta do not normally form 4-vectors but they can be integrated to give non-trivially conserved quantities . the global form a conservation law must take is that the total energy and momenta in a volume must change at a rate which is the negative of the flux of the quantity over the boundary , and this is what you get with the currents derived from noether 's theorem . it may be that other people will want to add comments here that dispute the validity of energy conservation in other ways . i refer once again to my new article at http://vixra.org/abs/1305.0034 where i refute all the objections that i have heard . triviality is dealt with in item ( 6 ) and 4-momentum is dealt with in item ( 8 ) . unless someone comes up with a novel objection i will just refer to the numbered objections in this paper in future . remember , there are no authorities in science and any expert may be shown to be wrong either by reasoning or by experiment .
seems good to me . you are right integrating only from 0 to $a$ because $\psi$ is zero in the region of infinite potential . the solution would be $\psi ( p ) =\frac{1}{\sqrt{a\pi h}}\int_o^ae^{-ipx/\hbar}\sin ( \pi x/a ) dx=\frac{\sqrt{a\pi \hbar^3 }}{\pi^2 \hbar^2 - p^2 a^2 } ( e^{-ipa/ \hbar }+1 ) $ as for the other question , that is what one typically does
i am not sure why you are using the displacement-time formula if you have the shape of the graph . the distance covered by the particle is given by the area under the graph from point 0 . after t = 2s , the distance the particle would have covered is the area under the trapezium , that is : $$\dfrac12 ( 1+2 ) ( 20 ) = 30$$ the position of the particle becomes $8 m + 30m = 38m$ . maybe you can work out the second part now . explanation : your formula assumes constant acceleration , which is not the case ! for the first 1 s , the acceleration is $0ms^{-2}$ , the next 1s , the acceleration is $-20 ms^{-2}$ ! if you used your formula for $0&lt ; t&lt ; 1$ and then $1&lt ; t&lt ; 2$ , then you would have obtained the desired answer . but it is just making the problem more complicated than it actually is at that point : )
here is a model that pictures the electrostatics of the creation of a lightning bolt . the usual cloud-to- ground discharge probably begins as a local discharge between the small pocket of positive charge at the base of the cloud ( the p region ) and the primary region of negative charge ( the n region ) above it . this local discharge frees electrons in the n-region that previously had been attached to water or ice particles . these electrons overrun the p-region , neutralize its small positive charge , and then continue on their trip to the ground . the horizontal extent is kilometers thus the field is built up from the ground , and there is no possibility of running out of the area . it is a matter of probabilities of the square meter or so where most of the energy will be dissipated and that depends on the local field distributions . a conductor like a lightning rod has a sharp field around it and provides the easiest path , if the strike were to happen within some meters of its location : strikes have a limited area , about 20 feet variability . a horse or a human may find themselves in the strikes path , there will be little difference except on the conductivity of the body , i have heard of lightning just burning the clothes . if the horse is with four feet in the air , again it is a matter of the random path of the strike and the conductivity of the horse . all in all it is the large geography of the spot that will determine the region where the bolt will strike and the local conductivities for the details . running would help if one entered a house or other solid shelter . falling on the ground or sheltering at a ridge is good advice to lessen the probability of being struck , because of the electric fields built up around high points : do not become the lightning rod by being the highest point in the region .
in order to talk about diffusion you need at least two particle species ( something has to diffuse relative to something else ) . then the continuity equation refers to the total mass density of the fluid $$ \rho = \int d\gamma\ , m_1f_1 + \int d\gamma\ , m_2 f_2 $$ and the fluid velocity is defined by $$ \rho\vec{u} = \int d\gamma\ , m_1v_1f_1 + \int d\gamma\ , m_2v_2 f_2\ , . $$ mass conservation in the collision term ensures that $\partial_t\rho+\vec{\nabla} ( \rho \vec{u} ) =0$ and no dissipative fluxes appear . now we can define a concentration , e.g. $$ c = \int d\gamma\ , f_1 / \int d\gamma\ , f_2\ , . $$ the quantity $\rho c$ satisfies a more complicated continuity equation , with a diffusive current $\vec{\jmath}$ on the right hand side . this current satisfies ( to leading order in gradients ) fick 's law , $$ \vec{\jmath} = -\rho d\vec{\nabla} c + \ldots $$ where the $\ldots$ are thermal diffusion terms , and higher order gradients .
it is a direct consequence of equilibrium . it can be proved mathematically that proving the moment is zero at one point of a sytem in equilibrium ensures it is zero everywhere . have a look at the first two pages of this , you can find its proof
never underestimate the power of googling . i googled " high speed nuclear explosion photos " and got a large number of them . here is one : this image captures two common elements : the spikes ( called " rope tricks" ) and an uneven surface shape the duration of the exposure is typically 10 nanoseconds at this stage of the detonation the surface of the fireball has a temperature of 20,000 degrees , three times hotter than the sun 's surface . at such temperatures the amount of thermal radiation ( light ) given off is so enormous anything it touches is vaporized ahead of the expanding fireball . the three spikes in this image result from the guide wires supporting the tower on which the bomb was located absorbing enough heat to turn into light emitting plasma . because thermal radiation travels faster than the fireball , the spikes extend out ahead of it . p.s. i should have remembered one frame cameras , since i had been using " gun cameras " from wwii to get images of cosmic rays in spark chambers by triggering , back in 1967 .
your pump needs to provide enough pressure to push the water all the way up to the deposit . that would amount to $\rho g h$ . if your inlet in the deposit is above water level , $h$ is measured to the inlet . if it is underwater , then it is measured to the water level . that pressure is enough to keep the water from backflowing , but not to push any more water up to the deposit . to do so , you will need additional pressure , that will go into kinetic energy of the water moving through the pipe , $\frac{1}{2}\rho v^2$ , and into overcoming friction losses in the pipe , which can be calculated with the darcy-weisbach equation . the velocity of the water in the pipe can be figured out from the pipe diameter and the mass flow being sent through it .
when you give the dose of an exposure in sv , you have already taken into account weighting factors related to the organ type being exposed ( some are more sensitive than others ) as well as the radiation type . in other words , the number reported represents " equivalent cancer risk if your entire body had been exposed with . . . " see for example http://en.m.wikipedia.org/wiki/sievert#calculating_protection_dose_quantities so yes - a trans continental plane flight ( new york to la ) on average increases your lifetime cancer risk by more than a chest x-ray . both are absolutely tiny . it is typically considered that 1 sievert carries with it a lifetime risk of cancer of 5% ( this depends of course on the age of exposure . . . all these things are " population risk " not " individual risk " . that means that one flight per week = 2 msv / year for fifty years ( 2500 flights ) will cause one additional cancer in one person in 2000 . and that is assuming linear scaling of radiation risk - there is some evidence that the body has some repair mechanisms that may make the curve nonlinear . so do not get too hung up on the detailed calculations . in radiation protection one uses the principle of alara - as low as reasonably achievable . the person who gets the highest radiation dose in the hospital due to a diagnostic imaging exam is probably the interventional radiologist - it is definitely not the patient getting the chest xray .
there are several ways of knowing what states should be there . for simple cases such as this , the easiest way is just by counting all the possibilities or micro-states . since you have 2 " equivalent " electrons in $p^2$ ( equivalent meaning that they share quantum numbers $n$ and $l$ , related to the energy of the system ) there are $$\left ( \begin{array}{l} 6 \\ 2 \end{array} \right ) = 6 \cdot 5/2 = 15$$ micro-states ( possible ways of assigning $n$ , $l$ , $m_l$ and $m_s$ to the outer ( valence ) electrons . know you have to count all the possible ( allowed by pauli 's principle ) different arrangements of $m_{l , 1}$ , $m_{l , 2}$ , $m_{s , 1}$ , $m_{s , 2}$ . you can find them explicitly in any physical chemistry book ( e . g . mcquarrie and simon ) . writing $m_{l , 1} , m_{l , 2}$ and $m_s$ omitted ( $m_s = 1/2$ ) or with an over bar ( for spin $m_s = -1/2$ ) , the microstates are : $ ( 1 , \bar{1} ) , ( 1,0 ) , ( 1 , \bar{0} ) , ( 1 , -1 ) , ( 1 , -\bar{1} ) $ , $ ( 0,1 ) , ( 0 , \bar{1} ) , ( 0 , \bar{0} ) , ( 0 , -1 ) , ( 0 , -\bar{1} ) $ , $ ( -1,1 ) , ( -1 , \bar{1} ) , ( -1,0 ) , ( -1 , \bar{0} ) , ( -1 , -\bar{1} ) $ now you have to group these states by characterising $l$ and $s$ ( since , barring spin-orbit coupling , for a given $l$ and $s$ the $m_l$ and $m_s$ states form a manifold of degenerate states ) . there are several ways of doing so , but i will be sketchy to avoid lengthiness . for instance you can first think in those cases with $m_{s , 1} \neq m_{s , 2}$ where $m_{l , 1}$ can be equal to $m_{l , 2}$ . then $m_s = m_{s , 1} + m_{s , 2} = 0$ ( $s$ can still be 1 -triplet or 0 - singlet ) . $m_l = m_{l , 1} + m_{l , 2} = 2,1,0$ . so you can have states with $l = 2,1,0$ . the states with $l = 2$ must be $s = 0$ since , as you realised , $m_{s , 1}$ cannot be equal to $m_{s , 2}$ . thus you identify 5 micro-states ( $m_l = +l , \ldots , 0 , \ldots -l$ ) corresponding to a single level $^1$d . of the remaining 10 states you can clearly see from the listing of micro-states that your $l=1$ level is a triplet ( you can find microstates with $m_s = 1$ and $m_l = 1$ so the remaining $m_s = 0 , -1$ and $m_s = 0 , -1$ have to be there too ) . for a $^3$p level there are $3\times 3 = 9$ microstates . finally , the remaining micro-state must correspond to a $^1$s state . regarding your question of the wave function . again , think only on your last 2 electrons ( it is not difficult to " enlarge " your slater determinant with the other electrons ) . each of your previous micro-states would correspond to a single slater determinant . for example , for $ ( 1 , \bar{0} ) $ you would have the wave function $$ \frac{1}{\sqrt{2}} \left| \begin{array}{cc} 2p_1 ( 1 ) \alpha ( 1 ) and 2p_0 ( 1 ) \beta ( 1 ) \\ 2p_1 ( 2 ) \alpha ( 2 ) and 2p_0 ( 2 ) \beta ( 2 ) \end{array} \right|$$ where $2p_{m}$ is the wave function in atomic orbital $2p_{m}$ and $\alpha$ and $\beta$ are the spin functions . some states belonging to the levels with well-defined $l$ and $s$ correspond directly to the micro-states . for instance $ ( 1 , \bar{1} ) $ is exactly $|l=2 , s=0 , m_l=2 , m_s=0\rangle$ . however , most states must be written as linear combinations of slater determinants . for instance , to the state $|l=2 , m_l=1 , s=0 , m_s=0\rangle$ corresponds the wave function $$\frac{1}{2} \left| \begin{array}{cc} 2p_1 ( 1 ) \alpha ( 1 ) and 2p_0 ( 1 ) \beta ( 1 ) \\ 2p_1 ( 2 ) \alpha ( 2 ) and 2p_0 ( 2 ) \beta ( 2 ) \end{array} \right| + \frac{1}{2} \left| \begin{array}{cc} 2p_0 ( 1 ) \alpha ( 1 ) and 2p_1 ( 1 ) \beta ( 1 ) \\ 2p_0 ( 2 ) \alpha ( 2 ) and 2p_1 ( 2 ) \beta ( 2 ) \end{array} \right|$$ whereas for the state $|l=2 , m_l=0 , s=0 , m_s=0\rangle$ the wave function would be $$\frac{1}{2\sqrt{3}} \left| \begin{array}{cc} 2p_1 ( 1 ) \alpha ( 1 ) and 2p_{-1} ( 1 ) \beta ( 1 ) \\ 2p_1 ( 2 ) \alpha ( 2 ) and 2p_{-1} ( 2 ) \beta ( 2 ) \end{array} \right| + \frac{1}{\sqrt{3}} \left| \begin{array}{cc} 2p_0 ( 1 ) \alpha ( 1 ) and 2p_0 ( 1 ) \beta ( 1 ) \\ 2p_0 ( 2 ) \alpha ( 2 ) and 2p_0 ( 2 ) \beta ( 2 ) \end{array} \right| + \frac{1}{2\sqrt{3}} \left| \begin{array}{cc} 2p_{-1} ( 1 ) \alpha ( 1 ) and 2p_{1} ( 1 ) \beta ( 1 ) \\ 2p_{-1} ( 2 ) \alpha ( 2 ) and 2p_{1} ( 2 ) \beta ( 2 ) \end{array} \right|$$
using this from my book ( physics for scientists and engineers with modern physics 9th ed ) : if more than one force acts on a system and the system can be modeled as a particle , the total work done on the system is just the work done by the net force . if we express the net force in the x direction as o fx , the total work , or net work , done as the particle moves from xi to xf is σw = the integral of σf from xi to xf a proof that d ) the speed of the particle must be unchanged is true : if σw = the integral of σf from xi to xf = 0 , then δx = 0 or σf = 0 ( this alone is proof that e ) there must be no displacement is false ) . in the case that δx = 0 , the particle is not displaced and the speed is unchanged . if σf = ma0+ma1+ . . . + man = m ( σa ) = 0 and m ≠ 0 , then σa = 0 and the speed is again unchanged . therefore , if σw = 0 , then it is necessarily true that the speed remains unchanged .
significant digits is a convention that only affects how you write numbers , not what the numbers actually are . so you only round when you are asked to drop down to a given number of significant digits - that is , at the end . think of it like this : there is a difference between a number , which is an abstract idea , and a written representation of a number . some numbers have exact written representations ; all numbers have approximate written representations , which represent another , nearby number . for example , the notation $5.82876$ is an exact representation of a particular number , and $5.8$ is an approximate written representation , to two significant figures , of the same number . $5.8$ is also an approximate written representation ( to two significant digits ) of many other numbers , such as $5.810394$ and $5.79928129$ . this is the idea behind uncertainty , and significant digits : if you are given the written representation $5.8$ , you do not know which actual number it represents - it could be anything between $5.75$ and $5.85$ . the only exception is if you are told that $5.8$ is an exact representation , which uniquely specifies which number you are supposed to take it to mean . when you calculate the product $1.8\times 2.01\times 1.542$ , you start with three written representations which you are supposed to assume are exact . then you multiply the first two of them , and get a number which is exactly represented by the notation $3.78$ . now , it is true that $3.8$ is an approximate written representation of that number . but does that fact change what the number is ? no . if you do the intermediate rounding , you are effectively deciding to replace one number , the one which is exactly represented by $3.78$ , which another number , the one which is exactly represented by $3.8$ . and the operation " replace one number with another number " is not part of the mathematical expression you are supposed to simplify . so do not do it .
assuming your pipette has a volume of 15ml , the error in measuring the volume of your sample is 0.02 in 15 i.e. 0.133% . the weight of 15cc of ethanol is 0.789 $\times$ 15 = 11.835g . the error in measuring the weight is 0.002 in 11.835 i.e. 0.025% . hedge physicists like me would immediately note that the error in the weight is a lot lower than the volume , so we can ignore it and just take the error in the density to be 0.13% . however , for the sake of the exercise let 's do this thoroughly . the density is given by : $$ \rho = \frac{m}{v} $$ so the percentage error in the density , $\sigma_\rho$ is given by : $$ \sigma_\rho = \sqrt{\sigma_m^2 + \sigma_v^2} $$ where $\sigma_m$ and $\sigma_v$ are the percentage errors in the weight and volume respectively . this gives : $$\sigma_\rho = 0.136\%$$ so the absolute error in the density of 0.789 would be 0.136% of 0.789 or 0.0011 . there is one last step to do : to distinguish the two fluids we need to measure their densities ( with an error of 0.0011 ) then subtract the measurements . because we are subtracting two measured densities , each with an error of 0.0011 , the error in the result is given by an expression similar to the one above : $$ \sigma_{diff} = \sqrt{0.0011^2 + 0.0011^2} = 0.0016 $$ note that this time we are using absolute errors not percentage errors . when you are multiplying or dividing you combine the percentage errors and when you are adding and subtracting you combine the absoluite errors . anyhow , the difference in the densities is 0.789 - 0.785 = 0.004 , so our result would be 0.004 plus or minus 0.0011 . assuming the errors quoted are the 1$\sigma$ errors , the error in the result is 4$\sigma$ , so we had be 98% confident we could tell the difference .
i did not find the equation and the argument you quoted in that paper . but , yes , it is the brouwer degree , deg$ ( \hat{\phi} ) $ , which equals the monopole number $$ n\equiv\frac{1}{4\pi}\int_{\mathbb{r}^3} \mathrm{tr} ( f_a\wedge d_a ( \phi ) ) =\frac{1}{4\pi}\int_{\mathbb{r}^3} d ( \mathrm{tr} ( \phi ) f_a ) =\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr} ( \phi f_a ) $$ $$ =\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr} ( \hat{\phi} f_a ) $$ where the one has used bianchi identity , stokes ' theorem , to obtain the first two equalities , and jaffe and taubes show in their book that one can replace $\phi$ by $\hat{\phi}$ . now this coincides with the brower degree , for which there is an explicit formula : $$n=\mathrm{deg} ( \hat{\phi} ) =-\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr ( \hat{\phi} d\hat{\phi}\wedge d\hat{\phi}} ) \in \mathbb{z}=\pi_2 ( s^2 ) = [ s^2 , s^2 ] $$ ( what you wrote . ) this is physically understood as an infinite wall potential , separating the monopole sectors corresponding to different integers . now , to actually answer your question , you can compute this integral for the t'hooft-polyakov monopole solution , for which $$ \hat{\phi}= ( \sin ( \theta ) \cos\phi , \sin\theta \sin\phi , \cos\theta ) _i\cdot \sigma^i , $$ and you will find $$n=-\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr ( \hat{\phi} d\hat{\phi}\wedge d\hat{\phi}} ) =+\frac{1}{4\pi}\int_{ [ 0,2\pi ] } \int_{ [ 0 , \pi ] }\sin\theta d\theta\wedge d\phi=1 . $$
there is much more matter in the interstellar medium than in the visible stars . our best estimate of the total matter/energy content of the universe is shown in this image from nasa : now the dark energy is not really matter - it acts like an anti-gravitational vacuum energy which is responsible for the accelerating expansion of the universe . the dark matter is probably some kind of matter that does not have any strong nuclear or electromagnetic interactions with our ordinary ( baryonic ) matter ( atoms ) . it may possibly only have gravitational interactions , but all the current searches for dark matter assume that it also has a weak nuclear interaction with ordinary matter . so that leaves $4.6\%$ of the universe that is made from atoms . it is believed that only about $\frac{1}{2}\%$ of the mass of the universe is in the form of the atoms that are inside stars ( and planets ) . so for every atom in a star , there are 9 atoms in the interstellar medium . some of the atoms in the interstellar medium would be gas clouds inside of galaxies and some of it would be the diffuse interstellar gas between galaxies and even clusters of galaxies . the amount of mass/energy in black holes is a more difficult question ( and about which i know much less ) . some of the atoms have already collapsed into black holes such as the super-massive black holes in the center of most galaxies and the black holes that may result when stars go supernova . the super-massive black holes in the centers of galaxies are still only a very small fraction of the total mass of the galaxy so all of these " known " black holes are probably only a small fraction of the $\frac{1}{2}\%$ of the atoms that are in the form of stars . however , there could be some primordial black holes left over from the big bang . these primordial black holes could constitute part of the dark matter of the universe . most standard cosmological theories do not predict any ( or many ) primordial black holes and there have been some searches for them with gravitational lensing . in this area , i know much less , but i believe these primordial black holes cannot be a big fraction of the dark matter , but i do not know what the upper limit is precisely .
this should be a comment as i am not knowledgeable about the lhc refrigeration , but it is too long for a comment and i can hazard a pretty good guess . sheer heat capacity likely accounts for a great deal of this time . assuming the total amount of kit that needs to be cooled is , say $2\times10^4$ tonnes , and if it has roughly a $1{\rm kj k^{-1} kg^{-1}}$ heat capacity , that means we have to extract $20{\rm gj}$ for every degree k that we cool . once we get down below $100{\rm k}$ were going to see some serious multipliers happenning . an ideal heat pump needs work input $w = q_{lhc} \left ( \frac{t_{out}}{t_{lhc}} - 1\right ) $ to pump out heat $q_{lhc}$ from the kit at temperature $t_{lhc}$ and dump it to the environment at $t_{out}$: this is the reversible heat pump . so if we are drawing this heat out and dumping it at $300k$ , say , the energy needed to get from $300k$ to $5k$ we get as a rough estimate ( assuming heat capacities stay constant , which they will not , but there will not be any phase changes of most of the kit ) : $$w_{total} = \sigma \int\limits_{t_{lhc}}^{t_{out}} \left ( \frac{t_{out}}{t} - 1\right ) \ , {\rm d}t$$ where $\sigma$ is the $20{\rm gj k^{-1}}$ total heat capacity i estimated above . plugging in the numbers $t_{lhc} = 5k$ ( not everything will need to be cooled all the way down to $1.9k$ ) and $t_{out} = 300k$ we get : $$w_{total} = \sigma \left ( t_{out}\left ( \log\left ( \frac{t_{out}}{t_{lhc}}\right ) -1\right ) +t_{lhc}\right ) = 933 \sigma \approx 20{\rm tj}$$ this is the total output of a $5{\rm gw}$ power station for over an hour , roughly the energy released by the first of the only two nuclear weapons brought to bear in combat . $5gw$ electricity generation is the electricity consumption of two million australians , and we are extremely greedy electricity users by world standards , so i do not know how many normal people this would represent . the lhc site quotes a peak power consumption of $180mw$ and about $30mw$ is used for cryogenics . $30{\rm tj}$ at $30mw$ is about ten days . another factor is stresses in the kit induced by too swift cooling or warming . i am slightly familiar with the design of some of the magnetic beamsteering hardware , and much of this kit is toleranced to within tens of microns . one can not brook even the tiniest of any irreversible , plastic deformations of the kit and still have it work properly . so it is likely that heat transfer could not go much faster than this even if the refrigeration capacity were there . there will also be economic considerations too . even if one can cool faster than with $30mw$ refrigeration and still meet technical / engineering constraints , refrigeration capacity is expensive , particularly if you are only using this full capacity for cooldown during maintenance . the rest of the time the refrigeration needs are much less , so you have an economic tradeoff between capital spent on capacity that is unused most of the time and the cost of project delays arising from downtime . i am absolutely sure exactly this calculation has been done , as it ones like it are done for all soundly managed engineering projects .
the following fact lies at the heart of this and many similar issues with sizes of things : not all physical quantities scale with the same power of linear size . some quantities , like mass , go as the cube of your scaling - double every dimension of an animal , and it will weigh eight times as much . other quantities only go as the square of the scaling . examples of this latter category include muscle strength ( a longer muscle can exert no more force than a shorter one of equal cross sectional area ) , heart pumping ability ( the heart is not solid but rather hollow , so the amount of muscle powering it goes as the surface area ) , the compression/tension that can be safely transmitted by a bone ( material strength is intrinsic and independent of size , so the pressure that can be supported is constant , so the force - cross sectional area times pressure - that can be supported goes as the square of size ) , and the ability to exchange material and heat the the environment ( single cells for example have a hard time growing large because their metabolism goes as the cube of the size , but their ability to transport nutrients across their outer membranes only scales as the area of those membranes ) , at least to a first approximation . you could also come up with other quantities that scale differently with size . as a result , simply scaling up an organism will undo the balance that has been achieved for that particular size . its muscles will likely be too weak , its bones will likely break , and it will generate so much internal heat ( if it is warm blooded ) that the only equilibrium achievable given its comparatively small surface area would be at a high enough temperature to denature many proteins . for a completely non-biological example , consider the fact that airplanes cannot be made arbitrarily large , and in fact different sizes of planes have very different shapes and engineering requirements . the surface area of the wings does not scale the same way as the total mass , and the stresses and pressures the material needs to withstand will not stay constant as you enlarge the plane .
there is no quantum mechanics of a photon , only a quantum field theory of electromagnetic radiation . the reason is that photons are never non-relativistic and they can be freely emitted and absorbed , hence no photon number conservation . still , there exists a direction of research where people try to reinterpret certain quantities of electromagnetic field in terms of the photon wave function , see for example this paper .
if you choose a local inertial frame at a specific point of space-time , the metric tensor , around this point , is : $g_{ij}= \delta_{ij} - \frac{1}{3} r_{ikjl} x^k x^l + o ( x^3 ) \tag{1}$ and the space-time volume element ( corresponding to the square root of the determinant of the metric ) is : $ d\mu_g = ( 1 - \frac{1}{6} r_{jk} x^j x^k + o ( x^3 ) ) ~d\mu_{euclidean}\tag{2}$ the fact that the contraction of the weyl tensor is zero , that is $c_{jk}=0$ , looking at equation $ ( 2 ) $ , means that the $c_{jk}$ part of $r_{jk}$ is zero , so the weyl tensor does not contribute to modifications of ( infinitesimal ) space-time volume . however , equation $ ( 1 ) $ indicates you , that the weyl tensor is participating to the modification of the metrics , because the $c_{ikjl}x^kx^l$ part of $r_{ikjl} x^k x^l$ is not zero . so , finally , the weyl tensor participates to a modification of the metrics , but without participating to the modification of a ( infinitesimal ) space-time volume , so the weyl tensor is associated to modifications of the shape of ( infinitesimal ) space-time volume , but without modification of volume . typically , this involves tidal forces , gravitational waves . for instance , for a ( basic ) gravitational wave ( here we suppose that the ricci tensor is zero , and the riemann tensor equals to the weyl tensor ) propagating along the $z$ axis , considering an infinitesimal space-time volume , the physical $\delta x$ could increase , and the physical $\delta y$ could decrease , so the shape is changing , however one variation is compensating the other , so that the infinitesimal space-time does not change . .
the idea that nature is described by a nonlinear system of equations was the idea that einstein had in the 1920s , and motivated his search for a unified field theory . it does not work , and it is philosophically less worthwhile than current theories anyway , so even if it did work , it would not be simpler than string theory , or as elegant . the idea that you can describe what is going on with local equations is false , as is demonstrated conclusively by bell 's inequality violations . the bell inequality tells you that you can send electrons to far-away locations with spins that can be measured in 3 directions , a , b , c . the spin of the two electrons in each direction are 100% correlated ( it is actually anti-correlated , but same difference for the argument ) , so if you measure the spin in direction a , and one electron is up the other is 100% certain to be up . same for direction b and c , the two electrons always report the same spin in any of the three directions . the spin in directions a and b are 99% correlated , meaning if you measure a on one of the electrons is up , then b on the other electron is up 99% of the time , and b is down 1% of the time . the spin in directions b and c are 99% correlated , so if you measure b is down on one electron , c is up on the other electron 1% of the time . from the 100% correlation of the electrons , you conclude that the nonlocal field state ( hidden variable ) on one electron has the property that a and b are 99% the same , 1% different b and c are 99% the same , 1% different from this you deduce that a and c must be at least 98% the same meaning that whatever field configuration is happening to make a , the field configuration for c can only give different results 2% of the time , the sum of those times when it gives different answers than b plus the times b gives different answers than a . this bound is called bell 's inequality , and it is violated by quantum mechanics . a and c are different 96% of the time . this means any type of local-in-space description , linear , nonlinear , complicated , simple , whatever , will never ever work to describe nature . your description is either nonlocal in the sense of faster-than-light communication , or nonlocal in the sense of having a global notion of state which is entangled nonlocally by measurements . this is why nobody looks for nonlinear field equations to describe nature anymore . it can not possibly work . but the main ideas of einstein 's nonlinear field theories have survived to inspire developments in later physics . the pions are excitations of a sigma-model , which is a type of nonlinear field theory . they are small oscillations of the quark condensate in the vacuum . the proton can be thought of as the topological soliton of the sigma model . in quantum mechanics , it can still be a fermion even though the sigma model has no fundamental fermionic variables . the field equations of 11-dimensional supergravity , which are a central part of string theory , generalize general relativity in pretty much the only nontrivial ways known--- they give the biggest extension of spacetime symmetry possible , and they include a new field , constrained by the supersymmetry . so these ideas are not a dead end , but they cannot work without quantum mechanics by themselves . if you want to understand quantum behavior emerging from some sort of nonlinear dynamics underneath , this dynamics can not be local .
the light from most sources spreads out in all directions so there is a straight line from any point on the surface to the source and a photon heading out from the source in that direction would hit that point . if you have a source which only emits light in one direction , like a laser , then only a small part of the surface is illuminated and you only see a spot of light
nothing will change with maxwell 's equations due to the discovery of the higgs bosons . maxwell 's equations describe the continuum of electromagnetic theory and know nothing about particle physics .
consider derivative at $t=0$ ; denote $\psi ( 0 ) $ as $\psi_{0}$ $ ( \partial /\partial t ) t\psi ( t ) \big|_{t=0}=\displaystyle lim_{h\rightarrow0} ( ( t\psi_{0} ) ( h ) -t\psi_{0} ) /h$ since $t\psi$ evolves according to $i ( \partial /\partial ( -t ) ) t\psi ( t ) =ht\psi ( t ) $ so $ ( t\psi_{0} ) ( h ) = exp ( ihh ) t\psi_{0}$ . hence we have : $\displaystyle lim_{h\rightarrow0} ( ( t\psi_{0} ) ( h ) -t\psi_{0} ) /h$ $=\displaystyle lim_{h\rightarrow0} ( exp ( ihh ) ( t\psi_{0} ) -t\psi_{0} ) /h$ $=\displaystyle lim_{h\rightarrow0} ( texp ( -ihh ) \psi_{0}-t\psi_{0} ) /h$ $\:\:\ ; \ ; \ ; $ ( since $t$ and $h$ commute , and $t$ is antilinear ) $=\displaystyle lim_{h\rightarrow0}t ( exp ( -ihh ) \psi_{0}-\psi_{0} ) /h$ $=t ( \partial /\partial t ) \psi ( t ) \big|_{t=0}$ notation : $ ( t\psi_{0} ) ( h ) $ means we first act $\psi_{0}$ by $t$ and then time evolve the resulting state by an amount of time $h$ . another argument : following argument seems more relevant here than above one :- we have a one parameter family of states $\psi ( t ) $ which satisfy $i ( \partial /\partial ( t ) ) \psi ( t ) =h\psi ( t ) $ for definiteness suppose $t\in [ 0,1 ] $ , and suppose we partition this interval into $n$ equal parts ( where $n$ is some large number ) as {$0=t_0&lt ; t_1&lt ; . . . . &lt ; t_{n-1}&lt ; t_n=1$} . denote $\psi ( t_j ) $ as $\psi_j$ for $j=0 , . . . , n$ , and let $1/n=\delta$ ( length of one small interval ) . then above differential equation can be written as a set of $n$ linear equations in terms of states $\psi_j$ 's as : $i ( \psi_1-\psi_0 ) /\delta=h\psi_0$ $i ( \psi_2-\psi_1 ) /\delta=h\psi_1$ . . . . $i ( \psi_j-\psi_{j-1} ) /\delta=h\psi_{ ( j-1 ) }$ . . . . $i ( \psi_n-\psi_{ ( n-1 ) } ) /\delta=h\psi_{ ( n-1 ) }$ now in zee 's book the one parameter family of vectors $t\psi ( t ) $ is required to satisfy the differential equation $-i ( \partial /\partial t ) t\psi ( t ) =ht\psi ( t ) $ . or in discretised form it is required that the set of vectors $t\psi_0 , t\psi_1 , . . . . . , t\psi_n$ satisfy following linear equations : $-i ( t\psi_1-t\psi_0 ) /\delta=ht\psi_0$ $-i ( t\psi_2-t\psi_1 ) /\delta=ht\psi_1$ . . . . $-i ( t\psi_j-t\psi_{j-1} ) /\delta=ht\psi_{ ( j-1 ) }$ . . . . $-i ( t\psi_n-t\psi_{ ( n-1 ) } ) /\delta=ht\psi_{ ( n-1 ) }$ now since $t$ is linear wrt addition of states so it can be taken out : $-it ( \psi_1-\psi_0 ) /\delta=ht\psi_0$ $-it ( \psi_2-\psi_1 ) /\delta=ht\psi_1$ . . . . $-it ( \psi_j-\psi_{j-1} ) /\delta=ht\psi_{ ( j-1 ) }$ . . . . $-it ( \psi_n-\psi_{ ( n-1 ) } ) /\delta=ht\psi_{ ( n-1 ) }$ in continuum limit these equations are equivalent to : $-it ( \partial /\partial t ) \psi ( t ) =ht\psi ( t ) $ edit : question : consider a one parameter family of states $\psi ( t ) $ which satisfy schrodinger equation $i ( \partial /\partial t ) \psi ( t ) =h\psi ( t ) $ . is it possible to find an invertible linear operator $t$ that commutes with $h$ and such that for any $\psi ( t ) $ as above , $t\psi ( t ) $ satisfies $-i ( \partial /\partial t ) t\psi ( t ) =ht\psi ( t ) $ ? our previous argument ( 2nd one ) extends to one proof that it is not possible ; here is another one : if $t$ is such an operator then $t\psi ( t ) =exp ( ith ) t\psi ( 0 ) $ . ( because $t\psi ( t ) $ solves time reversed schr . equation ) ----- ( 1 ) also $\psi ( t ) =exp ( -ith ) \psi ( 0 ) $ ( because $\psi ( t ) $ solves usual schr . equation ) . ------- ( 2 ) substituting ( 2 ) into ( 1 ) we get $texp ( -ith ) \psi ( 0 ) =exp ( ith ) t\psi ( 0 ) $ now using the fact that t is invertible we get : $exp ( -ith ) \psi ( 0 ) =t^{-1}exp ( ith ) t\psi ( 0 ) $ again using the fact that $t$ is linear and commutes with $h$ we get $exp ( -ith ) \psi ( 0 ) =exp ( ith ) \psi ( 0 ) $ ( note that if $t$ were antilinear then in place of $exp ( ith ) $ on rhs we would have $exp ( -ith ) $ , and hence there would be no problem ) now multiplying on both sides with $exp ( -ith ) $ we get $exp ( -2ith ) \psi ( 0 ) =\psi ( 0 ) $ differentiating with respect to $t$ and putting $t=0$ we get $h\psi ( 0 ) =0$ but $\psi ( 0 ) $ was any arbitrary state in our space of states . so we have $h=0$ identically . hence the required linear operator is not possible unless $h$ vanishes identically .
yes , you can make a unitary asymptotic s-matrix ( so asymptotic measurements ) when the intermediate states do not evolve in a unitary way . this is what ghost fields do--- the intermediate states in ghost-descriptions include negative probability objects , but when you make asymptotic measurements you do not see the ghosts , you only see the positive probability objects . in cases where you have a ghost description , there are often no-ghost formulations , like light-cone or axial gauges . in these formulations , the hamiltonian is well defined , so that you can ask about measurements on the intermediate states and get well defined answers . these formulations have a reduced symmetry compared to the ghost formulation , but they are manifestly unitary . in ghost formulations , you assume that every measurement is made on asymptotic states which have no ghosts . even if it is not true that every measurement is of an s-matrix quantity , the existence of the unitary formulation guarantees that anything you build out of asymptotic states will only end up measuring a quantity which has a reasonable positive probability interpretation .
the most general answer is " we engineer things with relativity . " the most striking example i know is the cebaf accelerator at jlab . like all modern accelerators it uses rf linacs to add energy to the beam . that only works if the size of the cavities and the frequency of the rf match the speed of beam . now , cebaf is special , beams of widely differing energy can propagate simultaneously through the linac at high efficiency , which only works because the speed of the beam is insensitive to the kinetic energy in the ultra-relativistic regime in which the machine works . pre-upgrade a 50 mev beam could coexist with a 4.5 gev beam . post upgrade those numbers will roughly double .
acuriousmind has it right . the properties of a scalar field would not in principle be different from any other scalar field theory . it depends on the lagrangian for the theory of course , but the form of the lagrangian would still be constrained by the same restrictions on other theories ( general covariance , renormalizability , phase invariance of some sort , etc . ) . the interactions would arise from a term like $\mathcal{l}_{int}\sim \lambda\phi ( x ) f ( g_{\mu\nu} ) $ where $\phi ( x ) $ is your scalar field and $f$ is some function of the tensor field in question . in the case of brans-dicke theory , this term is $\phi r$ with $r$ the ricci scalar for the metric tensor . the coupling constant would naturally determine the strength of the interaction .
that is , it will eventually change its direction of travel , but just because it is accelerating in the opposite direction of the current vector does not mean that it has changed direction ; yet . i think this is the core of your confusion . the object is not accelerating in the opposite direction of the current velocity . try drawing a diagram of the two vectors and you will see what i am talking about .
in $y$ direction you have accelerated movement with constant acceleration , thus $$v_y = v_{y0} - g t$$ and after putting initial conditions $$|v_y| = g t$$ i have no idea whatsoever what did you want to do with your calculation .
after some amount of on and off thinking here 's what i have come up with . please pardon the coarse picture . the interpretation of the dispersion as energy is applicable to non-interacting particle . in general , for interacting particles , $e ( \vec{k} ) $ cannot be interpreted as energy ( of ? ) . however , frequency $\omega$ is always proportional to energy of the system . one could see it in the following way : the schrödinger’s equation , $$i \frac{\partial}{\partial t} \psi = \hat{h} \psi , $$ on fourier transforming is given by , $$ \omega \psi = \hat{h} \psi . $$ therefore , the set of ( discrete ) frequency $\omega$ is the set of eigenvalues of the hamiltonian operator $\hat{h}$ . so the conjugate variable to time $t$ should always correspond to energy of the system . in non-interacting case $\omega \propto e ( \vec{k} ) $ . but , generically in the presence of interactions it should not be the case . comments and corrections are very welcome .
the biggest thing about this supernova is how close it is . a mere 21 million light years away ( as opposed to being a billion light years away ) . the folks at john hopkins think that studying a ype ia supernova is valuable for several reasons . sne ia are also very bright compared to other standard candles , which means they can be seen at high redshifts and so are important to cosmology . this is due to the following : the expansion of the universe is inferred from the observation of a correlation between recession velocity and distance -- the farther away an object is , the faster it is moving away from us . velocity relates to redshift , and so the ability to determine distances out to high redshifts allows us to measure the rate of expansion . this rate is given by hubble 's constant in the local universe where the expansion is a linear relationship : v=h_0*d . however , this relation can in general be much more complex , as it depends on the densities of the various components of the universe . for example , matter tends to slow down the expansion , and if the universe is curved that will also affect the expansion rate . we have observed that the expansion is accelerating , and since we do not know of anything that can cause this acceleration , we call it dark energy . they conclude with type ia supernovae are not only an odd astrophysical phenomenon , they are also an important tool for studying cosmology . the jhu-led mission adept hopes to study the nature of the dark energy using both a bao study and high redshift sne ia observations . in order for this to happen , we need to determine whether adept will be able to delineate between different cosmological models , and to do this realistically requires complex simulations , of which the list in section 4 is only a basic outline . the true conclusion will not be until nasa decides adept 's fate at the end of 2008 , so stay tuned . now that is from 2008 , so this is a wonderful opportunity to get an " up close " look at what a type ia will tell them . caltech also talks about using a type ia as a standard candle , as well as getting a better understanding of dark energy and the hubble constant .
it is most helpful to know about reversible computation , a bit about circuit complexity , and about universal sets of gates in classical computation ( e.g. &nbsp ; the standard spiel about the gate set {and , &nbsp ; or , &nbsp ; not} , or for that matter the single gate nand , being sufficient to compute any boolean function ) . probably all that you need to know about both of these subjects is contained in nielsen and chaung , which still retains it is status as the most popular introductory text book . most of the problems which are commonly treated in quantum computation have either a number-theoretic flavour ( involving integers modulo n for some integer n ; or possibly just vectors over the integers modulo 2 ) , or a graph-theoretic flavour . having a very basic familiarity with number theory &mdash ; prime numbers , multiplicative units , greatest common divisors , and so forth &mdash ; and with graphs is a good idea . any good first-year textbook on discrete mathematics which touches on these topics ought to be adequate . aside from the above , quantum computing is a highly multidisciplinary field with a broad range of topics . this means that beyond the basics , what you should study as background depends strongly on what it is that you are interested in learning about , or in researching . for the essentials of quantum computing , just the above will do a good job for anything which is not on the way to being an ongoing research project . depending on what further subjects you want to pursue , there are obviously other things you might want to investigate as background . here are two which are more likely to be useful to you in forming a perspective on subjects of interest . in many topics of quantum information theory , semidefinite programming often plays a useful role , as optimisation questions in quantum information involve a constraint of a density operator being positive semidefinite . if you had like a big-picture perspective on the power or limitation of quantum computers , you should learn some complexity theory , especially if you would like eventually to investigate problems such as the difference in computational power between the complexity classes np ( the class of difficult problems of p versus np fame ) , and bqp ( the class of problems efficiently solvable by an ideal quantum computer ) ; whose computational power are currently thought to be incomparable , i.e. &nbsp ; neither more powerful than the other . finally , many special topics in quantum computation are either explicit extensions of classical computational theory , or can in any case be construed as a natural generalization of a classical computational subject to the quantum mechanical r&eacute ; gime . so , for any subject which you hear about in quantum computation that strikes you as interesting , you should consider investigating whether there is a classical subject which might give you information regarding the quantum generalization .
it seems your three equations are not linearly independent and so check your math . you should have 3 independent equations to solve for 3 variables like $t$ , $f$ and $a$ . in general you solve for one variable in one equation and substitute it into the others . like equation #2 $t= m ( g-a ) $ to get $$ f = - m g \sin\theta - ( m+m ) a + m g $$ . then repeat for the next variable . in your case pluggin the above $f$ into equation #1 returns $m a=m a$ which is not helpful and hence i stated that your equations should be independent to solve correctly . can you post a diagram in order to check your equations also ?
i think there may be a better forum to ask this question in and it will likely be closed , but information theory is important to many branches of physics in , so here 's a quick answer . the bandwidth of a channel is simply the number of symbols you can send through it per unit time . by symbol , i mean here a single , real number , and this meaning arises through the shannon sampling theorem . see the wikipedia page for this theorem , and go through the proof so you will understand exactly what i mean . now , just one lone noiseless real number can in theory encode as much information as you like . there are $\aleph_0$ digits in a real number ! write out the whole of wikipedia as 0s and 1s and call it a binary fraction between 0 and unity and the whole of wikipedia is still a finite precision , rational binary number ! so you can see in theory that you can send heaps of data over channels that can send only a low number of symbols each second . this theoretical ideal is , of course , limited by noise . it effectively " coarse grains " the real numbers . if i have noise with an amplitude of 0.1units , and can send symbols with an amplitude of up to 1 units , then i roughly have 10 amplitude levels i can encode data on . otherwise put , i can tell apart ten levels . so i can encode $\log_2 10$ bits per symbol in this example . if my noise amplitude is 0.01 units , i can tell apart roughly 100 different levels per symbol . so i can encode $\log_2 100$ bits per symbol in this example . i think you should now be able to see what is going on : the number of bits you can send per unit time is roughly $$b \log_2 s/n$$ the actual shannon-hartley theorem is a little more complicated , but that is the idea . edit : for interest : 64-qam modulation is commonly used for digital communications . this is essentially where the " symbols " are one of 64 points on a regularly spaced grid in the argand plane representing the amplitude and phase of the signal . so this scheme has a spectral efficiency of six bits ( $\log_2 64$ ) per hertz ( i.e. . symbol per second ) . the ultimate spectral efficiency of a typical optical fibre link is of the order of 20 to 25 bits per hertz : see my answer here .
the direct calculation of the derivatives is not that hard . but one can also quickly see the values of the riemann tensor for a sphere – and similar simple shapes – by using the definition of the riemann tensor via the parallel transport of vectors . $$\delta v^\alpha = r_{\alpha\beta\gamma\delta}v^\beta d\sigma^{\gamma\delta}$$ around a point of the sphere $s^d$ , the transport around an area given by $d\sigma^{\gamma\delta}$ for fixed values of the indices ( locally orthonormal basis ) allows you to see that all of this is happening on an $s^2$ only . the other dimensions are unaffected . that is why you get $r_{\alpha\beta\gamma\delta}$ equal to $ ( 1/a^2 ) $ times $g_{\alpha\gamma}g_{\beta\delta}-g_{\alpha\delta}g_{\beta\gamma}$ . effectively , the antisymmetrized pair of indices $\alpha\beta$ has to be the same as the pair $\gamma\delta$ . i did not assume anything special about the point ; all points on a sphere are equally good by a symmetry . so the ansatz for the riemann tensor has to hold everywhere . note that it is multiplied by $1/a^2$ , the inverse squared radius of the sphere . many of your formulae omit it ; moreover , you are using a confusing symbol $r$ for the radius which looks like the ricci scalar – a different thing . in $d=2$ , the riemann tensor only has one independent component and the formula for the riemann tensor in terms of the metric tensor above actually holds for any surface if $1/a^2$ is replaced by $r/2$ . note that the two-sphere has ( ricci scalar ) $r=2/a^2$ . also , the ricci tensor is $r_{ij}=rg_{ij}/2$ in $d=2$ so that the vacuum einstein equations are obeyed identically . for $d=3$ , the riemann tensor has 3 independent components , just like the ricci tensor , so the riemann tensor may be written in terms of the ricci tensor . that is not true for higher dimensions , either .
i will mention just one example of the complexity that curved space introduces into the quantization process . consider minkowski space quantization of the free klein gordon field , which satisfies $$ ( \box+m^2 ) \phi=0$$ a fundamental step in the procedure is performing the mode expansion $$\phi ( x ) =\int{\frac{1}{\sqrt{2\omega_\bf{k}}}} ( a_{\bf{k}}e^{-ikx}+a^*_{\bf{k}}e^{ikx} ) d^{3}\bf{k}$$ here we have a splitting into a negative frequency part $$\phi^- ( x ) =\int{\frac{1}{\sqrt{2\omega_\bf{k}}}a_{\bf{k}}e^{-ikx}}d^{3}\bf{k}$$ and a positive frequency part $$\phi^+ ( x ) =\int{\frac{1}{\sqrt{2\omega_\bf{k}}}a^*_{\bf{k}}e^{ikx}}d^{3}\bf{k}$$ upon quantization , the $a^*_{\bf{k}}$ in the positive frequency parts become creation operators and the $a_{\bf{k}}$ in the negative frequency parts annihilation operators . the splitting is covariant - the exponentials contain lorentz scalars . now if we try to do the same thing in curved space to the ( covariant form of ) the klein gordon equation , we can find spacetimes for which there is no clear way to perform this splitting because in general , there is no " natural " time coordinate . in the minkowski case , we had the action of the poincare group to allow us to deal with the different possible time coordinates - we even had a poincare invariant vacuum state , but here there is no equivalent of the poincare group action . the particle content of the theory depends upon this splitting , and the ambiguity we have in the curved case ( or even in the flat case if we allow non inertial frames ) is the origin of the hawking and unruh effects . that was just a single example of a problem that crops up right from the word " go " in curved space quantization . there has been a lot of effort expended over the last few decades studying quantization on curved spacetimes . for a review , see here .
your intuition about the charge repulsion and strong force acting on protons more is less important that you think . the strong nuclear force is a few orders of magnitude greater than electromagnetism so coulomb repulsion just does not contribute much . what matters most is the nuclear binding energy to separate a proton from the nucleus . if the resulting system is below the proton separation energy it is possible for the proton to tunnel out . see proton emission and proton drip line for more information about this . it does happen but remember neutron emission is also rare . $\beta^+$ and $\beta^-$ and alpha emission are much more common .
i call it inertial force , but to be exact it should be called the rate of momentum change . $$ \vec{f} = \frac{{\rm d}}{{\rm d}t} ( m \vec{v} ) = m \vec{a} $$
the wave function is a mathematical solution of a specific quantum mechanical equation different for different potentials and boundary conditions . this formulation is validated by a plethora of experimental data , not only the two slit experiment . since we are familiar with sound and light interference we call the mathematically similar patterns of the two slit experiment with electrons " interference " patterns . this terminology just reflects that the probabilities in space of finding an electron on the screen are affected by the boundary condition of the two slits to create an interference pattern . the wavefunction given by the solution for " two slits and incoming electron " has the patterns .
1 ) why is that ? is this just a by-product of the reduced size of the cms hadronic calorimeter my investigations got me into this book : " at the leading edge . the atlas and cms lhc experiments " . i think that most of the general questions about the design of those detectors are answered in the book . while seems that it is very difficult to point out a particular reason for the design decisions . in chapter 10 the ideas beyond the design of the hcal for cms are explained . for the cms the tracker and ecal are emphasised , so the decision was to put the hcal inside the solenoid for better magnet design . another requirement is a easy maintenance . since you was critical to that , i am citing : a unique feature of cms is its moving-ring-based structure , allowing for very good access and maintenance of the detector elements . this design feature had the tradeoﬀ for the hcal in that the readout system ( front-end electronics system ) had to be placed inside the magnetic volume . any alternative location for the photodetectors was so far away that there would have been prohibitively large light loss in the long clear optical ﬁbers . so the hcal has to be small while incorporating the photodetectors . and everything has to operate in 4t magnetic field -- the point is that photomultiplier tubes cease to work there . i am citing : unfortunately , phototubes lose their gain quickly in a magnetic ﬁeld ( due to inability to focus the electrons ) . so it seems that answer to the question ( 1 ) are : reduced size , magnetic field and modularity . 2 ) are those resolutions final or can the collaborations make them better when they have more data ? how ? it depends on what do you mean under " final " . these resolutions were measured in beam-tests at sps . the description of those tests are given in references [ 10 ] and [ 11 ] or the paper in question . part of the modules were put under the various beams at sps . citing ref . about alice [ 10 ] : 5.7.2 hadronic end-cap performance . about 25% of the series production modules were exposed to beams of muons , electrons and pions with energies up to 200 gev at the cern sps [ 150 ] . . . 5.7.4 tile-calorimeter performance 5.7.4.1 stand-alone performance approximately 12% of all production modules of the tile calorimeter have been measured extensively in dedicated test-beam periods at the cern sps . . . . 5.7.4.2 combined lar and tile calorimeter test-beam measurements the combined performance of the barrel lar electromagnetic and tile calorimeters was measured in 1996 in the h8 beam at the cern sps . the setup used prototype modules of the two calorimeters . in that sense -- those resolutions are final . but also i asked some experimentalist and he told me that there is a procedure which is called " the understanding " of a detector . you have to measure the performance of the detector as a whole , accounting for everything : geometry , triggers , e.t.c. as far as i understood both detectors are not completely understood yet .
i am in part trying to understand this myself . the berry phase is computed from differential forms , such as the one-forms $\omega$ constructed from states $$ \omega~=~\langle\psi|d\psi\rangle $$ and with the covariant differential $d~=~d~+~\omega$ the two-forms $$ \omega~=~d\omega~=~d\omega~+~\omega\wedge\omega $$ the tensor components of the 2-form $f$ are elements of a self-adjoint principal bundle $p$ . the determinant of these elements $$ det\big|1~+~\frac{ixf}{2\pi}\big|~=~\sum_nc_jx^n $$ which is a characteristic polynomial which represents the chern class . each $c_n ( p ) $ is an element of $h^{2n} ( m ) $ . so the curvature form for the berry phase , or the fubini-study metric $\omega=~dz\wedge d{\bar z}/ ( 1~+~|z|^2 ) ^2$ is evaluated $\int\omega~=~2\pi i$ and gives $c_1~=~1$ so there is a nontrivial cocycle on the “2-level . for this projective geometry there are alternating betti numbers $1 , ~0$ for even and odd . if you had some product of states $\prod_n |\psi_n\rangle$ , say in an entangled state etc , you could apply the differential $d$ up to $n$ times and form and $n$-form . for instance the product $|\psi_1 , ~\psi_2\rangle$ $=~|\psi_1\rangle|\psi_2\rangle$ defines the one-form $$ \omega~=~d|\psi_1 , ~\psi_2\rangle~=~d|\psi_1\rangle|\psi_2\rangle~+~|\psi_1\rangle d|\psi_2\rangle $$ and one could then build up a system of differential forms on various chains . the analogue of the projective geometry for this is a $g_2 ( v ) $ grassmannian and this continues up for n-product spaces .
the two images are equally ' real ' . one is produced in each of your eyes , so they are similar , but viewed from slightly different angles . this means that a reflection of a bright light , which requires a precise alignment of light source , eye and reflecting surface , may be visible in one image and not the other .
ball lightning could definitely be some atmospheric pressure plasma phenomenon . you can make a pretty impressive ball plasma by discharging a kilojoule-scale capacitor bank into a bucket of salt water . check out free-floating atmospheric pressure ball plasma . in most of those pictures they are using a copper sulfate solution , but that is not essential ( sodium chloride also works ) . these ones only last a ( significant ) fraction of a second , but i am sure if you made a larger one ( e . g . by a lightning strike ) , they could last longer . btw , this was the subject of a killer science fair project : http://www.youtube.com/watch?v=se6sbanskoc
branes are ( usually ) extended objects ; $p$-branes are objects with $p$ spatial dimensions . d-branes are a special and important subset of branes defined by the condition that fundamental strings can end on the d-branes . this is literally the technical definition of d-branes and it turns out that this simple fact determines all of the properties of d-branes . perturbatively , fundamental strings are more fundamental than branes or any other objects . in that old-fashioned description , d-branes are " solitons " - configurations of classical fields that arise from the closed strings . they are analogous to magnetic monopoles - which may also be written as classical configurations of the " more fundamental fields " in field theory . in a similar way , d-branes ' masses diverge for $g\to 0$ . non-perturbatively , d-branes and other branes are equally fundamental as strings . in fact , when $g$ is sent to infinity , some d-branes may become the lightest objects - usually strings of a dual ( s-dual ) theory . when we include very strongly coupled regimes ( high values of the string coupling constant $g$ ) , there is a brane democracy . back to the perturbative realm . the condition that open strings can end on d-branes - and nowhere else - means that there exists a particular spectrum of open strings stretched between such d-branes . by quantizing these open strings , we obtain all the fields that propagate along ( and in between ) such d-branes . the usual methods ( world sheets of all topologies , now allowing boundaries ) allow us to calculate all the interactions of these modes , too . so yes , d-branes also vibrate . but because their tension goes to infinity for $g\to 0$ , you need even more energy to excite these vibrations than for strings . the quanta of these vibrations are particles identified with open strings - that move along these d-branes but are stuck on them . the insight that the d-branes are dynamical and may vibrate , and the insight that they carry ramond-ramond charges ( generalizations of the electromagnetic field one obtains from superstrings whose all rns fermionic fields are periodic on the world sheet ) were the main insights of joe polchinski in 1995 that made d-branes essential players and helped to drive the second superstring revolution . other branes typically have qualitatively similar properties as d-branes but one must use different methods to determine these properties . when we quantize a d-brane , we find open string states which are scalars corresponding to the transverse positions . it follows that d-branes may be embedded into the spacetime - in any way . the shape oscillates according to a generalized wave equation again . also , all d-branes carry electromagnetic fields $f_{\mu\nu}$ in them . these fields are excited by the endpoints of the open strings that behave as quarks ( or antiquarks ) . for a stack of $n$ coincident branes , the gauge group gets promoted to $u ( n ) $ . the electric flux inside the d-branes may be viewed as a " fuzzy " continuation of the open strings that completes them to " de facto closed strings " . those fields have superpartners in the case of the supersymmetric d-branes which are stable and the most important ones , of course . d-branes may collide and interact much like all other objects . the most appropriate interaction that allows the open strings to " disconnect " from d-branes is the event in which two end points ( of the opposite type , if the open strings are oriented ) collide . much like a quark and antiquark , these two endpoints may annihilate . in this process , an open string may become a closed string - which may escape away from the d-brane . the same local process of " annihilation of the endpoints " may also merge two open strings into one . such interactions are the elementary explanations of all the interactions between the fields produced by the open strings - for example between the transverse scalars and the electromagnetic fields within the d-brane . aside from that , some branes may also be open branes , and end on another kind of branes . the latter brane always includes some generalized electromagnetic fields that are sourced by the endpoints or end curves or whatever is the $ ( p-1 ) $-dimensional geometry of the boundary of the former brane .
firstly , and forgive me for asking the obvious , are you certain that there are no leaks anywhere in your setup ? i would suggest getting someone else to check it over in person - it may be something obvious you have overlooked that a fresh pair of eyes would see . again , apologies if you have already tried this :- ) assuming that is not it : as you increase the pressure in the hose , the air temp in the hose will also increase ( work is done on the gas to increase the pressure ) . as the temp settles back to equilibrium , the pressure decreases slightly according to $pv = nrt$ . ( i think you say you allow 1 hour for equilibrium prior to increasing the pressure ) . if this is true , then increasing the pressure again should result in a much smaller pressure drop ( since an increase of 18mb will result in much less heating than 1300mb ) . are you able to monitor temperatures inside the hose ? given that the pressure does seem to settle , this is where i would put my money .
i think you have a misunderstanding of the technical terms vapor pressure , boiling and partial pressure . vapor pressure or better equilibrium vapor pressure is the pressure at which an equilibrium is reached between evaporation and condensation at the liquid surface . usually it is a function of liquid temperature . e.g. water has a vapor pressure of about 0.03 bar at 25°c . thus , if the steam pressure in the surrounding is below 0.03 bar , more water will evaporate than steam condensates at the water surface . if the steam pressure is above 0.03 bar , more steam will condense at the water surface than water evaporates . so far this is not related to boiling . boiling occures if the liquid is heated so far that the vapor pressure of the liquid is as high as the ambient pressure . e.g. water at 100°c has a vapor pressure of 1 bar , which is the usual ambient pressure . so at a surface of hot water at 100°c the steam pressure totally replaces the ambient air and the steam is transported away from the surface . this strong evaporation process is called boiling . partial pressure is not related to evaporation processes at all . if you describe air as a mixture of oxygen and nitrogen the total pressure $p_t$ is just the sum of the partial pressures of oxygen $p_{oxygen}$ and nitrogen $p_{nitrogen}$: $p_t = p_{oxygen} + p_{nitrogen}$ . both oxygen and nitrogen are non-condensable gases under standard conditions . this concept applies for any gas mixture also without condensating gases . coming back to your statements : this one is wrong . vapor pressure is a physical property of a liquid at a given temperature . as described above it is the pressure at which condensation and evaporation are in equilibrium . so far it is not related to boiling at all . this one is right . if the steam pressure in the ambient atmosphere is equal to the vapor pressure , condensation and evaporation are in equilibrium and one could say evaporation has stopped . but , be aware that the term partial pressure is not related to evaporation processes only , because also non-condensable gases have a partial pressure in a gas mixture .
neodymium magnets are not made of elemental neodymium . they are $nd_2fe_{14}b$
the fact is , in the context of ideal circuit theory , the inductor voltages are equal in the circuits below : in the lower circuit , the inductor current has a constant component , i.e. , the lower circuit is equivalent to your $c \ne 0$ case . but , there is nothing remarkable or surprising about this . is there something else to your question that i am missing ? [ i ] am asking that why in in elementary physics text books the author directly writes c=0 for ideal inductor without mentioning the initial conditions ? the initial conditions are mentioned when the context is transient analysis . for example , from wikipedia : however , when the context is ac ( sinusoidal ) circuit analysis , the underlying assumptions are ( at least ) : ( 1 ) all sources are sinusoidal and of the same frequency ( 2 ) the circuit is in sinusoidal steady state , i.e. , all transients have decayed . when these conditions hold , can we use voltage and current phasors and the notion of impedance to analyze circuits .
no , the lhc does not violate the uncertainty principle . the principle only affects position and momentum ( not actually velocity ) in the same direction , but in a particle accelerator , you do not have to constrain both position and momentum in any one direction . it is only important to constrain the position in the transverse direction ( perpendicular to the beam ) and the momentum in the longitudinal direction ( along the beam ) .
your lower bound has certainly been considered . even wikipedia does the calculation in a number of places , noting that a black hole will be in equilibrium with the $2.7\ \mathrm{k}$ cmb at about the mass of the moon . the thing about black holes , though , is that there are multiple channels for them to be produced in nature . the remnants of collapsed stars should have masses around $1\ m_\odot$ . this number probably varies far less than the masses of stars themselves vary , but it is set by very complicated physics involving degenerate matter and stellar evolution as a whole . on the other hand , supermassive black holes in the centers of galaxies have masses well over $10^9\ m_\odot$ . so right there you are looking at at least $9$ orders of magnitude of observed variation . add to that another $7$ or so on the lower end if you believe lunar-mass black holes exist , and you can see why there is not a standard $1\ m_\mathrm{bh}$ unit . it should be noted that most all black hole properties of interest scale perfectly in proportion to some power of the mass , usually just the first power . thus one often normalizes quantities to the mass of the particular black hole in question .
do not get too confused by the " bladeless fan " marketing babble . something , probably a traditional blower , is pushing air around inside the device . this is ducted so that the flow blows in one direction from little nozzles on the inside of a ring . that causes a lot more air to be moved by bernoulli 's principle . basically , the ring and nozzles converts high pressure low flow air into low pressure high flow air . overall , this system is likely to be ( i do not have any numbers , just a guess on my part ) less efficient than a traditional fan . the claimed advantage is that you do not feel pulses as individual fan blades spin around . i find that argument rather hard to swallow since i never noticed pulses from a traditional fan . after a relatively short distance the flow will break up and become turbulent anyway , even if it started out perfectly smooth , so this whole issue smells strongly of marketing bs to me .
when the notions of electric and magnetic fields were conceptualized , they imagined that there was an invisible fluid being pushed around by charges , and they leveraged some of the equations and terminology of fluid mechanics . modern understanding of field have largely gotten rid of this picture , but some colorful langauge like " electric flux " remains . if you want to picture positive charge as " amount of fluid added to region per unit time " and negative charge as " amount of fluid removed from region per unit time " , you can , but this thinking only gets you so far . safer to just think of it as an abstract mathematical definition .
conservation of baryon number &lt ; -> global gauge invariance conservation of lepton number &lt ; -> u ( 1 ) symmetry conservation of strangeness is only for the strong ( su ( 3 ) symmetry ) and electromagnetic interactions ( local u ( 1 ) gauge invariance )
i think there is a non-zero induced current . 1 ) during the rise ( and fall ) of the current pulse in wire 1 , a changing , azimuthal , magnetic field is generated ( calculable by ampere 's law ) . 2 ) per maxwell , that changing b-field induces an electric field parallel to the wires , which : 3 ) causes a current to flow in wire 2 ( assuming a wire resistivity $\rho$ and applying ohm 's law ) . thus , if one applies a short pulse of current to wire 1 , you had see two even shorter pulses in wire 2 , one positive and one negative , aligned with the rise and fall of the wire 1 current . there is some variation in e with respect to the radial dimension , which would complicate an exact solution , but the variation is small ( and it does not result in cancellation ) .
okay ! i apply a transformation , which converts my comment into a slightly more self-contained answer . the transformations mentioned here are most naturally described by means of tensor calculus or , more generally , differential geometry . it is needed , roughly speaking , when one wants/needs to introduce a coordinate system at each point in space and study the relations between different points and their coordinate systems . in this case there is a coordinate transformation defined from cartesian system $x^\mu= ( x , y , z ) $ to ellipsoidal coordinates $x^{\mu'}= ( \eta , \mu , \nu ) $ , and an inverse transformation . $\mu$ and $\mu'$ are indices , which can take values from $1$ to $3$ . trasnformation matrix $t^{\mu}_{~\mu'}$ is defined as $t^{\mu}_{~\mu'}=\dfrac{\partial x^{\mu}}{\partial x^{\mu'}}$ . for the given transformation $t^{\mu}_{~\mu'}$ is diagonal , because $x$ depends only on $\eta$ , and so on . metric tensor with matrix $g_{\mu\nu}$ is a quantity , which defines the scalar product of basis vectors $g_{\mu\nu}\equiv \vec{e}_\mu\cdot\vec{e}_{\nu}$ . in cartesian coordinates $x^{\mu}$ , therefore , $g_{\mu\nu}=\textrm{diag}\{1,1,1\}$ . in primed coordinates , for a given point in space ( with its own set of basis vectors ) the same definition holds : $g_{\mu'\nu'}=\vec{e}_{\mu'}\cdot\vec{e}_{\nu'}$ . from tensor calculus , $g_{\mu'\nu'}$ and $g_{\mu\nu}$ are connected by $g_{\mu'\nu'}=g_{\mu\nu} t^{\mu}_{~\mu'} t^{\nu}_{~\nu'}$ , where summation over the same indices is implied . because $t^{\mu}_{~\mu'}$ is diagonal , $g_{\mu'\nu'}$ is also diagonal , as it should be . hence $\vec{e}_{\mu'}$ are orthogonal . now , in orthogonal bases $\vec{e}_{\mu}$ scale factors are defined as $h_{\mu}=\sqrt{\vec{e}_{\mu}\cdot \vec{e}_{\mu}}$ ( which has some physical meaning if one thinks about decomposing a vector in such a basis ) , here summation is not implied . in cartesians , therefore , $h_1=1$ , whereas in ellispoidal coordinates $h_{1'}=\sqrt{\vec{e}_{1'}\cdot \vec{e}_{1'}}=\sqrt{g_{1'1'}}=\sqrt{g_{\mu\nu} t^{\mu}_{~1'} t^{\nu}_{~1'}}=\sqrt{g_{11} t^{1}_{~1'} t^{1}_{~1'}}=t^{1}_{~1'}=t^{1}_{~1'} h_1$ . or , alternatively , one can derive ( without using $h_1 = 1$ ) that $h_1=h_{1'}t^{1'}_{~1} = h_{1'} ( t^{1}_{~1'} ) ^{-1}$ . using the above described , $t^{1}_{~1'}=\dfrac{\eta}{x}=\dfrac{\eta}{\sqrt{\eta^2-a^2}}$ . substituting $t^{1}_{~1'}$ and $h_{1'}$ into $h_1= h_{1'} ( t^{1}_{~1'} ) ^{-1}$ and using $\eta\sim a \rightarrow \infty$ , one gets $h_1 = 1$ . the answer might be hard to read if you have never studied differential geometry , but the key point is that scale factors are not scalars and a simple variable change does not work for them . however , as they are defined for orthogonal systems , the transformation rules that they follow are relatively simple .
given a motion function $\mathbf{x} = \mathbf{\chi} ( \mathbf{x} , t ) $ , the deformation gradient is given by $$ \mathbf{f} = \frac{\partial \mathbf{\chi} ( \mathbf x , t ) }{\partial \mathbf x} $$ if you carry this out , you should find that $det ( \mathbf f ) = ( 1+t ) ( 1+t^2 ) &gt ; 0 \quad\forall t&gt ; 0$ as for finding the inverse motion function , you have to do a bit of algebra . you are given $\mathbf{x} = \mathbf{\chi} ( \mathbf{x} , t ) $ , the components of which form 3 linearly independent equations which you must use to solve for $\mathbf{x}=\mathbf{\chi}^{-1} ( \mathbf{x} , t ) $ .
it is a measure of either the electron 's kinetic energy or it is total energy ( including mass ) . because the electron 's mass is $511\text{ kev} = 0.511\text{ mev}$ , it matters which in this case . by default particle physicists mostly talk about total energy , but nuclear physicist often talk about kinetic energy . in either case you had have to ask to be sure . assuming that is all kinetic energy , then in principle it could be used to ionize more than one hundred-thousand hydrogen atoms with each electron being asymptotically free but having no additional energy ; in practice the energy could not be brought to bear in so ordered a manner . if the particle was directed into a region of high purity hydrogen gas it would actually ionize many fewer atoms , but the ejected electrons would have positive asymptotic kinetic energy .
the reason is because the heat loss occurs mostly in the windows and the fenestration . the idea is that you would like the incoming air to be heated up . also , it creates an air curtain that prevents more heat from being lost through this exposed areas . the final reason is to make the temperature of the room more or less uniform . if the heaters were placed at the center of the room , you would create a large temperature gradient , resulting in drafts and discomfort for the occupant .
lets suppose that all n magnets are identical . assume that they are quite far from each other so we can replace them with n magnetic dipoles with dipole moment $\overrightarrow{m}$ . also assume that the dipoles are placed along the x axis in such a way that they repel each other and all n moments $\overrightarrow{m}$ are parallel . the first step is to find the interaction force between two dipoles . fortunately , wikipedia has a formula for the force so that we can save a large amount of work . link here i write only its x axis component which we need , to save space : $$f=\frac{6\mu_0 m^2}{4\pi x^4}$$ now , to find " spring rate " between two magnets , it is sufficient to find the differential $df$: $$df=-\frac{6\mu_0 m^2}{\pi x^5}dx$$ so for the small displacement the " spring rate": $$k ( x ) = \frac{6\mu_0 m^2}{\pi x^5} $$ now , since we know $k$ it is easy to find the effective spring rate for the top magnet . let $f$ be the force on the top magnet , other end magnet fixed . then the displacement of the top magnet is $\delta x= ( n-1 ) \frac{f}{k}$ so the effective spring rate : $$k_e ( x ) = \frac{6\mu_0 m^2}{ ( n-1 ) \pi x^5} $$
the explosion certainly is hemispherical , see , for instance , this explosion caused by the trinity bomb : the gas cloud that you posted , and what many would consider is synonymous to the nuclear weapons , comes after the explosion . nuclear bombs are actually usually ignited above ground for " maximum destruction . " since the nuclear reaction is immensely hot ( about 4000 k whereas the surface of earth is sitting pretty around 300 k ) , the gas rises much the same way a hot-air balloon rises . at some point , the cold air from around the explosion gets sucked under the mushroom cap and causes the thin column you see : thus , for the most part , it is the extreme temperatures that cause the explosion " bubble " to rise in the first place . and it is the convective air currents under the bubble that cause the column to form .
when we wish to solve a differential equation like your example , a good guess for when $f ( t ) =0$ is something of the form $x ( t ) = e^{i \alpha t}$ . this just gives us an algebraic equation for $\alpha$ , which we can just solve . i interpret your question to mean : but what is this solution ? why is it complex ? indeed , position should certainly be just a real number . the resolution is that a general solution to the differential equation will not be of the form $x ( t ) =e^{i\alpha t}$ for $\alpha$ solving our algebraic equation . in general there will be a couple solutions to the algebraic equation for $\alpha$--just two in your example--say $\alpha_1$ and $\alpha_2$ . then a general solution to your diffeq with $f ( t ) =0$ is of the form $x ( t ) = a e^{i \alpha_1 t} + b e^{i \alpha_2 t}$ for some complex constants $a$ and $b$ . we then need boundary conditions to choose $a$ and $b$ properly . the crux of the biscuit is we can choose $a$ and $b$ so that an $x ( t ) $ of this form is real for all times $t$ ( this assumes $a$ , $b$ , and $c$ are real in the original equation ) . so using the complex guess $x ( t ) = e^{i\alpha t}$ is just a computational trick which makes things easy because the complex numbers have nice properties with algebraic equations ( they are algebraically closed ) . we could have done the whole thing without $e$ , instead just using sines and cosines and it would have worked out too .
there are two related but distinct questions : how do you keep a wormhole stable ? how do you make the wormhole in the first place ? courtesy of matt visser we can give one answer to the first question . matt 's example is to make the wormhole cube shaped , and in that case all you need to do is construct a cube from string i.e. the twelve edges of the cube are made from string . however the string would have to have a negative tension , and indeed it would have to have the ridiculously high negative tension of $−1.52 \times 10^{43}$ joules/metre . this is where your exotic matter comes in since the tension in any string made from normal matter would always be positive . the second question is harder . matt 's analysis applies to a time independant wormhole , i.e. one that has existed for an infinite time . constructing your cube of exotic string would warp spacetime in the manner required for a wormhole , but calculating what happens as you tie the strings into a cube is probably impossible at present . response to comment : this is going to be a bit hard to explain , but the space inside the cube does not exist . it is not part of the manifold on which the universe exists . if you travelled towards the cube you would not hit anything - you had just keep going without feeling anything as you past where it is wall is , but now you had be travelling in the other region of spacetime on the other side of the wormhole . re your comment it does not sound like there is a lot of control , the wormhole matt describes is not the same as the sort of wormhole sci-fi writers use to allow interstellar travel . as far as i know there is no theoretical support for the interstellar travel type wormhole . the wormhole matt describes connects two regions of spacetime but makes no statement about the global topology , so the region of spacetime the other side of the wormhole need not be , and almost certainly is not , some distant region of the universe around us . the wormhole does not allow ftl travel to e.g. alpha centauri . it just allows travel ( at up to the speed of light ) to the new region of spacetime on the other side of the wormhole .
i am not an expert on this , so i would appreciate if errors are pointed out . to my understanding , the difference between the total energy and the free energy is due to statistical mechanics . your simulation works on the level of the smallest constituents of the system , and does not directly look for the lowest energy configuration , but samples the space of all configurations with a probability for each state that is a function of its energy and the temperature . all statistical phenomena should automatically emerge from this . for example , a state that is qualitatively equal to many other states ( essentially corresponding to a macrostate with high entropy ) will be sampled more often than a state of lower energy that is very unlikely . the transition from a lower energy state to this higher energy state should happen in your simulation exactly when the associated free energy change negative , which will depend on temperature .
you are trying all of the right things , but the problem is actually much simpler---you just need to use better relations for this problem . in particular , think about kepler 's law of orbital periods ; and some of the general ellipse equations , like : $r ( \theta ) = \frac{ a \ , ( 1 \ , - \ , e^2 ) }{1 \ , - \ , e \cos \theta}$ ( which is the equation you have , just expressed a little differently ) , and $e = \frac{ r_{max} \ , - \ , r_{min} }{r_{max} \ , + \ , r_{min}} = \frac{r_{max} \ , - \ , r_{min} }{2a}$ here , $a$ is the ' semi-major axis ' , $e$ the eccentricity , $\theta$ the angle of the orbit , and $r_{max}$ and $r_{min}$ are the maximum and minimum separations respectively ( also referred to as ' apocenter ' and ' pericenter' ) . does that help ?
he means that since it is a second order differential equation , it is completely determined by two sets of information , namely the value of $\phi$ and ${\dot \phi}$ at some time $t=t_0$ . in other words , $\phi ( t_0 ) $ and ${\dot \phi} ( t_0 ) $ paramaterizes the solution . we can therefore choose them to take any values , some which will imply a negative value of $\rho$ .
it depends on the fan , but i would guess the majority of domestic fans will use less power at lower speeds . i can state with authority that the fan in my car ( a ford focus ) uses roughly the same power regardless of speed because i have just had to replace the ballast resistor that is uses to control fan speed . when you select a lower speed the fan dissipates power as heat in the ballast resistor so the speed setting makes little difference to the power drawn . i can not be sure about domestic fans , but in the car fan the heat dissipated in the ballast resistor is very noticable and indeed theresistor gets too hot to touch . the fan on my desk does not get hot when used at a lower speed , so i think it is very likely it does not simply dissipate power to lower the speed and therefore it will use less power at lower speeds . it is probably significant that the car fan is dc while domestic fans are ac . it is much easier to control power in ac circuits because you can use a thyristor or something similar to control the power delivery in a lossless way . the only way to be sure is , as energynumbers suggests , to measure the power drawn . a simple power meter like this one is all you need . unfortunately i am working away from home this week otherwise i could measure the power drawn by my own fan and give you a definitive answer . however i am sure there must be some fan owning , power meter armed , experimental physicists reading this :- )
without knowing more about your data set i can only offer a few random suggestions : if you have some kind of pid reason to believe that the tracks might be electron you just assume that they are and compute the energy from the momentum and $m_e$ . in you have a calorimeter in the detector stack you measure the energy , and project the maximum likelihood energy loss back to the vertex . note , however that the measured signal may have error much larger than $m_e$ , so it may be better to use the device for pid and fall back on the previous suggestion .
well the answer is that the body will indeed loose the momentum . but since the mass of the body will decrease as well due to radiation , the velocity should not change .
efimov states in general efimov states are special states of three praticle systems . their existence is a purely quantum effect , because " size " ( i.e. . cross-section ) of these states can be much greater than the range of underlying particle-particle interaction . ( 1 , 2 ) for each system having an efimov state there are in fact infinitely many of such states . some properties of efimov states are universal and do not depend on the nature of underlying particle-particle interaction : ( 1 , 2 ) 1 ) size of $n$-th efimov state is $s_0 \approx 22.7$ times the size of $ ( n-1 ) $-th state 2 ) energy of $n$-th efimov state is $s_0^2$ times the energy of $ ( n-1 ) $-th state $s_0$ is the solution of the following transcedental equation ( 3 ) : $s_0 \cosh ( \pi s_0/2 ) = \frac{8}{\sqrt{3}} \sinh ( \pi s_0/6 ) $ there can be efimov states , so bound three particle states , when there are no bound dimer states . this is often depicted with borromean rings , since if we take one of the praticles away , the resulting two particle state will be unbound . efimov states and halo nuclei the original efimov work treated three identical bosons . however , the particles need not be identical , they can have different mases . the efimov theory can also be extended to describe fermions . ( 2 ) halo nuclei , that can be viewed as efimov states , are the nuclei with two-neutron halos : a two-neutron halo is exhibited by ${}^6he$ , ${}^{11}li$ , ${}^{17}b$ , ${}^{19}b$ and ${}^{22}c$ . two-neutron halo nuclei break into three fragments , never two , and are called borromean because of this behavior ( referring to a system of three interlocked rings in which breaking any ring frees both of the others ) . ( atomic nucleus-wikipedia ) the two neutrons in the halo " do not stick together " , so we indeed have a three-particle system . it is interesting , that [ lithium-11 ] has an exceptionally large cross-section of 3.16 fm , comparable to that of ${}^{208}pb$ . ( 4 ) for an introduction to halo nuclei and efimov states see the presentation halo world : the story according to faddeev , efimov and fano by indranil mazumdar : http://www.boseinst.ernet.in/capss/talks/2011/indranilmazumder.ppt i also advertise the freely available physics . aps . org article on efimov states : http://physics.aps.org/articles/v3/9
the curie temperature or curie point is the temperature at which a ferromagnetic or a ferri-magnetic material becomes paramagnetic when heated . the effect is reversible . on the other hand , the curie-weiss temperature is the temperature at which a plot of the reciprocal molar magnetic susceptibility against the absolute temperature t intersects the t-axis . the curie-weiss temperature can adopt positive as well as negative values . i hope , now you will get the difference .
as per genneth above : the equation given represents the electric field at the image surface , by superposition this is the sum of the two apertures , one term positive and one negative .
provided that these 2 perpendicular forces are the only forces acting on the body , there resultant can be easily found by parallelogram law of vectors . using it i got the acting force as 94.34n and dividing by mass i got the acceleration as 3.77m/s2 . note : fnet^2 = f1^2 + f2^2 ( from parallelogram law when angle between vectors is 90degrees )
a string is a " particle with a complicated internal structure " . to see the rough emergence of particle species , you may start with a hydrogen-string analogy . the hydrogen atom is a bound state of a proton and an electron . it may be in various energy eigenstates described by the quantum numbers $ ( n , l , m ) $ . they have a different angular momentum and its third polarization and different energies that mostly depend on $n$ . it is similar for a string . a string may be found in various states . the exact " spectrum " i.e. composition of these states depends on the background where the string propagates and the type of string theory ( more precisely , the type of the string theory vacuum ) . but for the rough picture , consider string theory in the flat space , e.g. in the 26-dimensional spacetime . take an open string . its positions $x^\mu ( \sigma ) $ may be fourier decomposed and each of the fourier modes , labeled by a positive integer $n$ , produces coordinates of a 24-dimensional harmonic oscillator . so an open string is equivalent to a $24\infty$-dimensional harmonic oscillator ( yes , it is twenty-four times infinity ) . each of the directions in this oscillator contributes $nn/ \alpha'$ to the squared mass $m^2$ of the resulting particle where $n$ is the total excitation level of the harmonic oscillators that arise from the $n$-th fourier mode . at any rate , the possible values of the squared mass $m^2$ of the particle are some integer multiples of $1/\alpha'$ . this dimensionful parameter $1/\alpha'$ is also called $1/l_{string}^2=m_{string}^2$ . the ground state of the string , $|0\rangle$ of the harmonic oscillator , is a tachyonic particle with $m^2=-1/\alpha'$ in the case of bosonic strings . these tachyons are filtered away in the superstring . the first excited state of an open string is $\alpha^\mu_{-1}|0\rangle$ which carries one spacetime lorentz vector index so all these states behave as a vector with $m^2=0$ . they give you a gauge boson . and then there are massive modes with $m^2\gt 0$ . closed strings of similar masses have twice larger number of indices , so for example , the massless closed string states inevitably produce a graviton . so different masses of the resulting particles arise from different values of $nn$ – and the very fact that the values may be different for different excitations is analogous to the same feature of the hydrogen atom or any other composite particle in the world . in string theory , however , one may also produce states with different values of the angular momentum – also somewhat analogous to the hydrogen atom which is a sufficient model – or different values of the electric charge and other charges . for example , in some kaluza-klein-like vacua , the number of excitations of $x^5_{n}$ , the fourier modes of the ( circular ) fifth dimension $x^5$ , will be interpreted as the electric charge and it will behave as the electric charge in all physical situations , too . there are other ways how $u ( 1 ) $ electric-like charges and other charges arise in string theory . see e.g. this popular review http://motls.blogspot.com/2012/08/why-stringy-enhanced-symmetries-are.html?m=1 of ways how yang-mills gauge groups and charges may emerge from different formulations and vacua of string theory . if even this review is too technical , you will have to be satisfied with the popular brian-greene-like description stating that particles of different mass , spin , or charges emerge from strings vibrating in different ways . i am sort of puzzled about your question – and afraid that my answer will be either too simple or too off-topic given your real question – because you must have heard and read these basic insights about string theory about hundreds of times already .
classical electrodynamics mainly deals with two kinds of proplems : a ) the action of a field on a charged particle and b ) the fields arising from the motion of such a field . of course , this can only be approximative but it turns out that a lot of phenomena can be described in this way . however , you are right , an entire treatment would include a ) and b ) simultaniously - including the whole dynamics of such a system with radiative reaction ( or , the abraham-lorentz-force and its relativistic counterpart ) . but as qmechanic pointed out in a comment , there may be no fully consistent way to do so within the framework of classical electrodynamics . jackson ( chapter 16 ) states : the difficulties presented by this problem touch one of the most fundamental aspects of physics , the nature of an elementary particle . although partial solutions , workable within limiting areas , can be given , the basic problem remains unsolved . thus , you will have to search for a really satisfactory answer within the description of quantum electrodynamics . otherwise , you may include the effect phenomenologically . this was done e.g. by barone and mendes in lagrangian description of the radiation damping ( pra , 2007 ) and they give a lagrangian of the form $$\mathcal{l}=\frac{m}{2}\dot{\mathbf{r}}_1\cdot\dot{\mathbf{r}}_2 - \frac{\gamma}{2}\epsilon\left ( \dot{\mathbf{r}}_1 , \ddot{\mathbf{r}}_2\right ) -v\left ( \mathbf{r}_1 , \mathbf{r}_2\right ) , $$ with $\gamma := 2e^2/3c^3$ , with $\epsilon = \epsilon_{ij}dx^idx^j$ beeing the levi-cevita tensor , and the $\mathbf{r}_i$ arise from the special treatment of the problem used in the paper employing some kind of image phase-space representation of the system . furthermore , $v\ , $ is the potential related to the abraham–lorentz–dirac-force which is given explicitely in the paper .
realistically , because the light emitted from the infalling object is quantized , you will observe the " last " photon emitted from the infalling object in ( very much ) finite time . if you keep waiting after that , you will eventually observe the black hole hawking-radiate away , and any " information " carried by the infalling object will be " encoded " in the hawking radiation . here 's an additional argument : when the infalling object passes through the event horizon , it will contribute to the mass of the black hole , which will cause the event horizon to expand outward even more . this ensures even further that the infalling object will no longer be observable from the outside in a relatively short time , compared with the lifetime of the black hole .
in qed there are 4 kinds of divergences : ultraviolet divergences . naive calculations depend on the cut-off in such a way that they go to infinity as the cut-off do . however , qed is a perturbatively renormalizable theory so that non-naive , well-done computations ( see regularization and renormalization ) give sensible results . landau pole . the coupling constant $\alpha={e^2\over \hbar \ , c}$ , which is the expansion parameter in the perturbative series , grows with energy and goes to infinity for a finite value of the energy . it turns out that this finite value of energy is larger than the electroweak scale , where qed merges with the weak interaction and qed is not a good theory of nature anymore . therefore , it is not a real ( phenomenological ) problem . infrared divergences . these are due to the fact that photons are massless . they however cancel out once one takes into account all the effects that contribute to a measurable observable . non-convergent series . the $n$-th term of the perturbative expansion is of the form $\left ( {\alpha\over 2\pi}\right ) ^{n}\ , ( 2n-1 ) ! ! $ , so that the series is not convergent but asymptotic because the factor $ ( 2n-1 ) ! ! $ grows very fast for large values of $n$ . this means that we cannot give a non-perturbative definition of qft by summing up all the terms of the series . however , the first terms are meaningful and actually give predictions that accurately agree with observations . the ' first terms ' are approximately $n\sim {\pi\over \alpha}\sim 430$ . and for this value of $n$ , $\left ( {\alpha\over 2\pi}\right ) ^{n}\ , ( 2n-1 ) ! ! \sim 10^{-187}$ . therefore , as long as we are not interested in a precision of one part in $10^{187}$ , this is not a real problem either . note that qed is the theory of nature that has been confirmed with greatest precision — one part in $10^{9}$ in electron 's anomalous magnetic dipole , for which $n=4$ . for qcd points 1 , 3 , and 4 are more o less the same . however , point 2 does not apply since in qcd the coupling constant $\alpha_s$ gets lower with the increasing of energy , and in fact it goes to zero as energy goes to infinity . see asymptotic freedom . to summarize , infrared divergences are due to not taking into account effects that contribute to the observable magnitude . the asymptotic nature of qft perturbative expansions prevents a non-perturvative ( exact ) definition of the theory ( through its series ) , but does not entail a practical problem when comparing predictions with measurements . the lack of perturbative divergences and landau-like poles are a necessary condition for a theory to be well-defined at arbitrarily high energies . however , theories that contain these divergences ( ultraviolet or landau-like poles ) can still be very useful at energies above some scale . on the other hand , theories without these divergencies ( ultraviolet or landau-like poles ) , such as qcd , do not have to be valid to all energies as theories of nature . as m . brown points out in the comments , there is a relation between instantons and renormalons and the asymptotic nature of series . please , see these notes snd the questions instantons and non perturbative amplitudes in gravity and asymptoticity of pertubative expansion of qft reply to graviton 's comment : in my opinion , a fundamental theory of nature ( whatever it means ) should have a non-perturbative definition . if the perturbative expansion is not convergent , it cannot provide this non-perturbative definition . however , in principle , this does not necessarily mean that theory cannot have a non-perturbative definition or an exact solution , but this must be given by other means .
you need to learn about the eikonal equation and its implications . when the electromagnetic field vectors are locally plane waves , i.e. over length scales of several wavelengths and less they are well approximated by plane waves , then the phase of either $\vec{e}$ or $\vec{h}$ ( or of $\vec{a}$ and $\phi$ in lorenz gauge ) can be approximated by one scalar field $\varphi ( x , \ , y , \ , z ) $ which fulfils the eikonal equation : $$\left|\nabla \varphi\right|^2 = \frac{\omega^2\ , n ( x , \ , y , \ , z ) ^2}{c^2}$$ where , of course , $n ( x , \ , y , \ , z ) $ describes your refractive index as a function of position . this equation can be shown to be equivalent to fermat 's principle of least time and also implies snell 's law across discontinuous interfaces . the ray paths are the flow lines ( exponentiation ) of the vector field defined by $\nabla\ , \varphi$ . otherwise put : the rays always point along the direction of maximum rate of variation of $\varphi$ , whilst the surfaces normal to the rays are the surfaces of constant $\varphi$ , i.e. the phase fronts . a little fiddling with the eikonal equation shows that the parametric equation for a ray path , i.e. $\vec{r} ( s ) $ as a function of the arclength $s$ along the path is defined by : $$\frac{\mathrm{d}}{\mathrm{d}\ , s}\left ( n ( \vec{r} ( s ) ) \ , \ , \frac{\mathrm{d}}{\mathrm{d}\ , s} \vec{r}\left ( s\right ) \right ) = \left . \nabla n\left ( \vec{r} ( s ) \right ) \right|_{\vec{r}\left ( s\right ) }$$ this is where you can take things up . you have $n ( x , y , z ) $ depends only on $x$ , so $\nabla n$ will always be in the $\vec{x}$ direction . everything stays on one plane ; let this be the $x-z$ plane and the position of the point on the path is $ ( x ( s ) , \ , z ( s ) ) $ . we thus get two nonlinear des which can be quite hard to solve : $$\frac{{\rm d}\ , n ( x ) }{{\rm d} s}\ , \frac{{\rm d}\ , x}{{\rm d} s} + n ( x ) \ , \frac{{\rm d}^2 x}{{\rm d} s^2} = n^\prime ( x ) $$ $$\frac{{\rm d}\ , n ( x ) }{{\rm d} s}\ , \frac{{\rm d}\ , z}{{\rm d} s} + n ( x ) \ , \frac{{\rm d}^2 z}{{\rm d} s^2} = 0$$ so you generally need to make some approximation depending on what kind of ray you are dealing with . in fibre optics , for example , you may want to assume that the rays make small angles with the $z$ direction so that $s\approx z$ , whence you would get : $$\frac{{\rm d}\ , n ( x ( z ) ) }{{\rm d} z}\ , \frac{{\rm d}\ , x ( z ) }{{\rm d} z} + n ( x ) \ , \frac{{\rm d}^2 x}{{\rm d} s^2} = n^\prime ( x ) $$ and then you would need to make further approximations depending on the fibre profile . a good reference for all this is born and wolf , principles of optics , chapter 4 or the first half of snyder and love , optical waveguide theory .
that seems like a fun question ! according to wikipedia the day is currently 2ms too long , so that is a factor of 2.31e-8 . so we need to reduce the angular momentum of the earth by this factor . to make life easy consider a mountain on the equator , with a mass $m$ , treat it as a point mass and assume we manage to move it $d$ meters nearer the centre of the earth . the change in angular momentum is : $$\delta l = m ( r_e - d ) ^2 - m ( r_e + d ) ^2 = -4mr_e d$$ where $r_e$ is the radius of the earth . assuming the earth is a uniform sphere it is angular momentum is : $$l_e = \frac {2}{5} m r_e^2$$ so i get the fractional change of the angular momentum to be : $$\frac {\delta l}{l} = 10 \frac{d}{r_e} \frac {m}{m}$$ bearing mind that we are modelling the mountain as a point mass , i would say about 10km was a reasonable distance to move it , i.e. from 5km above sea level to 5km below sea level , so taking $d$ as 10km and $r_e$ as 6380km and setting the change equal to 2.31e-8 gives : $$\frac {m}{m} = 1.5 \times 10^{-6}$$ so if the mass of the earh is about $6 \times 10^{24}$kg , you had need to move about $10^{19}$ kg of mountain . for comparison , a quick google suggests the mass of mount everest is of the order of $10^{15}$ to $10^{16}$ kg so that is somewhere between 1,000 and 10,000 mount everests .
i simply misfactorised the quadratic - i knew it was a stupid mistake . i am amazed that i did not see it , but even more amazed that nobody else did ! here is the correct solution . $$\phi_{01} ( t , x , y , z ) = \frac{1}{2\pi i}\oint d\xi \frac{\xi}{ ( x^{01'} ) ^2 ( \xi -\xi_1 ) ^2 ( \xi - \xi_2 ) ^2}$$ the residue at $\xi_1$ is \begin{align*} r_1 and = \rho_{\xi_1}\frac{d}{d\xi}\frac{\xi}{ ( x^{01'} ) ^2 ( \xi - \xi_2 ) ^2} \\ and = \frac{1}{ ( \xi_1 - \xi_2 ) ^2 ( x^{01'} ) ^2} -2 \frac{\xi_1}{ ( x^{01'} ) ^2 ( \xi_1-\xi_2 ) ^3} \\ and = \frac{\xi_1 - \xi_2 - 2\xi_1}{ ( x^{01'} ) ^2 ( \xi_1-\xi_2 ) ^3} \\ and = -\frac{x ( y+iz ) ^2}{2r^3 ( y+iz ) ^2} \end{align*} this is now the correct answer : ) .
the answer is definitely yes . the ground states ( and low-lying eigenstates ) of many-body systems are generically entangled . examples include the ground states of local quantum field theories ( which describe the fundamental particles and forces of nature ) and ground states of fermionic lattice models ( which describe much of the solid matter we see around us ) . the reason for this is simple : these systems are composed of many interacting constituents . take for example the magnetic dipoles associated with the spins of atoms in a lattice . these spins are quantum mechanical , which means the orientations of their dipole moments fluctuate even at low temperatures , due to the uncertainty principle . in addition , the dipoles interact with each other because of the magnetic fields that they produce . therefore these intrinsic quantum fluctuations become correlated with each other , via the mutual magnetic interaction of the spins . the appearance of local quantum fluctuations that are globally correlated is one of the essential features of entangled states . the entanglement of many-body ground states is highly relevant , since many systems are nearly in their quantum ground states even at everyday temperatures . examples include the electrons in a typical metal , or the electromagnetic radiation field at optical frequencies . this is true whenever the typical energy scales of the system are large compared to room temperature ( i.e. . the fermi energy for a metal , or the frequency of optical radiation ) . interestingly , the principle of locality places quite severe restrictions on the kinds of entangled ground states that actually appear in nature . typically , these states obey area laws . that is , the amount of entanglement between a sub-region of the system and the rest scales with the area of the boundary of the sub-region , rather than its volume . for more information on area laws , see j . eisert et al . , rev . mod . phys . 82 , 277 ( 2010 ) .
your question is really " how does the human eye work ? " , since the contact lens is designed to adjust the optics of the human biological lens . this image from the wikipedia article on the anatomical lens shows how the lens focuses incoming light from the left onto the retina ( right ) . for people who are nearsighted or farsighted , the light coming into the eye does not end up in focus . see this picture for the case of nearsightedness : you should imagine the contact lens being placed over the cornea ( left surface ) and causing the rays to adjust so that the image ends up in focus .
this is a very good question , but it is really two completely different questions in one . feynman 's propagator the probability amplitude for a photon to go from x to y can be written in many ways , depending on the choice of gauge for the electromagnetic field . they all give the same answer for scattering questions , or for invariant questions involving events transmitted to a macroscopic measuring device , but they have different forms for the detailed microscopic particle propagation . feynman 's gauge gives a photon propagator of : $p ( k ) = {g_{\mu\nu} \over w^2 - k^2 + i\epsilon}$ and it is fourier transform is $2\pi^2 p ( x , t ) = {g_{\mu\nu} \over {t^2 - x^2 + i\epsilon}}$ this is the propgation function he is talking about . it is singular on the light cone , because the denominator blows up , and it is only this singularity which you can see as propagating photons for long distances . for short distances , you see a $1/s^2$ propagation where s is the interval or proper time , between source and sink . to show that you recover only physical light modes propagating , the easiest way is to pass to dirac gauage . in this gauge , electrostatic forces are instantaneous , but photons travel exactly at the speed of light . it is not a covariant gauge , meaning it picks a particular frame to define instantaneous . the issues with the feynman gauge is that the propagator is not 100% physical , because of the sign of the pole on the time-time component of the photon propagator . you have to use the fact that charge is conserved to see that non-physical negative-coefficient-pole states are not real propagating particles . this takes thinking in the feynman picture , but is not a problem in the dirac picture . the equivalence between the two is a path integral exercise in most modern quantum field theory books . fermat 's principle and lagrangians fermat 's principle , as you noted , is not a usual action principle because it does not operate at fixed times . the analog of the fermat principle in mechanics is called the principle of maupertuis . this says that the classical trajectory is the one which minimizes $$j = \int p dx = \int \sqrt{2m ( e-v ( x ) ) } dx$$ between the endpoints . this principle is also timeless , and it can be used to construct an approximate form for the time fourier transform of the propagator , and this is called the gutzwiler trace formula . the gutzwiller trace formula is the closest thing we have to a proper quantum analog of the maupertuis principle at this time . lagrangian for light the analog of the lagrangian principle for light is just the principle of that light travels along paths that minimize proper time , with the additional constraint that these proper times are zero . the lagrangian is $ m\int ds = m\int \sqrt{1-v^2} dt$ but this is useless for massless particles . the proper transformation which gives a massless particle propagator is worked out in the early parts of polyakov 's " gauge fields and strings " as a warm-up to the analogous problem for string theory . the answer is : $ s= \int {\dot{x}^2\over 2} + m^2 ds$ the equivalence between this form and the previous one is actually sort of obvious in euclidean space , because of the central limit theorem you must get falling gaussians with a steady decay rate . polyakov works it out carefully because the anlogous manipulations in string theory are not obvious at all . the second form is not singular as m goes to zero , and gives the proper massless propagator . transitioning between the two introduces an " einbein " along the path , a metric tensor in one dimension .
i am afraid the actual situation is much more complicated than you have been told . for one thing , the superconductivity does not occur between neutrons , but between quarks themselves . the topic of high density qcd is a very cool interplay of condensed matter and high energy physics , and a very nice review is available by frank wilczek . however , that article does need some background in qcd and superconductivity simultaneously to appreciate . a shortened version might go something like this : inspiration : free fermions are incredibly unstable to superconductivity , in that any attractive interaction will cause it ( in fact , there is an old theorem by pierls ( ? ) that almost all interactions ( even repulsive ones ) will cause superconductivity if you cool far enough ) . in qcd , quarks naturally attract already ! so at sufficiently high density and low temperature , we can imagine that qcd can cause a strong attractive instability to a fermi gas of quarks . complications : 6 flavours , chiralities , masses of quarks are different , etc . simplification by complication : realise that the normal state of qcd ( i.e. . 3- and 2-quark combinations ) is just that : one possible state . other phases of qcd exist , and we can study the phase boundaries and so forth even if we can not compute things exactly ( universality saves the day ! ) . we find that at really high densities , quarks pair up to give a background diquark condensate , through which single quarks move , and through the anderson-higgs mechanism gains a large mass by eating some goldstone modes . all gluons become gapped ( again , anderson-higgs ) , apart from one which mixes with the photon . the symmetries of this solution is actually the same as that of normal matter --- replace baryons with quarks ( + their diquark condensed background ) and mesons with diquarks ; this suggests that they are really the same phase in theory . in practise , getting from one to another requires some other phases in the middle , which are more complicated and arise due to the quark masses and the number of flavours , etc .
i am going to assume that you have some experience with information theory as it relates to computation theory . this book : http://books.google.com/books/about/optical_computing.html?id=zffraaaamaaj is a good foundational look at how optical computing works , combining the physical processes with the computational understanding . this paper will provide a more up-to-date overview of where the state of optical computing is today : http://www.hindawi.com/journals/aot/2010/372652/ and more importantly provide further direction on where to look for more information .
am i mistaken in thinking that it would hit the wall slightly faster ? the insight of relativity is that this is not true . no matter whose clocks and rulers you use , you will always measure the speed of light ( in vacuum ) to be the same . this has as a consequence the fact that lengths and times are not as absolute as was once thought . if you are looking at a ruler that is moving relative to you , then that ruler will appear to be shorter ( along the direction it is moving ) ; and a moving clock will appear to be slower . this must mean that light is not moving relative to " space " as a whole . relativity does away with the idea of absolute space and instead all velocities are relative to some object or frame of reference ( hence relativity ) .
according to http://en.wikipedia.org/wiki/isotopes_of_carbon#table it emits two protons
( 1 ) first argument an ordinary object that is spinning on an axis has an angular momentum which is determined by how the mass of the object is distributed about the axis , and how fast the object is spinning . for a fixed angular momentum , if the mass is distributed farther from the axis the angular velocity is lower ; if the mass is distributed closer to the axis , the angular velocity is higher . think of a spinning ice skater who turns at one , slower speed with arms extended and at another , higher speed with arms pulled in overhead ( on the axis ) . no size has been found for electrons ; they appear to be point particles . if an electron has finite size ( which happens to be too small to see ) , it introduces issues in any classical description , like charge self-repulsion having infinite energy or having a surface which spins faster than the speed of light . if the electron is a point particle , i.e. , has no finite size , then it cannot have angular momentum due to spinning about it is own center of mass because the entire object is on its rotational axis . how to get out of this conundrum ? you cannot " see " an electron to determine whether it is spinning or not . the " spinning " of the electron is not measurable ; it makes no sense to speak of it in science . however , you can measure an electron 's angular momentum ; it makes sense to speak of angular momentum in science . therefore , do not think of the electron as a " spinning " object ( which we can never know or observe ) ; think of it as simply having " intrinsic " angular momentum . ( 2 ) second argument the usefulness of an analogy in science is determined by whether you can make inferences or understand other features in a first system under study by way of the analogy and knowledge of a second system . thinking of spin as " spinning " introduces conceptual problems classically , does not generalize easily to massless objects , does not handle half-integer spin well ( the representations of a classical spinning object do not look like a spin 1/2 object ) , and allows you to infer almost nothing correctly about the electron , other than feeling like you know where the angular momentum comes from . this point of view does not work well to start with , and is a dead end as you keep studying physics . on the other hand , thinking of spin as intrinsic angular momentum avoids all the above-identified issues and , you will find with further study , fits nicely into the rest of physics . ( 3 ) regarding spin-1/2 and the value ℏ√3/2 , these come from group theory and a particular choice of units for angular momentum . this can not be really explained well in a post ; study group theory for physicists . good luck !
if we assume that all the matter that is only visible in star systems ( baryonic matter ) , then we are not able to account for how galaxies rotate . similarly , we have problems trying to calculate movement in a cluster of galaxies , or the rate of expansion of the universe . it seems as if there is more gravitational interaction happening than just the matter we see . so , we postulate something called " dark matter " that is essentially matter which exerts gravitational force but is not visible to us since we can not see it ( any form of electromagnetic radiation ) . other observations such as gravitational lensing ( light bending around a heavy object ) due to invisible/dark objects also indicate the existence of dark matter . as for you question of why this mass is not to be found in black holes . . . if this mass were in a black hole , it would be highly localized . also , ( super ) massive black holes are typically found only in the centre of galaxies , whereas to explain the observed effects , we need dark matter to be quite well diffused all around the galaxy and some in between galaxies . we take into account gravitational interactions due to the mass which such black holes can have and ascribe the remaining discrepancy to dark matter . as always , the wikipedia page on dark matter gives more details if you are interested .
you could make an analogy between the pressure distribution of a sound wave and the mass density distribution of a realistic spring undergoing vibrations , but it would not give you the explanation you are looking for . as a matter of fact , that would be more like explaining a sound wave in terms of springs , rather than what you are trying to do , i.e. explaining a spring in terms of waves . although i am not intimately familiar with the details , basically what goes on at the microscopic level of a spring is that , when the spring is at equilibrium , the atoms are set in some sort of rigid structure . any given pair of atoms has a potential energy which is a function of the distance between those two atoms , so the entire spring has a potential energy determined by all the distances between every possible pair of atoms : $$u = \sum_{i , j} u_{ij} ( r_{ij} ) $$ in equilibrium , the spring will take a shape which minimizes this total potential energy . if you think about it , a metal spring might typically be formed by heating some metal to make it malleable ( or even melting it ) , and then forming it into the desired shape before it cools . the heat allows the atoms to move around relatively freely so that they can reach the equilibrium configuration that minimizes their potential energy , then once the spring cools , they are frozen in place . of course , the atoms are not completely frozen in place . as i see that georg has already written in his answer , the potential energy between two atoms ( $u_{ij} ( r_{ij} ) $ ) has a minimum at their equilibrium distance and goes up on either side . if you add some energy into the system , say by exerting a force on it , you can get the atoms to move closer together or further apart . when you stretch or compress a string , you are really just doing this to all the ( pairs of ) atoms in the spring simultaneously . the atoms will , of course , " try " to return to their equilibrium position , i.e. they will " try " to minimize their potential energy , and this is what you feel as the restoring force of a spring under tension .
i am confused . bernoulli 's eqn . says the static pressure inside the jet should be less than atmospheric . as you go further out and the jet slows down , then it should approach atmospheric pressure . the pressure gradient between the atmosphere outside and the low pressure inside the jet leads to air getting sucked into the jet ( entrainment ) .
strictly speaking , you have 4 equations and 5 unknowns . however , given that the coefficient a is applied to the incoming wave-function , you could arbitrarily set it equal to 1 ( because it represents 100% of the wave ) and solve the system of equations for e . then $t=e$ . this is how the problem is handled in most cases . alternatively , if you absolutely cannot set $a=1$ , then try assuming a is a given and solve the 4 equations for b , c , d , and e in terms of a . then , again , perform $t=e/a$ . in theory , the ratio for any a will be the same as for a=1 . ( i checked , it is , the a divides out in the end ) . edit you can easily solve for b , c , d , and e using matrices , where your four system equations are : $$\begin{pmatrix}-1 and 1 and 1 and 0 \\ i \mathcal l and \mathcal k and -\mathcal k and 0 \\ 0 and e^{\mathcal kd} and e^{-\mathcal kd} and -e^{i\mathcal ld} \\ 0 and \mathcal ke^{\mathcal kd} and -\mathcal ke^{-\mathcal kd} and -i\mathcal le^{i\mathcal ld}\end{pmatrix} \begin{pmatrix}b \\ c \\ d \\ e\end{pmatrix}=\begin{pmatrix}a \\ i\mathcal la \\ 0 \\ 0\end{pmatrix} $$ optionally , $a=1$ . but if you invert the matrix and solve for e , you should get : $$e={4ia\mathcal k\mathcal l\over\mathcal k^2 e^{i\mathcal ld-\mathcal kd}-\mathcal k^2 e^{d\mathcal k+id\mathcal l}+2i\mathcal l\mathcal ke^{id\mathcal l-d\mathcal k}+2i\mathcal l\mathcal ke^{id\mathcal l+d\mathcal k}-\mathcal l^2 e^{id\mathcal l-d\mathcal k}+\mathcal l^2 e^{id\mathcal l+d\mathcal k}}$$ and , of course , a=1
i think i can tackle the mechanics aspect . what you can see is an inverted metallic cone with a helical groove running down and around its surface . on zooming in , you the perforations in the cone 's surface . there is a pump which pumps the fluid back to the top as it flows down . as the empty cone begins to refill , the fluid also starts to leak out of the sides . that is why you see that flowery pattern emerge from the bottom and travel upwards . once the fluid reaches the top the cone is filled and at maximum pressure , when the pump is turned off and the cycle begins again . so the purely mechanical aspect of this construction is very clever on its own and presumably substituting other fluids would yield different kinds of fountains . the leafy structure is generated due to magnetic fields . i can not say how exactly . ps : on further reflection it seems that one can have an electromagnetic pump rather than a mechanical one . imagine a inductor running through the center of the cone . a sinusoidal current would generate a sinusoidal magnetic field within the inductor , which would alternately pull the fluid up the cone and then push it back down .
it is really not clear what hypothetical limits you are imposing . i take your question to mean that in the process of baryogenesis the various baryons like protons and neutrons highly favored up quarks ( lots more protons than neutrons ) . remember , quarks are subject to confinement so other than a quark-gluon plasma , quarks are confined to baryons . since stellar formation depends on hydrogen and hydrogen formation depends on protons and electrons primarily , it is unlikely that there would be any meaningful impact . also , remember that free neutrons are unstable and will undergo beta decay and convert to protons . if there is an excess of protons the reverse can happen via electron capture . i am pretty sure even if there were some wild physical law that forced there to always be a huge imbalance of up to down quarks there would still be enough hydrogen formation for stars to form . it is not clear what this restriction would do to hydrogen to helium fusion though .
there are many ways to do this . which option you choose depends on what degree of performance you require , and how much money you re willing to spend . first of all , you should understand that , while you could apply a wavelength selective coating to a lens , this would much more commonly and cheaply be done with a wavelength filter separate from the lens itself . now , the cheapest way to do this would be with a glass filter which absorbs short or long wavelengths selectively . there is a wide range of these available from schott glass . i use these commonly , and i have never had trouble . most vendors will be happy to produce custom shapes , thicknesses , etc . the down side to filter glass is that it does not have a particularly sharp cutoff between the bassband and the stopband . for that , you will need a dielectric coating engineered to your specifications . for that , i would look at cvi or newport , although there are other vendors out there . there may be something you can use in their catalogs , but custom orders are normal for the optical manufacturing industry , so do not hesitate to call up a sales engineer . in my experience , sales people in this industry are very well educated on their products , or will direct you to an engineer who can tell you exactly what they can produce for you . again , there are other vendors you could look at , but these are the ones i would go to first . at the very least , looking at their catalogs will give you an idea of what it is you are really looking for .
33 mj is the electrical energy . i think the projectile is about 1kg , so the efficiency is about 10% , not so bad . plasma from electrical breakdown , which then gets accelerated the same way the projectile does , with exb force
in the context of cellular radio communication the statement that the lower frequencies propagate further is correct . the frequency dependence is empirically illustrated in the hata formula for propagation in urban areas . this formula predicts the path loss ( the attenuation in decibels ) from the transmitter to the receive position . it actually gives the path loss averaged over an area of the order of maybe a hundred meters . a principal contributor to the frequency dependence in urban areas is the diffraction loss caused by propagation over/past buildings . see this review article . the other contributor to path loss - the free space loss - is common to both the cellular and satellite cases . the satellite , however , using line-of-sight communication does not have the diffraction component in the calculation .
i understand that a general interpretation of the $1/r^2$ interactions is that virtual particles are exchanged [ . . . ] general relativity does not suppose that zero-mass particles exchanged . you do not need quantum field theory for this . in a purely classical field theory , we have field lines , and the field lines from a spherically symmetric source should radiate outward along straight lines . in a frame where the source is at rest , we expect by symmetry that the field lines are uniformly distributed in all directions . the strength of the field is proportional to the density of the lines , which falls off like $1/r^2$ in a three-dimensional space . this whole description is complicated by the polarization of the field . gravitational fields have complicated polarization modes . nevertheless , the $1/r^2$ result is unaffected . finally , we have an issue unique to gr , which is that the field is the metric , and this means that the field itself affects the measuring tools that we use to measure things like $r$ , the field , and the area of a surface through which we are counting the number of field lines that penetrate . these are all strong-field issues , so for large $r$ , they do not affect the $1/r^2$ argument . is it a postulate ? no . in the standard formulation of gr , the main postulate is the einstein field equations . from it , we can prove birkhoff 's theorem , which says that the schwarzschild metric is the external field of a static , spherically symmetric source . the weak-field limit of the schwarzschild metric corresponds to a $1/r^2$ field .
ok i think i see where your confusion lies . you are talking about the four velocity and acceleration in the instantaneous rest frame $f'$ , and as you say in this frame the four velocity is $ ( 1 , 0 , 0 , 0 ) $ . your mistake is to assume the four velocity is constant in $f'$ , because it is not . remember that after an infinitesimal time $dt$ the rocket is not longer in $f'$ - it is in a new instantaneous rest frame $f''$ . the rocket 's velocity in the new rest frame $f''$ is still $ ( 1 , 0 , 0 , 0 ) $ , but in old $f'$ frame it has now changed due to the acceleration . hence $d{\bf u}/dt$ in $f'$ is not zero . if you are interested chapter 6 of gravitation by misner , thorne and wheeler derives the equations of motion that you started with .
typically for the wavelength band , power requirements and pulse duration you mention , ti:sapphire lasers are employed . afaik it is not uncommon to use the electro-optically shuttered output of a regeneratively mode-locked ti:sapphire as a seed for an ultrafast regenerative amplifier . for example , a spectra-physics spitfire ace regenerative amplifier can be seeded using a mai tai sp oscillator . the oscillator has a mode-locking rep rate of 84 mhz , but the regenerative amplifier has a maximum repetition rate on the order of several khz , so presumably some shuttering is done to pick out single pulses . in this manner , pulses in the 100 femtosecond regime can be created , with reasonably high power output . for shorter pulse generation ( $&lt ; 20$ fs ) , these pulses mentioned above can be used in a non-collinear phase-matched optical parametric amplifier with subsequent pulse-compression using fused silica prisms .
while i agree with the caveats made by dmckee in his comments , there is an obvious interpretation of stopping power as the change in momentum caused by the projectile . the mass and velocity of the projectile are $m$ and $v$ respectively , and the mass of the target is $m$ . since the target is stationary the initial momentum is just $mv$ . assuming the bullet transfers all its energy to the target ( i.e. . it stops within it and does not go through and out the other side ) then after the impact the projectile and the target will be moving with some velocity $v'$ , so the total momentum is $ ( m + m ) v'$ . conservation of momentum tells us that the initial and final momenta must be the same so : $$ ( m + m ) v ' = mv $$ and rearranging this gives : $$ v ' = \frac{m}{m + m} v $$ let 's say the stone weighs 0.1kg . my mass is about 70kg , so if the stone hit me then final velocity would be : $$ v ' = \frac{0.1}{0.1 + 70} 30 \approx 0.043 m/s $$ now for the bullet . i guess it depends on which pistol you choose , but the . 45 acp has a muzzle velocity of about 350 m/sec and a 185 grain bullet weighs about 0.012kg so : $$ v ' = \frac{0.012}{0.012 + 70} 350 \approx 0.06 m/s $$ which is actually pretty close to the stone . really the key bit of the calculation is the initial momentum of the projectile , $mv$ . the stone weighs about ten times as much as the bullet but is moving at about one tenth of the speed , so the two factors of ten cancel out and the initial momentum is roughly the same . this is presumably the basis of the claim about stopping power . but attend to dmckee 's comments . the velocity changes calculated above are around 0.04 m/s , and since humans can run at several metres per second you are not literally going to stop a running human with either the gun or the sling . presumably a gun stops someone because it makes them fall , and friction with the ground does the stopping , in which case you should probably avoid trying to shoot axe wielding maniacs when standing on an ice rink .
the resistivity of a given substance ( air ) is fixed . for constant potential difference , current will change as $i=\frac v r$ . however , air is not an ohmic substance so ohm 's law does not apply for air .
let 's see with help of an example . let the particle is at $ ( 0,0 ) $ moving with speed of $2m/s$ at $t=0$ and is subjected to acceleration $-2\hat i\ \text{m/s}^{-2}$ . now see after $2 seconds$ . we see displacement is zero , and distance travelled =$2m$ . also acceleration is constant but still $\langle speed\rangle\not=0$ whereas $\langle velocity \rangle=0$ now let 's see the graphs for this scenario . you see distance traveled is area under v-t curve ( all positive ) but displacement is are with signs . the area under x-axis is negative . so , graphs can help in cases where velocity changes direction .
fundamental particles are identical . if you have two electrons , one from the big bang and the other freshly minted from the lhc , there is no experiment you can do to determine which one is which . and if there was an experiment ( even in principle ) that could distinguish the electrons then they would actually behave differently . electrons tend to the lowest energy state , which is the innermost shell of an atom . if i could get a pen and write names on all of my electrons then they would all fall down into this state . however since we can not tell one electron from another only a single ( well actually two since there are two spins states of an electron ) electron will fit in the lowest energy state , every other electron has to fit in a unique higher energy level . edit : people are making a lot of comments on the above paragraph and what i meant by making electrons distinguishable , so i will give a concrete example : if we have a neutral carbon atom it will have six electrons in orbitals 2s1 2s2 2p2 . muons and tauons are fundamental particles with very similar properties to the electron but different masses . muons are ~200 times more massive than electrons and tauons are ~3477 times more massive than an electron . if we replace two of the electrons with muons and two of the electrons with tauons all of the particles would fall into the lowest energy shell ( which can fit two of each kind because of spin ) . if in theory these particles only differed in mass by 1% or even 0.0000001% they would still be distinguishable and so all fit on the lowest energy level . now atoms are not fundamental particles they are composite , i.e. composed of " smaller " particles , electrons , protons and neutrons . protons and neutrons are themselves composed of quarks . but because of the way that quarks combine , they tend to always be in the lowest energy level so all protons can be considered identical , and similarly with neutrons . to take the example of carbon , there are several different isotopes , different number of neutrons , of carbon ( mostly $^{12}c$ but also ~1% $^{13}c$ and ~0.0000000001% $^{14}c$ {the latter which decays with a half life of ~$5,730$ years [ carbon dating ] but is replaced by reactions with the sun 's rays in the upper atmosphere} ) . if we take two $^{12}c$ atoms , and force all of the spins to be the same . this is not too difficult for the electrons of the atom since the inner electrons do not have a choice of spin because every spin in every level is already full . so only outer electrons matter . the nucleons also have spin . with our two $^{12}c$ atoms with all of the same spins , we now have two indistinguishable particles which if you set up an appropriate experiment ( similar in principle to the electrons not being able to occupy the same state ) we will be able to experimentally prove that these two atoms is indistinguishable . answer time : are atoms unique ? no . do atoms have any uniquely identifying characteristic besides their history ? their history of a particle does not affect it* . no particles are unique . atoms may have isotopes or spin to identify one from another , but these are not unique from another particle with the same properties . would it contain information with which we could positively identify that they two are the same ? yes only because we could positively identify that this carbon atom is the same as almost every other carbon atom in existence . *unless it does , in which case it may be considered a different particle with different properties .
this is a statement about a congruence of null geodesics . we are looking for a conjugate point , which is just a place where the null geodesics cross each other . the theorem is putting a bound on how far you can advance the affine parameter $\nu$ along the geodesics before the conjugate point occurs ( this is what is meant by affine parameter distance ) . to derive the bound , you need to assume the null energy condition , which says that $r_{\mu \nu} l^\mu l^\nu \geq 0$ for all null vectors $l^\mu$ . you are also assuming that the geodesics you are working with are hypersurface orthogonal , which means the twist $\omega_{ij}$ vanishes , and does not contribute to the raychaudhuri equation . by noting also that the shear $\sigma_{ij}$ is a spatial tensor , so will have a positive definite norm , $\sigma_{ij}\sigma^{ij}\geq 0$ , we find that this equation is saying $$\frac{d\rho}{d\nu}\geq\rho^2 . $$ this differential equation is easily solved , and you find that $$\rho\geq\frac{1}{\rho_0^{-1}+\nu_0-\nu} . $$ when $\nu-\nu_0=\rho_0^{-1}$ , the rhs of this inequality goes to infinity , which means that $\rho$ diverged at some value of $\nu$ before that . when $\rho$ diverges , it means there was a conjugate point .
i do not know if this is what you mean to ask , but what i know as a quantum quench is a sudden change in the potential , sufficiently fast that it can be considered instantaneous . in that case the state does not change instantaneously , but obviously its time evolution does : from that point it evolves according to the new hamiltonian . if originally you were in an eigenstate , generally you will be in a superposition of states for the new hamiltonian after the quench .
an engine operating in a cycle can operate continuously i.e. for any number of cycles . if you just pull out one step in the cycle you do not have a useful engine because it can only operate once and then only for a short time . an isothermal process is reversible by definition because temperature is not defined in an irreversible process . so an isothermal process does not increase ( total ) entropy . incidentally this does not violate the second law since the second law says only that entropy cannot decrease . it does not forbid energy staying the same .
in the good old newtonian world the gravitational acceleration is just : $$ g = \frac{gm}{r^2} $$ the equation you give is just a rewriting of this . if you substitute : $$ r_s = \frac{2gm}{c^2} $$ into : $$ \frac { r^2 }{r_s} \frac {g}{c^2} = \frac {1}{2} $$ you will find it simplifies to the first equation . so there is nothing especially meaningful in this procedure . you say : but then at that point light cannot escape so i would think that $g \rightarrow c/t$ where $t=1$ sec . but this is not a well motivated statement as this is not the relationship between the acceleration and the ( newtonian ) escape velocity . the equation for the escape velocity is : $$ v_e = \sqrt{\frac{2gm}{r}} $$ so if you substitute the expression for $r_s$ you end up with : $$ v = c $$ this is the well known result that at the event horizon the escape velocity calculated from newtonian gravity is equal to the speed of light . however you should not take this result too seriously . if you want to understand why light can not escape from the event horizon you need to look at the general relativistic treatment . as it happens this is dicussed in the question why is a black hole black ?
unfortunately for you the authority says no . compound prefix symbols , that is , prefix symbols formed by the juxtaposition of two or more prefix symbols , are not permitted . this rule also applies to compound prefix names . source however , you might use $145\times10^3\text{ mpa}$ .
measure the angular distance between a star and the distant background stars . repeat 6months later when the earth is on the opposite side of the sum if you know the length of the baseline ( the earth 's orbit ) and the angle then you know the distance to the star . in fact we define the distance to stars in terms of this angle and the earth 's orbit - see http://en.wikipedia.org/wiki/parsec because of the blurring effects of the atmosphere it is difficult to measure angles much less than 1 arcsec , and so determine the distance to stars more than a few parsecs away directly by this method . the hipparcos satelite was able to make much more accurate measurements ( less than 1 milli-arcsec ) and so measured distances 1000x further
the criterion you mention is roughly the threshold for the formation of the coulomb gap in the hubbard model or the local moment in the anderson model . it is a common break-down region for many approaches starting from one of the limits ( insulator/local moments versus conductor/mixed valence ) . for perturbation theory in $u$ , see the prb 36 , 675 ( 1986 ) by horvatić et al . and references to and form that paper . a more comprehensive discussion can be found in the monograph by hewson . as far as i remember , perturbation in $u$ on the level of self-energy does not give the expected exponential dependence on $u$ for the kondo temperature . unfortunately , i do not know specifics of flex method to help you in more detail .
notice first that the phase space of any theory is nothing but the space of all its classical solutions . the traditional presentation of phase spaces by fields and their canonical momenta on a cauchy surface is just a way of parameterizing all solutions by initial value data -- if possible . this is often possible , but comes with all the disadvantages that a choice of coordinates always comes with . the phase space itself exists independently of these choices and whether they exist in the first place . in order to emphasize this point one sometimes speaks of covariant phase space . this is well known , even if it remains a bit hidden in many textbooks . for more details and an extensive and commented list of references on this see the $n$lab entry phase space . then notice that the phase space of every field theory that comes from a local action functional ( meaning that it is the integral of a lagrangian which depends only on finitely many derivatives of the fields ) comes canonically equipped with a canonical liouville form and a canonical presymplectic form . the way this works is also discuss in detail at phase space . a good classical reference is zuckerman , a more leisurely discussion is in crncovic-witten . this canonical presymplectic form that exists on the phase space of every local theory becomes symplectic on the reduced phase space , which is the space obtained by quotienting out the gauge symmetries . this quotient is often very ill-behaved , but it always exists nicely as a " derived " quotient , and as such is modeled by the bv-brst complex ( as discussed there ) . the whole ( lagrangian ) bv-brst machinery is there to produce the canonical symplectic form existing on the reduced phase space of any local action functional . since the einstein-hilbert action and all of its usual variants with matter couplings etc . is a local action functional , all this applies to gravity . recently fredenhagen et al . have given careful discussions of the covariant phase space of gravity ( and its liouville form ) , see the references listed here . it follows that the " dimension " of the covariant phase space of gravity does not depend on the " size of the universe " , nor does it make much sense to ask this , in the first place . a given cosmology is one single point in this phase space ( or rather it is so in the reduced phase space , after quotienting out symmetries ) . however , you might be after some truncations or effective approximations or coarse graining to full covariant gravity . for these the story might be different .
i did a bit of discussion on this subject in this thread on music . se . the fundamental does not necessarily have the strongest amplitude . as said by alfred centauri , it depends on the initial configuration : ideally , the string returns to exactly that configuration after each $\tfrac1{\nu_1}$ , and the amplitude of each harmonic in frequency space is proportional to its amplitude in the initial configuration in wavenumber space . now the initial configuration for plucked instruments happens to be roughly what i depicted in the first figure in the music . se answer : something between a triangle and a sawtooth , and as we know both have a monotonically descreasing 1 sequence of fourier coefficients ( $\mathcal{o} ( \tfrac1{n^2} ) $ for triangle , $\mathcal{o} ( \tfrac1{n} ) $ for sawtooth ) , so the fundamental does tend to have the strongest amplitude on the string in the beginning . but this may not hold true for an actual instrument sound for a variety of reasons : on the guitar , if you strike the string quickly with a pick , the initial configuration may be more like a smoothened dirac peak , for which all of the lower frequencies have the same amplitude . similarly for a loud piano note . then , resonances in the instruments ' bodies may lead to the fundamental decaying faster than some of the other harmonics , so after a while they are stronger , though instrument builders classically try to prevent this . similarly , as said by john rennie , the bodies may be more efficient at conducting some of the harmonics to the air than the fundamental and likewise the microphone at measuring the sound . finally , you should assure yourself of what this fourier spectrum of yours actually shows : in audio production , spectrum analysers do not display something proportional to the amplitude $a ( \omega ) $ at frequency $\omega$ , but in fact to $a ( \omega ) \cdot\sqrt{\omega}$ so that pink noise appears to have constant amplitude . ( for both historic / technical and convenience reasons . ) so if in such a spectrum the fundamental turns up slightly weaker than e.g. the second harmonic , it may in actuality still be stronger . 1 actually , the sequence of fourier coefficients of a triangular wave itself is not monotonic , but alternates between a monotonic sequence ( for odd $n$ ) and zero . since the fundamental has odd $n=1$ , that amounts to the same result .
for clarity , there is a common misconception about plasma here . plasma when being introduced for the first time to someone who does not know what it is , it is called " the fourth state of matter " which is an inaccurate description of it . since this term is used for introducing some one to plasma , it is no big deal . when a material changes from a distinct phase to another , it goes through a physical process called phase transition . when gas becomes plasma , it does not go through the standard phase transition . hence plasma-in a general sense-can not be regarded as a distinct phase as solid , liquid and gas phases . it is a phase of the gaseous state . in certain rare cases however , transition from gas to plasma can be described as phase transition . plasma by definition is a mixture of free electrons and their ions ( possibly negative ions ) . you need enough energy to liberate electrons from atoms . roughly speaking , when you put that energy in a solid , energy might be dissipated as heat . if you put that energy in a liqued , energy might be dissipated in vaporization . if you put it in a gas it goes into breaking atoms and molecules ( creating plasma ) . the following figure makes it clearer hopefully that was useful
you are quite correct , you had write the opposite velocity with a negative sign . you just need to decide what sign convention to use . in your example you are only considering motion in one dimension , so you could take motion to the right to be positive in which case motion to the left would be negative . or you could take motion to the left positive and motion to the right negative . it does not matter what convention you use as long as you are consistent . suppose we take motion to the right to be positive so in your first example both a and b are moving to the right . the velocity that a measures is $v_b - v_a$ , so in your first example it is 105 - 100 or 5 m/s . if b is moving in the other direction the velocity $v_b$ is -105 m/s . it is negative because it is moving to the left . the velocity a measures is now -105 - 100 or -205 m/s . the minus sign tells us the motion is to the left so a sees b moving to the left at 205 m/s .
the diagram looks like the fat man bomb that was dropped on nagasaki . the wikipedia article gives lots of info on the design if you are intereted in pursuing it further . the casing is just to make it aerodynamically stable so it fell in a controlled way . the bomb itself is spherical so the case could be spherical as well if it were not for aerodynamic requirements .
if the moon were exactly the same as the earth , then sure , there is no major reason to suspect it would be any different . it is in the same orbit around the sun as us , so it gets heated by the same amount . this would place it in the habitable zone . however , habitability is not the same as being in the habitable zone , and the detailed answer depends on how you make the surface gravity match that of earth . the surface gravity of a sphere of radius $r$ and average density $\rho$ is $$ g = \frac{4\pi}{3} g \rho r . $$ most rocky bodies in the solar system have about the same density - that of a rock - so making the moon 's gravity match the earth 's is just a matter of making it bigger . essentially it would become earth 's twin in every way . on the other hand , maybe you intended to keep the size the same . in that case you would have to increase the density . it is not clear what you would make the interior out of , but it is pretty certain you will not get the same geology as on earth . for one , smaller bodies cool off too fast to be geologically active at this age ( roughly 5 billion years ) . you see , when the planets condensed out of the gas and dust swirling around the sun billions of years ago , they were hot - gravitational potential energy went down , and so thermal energy went up . their heat capacity is proportional to their volume , but their heat losses are proportional to their surface areas . thus objects with high surface area-to-volume ratios ( i.e. . small things ) cool quickly . the thing is , earth 's geologic activity probably had a large role in building up and maintaining the atmosphere and oceans we know and love . in either case , there is also the problem of tidal locking . it is suspected by some that having tides was crucial for the development of life . the moon is already tidally locked with the earth - we only ever see one face of it - so it has no tides . if you scaled it up , you might tidally lock the earth as well . the moon would essentially be in a geostationary orbit , and we would not have tides . this is the case for the pluto-charon system , for instance .
you say : terminal velocity depends on two things- surface area and speed but i think you are getting slightly mixed up about the terminology . the drag ( i.e. . air resistance ) depends on surface area and speed , but the terminal velocity is the speed and it just depends on the surface area ( and air temperature , density , etc , etc that we will assume is constant ) . you say ; with the parachute you have a larger surface area but lower speed and this is quite correct but the speed is the terminal velocity so with the parachute you have a larger surface area but lower terminal velocity .
no . it does not measure time of flight . kinect is , deep down , a structured light scanner , meaning that it projects an infrared pattern ( so invisible for us ) . according to the underlying technology firm primesense , the structured light code is drawn with an infrared laser . this pattern is then read by an infrared camera and the 3d information is reconstructed from the distortions of the pattern . this results in a depth channel which is made available through usb . if you want to see the pattern , you may have some luck by turning off all the lights in the room , turn on the kinect , and try to use your cellphone camera . generally , these camera sensors are sensitive to ir , which appears as green . you may verify if this is the case by trying the same with a tv remote and pressing the buttons . the led should turn green .
yes current does flow until $q/v$ equals $c$ . in the case of two capacitors in series , the effective capacitance is $1/ ( 1/c_1 + 1/c_2 ) $ , because the voltage $v$ is effectively divided between them . maybe an easier way to see this is to define inverse capacitance $k = 1/c = v/q$ which is , for a particular capacitor , the amount of voltage $v$ needed to put a given charge $q$ on one plate ( and $-q$ on the other ) . then it is easier to see that $k = k_1 + k_2$ , because the voltages across them sum , because they are in series . the charge on the right plate of $c_1$ comes from the left plate of $c_2$ .
yes , your friend is right . within electrostatics , an electric field $\vec{e}$ should be curl-free $\vec{\nabla} \times\vec{e}= \vec{0}$ . the drawn electric field lines looks like the electric field is of the form $$ e_x=e_x ( y ) , \qquad e_y=0 , \qquad e_z=0 , $$ cf . the rule that to depict the magnitude $|\vec{e}|$ , a selection of field lines is drawn such that the density of field lines ( number of field lines per unit perpendicular area ) at any location is proportional to $|\vec{e}|$ at that point . here the $x$-axis is horizontal , the $y$-axis is vertical , and the $z$-axis perpendicular to the plane . this is only curl-free if $e_x=e_x ( y ) $ is independent of $y$ , which it is not on the figure .
you want something like the poisson-boltzmann equation , or its linearized form , the debye-huckel equation . to illustrate the effect , consider an immobile spherical charge $q$ at the center of your coordinate system , surrounded by small mobile charge carriers of charge $\pm q$ . gauss 's law will give you the potential $\phi ( r ) $ as a ( spherically symmetric ) function of $q$ and the number of mobile charges contained within the radius $r$ . the potential energy of a single mobile charge $q$ located at $r$ is $u = q\phi ( r ) $ . because these charges can easily move around , they will follow a boltzmann distribution . combining those two facts will give you a differential equation for the potential everywhere . it is a nasty equation , because it contains an $\exp ( \phi ( r ) /kt ) $ term . if the potential is much less than $kt$ , you can taylor expand the expoential as $\exp ( \phi/kt ) = 1 + \phi ( r ) /kt + o ( \phi^2 ) $ . that gives you a linear differential equation for $\phi$ . the result of that equation is that $\phi ( r ) \propto \frac{e^{-\kappa r}}{r}$ . so in addition to the $1/r$ dependence you see from the immobile charge , you acquire an exponential fall-off due to the " cloud " of mobile charges . $\kappa^{-1}$ is called the debye screening length . the most important characteristic is that the debye length is proportional to the square root of the concentration of your mobile ions . it should be possible to write out the poisson-boltzmann equation for an arbitrary fixed charge/potential . and if the ionic strength that you care about is sufficiently weak , it should be possible to also write down the corresponding debye-huckel equation . my guess is that the primary effect of adding the ions would still be the $e^{-\kappa r}$ fall-off .
it is the " latent heat of fusion " produced when water freezes that protects the crop , see p . 3 here : http://fruit.wisc.edu/wp-content/uploads/2011/02/understanding-frost-in-fruit-crops1.pdf
0 . caveat lector : this was done before i drank my morning coffee , so there may be some errors in the reasoning ( well , the physical reasoning , the mathematics should be kosher ) . 1 . perfect fluid . so we have two stress-energy tensors here . one is the stress energy tensor for a perfect fluid $$\tag{1}t^{\alpha\beta}_{\text{fluid}} = \rho \ , u^\alpha \ , u^\beta + p \ , h^{\alpha\beta}$$ where we have the worldlines of the fluid 's particles have velocity $u^\alpha$ the projection tensor $h_{\alpha\beta} = g_{\alpha\beta} + u_\alpha \ , u_\beta$ projects other tensors onto hyperplane elements orthogonal to $u^\alpha$ the matter density is given by the scalar function $\rho$ , the pressure is given by the scalar function $p$ . we had need extra terms if there were heat flow or shear involved . 2 . scalar field . now , we have another distinct stress-energy tensor for a massless scalar field : $$\tag{2}t^{\mu \nu}_{\text{scalar}} =\partial^{\mu}\phi\ , \partial^{\nu}\phi-\frac{1}{2}g^{\mu \nu}\partial_{\rho}\phi\ , \partial^{\rho}\phi$$ we would use this equation when modeling , e.g. , massless pions ( or some other massless spin-0 field ) . 3 . problem : are these two related ? now if we take our matter density to be , in the appropriate units , $$\tag{3a} \rho = 1 + \frac{1}{2}\partial_{\rho}\phi\ , \partial^{\rho}\phi $$ and the pressure $$\tag{3b} p = \frac{-1}{2}\partial_{\rho}\phi\ , \partial^{\rho}\phi $$ then ( 2 ) resembles ( 1 ) . this is after pretending $\partial^{\mu}\phi=u^{\mu}$ , which terrifies the original poster ( but that is what condensed matter physicists do , so i suppose i could end here content ) . is this kosher ? we should first note if we wanted to take the derivative of some function along the worldline $x^{\mu} ( s ) $ with respect to the " proper time " ( length ) $s$ we have $$\tag{4} \frac{\mathrm{d}f}{\mathrm{d}s}=\frac{\mathrm{d}x^{\mu}}{\mathrm{d}s}\frac{\partial f}{\partial x^{\mu}}$$ by the chain rule . for general relativity , we use the " comma-goes-to-semicolon " rule , but for a scalar quantity $f$ we have $$ \nabla_{\mu}f = \partial_{\mu}f . $$ ( if this is not obvious , the reader should consider it an exercise to prove it to him or herself . ) the punchline : identifying $\partial^{\mu}\phi=u^{\mu}$ is kosher . how ? observe in equation ( 4 ) the guy in front , the $\mathrm{d}x^{\mu}/\mathrm{d}s$ is just some vector . so in the very , very special case that equations ( 3a ) and ( 3b ) hold , and $\mathrm{d}x^{\mu}/\mathrm{d}s= ( 1,0,0,0 ) $ , we see that we can indeed recover the first stress-energy tensor as a special case of the scalar field 's stress-energy tensor .
yes , this should be possible using a chiral material or the faraday effect . first example . second example . however , the calcite + wave plate system is probably a lot easier .
these are essentially the same , as indicated by the formulas , with one having units of length and the other of time . so , ( 1 ) yes , the conformal time of emission of the light we see is simply today 's conformal time minus ( $1/c$ times ) the comoving distance to the galaxy ( though this is not directly measurable but rather must be found with e.g. the luminosity distance or the angular diameter distance ) . ( 2/3 ) $\eta$ is a rescaling of " normal " time $t$ ( proper time for an observer moving with the hubble flow in an frw universe ) , and so it still measures a " time . " if you foliate spacetime with spacelike slices of constant time , then $\eta$ can measure the distance between the slices containing two different events . $\chi$ you can take to be the proper distance between two events , if both events are projected onto our current time slice by following the hubble flow . note that the purpose of conformal time and comoving distance is to make the robertson-walker metric conformally equivalent to minkowski ( or something very similar in the non-flat case ) : $ds^2 \propto -c^2d\eta^2 + d\chi^2 + s_k ( \chi ) ^2 d\omega^2$ . thus photons ( $ds^2 = 0$ ) traveling in the radial ( $d\omega = 0$ ) direction simply move a comoving distance $\delta\chi = c \delta\eta$ in a conformal time $\delta\eta$ , giving the simple relation between comoving distance and conformal lookback time applicable to ( 1 ) .
you are right that the force balance is non-zero and that the pendulum-bob is not moving , but this does not mean that the pendulum-bob is not accelerating does it . so , at the moment the ' bob ' is still , it is accelerating back to the centre-line of the oscillation with it is maximum absolute velocity . see here for more information and a nice animation of this phenomenon . i hope this helps .
short answer : you cannot construct a reversible ( $\delta s_{cycle}=0$ ) cycle when $t_h&lt ; 0 , t_c&gt ; 0$ . so the expression for the efficiency does not apply . for the long answer we have to go through the derivation of the eficiency of the carnot cycle . as usual , one have to pay a lot of attention to signs . we have energy changes in heater and cooler related to their entropy changes : $$\delta e_h=t_h\delta s_h , \quad \delta e_c=t_c\delta s_c$$ the heater looses energy , while the cooler receives it : $$\delta e_h &lt ; 0 , \quad \delta e_c &gt ; 0$$ so the work done by a working body that transfers energy from heater to cooler is : $$\delta a = -\delta e_h - \delta e_c &gt ; 0$$ the efficiency of a cycle is a relation of the work done ( $\delta a&gt ; 0$ ) to energy lost by the heater ( $-\delta e_h&gt ; 0$ ) . $$\eta = \frac{\delta a}{-\delta e_h} = 1 + \frac{\delta e_c}{\delta e_h}=1 + \frac{t_c}{t_h}\frac{\delta s_c}{\delta s_h}$$ in " ordinary " case : $t_h&gt ; 0 , t_c&gt ; 0$ we , consequently , have : $$ \delta s_h&lt ; 0 , \quad \delta s_c&gt ; 0$$ and we provide an example of an invertible cycle ( the carnot cycle ) , which , therefore , have to obey : $$\delta s_{cycle} = \delta s_h+\delta s_c = 0 \quad \rightarrow \quad \frac{\delta s_c}{\delta s_h} = -1$$ which leads to the usual formula . if the heater have negative temperature : $t_h&lt ; 0 , t_c&gt ; 0$ we will have : $$ \delta s_h &gt ; 0 , \quad \delta s_c&gt ; 0$$ so there is no way you can create a cycle with $\delta s_{cycle}=0$ . the only thing you can say is that : $$\frac{\delta s_c}{\delta s_h}&gt ; 0 \quad \rightarrow \quad \frac{t_c}{t_h}\frac{\delta s_c}{\delta s_h}&lt ; 0 \quad \rightarrow \quad \eta&lt ; 1$$
you are using standard si units for all the other terms ( no unit multipliers ) . if you look at the specific heat of water ( at standard atmospheric pressure ) you will find the specific heat is $\approx 4.2 \mathrm{kj\ , kg^{-1}\ , k^{-1}}$ or $\approx 4200 \mathrm{j\ , kg^{-1}\ , k^{-1}}$ . it is the latter you want to use . i hope this helps .
re . 2nd question:- " if i plug in my memory stick/usb into my laptop and load it up with documents etc . will its mass/weight increase ? " its mass will not necessarily increase or decrease . it depends on the prior state of the memory . if there are the same average numbers of binary ones and zeros before and after the weight would be approximately the same . this is due to how the ones and zeros are stored on the memory stick . placed electrons create charges on the chip gates to store information , but if you are just rearranging them there is no extra weight . ref . wiki:- floating gate transistor " because the floating gate ( fg ) is electrically isolated by its insulating layer , any electrons placed on it are trapped there and , under normal conditions , will not discharge for many years . "
thermodynamics is a phenomenological description of macroscopic systems , and it is laws are based on empirical observations . the first law , first states that a state function called internal energy $u$ exists for macroscopic systems ( an experimental fact ) , that can be thought of as the analog of potential energy in mechanics , for macroscopic systems ; then defines heat intake of the system : 1 ) for an adiabatically isolated macroscopic system ( i.e. . , when the only sources of energy are mechanical ) , the amount of work required to change the state of the system only depends on the initial and final states . ( an observational fact ) 2 ) when the adiabatic constraint is removed the amount of work is no longer equal to the change in the internal energy , and their difference is defined as the heat intake of the system : ( definition of heat ) $$\delta q=du-\delta w$$ here , $\delta q$ and $\delta w$ are not separately functions of state , but their sum ( internal energy ) is . note that $\delta w$ is the work done on the system .
solution 1---guess-work : if the forces applied on the two ends are equal , say both $1.5\ n$ , the spring will get stretched $1.5\ m$ . a natural guess is that the stretch is determined by the average of the two forces at the ends , which in this case are both equal to $1.5\ n$ . therefore , for the case you mentioned ( $1\ n$ applied to one end , and $2\ n$ to the other end ) , the answer is again obtained from the average : the spring is stretched $1.5\ m$ . solution 2---precise analysis : if the spring is massless , the question is ill-posed ; the nonzero net force yields infinite acceleration . if the spring has mass $1\ kg$ ( you can extend the following analysis to arbitrary mass ) , then since the net force on it is $2\ n-1\ n =1\ n$ , the spring will have $1\ m/s^2$ acceleration to the right . now let us , as observers , accelerate along with the spring so at all times we are in rest with respect to each other . so we ( the observers ) have $1\ m/s^2$ acceleration to the right . since we are accelerating to the right , we are no longer inertial observers ; so if we want to use newton 's laws we should think everything we see is subject to a " gravitational field " with strength $1\ m/s^2$ to the left ( this is the push-back force you experience when a car you sit inside accelerates ) . therefore we see a spring with $1\ n$ force applied to its left end , $2\ n$ applied to its right end , and a gravitational pull of $1\ n$ on it to the left ; the total force is zero in our ( non-inertial ) accelerated frame , as it should be , since we see the spring at rest . now , applying newton 's second law ( we are still in the non-inertial frame remember ) to the tiniest bit of spring at the left end , we learn that the tension of the spring is $1\ n$ at the left end ; similarly the tension of the spring is $2\ n$ at the other end . the tension in the middle of the spring linearly interpolates between these two values . this linear gradient in spring 's tension is due to the left-wards gravitational field ; a similar ( gravitational ) effect results in linear vertical pressure gradient in liquids at rest . now you see why this problem is much more difficult than those with equal forces applied to both ends of the spring . here the tension varies along the spring , leading to further complications . however it is not too difficult to overcome these complications . . . the tiniest bit of the spring at the left end is subject to $1\ n$ at its both ends ; if every bit of the spring was like that , the spring would stretch $1 m$ . the tiniest bit of the spring at the right end is subject to $2\ n$ at its both ends ; if every bit of the spring was like that , the spring would stretch $2 m$ . but the parts of the spring in the middle are subject to a tension which linearly interpolates these two values , therefore on average the tiny pieces of the spring stretch in a way that results in the whole spring stretching $1.5\ m$ . this confirms the guess-work in solutions 1 . you can convince yourself that although the ( $1\ kg$ ) mass we introduced for the spring is necessary to make the problem well-defined , the final answer does not depend on the mass of the spring .
einstein 's theory of gravity is already relativistic so i think that what you are asking is this : beginning with newtonian gravity and making an analogy with coulomb 's law ( where mass is analogous to electric charge etc . ) , and taking into account special relativity effects of a ( mass ) current etc . , does the analog of magnetic force pop out ? the answer is : yes . however and unfortunately , the gravitational waves that also pop out , analogous to electromagnetic waves , transport negative energy .
kamland borexino has set moderately strict limits of the total power of a central geo-reactor . see for instance geo-neutrino : experiments ( pdf link ) a talk by one of my colleagues . ( jalena notes that borexino 's limit is the strongest one going , but kamland was the leader for a while . ) the upper extreme of these limits is less than half the total geological power , but quite non-trivial . the bottom goes all the way to zero . there is also a recent paper ( that i have yet to read ) on a variation of this idea : the kamland-experiment and soliton-like nuclear georeactor . part 1 . comparison of theory with experiment . i have no idea , how the rest of these ideas stand .
at the fundamental level there are four forces associated with the interactions of particles in the microcosm : the strong , the weak , the electromagnetic and the gravitational one . the last two are long range forces and influence the behavior of matter macroscopically too , in a collective manner . macroscopically phenomena of absorption can be observed which at the microscopic level end up being interactions of fundamental particles . for example : a sponge absorbs the water . a black wall absorbs the visible light falling on it etc . at the microscopic level all these are interactions of the electromagnetic field of the molecules and atoms with the incoming particles ( water molecules or photons in the two examples ) . one has to be clear about the scale of the phenomenon .
assume that the point mass , $m$ has two tiny thrusters , mounted so as to exert purely tangential force in the plane of the circular motion , one clockwise , and the other counter-clockwise . the magnitude of the constant velocity of the mass is $v$ , and the radius of the circle is $r$ . measure the position of the point mass in the standard cartesian coordinate way : angles are measured from the positive x-axis , counter-clockwise positive . at the point where the mass is at a position angle $\theta$ . the total radial force inward on the mass , $f_r$ is given by the centripetal force equation:$$f_r=\frac{mv^2}{r}$$ there are two forces that supply this radial force : the tension , $t$ in the string , and the inward radial component of the force of gravity:$$f_{g-r}=mg\sin ( \theta ) $$so:$$\frac{mv^2}{r}=t+mg\sin ( \theta ) $$and:$$t=\frac{mv^2}{r}-mg\sin ( \theta ) $$note that this implies that:$$v &gt ; =\sqrt{rg}$$ or the string tension will become negative near the top of the circle , an impossibility . the conditions of the question also require that at all times the net tangential force , $f_t$ , be zero . the tangential component of the force of gravity , $f_{g-t}$ is given by:$$f_{g-t}=-mg\cos ( \theta ) $$where a positive force implies counter-clockwise force . the thrusters are needed to supply the exact opposite force to the mass at all times .
maybe you can think about it like this : the equation of motion for a physical system ( here , an object ) which obeys newtonian mechanics is $$f ( t ) = m\frac{\mathrm{d}^2y}{\mathrm{d}t^2}$$ that is , given $f ( t ) $ and a fixed set of initial conditions , you can determine $y ( t ) $ . now suppose you have two arbitrary functions $f_1 ( t ) $ and $f_2 ( t ) $ . for each of these functions , imagine subjecting the object to a force that follows that function , assuming the same initial conditions , and then using the above equation to determine the resulting motion of the object , $y_1 ( t ) $ and $y_2 ( t ) $ respectively . $$\begin{align} f_1 ( t ) and = m\frac{\mathrm{d}^2 y_1}{\mathrm{d}t^2} \\ f_2 ( t ) and = m\frac{\mathrm{d}^2 y_2}{\mathrm{d}t^2} \end{align}$$ by adding these two equations , you get $$f_1 ( t ) + f_2 ( t ) = m\frac{\mathrm{d}^2 ( y_1 + y_2 ) }{\mathrm{d}t^2}$$ which shows that $y_1 ( t ) + y_2 ( t ) $ is the motion that results from the combined time-dependent force $f_1 ( t ) + f_2 ( t ) $ . if you are analyzing some physical system that is more complicated than a simple object , then the equation of motion may be something more complicated than $f_\text{net} = ma$ . but as long as the underlying equation of motion specifies a linear relationship between $y ( t ) $ and $f ( t ) $ , then you can use green 's functions .
current cosmological theorists suppose that the universe is exactly identical , no matter where it is viewed from , so long as it is viewed at the same time . at the time of the big bang , the distances between any two given points seems to shrink to zero ( or some nonzero value that we supposedly will derive from quantum mechanics ) . the conclusion is that the big bang happened everywhere , all at once . this is also how you get out of the ' was the big bang a black hole ? '-type questions : even though you had large concentrations of matter at times close to the big bang , they were spread out over all space , which is different than just having a clump of matter with finite extent ( the second thing would collapse to a black hole ) .
first , try and see if you can get the 6 year old to think about " what if there are colors we can not see " ? explain to her that the color we see is the color of " light " . now , show her a remote control , and press some button . there is an ir bulb up front , ask her if it flashes when you press the button ( it should not ) . now , use a phone camera to look at the ir bulb , most phone cameras will show white light when the button is pressed . explain to her that the light coming from the remote is " invisible " , in the sense that it is of a color we can not see . however , the camera can see it because the camera sees slightly more " colors " than we can , and when it tries to display it it shows it as white . explain to her that this is " infrared " light , a light that is " more red than red itself " . whenever someone turns on the tv , a light signal is sent to the tv . ( you may want to explain that this light has some " bending " capabilities , but that is not entirely necessary ) . this ought to get her past the mental block when it comes to " light that is not light " . mentioning that some animals see more/less colors than we do helps . now , talk about the spectrum : explain that the light that we can see is a very small portion of the kinds of light that actually exist . the spectrum is what she sees when she looks at a rainbow , but it really does not " stop " at red or purple ; she just can not see it . if you wish , you can then talk about radio waves , and how they are light that can easily " bend " ( i.e. . diffract ) . talk about x-rays , which is light that can pass through skin but not bones . this can actually lead to an interesting side track where you explain how an x-ray is nothing but a photograph with a different kind of light . once you reach here , it is easy to explain uv . mention that while the sun emits a lot of visible light , it is not limited to the visible spectrum and emits a significant amount ( much less , but not negligible ) of uv and ir as well . you can actually extend this to sound as well , talk about how there are sounds we can not hear . for that matter , sounds just outside your hearing range will be clearly audible to most six year olds . if you can generate increasing frequencies from your computer ( it is actually possible for our vocal cords to work in the inaudible ranges , but it takes some practice to get that to work so it is just easier to use a computer ) , you can both show here that different people/ages have different frequency ranges 1 , and that there are sounds that even she can not hear . ( to do the latter you may want to set up a microphone and have it show the amplitude on the screen or something ) . similarly , you can go to lower frequencies ( and show the transition from invisibly fast vibrations but audible sounds to visible vibrations and inaudible sounds in a string instrument or possibly a rubber band ) . it is a good opportunity to explain how a dog whistle works , too . the concept of there being light that we can not see and sound that we can not hear is a really amazing one when one hears of it first . i certainly was intrigued by it when i learned about this as a child . 1 . this may not be so easy and may not be desirable , see cleonis ' comment below
if i were you , i would use a different k , having units of 1/time . so i would first say velocity falls in a very simple way : $$v ( t ) = v_0 exp ( -kt ) $$ to see how much time $t_m$ it takes to reach a particular speed $v_m$ , you can solve for it in the above equation $$t_m = ln ( v_0/v_m ) /k$$ integrate $v ( t ) $ to see how far the puck moves : $$x ( t ) = ( v_0/k ) ( 1 - exp ( -kt ) ) $$ to see how the far the puck moves before it stops , just plug in infinity for $t$ . that gives you $ ( v_0/k ) $ . to estimate $k$ , just plot $x$ or $v$ against $t$ . draw a tangent to the curve at time 0 . the time where it intersects the asymptote is $1/k$ .
first of all , nantennas in general do not violate the second law of thermodynamics , so they are not perpetual motion machines of second kind . as long as the total entropy goes up , the second law is obeyed . in other variables , it really means that a part of the incoming heat has to heat the nantenna up but there may still be a lot of energy left for energy production , much like in any other heat engine . the wikipedia suggestion that natennas could violate the second law only referred to a particular application hypothesized by mr novack . if he could be cooling the room while getting energy out of it , and if the gadget to cool the room were not connected to any cooler heat bath , then it would indeed be a perpetual motion machine of second kind and it would be impossible . the reason why nature makes it impossible is kind of trivial . if the room has temperature $t$ , then the nantenna or " power plant " may only be kept at the same temperature $t$ if there is equilibrium . but if that is the case , the nantenna emits thermal radiation , too . so even if it absorbs some incoming radiation , it still radiates its own . they are balanced and the energy gain is zero . solar cells and " legitimate applications " of nantennas can only create energy because they work with incoming light whose " own " temperature is higher than the temperature of the solar cell or nantenna itself . for example , solar radiation has the temperature comparable to 5,500 celsius degrees . the solar cells are effectively heat engines operating between this high temperature and a much lower temperature of the ground . the same is really true about life on earth , too . the energy from the sun may be converted and is often converted to useful energy or work because the high-energy photons from the sun – which correspond to a high temperature and therefore a low entropy per unit energy ( $e\sim ts$ ) – are processed on earth and the energy is finally emitted in much lower-temperature " infrared " thermal photons – which carry a higher entropy . so the entropy can go up even if a part of the incoming energy is converted to useful work . the temperature inequality between the solar surface ( and the solar radiation ) on one hand and the cool temperature of the outer space is necessary for the sun to play this often praised beneficial role .
matrix mechanics is quantum physics without the schroedinger equation ; the dynamics is instead defined by the heisenberg equation on the observables . matrix mechanics is not very popular nowadays , but you find everything you need ( exept for background in linear algebra and calculus , which are assumed ) in my book classical and quantum mechanics via lie algebras .
christian huygens discovered in 1690 polarized light - this is the first quantum effect ever observed . the transformation behavior of rays of completely polarized light was ﬁrst described by etienne-louis malus 1809 ( who coined the name ”polarization” ) , and that of partially polarized light by george stokes 1852 . in modern terminology , the behavior described by malus ( resp . stokes ) is identical to that of a qubit in a pure ( resp . mixed ) state . stokes 1852 paper contains all modern quantum phenomena for a single qubit , discussed in classical terms . ( for details , see my lecture http://www.mat.univie.ac.at/~neum/ms/optslides.pdf ) the transverse nature of polarization was discovered by augustin fresnel 1866 , and the description in terms of ( what is now called ) the bloch sphere by henri poincare 1892 . in modern terminology , polarization is a manifestation of the massless spin 1 nature of the unitary representation of the poincare group deﬁning photons . the second oldest observed quantum effect are spectral lines , apparently first discussed in 1802 by william hyde wollaston . ( for the history of spectroscopy , see http://www.spectroscopyonline.com/spectroscopy/article/articledetail.jsp?id=381944 ) both phenomena require quantum physics for their explanation ( though polarization can also be explained by a statistical version of classical electrodynamics ) . but , of course , before 1900 nobody considered these to be quantum effects . spectral lines were first described as a quantum effect in 1913 by niels bohr . polarization was first described as a quantum effect in 1930 by norbert wiener .
a stationary uncharged black hole is described the the schwarzschild metric : $$ ds^2 = -\left ( 1-\frac{2gm}{c^2r}\right ) dt^2 + \frac{dr^2}{\left ( 1-\frac{2gm}{c^2r}\right ) } + r^2 ( d\theta^2 + sin^2\theta d\phi^2 ) $$ the event horizon is at $r = 2gm/c^2$ , where the $dr^2$ term goes to infinity , so it is a surface of constant $r$ i.e. it is indeed a sphere . your plug hole analogy comes from seeing 2d representations of the black hole geometry in text books . this is only intentded as an analogy and is somewhat misleading . the metric tells you what the geometry actually looks like . a stationary but charged black hole actually has two event horizons , and both are spherical . the rotating black hole also has two event horizons . the outer is an oblate spheroid : i would have to go away and look up the shape of the inner . i do not know of any system that would have an event horizon shaped like a cylinder , though i would not rule out the possibility that a suitable shaped system might have an event horizon shaped like an infinitely long cylinder i.e. with no ends .
if you take the simplest form of capacitor , two parallel plates , the the capacitance is proportional to the area of the plates and inversely proportional to the distance between the plates : $$c \propto \frac{a}{d}$$ when you are making a capacitor out of a snapple bottle you are actually making something similar to the simple " two plate " capacitor but the " plates " are curved round the surface of the bottle , with the foil as the external plate and the ( conducting ) salt water in the bottle acting as the internal plate . the $d$ in the equation above is the thickness of the glass . so you can make a capacitor out of any bottle , jar or anything similar . all that matters is the area of the foil and the thickness of the glass . if you want to increase the total capacitance you can just link any number of bottles in parallel i.e. link the external foil covers as one electrode and the internal brine solution as the other electrode . when you join up capacitors in this way , " in parallel " , you get the total capacitance just by adding up the individual capacitances of all the bottles you have joined . you ask about the effect of the increased voltage : the charge stored in a capacitor is given by : $$q = cv$$ where $c$ is the capacitance and $v$ the voltage , so using 12kv instead of 9kv just means you get 33% more charge for a given capacitance , or alternatively you can get away with a smaller capacitance to hold the same charge . do i sound a bit like a grandma if i point out you need to be careful with this experiment . a typical tesla coil can not generate enough current to kill you , but if you gang together enough capacitors the stored charge in them will kill you ! finally , i normally point people to wikipedia if they want to learn more , so see http://en.wikipedia.org/wiki/capacitor and http://en.wikipedia.org/wiki/leyden_jar for the sort of capacitor you are making . however be warned that the wikipedia article on the capacitor is a bit technical .
yes it is redundant . this is exactly what ads/cft is not . the degrees of freedom of the bulk are the degrees of freedom of the horizon . this is also why condensed matter analogs are rare--- the most common idea of identifying ads/cft boundary theories with condensed matter boundary theories is wrong , because in tranditional condensed matter systems , the boundary degrees of freedom are in addition to the bulk degrees of freedom , they are not dual to these degrees of freedom , as in ads/cft . the exception , where the condensed matter analog is right , is where the bulk theory is topological , like the chern-simons theory for the quantum hall fluid , where you can consider edge-states as describing interior physics . there might be more analogs of this sort . one has to be careful , because a lot of people have this wrong picture of ads/cft in the head , that it is boundary stuff in addition to bulk stuff .
the water will probably turn into plasma really fast . i am too lazy here to look up exactly how to figure this out but i can provide an outline to answering your question : ( 1 ) figure out how much energy is needed to split water into its atomic constituents . a ) figure out how much energy is needed to pull two hydrogens off of oxygen . then see if 2000 degrees c is more or less energy using the temperature to energy relationship , e = 0.5*kt , ( e is average kinetic energy of particles in question , k is boltzmann 's constant , and t is temperature in kelvins . ) note . this will give an approximate value of the energy in terms of temperature . if you have more than enough energy to do this , then use the remaining energy you have left and go onto part b . b ) figure out how much energy is needed to ionize an electron from hydrogen . multiply that by 2 , because you have two hydrogen atoms . then figure out how much energy is needed to ionize all of oxygen ( thats a lot of freaking energy ) . then again , see how much energy you have left . if you still have more than enough energy , then move onto part c . c ) so by now you have 2 protons , 2 electrons from the 2 hydrogens , and an oxygen nucleus and its how ever many electron . you can then figure out how much energy is needed to break apart the oxygen nucleus into free protons and neutrons , ( this is a lot of freaking energy , and i do not know how to calculate this . . . good luck finding that . ) if you still have more than enough energy , then move onto part d . d ) figure out how much energy is needed to split protons into quarks , lol . and see if you still have enough energy left . this is kinda overkill . again , do not know how to do this . sorry . so if you figured out how much energy is needed to break one water molecule into whatever part you get to , b , c , or d . multiply that by the number of water molecules you have to figure out the total amount of energy needed . so in conclusion , its just simply comparing how much energy is needed to break apart water molecules and seeing what it turns into .
i kept wondering about the same question for quite a time , it makes sense to me now it is true that he has first ionization potential ( or energy ) of 24.6 ev while o2 has a value of 12.6 ev for the same number . yet experimentally igniting he discharge is much easier than igniting o2 discharge in dbd mode . the reason in simple words is the mean free path . think of having two identical discharges one with he as operating gas and one with o2 . assume the local electric field is identical and the gas density is identical , the mean free path of electrons in helium is much longer than what it is in o2 , which basically means that the electrons are accelerated by electric field to higher velocities ( energies ) in he compared to o2 before they experience a collision . so the electrons in he have larger energy than they have in o2 under similar circumstances . the difference in energy gain overcomes the difference in ionization energy . if you wanted to test it your self , you can use a freely available software called bolsig+ . what this software basically does is computing the electron energy distribution function ( eedf ) given the cross section data of the gas ( which is experimentally obtained data ) . for the same electric field to gas density ratio , the mean energy of an electron in he gas is much larger than what it is in o2 . i did the following plot of mean electron energy as function of reduced electric field in both air and helium . the reduced electric field is the electric field devided by number density . its unit is townsend so for example at 300 td , mean electron energy of an electron in helium is higher than the first ionization energy of helium , while in air it is lower than first ionization energy of either o2 or n2 . the reason the mean free path differs significantly is that n2 and o2 are more chemically active compared to he , which means the electrons have too many possible ways of spending their energy in rotational and vibration excitation , dissociation and excitation to metastable states while in he those ways are very limited . also it is true that the mass of a helium atom is much smaller than the mass of o2 or n2 molecule , being the basic unit in the gas . so from a rough geometrical perspective , the he atoms are smaller in size than o2 or n2 molecules . the geometrical size is not really relevant but it helps to explain the concept . i hope that made it clear .
the basis of the hilbert space in schrödinger 's picture is assumed to be time-independent regardless of any properties of the hamiltonian . the hamiltonian is just another operator . if the hamiltonian is time-dependent , its eigenstates and eigenvalues are obviously time-dependent , too . both equations you write down only express the fact that the basis of eigenstates of $h ( t ) $ is still a basis , so a general ket vector , including the actual state vector of the system , may be expanded as a linear superposition of these basic vectors with some general complex coefficients $c_n ( t ) $ . the two expansions only differ by the phase one includes into the coefficients $c_n ( t ) $ or into the basis vectors $|n ; t\rangle$ . one convention includes the phase $\exp ( i\theta_n ( t ) ) $ , another one does not , and so on . obviously , there is no " universally mandatory " rule that would dictate the right phase of these vectors so there is some freedom about the notation . note that a phase factor times an eigenstate is still an eigenstate . whatever your convention for the phases is , if you carefully follow the maths and remember what the symbols mean – the defining equations – you will be able to derive the invariant claims about the adiabatic theorem . the wikipedia-sakurai conventions treat the phases wisely and naturally , to speed up the derivations .
the riemann tensor encapsulates all information about the 4-dimensional space-time . this information can generally divided into two sectors : information about the curvature of space-time due to the existence of matter . this is given by the ricci tensor according to the einstein equation $$ r_{\mu\nu} - \frac{1}{2} g_{\mu\nu} r = 8 \pi g t_{\mu\nu} $$ information about the structure of gravitational waves in the space-time . this is given by the trace-free part of the riemann tensor , namely the weyl tensor . often , we are not quite interested in the exact structure of the space-time , but only if gravitational waves can exist or their structure . in these cases , one studies the weyl tensor rather than the ricci tensor . for example , in the setup of quantum gravity , one requires to study the asymptotic structure of spacetime . in these theories , a good understanding of the weyl tensor is more important .
$\partial_t\equiv\frac\partial{\partial t}$ and $\partial^\mu\equiv g^{\mu\nu}\frac\partial{\partial x^\nu}=\left ( \sum_{\nu=0}^3g^{\mu\nu}\frac\partial{\partial x^\nu}\right ) _{\mu=0}^3$ are differential operators . $\partial^\mu$ is formally contravariant ( upper index ) and obeys the corresponding transformation laws . $\partial_t$ has a lower index and is ( up to a constant factor ) a component of the formally covariant operator $\partial_\mu$ via $\partial_0=\frac1c\partial_t$ , which , in general , is not equal to $\partial^0$ , the zeroth component of $\partial^\mu$ . the differential operator $\partial^\mu$ is known as gradient , which derives vector fields from potential functions . the gradient is not a natural operation on arbitrary manifolds and only available if there is a metric . its dual $\partial_\mu\equiv\frac\partial{\partial x^\mu}$ on the other hand is a natural operation corresponding to the differential $\mathrm d$ , taking potentials to 1-forms ( covectorfields ) . as a side note , $\partial_t$ can also be understood as a local vector field , as one of the intrinsic definitions of vectors on manifolds is via their directional derivatives . in mathematical literature , it is common to write the basis of the tangent space as $\{\frac\partial{\partial x^\mu}\}$ and its dual space as $\{\mathrm dx^\mu\}$ .
i believe that the " roughly " term is applied because of the associated experimental error when measuring its charge . the same cannot be said to the electron because " we " decided to make the electron the reference charge . so , the reference charge is definitely -1 . however the muon charge must be measured . according to this paper , muon mass and charge by critical absorption of mesonic x rays . s . devons , g . gidal , l . m . lederman and g . shapiro . phys . rev . lett . 5 no . 7 , 330–332 ( 1960 ) . the measured relative charge of a muon is $e_\mu / e_e = 1 \pm 10^{-5}$
i will translate your post into the language of translations . then i will answer this question about translations . then i will answer your original question . translation question i am confused about a trivial concept . let the displacement of a rigid body be described by the equation $\vec{x} ( t ) =\delta \vec{x} ( t ) +\vec{x} ( 0 ) $ , with $\delta \vec{x} ( 0 ) =0$ . then , at each instant there is only one unit vector in the direction of displacement that we may call $\hat{w} ( t ) $ and which we may take to be normalized . that vector $\hat{w} ( t ) $ is what geometrically we would call the ( instantaneous ) direction of velocity [ note : this sentence is wrong ] . kinematically , however , the instantaneous direction of velocity $\vec{v}$ is the derivative $\dot{\vec{x}} ( t ) =\dot{\delta \vec{x}} ( t ) $ . that is the direction of $\vec{v} ( t ) $ . as is obvious ( for example an object not moving in a straight line ) , in general $\hat{w} ( t ) $ and $\vec{v} ( t ) $ are not parallel . so , why are there two directions for velocity , and does $\hat{w} ( t ) $ play any role in the kinematics/dynamics of the motion ? answer to translation question you are wrong that $\hat{w}$ is the direction of velocity . $\hat{w}$ was defined as the direction of $\delta \vec{x}$ , that is , the direction of the displacement . as kevin said the total displacement is not needed because you only need to know a objects current position and velocity to get its motion in the future . answer to the rotation question now we are ready to answer the rotation question . we just translate the answer of the translation question to rotation language . you are wrong that $\vec{v}$ is the direction of angular velocity . $\vec{v}$ was defined as the axis of rotation for $r$ , that is , the direction of the rotation between the initial and final orientation . as kevin said this rotation is not needed because you only need to know a objects current position and velocity ( here we are talking about the velocity everywhere in the object , which for a rigid object can be summarized by a linear velocity and an angular velocity ) to get its motion in the future .
the spiral arms do not mean that the mass is getting sucked to the center . they are just wave-like density patterns . the bodies in orbit around the center of the galaxy are in stable orbit ; just like the earth around the sun and the moon around the earth . what happens is that gravity accounts for the centripetal force ( in the orbiting frame , gravity is balanced by the centrifugal force ) , so there is no net radial acceleration " left over " to suck the body in . the only reason things would fall into the center is if they were headed there . this can happen if two stars pass by each other and are slingshotted in opposite directions , one of which gets sent to the center of the galaxy .
provided that $\mathcal{l}$ is a lorentz scalar , the quantity $\partial\mathcal{l}/\partial ( \partial_{\mu}\phi ) $ has to carry an upper index . since $\mathcal{l}$ is a function of $\phi$ and $\partial_{\mu}\phi$ , the only object that can give such an index is $\partial^{\mu}\phi$ . hence \begin{equation} \frac{\partial\mathcal{l}}{\partial ( \partial_{\mu}\phi ) } \propto \partial^{\mu}\phi . \end{equation} then , \begin{equation} \begin{split} \frac{\partial\mathcal{l}}{\partial ( \partial_{\mu}\phi ) } \omega^{\sigma}{}_{\mu}\partial_{\sigma}\phi \ , and \propto \ , \omega^{\sigma}{}_{\mu}\ , \partial_{\sigma}\phi \ , \partial^{\mu}\phi\\ and =\omega^{\sigma\mu} \partial_{\sigma}\phi\ , \partial_{\mu}\phi\\ and =0 . \end{split} \end{equation} the last expression vanishes because $\partial_{\sigma}\phi\ , \partial_{\mu}\phi$ is symmetric under the interchange of indices while $\omega^{\sigma\mu}$ is antisymmetric . i actually do not understand why tong did not simply write \begin{equation} \delta \mathcal{l} = -\omega^{\mu}{}_{\nu} x^{\nu}\partial_{\mu}\mathcal{l} . \end{equation} after all , $\mathcal{l}$ should have the same transformation rule as $\phi$ because they are both lorentz scalars . one can verify the above equation by noting that \begin{equation} \delta \mathcal{l} = - \partial_{\mu} ( \omega^{\mu}{}_{\nu}x^{\nu}\mathcal{l} ) = -\omega^{\mu}{}_{\nu} x^{\nu}\partial_{\mu}\mathcal{l} - \omega^{\mu}{}_{\mu}\mathcal{l} , \end{equation} and that \begin{equation} \omega^{\mu}{}_{\mu} = \eta_{\mu\rho}\omega^{\mu\rho} = 0 \end{equation} because $\eta_{\mu\rho}$ is symmetric and $\omega^{\mu\rho}$ is antisymmetric .
comment to the question ( v1 ) : consider the composed function $$s ( a ) ~:=~ s [ x_a ] , $$ where $$x_a ( t ) ~=~ x_0 ( t ) + a \beta ( t ) , $$ for fixed $\beta$ . it should be stressed that the function $a\mapsto s ( a ) $ is not necessarily independent of $a$ , or equivalently , the derivative $s^{\prime} ( a ) $ is not necessarily zero for all $a$ , even if $x_0 ( t ) $ is a stationary path . however , if $x_0 ( t ) $ is a stationary path , then $s^{\prime} ( 0 ) =0$ by definition . the full derivation of euler-lagrange equations from the stationary action principle is done in many textbooks and websites , e.g. wikipedia . for more information , see also e.g. this phys . se post .
if the charge in the capacitor is large enough , you will get a nice little shock:- ) , as the capacitor will discharge through you . i remember grabbing a rectifying valve disconnected from the mains a long time ago - that was not pretty ( apparently , it contained a capacitor ) . i was " clever " enough to grab it again:- )
ok , so there are quite a few parts to your question - i hope that i address most of them to your satisfaction . the author of the notes is essentially using the background field method ( bfm ) to calculate the effective action . ⁰ many of his choices follow from his focus on calculating only the one-loop effective potential . the background field method works in all theories with scalar , gauge , fermions , gravity , superfields¹ etc . . . it also works at all loops , however at higher loops , diagrams are still helpful for organising the calculations . two of the seminal papers did not use diagramatics : schwinger 's on gauge invariance and vacuum polarization and the follow-up two-loop calculation of ritus . however , since the bfm perturbation theory still uses a propagator-interaction separation , diagrammatics are only natural . one advantage of using bfm calculations is that you only need to calculate " vacuum " diagrams , i.e. , those with no external legs . this makes the diagrammatics and combinatorics easier . the other advantage is that the calculations become gauge covariant . the trade-off is the use of more complicated propagators . it is most often used to calculate low energy corrections ( like the effective potential for scalar fields ) by keeping only the lower terms in the derivative/momentum expansion . in particular , to find the effective potential , you only need constant background scalar fields . to find the kinetic terms , you need fields with at most two derivatives . however , it can also be used to construct perturbation theories that are very similar to the standard feynman diagram calculations , but are explicitly covariant under gauge transformations . good examples of this are improved methods for supergraphs and new , improved supergraphs . a really nice three-loop calculation of the yang-mills beta-function using the covariant background field method is hep-th/0211246 . they step through the set-up of the calculation quite slowly , so it is a good paper to learn from . the bfm relies on the following idea , see , e.g. , abbott 's the background field method beyond one loop , but the result can be extended to other theories with more complicated background-quantum splittings : let $\gamma [ v ] $ be the effective action ( legendre transform of the connected generating function $w [ j ] $ ) where $v=v ( j ) =\frac{\delta w [ j ] }{\delta j}$ is the " classical " field generated by the sources $j$ . if we modify the classical action by splitting the quantum fields into quantum + background , then the resultant modified effective action $\gamma [ v ( j ) , v ] $ now depends on both $v ( j ) $ and the background fields $v$ . you can show ( under reasonable assumptions ) that $\gamma [ 0 , v ] = \gamma [ v ] $ . the effective action is gauge dependent , ² and the previous result is true in the background field gauge . in the notes you linked to , the beta function was found from the quartic , derivative free correction . it can also be found from the gauge kinetic term . this works , because the method is background gauge covariant , which forces the gauge coupling and field renormalisations to be related . in fact , the background gauge potential field never needs to leave the covariant derivative and the beta function could be found from the invariant $\mathrm{tr} ( f_{\mu\nu}f^{\mu\nu} ) $ term . this was done in the original paper by schwinger . see abbott for more of a discussion on this point . also read about the schwinger-dewitt expansion for how to control some covariant expansions in effective action calculations . the classic paper is the physics report by barvinsky and vilkovisky . see avramindi and kuzenko and mcarthur for more info on covariant methods . for a good 2-loop bfm calculation involving fermionic backgrounds , see jack and osborn . although they do not calculate the finite parts of the propagators . low energy calculations with fermion backgrounds can be quite tricky , which also makes some supersymmetric bfm low-energy calculations tricky . ⁰ that said , the notes do collect some nice arguments about the structure of the effective action and effective potential . ¹ although , in some theories ( e . g . , n=1 supersymmetry ) , the quantum-background splitting has to be nonlinear . ² a gauge independent version of the effective potential does exist - but it seems to be not very practical to calculate or use - at least , it is not used much . see , e.g. , vilkovisky or becchi . . .
the energy you seem to refer to is the electric part of the poynting energy expression for some volume $v$: $$ e_{\text{poynting}} ( t ) = \int_v \frac{1}{2}\epsilon_0 \left|\mathbf e ( \mathbf x , t ) \right|^2 + \frac{1}{2\mu_0}\left|\mathbf b ( \mathbf x , t ) \right|^2 \ , d^3\mathbf x . $$ the vector $\mathbf e ( \mathbf x , t ) $ in this expression is the electric vector at position $x$ at time $t$ . there is no integration over time in this expression . if you want to express this electric part of energy with help of the fourier amplitude $\tilde{\mathbf e} ( \mathbf x , \omega ) $ defined by $$ \mathbf e ( \mathbf x , t ) = \int_{-\infty}^{\infty}\tilde{\mathbf e} ( \mathbf x , \omega ) e^{i\omega t} \frac{d\omega}{2\pi} , $$ you can simply substitute in the above expression : $$ e_{electric} ( t ) = \int_v \frac{1}{2}\epsilon_0 \left|\int_{-\infty}^{\infty}\tilde{\mathbf e} ( \mathbf x , \omega ) e^{i\omega t} \frac{d\omega}{2\pi}\right|^2 d^3\mathbf x . $$ energy is a function of $t$ only , and you can try to find a formula for its frequency dependent fourier components $e_{electric} ( \omega ) $ by calculating ft of the last expression with respect to time $t$ .
yes , this equations applies to all waves . . . with the caveat that you replace c by the speed of the wave you are studying ! in a water wave , the product of the wavelength and the frequency will be the speed of the water wave , not of light . for sound waves in air , it will be the speed of sound , etc . because of this , the general form of the equation you provided is : $$\lambda \nu ~=~ v_{wave} . $$ another interesting thing is that the speed of the wave need not be constant . the equation is always valid , but it might be possible that the wavelength depends on the frequency , in which case the speed will also depend on the frequency . this happens with water waves ; you can notice not all waves travel at the same speed on the ocean . it also happens with light ; light of different colours travel at different speeds through glass , which is what allows a prism to disperse white light into a rainbow . when the wavelength depends on the frequency , we call it " dispersion " . if it does not , then the wave speed is the same for all waves of that type .
they would be linearly dependent if and only if there exist complex numbers $\alpha$ and $\beta$ such that $\alpha x_{1} ( t ) + \beta x_{2} ( t ) = 0 \forall t$ clearly , if $\omega_{0}=0$ then this is the case for $\alpha = 1$ and $\beta = -c_{1}/c_{2}$ . so then they are linearly dependent . however , if $\omega_{0}\neq0$ , you can not find a combination of $\alpha$ and $\beta$ that fulfills this requirement for all $t$ . the importance lies in the fact that ( 1 ) any linear combination $\gamma x_{1} ( t ) + \delta x_{2} ( t ) $ of the two functions is also a solution . ( just plug the linear combination into the equation to see this . ) ( 2 ) these are the only solutions . namely , if you would find a solution $y ( t ) $ you could always write it as a combination of $x_{1}$ and $x_{2}$ . so these are the only solution to care about , all the dynamics of the system is contained in them .
there is nothing wrong with your calculations . from the wikipedia article on supermassive black holes : " the average density of a supermassive black hole ( defined as the mass of the black hole divided by the volume within its schwarzschild radius ) can be less than the density of water in the case of some supermassive black holes " given that black hole masses scale with linear size , while objects we encounter in daily lives have a mass proportional to the cube of their linear size , makes it inevitable that beyond a certain size black holes are characterized by mass densities that we label as ' small ' . in other words : when growing an object while keeping it is mass density fixed , there is a maximum to how far you can grow such an object . beyond a certain size , the object acquires a gravitational horizon that starts expanding proportional to the object 's mass , thereby reducing the object 's mass density .
the solution for this problem for a dust equation of state and spherical symmetry is known as the oppenheimer-snyder solution . you model the interior of the distribution as a frw universe with positive spatial curvature , zero pressure , and zero cosmological constant . you model the exterior of the solution as the schwarzschild solution cut off at a time-dependent radius . so long as the matter distribution is dust , the thing satisfies all of the junction conditions you need . see poisson 's relativity book or mtw . a more general solution requires numerics . but one thing we can say for sure is that there is no need for the black hole to shed its ' hair ' in the case of spherical symmetry--the radial dependence of the solution will just compress into the singularity eventually , or scatter out to infinity . birchoff 's theorem tells us every spherically symmetric vacuum solution must be the schwarzschild solution ( perhaps with an electrostatic charge , which is technically not vacuum ) . this is related to the fact that there can be no monopole radiation in relativity . also , the general case for this problem is very likely chaotic . already , if the equation of state of the matter is that of a classical , spherically symmetric , klein-gordon field , which is a relatively simple generalization , the system exhibits a ( link is a large postscript file ) second-order phase transition , a result found by matt choptuik , and related to the settling of the hawking naked singularity bet .
here we are : such a pendulum was developed centurys ago : http://myreckonings.com/wordpress/2007/11/19/the-not-so-simple-pendulum/ the trajectory is a cycloid , a possible realisation is a kind of guide in form of the evolent of that cycloid close to the fixation point of the pendulum . afair , a similar cycloid is the fastest trajectory for a body ( on a rail ) in free fall .
i will here only comment on the traditional superstring theory story , say , from the first superstring revolution in the 1980s , and leave it to others to include more recent developments . traditionally , the $10$-dimensional target space $ ( m^{10} , g^{ ( 10 ) } ) $ with a metric $g^{ ( 10 ) }$ is viewed as a product $m^{10}=m^4 \times k^6$ with metric $g^{ ( 10 ) }=g^{ ( 4 ) }\oplus g^{ ( 6 ) }$ , where $ ( m^4 , g^{ ( 4 ) } ) $ is the $4$-dimensional spacetime with a $4$-metric $g^{ ( 4 ) }$ , which we see and observe ; and $ ( k^6 , g^{ ( 6 ) } ) $ is a compact $6$-dimensional riemannian manifold , whose characteristic length scales are so small that it has avoided experimental detection so far . i will assume that the word clustering in the question ( v2 ) essentially refers to if $ ( k^6 , g^{ ( 6 ) } ) $ could be a product $k^6=k^3\times l^3$ with metric $g^{ ( 6 ) }=g^{ ( 3 ) }\oplus h^{ ( 3 ) }$ of two $3$-dimensional manifolds $ ( k^3 , g^{ ( 3 ) } ) $ and $ ( l^3 , h^{ ( 3 ) } ) $ ? again , to have avoided experimental detection , the two $3$-dimensional manifolds $k^3$ and $l^3$ must both be compact . now , another bit of traditional string wisdom is , that to have unbroken $n=1$ supersymmetry i $4$ dimensions , the holonomy group of $ ( k^6 , g^{ ( 6 ) } ) $ must be the $8$-dimensional lie group $su ( 3 ) $ , see e.g. , green , schwarz and witten , " superstring theory " , chap . 15 . see also this question . on the other hand , the biggest holonomy group that a $3$-dimensional riemannian manifold can have , is the 3-dimensional lie group $o ( 3 ) $ , so $k^6=k^3\times l^3$ can at most have holonomy group $o ( 3 ) \times o ( 3 ) $ , which is $6$-dimensional , and therefore too small to be $su ( 3 ) $ . hence a product manifold $k^6=k^3\times l^3$ is ruled out .
so you want to know how much water a certain surface adsorbs . this is really dependent on the surface material/conditions . check adsorption and relative humidity on wikipedia . to where i have analyzed , it seems that there is about enough information in the two articles . i am not a specialist on the subject so i might be missing some important factor .
approximately $10^{15}$ . see this : photon flux of 540 nm light from the mechanical equivalent of light and the integrated spectral sensitivity of the human eye : $3.8&#215 ; 10^{15}\ photons/s$ ( photons per second ) $6.3&#215 ; 10^{-9}\ mol/s$ ( moles of photons per second ) also see this reference . note : this summarises robert 's answer in the question comments and is set to cw .
conservation of energy follows from invariance under translation in time , not inversion . this symmetry states that no matter when you do your experiment , it will give the same results . all isolated systems obey this symmetry ( and therefore conserve energy ) and no violation of it has ever been detected . ( needless to say , it would be a huge event if it were . ) in classical physics , only continuous symmetries - that is , symmetries that can be continuously connected to the identity transformation - have a corresponding conservation law . quantum physics does permit conservation laws for discrete symmetries but these laws are far harder to visualize . an example of this is conservation of parity , $p$ , which corresponds to invariance under inversion in space , and which gives the parity - even or odd - of wavefunctions . temporal inversion , $t$ , is even harder to turn into a physical quantity because it requires a full relativistic treatment in which time is a coordinate like space and not a parameter ( as it is in non-relativistic quantum mechanics ) . a third discrete symmetry is charge conjugation , $c$ , which exchanges particles for their antiparticles . it turns out that any consistent field theory must be invariant under all three operations when taken together - i.e. under $cpt$ . thus violation of parity - an experiment and its mirror image behaving differently - is possible , for example , if it comes together with violation of $c$ - i.e. the mirror experiment behaves like the original one if it is made of antimatter - , as was discovered in the sixties . violations of $c$ and $p$ together have also been discovered in recent years , which means that in some situations violations of $t$ must occur . the recent $b$-meson experiments confirm this . since the $t$ symmetry does not correspond to energy but to a far more abstract quantity ( which is not conserved ) , this does not lead to a nonconservation of energy .
generally speaking , when people talk about the universe having a beginning they are talking about the solution to the einstein equations called the flrw metric . this describes a universe that is homogenous and isotropic , and it seems to be a pretty good approximation to our universe . the flrw metric tells us how the density of the universe changes with time . nb it does not give the size of the universe . a homogenous isotropic universe is infinite in size by definition , and it remains infinite in size all the way back to the big bang . anyhow , if you wind time back to the big bang you find the density of the universe becomes infinite and the distance between any two points in the universe falls to zero , even though the universe remains infinite in size . this nonsensical result is why you will often hear it said that gr can not explain the big bang , and it is why we expect some theory of quantum gravity to take over and prevent the density becoming infinite . anyhow , the point of all this ranting is that by definition the flrw universe is the same everywhere . so if you believe the flrw metric is a good approximation to our universe ( and it seems to be ) then the entire universe behaved the same as the bit we can see . the entire universe , including our bit , had a beginning . however , this is a strictly classical perspective and does not describe phenomena like inflation that originate from quantum mechanics . the original theory of inflation did not change the overall behaviour much , it just introduced a period of exponential expansion shortly after the big bang . but there is a more recent theory called eternal inflation that drastically changes our view of the universe 's beginning . in eternal inflation the universe has been inflating for all time and will continue to inflate for all time , so the universe as a whole does not have a beginning . however areas within the universe stop inflating and turn into the sort of slowly expanding spacetime that we see around us . these areas then appear to have a big bang , but this is simply the point at which inflation stopped for that particular area . other areas would have big bangs that happened at a different time , while the universe as a whole had no beginning . so the answer to your question is yes and no depending on which theory you believe . unfortunately at the moment there is no experimental evidence to prove which answer is correct .
you are talking about a year period , right ? there is kepler 's third law : $$ {s_1^3 \over s_2^3} = {p_1^2 \over p_2^2} . $$ where s - is the distances between the ' center ' and a planet p - orbital period of the planet thus the further the planet is from the center ( sun ) , the longer its orbital period is . you can check that formula by taking examples of earth ( 150million kilometers and 365days ) and mercury ( 58million kilometers , and you can calculate the length of its year ) .
i do not think anyone here has really answered your question . in this case , the sound is " focused " using phased arrays . the face of the audio spotlight has multiple transducers : flickr the same signal is output from each of them , but delayed slightly by different amounts , so that the wavefronts all reach the same point in front of the device at the same time . this " virtual focus " is called beamforming . ref ref this is how modern radars focus their beams , too . instead of spinning a satellite dish around , they have lots of little elements that do not move , but the signals are delayed to produce different beam shapes .
your problem is deeper than it might seem at the first glance . apart from numbers , it involves a remarkable phenomenon , which was studied in detail only a few years ago in the paper phys . rev . lett . 90 , 248302 ( 2003 ) . suppose the puck is an infinitely thin disc , and it is sent across the ice sliding and rotating . what will stop first , translational or rotational motion ? the answer is they will stop simultaneously , and this does not depend on the initial translational and angular velocities . the origin is the intrinsic friction-mediated coupling between rotation and sliding . if rotation is too fast for a given speed $v$ , the translational friction will be very small . and vice versa , if rotation is too slow , the rotational deceleration is small . thus , there exists a " magic " value of $\epsilon=v/r\omega$ towards which every initial condition is attracted . in the paper cited above this magic value was found to be approximately 0.653 . this value fully characterizes the asymptotic motion of the puck and allows you to calculate the friction force distribution , deceleration and finally the path ( which is a straight line in this approximation ) . in fact , a part of that was already found in the paper cited above . however , this concerns only an infinitely thin disc , for which the pressure distribution is equal everywhere under the puck . with a thick disc you have more pressure under the front part of the puck , which additionally complicates the problem ( still , see some discussion in the paper ) . finally , if you change the shape of the pack ( as you in fact did by taking a ring instead of a disc ) , the magic value of $\epsilon$ also changes . see another paper that discusses this : phys . rev . lett . 95 , 264303 ( 2005 ) . see also this comment to this old paper on puck motion on the ice .
with the given $m$ and $k$ you indeed cannot calculate the damping coefficient $c$ . remember that you just have a model where you put some constants in and you can only derive other constants which somehow depend on them . the question concerning an actual measurement was answered in investigating the dampening of a spring . greets
the answer is that it depends on what you mean by pressure . to understand this imagine you have some gas in a canister at some pressure $p$ , and you look at the inside walls of the canister with a microscope so powerful that you can see the gas molecules whizzing about and bouncing off the walls . one of collisions would look like this . the gas molecules have a mass $m$ and some average velocity $v$ so they have a momentum $mv$ . when one molecule rebounds from the wall the change of momentum is $2mv$ , but the rate of change of momentum is just the force exerted on the wall . so if the number of collisions with the wall per second per unit area is $n$ , then the pressure on the wall will be $p = 2nmv$ ( not quite , because not all collisions are at right angles , but let 's skip over this ) . anyhow , the point is that pressure is caused by collisions with gas molecules . in a canister of gas ( like the room you are in ) the molecules are moving in all directions at random so the pressure they produce is the same everywhere . so for example the air pressure on you is the same all over your skin . but in your example of air escaping from a nozzle the air is not moving at random directions . instead it is escaping from the nozzle in something like a cone : suppose put a box in the path of the escaping gas , and we measure the pressures $p_1$ to $p_4$ on the four sides of the box . if the box were in a gas canister , as we talked about above , all four pressures would be the same because the gas molecule velocities are random so on average equal numbers of gas molecules per second would hit all four sides . but with the gas from the nozzle this clearly is not true because far more gas molecules will hit side 1 than sides 2 , 3 and 4 . the pressure $p_1$ would be much greater than $p_2$ and $p_3$ and $p_4$ would be close to zero . so this is why i started the answer with it depends . you can not simply define a single pressure in a gas stream from a nozzle because the pressure you measure will depend on how you orientate your pressure gauge . having said this , the obvious response is to ask what is the pressure $p_1$ . the answer is that the number of gas molecules per unit area will decrease as the square of the distance from the nozzle , so the pressure will be : $$ p_1 \propto \frac{1}{d^2} $$ where the constant of proportionality will be determined by the geometry of the gas flow from the nozzle .
well , when canonically quantizing a system with constraints , you have two methods : dirac 's approach " quantize , then constrain" ; reduced phase space approach " constrain , then quantize " . although these two approaches have analogs with path integral quantization , the path integral approach sweeps a lot of problems under the rug when you pick a particular gauge ( a la fadeev-poppov quantization ) . that is why the path integral approach is usually taught in quantum field theory courses : it is a straightforward recipe with few subtleties . the canonical approach requires a bit more work . i am aware , in quantum gravity at least , that you can recover the dirac quantized constraints from taking the functional derivative of the path integral with respect to the lagrange multipliers , and demanding it vanish . so i suspect there is a way to recover the dirac quantized version from the path integral approach . this is unique to general relativity , due to the inclusion of time . a formal derivation may be found in hartle and hawking 's " wave function of the universe " ( physical review d 28 12 ( 1983 ) pp . 2960–2975 eprint ) . halliwell and hartle 's " wave functions constructed from an invariant sum over histories satisfy constraints " ( phys . rev . d 43 ( 1991 ) pp . 1170–1194 eprint ) generalize this result for parametric systems . barvinsky shows in " solution of quantum dirac constraints via path integral " ( arxiv:hep-th/9711164 ) that the path integral directly solves the quantum constraints , for a generic first-class constrained system at the level of one-loop ( "semiclassical" ) approximations . klauder 's " path integrals , and classical and quantum constraints " eprint is a pedagogical review of quantizing constrained systems . you might want to look at henneaux and teiteilboim 's quantization of gauge systems , specifically chapter 16 . addendum the path integral using the faddeev-poppov method is completely equivalent to the following : suppose we want to change coordinates from just position $q$ to the gauge orbit $\lambda$ plus the physically meaningful position $\bar{q}$ . then the functional integral changes as $$\begin{align} \int \exp ( i [ q ] ) \mathcal{d}q and = \int \exp ( i [ \bar{q} ] ) \delta_{fp}\ , \mathcal{d}\bar{q}\mathcal{d}\lambda\\ and =\int\mathcal{d}\lambda\int\exp ( i [ \bar{q} ] ) \delta_{fp}\ , \mathcal{d}\bar{q} \end{align}$$ we have the $\int\mathcal{d}\lambda$ be infinite but trivial ( it is the volume of the gauge orbit ) . the $\delta_{fp}$ is the faddeev-poppov determinant . this approach is discussed in detail in emil mottola 's " functional integration over geometries " arxiv:hep-th/9502109 .
orders for differential equations refer to the largest number of derivatives that are taken with respect to the independent variable . in partial differential equations , there can be multiple independent variables , so you have to specify which one you are talking about -- hence the " in time " part . thus , first-order in time means just one derivative with respect to time . the classic example is the basic one-dimensional heat equation : \begin{equation} \frac{\partial \phi}{\partial t} = \frac{\partial^2 \phi}{\partial x^2}~ . \end{equation} you can see that this is second-order in space ( because $\partial^2 / \partial x^2$ is two spatial derivatives ) , but there is just one time derivative , so it is first-order in time . the heat equation is the classic example of dissipation because heat flows out irreversibly . this is related to the fact that this equation does not have time-reversal symmetry . that is , if you replace $t$ with $-t$ , this equation becomes \begin{equation} \frac{\partial \phi}{\partial t} = -\frac{\partial^2 \phi}{\partial x^2}~ , \end{equation} which is really a different equation because of that negative sign . second-order in time means two derivatives with respect to time . the classic example is the basic one-dimensional wave equation : \begin{equation} \frac{\partial^2 \phi}{\partial t^2} = \frac{\partial^2 \phi}{\partial x^2}~ . \end{equation} this equation does have time-reversal symmetry , because if you replace $t$ with $-t$ , you get the exact same equation back . and in particular , this equation is non-dissipative ; as the wave travels along , it does not lose any energy , and -- with the right boundary conditions -- it could even return to exactly its initial data . but the author said that second-order equations may be non-dissipative . an example of a second-order equation that is dissipative is the damped wave equation : \begin{equation} \frac{\partial^2 \phi}{\partial t^2} = -\frac{\partial \phi}{\partial t} + \frac{\partial^2 \phi}{\partial x^2}~ . \end{equation} note that this has both a single and a double derivative with respect to time , but it is still called second-order in time because order is defined as the highest number of derivatives . solutions to this equation are wave-like , except that the waves gradually lose energy , so there is dissipation . and you can see that this is not time-reversible because the equation changes if you flip the sign of $t$ , and becomes the " anti-damped " wave equation ( the waves grow in time ) .
you are confusing yourself with your own notation . instead of writing $dv$ all over the place , let 's keep it explicit as $dx^{1} dx^{2} dx^{3}$ and such . let $dx^1$ and so on be in cartesian and $dx^{1'}$ be for some other curvilinear coordinates . then you get $$dx^{1'} dx^{2'} dx^{3'} = ( \det j ) ^{-1} \ , dx^1 dx^2 dx^3$$ just move the determinant to the left and replace with $h$ 's . $$h_1 h_2 h_3 dx^{1'} dx^{2'} dx^{3'} = dx^1 dx^2 dx^3$$ and that is correct . there is some ambiguity in the notation because it is common in vector calculus to say $dv = \sqrt{|g|} dx^1 dx^2 dx^3$ , to treat $dv$ as an " invariant " quantity that merely has equivalent descriptions in various coordinate systems . for some reason this is not done in gr , so the meaning is slightly different . ( or perhaps this is why some prefer to say $d^4 x$ instead . )
if you put a schottky device into very-forward-bias , the iv curve becomes a straight line whose inverse slope is the resistance as usual . so you can still use tlm . of course , people do not normally bother to figure out contact resistance because the schottky aspect of the contact has a much much bigger effect on the device than the resistive aspect . for an ohmic contact that arises from tunneling through a schottky barrier , again , people do not normally bother to figure out the " barrier " height because the " barrier " is irrelevant for the electrical behavior of the device . but if you had to figure it out for some reason , the only method i know of is internal photoemission .
the nature of complex numbers in qm turned up in a recent discussion , and i got called a stupid hack for questioning their relevance . mainly for therapeutic reasons , i wrote up my take on the issue : on the role of complex numbers in quantum mechanics motivation it has been claimed that one of the defining characteristics that separate the quantum world from the classical one is the use of complex numbers . it is dogma , and there is some truth to it , but it is not the whole story : while complex numbers necessarily turn up as first-class citizen of the quantum world , i will argue that our old friend the reals should not be underestimated . a bird 's eye view of quantum mechanics in the algebraic formulation , we have a set of observables of a quantum system that comes with the structure of a real vector space . the states of our system can be realized as normalized positive ( thus necessarily real ) linear functionals on that space . in the wave-function formulation , the schrödinger equation is manifestly complex and acts on complex-valued functions . however , it is written in terms of ordinary partial derivatives of real variables and separates into two coupled real equations - the continuity equation for the probability amplitude and a hamilton-jacobi-type equation for the phase angle . the manifestly real model of 2-state quantum systems is well known . complex and real algebraic formulation let 's take a look at how we end up with complex numbers in the algebraic formulation : we complexify the space of observables and make it into a $c^*$-algebra . we then go ahead and represent it by linear operators on a complex hilbert space ( gns construction ) . pure states end up as complex rays , mixed ones as density operators . however , that is not the only way to do it : we can let the real space be real and endow it with the structure of a lie-jordan-algebra . we then go ahead and represent it by linear operators on a real hilbert space ( hilbert-schmidt construction ) . both pure and mixed states will end up as real rays . while the pure ones are necessarily unique , the mixed ones in general are not . the reason for complexity even in manifestly real formulations , the complex structure is still there , but in disguise : there is a 2-out-of-3 property connecting the unitary group $u ( n ) $ with the orthogonal group $o ( 2n ) $ , the symplectic group $sp ( 2n , \mathbb r ) $ and the complex general linear group $gl ( n , \mathbb c ) $: if two of the last three are present and compatible , you will get the third one for free . an example for this is the lie-bracket and jordan product : together with a compatibility condition , these are enough to reconstruct the associative product of the $c^*$-algebra . another instance of this is the kähler structure of the projective complex hilbert space taken as a real manifold , which is what you end up with when you remove the gauge freedom from your representation of pure states : it comes with a symplectic product which specifies the dynamics via hamiltonian vector fields , and a riemannian metric that gives you probabilities . make them compatible and you will get an implicitly-defined almost-complex structure . quantum mechanics is unitary , with the symplectic structure being responsible for the dynamics , the orthogonal structure being responsible for probabilities and the complex structure connecting these two . it can be realized on both real and complex spaces in reasonably natural ways , but all structure is necessarily present , even if not manifestly so . conclusion is the preference for complex spaces just a historical accident ? not really . the complex formulation is a simplification as structure gets pushed down into the scalars of our theory , and there is a certain elegance to unifying two real structures into a single complex one . on the other hand , one could argue that it does not make sense to mix structures responsible for distinct features of our theory ( dynamics and probabilities ) , or that introducing un-observables to our algebra is a design smell as preferably we should only use interior operations . while we will probably keep doing quantum mechanics in terms of complex realizations , one should keep in mind that the theory can be made manifestly real . this fact should not really surprise anyone who has taken the bird 's eye view instead of just looking throught the blinders of specific formalisms .
another very fresh paper presented at dark attack yesterday , one by hektor et al . , http://arxiv.org/abs/1207.4466 also claims that the signal is there – not only in the center of the milky way but also in other galactic clusters , at the same 130 gev energy . this 3+ sigma evidence from clusters is arguably very independent . all these hints and several additional papers of the sort look very intriguing . there are negative news , too . fermi has not confirmed the " discovery status " of the line yet . puzzles appear in detailed theoretical investigations , too . cohen at al . http://arxiv.org/abs/1207.0800 claim that they have excluded neutralino – the most widely believed identity of a wimp – as the source because the neutralino would lead to additional traces in the data because of processes involving other standard model particles and these traces seem to be absent . the wimp could be a different particle than the supersymmetric neutralino , of course . another paper also disfavors neutralino because it is claimed to require much higher cross sections than predicted by susy models : http://arxiv.org/abs/1207.4434 but one must be careful and realize that the status of the "5 sigma discovery " here is not analogous to the higgs because in the case of the higgs , the " canonical " null hypothesis without the higgs is well-defined and well-tested . in this case , the 130-gev-line-free hypothesis is much more murky . there may still exist astrophysical processes that tend to produce rather sharp peaks around 130 gev even though there are no particle species of this mass . i think and hope it is unlikely but it has not really been excluded . everyone who studies these things in detail may want to look at the list ( or contents ) of all papers referring to weniger 's original observation – it is currently 33 papers : http://inspirehep.net/search?ln=enp=refersto%3arecid%3a1110710
i am not an expert in 2d cft . however i hope following manipulations are valid . assume that your second equation follows from first one . then on rhs of your first equation taylor expansion of $o_2 ( w ) $ at point $z$ gives : $o_2 ( w ) =o_2 ( z ) + ( w-z ) \partial_z o_2 ( z ) + . . . $ taking derivative wrt $w$ on both sides we get $\partial_wo_2 ( w ) =\partial_zo_2 ( z ) + . . . $ using these two results in your first equation we get $o_1 ( z ) o_2 ( w ) = \displaystyle\frac{o_2 ( z ) }{ ( z-w ) ^2}+regular\:terms$ subtracting it from your second equation , multiplying with $ ( z-w ) ^2$ and taking limit $w\rightarrow z$ we conclude that $o_2$ and $o_1$ should be equal . since to begin with we did not assume any such thing regarding fields $o_2$ and $o_1$ so in general your second equation should not follow from the first one . i think equality of $o_2 ( w ) o_1 ( z ) $ and $o_1 ( z ) o_2 ( w ) $ ( assuming fields are ' bosonic' ) within time ordered product only implies that their ope should be symmetric under exchange of z and w . so if your first equation for ope can be realized for some ( bosonic ) fields , then by exchanging z with w on rhs you should get the same result within a regular term .
the acceleration equation needs to include force terms for air drag $f_a$ , and runner friction $f_f$ in addition to the gravity term $g \sin ( \theta ) $ where $\theta$ is the slope of the luge run and $g$ is gravitation . as singh pointed out , gravitation exerts a force proportional to mass , so the total acceleration is $$a = g \sin ( \theta ) - \frac{f_f + f_a}{m}$$ runner friction is more or less proportional to mass , so we can replace it with a constant , i.e. the mass of the rider is not a consideration , giving $$a = g \sin ( \theta ) - k - \frac{f_a}{m} $$ air drag depends on frontal surface area , $a$ and the square of velocity $v^2$ , so $$f_a = dav^2$$ where d is a drag coefficent to account for the rider 's aerodynamic smoothness ( or lack thereof ) . user11865 's answer points out that the rider 's surface area is proportional to $\sqrt{m}$ and this is what gives heavier riders an advantage , especially at higher speeds . let 's wrap the density and shape of the human body into a constant , say , $b$ , and the acceleration equation now looks like $$a = g \sin ( \theta ) - k - \frac{dbv^2}{\sqrt{m}}$$ the acceleration lost to air drag is the only term that depends on mass and having more mass makes it smaller .
the hamiltonian has the very legal definition that it is the legendre transform of the lagrangian function . so , in any physical case to find the hamiltonian of a system , you have to take $l = t - v$ and then perform the ritualistic legendre transform process shifting coordinates from $q , q'$ to $q , p$ . the time symmetry of the system leads to the conservation of the so called jacobian function and not the hamiltonian . the hamiltonian will however be the total mechanical energy if thesystem is conservative and in this case the jacobian function also equals the total mechanical energy . the jacobian is the mechanical one and not the mathematical one . ref goldstein .
disclaimer : i have no engineering background , so if anything i write is in error , definitely point it out . however , if the 3-axis accelerometer only returns the proper acceleration vector $\mathbf{a}$ , then if the object is moving around and physically accelerating , it is impossible to determine the orientation of the object without additional information . here is a formal counterexample . state 1: object at rest , roll is $\pi/4$ , pitch is 0 the object experiences a proper acceleration vector of $$\mathbf{a}=\left ( 0 , \frac{g}{\sqrt{2}} , \frac{g}{\sqrt{2}}\right ) . $$ state 2: object accelerating , roll is 0 , pitch is 0 with an inertial acceleration vector $\ddot{\mathbf{r}}=\left ( 0 , \frac{g}{\sqrt{2}} , -\frac{g}{\sqrt{2}}\right ) $ , the proper acceleration measured will be $$\mathbf{a}=\left ( 0,0 , g\right ) +\ddot{\mathbf{r}}=\left ( 0 , \frac{g}{\sqrt{2}} , \frac{g}{\sqrt{2}}\right ) . $$ since both state 1 and state 2 return the same acceleration vector but have different spatial orientations , the act of converting proper acceleration to orientation under the influence of an outside acceleration is not a one-to-one correspondence , and thus is underdetermined . in other words : how is the accelerometer supposed to tell the difference between state 1 and state 2 ? it can not , unless you have additional information .
i take it this still works : http://www.physicsforums.com/showthread.php?t=114165 .
your friends are correct . if there is no force in the left-right direction , then linear momentum will be conserved in that direction . because the new composite object has more mass than the original object , it will have a lower speed to the right . what about energy ? kinetic energy is not conserved in this case , because the collision is inelastic . the kinetic energy lost in the collision ( and the downward momentum lost , for that matter ) is absorbed by whatever wall is preventing the composite object from continuing to move downward .
in the book by p.g. de gennes superconductivity of metals and alloys , it is written $n ( 0 ) v &lt ; 0.3$ . also written : lead and mercury are two notable exceptions with low $\theta_{d} \left ( =\hslash \omega_{d}/k_{b} \right ) $ , giving , respectively , $n ( 0 ) v=0.39$ and $n ( 0 ) v=0.35$ . more details on p . 112 , p.g. de gennes superconductivity of metals and alloys , westview ( 1999 ) . the first edition dated back to 1966 . to my knowledge there is no change between the editions . see also the table 4-1 on p . 125 of the same book for several specific values for the pure metals . this table is reproduced below for commodity . $$\begin{array}{cccc} \mbox{metal} and \theta_{d}\left ( \mbox{k}\right ) and t_{c}\left ( \mbox{k}\right ) and n\left ( 0\right ) v\\ \mbox{zn} and 235 and 0.9 and 0.18\\ \mbox{cd} and 164 and 0.56 and 0.18\\ \mbox{hg} and 70 and 4.16 and 0.35\\ \mbox{al} and 365 and 1.2 and 0.18\\ \mbox{in} and 109 and 3.4 and 0.29\\ \mbox{tl} and 100 and 2.4 and 0.27\\ \mbox{sn} and 195 and 3.75 and 0.25\\ \mbox{pb} and 96 and 7.22 and 0.39 \end{array}$$ post-scriptum : there are ( surprisingly ! ) nothing about this question on the book by j.r. schrieffer , superconductivity , benjamin ( 1964 ) . there is not even a discussion on the gap equation as far as i can check . . . there is a repetition of the gennes data on the book by a.i. fetter and j.d. walecka , quantum theory of many-particle systems , dover publications ( 2003 , first edition 1971 ) , p . 448 . but this table is less complete . there is also no discussion about the numerical value in the original paper by bcs [ bardeen , j . , cooper , l . n . , and schrieffer , j . r . ; theory of superconductivity . physical review , 108 , 1175–1204 ( 1957 ) . http://dx.doi.org/10.1103/physrev.108.1175 -> free to read on the aps website ] , but there is a possibly interesting expression ( eq . ( 2.40 ) , written below in your notation / in the original bcs paper , the gap is written $\varepsilon_{0}$ ) : $$\delta\left ( t=0\right ) =\frac{\hslash\omega_{d}}{\sinh \frac{1}{n ( 0 ) v}}$$ which might be of help for calculating the critical line $\delta ( t ) $ .
i met a paper published in prl using the orietational order parameter and global bond_orietational order parameter . although the structures in this paper are about the short chain alkanes , i think the definitions of the two kind of order parameters may provide you some insights into your question .
the issue of particle annihilation is immaterial to the final mass of the merged black hole . if the traditional , " no-hair " view of gravitational collapse holds , and the particles lose their identity when crushed into a singularity , there would be no particle annihilation at all . if some newer and more exotic physics holds , such as string theory or loop quantum gravity , that rescues gravitational collapse from creating singularities , then even if the basic particles retain some sense of identity and can annihilate , these dynamics will still occur inside the event horizon of the black hole , and the energy released from the annihilation event will still be trapped inside the event horizon and register as mass from outside . the only issue at stake , then , is the bulk electrostatic potential energy as the two black holes approach each other . if the holes are oppositely charged , then potential energy will be converted to kinetic energy , and presumably some of this will get radiated away during the collision , resulting in a slightly lower mass for the resulting black hole . if the black holes are of like charge , then it will require more work to bring them together , and this work will probably end up reflected as a slightly larger mass of the resulting black hole . as a practical matter , however , the fractional difference in mass will be minute . all objects of astrophysical-scale masses , including black holes , will be found to have negligible net charge , due to the abundant presence of free electrons and ions in interstellar space . any object in space with a large net charge will rapidly accrete free charged particles , neutralizing itself . for tiny black holes on the primordial or quantum scale , stephen hawking calculated that such a black hole can only have a net charge of a few electrons ( eight , perhaps ? ) ; any more and not enough bound electron states could exist for such a " black hole atom " to be stable against the black hole nucleus accreting charged particles and neutralizing itself . i read this paper early in grad school and remember it relatively clearly , but so far i have not been able to find the reference . will update if i do . however , i did find http://arxiv.org/ps_cache/gr-qc/pdf/0001/0001022v1.pdf in this paper , on similar stability and half-life arguments , they claim that a primordial black hole could not have a charge greater than 70 .
there are a variety of methods used to measure distance , each one building on the one before and forming a cosmic distance ladder . the first , which is actually only usable inside the solar system , is basic radar and lidar . lidar is really only used to measure distance to the moon . this is done by flashing a bright laser through a big telescope ( such as the 3.5&nbsp ; m on apache point in new mexico ( usa ) , see the apollo project ) and then measuring the faint return pulse with that telescope from the various corner reflectors placed there by the apollo moon missions . this allows us to measure the distance to the moon very accurately ( down to centimeters i believe ) . radar has been used at least out to saturn by using the 305&nbsp ; m arecibo radio dish as both a transmitter and receiver to bounce radio waves off of saturn 's moons . round trip radio time is on the order of almost 3 hours . if you want to get distances to things beyond our solar system , the first rung on the distance ladder is , as wedge described in his answer , triangulation , or as it is called in astronomy , parallax . to measure distance in this manner , you take two images of a star field , one on each side of the earth 's orbit so you effectively have a baseline of 300 million kilometers . the closer stars will shift relative to the more distant background stars and by measuring the size of the shift , you can determine the distance to the stars . this method only works for the closest stars for which you can measure the shift . however , given today 's technology , that is actually quite a few stars . the current best parallax catalog is the tycho-2 catalog made from data observed by the esa hipparcos satellite in the late 1980s and early 1990s . parallax is the only direct distance measurement we have on astronomical scales . beyond that everything else is based on data calibrated using stars for which we can determine parallax . and they all rely on some application of the distance-luminosity relationship $m - m = 5log_{10}\left ( \frac{d}{10pc}\right ) $ where m = apparent magnitude ( brightness ) of the object m = absolute magnitude of the object ( brightness at 10 parsecs ) d = distance in parsecs given two of the three you can find the third . for the closer objects , for which we know the distance , we can measure the apparent magnitude and thus compute the absolute magnitude . once we know the absolute magnitude for a given type of object , we can measure the apparent magnitudes of these objects in more distant locations , and since we now have the apparent and absolute magnitudes , we can compute the distance to these objects . it is this relationship that allows us to define a series of " standard candles " that serve as ever more distant rungs on our distance ladder stretching back to the edge of the visible universe . the closest of these standard candles are the cepheid variable stars . for these stars , the period of their variability is directly related to the absolute magnitude . the longer the period , the brighter the star . these stars can be seen in both our galaxy and in many of the closer galaxies as well . in fact , observing cepheid variable stars in distant galaxies , was one of the original primary mission of the hubble space telescope ( named after edwin hubble who measured cepheids in m31 , the andromeda galaxy , thus proving that it was an “island universe” itself and not part of the milky way ) . beyond the cepheid variables , other standard candles , such as planetary nebula , the tully-fisher relation and especially type 1a supernova allow us to measure the distance to even more distant galaxies and out to the edge of the visible universe . all of these later methods are based on calibrations of distances made using cepheid variable stars ( hence the importance of the hubble mission to really nail down those observations .
there are two su ( 3 ) symmetries you can come across . basically , su ( n ) emerges everywhere when you have n quantum states and some physics does not distinguish these states - then all quantum superpositions of these states make a fundamental representation of su ( n ) significant to that physics ( but maybe insignificant to some other ) . thus in particle physics there are two su ( 3 ) discussed often . the su ( 3 ) of colors represents the exact , unbroken symmetry . then there is the su ( 3 ) of flavours , which mixes $u$ , $d$ , $s$ quarks in strong interactions . it is very rough and it is broken by the masses of quarks ( $m_s$ is about 150 mev , that is dozen percent of the qcd scale of about 1 gev ) . after breaking it leaves the flavour su ( 2 ) of only $u$ , $d$ quarks , and that is much more accurate ( $m_d-m_u$ in the order of unities of mev ) , though still approximate . nevertheless , these symmetries are used to classify hadron states , and to descibe physics on the hadron level ( which is the effective theory with respect to sm ) . it may be credited for the close masses of hadron multiplets , but this is ( almost ) another way to say " hadrons have close masses because quarks inside them have close masses " . also , $su ( 3 ) _c$ is a gauge ( local ) symmetry , while $su ( 3 ) _f$ is a global symmetry .
the answer seems to near 60 degrees . additional description here i have found this mathematica description of the ecliptic plane relative to the galactic plane . wolfram . com
without the internal part : the divergence $$\nabla_\mu ( x_\nu \theta^{\mu\nu} ) = x_\nu \nabla_\nu \theta^{\mu\nu} + \frac12 ( \nabla_\mu x_\nu + \nabla_\nu x_\mu ) \theta^{\mu\nu} $$ where i used that $\theta^{\mu\nu}$ is symmetric . recalling that the energy-momentum tensor is divergence free , the first term drops out . assuming that $x^\nu$ generates a dilation/scaling symmetry ( and not a bona fide symmetry ) , we know that its deformation $$ \nabla_\mu x_\nu + \nabla_\nu x_\mu \propto \mathcal{l}_x g_{\mu\nu} \propto g_{\mu\nu} $$ where $\mathcal{l}$ is the lie derivative . ( in the case $x^\nu$ generates a symmetry the term vanishes from killing 's equation . ) hence in this case for the current to be conserved ( that is , divergence free ) , we need that $g_{\mu\nu} \theta^{\mu\nu} = 0$ ; that is , the energy momentum tensor is tracefree .
liquid nitrogen boils when it comes in contact with skin , so small amounts of spatter are no danger at all-- the droplets just bounce off . i regularly pour a liter or so ( a bit at a time ) out on a lab table when i do liquid nitrogen demos , with no problems or safety gear . the biggest risk from the low temperature is getting it into fabric of some sort , which will hold it in closer proximity to skin for longer than the drops by themselves will . i have , on occasion spilled some on my pants , which is annoyingly cold , but not too bad unless you are wearing really tight clothing . really , the biggest hazard from any of the nitrogen demos i do is not the temperature but the expansion . when it boils , it expands to something like 700 times the volume of the liquid , so if you put some in a sealed container , it can make a big bang . i know of a case where a grad student at mit destroyed a bathroom with a 2-liter bottle of liquid nitrogen . you can use this expansion for a kind of cool demonstration if you take one of those little dropper bottles with the angled spouts that are common in chem labs , and seal a little nitrogen inside . put it down on the floor , and it spins like a firework . the problem with that is , you almost always end up getting a little water vapor condensing in the spout , which plugs it up , and then the bottle will go bang . i had a student a few years ago who had one go off in his hand , and he said it stung pretty badly . i have seen videos of people ( jearl walker in particular ) " drinking " liquid nitrogen by taking a small amount into their mouth , and holding it there for a second or so-- the instant boiling will keep it from giving you oral frostbite for a little while , and you can spit the liquid out after breathing out over it , which makes an enormous plume of steam . it is really cool to see , but kind of risky to do . i have never had the guts to try it myself .
the longitudinal components are derivatives of scalars , $\partial_\mu\phi$ . for a massive spin-1 $v_\mu$ they appear only in the mass term $m^2_v v_\mu^2$ . that is where the dimension $2$ comes from .
firstly , block b is the heavier of the two and coefficient of friction is lower on b 's side . if either of the blocks moved , it has to be b . given that b has a tendency to move first and its side has lower coefficient of friction , friction on that side would reach a limiting value before a . this is what the author must have assumed .
think about this : a function that maps points on a 2d space to numbers can describe the shape of terrain , but i would not say that it is the terrain . in the same way , a mapping of points to objects ( scalars , vectors , tensors , etc . ) is the mathematical description of a field , but if you think of the field as just the mapping , you are missing out . fields can have various physical properties . for example , just as a particle can have a certain amount of energy , so can an electromagnetic field . the difference is , since the field is spread throughout space , so is the energy ; therefore , it makes more sense to talk about the density of energy rather than the amount . same applies for momentum , or any other physical quantity carried by the field . just as the field could be described by a mapping of points to vectors , so the energy density can be described by a mapping of points to numbers . given the vector value of the field at any point , you can calculate the numeric value of the energy density at that point . but remember that these numbers ( i.e. . the mapping ) are just a mathematical description of the energy density . now , you may notice that the mapping that describes the energy density ( $u ( x ) $ ) satisfies the naive definition of a mathematical description of a field : it associates a number with each point in space . but physicists would not normally call that mapping a " field , " because in a sense , it is not really independent . mathematically , you can calculate $u ( x ) $ from $a ( x ) $ ; physically , the energy " field " is completely determined by the em field . in physics , we tend to reserve the term " field " to talk about something that can not be obtained by a simple calculation from some other field . does that mean , that the " numbers " that make up the field in each point in space stay constant with temperature or time ( or both , i am not sure . ) i am not sure how you got that from the quote you listed . . . no , the numbers that make up the mathematical description of the em field do not stay constant with either time or temperature . in fact , one of the things that characterizes a physical field is that it has dynamics - mathematically , this means that the numbers ( or whatever ) making up the field change with respect to time and space , but in a predictable manner which can be described with differential equations . but there are things you can calculate from a field that do stay constant . for instance , you can calculate the total energy stored in the field by calculating the energy density and then integrating it over the volume of the field . you could also calculate the temperature of the field , by some mathematical procedure . in many cases , these quantities are more closely based on the manner in which the field changes than the actual values that describe it . ( in fact , in some sense , it turns out that you can describe a field by the way that the numbers change , just as well as you can with the numbers themselves . read up on the fourier transform and momentum space if you are interested . )
if you break the $$su ( 2 ) \times u ( 1 ) $$ symmetry spontaneously $$\left|-ig\frac{\sigma}{2}w_\mu-ig&#39 ; b_\mu\phi\right|^2$$ and insert the vacuum expectation value of the scalar field you get terms $$\frac{1}{2}vgw_\mu^+w^{\mu-}+\frac{1}{8}v^2 ( w_\mu^3 , b_\mu ) \left ( \begin{array}{cc} g^2 and -gg&#39 ; \\ -gg&#39 ; and g&#39 ; ^2 \end{array} \right ) \left ( \begin{array}{c} w^{3\mu} \\ b^\mu \end{array} \right ) $$ if you diagonalize the mass matrix on the right to get the physical fields you get $$\frac{1}{8}v^2 [ gw_\mu^3-g&#39 ; b_\mu ] ^2+0\cdot [ g&#39 ; w_\mu^3+gb_\mu ] ^2=\frac{1}{2}m_zz_\mu^2+\frac{1}{2}m_aa_\mu^2$$ , that is a massive z boson and a massless photon
you make a grid , place your shapes on the grid , and solve laplace 's equation with a zero potential at infinity , and some potential on the grid . this will give you an electric field intensity at every point ( the gradient of the potential ) , and you sum up the implied charge . the ratio q/v is the capacitance . this is mathematically optimal , and the only improvement is to use non-grid approximations , like expansion in harmonics at large distances , and superior higher-order methods in the interior of the grid . for laplace 's equation on modern computers , there is no issue--- even the worst algorithm will give you an answer to 1% accuracy on an arbitrary shape within a reasonable time .
to a good approximation the deceleration felt by the tourist will be the same as that felt by the parachutist . there may be some elasticity in the link joining the two , but i would be surprised if this made much difference . as always ( is there anything not on wikipedia ) there are a couple of useful articles on wikipedia . http://en.wikipedia.org/wiki/parachuting#parachute_deployment reports that the deceleration is 3 - 4g . however http://en.wikipedia.org/wiki/tandem_skydiving reports that tandem skydives use a drogue parachute to reduce the terminal velocity and hence reduce the deceleration .
if you are careful about how you define the surface , then you will get the correct direction out of maxwell 's equations . in vector calculus ( and generally in math and physics ) , a surface has an orientation , which also specifies the orientation of the loop that forms its boundary . so you can not just pick a direction to go around the loop at random . the direction in which you go around the loop is related to the orientation of the normal vector to the surface . of course , you do not actually need to do a surface integral to figure this out . the electric and magnetic fields in an em wave ( at any given position and moment in time ) are related by the equation $$\mathbf{b} = \frac{1}{c}\hat{\mathbf{k}}\times\mathbf{e}$$ where $\hat{\mathbf{k}}$ is a unit vector that points in the direction of propagation of the wave . if you do not know which direction the wave is moving , you can not tell which way the magnetic field points .
you are missing the rotational kinetic energy at the bottom , $\frac{1}{2}i \omega ^2$ . the key word in the problem is , ' rolling without slipping ' . also remember the equation , $$v=r\omega$$ the cylinder 's moment of inertia should be looked up in a table or given . on wikipedia , the moment of inertia of a thin cylindrical shell is given as : $$i=mr^2$$
your mistake is in forgetting that the verictial component of force f holds up half the stick , so the friction force only needs to hold up the other half of the stick 's weight , not the whole weight .
your question is not quite accurate , this because you thinking of photon as a classical particle ( solid ball ) , but it is not , photon ( same as other particles ) are quantum particles , thus you should consider particle-wave duality , in other hand , because photon has no mass , it can not move at any speed other than speed of light ( in vacuum ) , otherwise it will not exist . finlay , the momentum of massless particle , can be thought of as " amount of motion " , or even more directly , it is related actually not to speed of photon ( like in classical physics ) but to wave length by the following famous de broglie relation : $$ p=\frac{h}{\lambda}$$ thus there is no sense of saying " stopping a photon " in vacuum , it can exist only in motion and strictly at speed of light , or " absorbed " fully or partially , what will change it is energy/momentum , but not it is speed ! p . s the situation is more complicated for photons in some media , anyway one can slow down and maybe " even freeze " them .
answering my own doubts in order:- no . it got to the correct answer , but was wrong . i think she was getting confused between cause ( force ) and effect ( acceleration ) . when the train brakes , the ball and train acquire a relative acceleration . no other force comes into play when one 's inertial frame of reference is the train 's frame . she was basically saying $ a_{iron} = a_{rubber} $ and $ m_{iron} &gt ; m_{rubber} $ , so therefore $ f_{iron} &gt ; f_{rubber} $ which is correct . but then she also says that the iron ball will move a longer distance which implies that both the balls will eventually come to a stop while simultaneously claiming to ignore air resistance and friction . so then which force eventually stops the balls ( as otherwise they will keep moving at the constant speed otherwise as dictated by one of newton 's laws of motion ) ? none that i could think of ( which are probably all of them considering this is such a simple question ) . not mentioned , but probably not . in most idealized physics questions ( especially textbook ones ) , air resistance is not something to be considered . this is to some extent implied by the phrase " on a smooth floor " . if we do not consider it , then no external forces will be acting on the ball ( barring friction ) and both the balls will move at the same speed till they like slide off the train 's surface or hit a wall or something meaning this was a trick question ( which seems atypical as it is not the modus operandi of the crappy , unenlightening educational system ) . if we do consider it , then b'coz of the reasons in the ( my ) original answer , the iron ball will have a slight greater speed than the rubber ball and thus will move a bit further . probably not as the question mentions the phrase " a smooth floor " . although it could be said that while this would affect the absolute values of friction , it would not change the relativity , i.e. , the fact that the rubber ball will experience more friction that the iron one . also , considering it would mean we should probably also consider air resistance . and then the question becomes unsolvable as there will be unknown variables required then . for that and the reason above , it should probably not be considered . 10x , @nathaniel and @tromik .
but how do you calculate the g factor of a point particle or an extend particle ? this is done for a point particle , and any experimental deviation from the calculated value for a point particle would suggest structure beyond a point particle . dirac theory predicts g=2 . anomaly from g=2 has qed , hadronic and weak contributions , which are each calculated . the hadronic and weak contributions are small and considered to be well understood . the qed contribution to the anomaly is the main contribution and extremely difficult to calculate . hundreds of feymann diagrams are involved . see new determination of the fine structure constant from the electron g value and qed for more information .
in this context , a " current " is an object obeying an affine lie algebra , also called current algebra and a special case of a kac-moody algebra . it is an algebra formed by unit weight operators : take for example a current $j^a ( z ) $ , where $a$ is a label and $z$ is a complex coordinate . the algebra is given by $$ [ j^a_n , j^b_m ] =i{f^{ab}}_cj^c_{n+m}+mkd^{ab}\delta_{n+m} , $$ where $$j^a_n=\frac{1}{2\pi i}\oint dz \ , z^{- ( n+1 ) }j^a ( z ) . $$ the integer $n$ denotes the mode number , the integer $k$ is the level and $d^{ab}= ( t^a , t^b ) $ defines the inner product between generators . the word " boundary " refers to the fact that the symmetry group underlying the algebra preserves a certain structure at the boundary of the geometry at infinity . in the case of the paper you are reading , the symmetry group is $u ( 1 ) $ and the boundary is given by $\mathcal{i}^+$ . additional information : affine lie algebras play a role in string theory/conformal field theory , where they can be used to generate states in certain representations of a group . for example , the state $$j^a_{-1}\tilde{\alpha}^{\mu}_{-1}|0\rangle$$ corresponds to a massless vector $a^{\mu a}$ in the adjoint representation of the underlying group ( $\tilde{\alpha}^{\mu}_{-1}$ is a creation operator ) .
let 's begin by choosing coordinates . let the orbit of the earth define the x-y plane . assume that just before the collision the earth 's orbital axis is pointing in the x-direction , that is $ ( 1,0,0 ) $ . i will assume that the earth absorbs the asteroid , and that after the collision the axis has been changed to $ ( \sin ( 27 ) , 0 , \cos ( 27 ) ) $ where 27 is the degree measure of the earth 's axis currently . so the relative change in angular momentum for the earth is $ ( 1,0,0 ) - ( \sin ( 27 ) , 0 , \cos ( 27 ) ) = ( 0.55,0 , - . 89 ) $ which is a vector of length 1.05 and so the angular momentum of the asteroid has to be 1.05 times the angular momentum of the earth . make the approximation that the earth is a sphere of constant density . then its angular momentum is given by $0.4 m_e\omega\ ; r_e^2$ where $m_e , r_e$ are the mass and radius of the earth and $\omega$ is its angular rate of rotation . we have $m_e = 6\times 10^{24}\ ; kg\ ; \ ; \ ; r_e = 6.4\times 10^6\ ; m , \ ; \ ; \ ; \omega = 2\pi/ ( 24\ ; hours ) = 2\pi/ ( 86400\ ; sec ) $ . i am using sloppy approximations here ; the earth is not constant density , i am not using the sidereal day , etc . this gives $7\times 10^{33}\ ; kg\ ; m^2/s$ as the approximate angular momentum contributed by the asteroid . the formula for angular momentum is mass x velocity x radius . this is maximum when the radius is maximum ; for an asteroid hitting the earth this happens when the radius is equal to the earth 's radius , i.e. $ 6.4\times 10^6\ ; m$ . dividing the asteroid 's angular momentum by this give its linear momentum : $p_a = 7\times 10^{33} / 6.4\times 10^6 = 10^{27}\ ; kg\ ; m/s$ the reason a relatively small asteroid can wipe out all life on earth is due to the kinetic energy it carries . upon collision , the kinetic energy is converted to heat . if the asteroid is large and fast enough , the heat will increase the temperature of the atmosphere enough to boil off the oceans and even vaporize the salt deposits left over . since kinetic energy is proportional to the square of velocity while momentum is only proportional to velocity , we will assume that our asteroid is moving as slowly as possible . the escape velocity of the earth is 11 km per second ; to achieve a slower velocity an asteroid would have to be very lucky . so in calculating the kinetic energy of the asteroid , we will assume a very conservative speed of 5 km/sec . putting $10^{27} = 5000 m$ we find the mass of the asteroid as $2\times 10^{23}$ kilograms . assuming a specific density of 5 , or 5000 kg per cubic meter , this gives a radius for the asteroid of 2000 kilometers or a diameter of 5000 km . this is far more than enough to destroy all life on the planet . the asteroid 's kinetic energy is $0.5\times 2\times 10^{23}\times ( 5000 ) ^2 = 2.5\times 10^{30}$ joules $= 6\times 10^{14}$ megatons . in other words , the collision would result in the release of kinetic energy equal to 600 million million hydrogen bombs going off , each with an equivalent energy of one million tons of high explosives . the earth 's surface area is 500 million square kilometers or 500 million million square meters . so the energy release is equivalent to having a 1 megaton bomb going off on each square meter of the earth 's surface . and since the vaporized rock from the blast is lighter than rock , this vapor will condense on the surface and apply its heat to the surface . for a video of a 500 km asteroid hitting the earth ( 1/1000 of the volume necessary to change the earth 's axis ) see : http://www.youtube.com/watch?v=vzizu42sn6w
given a arbitrary metric $g_{\mu\nu}$ you can introduce a reference ( background ) metric $\bar{g}_{\mu\nu}$ ( in the paper notation it is just the minkowski metric $\eta_{\mu\nu}$ ) in a way that $\delta{}g_{\mu\nu} = g_{\mu\nu} - \bar{g}_{\mu\nu}$ is small ( in some sense ) . you can reintroduce the background metric in a way that the perturbation remains small , this action is parametrized by an small diffeomorphism ( you can see why in the mukhanov 's review of perturbations : dx . doi . org/10.1016/0370-1573 ( 92 ) 90044-z ) generated by an arbitrary vector field $-\xi^\alpha$ , thus , the background change by $$\bar{g}_{\mu\nu}\rightarrow\bar{g}_{\mu\nu}-\mathcal{l}_\xi\bar{g}_{\mu\nu} , $$ where $\mathcal{l}_\xi$ is the lie derivative with respect to $\xi^\alpha$ . given this transformation , the perturbation transform as $$\delta{}g_{\mu\nu}\rightarrow\delta{}g_{\mu\nu} + \mathcal{l}_\xi\bar{g}_{\mu\nu} . $$ if you choose a covariant derivative compatible with $\bar{g}_{\mu\nu}$ , say $\bar{\nabla}_\alpha\bar{g}_{\mu\nu} = 0$ , the lie derivative can be written as $$\mathcal{l}_\xi\bar{g}_{\mu\nu} = 2\bar{\nabla}_{ ( \mu}\xi_{\nu ) } . $$ in the paper the background metric is just the minkowski metric , then in cartesian coordinates $$\mathcal{l}_\xi\eta_{\mu\nu} = 2\partial_{ ( \mu}\xi_{\nu ) } = 2\xi_{ ( \mu , \nu ) } , $$ where the comma represent the partial derivative . so in general you can write a metric in terms of the background , perturbation and a gauge transformation as $$g_{\mu\nu} = \bar{g}_{\mu\nu} + \delta{}g_{\mu\nu} + 2\bar{\nabla}_{ ( \mu}\xi_{\nu ) } . $$
spin1/2 particle ususally , in this kind of hamiltonian , people uses $s=s_z$ , where $$s=s_z=\left [ \begin{array}{cc} 1 and 0 \\ 0 and -1\end{array} \right ] . $$ then , your unperturbed hamiltonian $h_0$ is : $$h_0=-\mu s\cdot b_0 = -\mu \left [ \begin{array}{cc} 1 and 0 \\ 0 and -1\end{array} \right ] b_{0 , z} . $$ then the eigen vectors of energy are : $$|\psi^0_+\rangle=\left [ \begin{array}{c} 1 \\ 0\end{array} \right ] , $$ $$|\psi^0_-\rangle=\left [ \begin{array}{c} 0\\ 1 \end{array} \right ] . $$ perturbation solution then you want to compute $|\psi_+\rangle$ and $|\psi_-\rangle$ for the perturbed hamiltonian $h=h_0-\mu b_1 s_x$ , where $$s_x=\left [ \begin{array}{cc} 0 and 1 \\ 1 and 0\end{array} \right ] . $$ as you said , you have to compute the following quantities ( note i use $+ , -$ instead of $n=0,1$ . which became : $$\psi^{ ( 1 ) }_+=\sum_{n\neq +} \psi^{ ( 0 ) }_{n'}\frac{\langle\psi_{n'}^{ ( 0 ) }|-\mu b_1s_x|\psi_{+}^{ ( 0 ) }\rangle}{e_+^{ ( 0 ) }-e_{n'}^{ ( 0 ) }}=\psi^{ ( 0 ) }_{-}\frac{\langle\psi_{-}^{ ( 0 ) }|-\mu b_1s_x|\psi_{+}^{ ( 0 ) }\rangle}{e_+^{ ( 0 ) }-e_{-}^{ ( 0 ) }}$$ $$\psi^{ ( 1 ) }_-=\sum_{n\neq -} \psi^{ ( 0 ) }_{n'}\frac{\langle\psi_{n'}^{ ( 0 ) }|-\mu b_1s_x|\psi_{-}^{ ( 0 ) }\rangle}{e_-^{ ( 0 ) }-e_{n'}^{ ( 0 ) }}=\psi^{ ( 0 ) }_{+}\frac{\langle\psi_{+}^{ ( 0 ) }|-\mu b_1s_x|\psi_{-}^{ ( 0 ) }\rangle}{e_-^{ ( 0 ) }-e_{+}^{ ( 0 ) }}$$ put here vectors and matrices we just found and let me know if you get zero .
yes , single photon radio waves have been constructed . radio communication is now possible on the most elementary level : scientists at the eth zurich and the max planck institute for the science of light in erlangen have used two molecules as antennas and transmitted signals in the form of single photons , i.e. light particles , from one to the other . since a single photon usually has very little interaction with a molecule , the physicists had to use a few experimental tricks for the receiver molecule to register the light signal . a radio connection established via individual photons would be ideal for various applications in quantum communication – in quantum cryptography or in a quantum computer , for example . the discussion in this question should enlighten you , and the links given there . there is a one to one correspondence between classical and quantum electrodynamics , both rely on the maxwell equations as you will see if you read the detailed answer . in the link in my answer the way one goes mathematically from one to the other is shown . no need for extra experiments since the mathematics is rigorous . for low energies of the electromagnetic waves it is not very smart to use all the mathematical panoply if one can do the job with the classical representation , except as in this case of single photon transmissions .
electroweak symmetry breaking requires that $\partial v/\partial h_{u , d} = 0$ . combining the two expressions , we find the superpotential bilinear $\mu$ , $$ |\mu|^2 = \frac12 \left [ \frac{|m_{h_d}^2 - m_{h_u}^2|}{\cos 2\beta} - m_{h_u}^2 - m_{h_d}^2 - m_z^2\right ] _{\text{ew-scale}} $$ in the focus-point region $|m_{h_u}^2|$ and $|m_{h_d}^2|$ are " focused " to $\lesssim m_z^2$ at the electroweak scale ; this is insensitive to their values at the high-scale . careful with the sign of $m_{h_u}^2$ , though ; it is parameter unto itself , rather than the square of a real parameter , and it is typically negative at the electroweak scale . as a result $|\mu| \lesssim m_z$ , and certainly $|\mu| \ll m_1 , m_2$ . ( note that in minimal models like the cmssm , $\mu$ is calculated in this way , but in more general models , we can trade e.g. a free parameter $m_{h_d}^2$ for $\mu$ , and have $\mu$ as an input parameter and $m_{h_d}^2$ calculated . ) the lightest neutralino/chargino is therefore higgsino-like , with $$ m_{\chi_{1,2}} \approx |\mu| , \\ m_{\chi_{3}} \approx m_1 , \\ m_{\chi_{4}} \approx m_2 , \\ m_{\chi^\pm_{1}} \approx |\mu| , \\ m_{\chi^\pm_{2}} \approx m_2 . $$
the book answer seems correct ( it can be obtained from the equations for the bottom body ) . i do not know how you obtained your result , but there is a possibility that it is also correct and actually coincides with the book answer , just because the equations for the top body provide an extra relation between $\alpha$ and $\beta$ .
the geometry of special relativity is called lorentzian geometry , or in full : the " pseudo-riemannian geometry of minkowsk spacetime " . this is also the cartan geometry of the lorentz group inside the poincar&eacute ; group . see on the nlab at lorentzian geometry for further pointers . see the references there for introductions and surveys .
it goes a little something like this : \begin{align*} \delta g_{ab} ( \sigma ) and =g_{ab}^{\zeta} ( \sigma ) -g_{ab} ( \sigma ) \\ and =\exp ( 2\omega ( \sigma-\delta\sigma ) ) \frac{\partial ( \sigma^c-\delta \sigma^c ) }{\partial \sigma^a}\frac{\partial ( \sigma^d-\delta \sigma^d ) }{\partial \sigma^b}g_{cd} ( \sigma-\delta \sigma ) -g_{ab} ( \sigma ) \\ and \approx ( 1+2\omega ) ( {\delta^c}_a-\partial_a\delta \sigma^c ) ( {\delta^d}_b-\partial_b\delta \sigma^d ) ( g_{cd} ( \sigma ) -\delta\sigma^e\partial_eg_{cd} ( \sigma ) ) -g_{ab} ( \sigma ) \\ and \approx 2\omega g_{ab} ( \sigma ) -\partial_a\delta\sigma_b-\partial_b\delta\sigma_a-\delta\sigma^e\partial_eg_{ab} ( \sigma ) . \end{align*} the last expression we recognize as the lie derivative of the metric along the vector field $\delta\sigma^a$ . what you wrote down is an equivalent form using the covariant derivative . p.s. you made it to the hardest chapter : )
i think " solar storm " is a term more likely to be used by a newspaper than a helioseismologist . as far as the term has any strict meaning , it means the same as the rather better defined term " geomagnetic storm " . this is a disturbance to the earth 's magnetic field caused when charged particles from the sun hit the earth 's magnetic field . as usual , wikipedia has an excellent article on this at http://en.wikipedia.org/wiki/geomagnetic_storm . incidentally , i note that if you search for " solar storm " on wikipedia it redirects you to the " geomagnetic storm " article .
the heat equation for these kind of problems ( assume 1d ) , reads $$\frac{\partial t}{\partial t} = a \frac{\partial^2 t}{\partial x^2} $$ where of course $t$ is temperature , $t$ is time , $x$ is position and $a$ the thermal diffusivity : $a=\frac{\lambda}{\rho c_p}$ ( respectively thermal conductivity , density and heat capacity . this equation can be derived from fourier 's law . suppose you have a block at temperature $t_0$ which you put in contact with a block of temperature $t_1$ at $x=0$ . then you have a set of boundary conditions $$ t ( x , 0 ) = t_0 \\ t ( 0 , t ) = t_1 \\ t ( x\to\infty , t ) =t_0$$ it is not easy , but it has been derived that the solution to this equation is $$\frac{t-t_0}{t_1-t_0}=1-\frac{2}{\sqrt{\pi}}\displaystyle\int_0^{\frac{x}{2\sqrt{at}}} e^{-s^2}ds$$ where the solution to this integral is referred to as error function this solution describes the transient and spatial profile of the heated piece of material . for short times , only a certain amount of the material is heated . using the error function , one can define the penetration depth , which is $x_p=\sqrt{\pi a t}$ , which is obviously a measure for how far the increased temperature ranges into the material . suppose you domain has a finite length and is isolated at the other end . the heat transfer coefficient from the wall at $x=0$ is nearly constant , and the average temperature of the block will converge with an exponential decay to the boundary temperature ( see vladimir 's answer ) . this can be derived from the heat equation by assuming that $\frac{t-t_0}{t_1-t_0}=1 - f ( t ) g ( x/l ) $ note : this book was used as a reference for some of the equations .
this equation is wrong . as it has been pointed out in the comments , i can not equate the integrands of two integrals just because the integration limits are the same . the equation which is of relevance in the context of probability density and quantum mechanics is perhaps the well known continuity equation , $$ \frac{\partial \rho}{\partial t} + \nabla \cdot \textbf{j} = 0 $$ , where $\rho = \vert \psi \vert^2$ and $ \mathbf{j}=\frac{\hbar}{m}\text{im}\left [ \psi^*\nabla\psi\right ] $ is the probability flux .
there are some rules about what happens to light rays passing through lenses , which are derived from snell 's laws . in short : a ) a ray passing through the focal point into the lens will exit the lens parallel to the optical axis . b ) a ray passing straight into the center of the lens ( at any angle with respect to the optical axis ) will exit at the same angle . c ) a ray entering the lens parallel to the optical axis will exit the lens and pass through the focal point . these are standard in any intro physics text , but what the heck i will draw a simple diagram : the object is on the left , i have labeled the three rays appropriately , and the image is on the right ( focal points are dots ) . as you can see , the rays a ) and b ) are incident with the lens at an angle with respect to the optical axis , which should answer your question . the same rules apply for concave lenses as well , and also curved mirrors if you make the appropriate adjustments .
you are right that we do not know if the universe is finite or infinite in space . cosmologists do now think that it has an infinite future because of the accelerated expansion rate due to dark energy but this does not tell us anything about the question of infinite space . to answer the question for space we first have to assume spacial homogeniety , i.e. that space looks the same everywhere on large scales . if this assumption is wrong then we are stuck because we cannot see beyond the horizon of the observable universe which is due to the finite speed of light and the finite age of the universe . if we can not assume anything about what happens beyond the horizon then obviously we cant tell if it is finite or infinite . however , within the observable universe space does appear to be homogeneous so it is usual to assume , rightly or wrongly , that it is homogeneous everywhere . if that is the case then the question of the finiteness depends on the curvature of space and this is something that can be estimated with some precision using cosmological observations , especially that of the cosmic microwave background . if the curvature of space takes a positive value then it must be finite . if it is zero or negative then it is probably infinite though the possibility of a finite universe remains if it has an unusual topology like a tessellation of a finite polyhedron . when we measure the curvature we find that it is close to zero and we cannot tell whether it is positive , zero or negative with the present error bars , so we do not know if the universe is finite or infinite in space . this flatness is expected as a prediction of inflation theory . the best we can do is set a lower limit to how small the universe can be given limits on its measured curvature and that is what the papers you linked to are trying to do . they do not claim to settle the question of whether it is finite or infinite . your final question about quantum mechanics and the outside of the universe is unrelated and should have been asked as a separate question , but the answer is no .
there are " elementary " permutations sometimes called transpositions , which swap only a pair of elements . e.g. $$\text{abcde}\to \text{cbade}$$ every permutation $p$ can be written as a product ( composition ) of consecutive transpositions . there are many such decompositions for a fixed permutation . however , changing the decomposition , the number of these transpositions is always odd or always even , depending only on the considered $p$ . this way the sign of a permutation $p$ is defined : $\mathrm{sign} ( p ) =-1$ if $p$ can be decomposed as an odd number of transpositions , otherwise $sign ( p ) =1$ . with that definition it turns out that $$\mathrm{sign} ( pp' ) =\mathrm{sign} ( p ) \: \mathrm{sign} ( p' ) \: . $$
yes , the electrons are in the same state and yes they interact ( in the sense that identical bosons interact to create a bec ) . the explanation is kind of involved . in a collection of identical atoms , it is not possible for us to distinguish between them . this also applies to their electrons . this is true whether or not the atoms are cold enough to be in a bec state . now the rules of qm state that when you have identical particles ( electrons or atoms ) you have to symmetrize over them . thus a literal answer to your question " but what about the electrons or other fermions of the bec atoms ? are they in the same state ? " is " yes , they are in the same state but this does not have anything to do with the bec state . your second question : " do electrons of one atom interact with those of another ? " can be answered similarly . if you consider the electrons as identical particles which must be symmetrized over , then it is impossible to distinguish one electron from the other . therefore they must all be in the same state . so anything you do to " one " electron will effect all of them . therefore they do interact ; their waveforms are shared . now let 's be more specific about your questions in terms of the nature of the bec state . consider just a single atom . it is described by a wave function . as the atom is placed in a cooler environment it loses energy . this loss of energy changes the shape of the wave function . it makes the wave function bigger . the wavelength for the wave function of a gas atom is called the " thermal de broglie wavelength " . the formula is : $$\lambda_t = \frac{h}{\sqrt{2\pi m k t}}$$ where $h$ is the planck constant , $k$ is the boltzmann constant , $t$ is the temperature , and $m$ is the mass of the gas atom . the thing to note about the above formula is that as the temperature gets lower , the wavelength $\lambda_t$ gets bigger . when you reach the point that the wavelengths are larger than the distances between atoms , you have a bec . the reference book i have got is " bose-einstein condensation in dilute gases " by c . j . pethick and h . smith ( 2008 ) which has , on page 5: " an equivalent way of relating the transition [ to bec ] temperature to the particle density is to compare the thermal de broglie wavelength $\lambda_t$ with the mean interparticle spacing , which is of the order $n^{-1/3}$ . . . . at high temperatures , it is small and the gas behaves classically . bose-einstein condensation in an ideal gas sets in when the temperature is so low that $\lambda_t$ is comparable to $n^{-1/3}$ . " when the wavelengths are longer than the inter-particle distances , the combined wave function for the atoms ( which one must symmetrize by the rules of qm ) becomes " coherent " . that is , one can no longer treat the wave functions of different atoms as if they were independent . to illustrate the importance of this , let 's discuss the combined wave function of two fermions that are widely separated with individual wave functions $\psi_1 ( r_1 ) $ and $\psi_2 ( r_2 ) $ . for fermions , the combined symmetrized wave function is : $$\psi ( r_1 , r_2 ) = ( \psi_1 ( r_1 ) \psi_2 ( r_2 ) - \psi_1 ( r_2 ) \psi_2 ( r_1 ) ) /\sqrt{2} . $$ now the pauli exclusion principle says that exchanging two fermions causes the combined wave function to change sign . that is the purpose of the "-" in the above equation ; swapping $r_1$ for $r_2$ gives you negative of the wave function before the change . another , more immediate , way of stating the pauli exclusion principle is that you can not find two fermions in the same position . thus we must have that $\psi ( r_a , r_a ) = 0$ for any fermion wave function where $r_a$ can be any point in space . but for the case of the combined wave function of two waves that are very distant from one another there is no point $r_a$ where both of the wave functions are not zero ( that is , the wave functions do not overlap ) . thus the pauli exclusion principle does not make a restriction on the combined wave function in the sense of excluding the possibility that both electrons could appear at the same point . that was already intrinsically required by the fact that the two wave functions were far apart . now i used the above argument about " far apart fermions " because the pauli exclusion principle has a more immediate meaning to a lot of people than the equivalent principle in bosons . the same argument applies to bosons , but in reverse . bosons prefer to be found near one another , however if we write down the combined wave function for two widely separated bosons , there is still a zero probability of finding both bosons at the same location . again the reason is the same as in the fermion case : $\psi_1 ( r_b ) $ is only going to be nonzero in places where $\psi_2 ( r_b ) $ is already zero . the bosons are too far apart to interact ( in the sense of changing the probability of finding both at the same point from what you had otherwise expect classically ) . another way of saying all this is that in qm , there is no interaction between things except if their wave functions overlap in space . you use the thermal de broglie wave function to determine how big a wave function has to be . if the atoms are closer than that , they are interacting in the sense of bose ( or fermi ) condensates . so let 's apply our understanding to the question " do the electrons interact " in a bec . consider the $\lambda_t$ formula to the electron . since $m$ appears in the denominator , replacing the atom with the electron decreases $m$ by a factor of perhaps 3 or 4 orders of magnitude . this increases $\lambda_t$ by perhaps 2 orders of magnitude . therefore , any gas which is cold enough to be a bec will be composed of electrons that are much more than cold enough to also be coherent .
you are right , real photons always travel at the speed of light and would carry energy away from a magnet . from a field theory point of view , all static fields , whether electric , magnetic , the weak nuclear force or the strong nuclear force can be thought of as being mediated by virtual particles . so for either electric or magnetic fields , that would be virtual photons that mediate the field . see wikipedia for more information . another great article explaining virtual particles is from john baez and finally see this question and answer : what&#39 ; s the relation between virtual photons and electromagnetic potentials ? .
you are right . the thermal equilibrium will eventually be reached . in this process , heat is transferred from the water to the thermometer . this increases the temperature of the thermometer and decreases the temperature of the water until they are equal . however , generally , the amount of water is large so that the heat it loses is too small to significantly change its temperature .
yes . you have probably heard that string theory predicts the universe is 10 dimensional ( and m-theory predicts it is 11 dimensional ) while we only see 4 dimensions . however this is not because the number of dimensions has changed , but because 6 ( or 7 ) of the dimensions are rolled up into a very small circle . having said this , there have been suggestions that the universe started out as 2 dimensional ( 1 space and 1 time ) then the number of dimensions increased to 10/11/whatever as the universe evolved . however the idea comes from causal dynamical triangulation , and this is pretty speculative even by the standards of quantum garvity theories . there are even wilder suggestions that the spacetime dimension might be fractal .
the universe does not come equipped with any " clocks at each moment " that would immediately recognize different " moments " . indeed , the laws of physics - except for cosmology - are totally invariant with respect to translations in time , which is just a different way of saying that there is no way , even in principle , to find out whether an event occurred at time $t_1$ or $t_2$: the events are guaranteed to proceed in an identical way if the initial conditions are identical , whether the events occur at $t_1$ or $t_2$ . by noether 's theorem , this symmetry is inseparably connected with energy conservation . still , spacetime exists and events occur at different times , and the distances between events in time - duration of processes etc . - are well-defined numbers and can be measured by various tools . in particular , the proper time of world lines may be measured by all kinds of clocks . in the context of cosmology , the universe is expanding and the time-translational symmetry is broken by the expansion ( and so is the corresponding energy conservation law ) . it follows that one may define a " cosmic time " - the proper time of the " static " ( a frame in which the density of momentum of the cmb radiation vanishes ) world line stretched between a point of the big bang and a present event . this cosmic time is related to the current cmb temperature and many other things . still , the cmb temperature is obviously not accurate enough to measure the time separations of some very recent events from each other . in that case , we prefer things like atomic clocks that may achieve an unbelievable precision . archeologists use radioactive isotopes to determine the age of various things because it works and they do not use a more accurate method because no method that would be more accurate is available . pharaohs could have put ( and reset ) digital clocks into the pyramids when they built them except that they failed to do so and it is very hard to prosecute them for their negligence today . ; - ) at any rate , those questions are for geologists , historians , and archeologists - not physicists . if a historian asked whether physicists may come up with a better method , the answer is almost certainly no . but this is mostly a question about the creative engineering tricks and inventions that are possible given the known laws of physics ; it is not a question about physics itself . if we measure the age of objects and materials , we must look at the time-dependent changes of these objects and materials . chemistry does not bring us too far and nuclear physics is the way to go - we are back to various types of radioactive dating . as you can see , i probably do not understand the question - and i would bet that i will end up being in the majority - so i have no way to predicting whether the text above will be found satisfactory by hde .
i know little about baseball , but if you are willing to extend your question to include god 's own sport of cricket , the fastest speed recorded by a bowler was 100.3 mph . this speed was recorded at the moment when the ball leaves the bowlers hand ( the ball slows down once it is released ) so the bowler 's hand was indeed travelling at 100.3 mph . as alexander points out , you can attain these high speeds using leverage . a typical fast bowler 's arm is around a meter long from the shoulder joint to the cricket ball , and the rules demand the arm be kept straight during bowling . dividing the speed by the circumference of the circle traced by the arm give the rotation speed as one rotation in 140ms , which is not superhumanly fast . it is certainly faster than i can manage , but then i am not paid millions of pounds per year to play cricket !
starting from the hamiltonian formulation of qm one can derive the path-integral formalism ( see chapter 9 in weinberg 's qft volume 1 ) , where the hamiltonian action is found to be proportional to $\int \mathrm{d}t ( pv - h ) $ . for a subclass of theories with " a hamiltonian that is quadratic in the momenta " ( see section "9.3 lagrangian version of the path-integral formula " in above textbook ) , the term $ ( pv - h ) $ can be transformed into a lagrangian $l_h = ( pv - h ) $ . then the lagrangian action is proportional to $\int \mathrm{d}t l_h$ . both actions give the same results because one is exactly equivalent to ( and derived from ) the other . $$ \int \mathrm{d}t ( pv - h ) = \int \mathrm{d}t l_h$$ moreover , when working in the interaction representation you do not use the total hamiltonian but only the interaction . the derivation of the hamiltonian action is the same , except that now the total hamiltonian is substituted by the interaction hamiltonian $v$ . again you have two equivalent forms of write the action either in hamiltonian or lagrangian form . if you consider hamiltonians whose interaction $v$ does not depend on the momenta , then the $pv$ term vanishes and the above equivalence between the actions reduces to $$ - \int \mathrm{d}t v = \int \mathrm{d}t l_v$$ where , evidently , the interaction lagrangian is $l_v = -v$ this is what happens for instance in qed , where the interaction $v$ depends on both position and dirac $\alpha$ but not on momenta . note : there is a sign mistake in your post . i cannot edit because is less than 10 characters and i have noticed the mistake in a comment to you above , but it remains .
provided the intervals between all events are spacelike they can appear in any order . see this article for a popular science level description , or this paper for the full details .
your procedure gives : $$ a_{xz} = \sqrt{a_x^2 + a_z^2} $$ then : $$ a_{total} = \sqrt{a_y^2 + a_{xz}^2} $$ but if you substitute for $a_{xz}$ in the second equation you get : $$ a_{total} = \sqrt{a_y^2 + ( \sqrt{a_x^2 + a_z^2} ) ^2} = \sqrt{a_y^2 + a_x^2 + a_z^2}$$ so you do not need to split the calculation into two steps . your accelerometer may already exclude the acceleration due to gravity . if it does not then yes you need to use the inclination to work out the three components of gravity then subtract them from $a_x$ , $a_y$ and $a_z$ . it is hard to say exactly how to do this without knowing how your phone reports it is inclination . response to comment : suppose you have your device held flat so $a_z$ = -1 . now move the device downwards at and angle of $\theta$ as shown below : assuming it is moving in the $xz$ plane the value of $a_z$ will be decreased a bit and the value of $a_x$ will increase from zero . suppose you are applying an acceleration to the phone of $2g\space cos ( \theta ) $ - you will see why i have chosen this value in a moment . now the values of $a_x$ and $a_z$ are : $$ a_x = 2g\space cos\theta \space sin\theta $$ $$ a_z = g - 2g \space cos^2 \theta $$ you now calculate $a_{total}$ by just squaring and adding as we discussed above to get : $$a_{total}^2 = 4g^2 \space sin^2\theta \space cos^2\theta + g^2 + 4g^2 \space cos^4\theta - 4g^2 \space cos^2\theta $$ and a bit of rearrangement gives : $$a_{total}^2 = g^2 + 4g^2 \space cos^2\theta \left ( sin^2\theta + cos^2\theta - 1\right ) $$ and because $sin^2\theta + cos^2\theta = 1$ the quantity in the brackets is zero so you end up with : $$a_{total}^2 = g^2 $$ that is : $$a_{total} = g $$ which is the same as when the phone is stationary . so it is possible to be accelerating the phone and still have the total acceleration come out as $g$ ( $g$ = -1 in the phone 's units ) . that is why just subtracting one is not a reliable way to tell if the phone is accelerating .
conformal field theories do not have a mass-gap , which is one of the assumptions [ for the strong conclusions of non-mixing of poincare spacetime symmetries vs internal symmetries ] of the coleman-mandula no-go theorem . similarly , for its superversion : the haag-lopuszanski-sohnius no-go theorem . [ in the supercase , the poincare algebra is replaced with the super-poincare algebra . ]
a magnet is in essence made up of atoms with orbital and spin angular momentum , ( mainly due to the electrons ) and the forces that act on these electrons can be derived from dirac 's equation , but give you what is essentially the lorentz force law . you can think of the magnetic field acting on every single electron individually , and all these forces added up will apply a net force and a net torque to the magnet as a rigid body . if you want to do some calculations , you can imagine each atom as having a microscopic current proportional to the dipole moment of that atom ( basically , the magnetization ) . this " bound current " is $\vec{j} = \nabla \times \vec m$ . again , imagine an electron orbiting the nucleus as providing a " current " flowing around the nucleus -- the bohr magneton . it is just an idealization but you can make it rigorous if you want . now , currents of adjacent atoms cancel out where they intersect , because they are going in opposite directions , but on the boundary of the magnet they do not cancel out , because there is no neighboring atom there . thus it is perfectly mathematically and physically sensible to model a permanent magnet as a sheet of current moving along the magnet 's surface , roughly like a solenoid does , and given by $\vec{k} = \vec{n}\times \vec{m}$ , where $\vec{n}$ is the normal to the surface . now this surface current is a real current , and therefore it will experience a force when subjected to an external magnetic field ( and it is own magnetic field , btw . this is why transformers hum ) .
i think you are being a bit hard on scientific american . it is a popular ( if slightly geeky ) magazine so you would not expect its articles to have all the gory details . the best way to find info about areas like this is to search arxiv . org . for example googling for " super wimp site:arxiv . org/abs " finds http://arxiv.org/abs/0812.0432 and this looks like a good place to start . there have been various suggestions for particles that only interact by the gravitational force . one example is the sterile neutrino .
it should not be too hard with a van de graaff generator . assuming a generator of radius to the order of a decimeter , we need to generate a potential of $\frac{q}{r} \tilde~ \frac{10^9}{0.1}\tilde~10^{10} v$ to get a charge of one coulomb . that would be rather hard , though if we want a microcoulomb , that can be arranged with a ( van de graaff ) generator capable of producing voltages in megavolts . once we have this charg on the generator , we can transfer it to the electrode via conduction ( which will only transfer a fraction of it ) , or induction ( which will induce an opposite and equal charge on the electrode ) so yes , charges in nanocoulomb/microcoulomb/millicoulomb are not that hard to generate and collect . 1 coulomb of charge -- not so much . note that it is not too hard to have 1c of net charge in some given volume--the earth has some net charge which is probably in coulombs . the issue comes when you have to concentrate it enough to be able to transfer it . i somehow forgot about capacitors . capacitors can store a large amount of charge ( till one kilocoulomb ) though the net charge stored is zero . however , it is generally hard to transfer this high charge elsewhere without neutralizing it or pushing it into another capacitor .
your inability to see the dust until you narrow the slit has nothing to do with the narrowness of the beam but instead the dynamic range of light that your eye can see at one time . a bit of searching turns up reports of a contrast ratio for you eye at one time as between 100:1 and 1000:1 . this means if you are in a room with a range of brightness greater than about 100 to 1 the brightest things will all be washed out as white and the darkest things will all be essentially black . this is obvious in photos that are " backlit " like this one : these horses are not black but because the ratio of the bright light to the dark horses exceeds the dynamic range of the camera the sky is washed out white and the horses are in silhouette . your eye can adjust over time to a huge range but it can not utilize the whole range all at once . in the case of dust reflecting light , if you allow a lot of light into the room the relative brightness between the small amount of light the dust is reflecting and the rest of the illuminated room prevent you from seeing the dust . this is fundamental to signal processing . why can not you hear a whisper in a noisy room ? the noise of the crowd obscures the whisper . the difference between the signal you are trying to pick up and the background noise is called the signal-to-noise ratio . in the case of dust , the light let into the room is scattered and reflected in the room and causes the room to be illuminated . this is the noise that obscures the signal from light reflected off of the dust .
your textbook or lab manual will have what method they want you to use . but without more information , i would personally report the standard error of the mean , which is neither of the two methods you have shown . this method takes into account the number of measurements ; more measurements tends to result in a smaller reported error .
the book has a minus sign typo in the previous equation . the commutation in the step you are looking at does not introduce a minus sign , but the previous line had a term of the form $$ - c^r c^i b_r k_i$$ with a minus sign in front , which requires you to commute the two c 's past each other . the next step writes $$ - c^i c^r b_r k_i $$ incorrectly , it should be a plus sign , because the c 's are commuted , and then the next step says $$ c^i k_i c^r b_r $$ which fixes the error . you assumed that the minus sign came from moving the k , which does nothing , when it actually comes from commuting the c 's past each other . thank you for linking the book , it would have been impossible to find the error otherwise . given the unmotivated and needlessly formal introduction of brst in this book , i would recommend that you read the introduction to brst in the appendices of polchinski . the exercise in question does not require formal symbol manipulation gymnastics in an operator calculus , it is a simple question .
yes , you are literally simulating the flow of heat through a material . the diffusion coefficient is basically just a local measure of thermal conductivity and heat capacity . rather than convolving your image with a gaussian kernel , you could use something like the crank-nicholson method and make some number of timesteps . you could also adjust $d$ to make it greater in parts of the image with greater noise . $\nabla$ is the gradient operator . it is a vector operator that looks like this in two dimensional cartesian coordinates : $$ \nabla= \hat{\mathbf{x}} \frac{\partial}{\partial x} + \hat{\mathbf{y}} \frac{\partial}{\partial y}$$ where $\hat{\mathbf{x}}$ and $\hat{\mathbf{y}}$ are the unit vectors pointing in the $x$ and $y$ directions . in short , your intuitive understanding is correct .
your correct intuition stems from a symmetry of the hamiltonian called parity invariance , which means that it is the same under reflection ( so there is nothing to distinguish left and right ) . try looking at the parity operator , i.e. the unitary operation $\pi$ that takes $x\to -x$ and $p\to - p$ . specifically , you can write $\pi x\pi = -x$ and $\pi p \pi = -p$ . you should also be able to show that : 1 ) the hamiltonian commutes with $\pi$ ( since $\pi h \pi = h$ ) and therefore the energy eigenstates $\lvert n \rangle$ are eigenstates of $\pi$ . 2 ) $\pi^2 = 1$ . what does this tell you about the eigenvalues $\pi_n$ of $\pi$ ? 3 ) $\langle n \rvert p \lvert n \rangle = \langle n \rvert \pi^2 p \pi^2 \lvert n \rangle = - \pi_n^2 \langle n \rvert p \lvert n \rangle$ . why does this imply that $\langle n \rvert p \lvert n \rangle = 0$ ? finally , your last point does not follow from the hermiticity of $p$ . actually you have only the weaker condition $\langle 0\rvert p \lvert 1 \rangle = \left ( \langle 1\rvert p^{\dagger} \lvert 0 \rangle\right ) ^{\ast} = \langle 1\rvert p \lvert 0 \rangle^{\ast} . $
the answer can be calculated by referring to band emission tables of any introductory heat transfer book . i use the heat transfer book by incropera and dewitt where are concept is explained and the tables given in chapter 12 . here is how its done . the fraction of total energy emitted as wavelengths from 0 to a certain wavelength $\lambda$ can be obtained from the band emission table ( $f_{ ( 0-\lambda ) }$ ) . this way the emissions from 0 to 400 nm ( $f_{ ( 0-0.4\mu m ) }$ ) and between 0 to 800 nm ( $f_{ ( 0-0.8\mu m ) }$ ) can be obtained . the difference in these two quantities gives the emission from 400 nm to 800 nm and is the required answer . the energy fractions $f_{ ( 0-\lambda ) }$ as a function of $\lambda$ and $t$ are given in the table here $t$ = 2,573 k , while $\lambda$ is the upper limit of wavelength .
actually , the answer is a bit more subtle than just density . the principle that is behind floating objects is archimedes ' principle : a fluid ( liquid or gas ) exerts a buoyant force , opposite apparent gravity ( i.e. . gravity + acceleration of fluid ) on an immersed object that is equal to the weight of the displaced fluid . thus , if you have an object fully immersed in a fluid , the total force it feels is given by ( positive sign means down ) : $f = gravity + buoyancy = \rho_{object} v g - \rho_{fluid} v g = ( \rho_{object} - \rho_{fluid} ) v g$ thus , if the average density of the object is lower than that of the water , it floats . if the object is partially immersed , to calculate the buoyant force you have to consider just the immersed volume and its average density : $f = \rho_{object} v g - \rho_{fluid} v_{immersed} g$ note that when i was talking about density , i was talking about the average density of the object . that is its total mass divided by its volume . thus , a ship , even if it is made out of high-density iron it is full of air . that air will lower the average density , as it will increase the volume considerably while keeping the weight almost constant . if you want to understand this better you can give the following problem a try : ) what is the height an ice cube of side l floats in water ?
i completely agree with scott that this particular " grassmannization " is not equivalent to what supersymmetry is doing in physics . supersymmetry is a constraint that picks a subset of theories – ordinary theories with ordinary bosonic and fermionic fields that are just arranged ( and whose interactions are arranged ) so that there is an extra grassmann-odd symmetry . because supersymmetric theories are a subset of more general theories , of course that all the general inequalities that hold for the more general theories hold for supersymmetric theories , too . and there are many new inequalities and conditions that hold for supersymmetric theories – but not fewer constraints . in supersymmetric theories , what becomes grassmann numbers are never probability amplitudes . only particular observables are fermionic operators – operator counterparts of grassmann-number-valued quantities in classical physics . these fermionic operators only have nonzero matrix elements between grassmann-odd states and grassmann-even states ; for the same reason why bosonic operators only have nonzero matrix elements between states of the same grading . one may introduce a grading on the hilbert space but the amplitudes are still complex commuting $c$-numbers . there is a simple reason why probability amplitudes can not be grassmann numbers . to get physical commuting quantities out of grassmann numbers , one always has to integrate . that is why the grassmann variables may be integration variables integrated over in feynman 's path integral ; but that is also why they have to be set to zero if we are doing classical physics . there are not any particular nonzero values of grassmann numbers . on the other hand , probability amplitudes do not have to be integrated ; their absolute values should be just squared to obtain the probabilities ( or their densities such as differential cross sections ) . so if their construction is consistent at all , it is just a mathematical analogy of superspaces at a different level – amplitudes themselves are considered " superfields " even though in genuine quantum physics , amplitudes are always complex numbers . that is why the inequalities can not be considered analogous to bell-like inequalities and can not be applied to real physics . in particular , once again , tsirelson 's bound can not be violated by theories just because they are supersymmetric ( in the conventional sense , just like the mssm or type iib string theory ) because it may be derived for any quantum theory , whether it is supersymmetric or not , and supersymmetric theories are just a submanifold of more general theories for which the inequality holds . i would point out that it would not be the first time when michael duff and collaborators would be giving wrong interpretations to various objects related to quantum computation . some formulae for the entropy of black holes mathematically resemble formulae for entangled qubits etc . but the interpretation is completely different . in particular , the actual information carried by a black hole is $a/4g$ nats i.e. the black holes roughly parameterize an $\exp ( a/4g ) $-dimensional space of microstates . that is very different ( by one exponentiation ) from what is needed for the quantum-information interpretation of these formulae in which the charges themselves play the role of the number of microstates . so i think that at least michael duff has been sloppy when it came to the interpretation of these objects which was the source of his misleading comments about the " black hole entropy formulae emulating tasks in quantum computation " . there may be mathematical similarities – i am particularly referring to the cayley hyperdeterminant appearing both in quantum computing and black hole entropy formulae – but the black holes are not really models of those quantum algorithms because their actual hilbert space dimension is the exponential of what it should be for that interpretation and they are manipulating pretty much all the qubits at the same moment . the objects in the hyperdeterminant have completely different interpretations on the string theory and quantum computing side ; there is not any physical duality here , either .
i think the short answer is , you do not . the reason we call the unit of force a newton and not a kg m/s$^2$ is because it is convenient and it expresses the relation you want to convey when used elsewhere ( e . g . , $f=-kx$ for a spring ) . similarly , it is convenient to " hide " the mks base units into a single term , the potential $v$ in this case , so that the formula is easier to remember and that the relation is conveyed , in this case the relation between potential difference , current , and resistance .
please be aware that plutonium cores are supposed to be plated with another metal ( nickel or silver , if my memory serves me right ) . machining plutonium is very hazardous and is done with remote manipulators , since it increases risk of inhalation . source : http://toxnet.nlm.nih.gov/cgi-bin/sis/search/r?dbs+hsdb:@term+@na+@rel+plutonium,+radioactive : absorption through the skin can occur through occupational exposure . experiments show that the skin is an effective barrier and the percentage absorbed /seldom/ exceeds 0.05% for intact skin . [ seiler , h.g. , h . sigel and a . sigel ( eds . ) . handbook on the toxicity of inorganic compounds . new york , ny : marcel dekker , inc . 1988 . , p . 724 ] peer reviewed source : plutonium anl factsheet oct 2001 plutonium metal . plutonium isotopes are primarily alpha-emitters so they pose little risk outside the body . here the plastic bag , gloves , and outer ( dead ) layer of skin would each alone stop the emitted alpha particles from getting into the body . what happens to it in the body ? when plutonium is inhaled , a significant fraction can move from the lungs through the blood to other organs , depending on the solubility of the compound . little plutonium ( about 0.05% ) is absorbed from the gastrointestinal tract after ingestion , and little is absorbed through the skin following dermal contact . after leaving the intestine or lung , about 10% clears the body . the rest of what enters the bloodstream deposits about equally in the liver and skeleton where it remains for long periods of time , with biological retention half-lives of about 20 and 50 years , respectively , per simplified models that do not reflect intermediate redistribution . the amount deposited in the liver and skeleton depends on the age of the individual , with fractional uptake in the liver increasing with age . plutonium in the skeleton deposits on the cortical and trabecular surfaces of bones and slowly redistributes throughout the volume of mineral bone with time . what is the primary health effect ? plutonium poses a health hazard only if it is taken into the body because all isotopes but plutonium-241 decay by emitting an alpha particle , and the beta particle emitted by plutonium-241 is of low energy . minimal gamma radiation is associated with any of these radioactive decays . inhaling airborne plutonium is the primary concern for all isotopes , and cancer resulting from the ionizing radiation is the health effect of concern . the ingestion hazard associated with common forms of plutonium is much lower than the inhalation hazard because absorption into the body after ingestion is quite low . laboratory studies with experimental animals have shown that exposure to high levels of plutonium can cause decreased life spans , diseases of the respiratory tract , and cancer . the target tissues in those animals were the lungs and associated lymph nodes , liver , and bones . however , these observations in experimental animals have not been corroborated by epidemiological investigations in humans exposed to lower levels of plutonium . as a note , the common myth that plutonium is the “deadliest substance known to man” is not supported by the scientific literature . it poses a hazard but is not as immediately harmful to health as many chemicals . for example , for inhalation – the exposure of highest risk – breathing in 5,000 respirable plutonium particles , about 3 microns each , is estimated to increase an individual’s risk of incurring a fatal cancer about 1% above the u.s. average “background” rate for all causes combined . ) edit : as an aside : i recommend reading eileen welsome 's the plutonium files : america 's secret medical experiments in the cold war to get some idea of what early plutonium health safety experiments really meant ( e . g . injecting a solution of plutonium salt into a patient 's leg ) .
it is because once the higgs couples two weyl fermions together , they become the two chiralities of a massive charged fermion . the standard 4-component spinor formalism disguises how natural this coupling is because it makes every fermion into a dirac fermion , and projects out the unphysical states at every vertex . if you do not do that , if you only include the 2 spinor corresponding to each physical field , the higgs coupling is extremely natural : it is the most general renormalizable gauge-invariant coupling of an su ( 2 ) doublet higgs with hypercharge 1/2 ( in one usual normalization ) to chiral weyl 2-spinors . you can think of a yukawa coupling as a mass term with a scalar field taking the role of the mass . the mass term for a charged fermion has to be of dirac type , because it must be invariant under phase rotations , and only the term $\bar\psi\psi$ is invariant . this means that higgs fields will always couple opposite chiralities of charged massive fermions , i.e. everything in the standard model that couples to the higgs . to see a situation where the coupling is for one chirality only , consider the nonrenormalizable two-higgs two lepton interaction : $$ h h l l $$ where l is the su ( 2 ) doublet left-handed lepton field , h is the higgs field , also an su ( 2 ) doublet , and each l 's su ( 2 ) index is contracted with one of the h 's su ( 2 ) index using the su ( 2 ) epsilon tensor , and the l 's lorentz indices are contracted with each other using the space-time epsilon tensor ( in 2 index formalism ) . this nonrenormalizable term becomes a neutrino majorana mass , and it is suppressed by a large scale , but it is the right order of magnitude to explain the majorana masses . in this case , the two-higgs field is coupled to a single chirality . the allowed couplings are always determined by matching all the su ( 2 ) , su ( 3 ) and lorentz indices together in a 2-spinor formalism , and it becomes the chirality restriction only by coincidence in the standard model . if you had a fundamental su ( 2 ) symmetric tensor higgs field t ( it would be the su ( 2 ) " spin 1" representation , not spin 1/2 ) with twice the hypercharge of the standard model higgs , it could give neutrinos a majorana mass with renormalizable yukawa couplings , just by replacing $hh$ with $t$ above , and contracting the indices the same way .
if you define the speed of your electron in a de broglie way and just set $v = \frac{h}{m\cdot \lambda}$ , you can rewrite that expression to involve the wavenumber $k$ , i.e. $v = \frac{\hbar k}{m}$ . $k$ is constrained by the boundary conditions of the electron . in a large solid with periodic boundary conditions , $k$ can be arbitrarily small , so the particular electron would be arbitrarily slow . however , you do not see these electrons , as , due to the pauli principle , when you start filling up your solid with electrons , you have to go to ever larger values of $k$ . the largest of them is called the fermi wavenumber $k_f$ , which then defines you a fermi velocity $v_f$ which can be quite large . in experiments , you typically interact with electrons at , or close to , the fermi level , since these are the ones with the highest energy and hence the ones you can remove the easiest . therefore , you will not really see the slow electrons . if , on the other hand , you do not consider electrons , which are fermions and thus subject to the pauli exclusion principle , but bosons ( such as ${}^{85}rb$ atoms ) , you do not have to put them all in different quantum states . you are then free to put them all into the $k = 0$ state . indeed , this happens at very low temperatures . such a state is called a bose-einstein condensate . in time-of-flight experiments , it is then indeed observed that they all ( or , at finite temperature , at least most of them ) have essentially a velocity of $0$ .
normal matter structure is entirely constructed from the electronic bindings , so it is in the realm of the possible to engineer how the atoms are binded together exactly and this is the aim of lower-level nanotechnology . however , it is with current technogy and physics , impossible to create complex structures at lower scales ( i.e. : nuclear scale ) . and i do not think that is something that is in principle possible unless some dramatic breakthrough occurs in how we obtain larger-scale nuclear matter , which from the experimental limitations that there are to obtain high z nucleus that might probe the regions of higher islands of stability , one can safely infer that this is a very challenging problem in of itself update : this is not entirely related , but it shows an example of how assumptions as the one i have made above about manipulation of small scales being out of engineering reach can be twisted : this slide about non-homogeneous diffraction crystals shows how to do something that most physicists have thought for long to be essentially impossible ; x-ray and gamma-ray optics
yes , in general physical systems , one can make measurements that split the basis unequally . using normal physical language , we may say the same thing as follows : it is possible to measure an observable $j$ such that the spaces corresponding to the different possible eigenvalues of $j$ have different dimensions . but they are not measurements of a " qubit " . if the information is given in " qubits " , that assumes that the size of the remaining information is independent on the result of the measurement of the " qubit " , so the two sub-bases are equally large . in other words , we assume that the measured qubit is independent from the remaining ones . if it is independent , the hilbert space is isomorphic to $h_{\rm measured}\otimes h_{\rm remains}$ and such a tensor product explicitly has the same dimension of the vectors with $b_{\rm measured}=0$ and $b_{\rm measured}=1$ . to give you an example of a measurement that collapses into one of the spaces of unequal dimensions , consider the measurement of the total angular momentum $j$ of a two-electron system where the angular momentum only arises from the spin . there are various bases on this 4-dimensional space such as $$|\rm up , up\rangle , |\rm up , down\rangle , |\rm down , up\rangle , |\rm down , down\rangle $$ but there is also another natural basis classifying the state according to the total $j^2$ i.e. $j$ and the projection $j_z$ . these are the states written as $|j , j_z\rangle$: $$|0,0\rangle , |1 , -1\rangle , |1,0\rangle , |1,1\rangle$$ the second and fourth state in the last displayed equation are " down down " and " up up " states , respectively , but the first and third state in the last displayed equation are the difference and the sum of " up down " and " down up " , respectively . now , looking at the basis in the last displayed equation , we see that the measurement of $j$ either returns $0$ and " collapses " the state vector into a state in a one-dimensional hilbert space , or it returns $j=1$ and " collapses " it into a state in a three-dimensional hilbert space . any other splitting of a sufficiently large hilbert space dimension is possible , too : one can define observables ( matrices ) whose eigenstate subspaces split the original hilbert space in any imaginable way . however , these measurements are usually not considered in quantum computation where we want to keep the independence of qubits . this is analogous to the statement about classical computers where we could also find out whether a byte is a prime by " one measurement " . however , classical computers usually does not have circuits for such direct " unequal " operations to start with . instead , we do a sequence of classical computer operations ( analogy of unitary transformations ) and reduce the problem to a particular bit is being 0 or 1 at the end .
your brain has mainly 3 tricks for determining how close an object is . pure picture analysis . this is the part where the brain combines perspective and past experience of how big objects should be , to decide how far away something is . this is the only manner of depth perception you have available to you when watching conventional pictures and movies , and it can be toyed with , as in this video by richard wiseman . comparing the perspective of your two eyes . this is the one that becomes active in 3d movies . the easiest way to understand how it works is to close one eye , and bob your head from side to side , while focusing on a single object ( this might make you look like a fool ) . you can see things appear to move relative to one another , and things that are farther back seems to be more stationary . when you have both eyes open , you do not need the bobbing , as you already have input from two different perspectives , and a brain well versed in interpreting it . focal strength . the third way your brain percieves distance is it gauges the lenses in your eye . this in turn becomes a measure of how far away an object is . if you close one eye and hold one object quite close to your open eye , and shift focus from that object to the background immediately behindto it , i can almost promise you that your closed eye moves away from your nose . this is because your brain is anticipating that the lesser focus means you are looking at something farther back , and thus moves the other eye to try to focus on that instead ( even though it can not see anything ) . i have heard that number the reason many people have trouble with headaches etc . after watching 3d-movies , is that because no matter how much the perspective and eye movement tries to fool your brain into thinking that objects are closer and further back , your focal muscles are always focused on a single distance some 50-200 feet away ( the canvas ) . this dissonance makes your brain uncomfortable , and it will have a real physiological effect , just as dissonance between what your eyes see and your inner ear balance organ feels makes you nauseous ( that is motion sickness ) .
i do not know the exact number but i want to support johannes ' claim that the percentage is way smaller by a calculation . most of the light arguably comes from the milky way - especially the strip that gave name to the galaxy . the diameter of the milky way is 100,000-120,000 light years so the median star 's distance is something like 50,000 light years away from us . that is approximately $3\times 10^{9}$ times longer a distance than those 500 seconds for the sun . one must square the distance ratio to get the light power ratio , about $10^{17}$ , between the sun and the typical milky way star . even when $10^{-17}$ is multiplied by the number of stars in the milky way , about $1-4\times 10^{11}$ stars , one gets $1-4$ parts per million of the light , also assuming that the sun is an average-size star . my estimate is 3 orders of magnitude greater than johannes ' but it is still vastly smaller than 0.5% . just to check , sirius is the brightest star in the sky . it is 25 times brighter than the sun but it is 9 light years away , which is $500,000$ times further than the sun . square it and divide 25 by it to get $10^{-10}$ . that is the fraction of the sunlight obtained from sirius . you see that it is much smaller than the result for the generic milky way stars above , so individual bright ( and mostly nearby ) stars are unlikely to topple the statistical estimate . the weakest point of the statistical estimate is that the sun is not quite the average star . one may also check the contribution from other galaxies . there are about $2\times 10^{11}$ galaxies in the universe . however , even if you decide that the average distance from us is 5 billion years only , shorter than half of the age of the universe , it is 100,000 times further than the average milky way star discussed above ( 50,000 light years ) . square it to get $10^{10}$ for the ratio . if you multiply $10^{-10}$ by $10^{11}$ , you actually conclude that the total light from other galaxies is about 10 times greater than the total light from the milky way . but that is probably an overestimate because much of the very distant galactic light is redshifted , absorbed , and the older galaxies may have a lower luminosity . at any rate , it is unlikely that they will drive us above 1/100,000 of the sunlight . finally , instead of trying even more distant stars , let me mention that there is also the moon in the sky . it is actually dominating or almost dominating the luminosity at night , except for the new moon or eclipses . in average , we get 1 milliwatt from the moonlight which is 1/300,000 of the sun 's 342 watts ( averaged over places , seasons , day cycles ) . that is about the same what i got for the total strip of stars in the milky way – 3 parts per million of the sun – but my estimate of the stars was probably an overestimate and i believe the moon is brighter than the milky way combined .
at certain positions in the waves , the em field is zero and thus zero energy is stored at those positions . but at other positions , the em field is at a maximum , and those points are local maxima of energy . that pattern of oscillation between zero energy and maximum energy moves in the direction of propagation of the wave but never changes - in particular , the maximum value of the em field ( the amplitude ) stays constant , and there is no time at which the em field is zero everywhere . as for your conclusion from the definition of the poynting vector that the energy disappears at certain times : it is not correct , but i could not tell you why without seeing how you did it . what i can do is show the calculation for an electromagnetic plane wave , defined by $$\vec{e} ( z , t ) = e_0\hat{x}\sin ( kz - \omega t ) $$ the corresponding magnetic field is $$\vec{b} ( z , t ) = \frac{1}{c}\hat{k}\times\vec{e} ( z , t ) = \frac{e_0}{c}\hat{y}\sin ( kz - \omega t ) $$ since i am setting the direction of propagation as $\hat{k} = \hat{z}$ . check that this satisfies maxwell 's equations if you want . the energy density is $$\begin{align} u ( z , t ) and = \frac{\epsilon_0}{2}e ( z , t ) ^2 + \frac{1}{2\mu_0}b ( z , t ) ^2 \\ and = \frac{\epsilon_0}{2}\biggl ( e_0\hat{x}\sin ( kz - \omega t ) \biggr ) ^2 + \frac{1}{2\mu_0}\biggl ( \frac{e_0}{c}\hat{y}\sin ( kz - \omega t ) \biggr ) ^2 \\ and = \epsilon_0 e_0^2\sin^2 ( kz - \omega t ) \end{align}$$ using $\frac{1}{c^2} = \epsilon_0\mu_0$ . this energy density does vary from point to point , but at any fixed time , if you take the average over one cycle , a length $\frac{2\pi}{k}$ , you get $$k\int_0^{2\pi/k} u ( z , t ) \mathrm{d}z = k\int_0^{2\pi/k} \epsilon_0 e_0^2\sin^2 ( kz - \omega t ) \mathrm{d}z = k\epsilon_0 e_0^2 \frac{\pi}{k} = \pi\epsilon_0 e_0^2$$ which does not depend on time . so the average energy density is constant , it does not ever go to zero .
the paper absorbs water , and the adhesion energy is , per molecule , much stronger than the pull of gravity . you can make water climb up capillaries as far as the top of a tree from the bottom of the trunk , so it is not difficult to get the water to soak the paper against gravity . the paper has a large surface area , so it probably evaporated the water into the air . i do not believe it actually acts as a pump , like a siphon , to transfer water onto the surface of the radiator absent evaporation , because the water would have to detach at the radiator end in a continuous stream for this to work , setting up an actual steady state material flow , but it is an interesting question .
why they are not on the geostationary orbit ? it is because the geostationary orbit - and indeed , there is just one such orbit , a one-dimensional curve in space - only exists above the equator while the gps has to cover the whole planetary surface , including the points closer to the poles . however , it is untrue that the gps satellites are located at the leo , either . low earth orbit is defined as 160-2000 kilometers of altitude . however , the gps satellite constellation is located roughly 20,200 km above the surface - over one half of the geostationary radius - in such a way that the position of each satellite returns to the same place twice per 24 hours . this is very convenient for synchronization and planning . the galileo satellites will be at altitude 23,222 km . it is also an intermediate circular orbit , much like for the 19,100 altitude of the glonass whose orbital period is about 11 hours .
actually that is not too far off the mark , although i am not sure " knot " or " kink " is the best word . quantum field theory , the best theory we currently have to describe particles , says that particles correspond to excitations of a field , which are kind of like waves in water ; you could consider the surface of a pond " excited " whenever it is not flat . just as with water waves , there is an infinite variety of " shapes " you can have for these excitations . for example , you could have a repeating wave , in which the surface of the water cycles up and down over a large area , or you could have just one wave front that just propagates across the water without spreading out very much . the former case is pretty typical for things like light waves , and the latter case is pretty typical for particles of matter , although technically any kind of field ( whether it is the electromagnetic field for light , or a quark field in matter , or whatever ) can have any of the different types of excitations . by the way , according to special ( and general ) relativity , even an object that is standing still is moving through time . so all of these excitations move through spacetime in one way or another . but only certain ones ( the excitations in fields corresponding to massless particles ) can move through space in such a way that they appear to us to be traveling at the speed of light .
yes , if it is not a plastic covered car it is an effective faraday cage . if the tires are such that the car is insulated electrically , if it is hit it will take some time to discharge to the ground , but still the passengers would be safer than standing next to it outside . i have learned that modern tires are particularly constructed so that the static charge generated by the friction on the road is discharged so that would also help . ( in olden times they used to have chains trailing from the trucks in order to discharge the static . recently i saw a car with a discharger too , trailing on the road ! ) . lightning is essentially just an huge electric arc from the clouds to the ground , correct ? wrong , the current actually may start from the ground . that is the rational of the lightning rods , to create a path for a current to be generated by the potential difference to the cloud and to meet the current from the clouds in a prefered location instead of a random one . it is not wise to stand next to a rod , read in the link the amount of power dissipated by a bolt . the average peak power output of a single lightning stroke is about one trillion watts — one " terawatt " ( 1012 w ) , and the stroke lasts for about 30 millionths of a second — 30 " microseconds " . [ 18 ] and it is not wise to stand , because you may also give rise to leaders that will meet the lightning path . if in the open it is best you fall on the ground as much sheltered as possible . a colleague once was about 20 meters from a lightning bolt , and he was so shocked by the sound and fury , it took him a week to come down to normal .
i think you are sort of reversing the logic of chirality and helicity in the massless limit . chirality defines which representation of the lorentz group your weyl spinors transform in . it does not ' become ' helicity , helicity ' becomes ' chirality in the massless limit . that is , chirality is what it is , and it defines a representation of a group and that can not change . this other thing we have defined called helicity just happens to be the same thing in a particular limit . now once you take the massless limit the weyl fermions are traveling at the speed of light you can no longer boost to a frame that switches the helicity . i think its best to think of a fermion mass term as an interaction in this case and remember that the massive term of a dirac fermion is a bunch of left and right- handed weyl guys bumping up into one another along the way . conversely if you want to talk about a full massive dirac fermion that travels less than c and you can boost to change the helicity , but that full dirac fermion is not the thing carrying weak charge , only a ' piece ' of it is . see this blog post on helicity and chirality . as far as the left-right symmetry being broken people have certainly built models along these lines but i do not think they have worked out . does this answer your question ?
even if you change frames , the physics is still the same and the particle will follow the same path , no ? and there is certainly more than one way to change the lagrangian without affecting the path of least action--add any combination of total time derivatives to it . when i said it would follow the same path , i meant the same path after you take into account the fact that you shifted frames . if $q_1$ and $q_2$ label the same point even after you shift frames ( so that in new coordinates $q_1=q^{new}_1−ϵt$ and etc . ) then the particle will be at $q_2$ at $t_2$ if it was at $q_1$ at $t_1$ . i mean that if the particle starts at time $t_1$ at $q_1$ it does end up at $q_2$ at time $t_2$ provided you take into account how the points look different because of the new frame . the path taken by the particle must be the same ; physics does not depend on the inertial frame you are in and this is the point landau is making . if you think that in frame k ' the particle does not end up at $q_2$ at time $t_2$ then it is purely because the points are labeled differently in this frame . it also has nothing to do with v being infinitesimal ; that does not matter . as for the other question , you can also multiply the whole lagrangian by a constant . that is kind of obvious though . basically you need the kinetic energy term , and the only way you can further modify it is by adding terms right ? if you multiplied the lagrangian by a non-constant term , for instance , the form of the kinetic energy term would change . then he rules out what terms you can add . maybe if you go to this page and look under the section " is the lagrangian unique ? " it will help : en . wikibooks . org/wiki/classical_mechanics/lagrange_theory basically you change the lagrangian by adding terms , and it can be proved that only if this term is a total time derivative of a function of coordinates and time that the action is still extremized . in other words , no , it is not possible to keep the path of least action by adding any other function .
the integrals you have to compute are not that difficult . first of all , the spatial parts of your wavefunctions are real ( this is wise and always possible in one dimension ) , and therefore complex conjugates just do not matter . so , the remaining integral you have on the rhs is ( up to some expression dependent on t ( s ( t ) ) which we will leave for now ) : $$ i ( t ) = s ( t ) \int_0^a\phi_n ( x ) \phi_m ( x ) dx = s ( t ) \frac{2}{a} \int_0^a \operatorname{sin}\big ( \frac{n\pi x}{a} \big ) \operatorname{sin}\big ( \frac{m\pi x}{a} \big ) dx$$ now , you have to make a substitution $ t = \frac{\pi x}{a} $ to get $$ i ( t ) = s ( t ) \frac{2}{\pi} \int_0^\pi \operatorname{sin} ( n x ) \operatorname{sin} ( m x ) dx$$ first of all , if either n or m are 0 then i ( t ) = 0 and your initial state is normalized . now you integrate by parts ( or by mathematica if you can not by parts ) to obtain $$ i ( t ) = s ( t ) \frac{2}{\pi} [ - ( \frac{1}{m} \sin{n x} \cos{m x} ) |_0^\pi + \frac{n}{m} \int_0^\pi \cos{n x} \cos ( {m x} ) dx ] $$ the first term is 0 ( cause sin is 0 in all multiplicities of $\pi$ ) . now we can proceed with integration by parts of the second term . $$ i ( t ) = s ( t ) \frac{2}{\pi} ( \frac{n}{m} \int_0^\pi \cos{n x} \cos ( {m x} ) dx ) = s ( t ) \frac{2}{\pi} [ ( \frac{n}{m^2} \cos{n x} \sin{m x} ) |_0^\pi $$ $$ + \frac{n^2}{m^2} \int_0^\pi \sin{n x} \sin ( {m x} ) dx ] = \frac{n^2}{m^2} i ( t ) $$ so , there are two possibilities - either n=m or i ( t ) =0 ( n=-m is excluded as negative labels are not in the basis because sin ( -nx ) = -sin ( nx ) - linear dependency ) . what you get is orthogonality relation - if eigenvectors have different labels ( e . g . 1 and 2 as in your case ) they are orthogonal in $l^2 ( [ 0 , a ] ) $ which happens to be your hilbert space . there are theorems which state that this is always true for hamiltonian eigenstates - corresponding theory of differental operators is known as strum-liouville theory ( for finite dimensional hilbert spaces this is a trivial property of self-adjoined operators ) . now , to the second part of your question . the modulus squared of wavefunction is by definition probability density of finding a particle in an interval $dx$ so , the probability of finding particle in [ 0 , a/2 ] is simply $$\int_0^{a/2} |f ( x , t ) |^2 dx = \frac{1}{2}\int_0^{a/2} |\phi_1 ( x ) |^2 + |\phi_2 ( x ) |^2 dx + $$ $$ \frac{1}{2}\int_0^{a/2} \phi_1 ( x ) \phi_2 ( x ) \operatorname{exp} \big ( ( -1^2+2^2 ) \frac{-i\pi^2 h t}{2ma^2} \big ) + \phi_2 ( x ) \phi_1 ( x ) \operatorname{exp} \big ( ( -2^2+1^2 ) \frac{-i\pi^2 h t}{2ma^2} \big ) $$ $$ = \frac{1}{2}\int_0^{a/2} |\phi_1 ( x ) |^2 + |\phi_2 ( x ) |^2 dx + \cos{\frac{3\pi^2 h t}{2ma}} \int_0^{a/2} \phi_1 ( x ) \phi_2 ( x ) dx $$ first integral is simply 1 because we take half of the area of $sin ( x ) ^2$ and $sin ( 2x ) $ , and add it . second integral can be easily computed using double ' by parts ' formula from above and replacing $\pi$ with $\pi/2$ in integral limits . one gets $\frac{4}{3 \pi}$ therefore finally $$\int_0^{a/2} |f ( x , t ) |^2 dx = \frac{1}{2}+\frac{4}{3 \pi} \cos{\frac{3\pi^2 h t}{2ma}} $$ as $\frac{4}{3 \pi} &lt ; \frac{1}{2}$ the result makes sense . as the body is in mixture of two states , the probability is no longer constant in time .
actually , there are two different viscosity coefficients . you can see this from the stress tensor $$ \sigma_{ij} = -p_0 \delta_{ij} + \eta \left ( \frac{\partial v_i}{\partial x_j} + \frac{\partial v_j}{\partial x_i} - \frac{2}{3} \delta_{ij} \frac{\partial v_k}{\partial x_k} \right ) + \zeta \delta_{ij} \frac{\partial v_k}{\partial x_k} $$ which has the two coefficients of viscosity $\eta$ and $\zeta$ ( see landau and lifshitz , fluid mechanics , for example ) . the pressure $p_0$ is given by the thermodynamic equation of state , but this is not the whole pressure $p$ . the latter is given by the mean normal stress $$ p = - \frac{1}{3} \sigma_{ii} = p_0 - \zeta \frac{\partial v_k}{\partial x_k} $$ so that the stress tensor is $$ \sigma_{ij} = -p \delta_{ij} + \eta \left ( \frac{\partial v_i}{\partial x_j} + \frac{\partial v_j}{\partial x_i} - \frac{2}{3} \delta_{ij} \frac{\partial v_k}{\partial x_k} \right ) . $$ that is why sometimes you do not see the coefficient $\zeta$ ( often called second viscosity ) in the navier-stokes equation . it is hidden in the pressure , but it is there .
how galilean transformations which are wrong ( are approximately correct ) give the correct answer for k ? the lorentz prediction and the galilean prediction must agree in the limit that $v \to 0$ ( or in the limit that $c \to \infty$ ) . this is because $v=0$ corresponds to no transformation at all , so they had better both agree there . so if you take the transformation and evaluate it for smaller and smaller $v$ , you will find that $k=1$ still has to be true . why we should assume that there are two electric fields , one in the lab frame and one in the other , but just one magnetic field in both frames ? that is just the galilean transformation of the em field . to see how it relates to the relativistic case , the lorentz transformation of the em field is : $$\mathbf{e}' = \gamma \left ( \mathbf{e} + \mathbf{v} \times \mathbf{b} \right ) - \left ( {\gamma-1} \right ) ( \mathbf{e} \cdot \mathbf{\hat{v}} ) \mathbf{\hat{v}}$$ $$\mathbf{b}' = \gamma \left ( \mathbf{b} - \frac {\mathbf{v} \times \mathbf{e}}{c^2} \right ) - \left ( {\gamma-1} \right ) ( \mathbf{b} \cdot \mathbf{\hat{v}} ) \mathbf{\hat{v}}$$ when you take the limit that $c \to \infty$ , we know that $\gamma \to 1$ , so it just becomes : $$\mathbf{e}' = \mathbf{e} + \mathbf{v} \times \mathbf{b}$$ $$\mathbf{b}' = \mathbf{b}$$
spacetime is infinitely stretchable - in fact at the singularity in the centre of a black hole the spacetime curvature becomes infinite ( though quantum gravity probably intervenes before the curvature actually becomes infinite ) . spacetime cannot " snap " or " rupture " no matter how heavy the masses involved . i can not think of anything in general relativity that matches your description . just possibly something like this may happen in quantum gravity . see http://en.wikipedia.org/wiki/quantum_foam for more info , though note that the idea of a spacetime foam is purely speculative at the moment . i believe string theory allows topology change to occur , and i suppose this could be viewed as a " tearing " of spacetime . however i do not know enough about this area to comment .
for complete treatement , see [ this reference ] ( http://preposterousuniverse.com/grnotes/grnotes-seven.pdf ) page 172 ( formula 7.32 ) and following pages . the idea is to use an affine parameter $\lambda$ , such as : $$g_{\mu\nu} \frac{dx^{\mu}}{d\lambda} \frac{dx^{\mu}}{d\lambda} = - \epsilon$$ ( in a metrics $g = ( -1,1,1,1 ) ) $ for massive particles , you can choose $\lambda = \tau$ , which is the proper time of the particle , so $\epsilon = - 1$ for massless particles , $\lambda$ is different of $\tau$ , because $d\tau=0$ , in this case , you have $\epsilon = 0 $ . so you can make all the calculus with this $\epsilon$ , for instance , you will have an effective potential as : $$v ( r ) = \frac{1}{2} \epsilon - \epsilon \frac{gm}{r} + \frac{l^2}{2r^2} - \frac{gml^2}{r^3} $$ ( page 174 formula 7- 48 of the reference ) page 176 of the reference , you will see the different orbits for massive and massless particles .
i would expect metals to cool faster through convection because of a related heat property of theirs : conduction . metals are generally good heat conductors so as heat energy is removed from the surface of the metal by air , internal heat energy in the metal can quickly flow to the surface . the rate heat flows between two systems is dependent on their temperature difference . when air removes heat via convection , the surface of the object is cooler than the central portion of the object . if heat in the object flows slowly from the center to the surface like in many plastics then the surface stays closer to air temperature and heat transfers to the surrounding air more slowly . if the object is a metal , the heat removed at the surface is easily replaced by heat flowing from the center to the surface and the temperature delta between the surface and the air is greater so the object cools faster .
as brandon mentioned , two small objects could not orbit each other near a significant gravitational field . the hill sphere " approximates the gravitational sphere of influence of a smaller body in the face of perturbations from a more massive body . " therefore , your pebble 's hill sphere would be too small to permit orbits near earth . the wiki article has a calculation showing that an astronaut could not orbit the 104 tonne space shuttle 300 km above the earth since the shuttle 's hill sphere was only 120 cm .
to answer briefly , i am not very confident in this answer and invite editing or downvoting as appropriate ! . the energy levels of the electrons are determined via calculations of the electric potential/field around the nucleus . the electric field is a vector field that transforms under special relativity and hence we account for any relativistic effects through moderation of this field .
the net resistance depends on the point where you have applied the potential difference . indicating the direction of current is very useful . you can use $ [ $ $ ( $ $r_3$ series $r_4$ $ ) $ parallel $r_2$ $ ] $ to be $r_7$ now redraw the circuit and by naming the point with same potential as one point as i have done in my diagram . helps a lot : seems like i have made an error in direction of current in $r_7$ . it should be opposite . but it matters not as if you apply kirchoff 's law the current will come out to be negative .
first normalize the state to find $a$ . then you need to express the state as a superposition of the stationary states of the infinite square well : $$ \psi\left ( x\right ) = a x \left ( a-x\right ) = \sum_{n=1}^\infty c_n \psi_n\left ( x\right ) , $$ where $\psi_n\left ( x\right ) = \sqrt{2/a} \sin\left ( n \pi x / a\right ) $ is the $n$-th stationary state . you can do this using the orthogonality of the stationary states , $$ \int_0^a dx \ \psi^*_m\left ( x\right ) \psi_n\left ( x\right ) = \frac{2}{a} \int_0^a dx \ \sin\left ( \frac{m \pi x}{ a}\right ) \sin\left ( \frac{n \pi x}{ a}\right ) = \delta_{mn} , $$ by integrating the equation above : $$ \begin{align} \int_0^a dx \ \psi^*_m\left ( x\right ) \left [ a x \left ( a-x\right ) \right ] and = \int_0^a dx \ \psi^*_m\left ( x\right ) \left [ \sum_{n=1}^\infty c_n \psi_n\left ( x\right ) \right ] \\ and = \sum_{n=1}^\infty c_n \left [ \int_0^a dx \ \psi^*_m\left ( x\right ) \psi_n\left ( x\right ) \right ] \\ and = \sum_{n=1}^\infty c_n \delta_{m n} \\ and = c_m \end{align} $$ i will leave the $c_n = a \sqrt{2/a} \int_0^a dx \ \sin\left ( n \pi x / a\right ) x \left ( a-x\right ) $ integral for you to work out . once you have the $c_n$ 's , the most likely value of a measurement of the energy is the energy corresponding to the stationary state with maximum $c_n$ . to find the probability of measuring $9 \hbar^2 \pi^2 / 2 m a^2$ for the energy , determine the stationary state that this energy corresponds to , and compute $\left|c_n\right|^2$ . for the time evolution , since the potential is $0$ everywhere after $t=0$ , it is a free particle , and the general solution is : $$ \psi\left ( x , t\right ) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty dk \ \phi\left ( k\right ) \exp\left [ i\left ( k x + \frac{\hbar k^2}{2 m} t\right ) \right ] , $$ where $$ \phi\left ( k\right ) = \frac{1}{\sqrt{2 \pi}} \int_0^a dx \ \psi\left ( x , 0\right ) \exp\left ( -i k x\right ) = \frac{a}{\sqrt{2 \pi}} \int_0^a dx \ x\left ( a-x\right ) \exp\left ( -i k x\right ) . $$ so , now you just have to do this integral .
the sign of the gauge part of the covariant derivative is a convention , you can choose it any way you want , it just defines the sign of a . this sign has nothing to do with the metric convention , mostly + or mostly - . its arbitrary in either convention . the second part is just differentiating both sides of the previous equation for $d_0\phi$ , there is a t in the right hand side . so it is $\partial_0 ( t d_0 \phi ) $ , since a_0 is infinitesimal and gives a higher order correction , and he keeps the first part of this , where you differentiate t with respect to t , and ignores the second part , since time derivatives of $\phi$ are small by assumption that the monopole is stationary at t=0 and slowly accelerating .
classically a non-pointlike spinning charged object possesses a magnetic dipole moment due to the fact that charged particles in the object are spinning around some axis . in contrast , the electron has a dipole moment that arises from its intrinsic spin angular momentum . as you point out , the electron has no internal structure , so the spin does not refer to actual physical spinning . the dipole moment has spatial dimensions outside of the point where the electron exists because it arises from the quantum spin property of the electron , it is not itself a property of the electron . the magnetic dipole moment of the electron is related to its spin in the way described here .
the idea is just to make use of the relationship between luminosity ( the amount of energy emitted per second from the star - in other words , the power ) and flux ( the amount of power hitting the surface ) . because , the flux can be modeled as a large number of ( imaginary ) spherical energetic wavefronts emerging from the star in 3-d space as a function of time . also , this flux is based on the inverse-square law . so , it decreases with distance ( squared ) . and since the power distributed over every sphere is still the same , both are related by the area of spheres . $$\mathrm{flux=\frac{luminosity}{4\pi r^2}}$$ and since the question says that the astronomer idealizes it as a blackbody ( which we always do ) , we can use the stefan-boltzmann equation and say that the flux is $\sigma t^4$ . . . edit for confusion : simply relating the given flux with stefan-boltzmann only gives the temperature of the imaginary sphere at the distance at which the flux was measured . first , you can find the luminosity of the star by using the relation above . then , you need the flux at the surface of the star . given the radius of star , it can be used to determine how much power is transmitted to the surface through the luminosity . finally on relating with $\sigma t^4$ gives the answer . . .
you do not need gauss ' law . coulomb 's law will do the trick here along with some calculus .
i assume you have no qualms with the " large $\xi$" approximation - it is fairly obvious that $\xi^2-k^2\approx \xi^2$ for large enough $\xi$ . after that you are left with the differential equation $$\frac{d^2\psi}{d\xi^2}\approx-\xi^2\psi . \tag1$$ one way to solve this equation is by the method of divine inspiration : you somehow come up with two linearly independent functions you can write down which solve the equation , after which you know what the general solution is . however , the two functions that griffiths poses are not exact solutions of that equation , as you would know if you had done your proper diligences . indeed , $$ \frac{d^2}{d\xi^2}\left [ e^{\pm \xi^2/2}\right ] =\frac{d}{d\xi}\left [ \pm\xi e^{\pm \xi^2/2}\right ] = ( \xi^2\pm1 ) e^{\pm\xi^2/2} . $$ this means that we are looking for a solution that is approximately valid for the ( already approximate ) equation ( 1 ) . as for how one might get such solutions , there is obviously a myriad different possible paths . one really nice way of deriving the solutions is to factor the offending second-order differential operator into two different first-order operators : $$ \left ( \frac{d}{d\xi}-\xi\right ) \left ( \frac{d}{d\xi}+\xi\right ) \approx \left ( \frac{d}{d\xi}+\xi\right ) \left ( \frac{d}{d\xi}-\xi\right ) \approx \frac{d^2}{d\xi^2}-\xi^2 . $$ these are of course approximate inequalities , and of course you must work these out to see what terms got dropped and why . after that , you can simply work out solutions to the two equations $\left ( \frac{d}{d\xi}-\xi\right ) \psi=0$ and $\left ( \frac{d}{d\xi}+\xi\right ) \psi=0$ , which are in fact the solutions given by griffiths . these are first-order equations and therefore simply solvable by integrating . the ( approximate ) equalities above guarantee that a function in the kernel of either factor will be ( approximately ) in the kernel of the operator you do care about . of course , these factors are far from trivial inventions , and they are at the heart of the operator approach to solving the simple harmonic oscillator .
perhaps this is just rephrasing your last explanation , so i am not sure if you consider this as a " better argument " , but i will give you a good reference for further reading . quantum gravity may break global symmetries because the global charge can be eaten by virtual black holes or wormholes , see this paper .
wall crossing is any discontinuous change of an integer ( or at least rational ) quantity - or , more generally , any qualitative change of the spectrum etc . - that occurs when one moves to the opposite side from a " wall " in a moduli space or parameter space . a would-be index may suddenly change discontinuously . the wall - a codimension one locus in the parameter space - is then referred to as the " wall of marginal stability " . one one side , an object may be stable while it is unstable on the other side . it is typically unstable because a decay suddenly becomes plausible because the hypothetical decay product get light enough , if you wish . the objects are typically bps objects on the stable side and they can also be bps black holes or any other bps objects . to see objects that may be exactly stable , bps , but that are also sufficiently diverse , $n=2$ supersymmetry or eight supercharges is an ideal number of supercharges . that is why those considerations played an important role in the seiberg-witten insights about $n=2$ gauge theories , among related situations . paul aspinwall liked to say that 8 supercharges is the optimum number for nice physics . only nature did not manage to choose the number 8 for supercharges , but that is not aspinwall 's fault .
the reduced matrix is defined as the partial trace of the density matrix . be $a$ , $b$ finite dimensional hilbert spaces and be $t$ $\in$ $l ( a \otimes b ) $ ( linear operators on $a \otimes b$ ) , then the partial trace of t is defined as $\rm{tr}_b [ t ] $ in $l ( a ) $ is defined by \begin{equation} \langle a | \rm{tr}_b [ t ] | b \rangle = \sum_n \langle a | \langle n | t| n\rangle | b \rangle \end{equation} where $| n \rangle$ is an orthonormal basis in $b$ . finally , note that the reduced matrix is not the correct way of the describing a quantum state , is just a way to describe it as seen by looking just at a subsistem . this usually involves ignoring part of the information of the state and therefore the reduced density matrix of a pure state may be a mixed state . this is spectacular for the bell states , as their reduced matrix is $\rm{id}/2$ , the most disoredered state .
here is a table i made for you listing the elements with a density higher than 10 g/cm$^3$ and their approximate price per kg : i could not find any prices for einsteinium or actinium and some of the other prices might come from poor sources , but take it as a rough guide . now you only have to figure out how much you need and your budgetetary constraints , and choose the densest you can afford . as i have learned from the political debate in the us , teachers are apparently raking in big cash , so i suggest you go with osmium or rhenium . note : some of these might be unsuitable/infeasible for other reasons than their price .
jackson starts from $$\tag{1} \frac{d}{dx}\big [ ( 1-x^2 ) \frac{dp}{dx}\big ] +l ( l+1 ) p=0 $$ substitution of $p ( x ) =x^\alpha\sum_{j=0}^\infty a_j x^j$ in ( 1 ) will give : $$\tag{2} \frac{dp}{dx}=\frac{d}{dx}\sum_{j=0}a_jx^{\alpha+j}=\sum_{j=0} ( j+\alpha ) a_jx^{\alpha+j-1} $$ $$\tag{3} ( 1-x^2 ) \frac{dp}{dx}=\sum_{j=0} ( j+\alpha ) a_jx^{\alpha+j-1}-\sum_{j=0} ( j+\alpha ) a_jx^{\alpha+j+1} $$ $$\tag{4} \frac{d}{dx}\big [ ( 1-x^2 ) \frac{dp}{dx}\big ] =\\ =\sum_{j=0} ( j+\alpha ) ( j+\alpha-1 ) a_jx^{\alpha+j-2}-\sum_{j=0} ( j+\alpha ) ( j+\alpha+1 ) a_jx^{\alpha+j} $$ putting all together in ( 1 ) you get : $$\tag{5} \sum_{j=0} ( j+\alpha ) ( j+\alpha-1 ) a_jx^{\alpha+j-2}-\sum_{j=0} ( j+\alpha ) ( j+\alpha+1 ) a_jx^{\alpha+j}+ \\+l ( l+1 ) \sum_{j=0}a_jx^{\alpha+j}=0 $$ which gives jackson 's equation . $$\tag{6} \sum_{j=0}\{ ( j+\alpha ) ( j+\alpha-1 ) a_jx^{\alpha+j-2}- [ ( j+\alpha ) ( j+\alpha+1 ) -l ( l+1 ) ] a_jx^{\alpha+j}\}=0 $$ the two terms in the sum will have the same order when in the first "$j=j+2$" and this will give the relation you are looking for . indeed : $$\tag{7} ( j+\alpha+2 ) ( j+\alpha+1 ) a_{j+2}x^{\alpha+j}- [ ( j+\alpha ) ( j+\alpha+1 ) -l ( l+1 ) ] a_jx^{\alpha+j}=0 $$ and $$\tag{8} a_{j+2}=\frac{ ( j+\alpha ) ( j+\alpha+1 ) -l ( l+1 ) }{ ( j+\alpha+2 ) ( j+\alpha+1 ) }a_j $$
the reason is that the exposure on the camera is set so that the main subject of the image is properly exposed , ie not too dim and not too bright . because the typical objects being photographed are quite bright , the image detector ( camera ) will not get enough light from the stars for them to show up .
it seems right . you have $$ \frac{\partial w^{1/2}}{\partial x^\sigma}=\frac1{2\sqrt{w}}\frac{\partial w}{\partial x^\sigma} $$ change the order of the derivatives in the second term $$ \frac{d}{d\lambda}\frac{\partial w^{1/2}}{\partial \dot{x}^\sigma}=\frac{\partial}{\partial \dot{x}^\sigma}\left ( \frac{d}{d\lambda} w^{1/2}\right ) =\frac{\partial}{\partial \dot{x}^\sigma}\left ( \frac1{2\sqrt{w}}\frac{d w}{d\lambda } \right ) $$ product rule $$ \frac{\partial}{\partial \dot{x}^\sigma}\left ( \frac1{2\sqrt{w}}\frac{d w}{d\lambda } \right ) =-\frac1{4w\sqrt{w}}\frac{\partial w}{\partial \dot{x}^\sigma}\frac{dw}{d\lambda}+\frac1{2\sqrt{w}}\frac{\partial }{\partial \dot{x}^\sigma}\frac{dw}{d\lambda} $$ subtract , equate to zero , multiply $2\sqrt{w}$ you get $$ \frac{\partial w}{\partial x^\sigma}+\frac1{2w}\frac{\partial w}{\partial \dot{x}^\sigma}\frac{d w}{d\lambda}-\frac{d}{d\lambda}\frac{\partial w}{\partial \dot{x}^\sigma}=0 $$
you can prove it with a little trick . for a particle on a circle of radius $r$ we have $\vec{x} ( t ) \cdot\vec{x} ( t ) =r^2$ at each point in time . differentiating with respect to time we get $\vec{x}\cdot\vec{v}=0$ . differentiating again we get $v^2+\vec{x}\cdot\vec{a}=0$ . but $\vec{x}\cdot\vec{a} = - r a_{\mathrm{rad}}$ , and the relation $a_{\mathrm{rad}} = v^2/r$ follows .
take a look at this article . the authors present an elementary explanation for the two-to-one ratio of wobble to spin frequencies . update : in case you do not have easy access to the ajp , here it is : article .
more clues ? :- ) this is harder then the isobaric process because now the pressure is a function of volume . you need to write the pressure as a function of volume , then integrate it from the initial to final volume . for some clues see the wikipedia article on adiabatic expansion . although the question does not say so , you will need to assume the expansion is reversible as the question can not be answered otherwise .
the electric field of a negative point charge points towards the point charge as a result of the definition of the electric field of a point charge . to see this , recall that the electric field of a point charge $q$ is defined as $$ \mathbf e = \frac{1}{4\pi\epsilon_0}\frac{q}{r^2}\mathbf e_r $$ where , $r$ is the distance to the charge , and $\mathbf e_r$ is the outward pointing radial unit vector field emanating from the location of the charge . if $q$ is negative , then we see that the electric field points in the direction of $-\mathbf e_r$ , and therefore it points radially inward . you might then ask " well this is fine , but why is the electric field defined in this way ? could not we have defined the electric field in such a way that the electric field due to a negative point charge points radially outward ? " well , the definition of the electric field is motivated by coulomb 's law , the empirical fact that the electrostatic force exerted by a point charge $q_1$ on a point charge $q_2$ is $$ \mathbf f_{12} = \frac{1}{4\pi\epsilon_0}\frac{q_1q_2}{r^2}\mathbf e_{12} $$ where $\mathbf e_{12}$ is the unit vector pointing from charge 1 to charge 2 . the idea in defining the electric field is that we want to associate a vector field ( which we call the electric field ) to each point charge individually such that if we multiply that field by any other charge ( which is usually called a test charge ) , then we obtain the force that would be exerted on the test charge due to the original charge . if we factor $q_2$ out of the right hand side of coulomb 's law , then the rest of the stuff is precisely such a vector field associated to the charge $q_1$ $$ \mathbf f_{12} = q_2\underbrace{\left ( \frac{1}{4\pi\epsilon_0}\frac{q_1}{r^2}\mathbf e_{12} \right ) }_{\text{stuff that only depends on charge 1}} $$ so we define the electric field as the stuff in parentheses . notice , however , that we could just as well have factored out $-q_2$ in which case the electric field would have been opposite in sign to the conventional definition , and in this case , the electric field of the negative charge would have pointed radially outward by definition .
you seem a bit confused on your standing conditions , so we will go at this step by step . let 's start with this statement : the magnet does not move from this , we can easily deduce that $v=0$ now , $v=ir$ , and we have $r=0$ as well . thus , $0=i\times 0$ , and then $i$ can be any value . ( in case of a non-perfect superconductor , we have $r \approx 0$ , and we can derive $i=0$ , since $0=ir$ and $r\neq 0$ ) ok , so you got a constant current . what of it ? the current will only be able to course forever in the superconductor ring . you cannot try to draw energy off it , because all ways of drawing energy off current increase the resistance . if you add a load coil/etc , there will obviously be resistance . if you try to harness its magnetic field , the fluctuations in the field will induce a reverse current on the coil , acting as a resistance . if amps = volts / ohms , and ohms is 0 , it seems like amps should be infinity . you completely forgot the other alternative--volts can be zero instead . then you can easily see that the current can be any value .
firstly , i do not think your convention for the derivative is consistent with how you lower the index on $\lambda^b$ . let us start with $$ \frac{\partial}{\partial \lambda^b} \lambda^a = \delta_b^a $$ if you lower the index on $\lambda^b$ you get $$ \lambda_a = \epsilon_{ac} \lambda^c $$ putting this together gives $$ \frac{\partial}{\partial \lambda^b} \lambda_a = \epsilon_{ac} \frac{\partial}{\partial \lambda^b} \lambda^c = \epsilon_{ab} = -\epsilon_{ba} $$ which has a different sign from what you have . furthermore , the spinor bracket you write is lorentz invariant if you transform both spinors at the same time . the actions of $j^a$ and $j^b$ on $\langle ab \rangle = \epsilon_{cd} a^c b^d$ is given by \begin{align} j_{ab}^a\ , \epsilon_{cd} a^c b^d and = \left ( a_a \frac{\partial}{\partial a^b} + a_b \frac{\partial}{\partial a^a} \right ) \ , \epsilon_{cd} a^c b^d \\ and = \epsilon_{cd} \left ( a_a \delta_b^c + a_b \delta_a^c \right ) b^d = + ( a_a b_b + b_b a_a ) \\ % j_{ab}^b\ , \epsilon_{cd} a^c b^d and = \left ( b_a \frac{\partial}{\partial b^b} + b_b \frac{\partial}{\partial b^a} \right ) \ , \epsilon_{cd} a^c b^d \\ and = \epsilon_{cd} a^c \left ( b_a \delta_b^d + b_b \delta_a^d \right ) = - ( a_a b_b + b_b a_a ) \end{align} so if you sum them the transformation of $a$ is cancelled by the transformation of $b$ .
quite a simple answer : scattering of light ( rayleigh scattering would be more precise here . . . ) an observer in ground sees the sky as blue due to scattering of light by air molecules present in the atmosphere . for an observer in space , the water bodies reflect the color of sky . . . the water bodies ( ocean , lakes , river ) appear blue ( 'cause water is quite colorless ) because of the way sunlight is selectively scattered as it goes through our atmosphere . taking raman effect into account , water absorbs more of the red light in sunlight . by this way , water also enhances the scattering of blue light in the surroundings . by rayleigh scattering law : ( it is more important here ) the amount of scattering is inversely proportional to the fourth power of its wavelength . due to the larger amount of $n_2$ and $o_2$ molecules ( 78% and 21% ) in the atmosphere , blue light which is having shorter wavelength is scattered to a greater extent . thus , the earth would not be blue if it does not have enough $o_2$ and $n_2$ molecules in its atmosphere . the scattering depends on the characteristics of gaseous molecules in atmosphere . . . this is applicable to other planets also . ( like mars appearing red , venus appears yellow , etc . )
three-phase has two main reasons to exist : driving n . tesla 's polyphase induction motors used throughout industry . reducing the total cost of metal in cross-country power lines : w/single-phase lines , more metal would be needed to transfer the same rate of kilowatts . you are right : lighting as well as ac motors will briefly turn off at 120 times per second ( that is for usa 60hz line frequency . ) for this reason , metal-vapor streetlights and older inductor-ballast fluorescent tubes have significant " hum " modulation in their light output . this strobe effect can be very visible when your eyes sweep across strings of led-based xmas lights . the hum is greatly reduced with incandescent lamps , since the filament does not cool down significantly during the millisecond-long low point in the 60hz wave . and you will probably hear a 120hz sound when using high-speed carbon-brush motors under high mechanical loads , such as electric mixers and carpentry routers . these high-rpm products can not use ac induction motors which are naturally limited to 1800rpm by the 60hz drive frequency . the 120hz variation ends up in the torque output of high-speed brush motors . single-phase induction motors should have much less 120hz mechanical buzz , since they are essentially 2-phase motors with magnetic field torque which rotates rather than oscillates . either capacitance or inductance is used to give the motor a second electromagnet pole having shifted phase .
in your frame of reference , it does indeed look as though the difference in speed between a and b is greater than $c$ . but the question is - does a think that b is moving away at that speed ? and the answer is " no " . there is a thing called the lorentz transformation which describes how the observed speed of an object is a function of the speed of the observer ; conveniently , this prevents the breaking of the speed limit of special relativity . without giving you the math ( you asked for a " simple explanation" ) , there are two things that happen when you are in a moving inertial frame of reference : length contraction , and time dilation . clocks moving relative to you seem to go slower , and distances become shorter . these changes are described in the lorentz transform .netresult is that velocity is also changed - this is described in the einstein velocity addition equation which states $$u ' = \frac{u+v}{1+\frac{uv}{c^2}}$$ when we put $u=v=c/2$ , we get $u ' = 0.8 c$ , so no speed limit is broken . the key to understand here ( after i re-read your question i realized i needed to add this ) : it is ok for two things to appear to move faster than the speed of light relative to each other - for example , you can see two beams of light , one traveling to the left , and the other traveling to the right , and say " the difference in speed between these photons is 2c " . however , there is no frame of reference in which you can observe anything moving faster than the speed of light - and that is the condition that special relativity imposes . does that difference make sense ?
the article the path to metallicity : synthesis of cno elements in standard bbn attempts to quantify the amount of carbon produced during big bang nucleosynthesis . it concludes that the ratio of carbon-12 formed to hydrogen was $~4 \times 10^{-16}$ with lesser amounts of carbon-13 and carbon-14 .
i have called $\beta$ the angle that $\alpha + \beta = 90^o$ so $\tan\alpha = 1/\tan\beta$ since the triangle is not moving , and the situation is symmetrical : the horizontal component of $n$ will cancel out , while the vertical ones must compensate for the weight of the triangle . $$ 2n \cos \beta = m_2g$$ i will call $f_x$ the horizontal component of $-n$ the reaction on the rectangle of $n$ , $f_y$ the vertical component . $$ f_x = n\sin \beta $$ $$ f_y = n \cos \beta $$ friction must compensate the force on the horizontal axis $f_x$ so $$ f_{friction} = f_x \rightarrow \mu r = f_x $$ the reaction of the table is given by the vertical component of the sum of all forces so $r = m_1g + f_y$ from the first you get $$ n = \frac{m_2g}{2\cos \beta}$$ so $$\mu = \frac{f_x}{f_y + m_1g} = \frac{n\sin\beta}{n\cos\beta + m_1g} = \frac{\frac{m_2g}{2\cos \beta}\sin\beta}{\frac{m_2g}{2\cos \beta}\cos\beta + m_1g}$$ simplify you will get $$\mu = \frac{m_2g\tan\beta}{ ( 2m_1 + m_2 ) g} = \frac{m_2}{ ( 2m_1 + m_2 ) \tan\alpha}$$
let 's find the complete solution of the problem . a complete solution of the problem would be the solution to the linear ode , $m \dot{\mathbf{v}} =q\mathbf{v} \times \mathbf{b}-k \mathbf{v}$ assume without loss of generality that the magnetic field is pointed along the z-axis , so $\mathbf{b} = b \mathbf{\hat{z}}$ . so our equation simplifies to , $m \dot{\mathbf{v}} =qb\mathbf{v} \times \mathbf{\hat{z}}-k \mathbf{v}$ dividing both sides of the equation by $m$ and for simplicity in the notation , let $\omega=\frac{qb}{m}$ and $\gamma=\frac{k}{m}$ . so , $\dot{\mathbf{v}} =\omega\mathbf{v} \times \mathbf{\hat{z}}-\gamma \mathbf{v}$ using $\mathbf{v}=\begin{pmatrix} v_{x}\\v_{y}\\v_{z} \end{pmatrix}$ and writng the given eqaution in matrix form we have , $\dot{\mathbf{v}} =\begin{pmatrix} -\gamma and \omega and 0 \\ -\omega and -\gamma and 0 \\ 0 and 0 and -\gamma \end{pmatrix} \mathbf{v}=a \mathbf{v}$ this is linear ode which can be solved using the matrix exponenetial as , $\mathbf{v}=e^{at} \mathbf{v_0}$ to simply this equation we can find the eigenvaules of a , use a similarity transform to convert it to a diagonal matrix which this greatly simplifies the matrix exponential . the eigenvalues and the corresponding eigenvectors are , $\lambda_{1}=-\gamma , \mathbf{v_1}=\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$ $\lambda_{2}=-\gamma-i\omega , \mathbf{v_2}=\begin{pmatrix} 1 \\ i \\ 0 \end{pmatrix}$ $\lambda_{3}=-\gamma+i\omega , \mathbf{v_3}=\begin{pmatrix} i \\ 1 \\ 0 \end{pmatrix}$ using $s= [ \mathbf{v_1} \mathbf{v_2} \mathbf{v_3} ] $ and performing a similarity transform on the matrix a , $s^{-1}as=d$ where d is diagonal matrix with the eigenvalues as the diagonal elements . and here we witness the power of similarity transformations as , $e^{at}= s \begin{pmatrix} e^{\lambda_{1}t} and 0 and 0 \\0 and e^{\lambda_{2}t} and 0 \\ 0 and 0 and e^{\lambda_{3}t} \end{pmatrix} s^{-1}$ ( after some tedious calculations ans using $e^{ix}=\cos{x}+isin{x}$ ) $=\begin{pmatrix} e^{-\gamma t}\cos ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\ -e^{-\gamma t}\sin ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\0 and 0 and e^{-\gamma t}\end{pmatrix}$ therefore , $\mathbf{v}=\begin{pmatrix} e^{-\gamma t}\cos ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\ -e^{-\gamma t}\sin ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\0 and 0 and e^{-\gamma t}\end{pmatrix} \mathbf{v_0}$ writing out the components , $v_{x}=e^{-\gamma t} ( v_{x_0} \cos{\omega t}+v_{y_0} \sin{\omega t} ) $ $v_{x}=e^{-\gamma t} ( -v_{x_0} \sin{\omega t}+v_{y_0} \cos{\omega t} ) $ $v_{x}=e^{-\gamma t} v_{z_0}$ this the just the equation of a helix with both the pitch and radius decreasing exponentially with $\gamma$ . however , the angular frequency is the same as that without drag , $\omega$ . here is a sample trajectory ,
you should know that $\partial_\mu$ is shorthand for $\frac{\partial}{\partial x^\mu}$ where $x^\mu= ( t , x , y , z ) $ . furthermore , it can be seen from your question that the metric convention that is used is $+ - - -$ , so that $x_\mu= ( t , -x , -y , -z ) $ . then , one can easily write out your equation . the einstein summation convention is used . \begin{align} ( \partial_\mu s+ea_\mu ) ^2 and =m^2\\ ( \partial_\mu s+ea_\mu ) ( \partial^\mu s+ea^\mu ) and =m^2\\ \partial_\mu s\partial^\mu s+e^2a_\mu a^\mu+2e\partial_\mu s a^\mu and =m^2 \end{align} your final equation follows pretty much trivially , once you set $a_0=-\frac{\alpha}{er}$ and $a_i=0$ .
the angular momentum $l_{a/b}$ of a rigid body $a/b$ about its center of mass is $$l_{a/b} = i_{a/b} \omega_{a/b} , $$ where $i_{a/b}$ is the inertia matrix of $a/b$ about its center of mass in the world frame and $\omega_{a/b}$ is the angular velocity of $a/b$ . the angular momentum $l_{a/b}^0$ of a rigid body $a/b$ about the origin of the world frame is $$l_{a/b}^0 = l_{a/b} + x_{a/b} \times p_{a/b} , $$ where $x_{a/b}$ are the coordinates of the center of mass in the world frame . then the total angular momentum in the system with the rigid bodies $a$ and $b$ about the origin of the world frame is $l_{total}^0 = l_a^0 + l_b^0$ , which is supposedly conserved . the total linear momentum is $p_{total} = p_a + p_b$ . the total linear momentum is conserved as soon as the forces $f_a$ and $f_b$ are of equal magnitude and opposite direction ( $f_a = f = -f_b$ ) : $$ \frac{\mathrm d}{\mathrm{d}t} ( p_a + p_b ) = m_a \dot{v}_a + m_b \dot{v}_b = f_a + f_b = f - f = 0 , $$ where $v_{a/b}$ is the translational velocity of $a/b$ in the world frame . the derivative of the total angular momentum with respect to time is $$\begin{split} \frac{\mathrm d}{\mathrm{d}t} ( l_a^0 + l_b^0 ) and = \frac{\mathrm d}{\mathrm{d}t} ( l_a + l_b + x_a \times p_a + x_b \times p_b ) = \dot{l}_a + \dot{l}_b + x_a \times \dot{p}_a + x_b \times \dot{p}_b \\ and = \tau_a + \tau_b + x_a \times f_a + x_b \times f_b = r_a \times f_a + r_b \times f_b + x_a \times f_a + x_b \times f_b \\ and = ( x_a + r_a - x_b - r_b ) \times f . \end{split}$$ thus the total angular momentum is conserved if : $x_a + r_a - x_b - r_b = 0$ , that is the force pair acts at the same coordinates in the world frame , $f = 0$ , that is no force acts , $ ( x_a + r_a - x_b - r_b ) \ ||\ f$ , that is the force acts along the line of connection . fig . 1 satisfies condition number 3 and thus conserves the total angular momentum . fig . 2 satisfies none of the three conditions and thus does not conserve the total angular momentum . edit : the so post is angular momentum always conserved in the absence of an external torque ? contains a proof for point particles , which has an analogous requirement for the conservation to hold ( forces along the line of connection ) . the author of the proof corrected it by now .
$ ( 2.7.7 ) $ is about the closed string , while $ ( 4.1.11a ) $ is about the open string . you can compare the expansions of the closed string and the open string in $ ( 2.7.4 ) $ and $ ( 2.7.26 ) $ . you see that the term in front of $-ip^\mu \ln|z|^2$ is $\frac{\alpha'}{2}$ for the closed string , and $\alpha'$ for the open string . when looking at the stress-energy tensor ( see $2.4.4$ ) , you have quadratic quantities of $\partial_a x^\mu$ divided by $\alpha'$ , so you get a term $\frac{\alpha'}{4}$ for the closed strings and a term $\alpha'$ for the open string . now , the constant part of $l_o$ ( the zero mode of the stress-energy tensor ) ( $\sim p^2$ ) is directly related to this term , so it explains the difference .
by disposing of the $u ( 1 ) $ gauge symmetry , we also dispose of the global version of the $u ( 1 ) $ symmetry which gives rise to both a conserved current and conserved charge due to noether 's theorem . specifically , every continuous global symmetry , by noether 's theorem , gives rise to a conserved current $j^{\mu}$ which satisfies the continuity equation , $$\partial_\mu j^{\mu} = \frac{\partial j^{0}}{\partial t} + \nabla \cdot \vec{j} = 0 . $$ the conserved noether charge is defined as , $$q = \int_v d^3 x \ , \ , j^{0} , $$ and by demanding $\partial_t q = 0$ , we see this implies , $$\frac{\partial q}{\partial t} = -\int_v d^3x \ , \ , \nabla \cdot \vec{j} = -\int_{\partial v} \vec{j} \cdot nds = 0 , $$ or in other words the flux of $\vec{j}$ is zero , and hence $q$ is conserved locally . by losing $u ( 1 ) $ gauge symmetry , we lose a redundancy in the description of the system , but also a conservation law .
$f=ma$ . if $f=0$ , and $m=0$ , $a$ can be anything . most physical laws are not " a causes b " . they usually say that " a and b can coexist in these conditions " . so , it is not necessarily " force causes acceleration " . it is " an accelerating body can coexist with a force if $f=ma$" the net force on a massless string is always 0 -- it has to be ( otherwise it will have infinite acceleration ) . whenever we draw free body diagrams of systems that contain massless strings , we always take a tension force $t$ that represents the string " pulling " the body . take the reaction force of $t$ on the string and you will notice that the string is always in equilibrium . for example , take this system , where someone is pulling a set of two boxes interconnected by a string : note that the reaction force of $t$ ( in red ) on the string balances itself . for a more complicated system , take the following : ( i have taken a massless smooth pulley here . if the pulley was not smooth , then the tensions in the two portions of string would be different . if it was not massless , the force from the ceiling would be different ) in this system $t$ balances out on the string as well . this is precisely why we say that a section of string exerts $t$ on both ends &mdash ; to maintain equilibrium .
the proper derivation of the centripetal acceleration—without assuming any kinematic variables are constant—requires a solid understanding of both the stationary cartesian unit vectors $\hat{i}$ and $\hat{j}$ as well as the rotating polar unit vectors $\hat{e}_r$ and $\hat{e}_\theta$ . the cartesian unit vectors $\hat{i}$ and $\hat{j}$ are stationary and always aligned with the x and y axes respectively , while the polar unit vectors $\hat{e}_r$ and $\hat{e}_\theta$ rotate with an angular velocity of $\omega=\|\dot{\theta}\|$ and point in the directions of increasing radius and angle ( respectively ) . the included graphic below shows the two basis vector pairs overlaid on top of one another . the position vector of the object is obviously defined as : $\vec{p} ( t ) =x\hat{i}+y\hat{j}=rcos ( \theta ) \hat{i}+rsin ( \theta ) \hat{j}$ , with $\|\vec{p} ( t ) \|=\sqrt{ ( rcos{\theta} ) ^2+ ( rsin{\theta} ) ^2}=\sqrt{r^2 ( sin^2 ( \theta ) +cos^2 ( \theta ) ) }=r\sqrt{ ( 1 ) }=r$ less obviously , it can be shown that the polar unit vectors $\hat{e}_r$ and $\hat{e}_\theta$ can be expressed solely in terms of the cartesian unit vectors $\hat{i}$ and $\hat{j}$ and the angular position $\theta$ as , $\boxed{\hat{e}_r=cos ( \theta ) \hat{i}+sin ( \theta ) \hat{j}}$ and $\boxed{\hat{e}_\theta=-sin ( \theta ) \hat{i}+cos ( \theta ) \hat{j}}$ . these two equations are extremely important , as they will be the key to expressing the cartesian acceleration in polar coordinates , of which one of the terms will be our desired $v^2/r=\omega^2r$ centripetal acceleration . moving forward , the vector acceleration of the object in cartesian coordinates is simply $\vec{a} ( t ) =\frac{d^2}{dt^2}\left [ \vec{p} ( t ) \right ] =\ddot{x}\hat{i}+\ddot{y}\hat{j}$ . starting with $x=rcos ( \theta ) $ and $y=rsin ( \theta ) $ and differentiating once , we have $\boxed{\dot{x}=\dot{r}cos ( \theta ) -r\dot{\theta}sin ( \theta ) }$ and $\boxed{\dot{y}=\dot{r}sin ( \theta ) +r\dot{\theta}cos ( \theta ) }$ . differentiating again , we will have $\ddot{x}=\ddot{r}cos ( \theta ) -\dot{r}\dot{\theta}sin ( \theta ) -\dot{r}\dot{\theta}sin ( \theta ) -r\frac{d}{dt}\left [ \dot{\theta}sin ( \theta ) \right ] $ $=\ddot{r}cos ( \theta ) -2\dot{r}\dot{\theta}sin ( \theta ) -r\left [ \ddot{\theta}sin ( \theta ) +{\dot{\theta}}^2cos ( \theta ) \right ] $ , such that $\boxed{\ddot{x}= ( \ddot{r}-r\dot{\theta}^2 ) cos ( \theta ) + ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) ( -sin ( \theta ) ) }$ . similarly , the y acceleration $\ddot{y}$ becomes $\ddot{y}=\ddot{r}sin ( \theta ) +\dot{r}\dot{\theta}cos ( \theta ) +\dot{r}\dot{\theta}cos ( \theta ) +r\frac{d}{dt}\left [ \dot{\theta}cos ( \theta ) \right ] $ $=\ddot{r}sin ( \theta ) +2\dot{r}\dot{\theta}cos ( \theta ) +r\left [ \ddot{\theta}cos ( \theta ) -{\dot{\theta}}^2sin ( \theta ) \right ] $ , such that $\boxed{\ddot{y}= ( \ddot{r}-r\dot{\theta}^2 ) sin ( \theta ) + ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) cos ( \theta ) }$ . now , we must plug these scalar derivatives into our formulation for the vector acceleration . in cartesian coordinates , this is $\vec{a} ( t ) =\ddot{x}\hat{i}+\ddot{y}\hat{j}=\{ ( \ddot{r}-r\dot{\theta}^2 ) cos ( \theta ) + ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) ( -sin ( \theta ) ) \}\hat{i}+\{ ( \ddot{r}-r\dot{\theta}^2 ) sin ( \theta ) + ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) ( cos ( \theta ) ) \}\hat{j}$ which can be rearranged into the following form : $\vec{a} ( t ) = ( \ddot{r}-r\dot{\theta}^2 ) \{cos ( \theta ) \hat{i}+sin ( \theta ) \hat{j}\}+ ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) \{-sin ( \theta ) \hat{i}+cos ( \theta ) \hat{j}\}$ but as we have already seen , this is simply equal to $\boxed{\boxed{\vec{a} ( t ) = ( \ddot{r}-r\dot{\theta}^2 ) \hat{e}_r+ ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) \hat{e}_\theta}}$ as we can now appreciate from carrying out the full derivation , there are actually two components each to both the radial and tangential accelerations . the $\ddot{r}$ term is straightforwardly equal to the second derivative of the position vector magnitude . the second term , $r\dot{\theta}^2$ , is our long sought-after centripetal acceleration $r\dot{\theta}^2=\omega^2r=v^2/r$ , and ( as expected ) it points in the negative radial direction . the tangential terms are perhaps a bit less intuitive . the $r\ddot{\theta}$ term is the acceleration that occurs whenever the radius and angular acceleration $\ddot{\theta}$ are both non-zero ( imagine the tangential acceleration of a turbine blade of a jet engine as the engine spools up ) . the final term $2\dot{r}\dot{\theta}$ is what is commonly known as the coriolis acceleration , and it occurs whenever the radius and angle change simultaneously . it arises because , for a given angular velocity , the arc length travelled every second increases with radius ( tangential velocity increases with radius ) . thus , an object with a given angular velocity will have different tangential velocities at different local radii of rotation . if the radius changes with time ( $\dot{r}\not=0$ ) and the angular velocity $\dot{\theta}$ is not equal to zero , then the tangential velocity will change with time , which is by definition a tangential acceleration .
you are right in observing the converse to what you were expecting . by definition $\tau=r \times f$ . but also $\tau=i\alpha$ , with $\alpha$ the angular acceleration ( an analogy with newton 's second law $f=ma$ ) . now suppose we want to achieve a given angular acceleration $\alpha$ . the two equalities above can be combined to give $r\times f=i\alpha$ . by assumption both $i$ and $\alpha$ are constants . for the case of pushing near the hinge , we see that , to get the same product on the right side , we must exert a large force $f$ , because $r$ is small and we want to achieve a certain angular acceleration . for the case at the doorknob , the force $f$ needed is much smaller as $r$ is bigger and $f$ need not be so big to give the same angular acceleration . you encounter this phenomenon if you go on a seesaw as well . sit near the edge and you will make the seesaw accelerate faster . torque is perpendicular to the rotation by definition . i do not think there is much physical meaning to the fact that torque is perpendicular the rotation , but it fits in with the angular velocity also being defined perpendicular to the rotation .
1 ) why do you believe that instantaneous probability densities are not meaningful ? 2 ) essentially any non-stationary state for which you need to compute time-dependent wavefunctions : e.g. chemical reaction dynamics , particle scattering , etc . 3 ) yes , the time dependant schrödinger equation applies to isolated systems . 4 ) by definition energy is conserved in an isolated system . moreover , the schrödinger equation conserves energy because the generator of time translations is the hamiltonian and this commutes with itself $ [ h , h ] =0$ , i.e. energy is conserved . for isolated systems , the hamiltonian is time-independent ( explicitly ) and the time-dependent wavefunction $\psi$ has the well-known form $\psi = \phi e^{-iet/\hbar}$ , with $e$ the energy of the isolated system . 5 ) i do not understand the question .
yes , a hole with energy $e$ is the same as an electron with a negative energy $e$ missing - that is why it is called a hole and that is how paul dirac first encountered it in the relativistic context ( in the form of positrons ) . a positively-charged positron may look more " particle-like " but one may describe it as the very same holes in the otherwise filled sea of negative-energy electron states . in both cases - semiconductors and positrons - you may assume that the negatively-charged electrons are the only " real " particles . however , you will always derive the existence of positively-charged holes that behave " just like electrons " . if you find states such that the energy $e$ is an increasing function of the momentum , the system will first try to fill the low-momentum , low-energy states , and you may add additional higher-energy , higher-momentum particles ( electrons ) . but some part of the spectrum may have the property that the energy $e$ is a decreasing function of the momentum $k$ . in that case , the electron states with a higher value of momentum are filled first , and you are adding them " inwards " . this is counterintuitive , so it is more logical to exchange the convention for what we mean by a " filled state " and what we mean by an " empty state " . once we do so , we also change the charge and energy of the particle in each state . consequently , we will deal with positive-charge holes whose energy $e$ behaves just like $-e$ of the original electrons , and increases with $k$ just like for ordinary electron states . the only difference will be in the charge .
i can think of two simple ways . lever the first would be to eschew the scale altogether and build a rudimentary scale with a stick and a pivot , and balance out your nephew 's weight with a bunch of water in some buckets , or 2l bottles . knowing the density of water , you could equate the weight of your nephew to a volume of water . if you are interested in correcting for the weight of the buckets , you could weigh those using your available scales . or you could use a collection of solid objects and weigh these in turn on your existing scales . in the same spirit , with access to stick and pivot technology , along with a measuring stick , you could just make the lever arm on the side opposite your nephew a factor of 6 times longer ( 7 to be safe ) , and then find some objects around the house to balance out this rudimentary scale . that collection of objects ( or water again ) could be measured on your existing scales . knowing this mass , and the ratio of the lever arms on the two sides of the scale , you will get a weight for your nephew . the major source of error in both cases is likely to be the uncertainty in the location of the center of mass on both sides . i reckon you could get this down to about an inch or so without too much trouble , which would equate to an error in the mass of around 3% if your lever arm on each side was around 3 feet . not perfect but decent . with less certainty in the position of the center of mass , or care taken , i would expect an error at the 10s of percent level . sock scale in the interest of science , i will also report a method that i do not recommend . i was trying to think of an elastic medium that you would have access to in your home , which you could use to weigh your nephew incrementally . if you could imagine calibrating a single spring to a small mass , then you could use many such springs to measure your nephew . what spring-like material does everyone have access to many copies of ? socks ! so , in the interest of science i took off my two socks and tried to see if socks have a hookian enough response to be useful . experimental setup : i used two socks , a binder clip to secure the weights to it , a pen , some string , and multiple 20 fl oz ( = 1.3 lbs of water ) bottles , and tried to see if the response was hookian over several bottles . i got 1 bottle -- 1.5 distance units 2 bottles -- 2.8 distance units 3 bottles -- 3.9 distance units  i am being vague about the distance units because i did not actually have a ruler handy , so instead used the l scale on my sliderule to measure the extensions rendered as a plot we see : while this looks decent , i am troubled by the fact that it does not line up well with the zero point extension of the sock , also we were only able to take 3 measurements since with the addition of the 4th bottle , the binder clip gave way . regardless of the questions of hookian reliability of the socks , to weigh your nephew at around 30 lbs , you would need something like 25 nearly identical socks , or calibrate all of them individually , and figure out a way to reliably afix your nephew to the socks . due to the impracticality of the method , i can not recommend this approach , though in the interest of science , and so that others need not follow in my footsteps , i have shared this failure here .
let 's back up for a second . before going into the complexities of non linear waves , let 's ask what a linear wave is . actually , let 's go even further back and ask " what do we mean when we say linear ? " " linear " comes from the study of things like vector spaces . we have objects ( call them vectors , or arrows , or whatever ) that can be both added together and scaled by a number , with the result being another object of the same type . any collection of objects that satisfies certain conditions ( which basically boil down to " addition and scalar multiplication behave as expected" ) can be considered a vector space . now let 's talk about waves . but to keep things simple , let 's just talk about the effect of some waves at a single point , where the effect can change in time . one wave might have a value $\psi_1 ( t ) = \sin ( \omega_1 t ) $ at this point . another might have a different frequency : $\psi_2 ( t ) = \sin ( \omega_2 t ) $ . suppose we scale the waves by factors of $a$ and $b$ , and suppose we have them both affect the point together . if the waves ' effects just scale and add in the sensible way , then the value of the combined wave at the point will be $$ \psi_{ ( a\otimes1 ) \oplus ( b\otimes2 ) } ( t ) \equiv a \sin ( \omega_1 t ) + b \sin ( \omega_2 t ) = a \psi_1 ( t ) + b \psi_2 ( t ) . $$ here i am using the symbol "$\otimes$" to mean " physically scaled by the preceding factor " and "$\oplus$" to mean " combined physically . " in this particular case , $\otimes$ and $\oplus$ reduced to sensible scalar multiplication of the the wave value and regular addition of the values of two waves . we call these " linear " waves . one of their characteristics is that you can think of the waves as noninteracting $\psi_2$ will add its effect to the total in the same way , regardless of how much amplitude $\psi_1$ has already contributed . but i did not have to have that structure . in some cases , driving a physical displacement with twice the force does not result in twice the displacement , and having two different driving forces work together does not result in a force that gives a displacement that is is the sum of the independent displacements . for instance , perhaps the rule is $$ \psi_{ ( a\otimes1 ) \oplus ( b\otimes2 ) } ( t ) \equiv \sqrt{a \sin ( \omega_1 t ) + b \sin ( \omega_2 t ) } \neq a \psi_1 ( t ) + b \psi_2 ( t ) . $$ this then would be a nonlinear wave . they are defined by having the definition of how disturbances scale ( $\otimes$ ) and combine ( $\oplus$ ) be incompatible with scalar multiplication and regular addition of the waves ' values . that is , our physical definitions of $\otimes$ and $\oplus$ did not yield the structure of a vector space - at least not in any obvious way . the physics question remaining then is whether or not this situation is ever actually realized . the above discussion defines nonlinear waves , but it does not prove any such things exist . as it turns out , though , many waves important to physics show nonlinear behavior if you push them far enough . the classic example in optics is when the amplitude of an electromagnetic wave is so great that electrons in nearby atoms ( thinking classically here ) are pushed and pulled quite far from the " sweet spot " distance they want to have from their nuclei . then the restoring force that pushes them back to that sweet spot is not simply directly proportional to their displacement , their motion is anharmonic , and the wave becomes nonlinear .
you are almost there . i am assuming your square well $v_0 ( r ) $ is nonzero and negative only on some interval $-r_0 &lt ; r &lt ; +r_0$ about the origin . in that case , for positive $a , b$ you already have that the isosinglet well is deeper than the isotriplet well . remember that the finite square well has a finite number of bound states , each with energy $-|v_0| &lt ; e &lt ; 0$ . find the width and depth of a well with a single bound state ( for fun , with the correct binding energy , 2.2 mev ) . next find the minimum $b$ so that a same-radius well , shallower by $\frac{3}{4}v_0b$ , has zero bound states . tada : a bound isosinglet with no excited states , and an unbound isotriplet .
a ) cuold someone briefly explain to me why the block on top can accelerate , but the one hanging on the pulley does not ? both blocks do accelerate , and in fact they have the same acceleration . you will see that if you write out the full set of both the x and y components of newton 's second law for each system ( i.e. . for each block ) . in the given solution , they only wrote out the components that are directly needed to solve the problem . notice how the 3kg block is actually " attached " tot he 8kg block , yet the solution here did not include it in its free-body diagram and they even included the reaction force from the 3kg on the 8kg . compared with the cart problem where the hanging mass is also touching the cart , no reaction force was drawn and they even treated the three mass as a single mass . it is not attached . the only interactions between the 8kg and 3kg blocks are the normal force and static friction , neither of which qualifies as " attachment . " this is exactly analogous to the interaction between $m$ and $m_2$ in the first problem . the reason no reaction force was drawn in the first case ( by " reaction force " i assume you are referring to the reaction to the normal force acting on $m_2$ ) was that they did not draw a free body diagram for the object the reaction force acts on , namely $m$ . again , the given solution omits some pieces ( in this case , a free body diagram ) which are not necessary for solving the problem . if you draw a free body diagram for $m$ , you will see the reaction force . could you have solved the first problem with the cart without taking all three mass into the cart 's fbd ? is there another way of applying newton 's second law to the cart problem ? if by this you are asking whether you could have solved the first problem by drawing separate free body diagrams for each of the three masses $m_1$ , $m_2$ , and $m$ , then yes , you certainly can .
the terminology of collapse of the wavefunction is an unfortunate one . take an oscillating ac line and use a scope to measure it and display it . is the ac 50 herz wavefunction collapsed because we observe it on the scope ? the ac wave function is just a mathematical description of the voltage and current on the line and allows us to calculate the amplitude and time dependance of the energy it carries . an equally unfortunate concept is the matter wave . the particle is not a continuous soup distributing its matter in space and time the way of an ac voltage or other classical wave . you will never find 1/28th of a particle , it is either there in your measuring instruments or it is not , and it is governed by a probability wave mathematical description , not a " matter wave " even more so , the wavefunction manifestation of a particle does not collapse when we measure it the way a balloon collapses when pierced by a pin , because it is just a mathematical description of the probability to find a particle in a particular ( x , y , z ) with a particular ( p_x , p_y , p_z ) within the constraints of the heisenber uncertainty principle . when wavefunction collapse , does not σx become 0 ? , as we will know the location of the particle . or does standard deviation just become smaller ? we know the location of the particle at that specific coordinate where we had our measuring instrument with the specifice momentum that our insturments measured , within the instrument errors . the probability of finding it there after the fact is 1 . it is the nature of all probability distributions that after the detection they become one . example : the probability i will die in the next ten years is 50% . at the instant of my death the probability is one that i am dead . σx is not a standard deviation in the error sense . σxσp≥ℏ2 says that : if i want to know the location of my particle within a region about the x point with uncertainty/accuracy σx , the σp i can measure simultaneously is constrained to be within an uncertainty that follows the constraint σxσp≥ℏ2 . does the particle resurrect into a wavefunction form ? the particle keeps it dual nature of particle or probability wave according to the momentum it still carries and will be appropriately detected as a particle or a probability wave by the next experimenter . it is not a balloon to have been destroyed by the measurement . what can be an observer that triggers wavefunction collapse ? ( electron wavefunction does not collapse when meeting with electrons ; but some macroscopic objects seem to become observers . . . . ) in principle , any interaction of a particle that changes its momentum and position is an observer except that some interactions are quantum mechanical because of the hup and the nature of the interaction and some are macroscopic manifestations in our instruments of the passage of a particle or probability wave of a particle . we usually call observers the classical macroscopic detectors , be they people or instruments . at the microcosm quantum level we have interactions governed by the probability wave functions . what happens to the energy of a particle/wave packet after the collapse ? energy and momentum are conserved absolutely , so it will depend on what sort of detection of the particle took place . some will be carried off by the particle if it has not been absorbed into the detector , as for example these particles in this bubble chamber photograph which continually interact with the transparent liquid of the bubble chamber . in this case a tiny bit of the energy is taken by kicked off electrons ( first detector atom of liquid , final detector photographic plate ) which show by the ionisation the passage of the particle , which is certainly not idiotically " collapsing " .
bps objects are stable because they are the lightest objects with given values of certain conserved charges . so there exists no potential final state that would be lighter and that the bps state could decay into , by conserving the energy . the excess energy may be invested to the kinetic energy of the final energy but a deficit energy means that the decay is prohibited . as an analogy , note that the electron has to be stable because there exists no lighter $q=-e$ object than the electron ( and positron ) . bps objects are either those preserving some ( enough ) supersymmetry ; or objects saturating the bps bound $m=q$ , schematically speaking ( for branes , it is the tension equal to the charge density ; coefficients should be inserted everywhere ) . these conditions are equivalent because $$ \{q , q\} = m-z $$ schematically , for some conserved supercharge $q$ . so the expectation value of $\{q , q\}$ in the bps state is zero – because $q$ annihilates the state – but it is also equal to $m-z$ which means that the mass is equal to the charge . for non-bps states , we have the strict $m\gt z$ . here , $z$ is the conserved central charge .
it might be worth taking a look to the original text , galileo 's discourses on two new sciences . the reasoning you are looking for is on the third day , a translation of which may be found online . the relevant parts are labelled theorem i and theorem ii in the above-linked translation . to derive that distance in a uniformly accelerated motion ( e . g . free fall ) goes as time squared , galileo first argues that the time in which any space is traversed by a body starting from rest and uniformly accelerated is equal to the time in which that same space would be traversed by the same body moving at a uniform speed whose value is the mean of the highest speed and the speed just before acceleration began . this is argued on a graphical basis ( see above link ) . however , even though the pictures may look pretty similar to modern functional representations ( e . g . of velocity vs . time ) and arguments involve finding equal areas in different situations , the arguments never involve an actual calculation of an " area " with mixed units , which was not yet conceivable at the time ( e . g . $m/s \cdot s = m$ ) . in fact , the whole third day seems very convoluted precisely because the notion of velocity was not clearly numerical yet , since only commensurable ( same unit ) quantities could conceivably be operated with ( added , divided , . . . ) . proportions of non-commensurable quantities , however , could be compared ( today we had say they are dimensionless ) , as in if a moving object traverses two distances in equal intervals of time , these distances will bear to each other the same ratio as the speeds ( earlier in the third day )
let us ask the question the other way round . given a conductor closed shape and a zero field inside , what are the possible surface charge distributions and when is one of them constant ? the potential is solution of the laplace equation and can written as $$v ( r , \theta , \phi ) =\sum_{\ell=0}^\infty \sum_{m=-\ell}^\ell \left ( a_{\ell m}r^\ell+b_{\ell m}r^{-\ell-1}\right ) y_{\ell m} ( \theta , \phi ) . $$ the constants $a_{\ell m}$ and $b_{\ell m}$ will take different values whether we are inside or outside the surface . we know that $v^{\text{out}}\to 0$ for $r\to\infty$ ( so $a_{\ell m}^{\text{out}}=0$ ) and by hypothesis $a_{\ell m}^{\text{in}}=b_{\ell m}^{\text{in}}=0$ except $a_{00}=v_0$ is the value of the constant potential . by continuity of the potential , we have therefore , for any point on the surface $$\sum_{\ell=0}^\infty b_{\ell m}^{\text{out}}r^{-\ell-1}y_{\ell m} ( \theta , \phi ) =v_0 . \tag{1}$$ the charge distribution at a point on the surface is given by $\sigma=-\varepsilon_0\hat{n}\cdot \nabla v$ , where $\hat n$ is the normal unit vector to the surface . only the term of the outside potential contribute . $$\sigma=\varepsilon_0\sum_{\ell=0}^\infty\sum_{m=-\ell}^\ell ( \ell+1 ) b_{\ell m}^{\text{out}}r^{-\ell-2}y_{\ell m} ( \theta , \phi ) . \tag{2}$$ equations ( 1 ) and ( 2 ) are expression of a constant function on the surface . consequently there are proportionnal : there is a constant $\alpha$ such that $v_0=\alpha \sigma/\varepsilon_0$ . from a more fundamental point of view , equations ( 1 ) and ( 2 ) are implicit equations of the same surface written in the same coordinate system as $\psi ( r , \theta , \phi ) =\text{constant}$ , therefore the gradients of $\psi$ taken for each of them must be colinear . we can therefore write that , for all $\ell$ and $m$ $$ ( \alpha ( \ell+1 ) -r ) b_{\ell m}=0 , $$ which is possible if $b_{\ell m}=0$ or if $r$ is a constant . the case $r=\text{constant}=\alpha ( \ell+1 ) $ is the known spherical case . we note that in the spherical case , there is only one possible value of $\ell$ for which $b_{\ell m}\neq 0$ and it is known that it must be $\ell=0$ from gauss 's theorem . so the only non trivial solution is $r=r=\text{constant}$ . this gives the relation $\sigma=\varepsilon_0 v_0/r$ . the total charge is $q=4\pi\varepsilon_0v_0r$ and the potential in space is $v^{\text{out}} ( r ) =v_0r/r$ .
since the vector space is finite dimensional , say of dimension $n$ , then $\hat{a}_-$ is represented by a $n\times n$ matrix $a$ with respect to the basis that op mentions . as valdo suggests , define $\hat{a}_+$ via the hermitian conjugate matrix $a^\dagger$ . now use that any matrix $a$ can uniquely be written as $$ a~=~h+ik , $$ where $h$ and $k$ are hermitian . it is not hard to see that $$h~=~\frac{a+a^\dagger}{2} , \qquad k~=~\frac{a-a^\dagger}{2i} , \qquad a^\dagger~=~h-ik . $$
entanglement is just a correlation in measured properties of the subsystems ( particles ) expressed in a quantum way . you may only determine whether properties are correlated ( or entangled ) in a given , " initial " state if you repeat some measurements of the system with the same initial state many times . if you only measure two spins , for example , once , you get some result , like up-up or up-down or down-up or down-down but none of the four possibilities is more or less entangled than others . all of them may occur in entangled initial states and all of them may occur in non-entangled initial states . entanglement only means " predicted properties of the two subsystems are correlated , moreover correlated in a way that is not captured by a simple classical model of correlation " . whether the predicted properties are correlated may be determined from the probability distribution ( s ) but to measure the probability distribution ( s ) , you have to repeat the experiment with the same initial state many times . more precisely , entangled states are those that can not be written as a tensor product of wave functions describing the separate subsystems . once at least one of the entangled variables is measured , the entanglement becomes meaningless because the value of the variable is suddenly known and we are only left with some general wave function for the other , previously entangled variable which remains unknown up to the second measurement ( this reduction of the dependence of the wave function is misleadingly referred to as the infamous " collapse" ) . and if there is only one variable , it can not be entangled . but nothing physical is changing about the variable that has not been measured yet . the overall probability distribution for various outcomes $y$ remains the same after the first measurement of the ( faraway ) variable $x$ is performed ( imagine it is a probabilistic distribution $\rho ( y ) =c\rho ( x_\text{just measured} , y ) $ that is left afterwords , $c$ is such that $\int dy\ , \rho ( y ) =1$ ) . so no information can be transmitted by the fact that the first measurement took place . quite generally , quantum mechanics does not need any genuine ( one that could transfer useful information ) faster-than-light communication to guarantee things such as correlations of measurements done with entangled states . and in relativistic , local theories – especially quantum field theories and string theory – one may prove completely generally that a superluminal transfer of information is not only unnecessary for quantum mechanics to work ; it is actually prohibited and impossible , by the basic laws of special relativity .
let $$ \eta_{\mu\nu}={\rm diag} ( +1 , -1 , -1 , -1 ) \qquad \bar\eta_{\mu\nu}={\rm diag} ( -1 , +1 , +1 , +1 ) $$ with corresponding lorentz force laws ( in units where mass equals charge ) $$ \ddot x^\mu=\eta_{\nu\lambda}f^{\mu\nu}\dot x^\lambda \qquad \ddot{\bar x}^\mu=\bar\eta_{\nu\lambda}\bar f^{\mu\nu}\dot{\bar x}^\lambda $$ as the trajectories $x^\mu , \bar x^\mu$ should agree ( and so will all its derivatives ) for all initial conditions , we can equate the terms $$ \tag{1} \eta_{\nu\lambda}f^{\mu\nu} = \bar\eta_{\nu\lambda}\bar f^{\mu\nu} $$ contracting with the inverse $\eta^{\lambda\sigma}$ of $\eta_{\nu\lambda}$ finally yields $$ f^{\mu\sigma} = -\bar f^{\mu\sigma} $$ as $$ \bar\eta_{\nu\lambda}\eta^{\lambda\sigma} = -\delta_\nu^{\sigma} $$ this means the signs of the components of the electromagnetic tensor $f^{\mu\nu}$ do indeed depend on the metric convention . this also applies to $f_{\mu\nu}$ , whereas the tensor of mixed rank $f^\mu{}_\nu$ is independant of this choice ( which is just ( 1 ) ) .
the cosmic background radiation was always with us , it is not reaching us now . it just became cooler and cooler as time went on . one has to understand that when we are talking big bang and general relativity we are talking of the universe starting from one ( x , y , z , t ) point and as time goes on , expanding . this means that all ( x , y , z ) points of our universe trace back to that one point singularity that went " bang " . envision the two dimensional surface of a balloon , as shown in the wiki link . at time=0 the balloon is one point , call it r=0 . as it expands the points on its surface start receding from each other , and all points on that surface were at r=0 at t=0 . their neighborhood expands as the balloon blows up , and this means the electromagnetic radiation that started in the earth 's neighborhood hot , cools due to the expansion and becomes the cosmic microwave background . that is what i mean it is not coming from anywhere , it just is .
there is a more general statement : all 4d lorentz invariant field theories flow to cfts in the uv and the ir . a proof was given last year by luty , polchinski , and ratazzi in http://arxiv.org/pdf/1204.5221.pdf . their argument has some assumptions but they are fairly weak .
first of all write down an explicit expression for the summation over all microstates . edit since you are treating the system classically this includes an integral over phase-space and a summation over all possible spin-configurations . $$ \sum_{\mu_s} = \sum_{\{s_i\}}\int\frac{d^np d^nq}{h^{3n}n ! }$$ the second thing is to realize that your hamiltonian is non-interacting and the canonical density $e^{-\beta h} $ is just a product of one-particle hamiltonians $$ e^{-\beta h} =\prod_{i=1}^ne^{-\beta h_i} $$ where of course $$ h_i = {\frac{ ( p_i + \gamma s_i ) ^2}{2m} -b s_i} $$ ( i renamed $\beta$ to $\gamma$ , because you do not mean the inverse temp . here ) so you have to evaluate $$ \frac{1}{n ! }\sum_{\{s_i\}}\prod_{i=1}^n\int\frac{dp_i dq_i}{h^{3}} e^{-\beta h_i} = $$ because $h$ ist non-interacting , the n-particle phase-space-integral factorizes into n integrations over a 1-particle phase-space . similarily one may interchange the spin-summation and the product ( convince yourself that this is true ! e . g that one ends up with the same terms , whether you sum over spin first or not . ) that means , instead of summing over all the many-body microstates , one first sums over the possible configurations of a single particle and accounts for the fact , that there are many afterwards . additionally all $h_i$ are equivalent . they each just carry a different but redundant index : $$ z = \frac{1}{n ! }\prod_{i=1}^n\sum_{s_i=\pm1}\int\frac{dp_i dq_i}{h^{3}} e^{-\beta h_i} = \frac{1}{n ! }\left ( \sum_{s=\pm1}\int\frac{dp dq}{h^{3}} e^{-\beta h}\right ) ^n$$ where $h$ is $h_i$ but without an index , because the reference is not to a specific particle anymore . /edit you will have to think about , what to do with the momentum integration . i have not calculated the result , but it might be you will not end up with a solution in closed form . there might be an approximation needed to do the momentum summation . edit i think a gaussian integration will do the trick . /edit let us know what you end up with !
things are not always in states of minimum energy . this is something that applies to equilibrium states . simple examples of this idea are certainly familiar to you already - a ball comes to rest at the bottom of a valley , not partway down the side . if we want to find the equilibrium state for a white dwarf , there must be no forces on it . if there are no forces , then small changes to the white dwarf do not change its energy , since the change in energy is force*distance . that means the energy should be a " stationary point " , where the energy curve is flat . ( in this case , we are looking at a curve that describes energy as a function of radius . ) we want a minimum so that the equilibrium will be stable . you can apply different reasoning to get to the same result . for example , the white dwarf is in a cold universe , so thermodynamics says it will give away energy as much as it can since this increases the total entropy . this only stops when the white dwarf is at a minimum energy , or at least gets down to a few degrees k .
i have only skimmed the wikipedia article you link to . from a quick look i would say the paragraphs you quote are making points about what a theory of gravity needs to look like . for example you say " curvature of spacetime in only required in order to explain tidal forces " , but what that really means is that it is impossible to have a theory of gravity without curvature . that is because any theory of gravity inevitably has to describe tidal forces . you go on to say " as long as you ignore tidal forces , you can explain gravity without curvature " , but you can not ignore tidal forces so you can not explain gravity without curvature . to take your two specific questions : question 1 . gravity i.e. general relativity is not a theory of forces : it is a theory of curvature . by focussing on the " fictitious forces " you are getting the wrong idea of how gr works . when you solve the einstein equation you get the geometry ( curvature ) of space . this predicts the path a freely falling object will take . we call this a geodesic and it is effectively a straight line in a curved spacetime . if you want the object to deviate away from a geodesic then you must apply a force - and there is nothing fictitious about it . for example , gr predicts that spacetime is curved at the surface of the earth , and if you and i were to follow geodesics we had plummet to the core . that we do not do so is evidence that a force is pushing us away from the geodesic , and obviously that is the force between us and the earth . but , and it is important to be clear about this , the force is not the force of gravity , it is the force between the atoms in us and the atoms in the earth resisting the free falling motion along a geodesic . question 2 . again this is really just terminology . when you are free falling " gravity " is not eliminated . remember that " gravity " is curvature , and in fact the curvature is the same for all observers regardless of their motion . that is because the curvature tensor is the same in all co-ordinate frames . the existance of tidal forces is proof that gravity/curvature is present . when you are free falling you are moving along a geodesic . it is true to say that there are no forces acting , but this is always the case when you are moving along a geodesic . remember a geodesic is a straight line and objects move in a straight line when no forces are acting . there would only be a force if you deviated from the geodesic e.g. by firing a rocket motor . response to fiftyeight 's comment : this got a bit long to put in a comment so i thought i would append it to my original answer . i am guessing your thinking that if you accelerate a spaceship it changes speed , so when you stop something has happened , but when the earth accelerates you nothing seems to happen . the earth can apply a force to your for as long as you want , and you never seem to go anywhere or change speed . is that a fair interpretation of your comment ? if so , it is because of how you are looking at the situation . suppose you and i start on the surface of the earth , but you happen to be above a very deep mine shaft ( and in a vacuum so there is no air resistance - hey , it is only a thought experiment :- ) . you feel no force because you are freely falling along a geodesic ( into the earth ) , while i feel a force between me and the earth . from your point of view the force between me and the earth is indeed accelerating me ( at 9.81ms$^{-2}$ ) . if you measure the distance between us you will find i am accelerating away from you , which is exactly what you had expect to see when a force is acting . if the force stopped , maybe because i stepping into mineshaft as well , then the acceleration between us would stop , though we had now be moving at different velocities . this is exactly what you see when you stop accelerating the spaceship . it is true that a third person standing alongside me does not think i am accelerating anywhere , but that is because they are accelerating at the same rate . it is as though , to use my example of a spaceship , you attach a camera to the spaceship , then decide the rocket motor is not doing anything because the spaceship does not accelerate away from the camera .
a 242ω resistor is in parallel with a 180ω resistor , and a 240ω resistor is in series with the combination . a current of 22ma flows through the 242ω resistor . the current through the 180ω resistor is __ma . approaching this step by step , note that you can calculate the voltage across the $242 \omega$ resistor since you are given the current through it . by ohm 's law : $$v_{r_{242}} = 22ma \cdot 242 \omega$$ now , you are also given that this resistor is in parallel with a $180\omega$ resistor so the voltage across this resistor must be identical to the voltage across the $242 \omega$ resistor . thus , and again by ohm 's law , you can calculate the current through the $180\omega$ resistor . two 24ω resistors are in parallel , and a 43ω resistor is in series with the combination . when 78v is applied to the three resistors , the voltage drop across the 24ω resistor is___volts . again , approaching this step by step , the 2 parallel connected $24\omega$ resistors can be combined into 1 equivalent resistor of resistance $$r_{eq} = \dfrac{24 \cdot 24}{24 + 24}$$ now you can use voltage division to find the voltage across the equivalent resistance : $$v_{r_{eq}} = 78v \dfrac{r_{eq}}{r_{eq} + 43}$$ can you take it from here ?
pair production of electron/positron happens in the electric field of the atoms to satisfy conservation laws and the same will be true for off mass shell z0 going into a neutrino antineutrino pair , interacting with the weak field of the atoms . the equivalent to the photon weak interaction mediator is the z0 and neutrino antineutrino pairs can be formed that way . the weak interaction is orders of magnitude smaller than the electromagnetic one , and thus the probability of getting pairs of neutrinos-antineutrinos is high only in special situations as in the big bang or in a super nova explosion where the density of matter is high and there is energy available . the weak couplings lower the probability of interaction drastically so the advantage of a smaller neutrino mass with respect to the electrons is lost for experiments possible in the laboratory .
just to mention the basics for " other ways"' , you can hit your photon really hard with a fast moving electron or proton , i.e. inverse compton scattering . ics is very important in many astrophysical contexts . if you even reflect the photon off a moving mirror , you can slightly increase or decrease its wavelength
it can be justified using distribution theory .
it depends what you consider " radiation " . ultrasound does not involve radition . mri involves radiowaves , which are electro-magnetic radiation ( but not ionizing radiation ) . proton imaging and neutron imaging do not directly involve electro-magnetic radiation , but the term ionizing radiation is usually defined to include particles that cause ionization regardless of whether the particles are photons ( electro-magnetic radiation ) .
one should not imagine the t-duality between the two heterotic strings to be a $z_2$ group , like in the case of type ii string theories ' t-duality . in type ii string theory , there is only one relevant scalar field , the radius of the circle producing t-duality , and it gets reverted $r\to 1/r$ under t-duality . in the heterotic case , it is more complicated because more scalar fields participate in the t-duality . instead of a $z_2$ map acting on one scalar field , one must correctly adjust the moduli , especially the wilson lines generically breaking the 10d gauge group to $u ( 1 ) ^{16}$ , and find an identification between the points of the moduli space of the two heterotic string theories : there is one theory at the end . a fundamental reason why the t-duality holds is that one may define the heterotic string theories in the bosonic representation , using 16-dimensional lattice $\gamma^{16}$ which is the weight lattice of $spin ( 32 ) /z_2$ , and $\gamma^{8}\oplus \gamma^8$ which is the root lattice of $e_8\times e_8$ . one may also describe the compactification on a circle in terms of lattices . it corresponds to adding ( $\oplus$ ) the lattice $\gamma^{1,1}$ of the indefinite signature to the original lattice . the extra 1+1 dimensions correspond to the compactified left-moving and right-moving boson ( of the circle ) , respectively . now , the key mathematical fact is that the 17+1-dimensional even self-dual lattice exists and is unique which really means $$\gamma^{16}\oplus \gamma^{1,1} = \gamma^{8}\oplus \gamma^8\oplus \gamma^{1,1}$$ even self-dual lattices in $p+q$ dimensions ( signature ) exist whenever $p-q$ is a multiple of eight and if both $p$ and $q$ are nonzero , the lattice is unique . it is unique up to an isometry – a lorentz transformation of a sort – and that is how the identity above should be understood , too . so there is a way to linearly redefine the 17+1 bosons on the heterotic string world sheet so that a basis that is natural for the $e_8\times e_8$ heterotic string gets transformed to the $spin ( 32 ) /z_2$ string or vice versa . the compactified boson has to be nontrivially included in the transformation – the 17+1-dimensional lorentz transformation that makes the t-duality manifest mixes the 16 chiral bosons with the 1+1 boson from the compactified circle . a different derivation of the equivalence may be found e.g. in polchinski 's book . one may start with one of the heterotic strings and carefully adjust the wilson lines to see that at a special point , the symmetry broken to $u ( 1 ) ^{17+1}$ is enhanced once again to the other gauge group .
i will attempt an answer , though someone knowing the precise ground realities will most likely improve on my answer . you make a very good point about the speed staying constant if the space ship can be treated as a closed system . that is the sole point that we need to worry about . naturally , the atmosphere within a space ship has to be maintained ( at the values that can support human beings ) . if it was just a case of filling up the shuttle once with $21 \%$ oxygen and being done with it , astronauts would keep consuming it so that its levels would fall , and percentage of ${\rm co}_2$ would keep increasing . that is undesirable and in a simplified description , one can get around this by removing ${\rm co}_2$ via a chemical reaction with lithium hydroxide ${\rm lioh}$ . ( by the way , this is a fairly common use of ${\rm lioh}$ , as a carbon dioxide scrubber in breathing purification systems , as can be seen here . ) upon the reaction , these ''canisters'' can be stored and disposed off later . all the excess water ( i.e. . discounting the potable variety ) is directed to tanks , which can again be disposed later . excess heat is handled by converting to ammonia vapor and subsequent storage . ( though somewhat simplified , a description of this process can be found in the first link of this article . ) so , while space shuttles would ''maintain'' a requisite atmosphere , ( apparently ) nothing gets dumped on there an then basis . now , your question pertained to what would happen if this release happens ( or does not happen ) while the shuttle continues moving ahead at a uniform velocity $v$ - if it got dumped , then $v$ would change . does not seem to be the case . see , everywhere in physics , we make all sorts of approximations , the question is how valid they are in real situations . irrespective of which materials you may choose to build the spacecraft with , it will not make a perfect thermal insulator . while they may try to reduce this radiation loss to as low a value as possible , there will be some amount of heat radiated by the craft . so , while not absolutely ideal , it may be be a good approximation to an insulated body , or in the context , let 's say a closed system . in textbook situations , one always considers simplified descriptions . thus , armed with these links , i think you can safely go and pester your instructor , telling him that his original logic had a flaw ! !
the lift basically depends on the the velocity of the plane and the the density of the air around it . that means , in higher altitude , a plane needs a higher velocity to maintain its height , but : the velocity is not just for staying up , but obviously is a desired feature of an airplane , thus , they would not speed down even it was possible while maintaining height . on the other hand , the higher the pressure and thus the density of the surrounding air , the higher the drag . as we already find out that velocity is desired , the drag results in more energy needed to maintain the velocity . thus , it is more efficient to fly at higher altitudes .
the lhc is a discovery machine , and a lot of phenomenologists have worked hard to make predictions from various models , some of them using mainstream theories , some of them not . searching on the net i found this preprint which proposes checking the data for such signatures : the existence of the dark matter with amount about five times the ordinary matter is now well established experimentally . there are now many candidates for this dark matter . however , dark matter could be just like the ordinary matter in a parallel universe . if both universes are described by a non-abelian gauge symmetries , then there will be no kinetic mixing between the ordinary photon and the dark photon , and the dark proton , dark electron and the corresponding dark nuclei , belonging to the parallel universe , will be stable . if the strong coupling constant , ( αs ) dark in the parallel universe is five times that of αs , then the dark proton will be about five time heavier , explaining why the dark matter is five times the ordinary matter . however , the two sectors will still interact via the higgs boson of the two sectors . this will lead to the existence of a second light higss boson , just like the standard model higgs boson . this gives rise to the invisible decay modes of the higgs boson which can be tested at the lhc , and the proposed ilc . they have put bounds from their model using available lhc data . note that they are talking of two parallel universes , not infinite . so the video you watched probably has no connection to this , but i submit it to show how one can search for signatures of the invisible in the lhc data .
take 4 points arranged in a square . the middle of the square is a critical point by symmetry , and the midpoint of the four sides of the square would be a critical point just for the two vertices it joins together , ignoring the other two . but the other two are little more than twice as far away , so eight times weaker force , so if you bring the point closer to the center of the square by an amount about 1/8 of the way to the other side , you will cancel out the force from the far pair from the force on the near pair . so there are 5 critical points for four points on a square , and this holds for all sufficiently fast-falling-off forces . using squares-of-squares , i believe it is easy to establish that the number of critical points is generically $n^2$ . i believe it is a difficult and interesting mathematical problem to establish any sort of nontrivial bound on the number of critical points . the trivial bound is from the order of the polynomial equation you get , and it is absurdly large---- it grows like $2^n$ . edit : the correct growth rate the answer for a general configuration is almost certainly n+c critical points ( this should be an upper bound and a lower bound for two different c 's --- i did not prove it , but i have a , perhaps crappy , heuristic ) . for the case of polygons , there are n+1 critical points . for squares where the corners are expanded to squares , and so on fractally , the number of critical points is n+c where c is a small explicit constant . the same for poygons of polygons . i found a nifty way of analyzing the problem , and getting some good estimates , but i want to know how good the mathematicians are at this before telling the answer . perhaps you can ask this question on mathoverflow ?
ref [ 19 ] in the arxiv paper , c . m . caves and b . l . schumaker , phys rev a 31 , 3068 ( 1985 ) , gives a clean description of a parametric amplifier as the prototype of a two-photon device , at the bottom of its second page : in a [ parametric amplifier ] , an intense laser beam at frequency $2\omega$ —the pump beam— illuminates a suitable nonlinear medium . the nonlinearity couples the pump beam to other modes of the electromagnetic field in such a way that a pump photon at frequency $2\omega$ can be annihilated to create " signal " and " idler " photons at frequencies $\omega\pm\epsilon$ and , conversely , signal and idler photons can be annihilated to create a pump photon . one way to think of the present situation would be as a dual of this description . that is , the medium , the josephson junction , oscillates at a pump frequency $2\omega$ , and interacts nonlinearly with the vacuum state . from the arxiv paper itself , there is a clear parallel , quantum theory allows us to make more detailed predictions than just that photons will simply be produced . if the boundary is driven sinusoidally at an angular frequency $\omega_d = 2\pi f_d$ , then it is predicted that photons will be produced in pairs such that their frequencies , $\omega_1$ and $\omega_2$ , sum to the drive frequency , i.e. , we expect $\omega_d = \omega_1 + \omega_2$ . one of the comments on the nature news page , edward schaefer at 2011-06-06 12:39:04 pm , makes a point that i think is worth highlighting , that " the mirror transfers some of its own energy to the virtual photons to make them real . "
you are not getting your facts right at all . how do we know from this $\langle w \rangle = \int_{-\infty}^{\infty} \bar{\psi}\left ( -\frac{\hbar^2}{2m} \frac{d^2}{dx^2} + w_p \right ) \psi dx$ or this $\hat{h} = -\frac{\hbar^2}{2m}\frac{d^2}{dx^2} + w_p$ that we have an eigenfunctiuion and eigenvalue . answer : we do not . all i know about operator $\bar{h}$ so far is this equation where $\langle w \rangle$ is an energy expected value : \begin{align} \langle w \rangle = \int_{-\infty}^{\infty} \bar{\psi}\left ( -\frac{\hbar^2}{2m} \frac{d^2}{dx^2} + w_p \right ) \psi dx \end{align} no , you do not . here 's the mathematical side of what an eigenfunction and eigenvalue is : given a linear transformation $t : v \to v$ , where $v$ is an infinite dimensional hilbert or banach space , then a scalar $\lambda$ is an eigenvalue if and only if there is some non-zero vector $v$ such that $t ( v ) = \lambda v$ . here 's the physics side ( i.e. . qm ) : we postulate that the state of a system is described by some abstract vector ( called a ket ) $|\psi\rangle$ that belongs to some abstract hilbert space $\mathcal{h}$ . next we postulate that this state evolves in time by some hermitian operator $h$ , which we call the hamiltonian , via the schrodinger equation . what is $h$ ? you guess and compare to experimental results ( that is what physics is anyway ) . next we postulate for any measurable quantity , there exists some hermitian operator $o$ , and we further postulate that the average of many measurements of $o$ is given by $ \langle o \rangle = \langle \psi | o | \psi \rangle$ . connection to wavefunctions : we pick the hilbert space $l^2 ( \mathbb{r}^3 ) $ to work in , so $\psi ( x ) = \langle x | \psi \rangle$ , and $\langle o \rangle = \int_{-\infty}^{\infty} \psi^* ( x ) o ( x ) \psi ( x ) dx$ . ok , that is the end . the form of $h$ does not follow from the energy expected value . wait ! i have not even talked about eigenvalues and eigenfunctions . this is a useless post ! answer : well you do not have to . but it is useful to find the eigenvalues and eigenfunctions of $h$ , because the eigenfunctions of $h$ form a basis of the hilbert space , and certain expressions become diagonal/more easily manipulated when we do whatever calculations we want to do . so to find the eigenvalues of $h$ , we simply solve the eigenvalue equation as stated above : solve \begin{align} h | \psi_n \rangle = e_n | \psi_n \rangle . \end{align} this is in the form $t ( v ) = \lambda v$ . so as alfred centauri says , we simply want to find the eigenfunctions of $h$ . a more subtle question would be , how do we know they exist ? the answer lies in spectral theory and sturm-liouville theory but nevermind for now , as physicists we assume they always exist . so your additional question : $\hat{a} \psi$ is an eigenfunction of operator$\hat{h}$ with eigenvalue $ ( w-\hbar \omega ) $ . well . . . . that just follows straightaway . you said you already proved that $h a^\dagger \psi = ( w - \hbar \omega ) a^\dagger \psi$ . so here $t$ = $h$ , $a^\dagger \psi = v$ , and $\lambda = ( w - \hbar \omega ) $ . which is an eigenvalue equation $t ( v ) = \lambda v$ . thus , $a^\dagger \psi$ is an eigenvalue of $h$ with eigenvalue $ ( w-\hbar \omega ) $ .
you are assuming that the kruskal–szekeres ( u , v ) coordinates have to be defined in terms of the schwarzschild ( r , t ) coordinates , but there is nothing special or fundamental about the schwarzschild coordinates . general covariance says that we can use any coordinates we like . if the k-s coordinates had been the ones originally chosen by schwarzschild , then someone could have come along later and defined ( r , t ) in terms of ( u , v ) . we would then say that a disadvantage of the ( r , t ) coordinates is that they do not cover all four quadrants . in general , if you have a solution to the einstein field equations , it makes sense to extend it as much as possible . physically , if a test particle reaches a nonsingular point in a finite proper time , then its world-line can and should be extended beyond that point , not just terminated there . it is only at singular points that the laws of physics break down and geodesics can not be extended . in the case of the schwarzschild spacetime , extending it in this way results in all four quadrants of the kruskal diagram . the maximally extended schwarzschild spacetime is not a realistic model of a black hole , however . when a black hole forms by gravitational collapse , you do not get quadrants iii and iv .
as a completely engineering / practical statement , you would not let the first balloon burst before the second takes over . it would be better to use some more gentle engineered systems to transition to different stages designed for different altitude ranges . but provided you did this , could you continuously increase in altitude until you fall into the formal definition of space ? yes , this is the exact principle behind orbital airship proposals . http://en.wikipedia.org/wiki/orbital_airship they actually extend the idea further than what you have taken it . at high altitudes , buoyancy forces are combined with lift ( which is an airship by definition ) and even orbital acceleration to maintain the craft 's altitude . for such high altitudes , you have no choice other than to use $h_2$ gas , as opposed to the favored $he$ for low-altitude airships today . but to the underlying physics problem , yes , what you describe is entirely possible . the gas volume of the later stages does expand as it gets higher too .
the maxwell equations are linear equations , which implies that a linear combination of solutions is a solution itself . that is , if you know the magnetic field $\vec b_1$ arising from a current $\vec j_1$ , and the magnetic field $\vec b_2$ due to a current $\vec j_2$ , you know that the magnetic field in the presence of both currents , i.e. the current $\vec j = \vec j_1 + \vec j_2$ , is just the sum of the magnetic fields , namely $\vec b = \vec b_1 + \vec b_2$ . this is a little harder to implement using your formula , as it only gives you the magnitude of the magnetic field and not its direction , but i am positive that you will be able to figure that out , using , for example , the right hand rule on both wires to determine the direction of the magnetic field and then add the relevant components together .
let 's assume mass of the person plus spacesuit to be $m_1$=100kg asteroid density : $\rho=$2g/cm$^3$ ( source ) that is 2 000kg/m$^3$ 15km/hour is a good common run . that is roughly v=4m/s the orbital height is negligible comparing to the radius , assume 0 over surface . linear to angular velocity ( 1 ) : $$ \omega = {v \over r } $$ centripetal force ( 2 ) : $$ f = m r \omega ^2 $$ gravity force ( 3 ) : $$ f= g \frac{m_1 m_2}{r^2} $$ volume of a sphere ( 4 ) : $$ v = \frac{4}{3}\pi r^3 $$ mass of a sphere ( 5 ) : $$ m_2 = v \rho = \frac{4}{3}\pi r^3 \rho $$ combining ( 1 ) , ( 2 ) , ( 3 ) , reducing : $$ { m_1 r v^2 \over r^2 } = g { m_1 * m_2 \over r^2 } $$ $$ r v^2 = g m_2 $$ combining with ( 5 ) $$ r v^2 = g \frac{4}{3}\pi r^3 \rho $$ $$ r^2 = \frac{v^2}{\rho g \frac{4}{3}\pi} $$ $$ r = v ( {\frac{4}{3}\pi g \rho} ) ^{-{1 \over 2}} $$ substituting values : $$ r = 4 ( {1.33333*3.14159* 6.67300*10^{-11} * 2000} ) ^{-{1 \over 2}} $$ that computes to roughly 5.3 kilometers more interestingly , the radius is directly proportional to the velocity , $$ r [ m ] = 1.337 [ s ] * v [ m/s ] = 371.51 [ h/1000 ] * v [ km/h ] = 597 [ m*h/mile ] * v [ mph ] $$ so , a good walk on a 2km radius asteroid will get you orbiting . something to fit your bill would be cruithne , a viable target for a space mission thanks to a very friendly orbit . note , while in rest on cruithne , the astronaut matching the m_1=100kg would be pulled down with force of 4.5n while not in motion . that is like weighing about 450g or 1lbs on earth .
what you have worked out so far is the left hand side of the answer . you have written that the sum of the accelerations is zero , but that is not correct because the system is being driven by the roller moving across the sinusoidal surface . parameterize in terms of time by writing $x=ut$ , then rewrite your $y_0 ( x ) $ equation as $y_0 ( t ) $ . now that you have the vertical position of the roller as a function of time , you can find its acceleration by taking the derivative twice with respect to time . the result should look comforting . this new acceleration term goes with the force that drives the system .
it is important to remember that quantum field theory is a theory about fields , not particles . i know you said shy away from equations , so i am just going to reference one part of one , and you can see this equation on any o'l web site , like wikipedia . take the dirac equation , here there is a quantity $\psi$ that shows up . and part of the history of this $\psi$ was what it meant . ultimately , it was determined to be a field : the fermion field . this is our fundamental understanding as of now about the world , that there are fields , and that interactions take place between fields , mediated by quantum excitations of these fields . in light of this , the wave function you talked about corresponding to the electron is not the fermionic field i mentioned above . the fermion field can be excited either to produce or destroy certain fermions like electrons and positrons . as far as how deep the oscillator analogy runs , i will just say this : how deep or how far it runs is debatable , but i do not think anyone will argue its fundamental role in developing qft . quantizing fields and placing field variables in terms of canonical field variables is pivotal for an understanding of qft , and before even getting to qft , a good understanding of the sho in quantum mechanics is indispensable . this is because the creation and annihilation of excitations in qft is analogous to the creation and annihilation of energy states in the non-relativistic quantum sho . i hope this helps .
( i am considering the speakers are emitting some kind of music or something nonperiodic , the situation gets a bit boring if you consider a uniform source ) it basically means alice hears nothing . atleast , not until bob crosses ( at which time your equation is no longer valid , the $-$ in the denominator becomes a $+$ ) . she hears a sonic boom as bob crosses her , and then hears two sounds at once . the first sound is whatever is being played by bob after he crosses her , at a frequency $\frac{f}{3}$ . the second , more interesting sound , is that whatever sounds were emitted by bob are heard backwards , at a frequency $f$ ( this comes from the $-f$ you derived ) . so , if bob was playing mozart 's symphony 23 , and switched to coldplay 's yellow when he passed alice , alice hears : boom ; yellow at one-third the pitch and simultaneously symphony 23 playing backwards . would probably sound horrible ; - ) why is this ? remember , bob 's speed is greater than the speed of sound . so , wavefronts emitted by bob now are much closer to alice than the wavefronts emitted in the past : here , the moving dot is bob , and assume alice to be another dot to the right of bob in his path . the edge of the cone that you see being formed is the " sonic boom " . it is a region of a rapid rise and fall of pressure ( extremely high pressure ) . right after it passes , you see two kinds of wavefronts passing alice . the first is the " left sides of the circular wavefronts " . these have been emitted after bob passed , and are playhed normally , with a third of the frequency ( yellow in my example ) . the other kind is the right side of the circular wavefronts emitted before bob passed alice . as you can see , these are heard top-down , i.e. , the ones emitted last are detected first . for comparison , here is the same diagram if the relative speed was $&lt ; v_0$: to summarize , the negative frequency just means that the sounds emitted at that time are heard " backwards " at a later time--"reflected " at the point in time when bob crosses alice . bonus : http://what-if.xkcd.com/37/
no , general relativity is based on something called " intrinsic curvature " , which is related to how much parallel lines deviate towards or away from each other . it does not require embedding space-time in a higher dimensional structure to work . you are thinking of something called " extrinsic curvature " . in fact , many examples of extrinsic curvature - including your example of a stick being bent - do not have intrinsic curvature at all . let me try to be a bit clearer : imagine there is an ant who lives on your stick . as far as the ant is concerned , the world is one dimensional . now , suppose we tell the ant that space is really 3d and his little 1d world is inside ( "embedded in" ) that 3d space . there is absolutely no way the ant would be able to figure out if his stick was straight or bent the way you are describing . so , this is not the sort of curvature that interests us in general relativity . basically , intrinsic curvature is just concerned with the geometric relationship between nearby points . it is entirely possible to think about this in terms of embedding space-time into some higher dimensional world , but you do not have to : it works just fine if you confine yourself to the observable four space-time dimensions . " curved " in this sense is just a short hand way of saying " parallel lines do not do what they do in ' flat ' ( euclidean or minkowski ) space / space-time " .
comments to the question ( v1 ) : beware that different authors have different conventions for the horizontal order of indices for the christoffel symbols$^1$ $\gamma^{\lambda}{}_{\mu\nu}$ and the riemann curvature tensor $r^{\sigma}{}_{\mu\nu\lambda}$ . some may e.g. write $\gamma_{\mu\nu}{}^{\lambda}$ and $r_{\mu\nu\lambda}{}^{\sigma}$ instead . it may be useful to write objects such as $\gamma^{\lambda}{}_{\mu\nu}$ and $r^{\sigma}{}_{\mu\nu\lambda}$ with covariant and contravariant indices not merely on top of each other a la $\gamma^{\lambda}_{\mu\nu}$ and $r^{\sigma}_{\mu\nu\lambda}$ but horizontally displaced to keep track of the horizontal position of the indices . then the indices can be raised or lowered by a metric $g_{\mu\nu}$ with no ambiguity in notation of which index was raised or lowered . in case of supermanifolds , the indices may correspond to grassmann-odd coordinates , and it may become a tedious exercise in book-keeping to assign consistent transformation laws with correct sign factors for tensor components under coordinate transformations of supercharts . some choices of horizontal orders of indices may be more natural in the sense of minimizing the appearance of sign factors . we mention this 3rd point because both authors barnich and brandt are experts on brst formalism , where grassmann-odd variables play an essential role . -- $^1$ it is covenient to call $\gamma^{\lambda}{}_{\mu\nu}$ christoffel symbols even if the tangent-space connection $\nabla$ is not torsionfree nor compatible with a metric .
i would guess that the screws are secretly grounded through a hidden conducting path inside the wood block and the table . maybe the table is wet , maybe there are metal wires inside the block , i do not know the exact setup , but i think there is some grounding path . this means that they stay at zero potential , no matter what you do to the pvc . so when you charge up the pvc tube , the screws accumulate a large amount of charge to keep their potential zero in the tube background . the induced charge on grounded screws is much larger than the induced polarization on isolated screws , and the pointiness of the screw will lead to huge local fields near them , and will explain your observation . if this is the case , putting big books beneath the block should de-ground the screws . for this explanation to work , the screws do not have to be completely grounded . it is enough if you have the screws go all the way through the wood , and rest the block so that the screws touch a metal table . in this case , the metal will act as a not-so-infinite ground , and will supply charge to the semi-grounded screws . you can fix this by mounting the block on top of a dry book . to test any of these explanations , you can just hold up the block yourself without touching the screws , and rub , and see if you still get the large charge on the screw .
the fermion doubling is manifested through the existence of extra poles in the dirac propagator on the lattice . these poles cannot be made to disappear at the continuum limit . ( the number of doublers can be reduced by different discretizations but not eliminated at all , this is essentially the nielsen-ninomiya theorem ) . the reason for the fermion doubling lies in the existence of chiral anomaly . this anomaly exists in the continuum limit due to the chiral non-invariance of the path integral measure and not because of the non-invariance of lagrangian . in the lattice formulation of a chiral theory based on a discretization of the lagrangian , the anomaly is absent and the lattice formulation generates the extra species just to cancel this anomaly in the continuum limit . since the axial anomaly exists in nature $\pi^0 \rightarrow \gamma \gamma$ , this situation is unacceptable . the fermion doubling problem is an artifact of the realization of the theory by means of quarks where the axial anomaly is not present in the lagrangian but in the path integral measure . there are approaches of other types of discretizations such as by means of fuzzy spaces in noncommutative geometry , where the quarks are not the basic fields of the theory . in these approaches , the fermion doubling problems do not exist . update this is an update referring to marek 's first comment . for fermions , the axial anomaly can be recovored only by a non-trivial regularization of the path integral measure of the fermions . any finite dimensional approximation of this measure as a product of berezin-grassmann integrals does not produce the axial anomaly . this is the reason why the lattice regularization does not produce the axial anomaly and as a consequence the phenomenon of doubling occurs where the different species of doublers are of opposite chirality to cancel the anomaly . this is a property of fermion fields represented by grassmann variables . in effective field theories ( where the basic fields are pions ) such as sigma models , the axial anomaly is manifested through a wess-zumino-witten term in the lagrangian , but these theories are not even perturbatively renormalizable and i think this is the reason why they were not put on a lattice . one approach that i know of which solves the fermion doubling problem is the regularization by means of a fuzzy space approximation of the space-time manifold . the philosophy of this approach is explained in the introduction of mark 's rieffel 's article . the resolution of the fermion doubling using this approach is given nicely in badis ydri 's thesis . ( there is also more recent work on the subject by a . balachandran and b . ydri in the arxiv ) . the main idea is that the poisson algebra of functions over certain spaces ( such as the two sphere , and more generally coadjoint orbits of compacl lie groups ) can be approximated by finite dimensional matrices . these approximations are called fuzzy spaces . gauge fields and fernmions can be constructed on these fuzzy spaces , which have the correct continuum limit when the matrix dimensions become infinite . this formulation contains the axial anomaly inherently , thus it is free of the doubling problem . the only drawback that i can see of this approach is that it is applicable only to some special 4-dimensional manifolds such as $cp^2$ or $s^2 \times s^2$ , because the fuzzy manifold is required to be berezin-quantizable .
you can push the air faster than the speed of sound . if you do that , you will get a shock wave . a shock wave in this sense is a " wall " of supersonic-moving particles . you can definitely achieve this if you push on the air hard enough . a nuclear explosion is definitely " hard enough " : ) a shock wave will collide with " normal " stationary air , and give some of the energy to it . as the energy spreads to larger and larger volumes of air , the shock wave decays into a " normal " sound wave pulse . but before that , the wall of highly compressed air will travel at supersonic speed . a nuclear fireball is the region where the air and the debris from the explosion is so hot that it glows . in the first moments of the explosion , the shock wave compresses the air so hard , that it heats up and glows . as the shock wave loses energy , it will lose it is glow , first " redshifting " and then disappearing almost completely . this phenomenon can be seen in this video : https://www.youtube.com/watch?v=kqp1ox-sdri
i believe that andy and cumrun did not want to say that this manifold would have a complex structure modulus in isolation . however , as is clear from the " conifold " setup , the manifold given by $xy-zt=\mu$ is being incorporated into a larger manifold , so this equation only describes the vicinity of some region . when you exploit the fixed asymptotic shape of the $xy-zt=\mu$ manifold which is $\mu$-independent and when you extend this deformed conifold geometry into a larger manifold , such as the quintic , the parameter $\mu$ connected with the neighborhood of the ( deformed ) singularity becomes a complex structure modulus labeling inequivalent complex structures of the whole ( complicated ) calabi-yau manifold . alternatively , you could count the single complex modulus even for this simple manifold itself but you would have to impose the constancy of the asymptotics i.e. ban your " scaling " transformations that were used to show the equivalence regardless of $\mu$ .
i found the answer to my problem , and as expected it is embarrassingly trivial . put $z=\epsilon$ and expand everything to first order , then the result comes out directly . with respect to references , i found that the review paper $\mathcal w$ symmetry in conformal field theory contains a discussion of these kind of coset wess-zumino-witten models . furthermore papers on the newly proposed higher spin ads$_3$/cft$_2$ duality contains some discussions on this ( for example arxiv:1011.2986 , arxiv:1108.3077 and arxiv:1106.1897 ) . but i still welcome better references !
the kind of brain wave i have heard of is the result of the electrical activity inside a neurons ( nervous tissue cells ) . when in synergy ; neurons produce 6 kinds of waves , called alpha , beta , gamma and delta , mu and theta . they are differentiated by the frequency in them . they also originate from different parts of the brain . highest frequency waves represent higher neural activity , while lower freq . waves represent slower or passive activity . delta waves have lowest freq . and are considered to originate when we are in rem sleep . brain waves are recorded using eeg . but this thing about altering experiment , i have never read . anyways , it seems to me like a non-proved mystical experiment . there are many similar kind of documentaries being shown in discovery and nat geo . but i will not give much credibility to these claims , since if it would have been proved then there would have not been unnoticed by the main stream psychologist and physicist . edit1: there is one hypothesis by penrose , he calls the quantum mind . he argues that human brain has a quantum mechanical component , which makes the computation in the brain both quantum computation and hypercompution ( capable of doing infinitely many steps in a finite time ) . this idea comes from his conviction that we must be more than computers , and the fact that all natural phenomena ( with the exception of quantum gravity , since it is not completely understood ) are described by turing computation . he has explained some of these ideas in his book " the emperor 's new mind " . penrose does not lend support to the bogus nonsense in this question , and never has . in a related vein , he also said , regarding other things , that the mind is non-local , and one can influence an external entity by becoming conscious of it . there is nothing particularly strange about either statement , so long as one does not take it to mean nonsense like this experiment .
the speed of light in vacuum is constant and does not depend on characteristics of the wave ( e . g . its frequency , polarization , etc ) . in other words , in vacuum blue and red colored light travel at the same speed c . the propagation of light in a medium involves complex interactions between the wave and the material through which it travels . this makes the speed of light through the medium dependent on multiple factors which include the frequency ( other example factors being refraction index of the material , polarization of the wave , its intensity and direction ) . the phenomenon due to which the speed of a wave depends on its frequency is known as dispersion and is the reason why prism and water droplets separate white light into a rainbow .
the value of $\rho_b$ is incorrect . check the formula you are using . use $\vec{n}\cdot\ ( \vec{d_{out}}-\vec{d_{in}} ) =\sigma_f$ $\vec{n}\cdot\ ( \vec{p_{out}}-\vec{p_{in}} ) =\sigma_b$ on the respective surfaces .
it is not a "$g ( 2 ) $ lattice " one has to compactify the m-theoretical dimensions upon ( after all , the $g_2$ lattice is 2-dimensional ) ; it is the $g_2$ holonomy manifolds . there are lots of different topologies of these seven-dimensional manifolds . they are analogous to the calabi-yau manifolds but do not allow one to use the machinery of complex numbers .
in the picure below , you can see how the row-column grid correspond to the graphene structure . to have a ( n , m ) nanotube , you " just " have to roll your graphene sheet so that the ( 0,0 ) hexagon coincides with the ( n , m ) hexagon . of course , it is much easier said than done !
my understanding is that at the speeds involved the shear forces inside the material are so tremendous that the target no longer behaves rigidly , but more like a liquid , which you can see in slow motion videos of bullets hitting metallic targets . because the bullet is moving so fast , the " information " that the bullet has hit the target travels faster than sound , i.e. , a shockwave , giving rise to these enormous forces . once the shear strength of the material is overcome , the friction/drag between the bullet and the target is diminished , so that the majority of the bullet 's momentum makes it out the other end . if you want to send your iphone flying , equip it with a bullet-resist vest , which can handle the forces in question . think of hitting an apple with a cleaver . if you do it slow enough , you are not providing enough shear force for ( some of the ) the cell walls to tear , so the apple stays in tact , and moves with the cleaver . if you do it fast enough , you end up with two halves that barely move .
it is $\sqrt{\frac {gm}{r}}$ , where $m$ is the mass of the body ( earth ) the spacecraft is revolving around , $r$ is the distance from the center of that body ( earth ) , and $g$ is the universal gravitational constant . this can be easily derived from equating centripetal acceleration and gravitational attraction , i.e. $$\frac {mv^2}{r} = \frac {gmm}{r^2}$$ note : this is only applicable for spherical bodies .
as sebastian henckel said , there are many such conservation laws in non-relativistic physics which prohibits particle creation and annihilation . the number of particles of each allowed type is conserved separately . to see the symmetry associated with these numbers , e.g. the total number $n$ of elementary particles , one has to realize that the symmetry variation of an observable $l$ is equal to $$\delta l = \epsilon\{n , l\}$$ where $n$ is the symmetry generator and the bracket is a poisson bracket . clearly , $n$ depends neither on positions $x$ nor on the momenta $p$ which is why the bracket vanishes and the symmetry acts trivially – nothing transforms under it . so you may say that there is a symmetry and it is formally isomorphic to $u ( 1 ) $ but nature only allows objects that are invariant under it so you will never see any " change " linked to the symmetry . quantum mechanically , the bracket is replaced by $1/i\hbar$ times the commutator which is nonzero . the ket vectors simply transform as $$|\psi\rangle \to \exp ( i\lambda n ) |\psi\rangle$$ under the symmetry you are looking for . this is just the overall change of the phase which does not change the physical ( measurable ) properties of the ket vector . because the number of particles is conserved so perfectly , any complete enough measurement of the system will find it in an eigenstate of $n$ . the conservation law implies that the final state will be an eigenstate , too . so after one complete enough measurement , the subspaces of the hilbert space with different eigenvalues of $n$ are pretty much separated from each other – we say that they live in different superselection sectors . let me comment on a word in the title , " approximate " . because the conservation law is exact , the symmetry has to be exact as well , of course . it is uninteresting not because it is violated or approximate but because it acts trivially .
most materials contract on cooling . the notable exception to the rule are some phase transitions and water . but even ice contracts on cooling . water expands on cooling only between $0^\circ\text{c}$ and $4^\circ\text{c}$ ( including phase transition ) . this corresponds to the part of the graph below , in which density rises with temperature ( note suppressed zero ) . as regarding to what material contracts most , what are you looking for is the coefficient of linear thermal expansion $\alpha$ $$\frac{\text{d}l}{l} = \alpha \delta t . $$ there is plentiful of various tables available on web , e.g. http://www.engineeringtoolbox.com/linear-expansion-coefficients-d_95.html it seems plastic materials contract most on cooling . ethylene ethyl acrylate ( eea ) for example has the largest one coefficient among the solids in this table .
the spacetime in general relativity does not contain " holes " in the sense of excized regions because of a physical argument--- if you can shoot a particle at the region , it should continue into the region . this is the reason that geodesic completeness is used instead of completeness in gr . the condition of geodesic completeness says that the manifold must not have places where geodesics stop for no reason . of course , the singularity theorems guarantee that geodesic completeness fails inside a black hole . but the failure in the case of time-like singularities is mild--- the singularity is only reachable by light rays . the closest thing to an excized region is a black hole . the interior is excized in the sense that it is disconnected causally from the exterior . you can remove the interior and simulate the exterior only ( classically ) and you do not expect to run into too many troubles . whether this is completely true in the quantum version is not clear to me . as for other topological quantities , you can put them in by hand , but it is not clear if they can appear dynamically . there is the topological censorship conjecture , which states that you will not be able to see a topological transition in classical general relativity . i do not know the status ( or even the precise statement ) of this conjecture .
there is no evidence that the global earthquake frequency has changed significantly over that last few centuries . because the physical mechanism responsible for earthquakes is motion along faults , we believe there have been earthquakes for as long as the earth has had a crust and lithosphere comparable to today . the oldest continental rocks dated , and ample geological evidence , indicate that plate tectonics has been ongoing for over 4 billion years . there may have been some variation in the rate of tectonic processes over time , but there is no evidence that plate tectonics is ' slowing down ' because the earth is cooling . you also should not assume that the temperature of the earth 's core has been decreasing over this time . the radioactive decay and gravitational differentiation of the core could provide heat source in the earth 's interior . prior to the formation of the crust , billions of years ago , the processes of the primordial earth would have been different than the processes responsible for earthquakes today .
this was something that confused me for awhile as well until i found this great set of notes : homepages . physik . uni-muenchen . de/~helling/classical_fields . pdf let me just briefly summarize what is in there . the free klein-gordon field satisfies the field equation $ ( \partial_{\mu} \partial^{\mu} +m^2 ) \phi ( x ) = 0$ the most general solution to this equation is $\phi ( t , \vec{x} ) = \int_{-\infty}^{\infty} \frac{d^3k}{ ( 2\pi ) ^3} \ ; \frac{1}{2e_{\vec{k}}} \left ( a ( \vec{k} ) e^{- i ( e_{\vec{k}} t -\vec{k} \cdot \vec{x} ) } + a^{*} ( \vec{k} ) e^{ i ( e_{\vec{k}} t- \vec{k} \cdot \vec{x} ) } \right ) $ where $\frac{a ( \vec{k} ) + a^{*} ( -\vec{k} ) }{2e_{\vec{k}}} = \int_{-\infty}^{\infty} d^3x \ ; \phi ( 0 , \vec{x} ) e^{-i \vec{k} \cdot \vec{x}} $ and $\frac{a ( \vec{k} ) - a^{*} ( -\vec{k} ) }{2i} = \int_{-\infty}^{\infty} d^3x \ ; \dot{\phi} ( 0 , \vec{x} ) e^{-i \vec{k} \cdot \vec{x}}$ introducing an interaction potential into the lagrangian results in the field equation $ ( \partial^{\mu} \partial_{\mu} + m^2 ) \phi = -v&#39 ; ( \phi ) $ choosing a phi-4 theory $v ( \phi ) = \frac{g}{4} \phi^4$ this results in $ ( \partial^{\mu} \partial_{\mu} + m^2 ) \phi = -g \phi^3$ introduce a green 's function for the operator $ ( \partial^{\mu} \partial_{\mu} + m^2 ) g ( x ) = -\delta ( x ) $ which is given by $g ( x ) = \int \frac{d^4k}{ ( 2\pi ) ^4} \ ; \frac{-e^{-i k \cdot x}}{-k^2 + m^2}$ now solve the full theory perturabtively by substituting $\phi ( x ) = \sum_{n} g^n \phi_{n} ( x ) $ into the differential equation and identifying powers of $g$ to get the following equations $ ( \partial^{\mu} \partial_{\mu} + m^2 ) \phi_0 ( x ) = 0$ $ ( \partial^{\mu} \partial_{\mu} + m^2 ) \phi_1 ( x ) = -\phi_0 ( x ) ^3$ $ ( \partial^{\mu} \partial_{\mu} + m^2 ) \phi_2 ( x ) = -3 \phi_0 ( x ) ^2 \phi_1 ( x ) $ the first equation is just the free field equation which has the general solution above . the rest are then solved recursively using $\phi_0 ( x ) $ . so the solution for $\phi_1$ is $\phi_1 ( x ) = \int d^4y\ ; \phi_0 ( y ) ^3 \ , g ( x-y ) $ and so on . as is shown in the notes this perturbative expansion generates all no-loop feynman diagrams and this is the origin of the claim that the tree level diagrams are the classical contributions . . .
remember that your expression gives the energy difference per unit volume . so you need an additional factor of $\left [ l^{-3}\right ] $ on the left hand side .
the result is straight forward . as landau and lifshitz explain in p . 41 , when a body disintegrates into two pieces of masses $m_1$ and $m_2$ respectively , their momenta must be equal in magnitude and oppositely directed by the law of conservation of momentum . so , let each body have momentum $p_0$ . then , $ ( 16.1 ) $ and $ ( 16.2 ) $ say that the difference in the internal energies equals the kinetic energy of the reduced mass . $e_i - e_{1i} - e_{2i} = \frac{p_0 ^2}{2m}$ where $\frac{1}{m} = \frac{1}{m_1} + \frac{1}{m_2}$ let us now see what happens when the body splits into more than two parts . of course , we cannot say anything about the magnitudes and directions of the momentum of each body . but , we shall try to obtain some information about maximum possible kinetic energy and so on . so , now we have one part with mass $m_1$ that we are interested in . if we sum the momenta of the remaining parts , we can conclude that the total momentum of the remaining parts is equal in magnitude and directed oppositely to the momentum of $m_1$ just like the previous case . let it be $p_0$ . hence , we can now use the same equations for masses $m_1$ and $m-m_1$ $\frac{p_0 ^2}{2} ( \frac{1}{m_1} + \frac{1}{m-m_1} ) = e_i - e_{1i} - e_{i}^{'}$ which on simplification gives $\frac{p_0 ^2}{2m_1} \frac{m}{m - m_1} = e_i - e_{1i} - e_{i}^{'}$ and the result follows .
as jan noted , the hamiltonian should have a minus sign : $h=\frac{ ( p-qa ) ^2}{2m}$ where $p$ is the canonical momentum , and the expression $p-qa$ is the kinetic momentum $p$ . a homogenous magnetic field is an interesting case , because the vector potential in a given gauge does not exhibit translation invariance , but the physical system clearly does . the solution to this dilemma is that you can preserve translational invariance by changing the gauge as you move the coordinates . there is a conserved quantity associated with this symmetry , but it turns out not to be the canonical momentum ( or the kinetic momentum , either ) . i do not know if it has a particular name , and since it is gauge-dependent there is no universal expression you can write for it . but , for example , in the gauge where $a=\frac{b}{2} ( -y , x ) $ , it is just $ ( p+qa ) $ . if anyone has an insight into any physical interpretation of this quantity i would be interested to hear it . there is a very nice example of all this , worked out concretely , in these notes .
the angular size of the object can be calculated by basic trigonometry : $\theta=2\cdot \arctan ( r/d ) $ , where $r$ is the radius of the object you are viewing , and $d$ is the distance between you and the object ( $\theta$ is the angle ) . the average ( volumetric ) radius of saturn is 58,232 km . the distance between titan and saturn is 1,221,830 km . plugging the numbers in gives an angular size of 5.46° . doing the same for our moon gives you 0.52° . dividing one by the other gives you a factor of $\sim 10.5$ difference . &nbsp ; note 1: when you do this math with a calculator , verify you get the correct results for the moon from earth before you go on to something else . you may encounter issues where the results of your arctan ( ) function will be given in radians , not degrees . if the math gives you a weird result , multiply by $180/\pi \approx 57.3$ . note 2: saturn would not actually be visible from the surface of titan due to the thick atmosphere of the moon . also , tidal locking has nothing to do with this problem other than if saturn may be visible from an arbitrary location on titan ( if you could see through its atmosphere ) .
part of why you do not see colors in astronomical objects through a telescope is that your eye is not sensitive to colors when what you are looking at is faint . your eyes have two types of photoreceptors : rods and cones . cones detect color , but rods are more sensitive . so , when seeing something faint , you mostly use your rods , and you do not get much color . try looking at a color photograph in a dimly lit room . as geoff gaherty points out , if the objects were much brighter , you would indeed see them in color . however , they still would not necessarily be the same colors you see in the images , because most images are indeed false color . what the false color means really depends on the data in question . what wavelengths an image represents depends on what filter was being used ( if any ) when the image was taken , and the sensitivity of the detector ( eg ccd ) being used . so , different images of the same object may look very different . for example , compare this image of the lagoon nebula ( m8 ) to this one . few astronomers use filter sets designed to match the human eye . it is more common for filter sets to be selected based on scientific considerations . general purpose sets of filters in common use do not match the human eye : compare the transmission curves for the johnson-cousins ubvri filters and the sdss filters the the sensativity of human cone cells . so , a set of images of an object from a given astronomical telescope may have images at several wavelengths , but these will probably not be exactly those that correspond to red , green , and blue to the human eye . still , the easiest way for humans to visualise this data is to map these images to the red , green , and blue channels in an image , basically pretending that they are . in addition to simply mapping images through different filters to the rgb channels of an image , more complex approaches are sometimes used . see , for example , this paper ( 2004pasp . . 116 . . 133l ) . so , ultimately , what the colors you see in a false color image actually mean depends both of what data happened to be used to be make the image and the method of doing the mapping preferred by whoever constructed the image .
in statistical mechanics and thermodynamics you are describing systems with an extremely large number of possible variables or degrees of freedom , so describing exactly what happens becomes impossible . instead , you describe the average . to do this , you consider all physically possible configurations of your system , and say they are all equally probable . you use this ensemble of hypothetical systems to determine the average values of physical properties of your system . in nearly every case , the average value is dominated by the configurations " near " to your most probable configuration , and fringe cases ( like all the atoms in your room being in a cubic cm volume at the same time ) are impossibly rare . however , if you have arrangements of your system that are close enough to the most probable , they have a reasonable chance of occurring . so , for example , energy can transfer from a hot system to a cold system just from pure random chance of the collision between molecules in each system . this random movement around the most probable configuration of your system is " noise , " and you can apply similar logic to any system described in a statistical manner .
poking around on google with various search terms that included " parachute shape " i came upon " the parachute manual " by dan poynter . table 8.1.7 from that book catalogs empirical data on a host of parachute shapes . assuming that the parachute is round , as you say , and does not have any holes ( apparently , many designs purposefully include gaps to improve stability ) , the first section of that table ( on page 457 ) is the one to look at . to interpret the terms of the table , i tried reading the glossary . it gives the following three definitions : nominal diameter : the diameter of a circle made with the same amount of cloth as the parachute . projected diameter : the diameter of the real parachute when it is inflated . constructed diameter : the diameter of the real parachute when it is not inflated . from this it is clear that the table says " nominal diameter " but means " constructed diameter . " the table is in fact using a nominal diameter of 1 for all the parachutes it lists . using this correction , the table reveals that the ratio between projected diameter and nominal diameter for round parachutes varies between 0.6 and 0.7 this means that given the projected radius , the area is , roughly speaking , somewhere between $2\pi r^2$ and $3\pi r^2$ .
light is a wave - an electromagnetic wave . radio waves and microwaves are also electromagnetic waves , they just have different wave lengths . wikipedia has a nice picture showing the electromagnetic spectrum why does it have a wavelength . . . it has a wavelength because there is physical space between the peaks of the waves - it is a real , physical , wave . just like water waves and sound waves , you can do " wave things " to light waves , such as send them through diffraction gratings and see the interference . and what creates its wavelength whatever creates the light gives it energy , and the wavelength is proportional to that amount of energy : $$\lambda = \frac{hc}{e}$$ does the light vibrate too ? if so , then how ? sound waves are energy waves that compress matter - you can not have a sound wave in a vacuum . light waves are energy waves too , but they do not need matter to go forward . that is why we can see sunlight , but we can not hear the sun . ( and that is why in space , no one can hear you scream . ) this subject can get really complicated really fast . because although light is a wave , it is also a particle . a lot of really smart people have been scratching their really smart heads over that , and will be for a long time .
the white cloud you see in the water is steam bubbles . the grains of salt provide nucleation sites that allow the water to vaporize as they fall through the superheated liquid ( so bowlofred had it right--although it is steam that is forming , not dissolved gasses coming out of solution ) . if you raise a pot of water to near boiling and toss in a handful of salt , the water can explode out of the pot due to this effect . counter to what you might think , the addition of salt to water actually raises the boiling point by about one half degree celsius for every 58 grams of salt dissolved per kilogram of water . so the steam is not caused by salty water around the dissolving salt crystals boiling at a lower temperature . the nucleation effect diminishes as the salt diffuses throughout the water . note that the effect is not limited to salt . if you toss in something that does not dissolve--like sand--you will see the same nucleation effect .
as stated , $\mathbf{n}$ is a unit vector and $n_x$ , $n_y$ and $n_z$ are its cartesian components . $\mathbf{n}$ is just a vector pointing in an arbitrarily direction with magnitude 1 . taking $\mathbf{n} \cdot \mathbf{\sigma}$ , we have \begin{equation} \mathbf{n} \cdot \mathbf{\sigma} = n_x\sigma_x + n_y \sigma_y + n_z \sigma_z \\ = n_x \left ( \begin{array}{cc} 0 and 1\\1 and 0\end{array}\right ) + n_y \left ( \begin{array}{cc} 0 and -i\\i and 0\end{array}\right ) + n_z \left ( \begin{array}{cc} 1 and 0\\0 and -1\end{array}\right ) \end{equation}
$$i=\sum\lambda_x^2i_x+\sum_{cyc}2\lambda_x\lambda_yi_{xy}$$ where $\lambda_x \vec i+\lambda_y\vec j+\lambda_z\vec k$ is the unit vector along which moment of inertia is required . $i_{xy}$ is $\sum xy dm$ which is zero due to symmetry when you take origin at centre of ring as for every pair $ ( x , y ) $ you can find $ ( -x , -y ) $ . also , other terms in second summation contain $z$ term which is zero as body is in $xy$ plane . hence , the second summation is zero . now we should talk about first summation . $i_z=mr^2$ and $i_x=i_y=\frac 1 2 i_z$ . the unit vector you want is $\frac{1}{\sqrt 2}\vec i+\frac{1}{\sqrt 2} \vec k$ . you can apply the above formula you can derived the above general result easily from vectors and taking cross products . assume a tiny particle at $\vec r ( x , y , z ) $ and its distance from required axis is $d=|\vec r \times \vec\lambda|$ . now find $md^2$ .
it is spinning forever . as you see , change of angular momentum $$\frac{\text{d}\vec{l}}{\text{d}t} = \vec{\tau}$$ is always perpendicular to angular momentum itself , which means that angular momentum 's direction is changed , while its magnitude is constant . note the mathematical analogy with velocity and acceleration in case of circular rotation with constant velocity : $$\frac{\text{d}\vec{v}}{\text{d}t} = \vec{a}_\text{cp}$$
the actual reason why one can not interpret the equation $$ \nabla_\mu t^{\mu\nu}=0 $$ as a global conservation law is that it uses covariant derivatives . if a law like that were valid with partial derivatives , you could derive such a law . but there is a covariant derivative which is one of the technical ways to explain that general relativity in generic backgrounds does not preserve any energy : http://motls.blogspot.com/2010/08/why-and-how-energy-is-not-conserved-in.html the text above also explains other reasons why the conservation law disappears in cosmology . however , despite the non-existence of a global ( nonzero ) conserved energy in general backgrounds , the tensor $t_{\mu\nu}$ is still well-defined . as twistor correctly writes , it quantifies the contribution to the energy and momentum from all matter fields ( non-gravitational ones ) and matter particles . and if you can approximate the background spacetime by a flat one , $g_{\mu\nu}=\eta_{\mu\nu}$ , which is usually the case with a huge precision ( in weak enough gravitational fields , locally , or if you replace local objects that heavily curve the spacetime , including black holes , by some effective $t$ , using a very-long-distance effective description ) , then $\nabla$ may be replaced by $\partial$ in the flat minkowski coordinates and the situation is reduced to that of special relativity and the " integral conservation law " may be restored .
in qft , we reinterpret the probability density as the probability charge density . in other words , negative probabilities correspond to antiparticles . in fact , the dirac equation which describes spin-1/2 also has this property , and it led to the prediction of the positron as the antiparticle of the electron .
the answers given by @annav and @chrischarles , got me to the answer that the documentary possibly refers to the most recent experiment of the wheeler 's delayed choice experiment , because it is called the first " clean " experimental test of wheeler 's ideas , and one posible interpretation of the experimental data is that they were able to change retroactively the particle/wave behaviour . also this page has a pretty good description of the experiment , and explains its interpretations . this is the abstract of the article on science magazine , the full text is free upon registration . as a personal note : maybe the subatomic particles behave as both , waves and particles , and by meassuring we just change the image projected onto our perceived reality of some hiper-particle that in essense remains basicaly unaltered in the hiper-space .
you can have any function be the phase of an oscillator . whatever $f ( t ) $ is , you can speak of $\cos ( f ( t ) ) $ your particular expression would make particular sense for phase modulation of a wave . the carrier wave is at frequency $\omega$ . normally the phase modulation would have smaller amplitude than $1$ and probably a lower frequency than $\omega$ , but we can plot $\cos ( x+\cos ( x ) ) $ and get something that is nicely periodic , though distorted from a standard cosine wave .
water molecules don’t carry an electric charge ( and if they do , you don’t want them on your hands… ) . the dipole moment of water molecules can only be used to rotate them in space , not to move them . additionally , the forces that apply to water molecules on your hands also apply to water molecules in your hands . so even if you somehow managed to apply a sensible force on these water molecules , this would get rather uncomfortable . the same problem arises if you attempt to heat them up by means of electric resonance ( similar to a microwave ) . i therefore doubt that it would be possible to build a device based on electric fields rather than moving air , that removes water molecules from the surface of your skin . however , it might be possible to vaporise the water on your hands using strong infrared lamps . this might lead to other problems , though , such as the focusing of infrared radiation on small areas of the skin by water drops .
in the picture you posted the collector is on the other side of the tower so you can not see it . have a look at this pdf for detailed pictures of the tower - the collector is shown on page 40 , and it is indeed black .
instead of using catapults , magnetic propulsion systems ( especially superconducting ) would be more useful for a non-rocket space-launch . particularly , the energy required for an average guy ( 70 kg ) to get outta this world would be $43.904 × 10^8 j$ your catapult is somewhat comparable to a space gun . but , it cannot place the payload in a stable orbit , since gravity certainly would not allow that . . . mass drivers which use superconducting coils would be a more efficient ( more than 90% ) of attaining such a large amount of kinetic energy ( escape velocity ) with a single lift . but , they are all in the future proposals list . . . they are so much powerful and probably do not have a weight limit . yes , we could launch satellites directly into their geostationary orbits without the use of rockets . even though it would be a lot of success , some million new bills should be invested to obtain it . . . also , i will add that space elevators would be more useful when comparing budget . all those wikipedia links are quite good in this subject . . . edit : okay . . . a trebuchet consists of two arms - projectile arm and main weight arm . the ratio of the length of the main weight arm to the length of the projectile arm is typically between 1/2 and 1/5 . the main weight arm has the counter-weight required to shoot the projectile and it should always be in multiples of the projectile weight ( the height of main weight arms also matters here ) . the angle we use commonly for a projectile is 45° . here to attain the escape velocity ( at least close to ) , you must use at least a million ton counter weight , a 10 lbs projectile , and a trebuchet more than 2 mile long , and use an angle of 90° would be helpful . . ! theoretically possible i would say . . . refer these trebuchet range and projectile range papers . . . edit : your first two questions were reasonable . but now , it is quite impossible to attain . as i have already told , gravity would not allow your projectile to be placed in a geo-stationary orbit . even though there is no air resistance ( friction in atmosphere ) and you have broken the escape velocity , you had escape into outer space and never return . . ! hence in the absence of a rocket propulsion system , you require at least some kinda magnetic systems ( not just wood and clockworks . . ! ) @ehryk : if those papers were not useful , here is an information from a simulation which gives an idea . there are some software such as atreb , wintreb or trebuchet simulator . . . you could refer the list of simulators provided . . .
it seems that the cart is always released at the top . it is the first picket fence which is moved . thus smaller $d$ means you confine the measuring too the high speed regions of the track . at the last measuring you effectively measure the final velocity of the cart at the bottom . it is a nice lab , i remember a question about air track --- where the viscous friction take place ? standard error is the standard deviation of the error . just standard deviation of the results tells you how do the results scattered . standard error tells how accurate is the average of the results . you see , the more experiments you do , the more accurate is the average , right ? to answer is your calculation right , you should tell standard error of what quantity are you interested in ? in any case , there are too much digits in your stderr , you should leave in general only 1 significant digit , or maybe 2 for if the first digit is 1 . basically i suggest you reading your lab book , the beginning . it should tell what are the different quantities , how do you calculate them etc . it is really useful , you read it once for an hour and you are prepared for all the consequent labs , statistics courses , statistics modeling and so on .
the voltage across either horizontal resistor is zero so they can be removed from the circuit without changing the solution . this is most easily seen by simply removing the two horizontal resistors and then it is clear that the nodes the horizontal resistors connect to each have the same voltage . thus , by ohm 's law , there is no current through either horizontal resistor since there is no voltage across either resistor . in other words , it does not matter if the horizontal resistors are there or not so they can be removed without changing the solution . you are then left with 3 identical paths with resistance 2r each . of course , in a real circuit , the real resistors will never be identical so , this ideal solution is only an approximation .
the copenhagen interpretation consists of two parts , unitary evolution ( in which no information is lost ) and measurement ( in which information is lost ) . decoherence gives an explanation of why information appears to be lost when it in actuality is not . " the decay of the off-diagonal elements of the wave function " is the process of turning a superposition : $$\sqrt{{1}/{3}} |{\uparrow}\rangle + \sqrt{{2}/{3}} |{\downarrow}\rangle$$ into a probabilistic mixture of the state $|\uparrow\rangle$ with probability $1/3$ , and the state $|\downarrow\rangle$ with probability $2/3$ . you get the probabilistic mixture when the off-diagonal elements go to 0 ; if they just go partway towards 0 , you get a mixed quantum state which is best represented as a density matrix . this description of decoherence is basis dependent . that is , you need to write the density matrix in some basis , and then decoherence is the process of reducing the off-diagonal elements in that basis . how do you decide which basis ? what you have to do is look at the interaction of the system with its environment . quite often ( not always ) , this interaction has a preferred basis , and the effect of the interaction on the system can be represented by multiplying the off-diagonal elements in the preferred basis by some constant . the information contained in the off-diagonal elements does not actually go away ( as it would in the copenhagen interpretation ) but gets taken into the environment , where it is experimentally difficult or impossible to recover .
yes , it is possible . this is a well known field of entanglement distillation , whereby one can probabilistically transform less entangled states into more entangled states ( usually maximally entangled ) . however , the values of probabilities ensure that one cannot win . there have been calculations of thresholds of this probability ( for instance http://arxiv.org/abs/quant-ph/0501105 ) . i recommend reading nielsen and chuang , most of the basic quantum information is explained nicely there .
i ) the closest cosmetic resemblance between the nambu-goto action and the polyakov action is achieved if we write them as $$\tag{1} s_{ng}~=~ -\frac{t_0}{c} \int d^2{\rm vol} ~\det ( m ) ^{\frac{1}{2}} , $$ and $$\tag{2} s_{p}~=~ -\frac{t_0}{c}\int d^2{\rm vol}~ \frac{{\rm tr} ( m ) }{2} , $$ respectively . here $h_{ab}$ is an auxiliary world-sheet ( ws ) metric of lorentzian signature $ ( - , + ) $ , i.e. minus in the temporal ws direction ; $$\tag{3} d^2{\rm vol}~:=~\sqrt{-h}~d\tau \wedge d\sigma$$ is a diffeomorphism-invariant ws volume-form ( an area actually ) ; $$\tag{4} m^{a}{}_{c}~:=~ ( h^{-1} ) ^{ab}\gamma_{bc} $$ is a mixed tensor ; and $$\tag{5} \gamma_{ab}~:=~ ( x^{\ast}g ) _{ab}~:=~\partial_a x^{\mu} ~\partial_b x^{\nu}~ g_{\mu\nu} ( x ) $$ is the induced ws metric via pull-back of the target space ( ts ) metric $g_{\mu\nu}$ with lorentzian signature $ ( - , + , \ldots , + ) $ . note that the nambu-goto action ( 1 ) does actually not depend on the auxiliary ws metric $h_{ab}$ at all , while the polyakov action ( 2 ) does . ii ) as is well-known , varying the polyakov action ( 2 ) wrt . the ws metric $h_{ab}$ leads to that the $2\times 2$ matrix $$\tag{6} m^{a}{}_{b}~\approx~\frac{{\rm tr} ( m ) }{2} \delta^a_b~\propto~\delta^a_b $$ must be proportional to the $2\times 2$ unit matrix on-shell . this implies that $$\tag{7} \det ( m ) ^{\frac{1}{2}} ~\approx~ \frac{{\rm tr} ( m ) }{2} , $$ so that the two actions ( 1 ) and ( 2 ) coincide on-shell , see e.g. the wikipedia page . ( here the $\approx$ symbol means equality modulo eom . ) iii ) now , let us imagine that we only know the nambu-goto action ( 1 ) and not the polyakov action ( 2 ) . the the only diffeomorphism-invariant combinations of the matrix $m^{a}{}_{b}$ are the determinant $\det ( m ) $ , the trace ${\rm tr} ( m ) $ , and functions thereof . if furthermore the ts metric $g_{\mu\nu}$ is dimensionful , and we demand that the action is linear in that dimension , this leads us to consider action terms of the form $$\tag{8} s~=~ -\frac{t_0}{c}\int d^2{\rm vol}~ \det ( m ) ^{\frac{p}{2}} \left ( \frac{{\rm tr} ( m ) }{2}\right ) ^{1-p} , $$ where $p\in \mathbb{r}$ is a real power . alternatively , weyl invariance leads us to consider the action ( 8 ) . obviously , the polyakov action ( 2 ) ( corresponding to $p=0$ ) is not far away if we would like simple integer powers in our action .
first order of business is to find where the heck on earth you are . first , $\omega = 360 \sin ( \phi ) /day$ , where $\omega$ is 216.528 degrees ; $\phi$ is the latitude of your position . north of the equator is positive , south negative . this gives you a band to follow around the earth horizontally , positions where could possibly be . you can further narrow your position down because a foucault pendulum can be used to find the acceleration of gravity at its position . once you figure this out , you can go to nasa websites and check out when this location has its next or last total solar eclipse .
you have not really asked a precise question , or given an example , but i think i know what you are getting at . you have misunderstood what it means for the two states to ' have different symmetry ' . suppose , as you say , that $g$ is some operator representing a symmetry of the system . this means that $g$ is unitary , and $ [ g , h_0 ] = [ g , v ] = 0$ ( we could also consider $ [ g , v ] \neq 0$ , but i do not think this is what you need ) . since $g$ is unitary and commutes with $h_0$ , the ground state of $h_0$ will also be an eigenstate of $g$ ( here we assume non-degeneracy of the ground state ) : $g|\phi_0\rangle = \lambda|\phi_0\rangle$ for some complex number $\lambda$ . the same argument applies for $h$ , so $g|\psi_0\rangle = \lambda'|\psi_0\rangle$ for some ( possibly different ) complex number $\lambda'$ . now consider $\langle\psi_0|g|\phi_0\rangle$ . we can let $g$ act either ' forwards ' on $|\phi_0\rangle$ , or ' backwards ' on $\langle\psi_0|$ ( exercise : show that $\langle\psi_0|g = \lambda'\langle\psi_0|$ as a consequence of unitarity of $g$ ) , to get $$ \lambda\langle\psi_0|\phi_0\rangle = \lambda'\langle\psi_0|\phi_0\rangle ~ , $$ and therefore $$ ( \lambda-\lambda' ) \langle\psi_0|\phi_0\rangle = 0 ~ . $$ so if $\lambda ' \neq \lambda$ , we find $\langle\psi_0|\phi_0\rangle = 0$ . example : a good example would be a particle moving in one dimension , with $h_0 = \frac{p^2}{2m} + \lambda x^4$ , and $v = -\mu^2 x^2$ , where $\lambda$ and $\mu$ are real constants . there is a symmetry $g : x\to -x$ ; the ground state of $h_0$ is even under this symmetry , whereas the ground state of $h = h_0 + v$ is odd . in symbols , $g|\phi_0\rangle = |\phi_0\rangle~ , ~~ g|\psi_0\rangle = -|\psi_0\rangle$ .
poisson brackets play more or less the same role in classical mechanics that commutators do in quantum mechanics . for example , hamilton 's equation in classical mechanics is analogous to the heisenberg equation in quantum mechanics : $$\begin{align}\frac{\mathrm{d}f}{\mathrm{d}t} and = \{f , h\} + \frac{\partial f}{\partial t} and \frac{\mathrm{d}\hat f}{\mathrm{d}t} and = -\frac{i}{\hbar} [ \hat f , \hat h ] + \frac{\partial \hat f}{\partial t}\end{align}$$ where $h$ is the hamiltonian and $f$ is either a function of the state variables $q$ and $p$ ( in the classical equation ) , or an operator acting on the quantum state $|\psi\rangle$ ( in the quantum equation ) . the hat indicates that it is an operator . also , when you are converting a classical theory to its quantum version , the way to do it is to reinterpret all the variables as operators , and then impose a commutation relation on the fundamental operators : $ [ \hat q , \hat p ] = c$ where $c$ is some constant . to determine the value of that constant , you can use the poisson bracket of the corresponding quantities in the classical theory as motivation , according to the formula $ [ \hat q , \hat p ] = i\hbar \{q , p\}$ . for example , in basic quantum mechanics , the commutator of position and momentum is $ [ \hat x , \hat p ] = i\hbar$ , because in classical mechanics , $\{x , p\} = 1$ . anticommutators are not directly related to poisson brackets , but they are a logical extension of commutators . after all , if you can fix the value of $\hat{a}\hat{b} - \hat{b}\hat{a}$ and get a sensible theory out of that , it is natural to wonder what sort of theory you had get if you fixed the value of $\hat{a}\hat{b} + \hat{b}\hat{a}$ instead . this plays a major role in quantum field theory , where fixing the commutator gives you a theory of bosons and fixing the anticommutator gives you a theory of fermions .
you know the basic spring equation , right ? $f = xk$ , where $k$ is the spring constant , in units of force per distance . you also know work ( energy ) is force times distance , right ? so all you have got to do is integrate $xk dx$ from d1 to d2 . ( hint , you can pull $k$ out of the integral . ) you could do it on graph paper if you happened to know d1 and d2 . added : ok , here 's the graph paper approach : a graph of force $f$ versus displacement $x$ looks like this , right ?  | / | / | / F | / | / | / | / |/_______ x  the slope of the graph is $k$ . the area under the graph is work $w$ , because it is just the sum of a bunch of vertical slivers with area $f$ times the width $dx$ of each sliver . so here 's how you get the answer to your question : that help ?
they are variants , different kinds of quantum field theory , but they are not mutually exclusive . the different adjectives you mention separate quantum field theory to " pieces " in different ways . the different sorts of variants you mention are being used and studied by different people , the classification has different purposes , the degree of usefulness and validity is different for the different adjectives , and so on . conformal quantum field theory is a special subset of quantum field theories that differ by dynamics ( the equations that govern the evolution in time ) , namely by the laws ' respect for the conformal symmetry ( essentially scaling : only the angles and/or length ratios , and not the absolute length of things , can be directly measured ) . conformal field theories have local degrees of freedom and the forces are always long-range forces , which never decrease at infinity faster than a power law . they are omnipresent in both classification of quantum field theories - almost every quantum field theory becomes scale-invariant at long distances - and in the structure of string theory - conformal field theories control the behavior of the world sheets of strings ( here , the cft is meant to contain two-dimensional gravity but the latter carries no local degrees of freedom so it does not locally affect the dynamics ) as well as boundary physics in the holographic ads/cft correspondence ( here , cfts on a boundary of an anti de sitter spacetime are physically equivalent to a gravitational qft/string theory defined in the bulk of the anti de sitter space ) . conformal field theories are the most important class among those you mentioned for the practicing physicists who ultimately want to talk about the empirical data but these theories are still very special ; generic field theories they study ( e . g . the standard model ) are not conformal . topological quantum field theory is one that contains no excitations that may propagate " in the bulk " of the spacetime so it is not appropriate to describe any waves we know in the real world . the characteristic quantity describing a spacetime configuration - the action - remains unchanged under any continuous changes of the fields and shapes . so only the qualitative , topological differences between the configurations matter . topological quantum field theory ( like chern-simons theory ) is studied by the very mathematically oriented people and it is useful to classify knots in knot theory and other " combinatorial " things . they are the main reason behind edward witten 's fields medal etc . axiomatic or algebraic ( and mostly also " constructive " ) quantum field theory is not a subset of different " dynamical equations " . instead , it is another approach to define any quantum field theory via axioms etc . that is why it is a passion of mathematicians or extremely mathematically formally oriented physicists and one must add that according to almost all practicing particle physicists , they are obsolete and failed ( related ) approaches which really can not describe those quantum field theories that have become important . in particular , aqfts of both types start with naive assumptions about the short-distance behavior of theories and are not really compatible with renormalization and all the lessons physics has taught us about these things . constructive qfts are mainly tools to understand the relativistic invariance of a quantum field theory by a specific method . then there are many special quantum field theories , like the extremely important class of gauge theories etc . they have some dynamics including gauge fields : that is a classification according to the content . qfts are often classified according to various symmetries ( or their absence ) which also constrain their dynamical laws : supersymmetric qfts , gravitational qfts based on general relativity , theories of supergravity which are qfts that combine general relativity and supersymmetry , chiral qfts which are left-right-asymmetric , relativistic qfts ( almost all qfts that are being talked about in particle physics ) , lattice gauge theory ( gauge theory where the spacetime is replaced by a discrete grid ) , and many others . gauge theories may also be divided according to the fate of the gauge field to confining gauge theories , spontaneously broken qfts , unbroken phases , and others . string field theory is a qft with infinitely many fields which is designed to be physically equivalent to perturbative string theory in the same spacetime but it only works smoothly for open strings and only in the research of tachyon condensation , it has led to results that were not quite obtained by other general methods of string theory . we also talk about effective quantum field theories which is an approach to interpret many ( almost all ) quantum field theories as an approximate theory to describe all phenomena at some distance scale ( and all longer ones ) ; one remains agnostic about the laws governing the short-distance physics . that is a different classification , one according to the interpretation . effective field theories do not have to be predictive or consistent up to arbitrarily high energies ; they may have a " cutoff energy " above which they break down . it does not make much sense to spend too much time by learning dictionary definitions ; one must actually learn some quantum field theory and then the relevance or irrelevance and meaning and mutual relationships between the " variants " become more clear . at any rate , it is not true that the classification into adjectives is as trivial as the list of colors , red , green , blue . the different adjectives look at the framework of quantum field theory from very different directions - symmetries that particular quantum field theories ( defined with particular equations ) respect ; number of local excitations ; ability to extend the theory to arbitrary length scales ; ways to define ( all of ) them using a rigorous mathematical framework , and others .
there are continuous spectra , emission spectra , and absorption spectra . dense materials , such as the deeper parts of our sun , have continuous spectra . hot , low-density gases have emission spectra ( a black spectrum with bright lines in it ) . the continuous spectrum generated deep in our sun passes through the cooler and lower-density outer regions of the sun , where it is partially absorbed . the result is an absorption spectrum : a rainbow with certain dark lines notched out of it : http://en.wikipedia.org/wiki/absorption_spectrum a good way to think about this is that a line spectrum comes from individual atoms , which are quantum-mechanical . in systems with a very large number of particles , the quantum-mechanical behavior becomes undetectable . a dense gas , such as the deeper parts of the sun , has all its atoms so close together that they act like one big object with a very large number of particles . therefore you get no quantum-mechanical features , and it is just a continuous spectrum .
well , there is the " easy java simulations " ( ejs ) . it is open source and very intuitive . you do not need to know java to use it . i do not know , though , if it is good for big systems , but since it is written in java it should handle it quite well . it can be downloaded from here and you can also check some examples in that website . well for more complex things , mathematica .
rocket fuels initially , followed by a series of gravitational assists ( slingshots ) : http://en.wikipedia.org/wiki/gravity_assist the linked article mentions voyager 1 mission as an example .
if you mix d$_2$o and h$_2$o you quickly get dho due to the grotthuss mechanism . i assume this is what you mean by heavy water being contaminated on exposure to humid air . obviously it would not be contaminated by exposure to dry air because there is no hydrogen present in dry air . the hydrogen atoms in polyethylene are not mobile and will not react with d$_2$o . however polyethylene is more porous than you might think and will eventually let water through .
the question defines $v$ as the rate at which the string is pulled downward through the ring and consequently the rate at the which the radius changes , $-dr/dt$ ( negative as the radius is decreasing with time - the fixed length of string is being pulled down ) . this $v$ is different from $\underline{\mathbf{v}}$ , the velocity vector of the mass , given by $\underline{\mathbf{v}}=\frac{d\underline{\mathbf{r}}}{dt}$ where $\underline{\mathbf{r}}$ is the position vector ( independent of the coordinate system you choose ) . this is the quantity you rightly define in your second statement , $r ( t ) =\frac{|\underline{\mathbf{v}}|}{\omega ( t ) }$ . overall , you need to be careful between what are scalar quantities ( $v$ , $r$ and $dr/dt$ ) and what are vector quantities ( $\underline{\mathbf{v}}$ and $\underline{\mathbf{r}}$ ) . sometimes the vectors will only be bold which can make it difficult to distinguish but the context is key , it is rare to find an error in a textbook as much as we may sometimes wish it so . the devil is in the detail !
the schwarzschild metric for $d$ dimensions is the standard form $$ ds^2~=~-e^{2\phi}dt^2~+~e^{2\gamma}dr^2~+~r^2d\omega^2 $$ these metric terms in the einstein field equation gives $$ r_{tt}~-~\frac{1}{2}rg_{tt}~=~g_{tt}~=~ -e^{2\phi}\big ( ( d~-~1 ) \frac{e^{2\gamma}}{r} ~+~\frac{ ( d~+~1 ) ( d~-~2 ) }{2r^2} ( 1~-~e^{2\gamma}\big ) $$ a multiplication by $g^{tt}$ removes the $-e^{2\phi}$ and we equate this with a pressureless fluid $t^{tt}~=~\kappa\rho$ . so we think of the black hole as composed of “dust . ” some analysis on this is used to compute the $g_r^r$ gives the curvature term $$ g_r^r~=~\big ( ( d~-~1 ) ( \phi_{ , r}~+~\gamma_{ , r} ) \frac{e^{-2\gamma}}{r}~+~\kappa\rho\big ) . $$ which tells us $\phi~=~-\gamma$ , commensurate with the standard schwarzschild result , and that $$ e^{-2\gamma}~=~e^{2\phi}~-~\big ( \frac{r_0}{r}\big ) ^{d~-~2} . $$ the entropy of the black hole is then computed by writing the density according to these metric elements and computing the rindler time coordinates $s~=~2\pi ( d~-~2 ) a/\kappa$ . the results more or less follow as with the standard $3~+~1$ spacetime result . the connection with strings is to work with the entropy of the black hole . the $1~+~1$ string world sheet has $d~-~1$ transverse degrees of freedom which contain the field data . the entropy $s~=~2\pi ( d~-~1 ) t$ may be computed with the string length , which reduces to the holographic results in $d~=~4$ spacetime . the event horizon is $d~-~2$ dimensional , which for $10$ dimension means the horizon is $8$ dimensional . the singularity is not considered in these calculations . the factors $e^{-2\gamma}~=~e^{2\phi}$ become extremely large . the metric approximates $$ ds^2~\simeq~\big ( \frac{r_0}{r}\big ) ^{d~-~2}dt^2~+~r^2d\omega^2 $$ which is a $d~-~1$ dimensional surface where the weyl curvature diverges for $r~\rightarrow~0$ . for $d~=~4$ this has properties similar to an anti-desitter space . the theory of black holes essentially follows in arbitrary dimensions . it is interesting to speculate on what the singularity is from a stringy perspective . the event horizon contains the quantum field information which composes the black hole . this may then have some type of correspondence with the interior singularity , with one dimension larger . for a black hole that is very small $\sim~10^3$ planck units , the horizon is a quantum fluctuating region , as is the singularity , and the qft data on the two may have some form of equivalency .
typically you would attempt to measure rather than calculate the effect . perhaps by having a second , calibrated device with a long probe that provides an independent measurement of the temperature . you do this in situ if possible or in some reasonable test stand ( which might be as simple as disposable cooler filled with you working fluid ) . the only real alternative is to read the data sheet ; either for the whole device ( if is a off-the-shelf instrument ) , or for the particular chip ( if it is something that you manufactured to spec ) . as a desperate fall-back position you might be able to find a rule-of-thumb for devices in the same class , but those are unlikely to be centrally tabulated . ask around is my best suggestion .
you are not missing anything . when an object moves through a uniform fluid under the influence of a constant force and drag , the closer it gets to terminal speed , the less the net force on it is and the less it will accelerate . so it only approaches terminal speed asymptotically , never actually reaching it . as you know , the equation works out to $$v = v_t\tanh\biggl ( \frac{t}{t_0}\biggr ) $$ and if you try to find the time at which $v = v_t$ ( terminal speed ) , you do indeed get $\tanh^{-1} ( 1 ) $ which is $\infty$ . but for any given precision $\delta v$ , you can find a time after which $\lvert v - v_t\rvert \leq \delta v$ , so given the precision of your measurements , you can tell when the falling object 's speed will be indistinguishable from terminal speed .
usually i find it easiest to evaluate commutators without resorting to an explicit ( position or momentum space ) representation where the operators are represented by differential operators on a function space . in order to evaluate commutators without these representations , we use the so-called canonical commutation relations ( ccrs ) $$ [ x_i , p_j ] = i\hbar \ , \delta_{ij} , \qquad [ x_i , x_j ] =0 , \qquad [ p_i , p_j ] =0 $$ now , in order to evaluate and angular momentum commutator , we do precisely as you suggested using the expression $$ l_z = x p_y - y p_x $$ and we use the ccrs \begin{align} [ x , l_z ] and = [ x , xp_y-yp_x ] \\ and = [ x , xp_y ] - [ x , yp_x ] \\ and = x [ x , p_y ] + [ x , x ] p_y-y [ x , p_x ] - [ x , y ] p_x \\ and = -i\hbar y \end{align} in the last step , only the third term was non-vanishing because of the ccrs . i have also used the fact that the commutator is linear in both of its arguments , $$ [ aa+bb , c ] = a [ a , c ] + b [ b , c ] , \qquad [ a , bb + cc ] = b [ a , b ] + c [ a , c ] $$ where $a , b , c$ are numbers and $a , b , c$ are operators , and the following commutator identity that you will find useful in general : $$ [ ab , c ] = a [ b , c ] + [ a , c ] b $$
the link bjorn posted contains most of the information , currenty the afs ( active filling scheme ) reads : 50ns_480b+1small_424_12_468_36bpi15inj 50ns : bunch spacing 480b : bunches per beam 424: colliding bunches in the interaction points 1 ( atlas ) and 5 ( cms ) 12: colliding bunches in the ip 2 ( alice ) 468: colliding bunches in ip8 ( lhcb ) 36bpi15inj : 36 bunches per injection and 15 injections per beam
the crucial fact about these idealized circuits and electric potential differences that leads to the assertion you want to justify is wires are modeled as perfect conductors ( ohmic resistors with negligible resistance ) for which there is zero potential difference between any two points . ( this was edited from " perfect conductors are equipotentials . " ) if we assume that the wires are perfect conductors ( zero resistance ) , then ohm 's law immediately gives the result above . having this fact in hand , we first notice that we have the following mathematical identity ( which is basically the " loop rule" ) : $$ v_a + ( v_d-v_a ) + ( v_c - v_d ) + ( v_b-v_c ) + ( v_a - v_b ) = v_a $$ which we can rewrite as $$ \delta v_{da} + \delta v_{cd} + \delta v_{bc} + \delta v_{ab} = 0 $$ now using the fact above , we note that the potentials at any two points connected solely by a wire are the same , so that $$ \delta v_{da} = 0 , \qquad \delta v_{bc} = 0 $$ which gives $$ \delta v_{cd} + \delta v_{ab} = 0 $$ and therefore since $$ \delta v_{ab} =- \delta v_{ba} $$ we get the desired result $$ \delta v_{cd} = \delta v_{ba} $$
for a pole of order $n+1$ , $$\mathrm{res} [ f , w ] =\frac{1}{n ! }\lim_{z\to w}\frac{d^n}{dz^n} ( z-w ) ^{n+1}f ( z ) $$ so for a function of the type $$f ( z ) =\frac{g ( z ) }{ ( z-w ) ^{n+1}}$$ $$\mathrm{res} [ f , w ] =\frac{1}{n ! }g^{ ( n ) } ( w ) $$ where $g$ is regular . now the important part is to notice that $o$ depends on $w$ and it is not the result of evaluating $z$ at $w$ . also because $\epsilon z$ is regular , we expect that $\epsilon z t ( z ) o ( w ) $ is of the form $\epsilon zh ( z , w ) $ , where $h ( z , w ) $ has the singularities . with this it becomes clear that the term $\epsilon h o ( w ) $ cannot arise from a simple pole as otherwise we will not get the factor $\epsilon z$ . but for a pole of order 2 , $$g^{ ( 1 ) } ( w ) =\epsilon h o ( w ) \rightarrow g ( z ) =\epsilon z h o ( w ) $$ on the other hand , the term $\epsilon w\partial o ( w ) $ can arise from a single pole : $$g ( w ) =\epsilon w \partial o ( w ) \rightarrow g ( z ) =\epsilon z \partial o ( w ) $$ therefore $$\epsilon z t ( z ) o ( w ) = . . . +\frac{\epsilon z h o ( w ) }{ ( z-w ) ^2}+\frac{\epsilon z \partial o ( w ) }{ ( z-w ) }+ . . . $$ thus we get the result $$ t ( z ) o ( w ) = . . . +\frac{ h o ( w ) }{ ( z-w ) ^2}+\frac{ \partial o ( w ) }{ ( z-w ) }+ . . . $$
the term sector in quantum field theory typically refers to a portion of the lagrangian . for example , in the standard model lagrangian , we encounter a term , $$\mathcal{l}_{sm}= ( d_\mu \phi ) ^{\dagger} ( d^\mu\phi ) -\frac{m^2_h}{2v^2}\left ( |\phi|^2-v^2/2\right ) +\dots$$ plus others involving couplings with the higgs doublet $\phi$ , and we can refer to the subset as the higgs sector . the hidden sector features all the interactions and terms in the lagrangian which we expect exist , but have not explicitly written in the full standard model lagrangian .
yes it is . the total momentum vector of a system does not change at all ( constant length and direction ) , so the projection of it on a line ( or any function you apply to it ) will not change . projection is a linear operator , so that if you project each particle 's momentum on a line and then sum , you get the same result as summing first ( to get the total momentum ) then projecting .
the special conformal transformations as well as the translations , dilations and rotations are all continuously connected to the identity . this means that they contain parameters such that at some particular value the trasformation becomes trivial . for example , for $b=0$ the special conformal trasformation you write is simply $x_i\mapsto x_i$ . the inversion map $$x_i \mapsto \frac{x_i}{x^2} , $$ on the other hand , is not connected to the identity . we can also note that an inversion changes the orientation of the space , while the other conformal transformations preserve the orientation . in $d$ ( euclidean ) dimensions the special conformal transformations , translations , dilations and rotations together form the lie group $so ( d+1,1 ) $ . this is what , at least in physics , is normally referred to as " the conformal group " . if we also allow for inversions this group is extended to $o ( d+1,1 ) $ .
yes . from clausius theorem the following inequality can be deduced : $$\delta q \le tds$$ where the equality holds in the reversible case . so , a reversible adiabatic process is necessarily isentropic , but irreversible adiabatic processes are not so . to put it in another way , in an irreversible process , according to the above inequality , either entropy changes , or heat must be somehow removed from the system to make it possible to have zero change in entropy . so an irreversible isentropic process can not be adiabatic .
if you build the square of the wave function , the result is a gaussian curve . if you compare your result with the general form of a normal distribution you can see that x0 is the expectation value . . . http://en.wikipedia.org/wiki/normal_distribution
i would likely to further dmckee 's answer by answering the op 's follow-up question : can you please explain why considering secondary sources as in huygen 's principle is justified , i.e. , why do we get correct results by assuming secondary sources when there are actually no sources other than the original source ? as feynman explains in case of electromagnetic waves , it is because the diffracted wave is equivalent to the superposition of electric fields of a hypothetical plug containing several independent sources . huygen 's principle is actually a fairly fundamental property of solutions of the helmholtz equation $ ( \nabla^2 + k^2 ) \psi = 0$ or the d'alembert wave equation $ ( c^2\nabla^2 - \partial_t^2 ) \psi = 0$ . for these equations the green 's function is a spherical wave diverging from the source . all " physically reasonable " solutions ( given reasonable physical assumptions such as the sommerfeld radiation condition ) in freespace regions away from the sources can be built up by linear superposition from a system of these sources outside the region under consideration . already this is sounding like huygen 's principle , but one can go further and , with this prototypical solution and the linear superposition principle together with gauss 's divergence theorem , show that waves can be approximately thought of as arising from a distributed set of these " building block " spherical sources spread over the wavefront : this result leads to the kirchoff diffraction integral , thence to various statements of huygens 's principle . this treatment is worked through in detail §8.3 and §8.4 of born and wolf , " principles of optics " or in hecht , " optics " , which i do not have before me at the moment .
the " wave " part of the wave-particle duality for particles such as electrons and protons ( as opposed to em radiation ) is called their wavefunction . it does not have any classical analogue and any attempt at understanding it using classical intuition can only be a crude analogy . however , if you are happy with the concept of a probability wave then it is exactly that . why is not this a problem with the uncertainty principle , then ? well , there is a corresponding uncertainty principle between the wavelength of any wave ( or more precisely its wavenumber $k=2\pi/\lambda$ ) and its position in space . the wavelength of a wave is only precisely definable , to arbitrary precision , if you have an infinite wave ; otherwise , you can only measure a finite number of periods and divide , and that will yield an imprecise measurement of $\lambda$ . ( even worse , the amplitude will taper out near the edges , so it will be hard to tell where each peak or trough is . ) to have a better-defined wavelength , then you need a bigger wavepacket , but this means that the position of the wavepacket in space , which only makes sense to a precision of the wavepacket size , has bigger uncertainty . this trade-off game can be expressed as $$\delta k\delta x \gtrsim1 , $$ and can be made precise using fourier analysis of waves .
it is because you would not hide in the corners like your kitty does ! a electric radiator is designed to be directional and therefore it does not heat the unnecessary part of your room . it makes you feel warming in front of it , but some part of the room do not get heated like those corner and the ceiling . in comparison , a vacuum cleaner heating the gas instead , so it is much more uniform , but less efficient from your point of view because you do not feel it ( your kitty might be happy about it though ) . as what @johnrennie , they dissipate the same amount of heat ( and some become noise ) .
short answer , no . long answer , sort of . short answer : no , the e and m fields may be coupled by the lorentz transformations , but it is only when they work together to make a self-propagating wave that we can call it a particle . to separate them as individual fields is physically meaningless . so giving each field their own particle is equally meaningless . long answer : this can be thought of as an " is the moon really there when we are not looking at it ? " problem . the only way we can observe that an e or m field is present is when it interacts with something via the lorentz force . so if the field is not interacting with anything , is it really there ? in advanced physics , that answer turns out to be a resounding " no " . what we can say is that the e or m field interacts with objects via a photon . that is , the magnetic or electric field can be said to exists , but we can also say that the source of this field is interacting with our sensors or other particles by exchanging photons to produce a force . thus , to answer your question , we can in a way " quantize " the separate fields and visualize them as a particle , but only if we visualize them not as fields but the exchange of photons between the sources of the field and the objects influenced by it . but full photons , not half a photon .
this is simply the sum of the gravitational potential energy over all the points that make up the body . each point has a mass $\rho dv$ , meaning the mass density times the infinitesimal volume element , and this is multiplied by g and h , because the potential energy of a point at height h is $mgh$ . if you are asking why the potential energy is $mgh$ for a point , this can be argued using reversible elevators attached by pullies . if you want to raise a mass m by a certain amount h , you can do this by putting an equal mass on the other side of a pulley-elevator and lowering the mass by an equal amount . this process is easy--- you do not have to do work--- because the masses balance on the two sides . so if there is a conserved energy , it must be a quantity which is unchanged when you lower a mass by a given amount , so long as you raise the same amount of mass somewhere else by the same amount . since you can cut up big masses into small pieces , you can raise a big mass of mass 2m by h units of height by lowering two masses of size m each by h units . in all these processes , the sum of the mass times the height is conserved . when you have different gravitational forces , like deep in the interior of the earth , you can compensate by using a bigger mass . so it is mg which is the right unit to balance , the force , not the mass itself . this argument establishes that the potential energy is proportional to mgh , and that there is no numerical constant , that the potential energy is just equal to mgh , is just a best convention for defining the unit of energy from the unit of force . this argument is presented in detail in feynman 's lectures on physics vol 1 , in an early chapter . it is essentially due to archimedes , and it is also discussed in this answer to a related question : why does kinetic energy increase quadratically , not linearly , with velocity ?
assuming that by escape velocity you mean exhaust velocity , then the velocity comes from the maxwell-boltzmann distribution . this gives the velocity distribution of the particles in a gas as a function of temperature . for our purposes we can use the most probably speed , i.e. the peak in the distribution , as a rough estimate and this is given by : $$ v_e = \sqrt{\frac{2kt}{m}} $$ where $t$ is the temperature to which the reactor heats the propellant . so as you increase the molecular mass of the propellant the exhaust velocity falls as $1/\sqrt{m}$ . the problem is that if you carry a mass $m$ of propellant the total possible momentum change is $mv_e$ , so for propellant with a greater molecular weight you need to carry a greater mass of the propellant , $m$ , but then the extra weight of propellant adds to the weight of your rocket .
given some distribution or density $\rho ( x ) , $ a moment is the ' expectation value ' of some power of $x \in \mathbb{r}$ . to be precise , the $n$-th moment $m_n$ is given by $$m_n = \int_{\mathbb{r}} x^n \rho ( x ) \mathrm{d}x . $$ in the mechanics case , $\rho ( x ) $ is simply the mass density . you can extend this to vectors in $\mathbb{r}^d$ in a straightforward way ; for example , for the moment of inertia you replace $x^2$ by $\mathbf{x}^2 = x_1^2 + \ldots x_d^2$ to obtain $$i = m_2 = \int_{\mathbb{r}^d} \mathbf{x}^2 \rho ( \mathbf{x} ) \mathrm{d}^dx$$ which should match the definition given in your mechanics textbook . for the first moment of mass , you need to distinguish different directions . as you indicate , you can choose your coordinates such that $$\int_{\mathbb{r}^d} x_i \ , \rho ( \mathbf{x} ) \mathrm{d}^d x = 0$$ where $i$ runs over the coordinates . in three dimensions , you have $x_1 = x , x_2=y$ and $x_3=z . $
if i remember correctly they only do this in the turns and they use both arms in the straights . it is the outer arm that is active . this helps them turn in two ways . it helps accelerate the outer side more than the inner which is what is what turning really is . the reaction force at the shoulder also helps them lean into the turn which helps them stay stable through the acceleration of turning . this the same reason why bicycles and motorcycles lean into turns and why race tracks for some sports are banked . the mechanics of swinging arms in skating are similar to those in running and even walking . the main benefit is during the back/down part of the stroke . since the arm is moving back , down and also out , there is an opposing forward , up and inward reaction force on the torso at the shoulder . the forward component helps with acceleration , upwards helps extend the stride and the inward helps with balance . doing this only on one side helps turn to the opposite side as described above . there are ( at least ) two reasons why the return part of the arm stroke does not completely undo the benefit of the first part . one reason has to with timing . the active part of the stroke occurs while the foot on that side of the body is off the ground and the other foot is planted so it is helping propel the side of the body that is up moving and there is a longer lever from the shoulder to the fulcrum at the planted foot on the far side . during the return stroke that side of the body is on the ground so it does not slide back due to returning forces . also the leverage from the shoulder to the planted foot is shorter . when the opposite foot is off the ground the return stroke actually helps move the opposite site of the body forward . a simple experiment to demonstrate these effects is to swing just one energetically arm back and forth while standing on one foot and then on the other . observe the different ways your body responds to swinging the same arm in while standing each foot . the other possible reason the return strong does not undo the benefit of the initial stroke is that the arm can be in a more relaxed position during the return stroke so it has smaller moment of inertia and thus the reverse effect is lesser . also return stroke can be less aggressive . this is similar to the way falling cats turn themselves in mid air without violating conservation of angular momentum . i am not certain how big a role this mechanism plays in skating or running though . edit my explanation is only valid in cases where the skaters move as i described . according to @pulsar below , this may not be universally true . however , arms do go out of phase with legs and both arm are used at least some of the time
one of the biggest failure of theoretical condensed matter and/or material sciences is that up to now , nobody has ever been able to predict what compounds will be a good superconductors . of course , since we do not really understand high-tc superconductivity , we cannot predict which ceramic will or will not be a nice superconductor . but even in the case of more standard superconductors described by bcs or more refined theories ( like eliashberg 's theory ) , the predictive power of theoretical approaches is close to zero . to summarize , all superconductors are found experimentally , and then theorists try to explain why this particular alloy/compound has these properties .
i do not think there is a good general explanation of this ; the best i can do is give a few hand waving arguments . if you look at hydrogen sulphide , which the analogue of water moving one row down in the periodic table , you will find it does shrink when it freezes , just like most other liquids . so the difference between the h$_2$s and h$_2$o molecules must be responsible for the anomalous behaviour of water . the most obvious difference between the two molecules is the the h-o bond is highly polar and has a strong electric dipole associated with it . this means it interacts strongly with other h-o bonds ; the interaction is known as hydrogen bonding . hydrogen bonds are highly directional , as you had expect for an electric dipole , so the water molecules can be fitted together in ice any old how . they adopt well defined positions relative to each other , and the directionality of the bonds forces the water molecules into a relatively low density arrangement . it is interesting to note that the high pressure forms of ice are generally denser than water . presumably at high pressure the reduction in energy by the denser arrangement outweighs the reduction in the strength of the hydrogen bonds .
any object , whether it be magnetized or not ( and in particular whether it be ferromagnetic or para/diamagnetic ) , will only experience magnetic forces when it is placed in an external magnetic field . if you are thinking of a long , straight conductor , then the magnetic field it produces will increase as you get near it ( though it does decrease inside the conductor ) . that said , there are some configurations of currents that produce a magnetic field " outside " and but none " outside " . two examples are a toroidal solenoid , in which the field is confined to the solenoid interior and is zero in the donut hole , and two long cylindrical , coaxial solenoids carrying the same current in opposite directions , for which the field is confined to the middle region and cancels out in the inner region . a coaxial cable is an example of the latter . if you are thinking about an extended distribution of currents such as a wire of nonzero thickness , then yes , the field is zero at the centre and it is small nearby . however , you can hardly place a metallic object inside a wire .
have a look at http://en.wikipedia.org/wiki/sunlight as this gives the irradience of sunlight as a function of wavelength . the spectrum is very similar to a black body temperature of 5,600k . i was not sure from your question if you were asking about the units used for irradience . the units used in the wikipedia article ( and in the astm page it references ) are power per square metre per nanometre of bandwidth . for example a value of 1.5 at 400nm means that the total power of all the light between 400nm and 401nm is 1.5w/m$^2$ . to model photosynthesis you need to multiply the spectrum by the absorption spectrum of chlorophyll then integrate across all wavelengths to get the total power per square metre .
probably not , but it depends on the geometry of your coil . for a couple of dollars at the hardware store you can get a big stack of those coin magnets . if the answer to your question were yes in general , it would be harder to break apart the big stack of magnets that to separate two of them . that is not consistent with my experience . in general for a dipole $\vec m$ in a field $\vec b$ , the force is $\vec f=-\nabla ( \vec m \cdot \vec b ) $ . if the dipole moment is a constant , and the dipoles are free to rotate , they will orient themselves so that $\vec m$ is antiparallel to $\vec b$ . in that special case , the force simplifies to $$\vec f \approx m \vec\nabla b . $$ in other words , the dipoles " want " to align antiparallel to the field , then to move up the gradient into the strongest part of the field . a dipole in a uniform field feels a torque , but not a net force . if your coil is set up to generate a uniform field , your two stacked magnets will feel the same force — but that force will be zero . if your coil is generating a magnetic field with some dipole component , your stacked magnets will rotate to align with the field , then ( since we have already assumed there is a field strength gradient ) one of them will see a weaker gradient than the other . the gradient $\vec\nabla b$ typically vary like $1/r^4$ , where $r$ is the distance from the center of the coil , so the force on the two coin magnets can vary considerably over a short distance . this is why strong magnets like to suddenly " leap " together and pinch your fingers when you are playing with them . now you could put your two coin magnets next to each other , symmetrically about the axis of your coil , and they would see the same $|\vec\nabla b|$ in slightly different directions . but with the two coin magnets next to each other , they would exert torques on each other , since they prefer to be stacked pole-to-pole ; you could engineer a solution like this , but there would be extra parts involved . i actually happened to have some magnets with this shape on my desk . in response to carl 's comment , i built a little string lifting harness to measure the force on a single magnetized paper clip : adding a second magnet increased , but did not double , the weight that the harness could hold : stacked number of clips magnets that stayed up 1 11 2 17 3 23 4 24 5 24  the dominant effect in calculating the force is the field is right at the surface of the stack of magnets ; it looks like that gets saturated , an effect i did not consider in the first part of my answer .
the technical name for the structure in a battery is known as the " electrochemical cell " . it consists of an electrolyte ( or more of them ) , a liquid with ions that carry the electric charge . the asymmetry in the battery , the main reason why it works , is that these two types of carriers like to participate in chemical reactions in two different containers , near the two electrodes . positively charged ions – imagine $na^+$ from salt , although it is not the most realistic example – like to to get " attached " to one electrode while the negatively charged ions – imagine $cl^-$ ions from salt – like to get " attached " to the other one . these two types of reactions are called reduction and oxidation , respectively , according to the sign of the charge that the ion is gaining or losing . i do not want to get lost in sign errors so i have not assigned them to the first sentence of this paragraph . as these chemical reactions are running , they are producing a charge asymmetry , and therefore a discrepancy between the potentials of the two electrodes , and this asymmetry is compensated by the flow of electrons through the wires of the circuits ( outside the battery ) . in the long run , one is converting chemical energy ( sort of an electrostatic potential energy of ions – they are " higher " , using a gravitational analogy , before they react with the electrodes ) to a hopefully useful energy done by the circuit . the total energy is conserved . the energy deposited to a charged particle moving across voltage $v$ is $e=vq$ . emf , the electromotive force , is a special type of voltage , so its si unit is 1 volt , just like for any other voltage . i say it is a special kind of voltage because it is used for the " energy generating " parts of the circuits only , namely for batteries or parts that produce electricity by electromagnetic induction ( following faraday 's law ) like coils in a variable magnetic field . there is also voltage on resistors and capacitors but it is " passive " , or " consuming " the power created elsewhere , so we do not count it as emf . emf is meant to be the " ultimate source of life " in electrical circuits . concerning the derivation of the relation between current and voltage , i suppose you mean ohm 's law $v=ir$ . its microscopic form is $\vec j = \sigma \vec e$ , i.e. the current is proportional to the electric field where the coefficient $\sigma$ is known as conductivity . this formula holds for metals rather well because the electric field accelerates electrons up to an average speed dictated by the trade-off between the electric field acting on the electrons ; and the decelerating speed from the collisions . this trade-off leads to a velocity that is proportional to $\vec e$ as well , and $\vec j$ is the product of the electron density and the average velocity of these carriers .
assuming i have understood what you are asking ( ignore this if i have not ! ) the point is that the radius of the hole is equal to the minimum radius of the air-water interface and therefore its maximum pressure . if you place the sphere just under the surface of the water so the pressure difference is almost zero then the air-water interface forms a portion of a very large sphere with the centre positioned some distance away from the sphere . as you move the sphere farther down in the water the air water interface indents more and the air-water surface radius goes down , with its centre moving closer to the surface of the sphere . when the radius of the bubble equals the radius of the hole , the centre of the air-water interface is in the centre of the hole . increasing the pressure further causes the radius of the air-water interface to grow again , which reduces the pressure , so the water floods in . so the last paragraph of your question is quite correct . there is a bubble formed at all depths .
let 's start out using notation similar to the example you linked to : $$ \ddot{y}+b\dot{y}+\sin ( y ) =a\cos ( ct ) +d $$ as in the example , we will write this in autonomous form : $$ \mathbf{x}=\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix}=\begin{pmatrix}y\\\dot{y}\\ct\end{pmatrix} $$ the differential equation can now be written : $$ \mathbf{\dot{x}}=\begin{pmatrix}\dot{x_1}\\\dot{x_2}\\\dot{x_3}\end{pmatrix}=\mathbf{f ( x ) }=\begin{pmatrix}x_2\\-bx_2-\sin{x_1}+a\cos{x_3}+d\\c\end{pmatrix} $$ now , suppose we study the evolution of two different starting points in this system that are arbitraily close . let 's denote the trajectory of one of them with $\mathbf{x} ( t ) $ and the other one with $\mathbf{x} ( t ) +\mathbf{z} ( t ) $ , where $\mathbf{z} ( t ) $ is the vector difference between the two points . the time derivative of $\mathbf{z} ( t ) $ can be written : $$ \mathbf{\dot{z}}=\mathbf{f ( x+z ) }-\mathbf{f ( x ) } $$ if $\mathbf{z}$ is sufficiently small this can be written : $$ \mathbf{\dot{z}}=\mathbf{jz} $$ where $\mathbf{j ( x ) }$ is the jacobian of $\mathbf{f ( x ) }$: $$ \mathbf{j}=\begin{pmatrix} 0 and 1 and 0\\ -\cos{x_1} and -b and -a\sin{x_3}\\ 0 and 0 and 0 \end{pmatrix} $$ over a short period $\delta t$ , the change in $\mathbf{z}$ is given by : $$ \mathbf{z} ( t+\delta t ) =\mathbf{z} ( t ) +\delta t\mathbf{jz} ( t ) = ( \mathbf{i} +\delta t\mathbf{j} ) \mathbf{z} ( t ) $$ assuming $\mathbf{z}$ and $\delta t$ are small enough for the linearizations to hold we can decribe the change in $\mathbf{z}$ from time $t_1$ to $t_2$ with : $$ \mathbf{z} ( t_2 ) =\left [ \prod_i ( \mathbf{i} +\delta t_i\mathbf{j} ( \mathbf{x}_i ) ) \right ] \mathbf{z} ( t_1 ) $$ where $\delta t_i$ are sufficiently small time intervals from $t_1$ to $t_2$ and $\mathbf{x}_i$ is $\mathbf{x}$ at those times . what you need to do to calculate the lyaponov exponents of the this system is to numerically solve the trajectory $\mathbf{x} ( t ) $ for an arbitrary starting point $\mathbf{x}_0$ over a long time interval $t$ . then you need to calculate the matrix $\mathbf{a}$: $$ \mathbf{a}=\prod_i ( \mathbf{i} +\delta t_i\mathbf{j} ( \mathbf{x}_i ) ) $$ using the values for $\delta t_i$ and $\mathbf{x}_i$ from the solved trajectory . you then need to calculate the eigenvalues of $\mathbf{a}$ . let 's call them $\alpha_1$ , $\alpha_2$ and $\alpha_3$ . the lyaponov exponents are given by : $$ \lambda_i=\frac{1}{t}\log{\alpha_i} $$ assuming $t$ is long enough . you need to experiment with different $\mathbf{x}_0$ and $t$ . hopefully , the variations in the lyaponov exponents you calculate for different $\mathbf{x}_0$ will decrease with larger $t$ and converge for sufficiently large $t$ . one problem you will almost certainly run into is that the values of $\mathbf{a}$ will become too big to handle numerically . the way to get around this is to keep track of the magnitude of $\mathbf{a}$ as it is iteratively calculated and normalize it whenever its highest value reaches some threshold . the normalizing factors needs to be saved each time this is done . assuming this has been done you will in the end have a matrix $\mathbf{b}$ and a series of normalizing factors $n_j$ ( in my notation $n_j$ are the values you have divided by when normalizing ) relating to $\mathbf{a}$ as : $$ \mathbf{a}=\left ( \prod_j n_j\right ) \mathbf{b} $$ if $\beta_i$ are the eigenvalues of $\mathbf{b}$ , the lyaponov exponents can be calculated as : $$ \lambda_i=\frac{1}{t}\left ( \log{\beta_i}+\sum_j\log{n_j}\right ) $$ it is usually the largest of the lyaponov exponents that is of interest as it will dominate the exponential growth in $\mathbf{z}$ . at least one of the lyaponov exponents must be positive for the system to be chaotic . it is important to understand that $\mathbf{z}$ should be thought of as a vector that remains very small throughout the whole period $t$ . this might be a bit counter-intuitive since it grows exponentially . however , for any finite $t$ , the starting value of $\mathbf{z}$ can always be chosen small enough to make sure it reamins sufficiently small thoughout the time period .
as can be seen in this pie chart taken from the wikipeia article on " universe " , the significant parts of the universe are , in descending order , dark energy , dark matter , gas , stars , and the ghostly subatomic particles called neutrinos . that is the most laymanish terms that can be used , because nobody knows what the first two actually are .
looks like i have to answer this question :- ) let me first answer the math question : every zero-energy eigenstate is of the form of a symmetric polynomial times the laughlin wave function . to be concrete , let us consider an $n$ boson system , with delta-potential interaction $v=g\sum \delta ( z_i-z_j ) $ where $z_i$ is a complex number describing the position of the $i^{th}$ boson . the zero energy state $\psi ( z_1 , . . . , z_n ) $ satisfies $\psi ( z_1 , . . . , z_n ) =p ( z_1 , . . . , z_n ) exp ( -\sum_i |z_i|^2/4 ) $ where $p$ is a symmetric polynomial that satisfy $\int \prod_i d^2 z_i \ \psi ( z_1 , . . . , z_n ) ^\dagger v \psi ( z_1 , . . . , z_n ) =0$ . now it is clear that all the zero energy state are given by symmetric polynomial that satisfy $p ( z_1 , . . . , z_n ) =0$ if any pair of bosons coincide $z_i=z_j$ . for symmetric polynomial this implies that $p ( z_1 , . . . , z_n ) \sim ( z_i-z_j ) ^2$ when $z_i$ is near $z_j$ . the laughline wave function $p_0=\prod_{i&lt ; j} ( z_i-z_j ) ^2$ is one of the symmetric polynomials that satisfies the above condition and is a zero energy state . since any other zero-energy symmetric polynomial must satisfy $p ( z_1 , . . . , z_n ) \sim ( z_i-z_j ) ^2$ , $p/p_0=p_{sym}$ has no poles and is a well defined symmetric polynomial . so every zero-energy eigenstate $p$ is of the form of a symmetric polynomial $p_{sym}$ times the laughlin wave function $p_0$ . more discussions can be found in the first part of arxiv:1203.3268 . however , a physically more relevant math question is : every energy eigenstate below a certain finite energy gap $\delta$ is of the form of a symmetric polynomial times the laughlin wave function for any number $n$ of particles . ( here $\delta$ does not depend on $n$ . ) we only have numerical evidences that the above statement is true , but no proof .
db is basically a ratio measurement in logarithmic form . the sound intensity level $l_1$ is found by applying the following formula to two intensities such as $i_1$ and $i_0 $ . $$ l_\mathrm{i}=10\ , \log_{10}\left ( \frac{i_1}{i_0}\right ) \ \mathrm{db} \ , $$ i.e. , $i_1$ is $l_1~db$ higher than $i_0$ . in your question , $l_1 = 2.00 . $ we also know that intensity is proportional to the inverse of distance squared : $$\frac{i_1}{i_0}= ( \frac{r_1}{r_0} ) ^{-2}$$ where $r_1$ is the nearer distance ( thus higher intensity ) . we are also given that $r_1 = r_0+1$ with this you should be able to solve for everything .
you are on the right track ; there are just a few things i would add . the first is that you have made a slight mistake in your calculation . remember that a joule is 1kg m^2 / s^2 . 100g of h is 0.1 kg ; and since you have h and anti-h there are 0.2kg total . the total mass corresponds to an amount of energy $$ e = ( 0.2 kg ) \times ( 3 \times 10^8 m/s ) ^2 = 1.8 \times 10^{16} j $$ now about $e = mc^2$ itself . before einstein , we knew that objects could have energy due to motion ( kinetic energy ) , due to interaction with external fields ( gravitational and electrical potential energy ) , due to energy stored in internal forces ( e . g . harmonic potential energy in a spring ) , due to the kinetic energy of its microscopic constituents ( i.e. . internal energy , proportional to temperature , etc . ) . einstein added something to this list - any object with mass has a rest energy proportional to its mass given by $m c^2$ . in any process where mass is conserved ( as of the year ~ 1900 , all known physical processes ) , this energy is locked away and undetectable . einstein 's work showed that it must exist , and equivalently that processes violating conservation of mass could be possible as long as energy was still conserved . another important note : before einstein , the energy of some object ( ignoring all the potential energies and internal energy ) is $$ e = \frac{1}{2} mv^2 $$ after einstein , you do not just add $m c^2$ , i.e. , $$ e = mc^2 + \frac{1}{2} mv^2 $$ but instead $$ e = \frac{1}{\sqrt{1 - v^2/c^2}} m c^2 $$ at first this formula looks extremely different from $mc^2 + 1/2 mv^2$ , but it is equivalent at small velocities . for a small number $\epsilon$ , $$ \frac{1}{\sqrt{1 - \epsilon}} \approx 1 + \frac{\epsilon}{2} $$ which you can check with a calculator . in fact , there is a much better expression for the energy than $e = \frac{1}{\sqrt{1 - v^2/c^2}}m c^2$ . this is $$ e = \sqrt{m^2 c^4 + p^2 c^2} $$ here $p$ is the momentum of the object . for a particle with mass $m$ , this is $$ p = \frac{mv}{\sqrt{1-v^2/c^2}} $$ the reason this expression works better is that there are many particles , in particular the photons that make up light , which are massless but still carry momentum . a photon with wavelength $\lambda$ has momentum $2 \pi \hbar / \lambda$ . using the above , its energy is $$ e = \sqrt{m^2 c^4 + p^2 c^2} = p c = 2 \pi \hbar \nu = \hbar \omega $$
the answer you have been given is wrong . the energy after inserting the dielectric is $ku_0$
note that your formula $$e=\int p ( \omega , t ) dt=\int u ( \omega , t ) i ( \omega , t ) dt$$ can be rewritten as $$e=\int u ( \omega , t ) i ( \omega , t ) dt=\int\frac{u^2 ( w , t ) }{z}dt$$ now , let $u=u_0\sin ( wt ) $ and $z=const$ which is reasonable during a short period of time $t$ . thus : $$e=\frac{1}{z}\int_0^{t} u^2 ( w , t ) dt=\frac{u_0^2}{z}\int_0^{t} \sin^2 ( w , t ) dt$$ next : $$\int_0^{t} \sin^2 ( w , t ) dt=\frac{t}{2}-\frac{\sin ( 2wt ) }{4w}$$ so , if $w\rightarrow\infty$ , then $e$ does not depend of $w$ . edit : additions : to be more specific , the circuit is impedance $z$ depends on the frequency as $$z=\sqrt{r ( w ) ^2+\left ( wl-\frac{1}{wc}\right ) ^2}$$ where $l$ is the circuit is impedance and $c$ is the circuit is capacitance and $r$ is the resistance which depends also on $w$ due to the skin effect . that means $z \rightarrow wl$ as $w\rightarrow\infty$ if $l\neq 0$ so an answer , closer to reality is that $$e=\frac{u_0^2}{wl}\frac{t}{2} ; w\rightarrow\infty$$
no more than two objects in co-orbital configuration have been observed so far . it would be easy if there existed atleast one satellite ( instead of a planet ) to one of the planets orbiting around the star . the reason i say this is because , the orbital configuration of the planets can then be thought of as lagrangian points . though the l1 , l2 and l3 are much less stable compared to l4 and l5 , we can infer now that atleast 4 bodies can orbit the star . by this way , we can declare that they share their orbit . still , the configurations are less likely . a slight perturbation can modify their orbit or sometimes ( if unlucky ) , slam one onto the other . . . but it is ok to say that five planets can orbit at the same time . as dmckee and john said , klemperer rosette might be a good start . be sure that you still maintain the symmetry ( very necessary in physics ) ( i.e. ) a triangle in case of 3 objects , a pentagonal configuration in case of 5 , etc . . . iirc , this system is very very rare . as i have told already - during a planetary formation , it is still very unlikely that such a system ( with more than three co-orbital config ) can be formed .
( 1 ) as for the ( a ) the total force of the ground/hinge ( e . g . thrust or normal force + friction ) is generally neither vertical nor horizontal . edit : you can obtain the force of the ground/hinge by calculating the force of the rope first , and then add all three forces together to get zero . as for the ( b ) you have three forces acting to beam , force of the ground , gravitational force and force of the rope . since problem suggests " considering equilibrium " , torques of these three forces must equal zero . ( 2 ) force at point b is simply the force of rod bd to rod ac ( and vice versa ) . effectively , you have three forces acting on rod ac . note also that the force of the rod bd is along its direction ( because it is limited by two joints at its ends and there is no force in between ) .
the first formula ( scaling argument ) $$ \frac{u^2}{l} \sim \nu \frac{u}{\delta^2} , \tag{*} $$ comes directly from equations for boundary layer equations ( and not specifically for blasius boundary layer ) . we have continuity equation : $$\dfrac{\partial u}{\partial x}+\dfrac{\partial v}{\partial y}=0 $$ and x-component of momentum equation : $$u\dfrac{\partial u}{\partial x}+v\dfrac{\partial u}{\partial y}=-\dfrac{1}{\rho}\dfrac{d p}{dx}+{\nu}\dfrac{\partial^2 u}{\partial y^2} . $$ the general assumptions behind this equations is that along the x-axis quantities vary slower than along y-axis . let us denote $l$ the characteristic length scale along the x- axis , $\delta$ the scale along the y-axis and $u$ is the characteristic velocity of the fluid . ( $l$ could be , for instance , the length of the body and $\delta$ the boundary layer thickness . ) we thus have $l \gg \delta$ and $u \sim u$ . various derivatives could be estimated by $$\dfrac{\partial }{\partial x} \sim \frac{1}{l} , \qquad \dfrac{\partial }{\partial y} \sim \frac{1}{\delta} . $$ now applying this approximations to continuity equation we get for instance the scale of y-component of the velocity : $$v \sim \frac \delta l u . $$ now for the x-momentum equation both terms of the left side and the term with pressure on the right have all the following estimate : $$ u\dfrac{\partial u}{\partial x}\sim v\dfrac{\partial u}{\partial y} \sim \dfrac{1}{\rho}\dfrac{d p}{dx} \sim \frac{u^2}l , $$ ( the term with pressure could be rewritten using bernoulli 's equation as $u_\infty \dfrac{ d u_\infty}{dx}$ which would lead to the same estimate ) . the single remaining term of x-momentum equation will have the following approximation : $$ {\nu}\dfrac{\partial^2 u}{\partial y^2} \sim \nu \frac {u}{\delta^2} . $$ combining this estimates we get $ ( * ) $ . note , that i wrote it using the $\sim$ and not $\approx$ as in wiki page . now we further assume that the problem at hand is the blasius boundary layer , that is semi-infinite plate in uniform flow parallel to it . in this problem there is no intrinsic length scale ( because the plate is infinite ) , so role of scale along the x-axis would be played by the current value of the x-coordinate $l = l ( x ) = x$ , and the traversal scale $\delta$ also has to be dependent on $x$ . then we simply find from $ ( * ) $ $$\delta ( x ) \sim \sqrt{\frac{x \nu}{u}} , $$ which is yours second equation .
accelerated charged particles emit electromagnetic radiation . in this case , where the acceleration is caused by a magnetic field and is perpendicular to the velocity , the radiation is called cyclotron radiation . since the magnetic field does not work on ( electrically ) charged particles , the radius of the charged particle should reduce , as it is energy ( and so it is speed ) reduces due to the radiation .
surely . lets consider scattering of a 1-d particle on a small potential barrier . to solve the problem , we will find energy eigenstates : $$ h|\psi\rangle=e|\psi\rangle $$ set $h=p^2/2m+\epsilon u=h_0+\epsilon u$ , where $\epsilon$ is a small parameter . consider the equation $$ ( h_0-e ) |\psi\rangle=|\phi\rangle $$ let us write a solution as $$ |\psi\rangle=|\psi_0\rangle +g_0 ( e ) |\phi\rangle $$ where $|\psi_0\rangle$ lies in $e$-eigenspace of $h_0$ and $g_0 ( e ) $ is the operator with kernel ( in coordinate rep ) being the casual green function of $h_0-e$ . now we write the original problem : $$ ( h_0-e ) |\psi\rangle=\epsilon u|\psi\rangle$$ so $$ |\psi\rangle=|\psi_0\rangle +\epsilon g_0 ( e ) u|\psi\rangle $$ we want this to reduce to a free particle moving from left to right for $\epsilon=0$ , so we write in the coordinate representation $|\psi_0\rangle=\exp ( ip_ex ) , \ , p_e^2/2m=e , p_e&gt ; 0$ , and the equation : $$ \psi ( x ) =\exp ( ip_ex ) +\epsilon\int_{-\infty}^x g_0 ( e , x-x' ) u ( x' ) \psi ( x' ) dx ' $$ this is just what you want , with $k ( x , y ) =g_0 ( e , x-y ) u ( y ) , \lambda=\epsilon$ , it is a volterra second kind integral equation , and partial sums of liouville-neuman series give a perturbative solution to 1-d scattering .
this kind of pictures is often misleading . when a paradox appears , common sense tells us that there is very likely something wrong with the assumptions . here the bad assumption is to display the whole atmosphere as a single box what leads to an apparent paradox . in reality the density varies with altitude and this has for consequence that the mean free path of photons varies with altitude too . variations of all radiative parameters with altitude follow . to see the difference with a picture where the atmosphere is represented by n layers , let us take an example for n=2 . we suppose that a solid surface s is radiating f ( w/m² ) into an atmosphere constituted of 2 layers l1 and l2 and the system is in a steady state . we suppose farther that l1 absorbs all the radiation coming from s . l1 must reemit all the absorbed radiation ( steady state ) and because of isotropy , f/2 is reemitted up and f/2 is reemitted down . a part p of the upgoing f/2 will be absorbed by the upper layer l2 and of course again reemitted p/2 up and out of the system and p/2 down . as this downgoing p/2 will be absorbed by l1 and again reemitted p/4 down and p/4 up , it is proven that the flux going from the atmosphere to the surface s is f/2 + p/4 + . . . > f/2 . so it is actually no paradox that the downgoing flux is more than a half of the flux emitted up by the surface . the values depend on the detailed radiation properties of the layers . of course n=2 is still not realistic enough and i have omitted the algebra that can be done by any interested reader but the purpose was to show that an n layer atmosphere does not behave like a 1 layer atmosphere . among others i have omitted the source of energy that is necessary to make up for the difference between f that s emits and f/2 + . . . etc that s absorbs from the atmosphere . regarding the reflection , there is none in this particular case . the infrared radiation emitted by the surface between 0°c and 30°c is mostly absorbed by the vibrational spectrum of h20 molecules ( partly by co2 too ) . so this radiation is indeed absorbed and exactly reemitted with the exception of a small window that goes directly to space . obviously there is always some scattering ( clouds ) but the processes in the dense and warm lower atmosphere are dominated by absorption and emission in the ir spectrum .
wrong . you are neglecting the viscosity of the water . friction from the inner wall of the sphere will move the water , which will in turn move the marbles and they will likely rotate around the sphere , though at a speed slower than the rotation of the sphere . if your sphere was on earth , then the only reason the marbles stay at the bottom is gravity pulling them down . if you shake the sphere ( even assuming that the walls do not touch the marbles ) , then they will move in this situation too due to the force of the water flowing around the sphere .
you can reconcile both trains of thought by reconsidering your thoughts about pushing:- for dc circuits without changing magnetic fields , the voltage is the energy gained or lost per unit charge in moving from one position to another , say from the positive to the negative terminal of the battery . what a battery does , is that it creates certain junctions which intrinsically have a potential difference between them even in equilibrium . the battery , through chemical reactions , maintains this potential difference . the potential difference between the two ends of the battery is the culmination of all these junction potentials within the battery which is maintained by ongoing chemical reactions . so the battery 's job is to only maintain a potential difference across its terminals , and the rest of the drama plays out on its own . now , if we connect the two terminals of a battery by a conducting wire , the wire now has its ends at different potentials , that is , there is a potential difference across the ends of the wire . this potential difference sets up an electric field within the wire from the positive to the negative ( $\vec e=-\nabla v$ ) terminal . this field is what does all the pushing . this field propels the electrons towards the positive terminal . since the electrons keep colliding with the surrounding metallic kernels , they never acquire a constant acceleration , but they acquire a constant drift velocity with which all the electrons slowly edge towards the positive electrode . when the electrons collide , they lose energy as heat . therefore , when electrons having unit electric charge come from the negative end to the positive end , colliding along the way , the amount of energy dissipated as heat ( or any other way ) will be equal to the potential difference between the two ends of the wire , which is the terminal voltage of the battery . all what the battery does is that it carries on chemical reactions to maintain a constant potential difference between its ends . the condition when the circuit is setting up is distinct from the condition in steady state . when the circuit is starting the electric field is still not established along the wire , i.e. all the electrons along the wire do not feel the push of the electric field . here , the end closest to the terminal " feels " the potential difference first and starts moving . as only a small part of the conductor has a current , there will be unbalanced charges which make the next part feel the potential difference , and so on until the other end of the wire . this is similar to the kind of pushing you describe . but all of this happens to fast to matter in most circuits and the condition after the circuit is setup is described above .
the repelling is another way of saying that owing to the strength of the hydrogen bonding between water molecules , the water molecules are better off with themselves alone as compared to with non-interacting non-polar molecules within . a substance dissolves only in a solvent , where the solvent-solute interaction is as strong ( or stronger ) than the solvent-solvent interaction and therefore the solvent finds it better ( energetically and thermodynamically favourable ) to allow the solute molecules to dissolve , i.e. take up spaces between the molecules . but if the solute-solvent interaction is poor , ( as in the case of non-polar/hydrophobic molecules ) , the solvent finds it better to be among itself and not allow the hydrophobic molecules to take up spaces between the molecules , which is equivalent to having repelled the non-polar substances , i.e. their mixing with water is energetically opposed . this is also the reason why polar substances do not dissolve in non-polar solvent . there are hydrophobic surfaces which depend on the surface energy or the contact angle of water on that surface , but the same argument cannot be extended to molecules where there is no surface to account for the surface energy .
well , it is simple . if $\omega_x/\omega_y$ is irrational , then the evolution visits the neighborhood of any allowed point in the phase space arbitrarily closely . this is pretty much a more general form of the claim that $\cos kn$ for $k$ irrational and $n\in{\mathbb z}$ may belong to any interval $r\pm \epsilon$ for any $-1\lt r\lt 1$ and arbitrarily small $\epsilon$ . so in the irrational , aperiodic case , there can not be any extra conservation law . any initial condition is , with the help of some appropriate waiting ( shift in time ) , equivalent to any other within an arbitrarily small $\epsilon$ , so the candidate conserved yet continuous quantity has to change by $o ( \epsilon ) $ where $\epsilon$ is arbitrarily small : it can not change at all . only trivial ( constant ) quantities are conserved . if the frequency ratio is rational , then the trajectory on the phase space is periodic . for example , the vector $$ ( \cos ( p_1/q_1 ) t , \cos ( p_2/q_2 ) t ) $$ where $p_i , q_i$ are integers is periodic in time $t$ with the period $2\pi$ times $q_1q_2$ ( over the greatest common divisor of $q_1 , q_2$ , if you want to make the period as short as possible , the true one ) because both coordinates are periodic with this period . it follows that the closed trajectories on the phase space are non-intersecting ( initial conditions uniquely dictate evolution ! ) compact 1-dimensional curves , topologically circles , and there exist transverse dimensions to these curves that may be used to parameterize these closed curves . these parameters labeling the curves are therefore obviously conserved quantities , by definition , because i assigned one fixed value to each full curve ( i.e. . to the points in the phase space at any value $t$ given some initial conditions ) . so the only remaining work in a particular case is to find a convenient form of these conserved parameters . note that the kepler problem of the planetary motion predicts close elliptical curves . the extra conserved quantity associated with this periodicity is the runge-lenz vector ( pointing from the center to the focus of the ellipse , and this vector may be calculated from $x , p$ at any point along the elliptical orbit ) . that is perhaps the simplest example of the concept . locally on the phase space , one may always define parameters that label the trajectories , as some parameters transverse ( or not parallel ) to particular trajectories in the phase space . but trying to extend these candidate parameters globally ends up in trouble : if we return to the same small region where we started , we get contradictory values of these parameters : they are not single-valued if the trajectory is not periodic .
the question formulation ( v2 ) seems to mix the notions of invariant and covariant , which essentially is also the main point of user1504 's answer ( v1 ) . let 's say we have a group $g$ . the group $g$ could e.g. be a finite group or a lie group . when we say that a theory is invariant under $g$ , it normally implies at least two things . the group $g$ acts on the theory . this in particular means that there is a well-defined given prescription on how the constituents of the theory change under the action of the group . often in physics ( but not always ) , it happens that the group action is linearly realized , i.e. the fields , the matrix elements , and other objects form linear representations of the group $g$ . at this stage , the representations could be reducible or irreducible , finite dimensional or infinite dimensional . the corresponding object then behaves covariantly ( not necessarily invariantly ) under the action of the group . if a representation is completely reducible , we can decompose it in irreps . in case of an off-shell formulation : the action $s$ is off-shell invariant under the group $g$ . or phrased equivalently , the action $s$ form a trivial representation of the group . in case of an on-shell formulation : the equations of motion behave covariantly under the group $g$ . a refinement . ron maimon makes in a comment an important point that if there is a hierarchy of say two theories , and if the group action is a priori only defined in the smaller theory , then the group $g$ does not necessarily have to have a well-defined action on the larger theory . for instance , small theory = on-shell formulation ; large theory = off-shell formulation . small theory = minimal $s$-matrix formulation ; large theory = an underlying field theoretic formulation . small theory = formulation on gauge-invariant physical subspace/submanifold/phase space ; large theory = formulation on brst-extended space/manifold/phase space .
i guess lubos motl 's comment really refers to the terminology used in my post . if i try to insist on what i meant by " fermionic string " , the string formed by $s=s_{rns}-s_p$ , the massless free dirac action $s=\iint\limits_{s} i\hbar\gamma^\mu\partial_\mu\psi \mbox{ d}^2\xi$ , then i guess it would simply mean that the theory is inconsistent . the only way i can see that this is so , is that the " fermionic string " again is an inconsistent string theory . i think i get why this is so . if there are no fields $x^\mu$ in the action , then the string worldsheet can not get embedded into spacetime at all ( ! ) . this theory would then not exist . so the answer boils down to " the ( purely ) fermionic string is not studied because it is not even a consistent theory , since the string worldsheet would not be embedded into spacetime . "
i think the dominant effect might actually be the fact that the salt you add might not be at boiling temperature . but this is just based on the fact that the boiling-point elevation due to salt in water is actually quite low for typical amounts of salt used in cooking , say . i am not too familiar with the second effect you mention though .
for a perfect full moon , simply change am to pm and vice versa . for other phases , there is a different offset . first and third quarters would be six hour offsets , etc . , though i would have to think a while before stating which was plus and minus . during a total solar eclipse , the times would exactly coincide , and approximately so for a new moon . ( good luck finding the shadow cast by new moonlight . ) technically , for best precision you ought to account for the equation of time and the slight change in moon phase over the course of a moon-day . also , since the moon 's orbit is inclined with respect to the earth 's , you can not use the same latitude setting all the time , but this is a frikking repurposed sundial we are talking about . if you are really after precise timekeeping , use a technology invented in the last thousand years .
at $r=0$ we should have $−v_0 r ( r ) =er ( r ) $ , which implies $e=−v_0$ . ( is this allowed ? ) nope , not allowed . in any case , that ode is not as bad as it looks . change variables from $r ( r ) $ to $u ( r ) $ [ which was defined in the problem as $u ( r ) = r r ( r ) $ ] , and it will wind up looking very familiar . you will come up with a second-order ode that has two linearly independent solutions , $$u_1 ( r ) = \cdots , \quad u_2 ( r ) = \cdots$$ then you can get the two independent solutions to the original schrodinger equation as $r_i ( r ) = \frac{u_i ( r ) }{r}$ . the final physical solution for $r ( r ) $ needs to be well-defined at the origin . ( in fact you realized this in the second part , but you do not need to worry about it there because $r &gt ; r_0$ does not include the origin ; but you do need to worry about it here ) so you need to pick a linear combination of $r_1 ( r ) $ and $r_2 ( r ) $ that is not infinite at $r = 0$ . hint : what needs to be the numerical value of $u ( 0 ) $ ? the other part , with $r &gt ; r_0$ , is extremely similar . again , remember that there are two linearly independent solutions . you only found one of them . also , the solution you found does not blow up if $k$ is negative - but are you sure that $k$ is negative ? what do you know about the value of $e$ ? ( specifically , what does it mean for the particle to be in a bound state ? ) you do not need to care about what the solution to the second part does at the origin , because the region you are solving the equation in does not include $r = 0$ . but it does include $r \to \infty$ , so you will need to pick a linear combination of the solutions that stays finite in that limit .
you might like inward bound by abraham pais . author was a particle physicist . the book is mostly a history of particle physics , but quantum mechanics is heavily intertwined . otherwise it meets your criteria perfectly .
this could be solved using the buoyancy force equation : $ f = \rho v g $ where f is the buoyancy force , $\rho$ is the density difference , $ v$ is the displaced volume and $g$ is the local gravitational constant ( normally assumed to be equal to $9.81$ ) from your question : $ v = \frac4 3\pi r^3 = \frac4 3 ( 3.1416 ) ( 10 ) ^3 = 4188.8 \space m^3 $ and $\rho = 1.2-0.9 =0.3 \frac{kg} {m^3} $ using newton 's third law ; we found that the maximum weight it could carry equals to the lifting force of the balloon . ( just for reference ; the weight equals : $f_{max} =m_{max}g$ . ) therefore ; $$ f_{max}=\rho v g \\ f_{max} =0.3\times4188.8\times9.81\\ f_{max} =12327\space n $$ therefore , the maximum weight it could carry is $12000\space n$ ( rounded taking into account of the significant digits ) .
just because the maximum speed is $6\pi\text{ cm/s}$ does not mean that $6\pi = 6\pi \cos ( 3\pi t ) $ . keep in mind that speed is the absolute value of velocity $x&#39 ; $ .
one alkaline aa cell has about 11 kj of energy . for a laptop battery , it is 360 kj . chevrolet equinox fuel cell has 58 mj of energy . one kilogram of tnt carries about 4.184 mj of energy . divide the numbers from the previous paragraph by this constant to see that the aa cell , laptop battery , and electric car battery have 2.6 grams , 86 grams , or 14 kilograms of tnt . note that tnt usually releases all the energy abruptly . gunpowder has 3 mj per kg or so . it means you have to add about 35% to get the right estimate for the mass of equivalent gunpowder . if you could release the energy from the batteries very quickly , the explosion could be equally devastating as the corresponding gunpowder and tnt except that batteries can not release energy this quickly .
by definition an orbit occurs when gravity balances with the " centrifugal " force . it is essentially a free fall situation . so the answer is the same reason why you do not get stuck to the ceiling of a free falling elevator . both the spacecraft and the occupants are moving in-sync .
the book i am reading defines the position of the com of a two-particle system to be $x_{com}= \large\frac{m_1x_1+m_2x_2}{m_1+m_2}$ i am sorry if this seems like a trivial question , but could someone explain to me the interpretation of this definition ? perhaps even why they defined it to be this way . it is a weighted average of the position of the particles where the weighting is the mass . to see this , consider the two-particle , discrete case where the masses are the same , then it reduces to $x_{com} = \frac{x_1 + x_2}{2}$ which is clearly an average position . it is useful because it turns out that you can often ( but not always ) factor out com motion from motion relative to the com and simplify your like . physicists like to simplify their own lives . what does the author mean by , " the ' particles ' then become differential mass elements $dm$ ? that is just the usual continuum limit . you imagine breaking a continuous distribution into little boxes and treating them with the discrete equation then letter the size of the boxes get arbitrarily small .
answer : there is none . the issue at hand is that the kaehler invariance is just that - an invariance , not a continuous symmetry of the fields . most prominently the superpotential must transform as $$ \mathcal w \to \mathcal w e^{-h} $$ a general superpotential that leads to consistent theories is $$ \mathcal w =\frac{1}{2} m_{ab} \phi^a \phi^b + \frac{1}{3} y_{abc} \phi^a \phi^b \phi^c $$ with at least one of the $m_{ab}$ and $y_{abc}$ non-zero . from this is is obvious , that no transformation of the fields $\phi^a$ exists , such that $\mathcal w \to \mathcal w e^{-h}$ without redefining the couplings . thus , there is a kaehler invariance , which involves a redefinition of the couplings and has its value on its own ( e . g . on non-simply connected internal spaces , the kaehler potential might only be defined locally , with definitions on different charts being equal up to kaehler transformations $\mathcal k ' = \mathcal k + f ( \phi ) + \bar f ( \bar\phi ) $ ) , but this is not a symmetry in the sense of noether 's theorem .
yes . there are three mechanisms of heat loss ( this applies generally , not just to the man in this example ) radiation , conduction and convection . in most everyday cases radiation can be neglected so we just have conduction and convection . conduction is just the transfer of heat along a static object . for example if you hold the end of a metal bar in a flame pretty soon the heat will conduct along the bar and the end you are holding will get hot too . in this case if our man is surrounded by a region of still water he will lose heat by conduction into the water . convection is the transport of heat by a moving fluid . for example if you stand in still air on a winter day it may not feel too cold , but if a gale is blowing you will start feeling cold very quickly . this happens because your body heats the air immediately around it then the wind whisks that warmer air away and replaces it with cold air . now back to your question . we can not do anything about conduction . the man is in the water and we can not change the thermal conductivity of the water . but we can do something about convection . our man 's body heat will soon heat the water immediately around him , and we do not want water currents carrying away that warm water and replacing it with cold water . wrapping a blanket tightly around the man will trap a layer of water near his skin and prevent convection from cooling him .
no they will not . space is intrinsically isotropic , so assuming they are not aware of any specific reference points , and they are far enough away from a massive body as to experience an insignificant amount of gravity , there would be no way of knowing their orientation . gravity essentially provides observers with a force field that the body can utilise to establish orientation etc . hope this helps .
i am not sure what you mean by medium here , but i believe i can still provide an answer to your question . a fuel-thruster works by pushing the fuels reactants back and thus pushing the thruster forward . a human cannot swim in the vacuum because there arms do not push anything back . in water , a human can swim by pushing water back and thus pushing the human forward . by your use of words , i guess you can consider the " fuel " in this case as a " medium " for the reactive force . in answer to your first question , which i interpret as " does a force require both something that caused the force and something that receives the force " , the answer is yes . however , we can write quantities , such as the potential , that only depend on a source .
first problem : you say $v ( t ) = a x^2$ , but that is a function of position , not time . putting the definition right : $$ v = \frac{dx}{dt} = a x^2 $$ you can regroup terms on the same variable : $$ \frac{dx}{x^2} = a dt$$ and then do the integration : $$ \int \frac{dx}{x^2} = \int a dt$$ this is homework , so i will leave the integral limits and the following details to you , but i think this should clarify it enough . the key to your mistake is that you cannot simply do $\int x dt$ , because $x$ is a function of $t$ , but you do not know which one .
the simple answer is that the sun 's gravity produces the same acceleration on both the earth and the moon . the sun is pulling both of them along , but they are falling together . you may imagine two skydiver jumping out of a plane at the same time ( and we had better ignore air resistance ) . they are subjected to gravitational forces from the earth that vastly larger than the forces between them , but that does not rip them away from each other because they both experience the same acceleration .
it is clear from no signalling--- by entangling $a_1$ and $a_2$ , alice uses local operators which necessarily commute with the spin operators on $b_1$ and $b_2$ , so the reduced density matrix for $b_1$ and $b_2$ stays completely random . it makes no difference what method alice uses , unless it involves mucking around with bob 's electrons .
there is a subtlety here as to what is meant by ' one body ' and ' two body ' . what the author is trying to get at is the complexity of the mathematical description of the problem . in the lab frame , you need to specify the positions and momenta of each particle individually as e.g. $\vec{r}_1 , \vec{p}_1 , \vec{r}_2 , \vec{p}_2$ . if you want to work out what happens during the collision you end up solving a bunch of equations involving these quantities , either as functions of time , or their initial and final values , which can get a bit messy . the advantage in the center of mass ( actually i think it is more correct to talk about center of momentum here ) is that the total momentum in this frame is $0$ , so in the case of a two particle system , $\vec{p}_1 = -\vec{p}_2$ . the equations governing the collision usually end up a lot tidier in this frame , and involve only the coordinates of 1 particle ( after some trivial substitutions ) . the problem still involves 2 bodies , and we do in fact need a second pair of coordinates to get a full description of the system - the position and momentum of the com - but these do not appear in the equations governing the collision . a quick summary . . . in the lab frame : solve potentially " messy " equations for coordinates of two particles . in the com frame : transform to the com frame , note the coordinates of the com . solve potentially " cleaner " equations for the coordinates of 1 particle , get the second particle " free " from $\vec{p}_1 = -\vec{p}_2$ . transform back to the lab frame using the previously noted com coordinates . which one is actually " simpler " for a given problem is up for debate . . . in the end , the same problem is solved . finally , http://en.wikipedia.org/wiki/center-of-momentum_frame does an ok job of walking through a fairly general 2 body problem in the com frame and how the coordinates of the particles are potentially simplified in that frame .
if we do an interference experiment with a ( charged ) particle coupled to the electromagnetic field or a massive particle coupled to the gravitational field , we can see interference if no information gets stored in the environment about which path the particle followed ( or at least , if the states of the environment corresponding to the two paths through the interferometer have a large overlap --- if the overlap is not 1 the visibility of the interference fringes is reduced ) . the particle is " dressed " by its electromagnetic or gravitational field , but that is not necessarily enough to leave a permanent record behind . for an electron , if it emits no photon during the experiment , the electromagnetic field stays in the vacuum state , and records no " which-way " information . so two possible paths followed by the electron can interfere . but if a single photon gets emitted , and the state of the photon allows us to identify the path taken with high success probability , then there is no interference . what actually happens in an experiment with electrons is kind of interesting . since photons are massless they are easy to excite if they have long wavelength and hence low energy . whenever an electron gets accelerated many " soft " ( i.e. . , long wavelength ) photons get emitted . but if the acceleration is weak , the photons have such long wavelength that they provide little information concerning which path , and interference is possible . it is the same with gravitons . except the probability of emitting a " hard " graviton ( with short enough wavelength to distinguish the paths ) is far , far smaller than for photons , and therefore gravitational decoherence is extremely weak . these soft photons ( or gravitons ) can be well described using classical electromagnetic ( or gravitional ) theory . this helps one to appreciate how the intuitive picture --- the motion of the electron through the interferometer should perturb the electric field at long range --- is reconciled with the survival of interference . yes , it is true that the electric field is affected by the electron 's ( noninertial ) motion , but the very long wavelength radiation detected far away looks essentially the same for either path followed by the electron ; by detecting this radiation we can distinguish the paths only with very poor resolution , i.e. hardly at all . in practice , loss of visibility in decoherence experiments usually occurs due to more mundane processes that cause " which-way " information to be recorded ( e . g . the electron gets scattering by a stray atom , dust grain , or photon ) . decoherence due to entanglement of the particle with its field ( i.e. . the emission of photons or gravitons that are not very soft ) is always present at some level , but typically it is a small effect .
the difference is in how you are halting rotation once it starts . in a car , a low center of gravity means in effect that you have several " arms " extending outwards ( the tires ) to push against a solid surface every time the car tries to turn over , say in a sharp curve . the more horizontal those arms and the farther they extend outwards , the more stable your vehicle will be . that argues for making your center of gravity lower . there is another twist to it that i will get into below . for rotational inertial , think of those long poles that people who walk tightropes usually carry . the farther out those poles extend mass ( sometimes they have heavy balls on the ends for example ) , the greater the rotational inertia , and the more opportunity the walker will have to correct his balance by pushing against that inertia . so in that case , the rotational inertia directly contributes to her ability to remain stable . so there is an example of how , in the absence of a solid surface to push on , rotational inertia can be directly used to keep from tipping over . the twist i mentioned for vehicles is this : they too can benefit from the rotational inertia strategy , but usually do not bother ! the reason is it produces funny-looking vehicles that most folks would not want to be seen driving . take two big harley-davidson motorcycles , each with the wheel separation of and exactly half the mass each of that same small car . also , adjust their centers of gravity to be the same in height and front-to-back position as for the small car . now weld them together using lightweight bars , so that their wheels are the same distance apart left-to-right as in the small car . ( i suggest getting permission from the harley owners first ; they can be so picky about such things . . . ) you now have two vehicles with about the same mass , the same wheel patterns , and the same centers of gravity . the only difference is that in one case most of the mass is located close to the center of gravity ( the small car ) whereas the in the other case most of the mass is located near either set of wheels ( the dual harley ) . in terms of physics , this means that despite have identical masses , wheel positions , and even centers of gravity , the two vehicles are not identical because the dual harley will have much higher rotational inertial . so if you put them into an extreme race course designed to test flip-over stability , which vehicle do you think will win ? yep : the dual harley , for the same reason that a tight-rope walker is more stable with a pole than without one . so , bottom line : both having long " push arms " against a solid surface and having higher rotational inertial can help stabilize a ground vehicle . it is just that the pushing option is so much easier and gives such better-looking vehicles that no one bothers with the additional edge given by concentrating most of the mass over the wheels . addendum : the dual harley is not any better at roll resistance ! i made an error ! the dual harley will not do any better on curves than the same-mass vehicle ! i am flagging myself instead of editing out the error , though , since it is kind of an interesting error . i would have been fine if i had suggested this : take your vehicle and add two long poles with weights on their ends sticking out both sides ( passers beware ! ) . that would have kept the vehicle more stable in the same way as for the tightrope walker . it also would have made turns harder , though , since the same added rotational inertia would resist left-right turns just as well as rolls ! now in the case of the dual harley , there is nothing wrong with my assertion distributing mass towards the wheels gives higher overall rotational inertia -- it does . but there is a more subtle error in why it does not provide any additional roll resistance . anyone see it ? i will leave it open as a problem for someone else for now , and answer it if no one else does .
the path integral in quantum mechanics computes the evolution kernel , which is the matrix element of the evolution operator : $\mathrm{exp} ( ih t ) $ , ( $h$ is the hamiltonian ) , between two position eigenstates . the path integral expresses the evolution kernel as a sum over paths : $u ( x , t , x_0 ) = \int_{x ( 0 ) =x_0}^{x ( t ) =x} \mathrm{exp} ( \frac{is}{\hbar} ) \mathcal{d}x$ . where $u$ is the evolution kernel , $s$ is the classical action . on the other hand , the evolution operator has an expansion as a sum over the energy eigenstates : $u ( x , t , x_0 ) =\sum_n \mathrm{exp} ( \frac{-ie_n t}{\hbar} ) \psi_n ( x ) \psi_n^{*} ( x_0 ) $ where , $\psi_n ( x ) $ are the energy eigenstates . from this expression , it is clear that the evolution kernel has a discrete spectrum , whenever the energies are quantized . in other words , in the case of quantized values of the energy , the fourier transform of the evolution kernel : $\hat{u} ( x , \omega , x_0 ) \equiv \int_{-\infty}^{\infty} u ( x , t , x_0 ) \mathrm{exp} ( i\omega t ) dt$ will be a sum of dirac delta functions centered at the frequencies : $\omega_n = \frac{e_n}{\hbar}$ please observe that the weight of each dirac delta function is just the projection operator on the corresponding discrete eigenstate .
the quantity you are describing ( $s ( \omega ) $ ) is called the power spectral density . i can not say if your interpretation of the power spectral density in this case is mistaken or not , because i have not encountered it myself . but in context of a stationary physical process , the power spectral density describes how the total power in the system is distributed over various frequencies . here power is taken to mean the square of the signal , i.e. , if the signal is $f ( t ) $ , then the total power is given as $$ p = \frac{1}{t} \int_{0}^{t} |f ( t ) |^2 dt$$ the function that you have described is actually the fourier transform of the autocorrelation function , a result given by the weiner-khinchin theorem . if your $x ( t ) $ is a stochastic variable , then $s ( \omega ) $ is the spectral power distribution for that variable/process .
your diagram is not correct as far as camera is concerned . actually in a multifocus camera the focal length is actively changing , which causes the field of view to change . as you can see from the image , a wide angle lens ( having small focal length ) has widest range while the reverse is true for telephoto lens .
the major problem with ultrasound as a mechanism of purification is that it does not break molecules . heat at least denatures proteins and breaks hydrogen bonds , but ultrasound is of a just smaller order of magnitude of energy at the atomic scale , which can be of the order of the adhesive forces holding the liquid together , but not of stronger molecular bonds . but i think you can do it a different way : use a sound waves intensity gradient to move the biological impurities in the water to a part of the container , by having them walk down an effective potential gradient , like optical tweezers move molecules . this requires only that the density/stiffness of the molecules be different from water , so that the sound energy at a given mode is different inside the molecules than in the water . if it is greater , the molecule will move to the regions of greater intensity . if it is less , it will move towards the regions of smaller intensity . by arranging the sound wave to have an intensity gradient , you can make all the molecules segregate towards/away from the microphone , leaving water in the middle with only ionic or small molecule impurities , which are not affected by the sound . you can flush the sides away , and repeat to make a purer water . this might work for getting rid of prions , which are not disinfected by boiling . this article is the only thing i found that might be relevant , but it is paywalled : http://www.annualreviews.org/doi/pdf/10.1146/annurev.bb.20.060191.001541 i think this might be a very useful idea .
i work with stellar models , so i thought i would chip in here . my instant reaction is that you should not worry too much : determining the age of a star is difficult and different models will disagree ( sometimes significantly ! ) on that age . how reliable is this research ? i can not see an obvious reason to doubt the conclusion . what method do they use to measure the age of such a star as methuselah ? basically , one tries to measure as many properties about the star as accurately as possible , and then find the best fitting stellar model . these models are solutions to a set of differential equations ( in time and one spatial dimension ) that tries to capture all the relevant physics that determines how stars evolve . the bulk physics is a fairly well-defined problem but there are several potentially important components that are lacking in these models . ( i will expand on this if desired . . . ) the usual difficulty here is breaking down the degeneracy between brightness and distance . that is , a distant object is fainter , so it is hard to know whether a certain object is intrinsically faint or just further away . the principal result in this paper is the hubble-based parallax measurement , which makes a big improvement on that distance measurement and , therefore , the brightness of the star . the other things they use are proxies for the surface composition and the effective temperature of the star , as far as i can see . incidentally , this is where i would suspect the tension can be resolved . if you look at fig . 1 of the paper , they show the evolution of different stars for different compositions . what you are looking for , roughly speaking , is lines that go through the observed points . that figure shows that if the oxygen content is underestimated , then the best fit is actually about 13.3 gyr , which is no longer at odds with the age of the universe . take note of table 1 , where the sources of error ( at 1$\sigma$ ) are listed . it is interesting that , not only is the star 's oxygen content the largest source of error , but even the uncertainty of the oxygen content of the sun is a contributor ! which is more likely to be wrong , the age of methuselah or the current estimate of the age of the universe ? the age of methuselah , definitely . i would describe our estimates of the age of the universe as in some way " converegent": different methods point to consistent numbers . sure , planck shifted the goalpost by 80 myr or so , but it would be a real shock to see that number change by , say , half a billion years . could relativistic effects account for some of the age ? i have no idea and have not really thought about it . since i am pretty sure this is not a big problem , i do not think relativistic effects are necessary to explain the discrepancy .
your ear is an effective fourier transformer . an ear contains many small hair cells . the hair cells differ in length , tension , and thickness , and therefore respond to different frequencies . different hair cells are mechanically linked to ion channels in different neurons , so different neurons in the brain get activated depending on the fourier transform of the sound you are hearing . a piano is a fourier analyzer for a similar reason . a prism or diffraction grating would be a fourier analyzer for light . it spreads out light of different frequencies , allowing us to analyze how much of each frequency is present in a given source .
when you have a matrix $\phi = \begin{pmatrix} \phi_1\\ \phi_2\end{pmatrix}$ , with one column and two rows , and its transpose matrix $\phi^t = \begin{pmatrix} \phi_1 and \phi_2\end{pmatrix}$ , with one row and two columns , the product of the two matrix $\phi^t \phi$ is a matrix $p$ with one column and one row : $p =\phi^t \phi = \begin{pmatrix} \phi_1 and \phi_2\end{pmatrix}\begin{pmatrix} \phi_1\\ \phi_2\end{pmatrix} = ( \phi_1^2+\phi_2^2 ) $ because this matrix $p$ has one row and one column , it may considered as a scalar . for your particular problem , you have : $ ( \vec{\nabla}\phi ) ^t . ( \vec{\nabla}\phi ) = \sum\limits_{i=1}^n ( {\partial_i}\phi ) ^t ( {\partial_i}\phi ) = \sum\limits_{i=1}^n ( ( \partial_i \phi_1 ) ^2 + ( \partial_i \phi_2 ) ^2 ) $ the first equality comes from the definition of the inner product and the gradient , and the second equality comes from the definition of the transpose operation $t$ , and the manipulation of these matrices , as seen at the beginning of the answer .
the hup holds for elementary particle frameworks . a lamda goes into a pion and a proton , and when we calculate the rest frame we never define an ( x , y , z , t ) , we are interested in the momentum and energy four vectors of the produced particles . when we say a kaon hit a proton in the target , and assume the target at rest , the magnitudes of spatial uncertainty of the target , microns in this picture for the target and velocity the proton has due to the temperature of the target , plus the measurement errors on the momentum of the kaon , a few mev/c , are below our experimental discrimination and fulfill inevitably the hup .
it sounds like you may have been confused because intuitively , you would think a reaction occurs after the action , in response to it . as you have found , that is not what newton 's third law is saying . the reaction force is not a response to the action . for this reason , some people do not like the statement " for every action , there is an equal and opposite reaction . " a better statement of newton 's third law is to say that forces always occur in pairs . it is impossible for object a to exert a force on object b without object b also exerting a force on object a . the two forces are simultaneous , of equal magnitude , and in opposite directions . mathematically , this is stated as $$\mathbf{f}_{ab} = -\mathbf{f}_{ba}$$
yes , the summation is taking over all possible integer value of $m=0,1,2 , . . . $ except $m=n$ . it can be easily seen by following the derivation of the first order perturbation theory . in your example $\psi_1^{ ( 1 ) }$ , it is sum over $m=0,2,3,4 , . . . $ . note that sometimes the index start from 1 instead of 0 such as infinite square well , then you should skip 0 .
the field inside the sphere will not be zero if it is hollow and there is a point charge in the hollowed out part . the field will be zero in the conductor , because the field is always zero in a conductor in electrostatics . what you might be refering to is that the field will be zero inside the hollow sphere if it is charged , because the charges will distribute symmetrically over the sphere .
it looks like p and s have been a little bit dirty at this point . go back to equation 4.29 . . . you really need to take the limit $t\to \infty ( 1-i\epsilon ) $ of this expression . physically what is happening is that the true vacuum $|\omega\rangle$ is not the perturbative vacuum $|0\rangle$ . you are extracting the true vacuum contribution by evolving to large imaginary times . equality only holds in the limit . your first term is an increasing phase divided by $t$ so it goes like a constant , but the second term is a $t$ independent constant divided by $t$ so it disappears .
usually " quantum liquid " refers to the ground state of a hamiltonian that do not break translation symmetry of the hamiltonian . ( in a sense , " quantum gas " = " quantum liquid " . ) " quantum spin liquid " refers to the ground state of a spin hamiltonian that do not break spin-rotation and translation symmetries of the hamiltonian .
lagrangian : $$l~=~\frac{1}{3}t^2+2tv-v^2 , \qquad t~:=~\frac{m}{2}\dot{x}^2 . $$ lagrange equation : $$2 ( t-v ) v^{\prime}~=~\frac{\partial l}{\partial x} ~=~ \frac{d}{dt} \left ( \frac{\partial l}{\partial \dot{x}}\right ) ~=~ \frac{d}{dt} \left [ \left ( \frac{2}{3}t +2v\right ) m\dot{x}\right ] $$ $$~=~ \left ( \frac{2}{3}t +2v\right ) m\ddot{x} + \left ( \frac{2}{3}m\dot{x}\ddot{x} +2v^{\prime}\dot{x}\right ) m\dot{x} ~=~ 2 ( t+v ) m\ddot{x} +4tv^{\prime} , $$ or , $$- 2 ( t+v ) v^{\prime}~=~ 2 ( t+v ) m\ddot{x} . $$ in other words , one gets newton 's second law$^1$ $$ m\ddot{x}~=~-v^{\prime} . \qquad\qquad\qquad ( n2 ) $$ so the lagrangian $l$ is equivalent to the usual $t-v$ at the classical level . -- $^1$ one may wonder about the second branch $t+v=0$ , but since $t+v={\rm const}$ is a first integral to ( n2 ) , the second branch is already included in the first branch ( n2 ) .
your definition is quite good and works almost always . i am quite sure it is rigorously true in 2d . you will actually find it in some lecture notes . remember that a theory is conformal if the trace of the stress tensor vanishes : $t \equiv t_\mu^{\mu} = 0 . $ indeed there is a folk theorem that states that $t = \sum \beta_i \mathcal{o}^i$ where the sum runs over those operators $o^i$ in the theory with their beta functions $\beta_i$ ( up to terms generating the conformal anomaly in curved space ) . however , this is not completely true , and there are important classes of counterexamples where additional terms appear . recently , these examples have led to some confusion in the literature ( in the search for scale but not conformally invariant theories ) . all of this is well understood now and a good starting point for your studies would be 1204.5221 [ hep-th ] . edit : do not forget that operator dimensions are not protected and change under the rg flow .
the mass of the black hole only grows by $ ( 1-\epsilon ) m$ , i.e. the mass that has not been radiated away yet . that is guaranteed by the mass conservation . however , one must be careful about dividing mass and energy to " individual places " in general relativity ; in this case , it can be kind of done , but more detailed questions " where the mass/energy resides " could be meaningless . only the total mass/energy is conserved in general relativity ( in asymptotically flat and similar spaces ) . the local physics of electrons moving in magnetic fields etc . is always the same . the electron mass is always the same constant . to describe what electron is doing in a situation like this , go to a freely falling frame , find out what the values of the electromagnetic fields are in this frame , and use exactly the same electron mass etc . as you would use in the absence of any black hole . if you wanted to use a non-freely-falling frame ( or coordinate system ) to describe the behavior of an electron near the event horizon , you must be very careful to do it right . for example , the gravitational field near the event horizon makes the usual static coordinates extremely deformed relatively to the flat metric – a component of the metric tensor goes to zero or infinity near the event horizon – and there is a nonzero curvature etc . so i am sure that all people who think that general relativity is still essentially the same newtonian mechanics – and in between the lines , you make it likely that you belong to this set – would almost certainly make the calculations incorrectly in a curved system . that is why i am urging you to go to a freely falling frame .
i think you are on the right track . there are a couple of bits of advice you may follow : you may simply note that if $a \geq b$ , then it follows that $a = b$ is a valid solution , thus $a$ and $b$ must have the same units . therefore $\delta{p}\delta{x}$ has the same units as $h$ which has the same units as $\delta{e}\delta{t}$ . the method you used is called dimensional analysis and it is perfectly correct to use it .
correct . pure functions such as $\exp$ , $\sin$ , $\ln$ etc . are always dimensionless - they return $c$-numbers with no units attached . since $\sigma$ is a length , you can indeed conclude that $ [ g ] = l^{-2} . $
the flight actually took 25 minutes , the video you linked to is edited down , and the later stages of it look pretty turbulent . i would guess it was the shaking that eventually broke off the chair leg , possibly due to metal fatigue . note that the leg broke off after the balloon had burst and while the chair was falling back to earth . as anna says , the low temperature may well have been a factor . the ductile-brittle transition for mild steel is around -50c so at -60c it would be a lot more brittle than at room temperature . from the way the leg broke off i would guess it was the weld that failed rather than the steel itself .
i think you should try these books : 1 ) david tong : lectures on classical dynamics ( http://www.damtp.cam.ac.uk/user/tong/dynamics.htm ) 2 ) v . i . arnold , mathematical methods of classical mechanics 3 ) l . landau an e . lifshitz , mechanics
theoretically ? sure . practically ? no . the primary problem is not lack of knowledge about how to manipulate individual atoms , though this is very tricky and it might not currently possible to manipulate the right kind of atoms in the right kind of way for this sort of task . the central problem is one of scale . for an item like a milky way bar , you are talking about billions and billions of atoms . your problem would not be that the candy bar was melty , it would be that you would die thousands of years before there was enough of it put together to take a bite . also , the techniques for atom manipulation work reasonably well for placing atoms on some sort of substrate like printing circuits on silicone chips . when you start talking about things like assembling complex sugars atom by atom , the situation gets significantly more difficult .
with a lot of intuition you are well suited to all areas of physics . i think this is from the educational and the applying side a key difference to a lot of other sciences . for biology , chemistry , medicine and other sciences that are more concerned with the macroscopics than with the small details learning lots of facts is unavoidable . i do not think that there is any branch of physics where you can be successful by following a given path without a lot of intuition whether the idea might finally work or not . while you can ask yourself in which area your intuition is often the right answer a key point is : intuition comes from doing it and thinking deeply about it . a small remark to the last part of your question : there is hardly any problem that can be tackled by pure intuition . from my experience physicists with a very good intuition can pick the ten most likely ideas from a hundred possibilities , whereas the other ones have to try twenty or thirty ideas . only a few gifted ones working in a specific field sometimes for decades might narrow it down to just a few from intuition and experience . in the end the intuitive idea still has to be proven by experiment , calculation or both ; there is no shortcut around .
you can think of it in this way : to find position of any object we use reflected light from that object . for day-to-day life objects there is no problem . but for subatomic particle it means that we are giving them considerable amount of momentum and energy through photons . thus the very moment we measure their position we are also changing their momentum . thus both cannot be known with absolute certainty at the same time . hope this helps .
an electron volt is just the energy acquired when an electron of charge $e$ falls through a potential of 1 volt , which means $$1ev = e \times 1 = 1.6 \times 10^{-19} j$$ when you lift up your $2.5kg$ laptop ( a 15-inch apple macbook pro , for example ) by a foot , you do a work of approximately $2.5 kg \times 10 ms^{-2} \times 0.3 m = 7.5 j$ which is about $4.7 \times 10^{19} ev$ . so an $ev$ is a really low energy scale by everyday standards . one tev ( a tera electron volt ) is about the energy of motion of a flying mosquito .
a colleague in astronomy had a student a few years ago who did a calculation about the possibility of primordial black holes , created in the big bang . if the size of these was just right , they could be evaporating into nothing due to hawking radiation right " now " ( scare quotes because this would necessarily include distant black holes that evaporated many years ago , whose light is just reaching us now ) . the last burst of hawking radiation for these would look basically like a faint gamma-ray burst , in which case it ought to be directly detectable . i am not sure of the current status of this-- their preliminary result was , if i remember correctly , that you might be able to test this by measuring the probability distribution for gamma-ray bursts of the appropriate size and duration , but we did not have any telescopes capable of picking them up at the time . i am not sure if that is changed or not . anyway , that would give you a direct way to detect black holes of a certain size , though they would not be around after the detection , so it might not really fit the spirit of the question . . .
i have experienced something similar . i am not 100% sure if it is the same phenomenon you are describing , but i suspect so . it happens if you use coffee that is ground too finely , and does not have to do with boiling . what happens is that the fine coffee grains block all the holes in the mesh . this means that the water is under more pressure than usual , since it can no longer pass through the plunger . because of this the water ends up escaping by forcing a small part of the mesh away from the side of the carafe and squirting out at high velocity . the reason for the high speed is just that it is passing through a small aperture - it is the same effect as when you put your finger over a hose . to prevent this from happening , you could try a coarser grind , or if you already use coarse-ground coffee , try pressing even more gently on the plunger . if you meet resistance then try lifting the plunger slightly before continuing - this should redistribute the grounds slightly and hopefully unblock the holes in the mesh . from a physics point of view it is worth mentioning that boiling would be a very unlikely response to compressing hot coffee . it is possible for a liquid to be in a " superheated " state , where it is above its boiling point yet remains liquid . when water is in such a state it can indeed boil very suddenly . but this state can only be reached if there are no nucleation sites available to allow steam bubbles to form , and the coffee grounds would probably provide excellent nucleation sites , so if the water were superheated it would boil as soon as you poured it onto the grounds . ( this would probably produce very bitter coffee . ) liquids can also suddenly boil if their boiling point decreases below their temperature - but pressing the plunger increases the pressure , which increases rather than decreases the boiling point . this is true for all liquids ( by le chatelier 's principle ) , so we would never expect boiling to result from an increase in pressure .
i think you should understand the passage a bit more abstractly : take the space $\mathbb{r}$ . it is obviously one-dimensional . now , consider the space $\mathbb{r}\times\mathbb{r} = \mathbb{r}^2$ , the vector space over $\mathbb{r}$ with two dimensions . you have thus created a two-dimensional object from a one-dimensional one . let us now construct the book : each page is a rectangle , which is , if $i$ is the unit interval , $i \times i$ . the book has finitely many pages , so we take the set of page numbers $p = \{p_1 , \dots , p_n\}$ and consider the book to be $p\times ( i\times i ) $ . for any element $ x \in p\times ( i\times i ) $ , you would need three numbers to describe it : the page number , and the height/width on the pages , it is therefore , in a sense , three-dimensional . if we allow the book to have ( uncountably ) infinitely many pages , then $p \cong i$ , and the book becomes $i^3$ , a cube , a true three-dimensional space . for pages with " infintesimal " thickness , you would need this many pages to create a true 3d object . you could also turn the argument around : take a cube $i^3$ and slice it along each position in one of the intervals . you obtain uncountably many 2d slices , which make up the whole 3d cube . you are of course right that real pages will not be truly two-dimensional , therefore i think the passage is not meant to be understood literally .
let 's look to your own statements . first , time derivative after transformations is not equal to an " old " derivative : for $\mathbf r ' = \mathbf r - \mathbf u t = \mathbf r - \mathbf u t ' \rightarrow \mathbf r = \mathbf r ' + \mathbf u t'$ $$ \partial_{t'} = ( \partial_{t'}\mathbf r ) \partial_{\mathbf r} + ( \partial_{t'}t ) \partial_{\mathbf t} = ( \mathbf u \cdot \nabla ) + \partial_{t} , \quad ( \mathbf u \cdot \nabla ) = u^{i}\partial_{x_{i}} . $$ so , with $\nabla ' = \nabla$ , " bianchi " equations transforms to $$ ( \nabla \cdot \mathbf b' ) = 0 , \quad [ \nabla \times \mathbf e ' ] + \frac{1}{c}\partial_{t}\mathbf b ' + \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf b ' = 0 . \qquad ( . 1 ) $$ second , the form of $\mathbf {e}' ( \mathbf r ' , t' ) , \mathbf b ' ( \mathbf r ' , t' ) $ is not equal to $\mathbf e ( \mathbf r , t ) , \mathbf b ( \mathbf r , t ) $ . let 's use the lorentz force expression , $$ \mathbf f = q\mathbf e + \frac{q}{c} [ \mathbf v \times \mathbf b ] . $$ it does not depend on acceleration , so the statement that $\mathbf f ' = \mathbf f$ under galilean transformation is true . it means that $$ \mathbf e + \frac{1}{c} [ \mathbf v \times \mathbf b ] = \mathbf e ' + \frac{1}{c} [ \mathbf v ' \times \mathbf b' ] . $$ by using galilean transformation for speed , $\mathbf v ' = \mathbf v - \mathbf u$ , this equation can be rewritten as $$ \mathbf e + \frac{1}{c} [ \mathbf v \times \mathbf b ] = \mathbf e ' + \frac{1}{c} [ \mathbf v \times \mathbf b ' ] - \frac{1}{c} [ \mathbf u \times \mathbf b' ] , \qquad ( . 2 ) $$ so the statement that $\mathbf e = \mathbf e ' , \quad \mathbf b = \mathbf b '$ is not correct . so you need to find expressions $\mathbf e ' $ and $\mathbf b'$ via $\mathbf e $ , $\mathbf b$ . by rewriting $ ( . 2 ) $ , $$ \mathbf e + \frac{1}{c} [ \mathbf v \times ( \mathbf b - \mathbf b ' ) ] = \mathbf e ' - \frac{1}{c} [ \mathbf u \times \mathbf b ' ] , $$ in a reason of arbitrary $\mathbf u $ you can get the solution : $$ \mathbf b ' = \mathbf b , \quad \mathbf e ' = \mathbf e + \frac{1}{c} [ \mathbf u \times \mathbf b ] . $$ by substitution these equations to $ ( . 1 ) $ you will get $$ ( \nabla \cdot \mathbf b ) = 0 , \quad [ \nabla \times \mathbf e ] + \frac{1}{c} [ \nabla \times [ \mathbf u \times \mathbf b ] ] + \frac{1}{c}\partial_{t}\mathbf b + \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf b = [ \nabla \times \mathbf e ] + \frac{1}{c}\partial_{t}\mathbf b = 0 , $$ because for $\mathbf u = const$ $$ [ \nabla \times [ \mathbf u \times \mathbf b ] ] = \mathbf u ( \nabla \cdot \mathbf b ) - ( \mathbf u \cdot \nabla ) \mathbf b = - ( \mathbf u \cdot \nabla ) \mathbf b . $$ so the first pair of maxwell 's equations is clearly invariant under galilean transformations . let 's look to the other pair of maxwell 's equations : $$ [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e = 0 , \quad ( \nabla \cdot \mathbf e ) = 0 . \qquad ( . 3 ) $$ by using an expressions which were derived above , you can rewrite $ ( . 3 ) $ as $$ [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e ' - \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf e ' = $$ $$ = [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e - \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf e - \frac{1}{c^{2}}\partial_{t} [ \mathbf u \times \mathbf b ] - \frac{1}{c^{2}} ( \mathbf u \cdot \nabla ) [ \mathbf u \times \mathbf b ] = 0 , $$ $$ ( \nabla \cdot \mathbf e ) + \frac{1}{c} ( \nabla \cdot [ \mathbf u \times \mathbf b ] ) = ( \nabla \cdot \mathbf e ) -\frac{1}{c} ( \mathbf u \cdot [ \nabla \times \mathbf b ] ) = 0 . $$ the requirement of galilean invariance of second equation leads to te state that $\frac{1}{c} ( \mathbf u \cdot [ \nabla \times \mathbf b ] ) $ , which is not true in the general case . analogically reasoning can be used for the first equation . so the second pair of maxwell 's equations is not invariant under galilean transformations .
the problem is you have the wrong relations between $\{n_\mathrm{h} , n_\mathrm{he}\}$ and $\{n_\mathrm{p} , n_\mathrm{n}\}$ . every hydrogen contains 1 proton , and every helium contains 2 , so $n_\mathrm{p} = n_\mathrm{h} + 2 n_\mathrm{he}$ . the neutrons are only contributed to by helium in the accounting : $n_\mathrm{n} = 2 n_\mathrm{he}$ . inverting these relations yields \begin{align} n_\mathrm{h} and = n_\mathrm{p} - n_\mathrm{n} \\ n_\mathrm{he} and = \frac{1}{2} n_\mathrm{n} . \end{align} it looks like you got the direction wrong , in the sense that there should be fewer helium nuclei than protons or neutrons ( the 2 's are on the wrong side ) . also , " hydrogen " means ${}^1\mathrm{h}$ not ${}^2\mathrm{d}$ unless otherwise stated .
finally , i got it . the sketch is as following : in order to understand the fermionic measure , suppose that we did something suitable so that the spectrum of the appropriate dirac operator is discrete ( for exapmle , take the volume to be finite , work on compact manifold ) . let $\psi_n ( x ) $ be the eigenbasis for the operator . then we use $\det c = \exp ( tr \ln c ) $ for $c$ the transition operator . in our case it multiplies by $\exp ( \phi ) $ , so $\ln c$ is multiplication by $\phi$ . trace $\sum_n\int d\mu ( x ) \phi ( x ) \bar{\psi}_n ( x ) \psi_n ( x ) $ is regularised either by $\zeta$-regularization or by introducing $\exp ( -d^2/m^2 ) $ with $d$ the dirac operator . basically the same as the chiral anomaly in inspirehep . net/record/17430 , but we take the finite transformation instead of infinitesimal one .
let $\mathbf u ( t , \mathbf x ) $ represent the velocity vector field of the fluid . let $\mathbf x ( t ) $ denote the position of a particle moving with the fluid , then the velocity $\dot{\mathbf x} ( t ) $ of the particle at a time $t$ will be equal to the velocity of the fluid flow at the point $ ( t , \mathbf x ( t ) ) $ , namely $$ \mathbf u ( t , \mathbf x ( t ) ) = \dot{\mathbf x} ( t ) $$ now suppose that $\mathbf u ( t , \mathbf x ) \cdot\nabla h ( t , \mathbf x ) = 0$ . he want to show that this implies that $h$ is constant along the path of a particle moving with he fluid . notice that for any path $\mathbf x ( t ) $ we have $$ \frac{d}{dt}h ( t , \mathbf x ( t ) ) = \frac{\partial h}{\partial t} ( t , \mathbf x ( t ) ) +\dot{\mathbf x} ( t ) \cdot\nabla h ( t , \mathbf x ( t ) ) $$ assuming then that $\partial_t h = 0$ , and assuming that the path $\mathbf x ( t ) $ is that of a particle moving with he fluid , the equations written above imply $$ \frac{d}{dt}h ( t , \mathbf x ( t ) ) = \mathbf u ( t , \mathbf x ( t ) ) \cdot\nabla h ( t , \mathbf x ( t ) ) = 0 $$ so the quantity $h$ is constant along a flow line , as desired !
let 's start from the main question Do electrons move when only one end/pole of the battery is connected ?  they do ! let 's say that you attach a wire to the positive terminal of the battery and this terminal is at " conventional " +5v . imagine a lot of positively charged particles accumulated there , now electrons would move as near them as possible , creating the same +5v potential throughout the wire . a very similar effect takes place in the charging of a capacitor by dc source , in that case too the circuit is never completed but we know that charges flow because that is how the capacitor charges . i could only understand the other question as Does attaching conductors to battery or other sources of electrical energy ionize them?  it would be wrong to say that something like this happens , because when atoms get ionized they have quite a bit of freedom of movement for example $\text{na}^+$ and $\text{cl}^-$ in water are ionized and move freely similarly ions in hot plasma move about freely . while in conductors the concept of free electrons is accepted because electrons rather than completely leaving the atom , get into contact with several atoms and even though they leave a charged specie behind , that specie does not have freedom of movement and hence cannot be called ionized .
the voltage differences you can get from thermocouples are usually in the range of $\mu$v per kelvin for metals . in other materials this can be a bit higher but there is no material that can create thousands of volts for a temperature difference between the heating and other parts of the building . for static electricity you need a few conditions coming together : an object with a relatively high capacitance , e.g. your body , a method to create large potential differences , the triboelectric effect , and a good isolation so that the charges stay on your body . this can be a plastic floor or rubber shoe soles . wet air is usually a good protection against static electricity , as even on the surface of very good insulating materials ( teflon , etc . ) you will measure resistances of less than 10m$\omega$ for a path of a few cm .
any equations you set up will be uglier than the cross product ( they will basically by expansions of $\times$ ) remember , vectors only can define a direction . the issue with the magnetic field is that it is heavily related to planes , and is a perpendicular force . we can not assign a magnetic field analogous to the electric field ( as in : a field which shows the direction of force ) , as it depends upon the direction of velocity . the cross product enables us to define a plane with a vector perpendicular to it . aside from that , it gives perpendicular stuff . this clearly points to the use of the cross product . besides , the cross product is not that dirty . it makes stuff simpler , if anything . if you want to do it all via equations , use this : $$\vec{a}\times\vec{b}= \begin{vmatrix} \hat{i} and \hat{j} and \hat{k} \\ a_x and a_y and a_z \\ b_x and b_y and b_z \\ \end{vmatrix}$$ these rules may help you avoid the determinant form for simpler cross products : $$\hat{i}\times\hat{j}=\hat{k} ; \:\hat{j}\times\hat{k}=\hat{i} ; \:\hat{k}\times\hat{i}=\hat{j} ; \:\vec{a}\times\vec{a}=0 ; \:\vec{a}\times\vec{b}=-\vec{b}\times\vec{a}$$ over here , the right hand rule is implied by our choice of a right handed system of $\hat{i} , \hat{j} , \hat{k}$ or $x , y , z$ . so you do not need to touch it , it is already inherent in the coordinate system .
think about it physically . how much gas is in the tube ? there are three sources : the gas coming in at $x=0$ which was already in solution . the gas being carried away by the fluid at the other end , $x=l$ diffusion across the walls of the tube . these three terms are represented in the equation $$ \frac{d}{dt} \left ( a \int_{0}^{l} u ( x , t ) dt \right ) = v ( 0 ) au ( 0 , t ) - v ( l ) au ( l , t ) + p \int_{0}^{l} q ( x , t ) dx $$ the first term is the velocity of the liquid , $v$ , times the cross section , $a$ , giving the total volume of liquid coming in at $x=0$ . we multiply this by the concentration of gas in the liquid at $x=0$ to get the velocity that gas enters at $x=0$ . the second term goes in much the same way , but we have a negative sign because it is the gas which is leaving at the other end . you say you understand that the third term as diffusion through the walls , which is correct .
the phenomenon you describe is ferromagnetism not paramagnetism . ferromagnetic materials like iron behave as if they contain many tiny bar magnets ( called magnetic domains if you are interested to pursue this further ) , but because the magnet domains are aligned randomly the fields cancel out and there is no net magnetic field . however if you put a ferromagnetic material in a magnetic field the external field will cause partial alignment of the magnetic domains . this induces a magnetic field in the originally unmagnetised iron , and that is why your paper clip sticks to the ball . however if you remove the external magnetic field the domains will go back to their original alignment , the net magnetic field will go back to zero and the paper clip will fall off again . if you apply a very strong field and/or combine it with heating and cooling you can permanently change the alignment of the magnetic domains so they remain aligned when the external field is removed . this is how you make permanent magnets .
in short , no , not all fluorophores are dipoles in the permanent dipole sense . as a counterexample , anthracene has zero permanent dipole moment , but fluoresces blue under uv illumination . the reason why many fluorescent molecules are called " dipoles " is because electric dipole transitions between quantum states occur due to nonzero values of the transition dipole element between those states . so while anthracene 's ground state has zero permanent electric dipole , the transition dipole element between the ground state and the relevant excited state turns out to be nonzero .
nonperiodic signals can be described by their frequency spectrum . if you think of a sine , this signal has only one frequency . if you think of a signal that is the sum of three sine waves with different frequencies the signal has 3 frequencies . nonperiodic signals are viewed as the sum of an infinite sum of periodic signals . check out the fourier analysis . so , with nonperiodic frequencies you speak of dominant frequencies or dominant part of the spectrum .
if you are just working with $\hat p_{x}$ , you really only care about the integral over x , rather than the entire volume ( $d^{3}r=dxdydz$ ) . anyways , a hermitian operator is one such that $a^{\dagger}=a$ . this means that $\hat p^{\dagger}= ( \hat p^{*} ) '=\hat p$ where the prime indicates a transpose . a transpose in this case really means that the operator acts to the left . assuming the wavefunctions vanish on the integration boundary , you should be able to show that \begin{equation}\int dx \ , \psi^{*} ( x , t ) ( \hat p_{x} \psi ( x , t ) ) =\int dx \ , ( \psi^{*} ( x , t ) \hat p_{x}^{\dagger} ) \psi ( x , t ) \end{equation} which means that the momentum operator is hermitian . it may be instructive to work this out in 3d where $\hat p=-i\hbar \vec \nabla$ and the integral runs over the whole 3d volume .
the concept you are looking for is the attenuation length of light in water . this is the quantitative measure of how strongly different wavelengths get absorbed in water . the absorption of light is fairly simple to describe : it mostly depends on the material , but also if the light transverses a longer length of material , the attenuation will be stronger , and if there is more light to begin with , then more will be absorbed . in fact , the amount of light absorbed is proportional to the amount of light present , so that the decay will be exponential . to get rid of these factors and to get a constant which is purely a property of the material and not of how much of it there is or of the intensity of the light , we do two things : we measure absorption per unit length of material transversed , and we measure absorption as a percentage of the incident light . this is what the absorption length is : the length $l$ it takes for a given light intensity to decrease to 36.7% of its initial value . why this weird number ? since the decay of the intensity is exponential , it can be written as $$ i ( l ) =i ( 0 ) e^{-l/l} . $$ when $l=l$ , the intensity has decreased to $e^{-1}=0.367\ldots$ . an alternative , more useful quantity is the inverse of the attenuation length , $\kappa=1/l$ , which is called the attenuation coefficient of the medium , and this gets quoted somewhat more often as i understand it . here is a sample absorption spectrum of liquid water , which as you can see depends strongly on the wavelength : to find out the incident light spectrum at a given depth , you need to apply this attenuation factor to the initial spectrum , which is the one at sea level . to get the spectral irradiance ( solar power per unit area per unit wavelength ) at a wavelength $\lambda$ and at depth $l$ , you start with the spectral irradiance at the sea surface , $\sigma ( \lambda , 0 ) $ , and then you apply the beer-lambert law of exponential decay to it : $$ \sigma ( \lambda , l ) =\sigma ( \lambda , 0 ) \exp ( \kappa ( \lambda ) l ) ) . $$ this is the equation you were looking for . you might notice , though , that it does not get you that much closer to plotting a graph of this quantity , and that is because $\kappa ( \lambda ) $ is a complicated quantity with a complicated spectral dependence . to get this data , you can start e.g. here , though of course the amount of work you will need will depend on exactly how much detail you want in your final answer .
here 's a way \begin{align} x^0 ( \sigma^1 , \sigma^2 , \sigma^3 ) and = -\sigma^3 \\ x^1 ( \sigma^1 , \sigma^2 , \sigma^3 ) and = \sigma^1 \\ x^2 ( \sigma^1 , \sigma^2 , \sigma^3 ) and = \sigma^2 \\ x^3 ( \sigma^1 , \sigma^2 , \sigma^3 ) and = \sigma^3 \\ \end{align} where $ ( \sigma^1 , \sigma^2 , \sigma^3 ) \in\mathbb r^3$
the thermal radiation associated with some object is typically described in terms of the " black-body " spectrum for a given temperature , given by the planck formula . this formula is based on an idealization of an object that absorbs all frequencies of radiation equally , but it works fairly well provided that the object whose thermal spectrum you are interested in studying does not have any transitions with resonant frequencies in the range of interest . as the typical energy scale of atomic and molecular transitions is somewhere around an ev , while the characteristic energy scale for " room temperature " is in the neighborhood of 1/40 ev , this generally is not all that bad an assumption-- if you look in the vicinity of the peak of the blackbody spectrum for an object at room temperature , you generally find that the spectrum looks very much like a black-body spectrum . how does this arise from the interaction between light of whatever frequency and a gas of atoms or molecules having discrete internal states ? the thing to remember is that internal states of atoms and molecules are not the only degree of freedom available to the systems-- there is also the center-of-mass motion of the atoms themselves , or the collective motion of groups of atoms . the central idea involved with thermal radiation is that if you take a gas of atoms and confine it to a region of space containing some radiation field with some characteristic temperature , the atoms and the radiation will eventually come to some equilibrium in which the kinetic energy distribution of the atoms and the frequency spectrum of the radiation will have the same characteristic temperature . ( the internal state distribution of the atoms will also have the same temperature , but if you are talking about room-temperature systems , there is too little thermal energy to make much difference in the thermal state distribution , so we will ignore that . ) this will come about through interactions between the atoms and the light , and most of these interactions will be non-resonant in nature . in terms of microscopic quantum processes , you would think of these as being raman scattering events , where some of the photon energy goes into changing the motional state of the atom-- if you have cold atoms and hot photons , you will get more scattering events that increase the atom 's kinetic energy than ones that decrease it , so the average atomic ke will increase , and the average photon energy will decrease . ( or , in more fully quantum terms , the population of atoms will be moved up to higher-energy quantum states within the box , while the population of higher-energy photon modes will decrease . ) for thermal radiation in the room temperature regime , of course , the transitions in question are so far off-resonance that a raman scattering for any individual atom with any particular photon will be phenomenally unlikely . atoms are plentiful , though , and photons are even cheaper , so the total number of interactions for the sample as a whole can be quite large , and can bring both the atomic gas and the thermal radiation bath to equilibrium in time . i have never seen a full qft treatment of the subject , but that does not mean much . the basic idea of the equilibration of atoms with thermal radiation comes from einstein in 1917 , and there was a really good physics today article ( pdf ) by dan kleppner a few years back , talking about just how much is in those papers .
what you see depicted in sf movies is nonsense : http://imagine.gsfc.nasa.gov/docs/ask_astro/answers/970603.html
in ideal circuit theory , kvl holds period . consider the series rlc circuit driven by an arbitrary voltage source $v_s$ . the canonical differential equation for the series current $i ( t ) $ is : $$\dfrac{d^2i}{dt^2} + \dfrac{r}{l}\dfrac{di}{dt} + \dfrac{1}{lc}i = \dfrac{1}{l}\dfrac{dv_s}{dt}$$ where does this equation come from ? it comes from writing the kvl equation around the loop in terms of the series current $i$: $$v_s ( t ) = ri ( t ) + l\dfrac{di ( t ) }{dt} + \dfrac{1}{c}\int_{-\infty}^ti ( \tau ) d\tau$$ now , it is true that if the assumptions of ideal circuit theory do not hold , kvl does not hold . however , understand that ac circuit analysis is under the umbrella of ideal circuit theory thus , in that context , kcl holds for ac circuit analysis . in response to a comment : then say for example , the current in a circuit with resistance and a capacitance is $i$ . then $v_r=ir$ and $v_c=i/ωc$ . but the supply voltage $v=\sqrt{v^2_r+v^2_c}$ and not $v=v_r+v_c$ . does this not violate kvl ? first , the phasor voltage across the capacitor is $\vec v_c = \dfrac{1}{j \omega c}\vec i$ . phasors are complex numbers . the sum of magnitudes is generally not equal to the magnitude of the sum : $$|z_1| + |z_2| \ne |z_1 + z_2|$$ thus , the sum of resistor and capacitor phasor voltage magnitudes is not meaningful but the sum of the resistor and capacitor phasor voltages is . let $v_1 ( t ) = v_1 \sin \omega t$ and $v_2 ( t ) = v_2 \cos \omega t$ be the time domain voltages across two series circuit elements . by kvl , the voltage across the series combination is $$v_s ( t ) = v_1 ( t ) + v_2 ( t ) = v_1\sin \omega t + v_2 \cos \omega t = \sqrt{v^2_1 + v^2_2}\cos ( \omega t - \phi ) $$ where $$\tan\phi = \frac{v_1}{v_2}$$ note that , using phasors , the above is $$\vec v_s = \vec v_1 + \vec v_1 = -jv_1 + v_2 = e^{-j \phi}\sqrt{v^2_1 + v^2_2}$$ thus , the sum of the phasor magnitudes , $v_1 + v_2$ , is not meaningful and certainly is not an application of kvl .
light cannot move outwards inside the event horizon . i would guess you are thinking that an outgoing light ray might leave you in the outgoing direction , then slow to a halt and return - hence you would see yourself . however this does not happen . the light leaving you moves inwards not outwards , but since you fall inwards faster than the light does , the light still leaves you ( at velocity $c$ ) and never returns . this is discussed in some detail in the question if you shoot a light beam behind the event horizon of a black hole , what happens to the light ? . to show what happens to you and the light we draw a spacetime diagram . we’ll assume all motion is radial , so the diagram will just show distance from the singularity and time . the trajectory of any object in spacetime is a curve on the diagram called a worldline , and when two objects meet their worldlines intersect . so to show the black hole cannot act as a mirror we draw your worldline and the worldline of the light and show that they only intersect once . the problem is that we can’t use the usual coordinates $r$ and $t$ because these are singular at the event horizon . instead we use kruskal-szekeres coordinates $u$ and $v$ . i’m not going to go into how these coordinates are defined , see the wikipedia article for details , because we’d be here all day . the $u$ coordinate is spacelike both outside and inside the event horizon , and likewise the $v$ coordinate is timelike both outside and inside the event horizon . using these coordinates the spacetime diagram of the black hole looks like this : on this diagram the diagonal dashed lines are the event horizon , and the red hyperbola at the top is the world line of the singularity . the blue curve is your worldline as you fall into the black hole . we’re only interested in the top right half of the diagram – the bottom half shows a white hole and a parallel universe linked by a wormhole ( ! ) but that’s a discussion for another day . for our purposes the key feature of this diagram is that light rays follow straight lines with gradient $\pm 1$ . ingoing light rays travel from lower right to upper left ( gradient $-1$ ) while outgoing light rays travel from lower right to upper left ( gradient +1 ) . the worldlines of massive objects have a gradient closer to the $v$ axis than light rays , and the faster the object is travelling the closer its worldline gets to a gradient of $\pm 1$ . now we’re in a position to answer rijul’s question , but let’s zoom into the top right bit of the diagram so we can see what happens : at some point after you’ve crossed the event horizon you shine two light rays , one inwards and one outwards , and these are shown by the magenta lines . remember that light rays always travel at 45° on this diagram , so it’s easy to draw the worldlines of the light rays because they are just straight lines . your worldline is approximate in the sense that i didn’t sit down and calculate it , but it must everywhere be at an angle greater than 45° , and as you accelerate the gradient approaches 45° . so your worldline will look something like the blue line i’ve drawn , and in any case the exact shape of your worldline doesn’t matter for this proof . and with that we’re done ! the briefest glance at the diagram shows that your worldline and the worldlines of the light rays can only intersect at one point , i.e. the point you shine the light rays inwards and outwards . so the black hole can’t act as a mirror . note also that both light rays end up intersecting the worldline of the singularity so even the light ray directed outwards ends up falling into the singularity .
it is largely a matter of definition . here are some quotes . from springer reference : definition there is no universally recognized definition of chaotic flows . flows with properties that are neither constant in time nor presenting any regular periodicity are normally referred as chaotic . fluid turbulence is generally found to be chaotic . it is also random , dissipative , and multiple scaled in time and space . it is a complex system of infinite degrees of freedom . a paper full of state diagrams ( but rather heavy on the math ) for transitions to chaotic flow : https://tspace.library.utoronto.ca/bitstream/1807/25484/1/transition%20to%20chaos%20in%20converging-diverging%20channel%20flows.pdf and finally , a nice discussion at quora . com ( registration apparently required ) as piyush grover pointed out , a chaotic flow has to have mixing by definition . the following answer assumes ' chaotic ' to mean ' random ' which is technically incorrect . but i have decided to leave it here anyway , because i think that is what the op meant . however , i still think that you will not observe a k^ ( -5/3 ) spectrum in the case of chaotic advection . i am also adding piyush 's comment as a part of this answer . chaotic but non-turbulent flows can have exponential mixing . there is a whole field of chaotic advection based on this fact . in fact , you can have exponential mixing of mass in stokes flow , which is as far away from turbulence as possible . this is often used to mix fluid efficiently at micro devices ( low re ) , where turbulence is simply not feasible due to energy considerations . a chaotic flow is one in which there seems to be a high irregularity in the behavior of one/all flow variables with time/space . while a turbulent flow certainly exhibits this behavior , there are also other properties that should be present for a flow to be called turbulent , one of which is high levels of mixing , i.e. , mass/momentum/heat transfer . this is a distinct ( and perhaps the most useful ) property of turbulent flows which is frequently exploited . when you try to mix the sugar in a cup of coffee by stirring it , you are essentially making use of this property . this effect can be clearly seen by looking at the velocity profiles of laminar and turbulent flows through a pipe . ( taken from page on flowcontrolnetwork ) the lines show the magnitude of horizontal/streamwise velocities with respect to height along a pipe . you can see that the turbulent flow has a much flatter velocity profile than a laminar flow , i.e. , there are higher velocities close to the wall for a turbulent flow compared to a laminar flow , while there are lower velocities close to the centerline for a turbulent flow compared to a laminar flow . this shows that velocity ( momentum for an incompressible/constant density flow ) is transferred to a greater extent from the fluid elements close to the centerline to the fluid elements close to the walls in case of a turbulent flow . a good example of a flow which is chaotic but is not turbulent is the trail behind an aircraft . though the flow inside the jet trail is highly chaotic , it is not turbulent because it maintains the shape ( diameter ) for very large distances behind the aircraft , which means that there is very low/negligible mixing with the surrounding atmosphere .
at high enough pressure you can keep water as a liquid above 100°c . with even more pressure you can even keep ice above 100°c . similarly you can boil water at room temperature with a low pressure . ( https://en.wikipedia.org/wiki/phase_diagram ) the phase diagram of water shows what state it is in at any given temperature and pressure . edit : to answer phil 's question . the very steep vertical line between the blue ( solid ) and liquid ( green ) at 0°c shows why pressure is not the reason ice skates have a film of water to slide on . you would need to increase the pressure to a few kbar to get liquid at even slightly below freezing . see how far vertically you would need to go to hit the green area starting to the left of the vertical line at 0°c it is actually the friction between the blade and the ice that creates heat which melts the surface .
a magnetic field exerts a force on a moving charge . given a magnetic field , $\vec{b}$ , and a charge , $q$ , moving with velocity , $\vec{v}$ , the magnetic force , $\vec{f}$ , on the charge is:$$\vec{f}= q ( \vec{v} \times \vec{b} ) $$ the directions of these with respect to one another can be found using the right hand rule . see the picture below . the magnetic force is perpendicular to the direction of magnetic field because $\vec{v}$ and $\vec{b}$ are in cross product . give this a read .
$\newcommand{\ket} [ 1 ] {\left|#1\right&gt ; }\newcommand{\bra} [ 1 ] {\left&lt ; #1\right|}$ you actually got the notation slightly wrong and had a typo in $i$ , but you are almost right . the usual convention are $$ \begin{align} \ket0 and =\begin{bmatrix}1 \\ 0\end{bmatrix} and \ket1 and =\begin{bmatrix}0 \\ 1\end{bmatrix}\\ \text{if }\ket\psi and =\begin{bmatrix}\alpha \\ \beta\end{bmatrix} and \text{then }\bra\psi and =\begin{bmatrix}\alpha^* and \beta^*\end{bmatrix} \end{align} $$ so we have $$ \begin{align} i and =\ket0\bra0+\ket1\bra1\\ \sigma_x and =\ket1\bra0+\ket0\bra1\\ \sigma_y and =i\ket1\bra0-i\ket0\bra1\\ \sigma_z and =\ket0\bra0-\ket1\bra1 \end{align} $$
use cgs units : grams and cm , as you asked .
yes , what you have formulated is fine . the pressure acting on the water from the bottom of the straw will be equal to the weight of the water times the cross section area . so $101325\pi r^2$ is the force acting from below also . that is precisely why there is an equilibrium and the water is not falling .
it is not quite correct to say that n-type materials are doped with " extra " electrons . to be sure , n-type material is charge neutral . it is more correct to say that n-type material is doped with atoms that " donate " an electron to the conduction band ; n-type material has excess mobile electrons versus intrinsic material . so , naturally , n-type material is a good conductor because there are plenty of mobile electrons to participate in an electron current . p-material is doped with atoms that " accept " an electron from the conduction band so there is a deficit of mobile electrons and thus you would not expect there to be much mobile electron current . however , there are plenty of " holes " that can participate in a " hole " current . now , with respect to your question . when an ehp is generated in either type of material , the material remains charge neutral . however , if the ehp is separated by the intrinsic e-field of the pn junction , there is no longer charge neutrality . when an ehp is generated , it is the minority carrier that will be swept across the junction to become a majority carrier on the " other side " . since there is no longer charge neutrality , charge will flow in an external circuit to restore it .
[ another comment to answer transplant ] it seems like you are asking about a classical analog to the superselection sectors of quantum mechanics . one situation where this occurs in classical mechanics is when considering particle motion of a manifold with non-trivial topology - i.e. with holes and handles . in such cases there can be more than one extremal path from a to b . an example is an arcade pin-ball machine , if you are familiar with those . also , when you talk about optics it is important to keep in mind that there are two different regimes , those of wave optics and geometric optics . in the second case one has well-defined " trajectories " and you can find extremal trajectories . not so in the first case .
the majority opinion is that einstein was wrong . however , i see some problems with the standard quantum mechanics ( sqm ) approach that you outlined . sqm contains two major parts : unitary evolution ( described , e.g. , by the dirac equation ) and the measurement theory ( e . g . , collapse , or the projection postulate , which , loosely speaking , states that , after measurement of some observable , the system stays in an eigenstate of that observable with the relevant eigenvalue ) . both of these parts are used in your reasoning : on the one hand , unitary evolution ensures that the particles have zero total spin all the time , and the projection postulate ensures that , if particle a was measured to have spin up , it is in the relevant eigenstate . so far so good . the problem is that the two parts of sqm are mutually contradictory ( the notorious measurement problem in quantum mechanics - http://plato.stanford.edu/entries/qt-measurement/ ) . for example , unitary evolution cannot introduce irreversibility or turn a superposition of two states into their mixture , whereas the projection postulate does just that . so if you include the instrument ( and the observer , if you wish ) into the system containing particle a and then run unitary evolution to describe the measurement process , you will get a superposition of states with different spin projections and , strictly speaking , will not get a definite outcome . actually , your reasoning is close to that used in the proof of the bell theorem to prove that the bell inequalities can be violated in quantum mechanics . however , such proof contains mutually contradictory assumptions . on the other hand , there have been no loophole-free experiments demonstrating violations of the bell inequalities , so there are both theoretical and experimental difficulties with your reasoning , although it is widely used . therefore , i tend to think that your question has not been resolved yet , as its resolution may demand either resolution of the measurement problem in quantum mechanics or loophole-free bell experiments .
the second paragraph of this paper by kouwenhoven et al . has a great summary : practically all o-type stars ( mason et al . 1998 ) and b/a-type stars ( shatsky and tokovinin 2002 ; kobulnicky and fryer 2007 ; kouwenhoven et al . 2007b ) are found in binary or multiple systems . abt and levy ( 1976 ) report a multiplicity fraction of 55% among f3−g2 stars , and in their coravel spectroscopic study of f7−g9 stars , duquennoy and mayor ( 1991 ) find a binary fraction of ∼60% . the binary fraction among m-type stars is 30−40% ( fischer and marcy 1992 ; leinert et al . 1997 ; reid and gizis 1997 ) . for late m-type stars and brown dwarfs the binary fraction decreases to 10−30% ( e . g . , gizis et al . 2003 ; close et al . 2003 ; bouy et al . 2003 ; burgasser et al . 2003 ; siegler et al . 2005 ; ahmic et al . 2007 ; maxted et al . 2008 ; joergens 2008 ) . ( note those references can be found easily on the sao/nasa ads here . ) the rest of the paper describes some of the subtleties at work when astronomers combine modeling with observational data .
every term contains one $\lambda$ in the superscript and one in the subscript , so you sum over those . the only indices which do not appear in both superscript and subscript in the same term are $\mu$ and $\nu$ . example : $$\gamma_{\lambda\sigma}^\lambda\gamma_{\mu\nu}^\sigma = \gamma_{00}^0\gamma_{\mu\nu}^0 + \gamma_{01}^0\gamma_{\mu\nu}^1 + \cdots + \gamma_{10}^1\gamma_{\mu\nu}^0 + \cdots$$
i would say that you have several regimes that are well defined : the behavior of the fluid as it exits in inlet jets and enters the bulk without interference from the cubes . [ length scale set by the exit aperture ? ] flow of the fluid around isolated cubes when far from the edges of the tank ( far being several times the characteristic size of the cube ) . [ length scale set by the side of the cube . ] flow of the fluid toward , along and away from the sides of the tank away from the jets and without interference from the cubes . [ length scale set by the boundary behavior ? ] which is the good news , unfortunately you also have all the cases that mix and match the various length scales : case with cubes interacting with the jet near the aperture case with cubes in motion near the walls case with cubes in close proximity to one another you can probably find existing treatments for all the former cases , but the latter ones are going to be tricky , and you will note that they feature at least two length scales . yuck . this must be part of why they say cfd is hard .
there is no such limit . note that hitting something is a bad example for explaining about force : the impact is very short , so you get a short burst of strong force , but it is hard to say how much exactly . in an idealised sense , the force is in fact infinite for any such hit – but only for an infinitely short time ! a car with a piece of paper in front of it is better . here , the force is indeed limited , but not by anything fundamental but just by the car 's ability to accelerate . if the car was able to accelerate arbitrarily rapid , the the force could indeed become as big as you want – however , since the car has so much more mass than the paper , the internal forces in the car would always be greater by that factor as well , so any noteworthy-sounding force on the paper would mean you had rip the car itself apart .
in general , invariance of the action under some transformations , such as lorenz rotations , is an extra condition that we impose on a theory . this is not related to the extreme condition that just gives trajectories along which the classical evolution goes . in your case , i guess , there is some confusion in formulation of the idea . according to the 1st postulate of special relativity , all physical laws are the same in any inertial frame ( related by a lorenz transformation ) . to realise this theoretically , we need to make our lagrangian invariant under such transformations . then it will give the same physics in all frames . hence , one should impose some extra condition to satisfy the 1st postulate .
i do not have an answer to the question " why would one want to consider such crazy stuff in physics ? " since i do not know much physics , but as a mathematics student i do have an answer to the question " why would one want to consider such crazy stuff in mathematics ? " what physicists call grassmann numbers are what mathematicians call elements of the exterior algebra $\lambda ( v ) $ over a vector space $v$ . the exterior algebra naturally arises as the solution to the following geometric problem . say that $v$ has dimension $n$ and let $v_1 , . . . v_n$ be a basis of it . we would like a nice natural definition of the $n$-dimensional volume of the paralleletope defined by the vectors $\epsilon_1 v_1 + . . . + \epsilon_n v_n , e_i \in \{ 0 , 1 \}$ . when $n = 2$ this is the standard parallelogram defined by two linearly independent vectors , and when $n = 3$ this is the standard paralellepiped defined by three linearly independent vectors . the thing about the naive definition of volume is that it is very close to having really nice mathematical properties : it is almost multilinear . that is , if we denote the volume we are looking at by $\text{vol} ( v_1 , . . . v_n ) $ , then it is almost true that $\text{vol} ( v_1 , . . . v_i + cw , . . . v_n ) = \text{vol} ( v_1 , . . . v_n ) + c \text{vol} ( v_1 , . . . v_{i-1} , w , v_{i+1} , . . . v_n ) $ . you can draw nice diagrams to see this readily . however , it is not actually completely multilinear : depending on how you vary $w$ you will find that sometimes the volume shrinks to zero and then goes back up in a non-smooth way when really it ought to keep getting more negative . ( you can see this even in two dimensions , by varying one of the vectors until it goes past the other . ) to fix that , we need to look instead at oriented volume , which can be negative , but which has the enormous advantage of being completely multilinear and smooth . the other major property it satisfies is that if any of the two vectors $v_i$ agree ( that is , the vectors are linearly dependent ) then the oriented volume is zero , which makes sense . it turns out ( and this is a nice exercise ) that this is equivalent to oriented volume coming from a " product " operation , the exterior product , which is anticommutative . formally , these two conditions define an element of the top exterior power $\lambda^n ( v ) $ defined by the exterior product $v_1 \wedge v_2 . . . \wedge v_n$ , and choosing an element of this top exterior power ( a volume form ) allows us to associate an actual number to an $n$-tuple of vectors which we can call its oriented volume in the more naive sense . if $v$ is equipped with an inner product , then there are two distinguished elements of $\lambda^n ( v ) $ given by a wedge product of an orthonormal basis in some order , and it is natural to pick one of these as a volume form . alright , so what about the rest of the exterior powers $\lambda^p ( v ) $ that make up the exterior algebra ? the point of these is that if $v_1 , . . . v_p , p &lt ; n$ is a tuple of vectors in $v$ , we can consider the subspace they span and talk about the $p$-dimensional oriented volume of the paralleletope given by the $v_i$ in this subspace . but the result of this computation should not just be a number : we need a way to do this that keeps track of what subspace we are in . it turns out that mathematically the most natural way to do this is to keep in mind the requirements we really want out of this computation ( multilinearity and the fact that if the $v_i$ are not linearly independent then the answer should be zero ) , and then just define the result of the computation to be the universal thing that we get by imposing these requirements and nothing else , and this is nothing more than the exterior power $\lambda^p ( v ) $ . this discussion hopefully motivated for you why the exterior algebra is a natural object from the perspective of geometry . since einstein , physicists have been aware that geometry has a lot to say about physics , so hopefully the concept makes a little more sense now . let me also say something about how modern mathematicians think about " space " in the abstract sense . the inspiration for the modern point of view actually derives at least partially from physics : the only thing you can really know about a space are observables defined on it . in classical physics , observables form a commutative ring , so one might say roughly speaking that the study of commutative rings is the study of " classical spaces . " in mathematics this study , in the abstract , is called algebraic geometry . it is a very sophisticated theory that encompasses classical algebraic geometry , arithmetic geometry , and much more , and it is in large part because of the success of this theory and related commutative ring approaches to geometry ( topological spaces , manifolds , measure spaces ) that mathematicians have gotten used to the slogan that " commutative rings are rings of observables on some space . " of course , quantum mechanics tells us that the actual universe around us does not work this way . the observables we care about do not commute , and this is a big issue . so mathematically what is needed is a way to think about noncommutative rings as " quantum spaces " in some sense . this subject is very broad , but roughly it goes by the name of noncommutative geometry . the idea is simple : if we want to take quantum mechanics completely seriously , our spaces should not have " points " at all because points are classical phenomena that implicitly require a commutative ring of observables , which we know is not what we actually have . so our spaces should be more complicated things coming from noncommutative rings in some way . grassmann numbers satisfy one of the most tractable forms of noncommutativity ( actually they are commutative if one alters the definition of " commutative " very slightly , but never mind that . . . ) , and even better it is a form of noncommutativity that is clearly related to something physicists care about ( the properties of fermions ) , so anticommuting observables are a natural step up from commuting observables in order to get our mathematics to align more closely with reality while still being able to think in an approximately classical way .
first and foremost , the bec systems studied in detail today do not involve the formation of any bonds between atoms . bose-einstein condensation is a quantum statistical phenomenon , and would happen even with noninteracting particles ( though as a technical matter , that is impossible to arrange , but you can make a condensate and then manipulate the interactions so they are effectively non-interacting , and the particles remain a condensate ) . the " high school physics " version of what happens at the bec transition is this : particles with integer intrinsic spin angular momentum are " bosons , " and many of them can occupy the same energy state . this is in contrast to particles with half-integer spin , such as electrons , termed " fermions , " which are unable to be in exactly the same quantum state ( this feature of electrons accounts for all of chemistry , so it is a good thing ) . when we talk about a confined gas of atoms , quantum mechanics tells us that we must describe it in terms of discrete energy states , spaced by a characteristic energy depending on the details of the confinement . because of this , the two classes of particles have very different behaviors in large numbers . the lowest-energy state for a gas of fermions is determined by the number of particles in the gas-- each additional particle fills up whatever energy state it ends up in , so the last particle added goes in at a much higher energy than the first particle added . for this reason , the electrons inside a piece of metal have energies comparable to the hot gas in the sun , because there are so many of them that the last electron in ends up moving very rapidly indeed . the lowest-energy state for a gas of bosons , on the other hand , is just the lowest-energy state available to them in whatever system is confining them . all of the bosons in the gas can happily pile into a single quantum state , leaving you with a very low energy . it turns out that , as you cool a gas of bosons , you will eventually reach a point where the gas suddenly " condenses " into a state with nearly all of the particles occupying a single state , generally the lowest-energy available state . this happens with material particles because the wave-like character of the bosons becomes more and more pronounced as you lower the temperature . the wavelength associated with them , which at room temperature is many times smaller than the radius of the electron orbits eventually becomes comparable to the spacing between particles in the gas . when this happens , the waves associated with the different particles start to overlap , and at some point , the system " realizes " that the lowest-energy state would be for all the particles to occupy a single energy level , triggering the abrupt transition to a bec . this transition is a purely quantum effect , though , and has nothing to do with chemical bonding . in fact , strictly speaking , the dilute alkali metal vapors that are the workhorse system for most bec experiments are actually a metastable state-- at the temperatures of these vapors , a denser gas would be a solid . they form a bec , though , because the density of these gases is something like a million times less than the density of air . the atoms are too dilute to solidify , but dense enough to sense each others ' presence and move into the same energy state . the underlying physics is described in detail in most statistical mechanics texts , though it is often dealt with very briefly and in an abstract way . there are decent and readable descriptions of the underlying physics in the new physics for the twenty-first century edited by gordon fraser , particularly the pieces by bill phillips and chris foot , and subir sachdev .
the weak force " looks " different because in the first ( and still most important ) reincarnation we have encountered it - namely beta-decay ( including the decay of the neutron ) - the force seems to be a contact interaction : it has an extremely short range , essentially zero . however , any phenomenon that differs from the indefinite existence of an object that moves in the same direction by the same speed forever requires a force to be explained . the force required for the beta-decay is the weak nuclear force . while the decay seems to " directly " transform a neutron into a proton , electron , and antineutrion , a closer investigation of the force that began in the 1960s has demonstrated that this force is actually analogous to other forces , including electromagnetism , because its range is finite ( nonzero ) . it is only limited because it is mediated by the w and z bosons which are , unlike photons , massive . so the force does not get " too far " . however , in our modern description of the forces , electromagnetism and the weak force have to be described by a unified " electroweak " theory and they mix with one another . at distances much shorter than the range of the w/z bosons , the electromagnetic and weak forces become equally strong and , in some proper sense , indistinguishable .
the key output of the flrw metric is the scale factor $a ( t ) $ as a function of time . from this we can calculate the time derivative $\dot{a} ( t ) $ ( which is what the red shift measures ) then check whether or not it satisfies the equation : $$ \left ( \frac{\dot{a}}{a}\right ) ^2 = \frac{8\pi g}{3} ( \rho_{radiation} + \rho_{matter} + etc ) $$ where the etc includes dark energy and anything else you may wish to throw in . so basically the test is to measure the redshift as a function of distance . the problem is that this is extraordinarily hard to do on the scales where the matter distribution is homogeneous . there are various approaches being tried such as baryon acoustic oscillations and properties of galaxy clusters but it is still early days .
in this question i showed that the power received from a light source moving away from the receiver at speed $\beta=v/c$ is $$ p = \frac{p_0}{\gamma^2 ( 1+\beta ) ^2} = p_0 \frac{1-\beta}{1+\beta} $$ taking into account redshift , time dilation , and the effect of the changing travel distance for the photons . your question is essentially the same , except that you have the source and receiver approaching ; in that case the sign of $\beta$ changes and you get a small increase in the received power .
honestly , i think this is one of those cases where you should just accept it and push on . this ' derivation ' is really nothing more than a pedagogical device to make field theory seem somewhat natural to students with a background in classical mechanics . what we are trying to do is to take the continuum i.e. $n\to \infty$ limit of the following lagrangian : $$l_n=\frac{1}{2} \biggl ( \sum_{i=1}^n\delta x \frac{m}{\delta x} \dot{\phi_i}^2-\sum_{i=1}^{n-1}\delta x\ k\delta x \biggl [ \frac{\phi_{i+1}-\phi_i}{\delta x}\biggr ] ^2\biggr ) $$ define $\mu=\frac{m}{\delta x}$ and $y=k\delta x$ clearly , for a continuum limit , we get infinitely many particles , so the total kinetic energy of the system should diverge . . . unless we impose ( or put in by hand , as they call it ) , that $\mu$ remains constant , not $m$ . similarly , it is obvious that the equilibrium force of each spring $f=k \delta x$ should vanish . . . unless we impose that $k\delta x$ is constant when we take our limit . with these ad-hoc assumptions , and replacing the discrete index $i$ with a continuous spatial coordinate , we get $$l\equiv \lim_{n\to \infty}l_n=\frac{1}{2}\int_0^l \mathrm{d}x \biggl ( \mu\dot \phi^2 -y ( \nabla\phi ) ^2\biggr ) $$ this gives us the right action for a free , massless , scalar field \begin{align*}s [ \phi ] and =-\frac{y}{2}\int_0^l \mathrm{d}x\ \mathrm{d}t \biggl ( -\frac{\mu}{y}\dot \phi^2+ ( \nabla \phi ) ^2\biggr ) \\ and =-\frac{\mu c^2}{2} \int_0^l \mathrm{d}x\ \mathrm{d}t \biggl ( -\frac{1}{c^2} ( \partial_t\phi ) ^2+ ( \nabla\phi ) ^2\biggr ) \hspace{2cm}c=\sqrt{\frac{y}{\mu}}\\ and =-\mu c^2\int_0^l\mathrm{d}^2x\ \frac{1}{2}\eta^{\mu\nu}\partial_\mu\phi\partial_\nu\phi\end{align*} the definition of $c$ is the standard one for the speed of longitudinal waves , and as one can see this lagrangian is also reminiscent of the action for a relativistic point particle ( especially the prefactor ) . this is , of course , a very nice result , so we can be happy about the way we took our limit , even if we had to make some ad-hoc assumptions .
first , note that this comes from the heisenberg uncertainty principle , $$ \delta x\delta p\geq \frac\hbar2 $$ where $\hbar\approx10^{-34}$ j$\cdot$s ( i.e. . , a very small number ) . this is a constraint on the simultaneous measurements of momentum and position . if you know the position of the coin , then it can not actually be anywhere else because it is measured to be there . next , i quote sean carroll : quantum mechanics features a " classical limit " in which objects behave just as they would had newton been right all along , and that limit includes all of our everyday experiences . for objects such as cats that are macroscopic in size , we never find them in superpositions of the form "75 percent here , 25 percent there" ; it is always "99.9999999 percent ( or much more ) here , 0.0000001 percent ( or much less ) there . " classical mechanics is an approximation to how the macroscopic world operates , but a very good one . the real world runs by the rules of quantum mechanics , but classical mechanics is more than good enough to get us through everyday life . it is only when we start to consider atoms and elementary particles that the full consequences of quantum mechanics simply can not be avoided . for your coin , we can describe it correctly by classical mechanics ( meaning we can measure its position and momentum simultaneously ) , so there is no need to invoke quantum mechanics in regards to this thought experiment .
it is because when $$v ( x , y , z ) = v_x ( x ) + v_y ( y ) + v_z ( z ) , $$ ( i guess that your extra identity $v ( x , y , z ) =v ( z ) $ is a mistake ) , we also have $$ h = h_x + h_y + h_z$$ because $h = ( \vec p ) ^2 / 2m + v ( x , y , z ) $ and $ ( \vec p ) ^2 = p_x^2+p_y^2+p_z^2$ decomposes to three pieces as well . one may also see that the terms such as $h_x\equiv p_x^2/2m+v_x ( x ) $ commute with each other , $$ [ h_x , h_y ] =0 $$ and similarly for the $xz$ and $yz$ pairs . that is because the commutators are only nonzero if we consider positions and momenta in the same direction ( $x$ , $y$ , or $z$ ) . at the end , we want to look for the eigenstates of the hamiltonian $$ h|\psi\rangle = e |\psi \rangle$$ and because we have $h = h_x+h_y+h_z$ , a hamiltonian composed of three commuting pieces , we may simultaneously diagonalize them i.e. look for the common eigenstates of $h_x , h_y , h_z$ , and therefore also $h$ . so given the separation condition for the potential , we may also assume $$ h_x |\psi\rangle = e_x |\psi\rangle $$ and similarly for the $y , z$ components . however , the equation above is just a 1-dimensional problem that implies that $|\psi\rangle$ must depend on $x$ as a one-dimensional quantum mechanical energy eigenstate wave function , $$ \psi ( x ) = c\cdot \psi_n ( x ) $$ which is an eigenstate of $h_x$ . this has to hold but the normalization factor is undetermined . we usually say that it is a constant but this statement only means that it is independent of $x$ . in reality , it may depend on all observables that are not $x$ such as $y , z$ . so a more accurate implication of the $h_x$ eigenstate equation is $$ \psi ( x , y , z ) = c_x ( y , z ) \cdot \psi_{n_x} ( x ) . $$ in a similar way , we may show that $$ \psi ( x , y , z ) = c_y ( x , z ) \cdot \psi_{n_y} ( y ) $$ and $$ \psi ( x , y , z ) = c_z ( x , y ) \cdot \psi_{n_z} ( z ) $$ and by combining these three formulae , we see that the whole function must factorize to a product of functions of $x$ and $y$ and $z$ separately . if you need a rigorous proof of the last simple step , take e.g. the complex logarithms of the three forms for $\psi$ above and compare e.g. the first pair : $$\ln\psi = \ln c_x ( y , z ) +\ln\psi_{n_x} ( x ) = \ln c_y ( x , z ) +\ln \psi_{n_y} ( y ) $$ take e.g. the partial derivative of the last equation with respect to $y$: $$ \frac{\partial \ln c_x ( y , z ) }{\partial y} = \frac{\partial \ln\psi_{n_y} ( y ) }{ \partial y }$$ the other two ( 1+1 ) terms are zero because they did not depend on $y$ . the right hand side above only depends on $y$ , so the same must be true for the left hand side . i am going to make a simple conclusion but to make it really transparent , let 's differentiate the latter equation over $z$ , too . the $\psi_{n_y}$ term disappears as well so we have $$\frac{\partial^2 \ln c_x ( y , z ) }{\partial y\ , \partial z} = 0$$ it means that $\ln c_x ( y , z ) $ must have the form $k_x ( y ) +l_x ( z ) $ , and $e^{k_x ( y ) }e^{l_x ( z ) }$ must be the remaining factors in the wave function . we say that the wave function in the product form is a " tensor product " of the three independent one-dimensional wave functions and more " operationally " , as another user mentioned , the method described above is the method of " separation of variables " .
i ) well , gaussian integrals $$\tag{1} \int_{\mathbb{r}^n} \ ! d^n x ~e^{-\frac{1}{2} x^t a x} ~=~ \sqrt{\frac{ ( 2\pi ) ^n}{\det a}}$$ are easy to calculate exactly , where the matrix ${\rm re} ( a ) $ is positive definite . ii ) but if op just wants to confirm that the power $p$ of the determinant $\det a$ on the rhs . of eq . ( 1 ) is $p=-1/2$ ( as opposed to some other power $p$ ) , then indeed one may use dimensional analysis . if the integration variables $x^i$ have dimension of length $ [ x^i ] =l$ , then the matrix elements $a_{ij}$ have dimension $ [ a_{ij} ] =l^{-2}$ to keep the argument of the exponential dimensionless . therefore $\det a$ has dimension $ [ \det a ] =l^{-2n}$ . moreover both sides of eq . ( 1 ) must have dimension $l^n$ . hence the power $p=-1/2$ of the determinant $\det ( a ) $ .
if you start with a finite amount of gas in the inner sphere and then deposit a massive amount of energy , the molecules of the gas begin moving rapidly outwards and piling up , creating the blast wave . however , the rate at which the gas is moving outwards may not be balanced by the amount of gas molecules being created by the explosive . if this is the case , then the pressure must decrease below ambient as the molecules are pushed outwards with the blast wave . you can see this in videos of blast waves . the initial wave continues to move outwards , but the smoke/dirt/debris caused by the explosive will move outwards initially , then inwards as the lower pressure region sucks it back in towards the center . there is actually considerably banging that goes on where the low pressure behind the blast wave moves inwards and outwards until it relaxes back to atmospheric pressure . here is a great video that shows the blast and resulting banging as the pressure relaxes .
let $y_1 , y_2$ $2$ complex vectors and let $&lt ; , &gt ; $ be a complex inner product defined by $&lt ; y_1 , y_2&gt ; = \vec y_1^* . \vec y_2$ . let $\vec a$ and $\vec b$ the real and imaginary part of $\vec y$ : $\vec y = \vec a + i \vec b$ then : $$&lt ; y_1 , y_2&gt ; = ( \vec a_1 . \vec a_2 + \vec b_1 . \vec b_2 ) + i ( \vec a_1 . \vec b_2 - \vec b_1 . \vec a_2 ) = u ( y_1 , y_2 ) + iv ( y_1 , y_2 ) $$ the cauchy-schwartz inequality gives : $$&lt ; y_1 , y_1&gt ; &lt ; y_2 , y_2&gt ; ~~\ge ~~|&lt ; y_1 , y_2&gt ; |^2$$ we note that : $&lt ; y_1 , y_1&gt ; = u ( y_1 , y_1 ) $ , so we have : $$u ( y_1 , y_1 ) ~~\ge ~~ \frac{u^2 ( y_1 , y_2 ) + v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }$$ now , fixing a particular $y_1$ , we limit the set of $y_2$ to those which respect $u ( y_1 , y_2 ) =0$ . so , we have now : $$u ( y_1 , y_1 ) ~~\ge ~~ \frac{ v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }~~ ~~ ~~ ~~ ~~ ( 1 ) $$ now , take explicitely $y_2$ defined by $ \vec a_2 = - \vec b_1 , \vec b_2 =\vec a_1 , $ , we see that $\vec a_1 . \vec a_2 + \vec b_1 . \vec b_2 = 0$ , that is $u ( y_1 , y_2 ) = 0$ , so this choice is coherent with our previous hyphothesis . morevoer , we have $v ( y_1 , y_2 ) = \vec a_1^2 + \vec b_1^2 $ , and $u ( y_2 , y_2 ) = \vec a_1^2 + \vec b_1^2 $ , so we have , for this particular $y_2$ . $$u ( y_1 , y_1 ) ~~= ~~ \frac{ v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }~~ ~~ ~~ ~~ ~~ ( 1 ) $$ so , we see , that the inequality $ ( 1 ) $ is effectively saturated by our choice of this particular $y_2$
if you have a smart phone with a flash , you probably have a strobe feature at your disposal . you can put a drinking straw into it and record the sound it makes , and measure the frequency in sound-editing software . imagine the fun explaining this to paramedics . similarly , a laser-pointer , a mirror , and a photo-diode cleverly hooked up to your computer 's sound card can be coaxed into doing science . finally , a laser pointer , two mirrors and knowledge of the speed of light in your room can be used to to determine the rotational speed of the fan , provided your room is big enough , and the fan is moving fast enough . if this is going to work , it will probably be the least accurate but most fun .
some students learning physics for the first time mistakenly think that objects that are accelerating have force . force is not a property possessed by an object , but rather something you do to an object that results in the object accelerating ( changing its speed ) , given by the equation f = ma . that is , forces cause acceleration , not the other way around . this means that if you observe an object accelerating , then it implies a force is acting on the object to cause such an acceleration . in this case , as the object strikes the hand , your hand applies a force to the ball causing it to slow down ( decelerate ) , and the ball applies an equal and opposite force to your hand causing it to accelerate ever so slightly ( newton 's third law ) , which is detected by your sensory neurons .
the proof is most easiest if we use the vector notation . we have $$\vec \tau = \int {d\vec \tau } = \int { ( \vec r \times dm\vec g ) } = \left ( {\int {\vec r dm} } \right ) \times \vec g$$ where i have used the assumption that near the earth $\vec g$ is constant . now according to the definition of center of mass we have , $${{\vec r}_{cm}} = \frac{1}{m}\int {\vec rdm}$$ therefore , we find $$\vec \tau = m{{\vec r}_{cm}} \times \vec g = {{\vec r}_{cm}} \times m\vec g$$ which is the desired result .
you are absolutely right that in principle each scattered photon undergoes a change in energy due to the recoil of the object it scatters off . in rayleigh scattering , that change is very small , though , and can generally be neglected . in general , when the energy of the photon is much smaller than the rest energy of the massive particle , the photon undergoes only a small change in energy . here 's a qualitative argument for why this is true . let $p$ be the momentum of the incoming photon . the momentum transferred to the massive particle is at most of order $p$ ( specifically , it is at most $2p$ ) . for a nonrelativistic particle , the kinetic energy is $p^2/ ( 2m ) $ , so the energy transferred is of this order . so the fractional energy lost by the photon is of order $$ {\delta e\over e}\sim{p^2/2m\over pc}={pc\over 2mc^2}={e\over 2mc^2} , $$ where $e =pc$ is the original photon energy . so when $e\ll mc^2$ , the change in energy is small in comparison to the original energy . one common way to express this more quantitatively is via the compton scattering formula , $$ \delta\lambda={h\over mc} ( 1-\cos\theta ) , $$ where $m$ is the mass of the massive particle and $\theta$ is the angle through which the photon scattered . this formula is usually used for much higher-energy photons scattering off charged particles , but it is derived just from energy and momentum conservation , so it applies to any situation in which a photon scatters off a massive particle initially at rest . if i have done the arithmetic right , when $m$ is the mass of an atom or molecule , $\delta\lambda$ comes out to be of order $10^{-16}$ m , which is certainly small enough to be ignored for visible light . to answer your last question , even though atoms and molecules are neutral , they are made of charged parts . the photon can and does interact with those parts .
you have the right idea . let 's use index notation for convenience . we have \begin{align} p^i ( t ) = \int_{\mathbb r^3} d^3x \ , x^i\rho ( t , \mathbf x ) \end{align} and as you note , we have the continuity equation ; \begin{align} \partial_jj^j = -\frac{\partial\rho}{\partial t} \end{align} so we get \begin{align} \dot p^i = -\int_{\mathbb r^3} d^3 x \ , x^i\partial_j j^j \end{align} where i am using the summation convention for the dot product . this is the point to which you have basically gotten . the trick is now to essentialy perform an integration by parts . in other words , we use the product rule for differentiation to note that \begin{align} \partial_j ( x^i j^j ) and = ( \partial_j x^i ) j^j + x^i \partial_jj^j \\ and = \delta^i_jj^j +x^i\partial_jj^j \\ and = j^i + x^i\partial_jj^j \end{align} and plugging this into the above expression for $\dot p^i$ gives \begin{align} \dot p^i = -\int_{\mathbb r^3} d^3 x \partial_j ( x^ij^j ) + \int_{\mathbb r^3} d^3x j^i \end{align} the first term is the volume integral of the divergence of $x^ij^j$ . since we are integrating over all space , this turns into a boundary term at infinity which vanishes for any finite ( or sufficiently rapidly decaying ) charge density . the result you want then follows \begin{align} \dot p^i = \int_{\mathbb r^3} d^3 x j^i \end{align}
$e=\hbar \omega$ is the energy of any particle , not just a photon . the terminology eigenvalue comes from linear algebra . given a matrix $m$ , an eigenvector $v$ of $m$ with eigenvalue $\lambda$ is a solution to the equation \begin{equation} mv = \lambda v \end{equation} in quantum mechanics the wave function is to be thought of as a kind of vector . observables are represented by ( hermitian ) operators ( which are morally the same as ( hermitian ) matrices ) , and the eigenvalues of those operators are the possible values the observable can take . so setting $m=i\hbar \frac{\partial}{\partial t}$ , $v=\psi$ , and $\lambda=\hbar \omega$ , you see the equation you wrote is just an eigenvalue equation for the " energy operator " $i\hbar \frac{\partial}{\partial t}$ . the eigenvalue is $\hbar \omega$ . it is called an energy eigenvalue because the observable is the energy . your physical system will be described by a wave function ( i prefer to call it a " state" ) $\psi$ . if $\psi$ satisfies that eigenvalue equation , then you will measure the system to have the energy $e$ with 100% probability . in general $\psi$ will not be an eigenvector of the energy operator . however it can be written as a sum of energy eigenvectors due to the magic properties of hermitian operators . in that case if you measure the energy you will measure it to be one of the eigenvalues of the eigenvectors making up $\psi$ , with probability given by the square of the amplitude of that eigenvector . if the notes you are following did not explain this clearly before diving into talking about energy eigenvalues , i would strongly recommend looking for alternative references to read . there are a million references that explain quantum mechanics at all levels and in many different ways . this point you are asking about is the central concept of all of quantum mechanics , so it is definitely worth reading many sources about this to find the one that has the explanation you find the clearest .
negative resistance is not uncommon . you see it in arc lamps , too . for your motor ( used as a generator ) , i would guess that it is most efficient at higher current , possibly having field coils and not permanent magnets . the voltage from a generator comes from moving a wire through a magnetic field . with field coils in series , the magnetic field is generated by the current flowing through the ( generator ) . there is probably a small residual permanent field in the iron , so you get some voltage even when there is no current . i bet if you could run up to higher currents , eventually you would get max power from the generator , and start seeing voltage go down again . i am going on this being a ' universal ' motor you have salvaged from somewhere . read about universal or series wound motors at the motor wiki : http://en.wikipedia.org/wiki/electric_motor i found a discussion group with one sensible ( to my mind ) answer amongst the cruft ( see engineertony 's reply ) here : http://cr4.globalspec.com/thread/77573/how-to-make-a-generator-from-a-universal-motor my own take is , go for it ! it kinda works , it looks like you can get real power from it , though the voltage is all over the map , you will want to regulate it .
the seasons on earth are caused by the rotation of the earth around the sun in addition with the inclination of the axis of the earth . this causes the angle of incident sunlight to be lower in winter and higher in summer . now , the inclination of the axis of the earth is pretty constant . but if that axis would shift in some funny way , you could have any season any time . so , for a summer that lasts several years , the axis would have to shift so that the light incident on a given hemisphere remains at a high angle . however , even defining a year might be hard on such a planet . on earth , we think of a year as the time it takes for the earth to circle the sun once . but that is not how ancient civilizations measured it . they looked at the height of the sun in zenith and noted that there was a day when the sun stood highest , and a day where the sun stood lowest , and these are summer and winter solstice , respectively . with the earth 's axis 's angle fixed , the time span from one summer solstice to the next is the same as the time it takes for the earth to completely circle the sun , but in our imaginary planetary system , these two movements would not be related anymore . but with random axis movement , it might be impossible to tell when a circle around the sun is completed . . .
the azimuthal momentum $$p_{\phi}~:=~\frac{\partial l}{\partial \dot{\phi}}$$ is the ( polar ) $z$-component of the angular momentum $l_z$ of the point mass $m$ relative to the heliocentric reference frame . it is a constant of motion because the azimuthal angle $\phi$ is a cyclic coordinate .
when you snap your fingers there are multiple sound waves , but the speed of sound is so fast you can not distinguish individual waves . the frequency of sound waves is around 100hz to 10khz so each wave completes one oscillation in 0.01 to 0.0001 seconds . what you are hearing when you snap your fingers is the envelope i.e. the overall amplitude of the sound waves . when you throw a stone into still water you get an expanding ring of waves moving out , so at the centre it is still then as you move outwards you pass through the waves and beyond them the water is still again . hearing the finger snap is the same as being struck by the expanding ring of ripples .
from wiki : for a finite-dimensional vector space , using a fixed orthonormal basis , the inner product can be written as a matrix multiplication of a row vector with a column vector : $ \langle a | b \rangle = a_1^* b_1 + a_2^* b_2 + \cdots + a_n^* b_n = \begin{pmatrix} a_1^* and a_2^* and \cdots and a_n^* \end{pmatrix} \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{pmatrix}$ based on this , the bras and kets can be defined as : $\langle a | = \begin{pmatrix} a_1^* and a_2^* and \cdots and a_n^* \end{pmatrix}$ $ | b \rangle = \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{pmatrix}$ and then it is understood that a bra next to a ket implies matrix multiplication . the conjugate transpose ( also called ''hermitian conjugate'' ) of a bra is the corresponding ket and vice-versa : $\langle a |^\dagger = |a \rangle , \quad |a \rangle^\dagger = \langle a |$ because if one starts with the bra $\begin{pmatrix} a_1^* and a_2^* and \cdots and a_n^* \end{pmatrix} , $ then performs a complex conjugation , and then a matrix transpose , one ends up with the ket $\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix}$
impact on water is a very complex topic . your simple calculation just figures out the velocity of a free-falling body after a 50 m drop . that just tells you the initial relative velocity of body and water surface . it does not tell you much about the force at impact , or whether the person survived . there are two things that might kill on impact : high local pressure on the surface of the body - this can cause lacerations , bleeding , fractures ; and a deceleration of the body that causes internal organs ( the brain or the kidneys ) to rip loose , resulting in severe internal bleeding and death . the key to survival , then , is to minimize the local pressure and the deceleration . this is why you see stunt men dive in the most streamlined manner possible - they try to minimize the area with which they enter the water , which limits the force on the body and thus maximizes the time to decelerate . it also helps to fall into " less dense " water - for example , the frothy water right behind a cruise ship will have a lot of bubbles in it , and this will increase the distance you move before slowing down . there is an additional complication which relates to the shape of the contact area - you may be familiar with the " belly flop " , where you fall flat on the water and it hurts a lot . this is not just because you slow down quickly - it is because there is a brief moment when the contact point between your body and the water moves faster than the speed of sound in water , and this results in an " attached shock wave " which can cause the pressure of the water to briefly become very high ( see for example http://www.dtic.mil/dtic/tr/fulltext/u2/a259783.pdf - a bit of a long paper . . . ) . it seems that the world record for a stunt man surviving a fall into water stands at just over 54 m ( olivier favre , https://www.youtube.com/watch?v=mld529gwkj4 ) . and that is " still " water . as i mentioned above , in frothy water it might be possible to survive a higher drop . at the same time , people frequently kill themselves by jumping off the golden gate bridge - about 70 m height . found an interesting link discussing this - the last answer sounds quite credible in its analysis , making some attempt at quantifying the forces during impact . i do not agree with the entire analysis ( in particular the " slamming into a stationary body at half the speed " is too much of an approximation for my taste ) but it makes some good points : http://www.newton.dep.anl.gov/askasci/gen01/gen01790.htm
however , this same black sky effect does not occur when looking out the window of a plane . it does not ? ( image credit : http://www.123rf.com/photo_10994787_view-of-jet-plane-wing.html )
well here 's an easier way to see it : as the universe is a fabric of space and time , the fabric can be rolled up and spread according to our needs . the problem : large amounts of energy is necessary for this to happen . in the starting , the big bang provided the energy necessary to do it . the speed of light limit is on actual objects , not on expansion of space , as observed during the period just after big bang . thus we can bend space behind us and expand it in front of us at speeds faster than light , appearing as if we are travelling faster than light .
since radioactivity is a random process , you had expect some fluctuation in the number of decays , i.e. if you wait for the half life , there is no guarantee that exactly half of the nuclei will have decayed ! based on your 3 estimates of the half life , you could just take the mean and go with that . and how to find the isotope ? there is lists for that . i just googled " half lives of radioactive isotopes " and , voila : http://en.wikipedia.org/wiki/list_of_radioactive_isotopes_by_half-life
you are working a finite number of impurities and expecting an answer in the thermodynamic limit . this should be clear if you do your calculation at a fixed density of impurities , and resum the infinite series , taking the leading term at each power of the impurity density . this will give you a correction to the self-energy . alternatively , as you suggest , you could do the born approximation for a single impurity . note that to get anything non-trivial you need to work to second order in the scattering potential . you would then need to multiply this by the density of impurities , ( or maybe something like the mean free time between scatterings , do not know off the top of my head ) to get the appropriate correction to the self energy . also note that the second born approximation knows about only a fragment of the behavior we expect from an electron in a disordered system . it know only that the momentum becomes randomized by repeated scattering . it does not know anything about even simple things like diffusion . to get that you need to look at other diagrams in the perturbation theory .
the probability of finding the particle exactly at a particular radial coordinate is zero whilst the probability of finding the particle in the infinitesimal neighborhood of that coordinate is infinitesimal . $\int_r^r \rho ( \tau ) d\tau = 0$ $\int_r^{r + dr} \rho ( \tau ) d\tau = \rho ( r ) dr$ this follows from the fundamental relationship : $f ( x + dx ) = f ( x ) + f' ( x ) dx$
the more complicated integrals can be easily reduced to the basic integral from equation 3.2 . you start with modifications that simplify the denominator . first of all , $2pq$ in the denominator may be eliminated by completing the square : $$ p^2+2pq + d = ( p+q ) ^2 + ( d-q^2 ) $$ which is of the same form as the original integral with $1/ ( p^{\prime 2}+d&#39 ; ) ^b$ , as long as $p&#39 ; =p+q$ and $d&#39 ; =d-q^2$ . second , the polynomials $p^\alpha p^\beta\dots$ in the numerator – which have already been rewritten in terms of the new variable $p$ so that the denominator is $1/ ( p^2+d ) ^b$ – can be easily calculated because the integral is a tensor so the integral with $2n$ copies of $p^\alpha$ in the numerator must be proportional to $$g^{\alpha\beta} g^{\gamma\delta} \dots g^{\alpha_n\beta_n}+{\rm permutations} $$ times the integral with $ ( p^2 ) ^n$ replacing the product of the $p^\alpha$ factors where the overall coefficient may be calculated in a straightforward way by checking the same identity with $n$ contractions .
the primary utility in introducing the generating functional is in using it to compute correlation functions of the given quantum field theory . let 's restrict the discussion to that of a theory of a single , real scalar field on minkowski space , and let $x_1 , \dots , x_n$ denote spacetime points . of central importance are time-ordered vacuum expectation values of field operators evaluated at such points ; \begin{align} \langle0|t [ \phi ( x_1 ) \cdots\phi ( x_n ) ] |0\rangle . \end{align} it can be shown that these objects can be obtained from the generating functional by taking functional derivatives with respect to the $j ( x_i ) $ as follows : \begin{align} \langle0|t [ \phi ( x_1 ) \cdots\phi ( x_n ) ] |0\rangle = \frac{1}{z [ 0 ] }\left ( -i\frac{\delta}{\delta j ( x_1 ) }\right ) \cdots \left ( -i\frac{\delta}{\delta j ( x_n ) }\right ) z [ j ] \bigg|_{j=0} . \end{align} this standard fact is proven in many books on qft . it is often proven using the path integral approach which makes it pretty transparent why it is true . the crux of the argument is that every time you take a functional derivative with respect to the source $j ( x_i ) $ , it pulls down a factor of the field $\phi ( x_i ) $ . dividing by $z [ 0 ] $ is an important normalization relating to vacuum bubbles , and setting $j=0$ after computing the appropriate functional derivatives eliminates terms with more than $n$ factors of the field and renders the final result source-independent as it should be .
a push up is a form of lever . the athlete must exert roughly half her body weight ( under some assumptions i will clarify at the end of the post . ) we can solve this problem using the principle of virtual work . assume the athlete raises her body through a small angle $\textrm{d}\theta$ . then her center of mass rises by $l \cos\theta \ \textrm{d}\theta$ , with $l$ the distance from her feet to her center of mass . the work done is $$\textrm{d}w = mgl\cos\theta \ \textrm{d}\theta$$ this work is equal to the force multiplied by the distance over which the force is exerted . if we call the distance from her feet to her shoulders $l$ , then $$mgl\cos\theta \ \textrm{d}\theta = f l \cos\theta \ \textrm{d}\theta$$ $$f = \frac{mgl}{l}$$ your center of mass is roughly half way up your body , so a push up requires you to lift about half your weight . this answer assumes the pushup is infinitely slow ( i.e. . no acceleration ) . this is actually not so bad an approximation as it sounds , but people doing fast push ups will probably exert a higher force at the bottom of the push up , then let gravity do negative work to slow them down as they near the top . this is the basis for " clapping push ups " ( which i am too weak to perform ) . we have only calculated the component of force in the direction of motion , so , we are assuming the athlete pushes directly in the direction of motion of her shoulders . this means she does not push along the red line in your picture , but diagonally ahead of it . in reality , she might push more straight down , increasing the force and decreasing the angle through with her elbows flex . the force may actually change throughout the push up due to this effect . if you lean into the arm of a couch and do a push up off that , you will find it is much easier at this high angle . the reason is you are not pushing straight down any more , so it is a little bit like going up a ramp ( in that the distance over which the force is exerted is lengthened to accomplish the same amount of work ) . i also assumed the athlete 's center of mass does not move , which means i am pretty much thinking of her arms as massless , and the rest of her is a rigid board . finally , i am assuming her hands and feet do not slip , the floor is too big to move , etc . an empirical way to determine the force would simply be to put scales underneath the hands , but the reading could be a little off depending on whether the scales are constructed to measure total force on them , or only the vertical component of the force . finally , an actual human is not simply one force pushing in one direction . there are muscles , bones , ligaments , etc . all sorts of different forces are going on in different places . this calculation gives the force that the arms exert on the main body . the force that the triceps exert on the elbow , for instance could be much higher .
in the many-worlds interpretation of quantum mechanics , there is at all times just one state vector for the entire universe . it is a vector in a certain ( infinite-dimensional ) vector space , and that vector space is always the same . so there is nothing in the theory whose dimension changes when the number of particles in the universe changes . to be a bit more precise , the space in which the state vector of the universe lives is ( something like ) a fock space . vectors in that space include states with all possible different numbers of particles , as well as superpositions containing different numbers of particles . so if a particle-antiparticle pair is created , the state vector simply " wanders " from one part of that space to another ; the space itself need not get any bigger .
this is in addition to my answer posted elsewhere since op wanted a more general answer . that example captured the essence based on the idea of how information can be encoded -- it is a somewhat constructive argument in spirit . another way of thinking about the amount of information is as follows : if an event that is very probable happens , then you cannot get much information out of it . it was going to happen any way . on the other hand , if something unusual happens , then that should give you something to think about . such an event carries more " information " . ( for eg : the occurrence of a certain event conveys no information ) if two independent events happen , then the information you glean from them must " add up " . since their probabilities multiply to give the probability of the combined event , the information gleaned from each event must be proportional to the $\log$ of its probability . in typical treatments , one solves a functional equation for the dependence of the entropy on the probability distribution -- with the two conditions mentioned above . the latter gives the $a \log [ ] + b $ while the former fixes the additive constant $b$ to zero . the scale factor $a$ depends on the base to which you take the logarithm .
i think the harvard physics problems of the week are pretty nice : http://www.physics.harvard.edu/academics/undergrad/problems.html
your mistake is in a misunderstanding of what the graphical representation of the vectors means . to get the components of a vector that is drawn on a diagram , imagine moving the tail of the vector to the origin while keeping the orientation of the vector fixed , and then its components are the projections onto the $x$ and $y$ axes . in particular , when a horizontal vector is drawn , this means that its vertical component is zero , and when a vertical vector is drawn , this means that its horizontal component is zero . so for example , you write that vector $\mathbf b$ has a $y$-component of $51\ , \mathrm{ft}$ , but that is because you are taking into account the position of its " tail , " which is not what the diagram means . instead , imagine moving $\mathbf b$ to the origin without changing its orientation , then it will point along the positive $x$-axis , but it will have no $y$-component , and you should get $$ \mathbf b = ( 45\ , \mathrm{ft} ) \ , \mathbf i . $$ i will let you do the rest . i hope that helps ! cheers !
there exists such a model and it is described in a wikipedia article , the ekpyrotic universe . the ekpyrotic model came out of work by neil turok and paul steinhardt and maintains that the universe did not start in a singularity , but came about from the collision of two branes . this collision avoids the primordial singularity and superluminal expansion while preserving nearly scale-free density fluctuations and other features of the observed universe . the ekpyrotic model is cyclic , though collisions between branes are rare on the time scale of the expansion of the universe to a nearly featureless flat expanse . observations that may distinguish between the ekpyrotic and inflationary models include polarization of the cosmic microwave background radiation and frequency distribution of the gravitational wave spectrum .
i supposed you are in a context of bound states , with normalized eigenfunctions $\psi_n ( x , t ) = \phi_n ( x ) e ^{ie_nt}$ . of course , if you calculate $\langle x ( t ) \rangle_{\psi_n} = \int dx \bar \psi_n x \psi_n$ , you will find a position expectation value which does not depend on time . now , this is not the general case , if you take a linear combination of the $\psi_n$ : $\psi = \sum a_n \psi_n$ , and calculate $\langle x ( t ) \rangle_{\psi} = \int dx \bar \psi x \psi$ , you will find a positition expectation value which depends on time .
there is not that much to remember as far as fundamental particles are concerned . we have matter particles and field particles . matter particles are quarks and leptons and the latter include electrons and neutrinos . both quarks and leptons come in pairs ( up/down quark , electron/neutrino , etc . ) which means they are charged under the weak force . quarks are moreover colored and form rgb triplets which means they are charged under the strong force . that is pretty much it except for unknown reasons there are three copies of each particle with identical properties but distinct masses ( e . g . electron/muon/tauon ) . you do not really need to know much about the two heavier families since these are unstable ( decaying into the lower families which are stable since there is nothing else they could decay into ; at least besides the down quark that can decay into up quark ; this is the famous radioactive beta decay ) and you need expensive equipment ( such as lhc or jets from galaxies ) to produce them . as for the field particles , there is one type for every force although it gets a bit more tricky with weak and strong forces . for em we have the usual photon . weak force will give us three more -- the $w^{\pm}$ and $z$ which are massive -- and finally the strong force has eight massless gluons ( of various colors ) . additionally there may or may not be the higgs particle ( or more of them ) , superpartners , etc . but all of this is as yet unconfirmed . so , there , that is all there is to particle zoo . it is only pain when you try to learn it by heart . but if you instead try to understand the concepts related to these particles ( role of the weak particles and neutrinos in the beta decay , muons in cosmic rays , features of quantum chromodynamics , etc . ) then it should be fun .
real batteries have a finite energy storage capacity . adding additional cells adds additional capacity ( this is why i would add them , you have not really specified any context so it is hard to say what you are looking for ) . also , it is worth noting : i am assuming that all the cells are identical , and internal resistance is negligible . this is an important assumption . in a real circuit , you would lose some power ( $i^2r$ ) to the internal resistance of the battery . you can reduce this power loss by adding cells , reducing the current each cell provides .
so , as noted , we use poynting 's theorem to get : $$\frac{1}{\mu_0} \oint_{\partial v} ( \vec{e} \times \vec{b} ) \cdot d\vec{s} = -\int_v \vec{e} \cdot \vec{j} dv$$ [ the static version of poynting 's theorem is just : divergence theorem , $\nabla \cdot ( \vec{e} \times \vec{b} ) = \vec{b} \cdot ( \nabla \times \vec{e} ) - \vec{e} \cdot ( \nabla \times \vec{b} ) $ , then $\nabla \times \vec{e} = 0$ and $\nabla \times\vec{b} = \mu_0\vec{j}$ ] the electric field is just the negative gradient of electric potential . we can use integration by parts in higher dimensions : $$-\int_v \vec{e}\cdot\vec{j} dv = \int_v ( \nabla \phi ) \cdot \vec{j} dv = \oint_{\partial v} \phi \vec{j} \cdot d\vec{s} - \int_v \phi ( \nabla \cdot \vec{j} ) dv$$ the current density does not diverge . so it is just the first term , basically , the surface integral of voltage times current . of course your boundary condition says there is no current out of the surface , still , the previous statement holds in static situations even without that boundary condition .
there is one and only way to cancel something : add its negative to itself . however , there is an alternative to cancellation for shielding a region from external electromagnetic fields . generally speaking , methods of isolating a region from external electromagnetic fields ( em shielding ) can be divided into two categories , passive and active . a passive shield prevents the external field from reaching the isolated internal region . whatever the field is outside , the field is zero inside . this is convenient if the strength of the external field is variable or unknown . faraday cages ( shields made from a mesh of conducting material ) are examples of passive shields against static ( and non-static ) electric fields . alternatively , if you know the value of the external field from which you want to isolate a region , you can generate an equal and opposite field to the external field to actively " cancel it out " . the active alternative to a faraday cage for blocking electrostatic fields is a capacitor , whose geometry is precisely shaped so that the electric between the two charged plates exactly cancels the external field in the region of interest . the magnetostatic analog for active shielding is field cancellation using solenoids with the appropriate geometry . the passive alternative for magnetostatically shielding a region ( analog to the faraday cage ) is an enclosing surface made of material ( metal alloys ) with high magnetic permeability . they do not exactly block the external magnetic field per se in the same way a faraday cage blocks an external electric field , but rather draw the field into themselves , providing a path for the magnetic field lines around shielded cavity . however , the effectiveness of passive magnetostatic shielding diminishes for very weak fields . from a practical standpoint , this usually makes active shielding by electromagnets ( solenoids , helmholtz coils , etc . ) the more useful of the two options .
i do not just mean reactions that require heat to proceed , storing surplus energy in chemical bonds . i wonder about strongly endothermic reactions that suck heat out of environment a reaction that requires heat to proceed , a reaction that sucks heat out of the environment , and an endothermic reaction are all the same . these are all just descriptions of reactions that occur because entropy ( s ) increases , despite that fact that enthalpy ( h ) also increases . $g = h -ts$ a reaction is favorable if gibbs free energy decreases . the reaction can still be favorable , despite enthalpy ( h ) increasing , if entropy ( s ) increases enough . you take some substance a ( e . g . ammonium nitrate ) , and some substance b ( e . g . water ) . . . i wonder what happens on the microscopic scale you have to separate molecules/ions/atoms of a from others of a . this could involve breaking apart ionic interactions in a crystal , or intermolecular interactions for example . this takes energy . you have to seperate molecules/ions/atoms of b from others of b . for a solvent , this would involve breaking apart intermolecular interactions such as dipole-dipole interactions . this takes energy . new interactions between a and b form . this releases energy . if the energy released in step 3 is less than the energy required for steps 1 and 2 , the process is endothermic .
maybe that was just window frost ( http://www.its.caltech.edu/~atomic/snowcrystals/frost/frost.htm - " forms when a pane of glass is exposed to below-freezing temperatures on the outside and moist air on the inside" ) ?
i have not seen the term electrostatic pressure used explicitly before , but i can explain how to think about the problem . you need to consider the total force on each hemisphere , which is of course the integral over the sphere of the ( vector ) force per unit area . take , then , a surface element $da$ , with charge $\sigma da$ . as is nicely explained by purcell , the force on such a surface element is given by the average of the electric field inside and outside . since the field inside vanishes , the total force on the surface element is then $$d\mathbf{f}=\frac{1}{2}\sigma da\times\frac{4\pi r^2\sigma}{4\pi\epsilon_0}\frac{\hat{\mathbf{r}}}{r^2}=\frac{\sigma^2}{2\epsilon_0}\hat{\mathbf{r}}\ , da . $$ by symmetry , the total force on each hemisphere will be along the axis of the problem , which i take in the $z$ direction . this total force will then be $$\mathbf{f}=\int d\mathbf{f}=\hat{\mathbf{z}}\int\frac{\sigma^2}{2\epsilon_0}\hat{\mathbf{z}}\cdot\hat{\mathbf{r}}da=\hat{\mathbf{z}}\frac{\sigma^2}{2\epsilon_0}r^2\int\cos ( \theta ) d\omega=\frac{\sigma^2\pi r^2}{2\epsilon_0}\hat{\mathbf{z}} . $$ the effect is indeed like having a gas inside exerting an outward pressure $p=\frac{df}{da}=\frac{\sigma^2}{2\epsilon_0}$ , but this is hardly general - it depends on the precise , global arrangement of charges of this particular problem , while giving the impression of being a purely local thing ( since it depends only on the " local " charge density , which is of course also a global parameter ) . if you do accept this " pressure " then yes , the total force is this constant pressure times the area vector of the surface , which is $\pi r^2\hat{\mathbf{z}}$ .
you already received several answers . however the fundamental physical reason is elementary : in classical , quantum and relativistic physics the physical laws describing an isolated physical system in an inertial reference frame are the same ( are invariant ) if you rotate ( with an element of $so ( 3 ) $ ) the system ( there are many other symmetries depending on the theory , but the point is that rotations are symmetries ) . this is a fundamental postulate of all physics , valid at small ( noncosmological ) scales at least . so , for instance a curve describing the evolution of the system in the space of states remains a curve describing ( another ) evolution of the system if we apply the same rotation to the former curve at every time . there are many other examples . you understand that , unless the space of states be the physical space isomorphic to $\mathbb r^3$ , the action of $so ( 3 ) $ on the states cannot be implemented directly using the matrices $r\in so ( 3 ) $ . instead you should faithfully " represent " the elements group $so ( 3 ) $ in terms of natural transformations of the space of states . in quantum mechanics , for many theoretical reasons ( already mentioned wigner and kadison theorems ) these natural transformations are given by unitary ( and anti unitary ) operators , as theoretical consequence of the fact that the most elementary expected preservation property of these symmetries is that of probabilty transitions of pairs of pure states . ( actually one should use projective unitary representations , but i do not think it is the case to enter into the details here . ) another intersting fact of $so ( 3 ) $ ( actually $su ( 2 ) $ ) representations is that , due to the compactness of the group , all possible unitary representations are constructed out of the irreducible ones via the standard procedure of direct sum ( this is not a trivial fact , because usually one should deal with the much more complicated tool of the direct integral ) .
there are few relevant points : the first is that we very rarely consider the magnetism of nuclear moments in a solid because the nuclear magneton is about 2000 times smaller than the bohr magneton . so we are really stuck with electrons as the only relevant degrees of freedom for magnetism . but , let 's imagine for a minute that we could kick out the electrons . the whole point of a spin liquid is to be driven to paramagnetism at low temperatures by quantum fluctuations . the high ordering temperature scales in typical magnets are set by the " exchange " scale ( of electrons literally , though virtually , exchanging places in the solid ) . dipole-dipole interactions are not strong enough to be the origin of magnetism in solids ( this was a big deal in the early days of quantum mechanics ! ) and dipolar interactions between nuclear spins even more so ; so functionally , it would be impossible to determine experimentally if such a system was a spin liquid or just a thermally disordered paramagnet even at the lowest possible experimental temperatures .
consider the ratio of their shifts in energy . since all the numbers are the same except for the reduced mass you are looking at something like : $$ \frac{ ( e_i -e_f ) _p}{ ( e_i -e_f ) _d}=\frac{\mu_p}{\mu_d}=\frac{m_p m_e ( m_d +m_e ) }{ ( m_p +m_e ) m_d m_e} $$ if we cancel the $m_e$ and then pull a $m_d$ out of the top and a $m_p$ out of the bottom we get $$ \frac{ ( e_i -e_f ) _p}{ ( e_i -e_f ) _d}=\frac{ ( 1+\frac{m_e}{m_d} ) }{ ( 1+\frac{m_e}{m_p} ) } $$ i hope this helps .
i am not an expert on non-neutral plasma but i am going to explain what i know . one of the wave modes that exists in non-neutral plasma is langmuir waves , which has a dispersion relation of ( in neutral plasma ) : the previous equation is also called bohm-gross dispersion relation . the terms in the right hand side are the plasma frequency of electrons squared and the other term is 3 by the wave vector squared by the thermal velocity of electrons squared . the previous equation allows for a zero value of the wave vector . in such a case the frequency of oscillation is equal to plasma frequency . in that case that oscillation is called plasma oscillation but it is not a wave since the wave vector is zero . for non-neutral plasma , the plasma frequency as a parameter exists . langmuir waves exist as well . if your question is whether plasma oscillations exist ( for zero wave vector ) , then the answer i suppose is no . in this following papers have a look at the dispersion relation of langmuir waves for purely ionic plasma and purely electronic plasma ; you see that the frequency is zero when the wave vector is zero ( i am not certain whether it is exactly zero or if it’s lower by orders of magnitude than the frequency ranges plotted ) . comparing those plots with the same plots for langmuir waves in neutral plasma , it is clear to see from the last equation that for zero wave vectors , the frequency of oscillation is non-zero . see the figure below for neutral plasma ( taken from chen chapter 4 ) hope that helped ! !
as a rule of thumb , the relative stability and precision which you can hope to achieve with any oscillator is limited by the number of periods you can observe your system for . for the current definition of a second the oscillator is a microwave transition at about $9\textrm{ ghz}\approx 10^{10} \textrm{ hz}$ . since trapping the atoms shifts the energy levels , you need to chuck them up and measure them when they fall back down , which means that the effective interaction time is of the order of seconds . on the other hand , using a transition in the optical part of the spectrum would keep observation times at about the same order but increase the frequency of the radiation up to about $10^{15}\textrm{ hz}$ . as gill points out , this would mean uncertainties lower by two or three orders of magnitude , simply because you observe a much bigger number of periods . the definition of a second is fine as it is for what we are doing now . however , cold-ion clock technology is indeed close to this fundamental limit . as the presentation shows ( page 3 ) , optical clocks have overcome many of the technical reasons that make them difficult to work with , as well as some fundamental issues solved by the frequency comb , to catch up with fountain clocks . it is therefore the time to ask whether we should not make optical transitions the fundamental standard and stop worrying about calibrating them with a ( less accurate ) fountain clock .
i would like to add a bit of mathematical detail the ( correct ) statements by djbunk . let a scalar function $f$ be given ( let 's not restrict ourselves to the electric potential ) . for any unit vector $\mathbf n$ , we can define the directional derivative $d_\mathbf{n}$ of the function $f$ in the direction $\mathbf n$ as follows : $$ d_\mathbf{n}f ( \mathbf x ) = \mathbf n\cdot\nabla f ( x ) . $$ the directional derivative gives the rate of change of the scalar function $f$ in the direction of the unit vector $\mathbf n$ . notice that $$ \mathbf n\cdot \nabla f ( \mathbf x ) = |\nabla f ( \mathbf x ) |\cos\theta $$ where $\theta$ is the angle between $\mathbf n$ and $\nabla f ( \mathbf x ) $ , so the directional derivative is maximized when $\theta = 0$ , and is minimized when $\theta = -\pi$ . in other words ; the the rate of change of a scalar function $f$ at a point $\mathbf x$ is positive and greatest in magnitude in the direction of the gradient of $f$ at $\mathbf x$ . this confirms bjbunk 's statements .
the question is : does the air+alcohol-gas burn creating a big pressure or does the container need of a little hole to burn ? strange " need of a little hole " , what does that mean ? the question was : do we need of hole to start the ratio of oxygen+alcohol another time strange " need of hole " . and on the whole two rather different questions . one thing i can say : in a container with alcohol vapor ( i assume ethyl alcohol is meant ) and air one will have 40 mmhg partial pressure of ethanol at 19 °c 720 mmhg partial pressure of air this makes 40/760 = 5.3 vol-% of ethanol . a view in tables says that ethanol has lower explosive limit ( lel ) 3.3 vol% upper explosive limit ( uel ) 19 vol% answer is : your container will explode , the liquid ethanol will be sprayed around , burning . i would prefer to be several dozens of meters away from such " experiments " , at least .
i ) two square matrices $a$ and $b$ are similar matrices if they are connected via a relation $$\tag{1}ap~=~pb$$ for some invertible matrix $p$ . ii ) two square matrices $a$ and $b$ are unitarily similar matrices if $p$ in eq . ( 1 ) is a unitary matrix .
you have a 2d point source , so the energy of the wave is spread out along a circle of circumference $2\pi r$ where r is the distance from the point source . this means the intensity of the wave varies as 1/r . it is a 2d version of the inverse square law . obviously the amplitude can not be constant everywhere on the second wall or the energy of the wave would be infinite . you get the curve drawn for the amplitude on the second wall because the distance from the point source to the wall increases as you move along the away from the point immediately opposite the slit .
a plasma sheath is the interaction of a plasma with a boundary . as long as you let enough time for the plasma to be created , you let enough time for the plasma to create a self-consistent electric field in the sheath . in stationary plasmas , the only case when there is no sheath , is when the surface that the plasma is in contact to , is at the plasma potential ( i.e. . same potential as the plasma ) , which means you have to supply current to this surface ( in langmuir probes in dc glow discharges , for example ) in order to compensate for the fact that electrons arrive more quickly - and hence , at a higher rate - than positive ions , to the boundary , producing a net negative charge flux and thus a current . and in this case , there is no electric field either . in all other cases , the sheath is necessarily accompanied by an electric field of its own , because the electric field is what allows the sheath to exist , as an entity with different properties and structure than the plasma bulk . if you increase or decrease the electric field applied to your discharge , the sheath will change , and adapt to that field . basically , if you try to increase the voltage in a dc glow discharge , the cathode sheath will first cover all of the cathode area , then the current across the sheath will increase to allow the potential drop across the sheath to increase . then the cathode will heat up during the transition to arc , and the sheath will change to take into account the increase ion and electron emissivity of the cathode . all these processes are rather complex and depend highly on the situation . so i think one might say : sheath and electric field are bound together by many relations , so it is difficult to answer without knowing a little more about your particular case !
there is one formula relating the speeds of any two " platforms " ( say $p$ and $q$ ) between each other : $$v_{p} [ q ] = v_{q} [ p ] . $$ and there is of course the well known symbol for " speed of light ( in vacuum ) " , as determined of light signals exchanged by members of any one platform between each other : $c$ . the speed of any one platform ( $q$ ) as determined by members of any other platform ( $p$ ) can thereby be expressed ( and this will be found convenient below ) as $$v_{p} [ q ] = c \ , \ , \beta_{p} [ q ] $$ with the appropriate real number value $\beta_{p} [ q ] $ . correspondingly of course : $$\beta_{p} [ q ] = \beta_{q} [ p ] . $$ and then there is one formula relating the ( pairwise ) speeds , or for simplicity rather the corresponding $\beta$-numbers , of any three " platforms " ( say $g$ , $h$ , and $j$ ) between each other : $$1 - \left ( \ , \beta_{h} [ g ] \ , \ , \ , \beta_{h} [ j ] \ , \ , \ , \text{cos} [ \angle_{h} [ g , j ] ] \ , \right ) = \sqrt{ \frac{ ( 1 - \beta^2_{h} [ g ] ) \ , ( 1 - \beta^2_{h} [ j ] ) }{ ( 1 - \beta^2_{g} [ j ] ) } } . $$ in case $\text{cos} [ \angle_{h} [ g , j ] ] = -1$ which should be applicable to the " stack of platforms " described in the question this simplifies to the surely familiar formula $$\beta_{g} [ j ] = \frac{ ( \beta_{h} [ g ] + \beta_{h} [ j ] ) }{ ( 1 + \beta_{h} [ g ] \ , \ , \beta_{h} [ j ] ) } , $$ or likewise : $$\beta_{g} [ j ] = \frac{ ( \beta_{g} [ h ] + \beta_{h} [ j ] ) }{ ( 1 + \beta_{g} [ h ] \ , \ , \beta_{h} [ j ] ) } . $$ now , this formula can be applied to the speed values given in the question $v_1 := \beta_1 \ , c$ , $v_2 := \beta_2 \ , c$ , . . . $v_k := \beta_k \ , c$ and so on ; where $v_1$ is the speed of the bullet wrt . the first platform , $v_2$ the speed of the second platform wrt . the first , and so on ; successively in the proposed " russian doll " setup . of particular interest are surely the resulting speed value $v_{ [ 0 , k ] }$ ( or the corresponding real number $\beta_{ [ 0 , k ] }$ ) of the bullet wrt . the $k$ th platform . with the above formula follows $$ \beta_{ [ 0 , ( k + 1 ) ] } = \frac{ ( \beta_{ [ 0 , k ] } + \beta_k ) }{ ( 1 + \beta_{ [ 0 , k ] } \ , \ , \beta_k ) } $$ noting the similarity of this formula to the addition theorem of the " hyperbolic tangent " function $\text{tanh}$ , $$\text{tanh} [ x + y ] = \frac{ ( \text{tanh} [ x ] + \text{tanh} [ y ] ) }{ ( 1 + \text{tanh} [ x ] \ , \ , \text{tanh} [ y ] ) } , $$ we obtain $$ \beta_{ [ 0 , ( k + 1 ) ] } = \text{tanh} [ \text{arctanh} [ \beta_{ [ 0 , k ] } ] + \text{arctanh} [ \beta_k ] ] $$ . applying this to all ( "$n$" ) given speed values ( or corresponding $\beta$ values ) then $$ \beta_{ [ 0 , n ] } = \text{tanh} [ \sum^n_{j = 1} \text{arctanh} [ \beta_j ] ] , $$ or correspondingly $$ v_{ [ 0 , n ] } = c \ , \ , \text{tanh} [ \sum^n_{j = 1} \text{arctanh} [ \frac{v_j}{c} ] ] . $$ the values of the $\text{tanh}$ function are or course approching $1$ , but do not reach the value $1$ for any argument value ; cmp . http://commons.wikimedia.org/wiki/file:sinh%2bcosh%2btanh.svg therefore the total bullet speed ( wrt . the " final , $n$ th platform" ) that may be reached in the described " russian doll " setup with $n$ successive platforms cannot reach ( or even exceed ) the speed of light , $c$ .
this question is the black hole information paradox . if you take two entangled particles , make a black hole by colliding two highly energetic photons , throw in one of the two entangled particles , and wait for the black hole to decay , is the remaining untouched particle entangled with anything anymore ? in hawking 's original view , the infalling particle would no longer be in communication with our universe , and the entanglement would be converted to a pure density matrix from our point of view . the particle outside would no longer be entangled with anything we can see in our causal part of the universe . then when the black hole decays , the outgoing hawking particles of the decay would not be entangled with the untouched particle . this point of view is incompatible with quantum mechanics , since it takes a pure state to a density matrix . it is known today to be incorrect , since in models of quantum gravity when ads/cft works , the theory is still completely unitary . this means that the particle stays entangled with something as its partner crosses the horizon . this " thing " is whatever degrees of freedom the black hole has , those degrees of freedom that make up its entropy . when the black hole decays completely , the outgoing particles are determined by these microscopic variables , and at no point was there ever a loss of coherence in the entanglement . this point of view requires that the information about the particle that fell through the horizon is also contained in the measurable outside state of the black hole . this is t'hoofts holographic principle as extended into susskind 's black hole complementarity , the principle that the degrees of freedom of a black hole encode the infalling matter completely in externally measurable variables . this point of view is nearly universal today , because we have model quantum gravity situations where this is clearly what happens . the details of the degrees of freedom of a four dimensional neutral black hole in our universe are not completely understood , so it is not possible to say exactly what the external particle is entangled with as the infalling particle gets to the horizon . but the basic picture is that the infalling particle does not fall through from the external point of view , but smears itself nonlocally on the horizon ( like a string theory string getting boosted and long ) . the external particle is still entangled with this second representation of the infalling particle . this means that the same thing is described in two different ways , the interior and the exterior . but since no observer measures both at the same time , it is consistent with quantum mechanics to just have a unitary transformation that reconstructs the interior states from those states of the exterior that can be measured at infinity by scattering experiments .
so is it correct to say that magnetic field are ultimately caused by currents ? no , think of a magnetic field as a field that permeates all of space and time , existing independent of anything else . ( however an empty field does not do anything so changes in $\vec b$ field are what matter . ) the ( change in ) magnetic field can be created by currents but also by other stuff too . using the gauss 's law : $$ \nabla \cdot \vec b = 0 $$ we can see that the magnetic field does not have any divergences i.e. no sources or sinks ( mono-poles ) . you can imagine it as an incompressible fluid ( like a tank of water , the water can move but you can not create high density water or vacuum volumes ) . so we have established there are no divergences however there can be curls in the field , i.e. the field can flow , but since there are no divergences these flows must be closed loops . the equation you describe , ampère 's circuital law ( with maxwell 's correction ) , is given by : $$ \nabla \times \vec b = \mu \vec j + \epsilon \mu \frac {\partial \vec e}{\partial t} $$ this states that there are two ways to create a curl in the $\vec b$ field . the first is with a current density i.e. movement of charge which necessarily requires electric charges . so yes ( curls in ) magnetic fields can be created by electric charges . however there is a second part which states that curls in magnetic fields can also be created by $\vec e$ fields changing in time . this usually requires a charge particle but it can also be caused by a changing magnetic field . this is how light propagates , changing $\vec b$ creates a changing $\vec e$ field which creates a changing $\vec b$ etc . if magnetic mono-poles exist then they can be used to create the first changing $\vec e$ field . which eliminates the need for charges ( well they are magnetic charges ) .
what you are looking for is usually called a window , not a filter . you need to check different manufacturers to choose a material that fits your requirements , including not only light transmission but also the pressure differential , chemical inertness , temperature range , etc . checking ars ' website , calcium fluoride ( caf2 ) might be what you are looking for .
the acceleration ${\bf a} ( t ) $ is simply computed from newton law $f = m a $ . it is a function of the forces on the particle , which is ( assumedly ) computable from the positions ${\bf r} ( t ) $ ( of the entire system , i.e. all particles ) at time $t$ . this can be seen also in the figures , in the three schemes the acceleration is computed from ( and only from ) the positions in that instant of time .
no , the elements of the periodic table do not form any representation of a group or , more precisely , any irreducible representation . even more precisely , the real insights by mendeleev – that the reactivity etc . is a repeating function of the atomic number – does not follow from any property of a representation that could be derived by group theory . the periodic table boils down to the electron 's filling the shells in the atom , quantum states that are close to the energy eigenstates of a rescaled hydrogen atom . the closest thing to your project that actually can be done is to solve the full hydrogen by the $so ( 4 ) $ symmetry – the rotational symmetry enhanced by the runge-lenz vector : http://motls.blogspot.com/2011/11/hydrogen-atom-and-so4-symmetry.html this solution dictates not only degeneracies but even the energies because the hamiltonian is a function of a casimir . and these energies are important to determine which $z$ produce more reactive elements . more complicated atoms do not have any $so ( 4 ) $ symmetry , only $so ( 3 ) $ , and they can not be solved purely by symmetries . the eightfold symmetry is useful because the elementary building blocks are numerous and they carry various labels – like quarks come in different flavors . but that ain't the case of atoms in the approximation of chemistry or atomic physics for which the nucleus only matters when it comes to its charge , i.e. $z$ , and electrons are the only other particles that matter , without any flavor indices . so there is simply no room for eightfold symmetries etc . the fundamental symmetry between elementary particles is $u ( 1 ) $ , not $su ( 3 ) _f$ as it is for light quarks . if we neglect the electron-electron interactions in the atoms , we get another solvable problem – one in which we literally fill shells of the hydrogen atom . this system is a second-quantized hydrogen atom of some sort and it is solvable . we could say it is solvable by group theory . of course , this approximation ultimately leads to a wrong ordering of shells and the predicted periodic table would be wrong for high $z$ , too . to conclude , physical systems that may be fully solved just by group theory – and even properties of physical systems that may be determined by group theory – are rare enough , a small enough minority of the questions we may ask . atoms are complicated enough so that their properties mostly boil down to more complicated dynamics than just symmetries .
using gradshteyn and ryzhik ( seventh edition ) 3.876 ( 1 ) $$\int_0^\infty \frac{\sin{ ( p \sqrt{x^2+a^2} ) }}{\sqrt{x^2+a^2}} \cos ( b~x ) dx=\frac{\pi}{2} j_0 ( a\sqrt{p^2-b^2} ) ~~ [ 0&lt ; b&lt ; p ] \\ = 0~~ [ 0&lt ; p&lt ; b ] $$ differential of both sides with respect to $b$ will give the integral you want to calculate .
the problem of whether and how to include electrons/positrons in mass-energy calculations is based on the difference between the entities involved in the reaction , and the entities for which masses can be determined . this becomes more complicated when there may be both orbital electrons and nuclear-sourced electrons/positrons involved . consider the last equation in the op 's question . the physical process is clear : four bare protons with a total charge of +4 , fuse to form a single bare helium nucleus or alpa-particle ( charge +2 ) , and two positrons ( total charge +2 ) ( "bare " here means no orbital electrons ) charge is conserved , as is mass-energy . if you knew the rest masses of a proton , alpha-particle and positron , you could just add up the two sides of the equation to find the " missing " mass and equate it to the energy liberated . but most sources of isotope mass list the mass of the nucleus including the element 's appropriate number of orbital electrons . suppose you used these tabulated values for the hydrogen and helium in this reaction . using the tabulated value for $_1h^1$ would mean that you incorrectly included the mass of four extra electrons on the left-hand side . using the tabulated value for $_2he^4$ would mean that you incorrectly included the mass of two extra electrons on the right-hand side . so , to get the right answer , you would need to add the two positron masses on the rhs , and an additional two electron masses on the rhs to correct for the " wrong " electrons . in the example above , the alpha mass seems to explicitly exclude the extra electrons . the nature of the proton mass is not stated . without this information , the need for " bookkeeping " electron masses cannot be determined .
the attraction is that between a charge and an induced dipole . if the charged object is a sphere , the field is $qe\over 4\pi r^2$ , in units where $\epsilon_0=1$ . this field is what induces the dipole and causes the attraction . once you get a dipole moment in the dust , the dust dipole is attracted to regions of stronger field with a force that goes like the derivative of the e field . the total dipole is neutral , so you get net opposite positive and negative forces which only fail to cancel out to the extent that the e field is stronger in the regions of induced positive charge . the magnitude of the force is ( in the nearly perfect small-dust approximation ) $ d \cdot \nabla e$ , the dipole moment can be thought of as defined by this equation ( although that is not the definition--- the dipole moment vector d is the sum of qx over all the little infinitesimal regions in the dust , where q is the charge of the region , which sums to zero over the dust particle , and x is the position ) . so if the dipole moment is of fixed size , the force is $2qd\over r^3$ . but the dipole itself is proportional to e , with a coefficient that i will call " p " for polarizability , so you get a force which is $$ {2 q \over 4\pi r^3} { pq\over 4\pi r^2} = {2pq^2\over 16\pi^2 r^5}$$ and this is the force between a sphere and a dipole ( in units where $k={1\over 4\pi\epsilon_0} = 1$ ) . you see it falls off as $1/r^5$ , two powers of r for the induced dipole strength , proportional to the e field , and another three powers from the force , which is as the gradient of e . to calculate p requires knowing something about the material , namely it is dielectric polarizability constant for dc electric fields . this is radically different for different materials , depending on whether the molecules are polar themselves , and how mobile they are , whether it is liquid or solid . in general , you can figure this out from the extent to which the electric field in the interior of the dust is reduced from the exterior , assuming a bulk block of dust material . this is the dielectric constant $\epsilon$ of the dust . for a solid block of dust in a constant electric field e perpendicular to the plane surface of the dust-block , the electric field in the interior of the dust block is $e\over \epsilon$ where $\epsilon$ for dc fields is always bigger than 1 . for a metal , the field in the interior is zero , and this corresponds to infinite $\epsilon$ . i will calculate the polarization constant in an order of magnitude for metal dust , but for the largely nonpolar typical carbon-chain dust material , the actual answer will range from 1%-10% of the metal answer . for the metal , there is a nice useful trick for getting the induced dipole moment magnitude , which is the method of images/conformal maps . a dipole at the origin is conformally equivalent by an inversion around a sphere of certain radius to a constant electric field at infinity , so if you place a dipole at the origin , and the equal electric field inverted , the result makes the sphere have the same potential . the electric potential of a dipole is by differentiating the electric potential of a point charge : $$ \phi = {dz \over 4\pi r^3}$$ the potential of constant z-direction electric field is $$ \phi = ez$$ adding the two , the surface at potential zero ( only a zero potential stays an equipotential under inversion ) is a sphere : $$ ez - {dz\over 4\pi r^3} = 0$$ this is solved by a sphere ( away from the z=0 plane which is an accidental equipotential by symmetry ) $$ d = 4\pi e r^3 $$ and so the induced dipole moment is proportional to the cube of the sphere size . the coefficient p is $4\pi a^3$ for a metal spherical dust of radius a . so the answer for an ideal resistive metal dipole ( perfectly screening object , zero response time to adjust to a new field , these are good assumptions for this situation ) is $$ f = {q^2 a^3 \over 4\pi \epsilon_0 r^5} $$ where i restored $\epsilon_0$ in the final answer . for a non-metal resistive dipole , the induced moment is less by the facor $ ( 1-{1\over \epsilon} ) $ . for hydrocarbons , the dielectric constant is about 2 , so you get 1/2 the force . note than when r is of order a , the maximum force goes up as the object gets small .
non-associative algebras represent a challenge in quantum theory because they cannot be realized as linear operators on a hilbert space which are automatically associative by construction . these algebras appear in the classical description of a particle moving under the influence of a magnetic monopole in $\mathbb{r}^3$ as well as in string theory in backgrounds with nonvanishing $h$-field ( i.e. . , with nonconstant $b$-field ) . please , see for example the following review by dieter lüst . in both cases the classical phase spaces become 2-plectic manifold instead of symplectic . in the case of the magnetic monopole , the poisson bracket of two generalized momenta of a particle moving in a magnetic field background is given by : $\{\pi_i , \pi_j\} = ie\frac{\hbar}{c}\epsilon_{ijk}b^k ( x ) $ in this case , the finite translation operators $t ( \mathbf{a} ) = exp ( \frac{1}{i \hbar}\mathbf{a} . \mathbf{\pi} ) $ satisfy : $ ( t ( \mathbf{a_1} ) t ( \mathbf{a_2} ) ) t ( \mathbf{a_3} ) = exp ( -\frac{ie}{\hbar c} \phi ( \mathbf{a_1} , \mathbf{a_2} , \mathbf{a_3} ) ) t ( \mathbf{a_1} ) ( t ( \mathbf{a_2} ) t ( \mathbf{a_3} ) ) $ where $\phi ( \mathbf{a_1} , \mathbf{a_2} , \mathbf{a_3} ) $ is the flux through the tetrahedron generated by the three translations $\mathbf{a_1} , \mathbf{a_2} , \mathbf{a_3}$ . and the jacobi relation of three generalized momenta becomes $ \epsilon_{ijk}\{\pi_i , \{\pi_j , \pi_k\}\}= \frac{2e\hbar^2}{c}\mathbf{\nabla} . \mathbf{b}$ in the presence of a magnetic monopole , the flux through the tetrahedron is nonvanishing , leading to the loss of associativity of the finite translation operators . also $ \mathbf{\nabla} . \mathbf{b} \ne 0$ leading to the violation of the jacobi identity . due this violation , the poisson brackets are called twisted poisson brackets . in the magnetic monopole case , the quantization of the magnetic charge leads to the quantization of the flux through the tetrahedron and the algebra of the finite translation operators becomes associative . however , the jacobi identity is still violated in this case . this problem is avoided if the sources of magnetic charge lie outside the configuration space , then the jacobi relation is satisfied on it . in this case the configuration space will not be $\mathbb{r}^3$ . for example , if the particle is restricted to move on a two dimensional sphere , these problems would have been avoided . another way out is to declare the generalized momenta as nonohysical and work only with subalgebras of operators which satisfy the jacobi identity , for example , the angular momenta $j_k = \epsilon_{ijk} x_i\pi_j$ as mentioned above , nonassociative algebras cannot be represented by linear operators on a hilbert space . however , it is well known that the hilbert space description is not the most general description of quantum theory . in the case of nonassociativity , mainly due to the string theory applications , there are attempts to find quantization methods , in which nonassociative algebras can be represented . this subject is very new and not fully understood yet , please see for example the following article by mylonas schupp and szabo . the main tool used is deformation quantization which is one of the most general methods of quantization . the reason why this method works is that star products can be nonassociative , for example the moyal star product becomes nonassociative when the poisson bivector is not the reciprocal of a closed two form .
how can we measure meson decay constants ? i am not an experimental physicst , but i think that the best way to obtain the decay constant is to study processes like $\pi^+\to \mu^+ \nu$ and extract them from the branching ratio : $$\rm{br} ( \pi^+\to \mu^+\nu ) =\dfrac{g_f^2 m_{\pi^+} m_\mu^2}{8 \pi}\left ( 1-\dfrac{m_\mu^2}{m_{\pi^+}^2} \right ) ^2 f_{\pi^+}^2 |v_{ud}|^2 \tau_{\pi^+} , $$ which is measured nowadays with great precision . @dmckee answer 's suggests that we can also extract the decay constant from the pion form factor , but this method seems less precise , because it is more difficult to measure form factors than decay constants ( but maybe i am wrong . . . ) . if you take a look at pdg , you will see that the process $\pi^+\to \mu^+\nu$ is measured with an incredible precision . one last comment about decay constants : actually , these quantities can be computed for pions using lattice qcd methods and the theoretical error bars are comparable to the experimental ones ! you can even find very precise computations for more exotic mesons , like $d$ , $b$ and $b_s$ . for your theoretical question : it depends on the process you are considering ! for example , if you have $\pi^+\to \mu^+\nu$ , then you must take a second order term . in this term , you need a current $j^\mu_{q}$ related to the annihilation $u \bar{d}\to w^+$ and a leptonic current $j^\mu_\ell$ related to the creation $w^+\to\mu^+\nu$ . then , the time ordered product will only apply to the $w^+w^-$ term and it will give you simply the $w^+$ boson propagator . from my experience , i would suggest you to integrate-out the vector bosons , because the corrections to the fermi theory are negligible . in this case , you can write an effective hamiltonian : $$\mathcal{h}_{\text{eff}}=-\sqrt{2} g_f v_{ub} [ \overline{u}\gamma_\mu ( 1-\gamma_5 ) d ] [ \bar{\mu}_l \gamma^\mu {\nu_\mu}_l ] +\text{h . c . } , $$ and it is much simpler to read the amplitude and to relate it with the decay constant , because the hadronic part factorizes : $$\mathcal{a}=-i\langle \mu^+ , \nu | {h}_{\text{eff}} |\pi^+\rangle =i\sqrt{2} g_f v_{ub} \langle 0 |\overline{u}\gamma_\mu \gamma^5 d|\pi^+\rangle\cdot \bar{u} ( p_\nu ) [ \gamma^\mu ( 1-\gamma_5 ) /2 ] v ( p_\mu ) , $$ where $$\langle 0 |\overline{u}\gamma_\mu \gamma^5 d|\pi^+\rangle=-i p_\mu f_{\pi^+} . $$
decibels measure a ratio of power on a logarithmic scale . the decibel measure of a ratio between powers $p_1$ and $p_2$ is given by $10\log_{10} ( p_1/p_2 ) $ . dbm is an expression of power relative to 1 mw . dbw is similarly the ratio between some amount of power and 1 w . remember , db measure a ratio . dbm or dbw actually specify an amount of power . 100mw --> +10 db --> -2 db --> point ' x ' --> -2 db --> -2 db --> point ' y ' 100 mw is +20 dbm , because $10\log_{10} ( \dfrac{100\ \mathrm{mw}}{1\ \mathrm{mw}} ) $ is 20 . 20 + 10 is 30 . 30 - 2 is 28 . 28 - 2 is 26 . 26 - 2 is 24 . so after all these gains and losses you have +24 dbm . 24 dbm is 251.19 mw because $10^{\frac{24}{10}}$ is 251.19 . i was able to come up with an answer of ~251 mw by repeatedly using the power 2 = power 1 x 10^ ( gain or loss / 10 ) formula . it looks like you got the right answer , but i am not sure why you had to repeatedly use a complicated formula . after you convert the input power to dbm , it is just additions and subtractions to work out the output power . then convert back to mw if you want the answer in mw . that is the advantage of working on a log scale : multiplications and divisions turn into additions and subtractions .
this is somewhat controversial issue . but let me present the reasons , as far as i understood , why people like sir penrose thinks so . their arguments are roughly as follows : 1 ) the basic microscopic laws of physics are perfectly time symmetric . they are not biased in any time direction past or future . 2 ) second law follows from the fact that that given an initial condition of a system which is not in the most probable state will tend to go towards the most probable state by the same microscopic laws . since number of disordered states are much higher the system will become more and more disordered with time . accordingly its entropy will increase until a maximum value when the system comes to the thermal equilibrium . 3 ) since the microscopic laws are time symmetric the same argument can be made towards the past time direction as well . given an an initial condition of a system which is not in the most probable state should go towards more disordered ( high entropy ) states towards past as well . that is what the mathematics of the laws tells us . 4 ) this is against our experience . either all the parts of the universe we are observing ( including our memories of past ) has just undergone a huge fluctuation right now to give the impression that there was a more ordered past ( which is crazy ) or the system was already even more ordered ( low entropy ) and more special in the past . but that means even more huge a fluctuation . this reasoning will lead us to conclude that at the moment of big bang the universe was extra ordinarily ordered and most special . it should be so special that it requires explanation . critics often point out that prediction and retrodiction is not the same thing forgetting that when one talks about the very " arrow of time " no one can say with justification which is prediction and which is retrodiction . other than that it is also questionable whether second law can be applied this way to the whole universe or not .
the remaining 5% could in a normal , air-operated concentrator be `anything else ' that was present in the air . looking at this example and explanation , anything that ends up in the outlet stream is what was not absorbed . then the question remains : does absorption occur equally for all air components ( except $o_2$ of course ) or does it differ ? i can give you some numbers ( see also the presentation i referenced above ) , but the absorption depends strongly on the type of zeolite . for example this $aga$ zeolite has a 1.63:1 $ar$ selectivity and a 5:1 $n_2$ selectivity with respect to $o_2$ . whereas the $liagx$ zeolite has only 1.1:1 $ar$ selectivity . in this article they mention that they can get 95% $o_2$ with the remaining 5% being almost completely $ar$ . however , they use a contaminant free inlet mixture of $o_2$ , $n_2$ and $ar$ . the most interesting for you is probably this article . they study how $h_2o$ and $co_2$ affect the operation of a zeolite oxygen concentrator . what they show is that there is not going to be any $co_2$ in the outlet stream , but instead $co_2$ adsorbes so strongly on the zeolite that it will degrade its overall efficiency . in summary , to answer your question , species like $co_2$ and $no_2$ will deactivate your zeolite , but this is due to excessive adsorption so this means that the outlet flow will only contain $o_2$ and $ar$ and sometimes a small amount of $n_2$ .
you are asking so many questions that the only way to answer satisfactorily would be to completely rejustify special relativity . i suggest you take a look at a book like special relativity which does such a thing ! i will try to answer your first question : how does this happen ? if a photon is at position $ ( 0,0 , ct ) $ at time $t$ , that is , it has a spacetime event at time t of $e= ( ct , 0,0 , ct ) $ , one has to apply the lorentz transform , which would yield $e'= ( ct ' , x ' , y ' , z' ) = ( \gamma c t , -\beta\gamma ct , 0 , ct ) $ . so $ct=ct'/\gamma$ , and therefore : $$e'= ( c t ' , -\beta c t ' , 0 , ct'/\gamma ) $$ now , if we want to find the speed of the photon in this frame , that will be the magnitude of the spatial portion of the event $e'$ , differentiated by $t'$ ( and in this case , since everything is linear , that has the same effect as dividing by $t'$ ) : $$\| ( -\beta c , 0 , c/\gamma ) \|=\sqrt{c^2 ( \beta^2+ ( 1-\beta^2 ) ) }=c$$ it moves at the speed of light , still . the reduction in the y-direction , in this frame of reference , is a consequence of the lorentz transformation , which can be viewed as a consequence of the constancy of the speed of light . of course you can notice that if the motion in the y direction did not decrease , motion in the $x$ and $z$ directions would have to stay the same for the speed of light to stay constant , but this does not make sense , because it implies the beam of light is being dragged along with your reference frame . also , in lorentz transformation , suppose two observers measure a rod , can one of the observers see that the other person who is moving w.r.t. him , measure the rod to be less than his own measurement ? in other words , can the metre sticks of a moving observer get bigger than the observer who views the moving observer and sees himself to be at rest ? i will limit my reply to this . try to work it out from the definition of a lorentz transformation . the answer to " can one of the observers see that the other person who is moving w.r.t. him , measure the rod to be less than his own measurement " is yes . in your next sentence , if you mean to imply that some observer measures a meter stick at longer than a meter , the answer is no . a meter stick at rest is a meter , and a meter stick flying by ( along its axis ) in either direction is slightly shorter than a meter , by a factor of $1/\gamma$ .
i will assume you have heard something about proton decay . proton decay is a hypothetical form of radioactive decay in which the proton decays into lighter subatomic particles , such as a neutral pion and a positron . 1 there is currently no experimental evidence that proton decay occurs . there are a number of experiments which test whether such decays occur . for example a current one is here . there exist limits and in this review they are given depending on the theory that would predict proton decay . neutrons decay when free anyway into a proton an electron and an e-antineutrino . reappearance would require something like a new big bang , i.e. all matter/energy become a plasma and then by expansion the reappearance of nucleons .
as lurscher mentioned in a comment , you are using the wrong units for magnetic susceptibility . $\chi$ is actually a dimensionless number that is related to the magnetic permeability of a material relative to that of a vacuum . i think you were mixing it up with the molar magnetic susceptibility , which is $\chi_\text{mol} = \mathcal{m}\chi/\rho$ , where $\mathcal{m}$ is the molar mass of the substance ( units of $\mathrm{kg/mol}$ ) and $\rho$ is the density ( units of $\mathrm{kg/m^3}$ ) . $\chi_\text{mol}$ is the thing with units of $\mathrm{m^3/mol}$ , but it is $\chi$ that actually appears in the magnetic levitation formula . with that cleared up , let 's look at the equation . the left side , naturally , has units of $\mathrm{t^2/m}$ . if you include the magnetic constant on the right side , as wikipedia ( correctly ) does , you have $$\biggl [ \mu_0\frac{ \rho g }{\chi}\biggr ] = [ \mu_0 ] \frac{ [ \rho ] [ g ] }{ [ \chi ] } = \biggl ( \frac{\mathrm{t\ , m}}{\mathrm{a}}\biggr ) \frac{\mathrm{ ( kg/m^3 ) ( m/s^2 ) }}{1} = \frac{\mathrm{t\ , kg}}{\mathrm{m\ , s^2\ , a}}$$ here i am using the notation where putting brackets around a quantity designates the units of that quantity . for example , the units of the magnetic constant are $\mathrm{t\ , m/a}$ , so $ [ \mu_0 ] = \mathrm{t\ , m/a}$ . now you can equate the units of the two sides of the equation : $$\frac{\mathrm{t^2}}{\mathrm{m}} = \frac{\mathrm{t\ , kg}}{\mathrm{m\ , s^2\ , a}}$$ which simplifies to $$\mathrm{t} = \frac{\mathrm{kg}}{\mathrm{s^2\ , a}}$$ so if this equivalence is correct , then it shows that the original equation is dimensionally consistent . and if you look on the wikipedia page for the tesla , it does indeed give $\mathrm{t} = \frac{\mathrm{kg}}{\mathrm{s^2\ , a}}$ as one of the definitions of that unit . alternatively , you could check it using a formula involving magnetic field and current , such as $\vec{f} = i\mathrm{d}\vec{l}\times\vec{b}$ . the units of this are $\mathrm{n = a\ , m\times t}$ , and since $\mathrm{n} = \mathrm{kg\ , m/s^2}$ , you can set $\mathrm{kg\ , m/s^2 = a\ , m\ , t}$ and find exactly that $\mathrm{t} = \frac{\mathrm{kg}}{\mathrm{s^2\ , a}}$ . this is a useful trick that keeps you from having to memorize the definitions of all the si ( or other ) units .
i am assuming from your drawing that $f_{seafloorresistance}$ refers to the normal force that the seafloor applies on the bottom of the block when it is resting on the seafloor . your question seems to be " what is pushing on the bottom of the block if it is not touching the seafloor but also has no water beneath it ? " this question seems to be an illustration of what happens when the idealized " frictionless plane " world of physics class runs into the slightly messier reality in which we live . the same hydrostatic pressure experienced by the top of the block is experienced by the sides of the block as well , but of course this does not contribute to the net vertical force . in " the real world " , this pressure would cause water to fill in all cracks and crevices at any opportunity , allowing water to get beneath the block and balance the downward force from the hydrostatic pressure above . now you stipulated that everything is " perfectly polished " , presumably meaning there is no crack or crevice through which water can flow . let 's imagine there is dent in the middle of the bottom face of the block with no air in it ( a vacuum ) ; the dent can be as small as you like . your idealized , completely smooth ( except the tiny dent ) , un-deformable block has just become the perfect suction cup . in this case , you are right that much more force would be required to lift the block : the force would be $a \cdot p$ , where $a$ is the area of the top surface of the block and $p$ is the hydrostatic pressure of the seawater . ( note : you do not need the dent at all : as long as there is no water pushing below , you will have a suction cup scenario . i just put the dent there to conjure up the image of a suction cup . )
the reflector focuses the light from the bulb which is a point source to a straight line .
the no-go results from algebraic and constructive qft you mention deal with related but slightly different matters . ( edit : the previous version of the following paragraph was slightly misleading - haag 's theorem is actually stronger than i stated before ; see below for details ) haag 's theorem ( which actually slightly predates the inception of algebraic qft ) tells us that we cannot write interaction picture dynamics within hilbert spaces which are free field representations of the ccr 's . this is not the same as to say that interacting dynamics does not exist at all - it simply says that we cannot implement it as unitary operators in the interaction picture . this is done by showing that the possibility to do so in some hilbert space does imply that we are dealing with a free field representation of the ccr 's . the argument is closed by a " soft triviality " result by jost , schroer and pohlmeyer arguing that the latter implies that all truncated $n$-point functions vanish for $n&gt ; 2$ , hence the field is really free - in particular , the " interaction hamiltonian " is zero . this has consequences for both scattering theory and attempts to rigorously construct field theoretical models starting from free fields . in the first case , haag 's theorem is circumvented by either the lsz of haag-ruelle scattering formalisms , which obtain the s-matrix by respectively taking infinite time limits in the weak ( matrix elements ) and strong ( hilbert space vectors ) sense . recall that both setups require the assumption of a mass gap in the joint energy-momentum spectrum ( i.e. . an isolated , non-zero mass shell ) , otherwise we run into the notorious " infrared catastrophe " , which is dealt with using " non-recoil " ( i.e. . bloch-nordsieck ) approximation methods in formal perturbation theory but remains a challenge in a more rigorous setting , save in some non-relativistic models . in the second case , one is led to consider representations of the ccr 's which are inequivalent to free field ones . since field theories living in the whole space-time have infinite degrees of freedom , the stone-von neumann uniqueness theorem no longer holds ( actually , haag 's theorem can be seen as a manifestation of this particular failure mechanism ) , and hence such representations should exist in abundance . motivated by these results , algebraic qft was devised with a focus on structural ( i.e. . " model-independent" ) aspects of qft in a way that does not depend on a particular representation ; on other front , one may also try to explore this abundance of representations to construct models rigorously , which brings us to the realm of constructive qft . the " existence " ( a . k . a " non-triviality" ) and " non-existence " ( a . k.a. " triviality" ) results in constructive qft tell us which interactions survive after non-perturbative renormalization . more precisely , you construct field theoretical models in a mathematically rigorous way by first considering " truncated " interacting theories ( i.e. . with uv and ir cutoffs ) , and then carefully removing the cutoffs in a sequence of controlled operations . the resulting model may be interacting ( i.e. . " non-trivial" ) or not ( i.e. . " trivial" ) , in the sense that its truncated $n$-point correlation functions for $n&gt ; 2$ may be respectively non-vanishing or not . in the first case , any representation of the ccr 's in the hilbert space where the interacting vacuum state vector lives is necessarily inequivalent to a free field one - in particular , one cannot write the interacting dynamics as unitary operators in the interaction picture , in accordance with haag 's theorem . in the second case , you really obtain a free field representation of the ccr 's , but here because renormalization has completely killed the interaction . finally , it is important to notice that triviality of a model may stem from reasons unrelated to the underlying mechanism of haag 's theorem . the latter , once more , is a consequence of having an infinite number of degrees of freedom in infinite volumes ( this theorem does not hold " in a box " , for instance ) , whereas the former usually derives from an interaction which has too singular a short-distance behavior , as argued in the previous paragraph . this can be intuitively be understood by the ( local ) singularity and ( global ) integrability of the free field 's green functions : the lower the space-time dimension , the better the singular ( uv ) behaviour and the worse the integrability ( ir ) behaviour , and vice-versa . that is the underlying reason why $\lambda\phi^4$ scalar models are super-renormalizable in 2 and 3 dimensions ( having only tadpole feynman graphs as divergent in 2 dimensions ) and non-perturbatively trivial in $&gt ; 4$ dimensions . ah , i have almost forgotten about the references : in my opinion , the best discussion of triviality results in qft from a rigorous viewpoint is the book by r . fernández , j . fröhlich and a . d . sokal , " random walks , critical phenomena , and triviality in quantum field theory " ( springer-verlag , 1992 ) , specially chapter 13 . there both the above " hard triviality " results for $\lambda\phi^4$ models and " soft triviality results " such as the jost-schroer-pohlmeyer theorem ( which underlies haag 's theorem , as mentioned at the beginning of my answer ) are discussed . the book is not exactly for the faint of the heart , but the first sections of this chapter provide a good discussion of the statements of the theorems , before proceeding to the proofs of the above " hard triviality " results . for a detailed discussion of jost-schroer-pohlmeyer 's and haag 's theorems , as well as their proofs , i recommend the book of j . t . lopuszanski , " an introduction to symmetry and supersymmetry in quantum field theory " ( world scientific , 1991 ) . the classic book of r . f . streater and a . s . wightman , " pct , spin and statistics , and all that " ( princeton univ . press ) also discusses these two results .
http://www.nature.com/ncomms/journal/v2/n4/full/ncomms1263.html above is a reference with interference of 6 nm molecules achieved here is a proposal to use much larger molecules in space : http://arxiv.org/abs/1201.4756 here is a proposal to use larger molecules in an optical interfereometer : http://arxiv.org/abs/1103.4081
if you think of the two parallel glass sides as canceling each other out you are pretty close to it . the first impact ( low to high indices ) does in fact disperse frequencies if the light is coming in at an angle , but the exits ( high to low ) mostly cancels that effect . there actually can be some small residual effects leading to small colored fringes . the prism works better because the oppositely angled sides enhance rather than cancel the dispersion effects . here are the results of some further analysis and experimentation . part of the answer for why color effects are so hard to find when light passes through flat glass plates appears to be in the eye of the beholder . . . literally ! here 's the scoop : angled light entering a flat plate should at first fan out its angles by color while within the plate . by symmetry , however , those slightly fanned out rays of colored light return to their original paths when they reach the second surface an re-emerge . so , the new rays will show essentially no difference in direction from their paths in the original beam , but will no be spaced very slightly apart from each other in rainbow order . for a typical plate of glass this separation would seldom be more than a millimeter , and for most glasses would be a lot less than that . now picture a point of light on one side of the glass and a human eye on the other side . arrange both so that the line between then is at a sharp angle to the surface of the glass . let 's look at the ray going from the point to the center of you pupil . your eye focuses that parallel white light as a single point on the retina , as expected . but when the angled glass plate is inserted , the same ray of light gets spread out along a tiny distance , usually much less than a millimeter . however , each colored ray in this bundle remains parallel to the original path . this little sub-millimeter bundle then enters the pupil of the eye , carrying pretty much the same light as before , all traveling in parallel . what does your eye do with it ? it forms the same white colored point image as before , since the light is all traveling in parallel . think for example of red and blue light entering opposite sides of a magnifying glass : both will end up near the center . there will be some chromatic aberration , sure , but it turns out that vertebrate eyes are very , very good at eliminating that form of chromatic aberration at the image level . the bottom line becomes this : as long as the plate is not too thick , the physical separation of chromatic components will fall within the size of the human eye pupil , and the image will appear to be white - color free and pretty much just like the original , with just a bit more blurring . that also leads to an experimental prediction that i have not yet tried : if you hold a pinhole in front of your eye while observing a pinhole light on the other side of an angled piece of glass , you may be able to see a short colored line instead of a white dot . i can not guarantee it , but it is likely enough that it would be interesting to try . now to the final part of the analysis : what if the glass is so hugely thick that there is no way the separated components can be captured by the human eye all at once ? should not that lead to some visible color effects , such as blue and red fringes on either side of a point of white light ? specifically , for a white dot or light , a blue fringe should appear towards the side angled away from the viewer , and a red fringe on the side of the glass that is closer to the viewer . for a black line on a light background this would be reversed , with the red on the glass-is-farther edge of the black line ( since that is the nearer edge of the lighter part ) , and blue on the glass-is-nearer edge of the black line . ( you can work out why that is with simple dispersion diagrams . ) but since the space-form chromatic separation effect is going to be small even for a quite thick piece of glass , where can you find something thick enough to show such fringes ? fish lovers have conveniently provided a solution : they are called aquariums ! the combination of glass and water makes a quite good approximation of a very thick piece of glass with decent chromatic dispersion . but does it really work ? if like me you do not currently have an aquarium , here is a convenient online image of a see-through aquarium that is angled sharply away on the right . on the other side of the tank are both vertical bright lights from curtain folds ( near the right side ) , and vertical dark lines from a picture frame ( on the left ) . if you magnify the image , you will see blue fringes on the right sides of the curtain folds . neither effect is intense , but both are definitely in this image . if you do happen to have an aquarium , you should of course try it for yourself , since good direct experiment always trumps theory if they disagree ! do not look directly into a light , since modern led lights are very bright and should never be gazed at directly . instead , place a thin vertical stripe of white paper on a black background and illuminate that with a bright light pointing away from the observer . you can also try holding a small pinhole in aluminum foil in front of you eye to enhance any color fringing effect you may see . and with that . . . i think i will give this one a rest . further discussion , especially actual results from experimenters with real aquariums , would be great though !
op is formally correct that $$\frac{\partial}{\partial x^{\mu}}~\in~ \gamma ( tm|_{u} ) $$ is a vector field ( defined in a local coordinate neighborhood $u$ ) , and not a one-form . what weinberg simply means by casually saying that the partial derivative operator $\partial/\partial x^\mu$ is a covariant vector , or in other words a 1-form , [ . . . ] is just that the local basis of vector fields $$\tag{1} \frac{\partial}{\partial x^{\mu}} ~=~\frac{\partial y^{\nu}}{\partial x^{\mu}} \frac{\partial}{\partial y^{\nu}}$$ transforms in the same way as the components $$\tag{2} \eta^{ ( x ) }_{\mu}~=~\frac{\partial y^{\nu}}{\partial x^{\mu}}\eta^{ ( y ) }_{\nu} $$ of a 1-form/covector [ or a covariant ( 0,1 ) tensor ] $$\eta~=~\eta^{ ( x ) }_{\mu} dx^{\mu} ~=~\eta^{ ( y ) }_{\nu} dy^{\nu}$$ under a local coordinate transformation $x^{\mu} ~\to~ y^{\nu}=y^{\nu} ( x ) $ . the point is that the ( traditional ) physicist often thinks of a $ ( r , s ) $ tensor $$t~=~ \frac{\partial}{\partial x^{\mu_1}}\otimes \cdots \otimes \frac{\partial}{\partial x^{\mu_r}} ~t^{\mu_1\ldots\mu_r}{}_{\nu_1\ldots\nu_s} ~ dx^{\nu_1}\otimes \cdots \otimes dx^{\nu_s}$$ merely in terms of its components $t^{\mu_1\ldots\mu_r}{}_{\nu_1\ldots\nu_s}$ , and in particular , the transformation property thereof under local coordinate transformations . local basis elements , such as , e.g. , $\frac{\partial}{\partial x^{\mu}}$ and $dx^{\nu}$ are often viewed as merely bookkeeping devices . in conclusion , ref . 1 is probably not the best textbook to learn differential geometry from . for instance , already in eq . ( 4.11.12 ) on the very next page 116 weinberg claims that the fact that the exterior derivative squares to zero , $d^2=0$ , is known as poincare 's lemma . this is definitely not correct , cf . wikipedia : the identity $d^2=0$ means that exact forms are closed , while poincare 's lemma states that the opposite holds locally : closed forms are locally exact ( except for zero-forms ) . references : s . weinberg , gravitation and cosmology , 1972 .
i cant speak in the case of full generality , but at least for schwarzschild and reissner-nordstrom ( electrically charged ) non-rotating black holes , there exists a coordinate system called ' isotropic coordinates' ; it turns out that in these coordinate systems the ' interior ' and ' exterior ' regions are actually isomorphic ; this means that representing the solution in these coordinates gives rise to two identical yet causally disconnected regions of space-time . these two space-times share a common boundary , namely the event horizon . as such , the two regions can be thought of representing the same physics and the answer to your question is then a definite ' yes': the information contained within the ' interior ' section also amasses on the horizon .
you are correct that you can not really use a gaussian of length 10 meters . instead , consider using a very short gaussian cylinder near your area of interest . in this limit , you should still be able to approximate the electric field as " nice and symmetric " with respect to the area vectors $d\vec{a}$ . by the way , the same thing is done for finite sheets of charge . if you use a small enough gaussian surface that is located near the center of the sheet , the procedure and results become the same as that due to an infinite sheet . the only difference is the limited region of applicability .
the quantum number $n$ of the harmonical oscillator runs from $0$ to $\infty$ . ( your sum starts at 1 . ) $\sum_{n=0}^{\infty} e^{-\theta ( n+1/2 ) } = \frac{e^{-\theta/2}}{1-e^{-\theta}} = \frac{e^{\theta/2}}{e^\theta - 1} = \frac{1}{e^{\theta/2}-e^{-\theta/2}}$ . i guess there just is an error in your exercise . ( tas make mistakes , too . ) the faq says no homework questions . let 's hope they do not tar and feather us . ; - )
i agree that the definition " particles are a small oscillation around a vacuum " is not correct , it is some attempt to apply semi-classical reasoning via the path integral to qft . however , it does not mean anything for a field to be " physical , " fields are not physical . they are a computational tool that allow us to ensure that amplitudes we write down are local and causal . therefore , zero-vev cannot be a physical principle , nor can it be derived from one . the physical question is : does the vacuum of the theory respect the symmetry in question . if the answer is no , the physical consequence is that there exist massless excitations . to see an example of this , consider the s-matrix field redefinition theorem . lsz reduction basically tells you that it does not matter which field operator you use to do calculations , as long as the two fields have overlap on the 1- particle states and you compensate your choice with an appropriate normalization factor and put external lines on-shell ( this is explained properly in weinberg vol 1 ) . the definition of the s-matrix involves a specification of asymptotic states . specifically , you assume that the asymptotic states are 1 particle states of some free hamiltonian ( usually assumed to be the hamiltonian you are perturbing about , although this assumption fails for theories like qcd in the ir ) . in particular , it assumes a particular vacuum state from which particle excitations may be built using creation operators associated with the free hamiltonian . so while it does not matter which field you use to calculate correlators , in order for lsz reduction to work you have to put the external states on shell . since you understand how ssb works , you know that you can show in many different ways that there must be a massless particle in the spectrum . so the point is this : it is not that the field with no vev is any more " physical " than the regular field , this is a meaningless statement . but if you are intending to read off the masses of external states from the tree level lagrangian , you will be doing the wrong calculation unless some of the fields in the lagrangian are massless . if you have assumed that ssb has occurred and the vacuum is no longer a singlet , then you need to be using m=0 when you compute amplitudes for the goldstone mode scattering . and you need the other feynman rules to be consistent with this feynman rule . so you make the expansion you refer to so that you may derive feynman rules that consistently treat the external state as massless and consistently describe the other interactions . if you do not do this , you are doing the wrong calculation , using asymptotic states that do not actually exist in your theory . hope this helps !
the rule of thumb for me is how would the solutions of the schrodinger equation look for this potential . when the energy of the electron is above the potential well the solutions can be continuous ( although there can be discrete resonances with an energy width ) . below the lip of the potential , as for example in the hydrogen atom , the solutions will be discrete , quantized . if the walls of the potential go to infinity , as in your example , then all the solutions will be quantized as in this example of the particle in a box . in the limited in energy potential ( not going to infinity ) the energy levels with large energy quantum numbers are very dense and resemble a continuum , although they can be labeled with a discrete number , as for example in the hydrogen atom . above the energy of the well are the continuum solutions , not discrete .
kirchhoff 's loop rule is also called kirchhoff 's voltage law ( kvl ) . which is different from kirchhoff 's current rule which is also called kirchhoff 's current law ( kcl ) . kvl is derived from maxwell–faraday equation for static magnetic field ( i.e. . the derivative of b with respect to time is zero ) . kcl is derived from charge continuity equation which is equation 3 here . a well known case in which kvl does not apply is when having a varying magnetic field enclosed by the circuit being studied . the presence of time varying magnetic field makes the measured voltage non-unique ( depends on the branch used to measure the voltage ) . have a look at page 3 of this presentation . a well known case in which kcl is limited is when having a voltage source with very high frequency such that effects like parasitic capacitance can no longer be ignored . in those cases wires ( or conducors ) are treated as transmission lines . in such a case a current can flow even in an open circuit . . for further information have a look at limitations sections of kcl and kvl here .
the wind is certainly doing work , because it applies a force and the point where the force is applied is displaced . however it is not doing any work on the boat , it is doing the work on the water . the key point is that the net force on the boat is zero . we know the net force on the boat is zero because the boat is moving at constant velocity - if the net force were non-zero the boat would be accelerating . since the net force on the boat is zero no work is being done on the boat . the velocity of the boat is constant because the drag of the water is balancing out the force applied by the wind . so overall the wind is applying a force to the water - the boat is just the instrument through which the force from the wind is communicated to the water . so the wind is doing work on the water , but not on the boat .
imagine the anchor is hanging from the bottom of the boat , dangling mid-water like this : ( there is no difference between the anchor hanging in the water and sitting in the boat , since the system boat + anchor weighs the same either way . ) the water level depends on how heavy the anchor is . if you make the anchor heavier , it pulls the boat down further , pushing the water out of the way . this water goes out to the sides and raises the water level a bit . if you make the anchor lighter , the boat rises up some , leaving a gap . water rushes in underneath the boat , and the water level goes down . imagine making the chain longer until at last the anchor starts to rest on the bottom of the tank . since the bottom of the tank is supporting the anchor , it does not pull down on the boat as much . from the boat 's perspective , it is as if the anchor got lighter . thus , the boat rises and the water level falls . we must assume that the anchor is more dense than water , but that is all . if you wanted to calculate how much the water level falls , you would need to know the density and weight of the anchor .
the particles of light waves - the photons - have the rest mass $m_0$ equal to zero . however , at the speed of light , $v=c$ , the total mass $$ m= \frac{m_0}{\sqrt{1-v^2/c^2}} $$ is increased to an indeterminate form , $0/0$ , which should be evaluated as a finite number . the photons - and everything else - carry the total mass that is proportional to the total energy via the famous $e=mc^2$ relation . yes , this mass may be measured . for example , uranium nuclear power plants burn the uranium and reduce its mass by 0.1 percent or so because the waste products ( the nuclei ) are actually a little bit lighter . this energy may be completely transformed to the radiation coming from light bulbs - and the light from these light bulbs carry 0.1 percent of the uranium mass away . this mass is a source of gravitational field and adds inertia to boxes with this light etc . sound is different . the speed of sound is much smaller than the speed of light . while " phonons " in low-temperature condensed matter physics - particles of sound - are analogous to photons in many respects , and $e=mc^2$ still applies , the same is not true for sound waves in the air etc . because the temperature of the air is nonzero , the " ground state " - the lowest-energy state at fixed conditions , with the minimum number of " sound quanta " or " phonons " - is not really unique . instead , there are many states of the air " without any sound " which correspond to chaotic configurations of the air molecules . so one can not consistently divide the energy of the air to the energy of its ground state and the energy of the phonons . but of course , if you produce some loud sounds , they will carry lots of energy in the air and the mass of the air will inevitably increase by $m=e/c^2$ which is , well , not too high because $c^2$ is a large number .
the rutherford model of the atom did not respect any quantization : it was a classical planetary model . the bohr-sommerfeld model had the quantization of the allowed orbit from your first picture ; however , you conflated these two models and spoke about " rutherford-bohr " model which has never existed . the third thing that you conflated is the actual quantum mechanical equation that describes the atom correctly - in the non-relativistic limit - while neither the rutherford model nor the bohr model are correct in details . the states $1s , 2s , 2p , 3s , 3p , 3d , \dots$ that you refer to on your second and third picture only exist in the correct quantum mechanical model that predicts three quantum numbers for the electron , $n , l , m$ ( if we ignore the spin ) . the bohr model only predicts ( incorrectly ) one quantum number $n$ , so it would only have states $n=1,2,3,4$ and no extra $s , p , d$ labels that distinguish different values of $l$ . in some sense , the bohr model has angular momentum $l=n$ and it does not allow any values $l&lt ; n$ while the correct quantum mechanical models only allows $l&lt ; n$ but all of them - it predicts $l=0,1,2 , \dots n-1$ . so you should recognize the different models . the rutherford model is classical and hopeless - and only included the insight that the nuclei are smaller than the atoms . it did not know anything correct about the motion of the electron . the bohr-sommerfeld model knew something " qualitative " about the motion of the electron , namely that there was something quantized about it , but it was still too classical and it was the wrong model that only happened to " predict " the right energies after some adjustments but this agreement for the hydrogen atom was completely coincidental and related to the fact that the hydrogen atom may be solved exactly ( and the answer for the allowed energies is very simple ) . so the answer how you can see $1s , 2s , 2p , \dots$ states in the bohr ( or even rutherford ) model is obviously that you can not see them because the bohr and rutherford models are invalid models of these detailed features of the atom . if you decided to learn quantum mechanics and abandoned the naive ideas such as the rutherford and bohr-sommerfeld models , you could also discuss other properties of the electron states in the hydrogen atom . for example , the states $2px , 2py , 2pz$ from your first picture are particular complex linear combinations of the usual basis of states $2p_{m=-1} , 2p_{m=+1} , 2p_{m=0}$ . in fact , $2pz=2p_{m=0}$ while $2px\pm i\cdot 2py = 2p_{m=\pm 1}$ , up to normalization factors .
first , i have to correct you : compactifications of string theory clearly do not give you just " supergravity " – which is an inconsistent theory at short distances . they give you the full string theory , a compactified one . type iia and type iib are t-dual to each other . it means that the compactification of one on a circle $s^1$ of radius $r$ is equivalent to the compactification of the other on a circle of radius $\alpha ' / r$ where $\alpha'$ is the squared string length ( the regge slope ) or $1/r$ in some natural " string units " of length . t-duality is covered in every modern textbook of string theory and to reliably explain how it works , one would have to reproduce several chapters of an introduction to string theory here which is counterproductive . it is ultimately a symmetry because it is a parity i.e. a sign flip of $\partial_z x^9$ that is only applied to the left-movers but not to the right-movers $\partial_{\bar z} x^9$ . that is what interchanges $\int d\sigma \partial_\tau x^9$ , a total momentum component , with $\int d_\sigma \partial_\sigma x^9$ , the total winding . the left-movers and right-movers may be treated rather separately in cfts because of the conformal symmetry . you talk about the compactifications on $t^2$ which may be achieved by compactifying one more dimension . because the two $s^1$ compactifications – of type iia and type iib – were already equivalent to each other , they remain equivalent to each other if you compactify one more dimension , of course . the moduli spaces obtained by compactifying m-theory or type iia or type iib on tori are all identical – and they still have a u-duality group identifying different choices of the radii and other parameters . the u-duality group for m-theory on $t^k$ is formally $e_{k ( k ) } ( z ) $ which is only truly exceptional if $k$ is greater than five . there is no conflict with chirality because the theory with 8+1=9 large dimensions that you get by compactifying type iia or type iib on a circle is already non-chiral – after all , in an odd spacetime dimension , almost all theories are non-chiral . for example , there are no weyl ( chiral ) spinors in an odd space or spacetime dimension . the only chiral vacuum among the compactifications of m-theory , type iia , or type iib theories on tori is the uncompactified type iib string theory in 10 dimensions . everything else is non-chiral . you lose the chirality once you compactify a theory on a circle . the left-right symmetry of the lower-dimensional theory is guaranteed because it may be obtained by flipping the sign of one large ( remaining ) dimension as well as one compactified ( circular ) dimension and this flip of two dimensions is just a 180-degree rotation which is always a symmetry . so parity symmetry can not be violated by the circular compactifications .
i will assume that " enhance the vibrancy of colour " means something like turning up the saturation in a photo editing application . i think it is highly unlikely that this could be accomplished using analogue optics , because it is a highly nonlinear operation . this is because we need to block or let through light in three different frequency ranges ( red , green and blue ) by an amount that depends on the amount of light of other frequencies that is also hitting the filter . for example , suppose there is a small amount of blue light hitting one part of the filter . if this is the only light hitting it then it will be perceived as a dark but vibrant blue and we should let as much through as possible . however , if there is also a larger amount of red and green light hitting the filter then the blue light is merely diluting the saturation of a yellow shade , making it appear greyish , and we should block some of the blue light while letting the other frequencies through . this sort of thing would need to happen reliably across a very wide range of light intensities . while materials that have non-linear effects on the frequency of light do exist , creating something with properties this specific is probably impossible . of course , it is possible to enhance the saturation of an image digitally , so could we do that in something the size of a contact lens ? i strongly suspect this is also impossible , for a few different reasons . it is certainly far beyond the reach of modern technology . the first reason is the miniaturisation of the screen . it would have to have an enormous resolution , a dynamic range ( difference between the lowest and highest intensities it can emit ) many orders of magnitude higher than any currently available display , and draw an incredibly tiny amount of power . the second reason is the camera . not just because of the resolution , sensitivity and size of the sensor , but also because the distance between this camera 's lens and its sensor would have to be less than a millimetre , and i do not think there is any trick that would allow a sharp image to be produced under such circumstances . finally , the eye can not focus on something that is attached to its surface , so the screen would somehow have to emit parallel rays of light , rather than emitting light in all directions like a normal screen . i do not know whether this is possible for such a small device , but i suspect not . so unfortunately the answer to your question is , this certainly will not be possible in the near future , and it probably never will . on the other hand , if you do not mind wearing goggles then it is possible with current technology : you just need a virtual reality helmet with cameras on the front . i have tried such a device , and it is surprising how much it feels like you are not looking through a screen .
you seem to have the right idea with your equation , but some practical difficulties . first , i would recommend combining your independent variables into simply $\vec{r}$ rather than the $\vec{x}$ and $\vec{r}$ , as you have it right now . rocket motion is very complicated , however , and consider that nearly all of your forces will be functions of $\vec{r}$ and $\dot{\vec{r}}$ ( e . g . air resistance is a function of velocity as well as position - you currently have it as only a function of position ) . after solving the final equation ( presumably numerically , since a symbolic result for such a complex relationship will not be closed-form ) , you will get a function which models the motion of your rocket at a given time . to account for " parabolic motion , " by which i think you mean " turning , " i would make thrust a function of time . for example , you may want to fire the thrusters 60 minutes into the flight to adjust the orbit , so you would define $t$ to have a certain magnitude and direction when $t = 60 \text{ min}$ .
it is true that at the speed of sound , you will have a huge amount of drag . the reason is that the air in front of you has to move out of the way , and if you are moving at the speed of sound , the pressure wave that pushes the air out of the way is moving at exactly the same speed as you . so in the continuum mechanics limit , you can not push the air out of the way , and you might as well be plowing into a brick wall . but we do not live in a continuum mechanics universe , we live in a world made of atoms , and the atoms in a gas bounce off your airplane . at the speed of sound , you get a large finite push-back which is a barrier , and above this , you still have to do the work to push a mass of air out of the way equal to your plane 's cross section with ballistic particles . as you go faster , the amount of drag decreases , since the atomic collisions do not lead to a pile-up on the nose-cone . but if you look at wikipedia 's plot here , the maximum drag at the supersonic transition is only a factor of 2 or 3 higher than the drag at higher supersonic speed , so it is possible to travel at mach 1 , it is just not very fuel efficient .
it is unusually symmetric . all four nucleons are in 1s spatial orbitals , in singlet pairs of spin and of isospin . the more symmetry as system has , the lower its energy .
the field strength is the force on a unit charge , so the field strength at the surface of sphere 1 is : $$ f_1 = \frac{1}{4\pi\epsilon_0} \frac{q_1 . 1}{r_1^2} $$ and the field strength at the surface of the second sphere is : $$ f_2 = \frac{1}{4\pi\epsilon_0} \frac{q_2 . 1}{r_2^2} $$ lets take the ratio $f_1/f_2$ to see which is greater . the constants cancel to give us : $$ \frac{f_1}{f_2} = \frac{\frac{q_1}{r_1^2}}{\frac{q_2}{r_2^2}} $$ and i am going to rewrite this slightly to make it obvious how you use your equality $q_1/r_1 = q_2/r_2$: $$ \frac{f_1}{f_2} = \frac{\frac{1}{r_1}\frac{q_1}{r_1}}{\frac{1}{r_2}\frac{q_2}{r_2}} $$ because $q_1/r_1 = q_2/r_2$ we can cancel them on the top and bottom of the fraction and we are left with : $$ \frac{f_1}{f_2} = \frac{r_2}{r_1} $$ and because $r_2 &lt ; r_1$ this means the field strength at the surface of sphere 2 is greater than at the surface of sphere 1 .
your question can not be answered because the qualifier when it was only the size of our solar system is meaningless . the size of the universe is a rather vague concept . the universe may well be infinite ( it is unlikely we will ever know for sure ) in which case it was always infinite and it does not have a size . you could ask about the size of the observable universe i.e. the bit we can see , but even that is tricky . we can see about 13 billion light years , but the bits we see 13 billion light years away we are seeing as they were 13 billion years ago . the current distance of those bits is about 46 billion light years . so is the size ( radius ) of the observable universe 13 billion light years or 46 billion light years ? but i think we can address the spirit of your question if not the exact text . there is a well defined measure of size that works even for an infinite universe called the scale factor . if you take two points in the universe then the distance between those points changes with time according to the equation : $$ d = a ( t ) d_0 $$ where $a ( t ) $ is called the scale factor and $d_0$ is a constant . it is conventional to define $a ( t ) $ to be $1$ at the current time , in which case $d_0$ is the current distance . the average density of the universe is then given by : $$ \rho ( t ) = \frac{\rho_0}{a^3 ( t ) } $$ where $\rho_0$ is the current average density . we can use this equation to work out at what time the density of the universe was equal to that of a neutron star . the scale factor is calculated by solving einstein 's equation for a homogenous isotropic universe , and the result is the flrw metric . as you would probably expect for anything related to general relativity this does not give a simple answer . as pulsar explains here the scale factor is given by : $$ \begin{align} t ( a ) and = \int_0^a \frac{a'\ , \text{d}a'}{a'^2h ( a' ) }\\ and = \frac{1}{h_0}\int_0^a \frac{a'\ , \text{d}a'}{\sqrt{\omega_{r , 0} + \omega_{m , 0}\ , a ' + \omega_{k , 0}\ , a'^2 + \omega_{\lambda , 0}\ , a'^4}} , \end{align} $$ you have to calculate $a ( t ) $ by numerically integrating this expression , but this is not as hard as it looks and i did it in excel to get : there are some interesting features to this . up to about 6gyrs after the big bang the expansion was slowing as you had expect because the mutual gravity of all the matter is decelerating it . however more recently dark energy has been causing the expansion to accelerate , and you can just see the line beginning to curve upwards . this curvature will get more pronounced in the next few tens of billions of years . anyhow , back to your question . the current radius of the observable universe is about 46 billion light years , and the radius of neptune 's orbit is about $4.5 \times 10^{12}$ m , so the ratio of the two is about $9.7 \times 10^{13}$ . the current density ( including dark matter and dark energy ) is about 5 hydrogen atoms per cubic metre , so the density when the observable universe was the size of neptune 's orbit was about $5 \times 10^{42}$ atoms of hydrogen per cubic metre . this is about $7.5 \times 10^{15}$ kg/m$^3$ . the density of a neutron star is around $5 \times 10^{17}$ kg/m$^3$ so actually the density of the universe at this time was not that far off neutron star densities . we can use our calculation of $a ( t ) $ to work out what time the universe had this density , and it comes out to be about $10^{-25}$ seconds . however i would be cautious about this number because the calculated time lies within the range that the electroweak symmetry breaking was happening , and this may include new factors that influence the scale factor .
however , they must somehow occupy space , as i have read that light waves can collide with one another . that is not true . yes , light waves can " collide " and interact with each other ( rarely ) , but that itself does not imply that they need to occupy space . it is not even entirely clear what it means for a subatomic particle to occupy space . a particle like a photon is a disturbance in a quantum field , and is " spread out " across space in a sense ; it does not have a definite size in the same sense that a macroscopic material object does . but you will probably agree that , if it is possible to make any sensible definition of " occupying space " for a subatomic particle , it should involve preventing other things from also occupying that same space . photons do not do that . they are bosons , and as a consequence of that they are not subject to the pauli exclusion principle , so if you have a photon occupying some space ( whatever that may mean ) , you can in theory pack an unlimited number of additional photons into the same space .
the sentence in peskin 's and schroeder 's book that " the weak interactions preserve cp and t " is a bit misleading but there is a sense in which it is right . experimentally , cp and t is known to be violated and cpt is always a symmetry . theoretically , cpt is always a symmetry , too – it is proven by the cpt theorem . the cpt transformation is effectively a rotation by $\pi$ in the $t_ez$ plane in the euclideanized spacetime which is a symmetry due to the lorentz symmetry and analyticity of the theory ( the charge conjugation is automatically added because by sending the particles backwards in time , they become antiparticles , so the geometric operation is physically interpreted as cpt and not just pt ) . theoretically , we know that cp and t may be violated . note that because cpt is always a symmetry , a theory is symmetric under cp if and only if it is symmetric under t . there are various potential physical phenomena that violate cp and t – including the " theta angle " of qcd ( which would mean that the strong force also breaks cp and t ) – but the only experimentally observed source of cp violation is the " complex phase in the ckm matrix " . the ckm matrix is a unitary transformation that transforms the upper-type quark mass eigenstates to the $su ( 2 ) _w$ upper partners of the down-type quark mass eigenstates . all the quark masses are generated by the higgs mechanism – from the yukawa couplings and the vev – and all the higgs-related things may be incorporated into the " weak interaction " . but at least up to some extent , the known breaking of the cp occurs due to mass terms which are not " interactions at all " ( they are quadratic terms in the lagrangian while interactions have to be higher-order ) . in this sense , the cp and t violation is not caused by the " weak interaction " only – only by a subtle combination of the weak interaction and subtleties in the mass matrices that would not matter separately if there were no interactions . at any rate , the cp and t are known to be violated much more " unambiguously " than the formulation in the peskin-schroeder textbook seems to suggest . the breaking of this symmetry is there ; its effects are just a few orders of magnitude weaker ( and less qualitative ) than the effects of the breaking of c and p . the latter symmetry violations are " immediately obvious " when we consider the weak force – for example because the observed neutrinos are always left-handed ( and antineutrinos are right-handed ) .
the premise of the question is not correct , but there is a general shape to rivers . from leopold and langbein , writing in scientific american : a sample of 50 typical meanders on many different rivers and streams has yielded an average value for this ratio of ahout 4.7 to one . the ratio they use in that article is different from the definition you ( and wikipedia , and comments ) are using . i am sure it is simple algebra to convert one ratio to the other . but that article also notes that : for the large majority of meandering rivcrs the value of this ratio ranges between 1.3 to one and four to one . whatever the conversion comes out to , there appears to be quite a range of real-life meander ratios . that scientific american article is a summary of a longer , more technical article by the same authors . the method they use is to fix beginning and end points a and b , and allow the river to random walk from a to b . the most probable shape for such a path is what they call a " sine-generated " curve . at a given point , the angle between the tangent to the river and the mean direction of the river the sine of the distance along the channel . the resulting curve is not quite a semi-circular curve , so the meander ratio is not predicted to be $\pi$ . a more recent study by garret williams confirms leopold and langbein 's results , and reports that the most common value for the ratio of the radius of curvature to the channel width is between 2 and 3 . the other major effect driving river shape is how easily the river can erode the soil that it passes through . the river will tend to flow more directly downhill if the surrounding soil is difficult to erode . in areas where the soils erodes quite easily , the river will assume this sine-generated curvature . as an example of that , you can look at the mississippi river in the united states . it has the classic sine-generated shape all the way from about cairo , il south to new orleans , la . but it is much straighter up along the illinois/iowa border .
the mass of the original polonium atom is 209.9828737 ( 13 ) au , while the mass of the lead atom is 205.9744653 ( 13 ) au and the mass of the helium is 4.00260325415 ( 6 ) au . the mass deficit gives you the amount of energy released .
poisson brackets are closely related also to transformations of a system . consider a " generator " $\delta g$ and some quantity $a$ . ( all the quantities mentioned are functions of $p_i$ , $q_i$ , and do not depend on time explicitly . so i will not write the arguments unless it is necessary ) . the small transformation , generated by $\delta g$ is : $a \to a+\delta a , \quad\quad \delta a = -\{\delta g , a\} $ one have to be careful with the signs in the definition . i use the one from wikipedia . example 1 -- momentum $\delta g = \epsilon_i p_i \quad \delta a = -\epsilon_i\{p_i , a\} = \epsilon_i\frac{\partial a}{\partial q_i} , $ $\quad \rightarrow \quad a ( p_i , q_i ) \to a ( p_i , q_i ) +\epsilon_i\frac{\partial a}{\partial q_i} = a ( p_i , q_i+\epsilon_i ) $ the momentum is the generator of translations . example 2 -- angular momentum $\delta g = \epsilon_i e_{ijk} p_jq_k \quad \delta a = -\epsilon_i e_{ijk} \{p_jq_k , a\}$ here $e_{ijk}$ -- is a levi-civita symbol . expanding : $\delta a = -\epsilon_i e_{ijk} \sum_\alpha \left ( \frac{\partial p_jq_k}{\partial q_\alpha}\frac{\partial a}{\partial p_\alpha}-\frac{\partial p_jq_k}{\partial p_\alpha}\frac{\partial a}{\partial q_\alpha}\right ) = -\epsilon_i e_{ijk}\left ( p_j\frac{\partial a}{\partial p_k}+q_j\frac{\partial a}{\partial q_k}\right ) $ $\quad \rightarrow \quad a ( p_i , q_i ) \to a ( p_i , q_i ) -\epsilon_i e_{ijk}\left ( p_j\frac{\partial a}{\partial p_k} + q_j\frac{\partial a}{\partial q_k}\right ) = a ( r_{ij}p_j , r_{ij}q_j ) $ for infinitesimal rotations $r_{ij}$ so the angular momentum is the generator of rotations . example 3 -- energy by using hamilton equation one can calculate to which transformations the energy corresponds to ( it is essentially a " reverse " of what is written here ) : $\delta g = \epsilon h \quad \delta a = -\epsilon\{h , a\} = \epsilon\frac{d a}{d t} , $ $\quad \rightarrow \quad a ( p_i ( t ) , q_i ( t ) ) \to a ( p_i ( t ) , q_i ( t ) ) +\epsilon\frac{d a}{d t} = a ( p_i ( t+\epsilon ) , q_i ( t+\epsilon ) ) $ the energy is the generator of time evolution . from here one can directly see the relation between symmetries and conservation laws . if a hamiltonian is symmetric under certain transformation ( and , again , does not depend on time explicitly ) , then the bracket with the corresponding " generator " must vanish $\{\delta g , h\} = 0$ . but this also means that the value of the " generator " does not change with time .
i will start with the second one . $\int\phi^\ast\psi\ , \mathrm{d}x$ is , as chris says in the comments , the scalar ( or dot ) product of $\phi$ and $\psi$ . in the dirac notation , it is written as $\langle\phi|\psi\rangle$ and it gives the overlap of the two wavefunctions . in other words , it gives the probability amplitude ( i.e. . , what you call square root of probability ) that starting from $|\psi\rangle$ we will measure the state to be $|\phi\rangle$ . the first integral works in pretty much the same way ; it is just the scalar product of $|\phi\rangle$ with $\hat{a}|\psi\rangle$ . this means that you first apply $\hat{a}$ to the state $|\psi\rangle$ and then measure its overlap with $|\phi\rangle$ . but this is not the same as calculating the amplitude probability of measuring $|\phi\rangle$ after the measurement of $\hat{a}$ is performed on $|\psi\rangle$ . if you measured $\hat{a}$ , that would be described by a set of projectors $\hat{\pi}_i = |\chi_i\rangle\langle\chi_i|$ , where $|\chi_i\rangle$ is an eigenvector of $\hat{a}$ . the state $|\psi\rangle$ collapses to $|\chi_i\rangle$ with probability $p_i = |\langle\chi_i|\psi\rangle|^2$ which then collapses to $|\phi\rangle$ with probability $q_i = |\langle\phi|\chi_i\rangle|^2$ . the overall probability is then $p = \sum_i p_i q_i$ , which is not the same as $|\langle\phi|\hat{a}|\psi\rangle|^2$ , as you can see if you write $\hat{a}|\psi\rangle = \sum_i \chi_i|\chi_i\rangle\langle\chi_i|\psi\rangle$ , with $\chi_i$ being the eigenvalues of $\hat{a}$ . edit : the interpretation of $\hat{a}|\psi\rangle$ can be divided into several cases : if $\hat{a}$ is unitary , $\hat{a}|\psi\rangle$ can be understood as a time evolution of $|\psi\rangle$ governed by a hamiltonian $\hat{h}$ fulfilling $\hat{a} = \exp ( i\hat{h}t ) $ . if $\hat{a}$ is not unitary , but is trace-decreasing ( i.e. . , $\langle\psi|\hat{a}^\dagger\hat{a}|\psi\rangle \le \langle\psi|\psi\rangle$ for each $|\psi\rangle$ ) it can describe some probabilistic evolution $|\psi\rangle\to|\varphi_i\rangle$ with probability $p_i\le 1$ . the general case cannot be , i believe , interpreted generally . it requires renormalization ( as does case 2 ) but this time the norm cannot be interpreted as the probability . thus , it depends on the specific operator used . as daaxix pointed out in the comments , the expression $\langle\phi|\hat{a}|\psi\rangle$ can be understood as a form of generalized mean value . assuming $\hat{a}$ is hermitian and writing $|\psi\rangle = \sum_k c_k|\chi_k\rangle$ , $|\phi\rangle = \sum_k d_k|\chi_k\rangle$ , we have $$ \langle\phi|\hat{a}|\psi\rangle = \sum_{k , l} c_k d_l^\ast \langle\chi_l|\hat{a}|\chi_k\rangle . $$ due to orthogonality of $|\chi_k\rangle$ , we can write $\langle\chi_l|\hat{a}|\chi_k\rangle = \delta_{kl} \langle\chi_k|\hat{a}|\chi_k\rangle$ , so we have $$ \langle\phi|\hat{a}|\psi\rangle = \sum_k c_k d_k^\ast \langle\chi_k|\hat{a}|\chi_k\rangle . $$ this gives a sum of the expectation values $\langle\chi_k|\hat{a}|\chi_k\rangle$ ( i.e. . , the eigenvalues $\chi_k$ ) , with weight coefficients given by the expansion of the vectors $|\psi\rangle$ , $|\phi\rangle$ , namely $c_k d_k^\ast$ .
the radiation density has two components : the present-day photon density $\rho_\gamma$ and the neutrino density $\rho_\nu$ . the photon density as a function of frequency can be derived directly from the cmb : the photon number density follows the planck law $$ n ( \nu ) \ , \text{d}\nu = \frac{8\pi\nu^2\ , \text{d}\nu}{e^{h\nu/k_b t_0}-1} , $$ with $k_b$ the stefan-boltzmann constant , and $t_0$ the current cmb temperature . the photon energy density is then $$ \rho_\gamma\ , c^2 = \int_0^{\infty}h\nu\ , n ( \nu ) \ , \text{d}\nu = a_b\ , t_0^4 , $$ where $$ a_b = \frac{8\pi^5 k_b^4}{15h^3c^3} = 7.56577\times 10^{-16}\ ; \text{j}\ , \text{m}^{-3}\ , \text{k}^{-4} $$ is the radiation energy constant . with $t_0=2.7255\ , \text{k}$ , we get $$ \rho_\gamma = \frac{a_b\ , t_0^4}{c^2} = 4.64511\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} . $$ the neutrino density is related to the photon density : in eq . ( 1 ) on page 5 in the paper , you see that $$ \rho_\nu = 3.046\frac{7}{8}\left ( \frac{4}{11}\right ) ^{4/3}\rho_\gamma . $$ this relation can be derived from physics in the early universe , when neutrinos and photons were in thermal equilibrium . so $$ \rho_\nu = 3.21334\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} , $$ and the total present-day radiation density is $$ \rho_{r , 0} = \rho_\gamma + \rho_\nu = 7.85846\times 10^{-31}\ ; \text{kg}\ , \text{m}^{-3} . $$ we can also express this relative to the present-day critical density $$ \rho_{c , 0} = \frac{3h_0}{8\pi g} = 1.87847\ , h^2\times 10^{-26}\ ; \text{kg}\ , \text{m}^{-3} , $$ where the hubble constant is expressed in terms of the dimensionless parameter $h$ , as $$ h_0 = 100\ , h\ ; \text{km}\ , \text{s}^{-1}\ , \text{mpc}^{-1} , $$ so we get $$ \begin{align} \omega_{\gamma}\ , h^2 and = \dfrac{\rho_\gamma}{\rho_{c , 0}}h^2 = 1.71061\times 10^{-5} , \\ \omega_{\nu}\ , h^2 and = \dfrac{\rho_\nu}{\rho_{c , 0}}h^2 = 2.47282\times 10^{-5} , \\ \omega_{r , 0}\ , h^2 and = \omega_{\gamma}\ , h^2 + \omega_{\nu}\ , h^2 = 4.18343\times 10^{-5} . \end{align} $$ for a hubble value $h=0.673$ , one finds $\omega_{r , 0} = 9.23640\times 10^{-5}$ .
in an ideal situation ( no air resistance ) there will be absolutely no difference in the place where the coin lands ! whether you toss the coin up from inside the train or while standing on the roof , the coin will land back in your hand ( provided you have tossed it perfectly vertically ) . however , in practice , while standing on a fairly fast train 's roof , there is a lot of wind because you are moving at high speed . the moment you toss the coin , the wind force acts on it , and creates an acceleration in the backward direction , making it go slower than the train ( and you ) . however , it will not land in exactly the same place from where you tossed it every time ! this purely depends on the retarding force acting on the coin - in this case , the wind .
i have found the solution to my problem . first of all , i had a factor 2 discrepancy due to the pixel dimension and , more important from an optical point of view , the laser beam was underfilling the entrance pupil of the objective . this means the focusing was worse !
the photon propagator $d_{\mu\nu} ( x , y ) = \langle 0 | a_\mu ( x ) a_\nu ( y ) |0\rangle$ is a building block for amplitudes , but it is not necessarily an amplitude itself . the source for an electromagnetic field has to be a conserved current , which basically means that you create states from the vacuum using linear combinations of $a_\mu ( x ) $ operators whose coefficients are conserved currents . $$ |j\rangle = \int j^\mu ( x ) a_\mu ( x ) dx |0\rangle $$ where $\partial_\mu j^\mu = 0$ . you can show by direct computation that the amplitude $\langle j_1 | j_2 \rangle = \int\int j_1^\mu ( x ) d_{\mu\nu} ( x , y ) j_2^\nu ( y ) dxdy$ is gauge invariant if the currents $j_1$ and $j_2$ are conserved .
the difference is that the classical doppler effect assumes a static background . in atmosphere , there is a marked difference between a moving observer and a stationary one - a gentle ( or not so gentle ) breeze . to exaggerate these effects , consider two jets flying above mach 1 . if the first jet is ahead of a second , the first jet will not hear any of the noise the first one makes , even if their relative speed is 0 . note that with the relativistic doppler effect , the situation is symmetric , and the doppler shift of light will depend only on relative measures of velocity/position between the two objects . this is because relativity does not assume a static/fixed background in the same sense as an atmosphere .
this question can be split into two parts : how do three generations affect the way the universe runs within the standard model , and how do they affect beyond standard model physics ? it is likely that the three generations are important in beyond standard model physics and without them particle physics would be very different . however we do not have any definite idea about how that works so let 's concentrate on standard model physics . it is true that almost all physics that affects the way the universe runs depends on only the first generation of matter particles , i.e. the up and down quarks , electron and electron neutrino , plus the gauge bosons . although we can now easily observe particles from the second generation i.e. muons , strange particles and charm , these have almost no significance in chemistry or even main processes of nuclear physics . hence their effect on how the universe runs is very small . the third generation has even less influence . however there are some exceptions and possible exceptions . ( 1 ) muons are an important component of cosmic rays and it is now thought that cosmic rays can play an important part in weather and climate . there have been studies to observe correlations between muon flux and upper atmosphere temperature . even if this is observed it does not necessarily mean that muons have a causal affect on weather but it is possible . atmospheric muons could have other important influences such as gene mutation , but this is speculative as far as i know . ( 2 ) cp violation can occur in the standard model at observable rates only through the ckm and pmns mixing matrices for three generations . with only one or two generations cp violation in the standard model can only happen via unobservable non-perturbative effects . while cp violation does not have any affect on the way the universe runs now , it is thought to be essential in big bang cosmology in order to create an imbalance between matter and anti-matter . without it all matter would have annihilated with antimatter as the universe cooled . however , it is not known if other cp violating processes from beyond standard model physics that do not depend on the three generations are more important . ( 3 ) in calculations of particle masses and decay rates the existence of second generation particles as virtual particles has some effect . you could argue that the masses of protons and neutron might be slightly different etc if there were no muons etc . ( 4 ) the three generations of neutrino have observable effects on decay rates of weak gauge bosons and also on cosmological parameters measured in the cosmic microwave background . whether this affects the running of the universe in some significant way is another question . ( 5 ) the top quark mass is an important factor affecting the stability of the vacuum and the allowable mass range of the higgs boson for stability . you could therefore argue that the top quark and perhaps some of the other heavy flavour particles are needed to stop the vacuum decaying . in several of these possibilities you could make the point that without the second and third generation the parameters of the standard model could be adjusted to produce the same effect , so it is not certain that the second and third generations have any real necessity in the running of the universe .
i am going to take c=1 . the top orange line , which is the t ' axis in your drawing , is drawn with a slope of 1/0.6 ; this follows from the definition of velocity . the bottom orange line , which is the x ' axis , has a slope of 0.6 . what is a little harder is to get the scale on the orange axes . the scale follows from the fact that area is preserved by lorentz transformations . the velocity of 0.6 turns out to correspond to a lorentz transformation in which a square is distorted into a parallelogram with its long axis stretched by a factor of 2 , and its short axis contracted by 1/2 . if you want to see this approached developed in more detail , see ch . 7 of this online book : http://www.lightandmatter.com/area1sn.html . ( i am the author . ) there are a couple of recent commercial textbooks that use similar geometric approaches : mermin , it is about time : understanding einstein 's relativity takeuchi , an illustrated guide to relativity
the state \begin{equation} |\psi \rangle = \frac{1}{\sqrt{2}}\left ( |\psi_1\rangle +|\psi_2\rangle \right ) \end{equation} is a pure state . meaning , there is not a 50% chance the system is in the state $|\psi_1\rangle$ and a 50% it is in the state $|\psi_2\rangle$ . there is a 0% chance that the system is in either of those states , and a 100% chance the system is in the state $|\psi\rangle$ . the point is that these statements are all made * before * i make any measurements . it is true that if i measure the observable corresponding to $\psi$ ( $\psi$-gular momentum : ) ) , then there is a 50% chance after collapse the system will end up in the state $|\psi_1\rangle$ . however , let 's say i choose to measure a different observable . let 's say the observable is called $\phi$ , and let 's say that $\phi$ and $\psi$ are incompatible observables in the sense that as operators $ [ \hat{\psi} , \hat{\phi} ] \neq0$ . ( i realize i am using $\psi$ in a sense you did not originally intend but hopefully you know what i mean ) . the incompatibliity means that $|\psi_1 \rangle$ is not just proportional to $|\phi_1\rangle$ , it is a superposition of $|phi_1\rangle$ and $|\phi_2\rangle$ ( the two operators are not simulatenously diagonalized ) . then we want to re-express $|\psi\rangle$ in the $\phi$ basis . let 's say that we find \begin{equation} |\psi\rangle = |\phi_1\rangle \end{equation} for example , this would happen if \begin{equation} |\psi_1\rangle = \frac{1}{\sqrt{2}} ( |\phi_1\rangle+|\phi_2\rangle ) \end{equation} \begin{equation} |\psi_2\rangle = \frac{1}{\sqrt{2}} ( |\phi_1\rangle-|\phi_2\rangle ) \end{equation} then i can ask for the probability of measuring $\phi$ and having the system collapse to the state $|\phi_1\rangle$ , given that the state is $|\psi\rangle$ , it is 100% . so i have predictions for the two experiments , one measuring $\psi$ and the other $\phi$ , given knowledge that the state is $\psi$ . but now let 's say that there is a 50% chance that the system is in the pure state $|\psi_1\rangle$ , and a 50% chance the system is in the pure state $|\psi_2\rangle$ . not a superposition , a genuine uncertainty as to what the state of the system is . if the state is $|\psi_1 \rangle$ , then there is a 50% chance that measuring $\phi$ will collapse the system into the state $|\phi_1\rangle$ . meanwhile , if the state is $|\psi_2\rangle$ , i get a 50% chance of finding the system in $|\phi_1\rangle$ after measuring . so the probability of measuring the system in the state $|\phi_1\rangle$ after measuring $\phi$ , is ( 50% being in $\psi_1$ ) ( 50% measuring $\phi_1$ ) + ( 50% being in $\psi_2$ ) ( 50% measuring $\phi_1$ ) =50% . this is different than the pure state case . so the difference between a ' density matrix ' type uncertainty and a ' quantum superposition ' of a pure state lies in the ability of quantum amplitudes to interfere , which you can measure by preparing many copies of the same state and then measuring incompatible observables .
i think , on would need a model to understand how it works in principle . let us consider the simplified system represented on the figure . you have two metallic spheres with radii $r_1$ and $r_2$ respectively . they are linked via a metallic wire as well . because all these guys constitute a single metallic object , they must have the same potential that i noted $v$ . let us assume for simplicity that the charges are uniformly distributed on the two spheres with two different surface charge densities $\sigma_1$ and $\sigma_2$ . now , that the stage is set , there are two competing effects which have opposite influence on the increase of the charge density : the charge on a metallic object , in the linear regime , is proportional to its potential which is denoted by $q = cv$ where $c$ is the capacitance . the capacitance is such that , for simple convex shapes , it increase with the system size such that for a sphere $c \propto r$ . from this , we can assert that $\frac{q_1}{q_2} = \frac{r_1}{r_2}$ . hence the total charge carried by sphere 1 is bigger than that of sphere 2 . there is a second effect ( which is a simple one ) that consists in noticing that the charge density is inversely proportional to the square of the size of the object essentially $\sigma \sim r^{-2} = \frac{q}{4 \pi r^2}$ for a sphere . hence $\frac{\sigma_1}{\sigma_2}=\frac{q_1}{q_2}\frac{r_2^2}{r_1^2}$ overall , we finally get that $\frac{\sigma_1}{\sigma_2}=\frac{r_2}{r_1}$ and hence " points " have a larger charge density than bigger objects .
i feel i could make sense of this if i first consider the system of only the atmosphere . in that case , one assumes the atmosphere is at constant pressure $p_\text{atm}$ . the change in volume $\delta v$ of the atmosphere could be calculated by knowing the initial and final states of the gas in the cylinder , and first calculating the amount ( $n$ ) of gas entering the atmosphere . one could then calculate work by $w=p_\text{atm}\delta v$ . not sure if this result could apply to the system of gas originally enclosed in the cylinder . someone better than me might chime in .
you have the right ideas , but you do not dare to put them in maths ! momentum conservation is vectorial . here , you have a 2d system , so let 's write it with 2 component vectors . i will denote $\vec{p}_{i/f}$ the initial and final momentum respectively . so the momentum conservation yields $ \vec{p}_i = \vec{p}_f $ i choose to represent the $x$-axis as the first component of my vector , and $y$ the second one . thus , $\begin{pmatrix} m_av_a \\ 0 \end{pmatrix} = \begin{pmatrix} m_bvb_x + m_cvc_x \\ m_bvb_y + mcvc_y \end{pmatrix} \quad \quad \quad ( 1 ) $ now the exercise tells you several things . first of all , you know $m_a , v_a , m_b$ and the angle at which block $b$ takes off . you have to trigonometrically decompose the speeds to find out how to write them in vector form . for instance , $vb_x = v_b\cdot\cos ( 35 ) $ , and $vb_y = vb\cdot\sin ( 35 ) $ . with the same reasoning , $vc_x = 0$ and $vc_y = vc$ because of momentum conservation being vectorial , momentum is conserved on the $x$-axis as well as the $y$-axis independently . from ( 1 ) , you see that $\left\{ \begin{array}{l} m_av_a = m_bv_b\cdot\cos ( 35 ) \\ 0 = mbv_b\sin ( 35 ) + m_cv_c \end{array} \right . $ the only unknowns are $v_b$ and $v_c$ . all you need to do is solve this equation system .
the standard way is to use generating functions ( in this case a la coherent states ) . usually one would like the resulting formula to be normal-ordered . recall the following version $$\tag{1} e^ae^b~=~e^{ [ a , b ] }e^be^a$$ of the baker-campbell-hausdorff formula . the formula ( 1 ) holds if the commutator $ [ a , b ] $ commutes with both the operators $a$ and $b$ . put $a=\alpha a $ and $b=\beta a^{\dagger}$ , where $\alpha , \beta\in\mathbb{c}$ . let $ [ a , a^{\dagger} ] =\hbar {\bf 1}$ , so that the commutator $ [ a , b ] =\alpha\beta\hbar {\bf 1}$ is a $c$-number . now taylor-expand the exponential factors in eq . ( 1 ) . for fixed orders $n , m\in \mathbb{n}_0$ , consider terms in eq . ( 1 ) proportional to $\alpha^n\beta^m$ . deduce that the the antinormal-ordered operator $a^n ( a^{\dagger} ) ^m$ can be normal-ordered as $$\tag{2} a^n ( a^{\dagger} ) ^m~=~\sum_{k=0}^{\min ( n , m ) } \frac{n ! m ! \hbar^k}{ ( n-k ) ! ( m-k ) ! k ! } ( a^{\dagger} ) ^{m-k}a^{n-k} . $$ finally , deduce that the normal-ordered commutator is $$\tag{3} [ a^n , ( a^{\dagger} ) ^m ] ~=~\sum_{k=1}^{\min ( n , m ) } \frac{n ! m ! \hbar^k}{ ( n-k ) ! ( m-k ) ! k ! } ( a^{\dagger} ) ^{m-k}a^{n-k} . $$
$$y = x\tan\theta - \frac{g}{2v_0^2 \cos^2\theta}x^2 . $$ $$y=x . \tan\theta-\dfrac{gx^2}{2v_0^2}-\dfrac{gx^2\tan^2\theta}{2v_0^2}$$ $$a\tan^2\theta+b\tan\theta+c=0$$ as $\tan\theta\in\bbb r$ , so $b^2-4ac\ge0$ must hold , for the above equation to have real roots for $\tan\theta$ . use that and you will get $$y\le \dfrac{v^2}{2g}-\dfrac{g}{2v^2} . x^2 $$ that defines a area under the parabola , within which any target can be hit .
when two or more sources of light combine incoherently ( not in any fixed phase relation ) , you can only " add " , and that is intensity ( power ) - electromagnetic field " squared " and averaged , in the appropriate sense . when you have control over phase relations between two beams , yeah , sure you " add " the two at some point . to " subtract " all you need to do is shift one beam by 180degrees . ( you are dealing with monochromatic light ? ) just add an optically transparent material sufficient to delay the beam by half a wavelength , and there you go . note that " subtract " and " add " may be meaningful only in a limited sense , in the change of phase relationship of two beams in a stable optical apparatus . that is , it is not in the phase relationship itself , but in how it changes by 180 degrees . this is practical and sufficient in many experiments . you do not really have control over things such as spacing of holes in an optical table down to 100nm accuracy , but whatever the distances are , you can be sure they will not change by much during the course of the experiment ( &lt ; 100nm ) . if you have absolute geometric control to several-nanometer scale accuracy all the way from beam splitter to beam combiner ( or detector ) then you could compare the optical paths and say that the beams add , or subtract , or combine in an in-between phase relation , depending on the difference of optical paths being integer , half-integer or other number of wavelengths .
the initial phase $\phi$ should be $-\pi/2$ , or in other words , the solution should be $a\sin ( \omega t ) $ because $\cos ( \theta-\pi/2 ) =\sin ( \theta ) $ . how did you find that $\phi$ is zero ? you are supposed to use the fact that $x$ is zero when $t$ is zero . which is what let 's you set $\phi $ to $-\pi/2$ ( modulo $\pi$ ) .
excellent question ! in short , there are two logical possibilities to explain the data : there is dark matter and a cosmological constant ( standard model ) gravity needs to be modified interestingly , both possibilities have historical precedent : the discovery of neptune ( by johann gottfried galle and heinrich louis d’arrest ) one year after its prediction by urbain le verrier was a success-story for the dark matter idea . ( of course , after its discovery by astronomers it was no longer dark . . . ) the non-discovery of vulcan was the a failure of the dark matter idea - instead , gravity had to be modified from newton to einstein . ( funnily , vulcan actually was observed by lescarbault a year after its prediction by urbain le verrier , but this observation was never confirmed by anyone else . ) so basically you are asking : are we in a neptune or a vulcan scenario ? and could not the vulcan scenario be more credible ? the likely answer appears to be no . modifications of gravity that seem to explain galactic rotation curves are usually either in conflict with solar system precision tests ( where einstein 's theory works extraordinarily well ) or they are complicated and less predictive than einstein 's theory ( like teves ) or they are not theories to begin with ( like mond ) . besides the gravitational evidence for dark matter , there is also indirect evidence from particle physics . for instance , if you believe in grand unification then you must also accept supersymmetry so that the coupling constants merge in one point at the gut scale . then you have a natural dark matter candidate , the lightest supersymmetric particle . there are also other particle physics predictions that lead to dark matter candidates , like axions . so the point is , there are no lack of dark matter candidates ( rather , there an abundance of them ) that may be responsible for the galactic rotation curves , the dynamics of clusters , the structure formation etc . note also that the standard model of cosmology is a rather precise model ( at the percent level ) , and it requires around 23% of dark matter . there are a lot of independent measurements that have scrutinized this model ( cmb anisotropies , supernovae data , clusters etc . ) , so we do have reasonable confidence in its validity . in some sense , the best evidence for dark matter is perhaps the lack of good alternatives . still , as long as dark matter is not detected directly through some particle/astro-particle physics experiment it is scientifically sound to try to look for alternatives ( i plead guilty in this regard ) . it just seems doubtful that some ad-hoc alternative passes all the observational tests .
the news release from nasa does mention this . apparently in this transitional phase the field is not well represented as a dipolar field . right now it may be a quadra polar or higher ( even numbered ) field . think of the total field being the superposition of multiple non-axial non-centered dipoles . the field is weakening in intensity too and may become weaker still before it strengthens into a reversed dipole field .
the order of magnitude given by gugg is correct . the molar volume for the succinic acid is $v_m=\frac{m}{\rho}=\frac{118.09}{1.59}\frac{cm^3}{mol}=74.27cm^{3}/mol$ where $m$ is the molar mass and $\rho$ the density . from this , you find the volume of the molecule to be $v_{molec}=\frac{v_m}{n_a}=1.23\cdot10^{-22}cm^3 $
the article 's preprint mayer h . c . , krechetnikov r . " walking with coffee : why does it spill ? , " phys . rev . e 85 , 046117 ( 2012 ) . is available from the ucsb site . from a glance of the article the phenomenon is not specific only to coffee . the authors make use of the next formula : the natural frequencies of oscillations of a frictionless , vorticity-free , and incompressible liquid in an upright cylindrical container ( cup ) with a free liquid surface are well known from liquid sloshing engineering : $$\omega_{mn}^2 = \frac{g \epsilon_{mn}}{r} \tanh\left ( \epsilon_{mn} \frac{h}{r} \right ) \left [ 1 +\frac{\sigma}{\rho g} \left ( \frac{\epsilon_{mn}}{r} \right ) ^2 \right ] $$ $h$ is the liquid height , $r$ is the cup radius , $\rho$ is the liquid density , $\sigma$ is its surface tension , $g$ is thethe gravity acceleration . $\epsilon_{mn}$ are coefficients connected to bessel functions . the only parameter that can be significantly different between water and coffee is surface tension $\sigma$ , but then the authors rule it out : for a typical common size of a coffee cup , $r$ $3.5$ cm and $h$ $10$ cm , which is studied here , the surface tension $\sigma$ effect is negligible . that is their calculations are applicable both to water and coffee . their work seems to be all about biomechanics , the way human moves with unwanted frequencies .
single ℂ$^n$ is a state space for single photon , let $h_p$ stands for a hamiltonian of a single photon . you are going to struggle here because there is no " hamiltonian of a single photon " . photons are not conserved particles like electrons or protons , they can only be described as excited states of a quantum field . so the idea of an atom interacting with one photon at a time does not really make sense ( if we are considering the free radiation field , not a cavity ) , since extra photons may be created as a consequence of the interaction . nevertheless , a single two-level atom interacting with a heat bath of photons is a canonical theoretical problem , treated extensively in almost any text on quantum optics or open quantum systems . see breuer and petruccione , for example . without going into details , the bound electron in the atom interacts with the quantum and thermal fluctuations of the electric field at the position of the atom . the temperature of the electromagnetic field state determines the mean number of photons flying around for the atom to interact with . after a long time , the internal electronic state of the atom reaches thermal equilibrium with the radiation field via processes of absorption and emission . the atom-field hamiltonian describing this situation is a specific instance of the so-called spin-boson model . study of the spin-boson model is basically a research sub-field in itself . the standard review article for the field is dynamics of the dissipative two-state system , a . leggett et al . , rev . mod . phys . 59 , 1-85 ( 1987 ) , unfortunately behind a paywall . however , this article is quite old now and many new developments and extensions have been made , so it is worth just googling " spin-boson model " to see what you can get .
comments to the question ( v1 ) : last thing first . on-shell means ( in this context ) that equations of motion ( eom ) are satisfied . equations of motion means euler-lagrange equations . off-shell means strictly speaking not on-shell , but in practice it is always used in the sense not necessarily on-shell . [ let us stress that every infinitesimal transformation is an on-shell symmetry of an action , so an on-shell symmetry is a vacuous notion . therefore in physics , when we claim that an action has a symmetry , it is always implicitly understood that the symmetry is an off-shell symmetry . ] op wrote : here is my first question : is this really the demonstration for conservation of charge ? no , because the $4$-gauge-potential $a_{\mu}$ , the maxwell term $f_{\mu\nu}f^{\mu\nu}$ , and the minimal are missing in op 's action . it is in principle not enough to only look at the matter sector . on the other hand , global gauge symmetry for the full action $s [ a , \psi ] $ of both gauge and matter fields leads to electric charge conservation , cf . noether 's first theorem . [ two comments to drive home the point that it is necessary to also consider the gauge sector : ( i ) if we were doing scalar qed ( rather than ordinary qed ) , it is known that the noether current $j^{\mu}$ actually depends on the $4$-gauge-potential $a_{\mu}$ , so the gauge sector is important , cf . this phys . se post . ( ii ) another issue is that if we follow op 's method and are supposed to treat the $4$-gauge potential $a_{\mu}$ as a classical background , then presumably we should also assume maxwell 's equations $d_{\mu}f^{\mu\nu}=-j^{\nu}$ . maxwell 's equations imply by themselves the continuity equation $d_{\mu}j^{\mu}=0$ even before we apply noether 's theorems . ] there is no conserved quantity associated with local gauge symmetry per se , cf . noether 's second theorem . ( its off-shell noether identity is a triviality . see also this phys . se question . ) perhaps a helpful comparison . it is possible to consider an em model of the form $$s [ a ] ~=~\int\ ! d^4x~ \left ( -\frac{1}{4}f_{\mu\nu}f^{\mu\nu}+j^{\mu}a_{\mu}\right ) , $$ where $j^{\mu}$ are treated as passive non-dynamical classical background matter sources . in other words , only the gauge fields $a_{\mu}$ are dynamical variables in this model . before we even get started , we have to ensure local ( off-shell ) gauge symmetry of the action $s [ a ] $ up to boundary terms . this implies that the classical background sources $j^{\mu}$ must satisfy the continuity equation $d_{\mu}j^{\mu}=0$ off-shell . thus a conservation law is forced upon us even before we apply noether 's theorems . note that global gauge symmetry is an empty statement in this model .
there is two kinds of vibration that would make this " work": thermal vibration and actual shaking . actual shaking is out of the question because that would involve pushing and pulling a person back and forth such that their average velocity was near-light speed . the " back and forth " part of this would involve way more acceleration , hence force , than the human body could handle . to have a look at thermal vibration , i.e. heating , we use the following formula for the average velocity of molecules in a body of temperature $t$ ( see below for note ) : $$v = \sqrt{\frac {3kt}{m}}$$ and in so doing make the assumptions that the body is made up of molecules with the same mass $m$ with uniform temperature $t$ . let 's take a human made entirely of carbon-12 for which $m$ is 12 kilograms divided by avogadro 's number . for $v \approx c$ , $$t \approx \frac {mc^2}{3k} \approx \frac {2*3^2}{3*1.38}*10^{-23+2*8- ( -23 ) } \approx 4.4*10^{16}k$$ which is pretty hot . so i am not really answering your question . do vibrations work the same as normal movement with regards to time dilation ? yes . so a person could walk into such a machine , and walk out hundreds of years in the future , even though a much smaller amount of time would have passed from their perspective ? well you could do it in principle , and the particles you started with would have travelled into the future , but it would be a stretch to say that the thing you end up with in the future is the person you started with in either method of vibration . note : this formula only holds for ideal gases , which a relativistically heated gas is not . but it gives an estimate of the ballpark of temperatures we are working in . if someone has the expression for the hyper-relativistic thermal velocity i would appreciate a comment or edit : )
so this will take a really simplistic look at it , ignoring things like flexibility in the tires/wheels/bike and assuming that you do not go too fast to slide out . the work done to turn is the force to turn times the distance of the turn . the force is $mv^2/r$ where $m$ is the mass of the system , $v$ is the speed of the turn , and $r$ is the radius of the turn . so the force gets higher as either the speed increases or the turn gets smaller , as expected . the distance of the turn is the arc-length of a circle ( assumption of course ) of the radius , which is $d = \theta r$ . this means the work done is $$w = \frac{mv^2}{r} \theta r$$ or , in other words , if you are not going to slide out and you can generate the forces you need to make the turn , it does not actually matter what radius you choose , the work done is the same either way . now this is where the assumptions become obviously violated . we can not turn instantly ( $r = 0$ ) because there is not enough friction to keep us upright . and the amount of friction increases as the turn gets sharper . the turning friction dominates the friction along the arc-length ( unless your tires are slipping riding in a straight line ) , so all of this comes together to imply that you save energy by taking a bigger turn faster than a narrower turn slower . but you obviously want to take the tightest turn you can at the fastest speed you can without crashing to maximize everything .
if you were to surround the atmosphere by an adiabatic envelope and allow it to come to equilibrium , it probably would settle into such a state . however , the atmosphere is not a static place . it is actively mixed due to heating of the ground by the sun , and by cooling of the upper atmosphere by radiation into space . this makes the surface air less dense than the air above it , causing highly turbulent convection cells to form . also significant is the differential heating between the equator and the poles , which also drives convection on a global scale . the mixing effect of all this turbulent convection is much greater than the very slow tendency for the gases to form concentration gradients due to their differing densities .
yes , it will affect angular velocity since different mass distribution have different moment of inertia $i$ in general . the effect of torque $\tau$ on the angular velocity $\omega$ of the object is given by $$\tau=\frac{d}{dt} ( i\omega ) $$ the moment of inertia of a point mass is given by $i=mr^2$ , so in your case , the radius differ by 10 time so moment of inertia differ by 100 times and so does the angular velocity . note that when you mention torque , you dont necessary need to specify which point it acts on .
the ontological phrasing you use to describe quantum teleportation should be a red flag . there is no sense it which the quantum state " appears " . it may be the case that you are confounding the physical system itself ( real , ontological ) with the quantum state assigned to it by the experimenter ( subjective , epistemic ) . in other words , think of the quantum state as the information one has about a physical system and not the physical system itself . if the fidelity between the final state and the input is not 1 , then some operation in the protocol was not perfect . of course , this will always be the case . so , you should understand " quantum teleportation " as the protocol itself and not the act of transferring exactly the quantum state of one physical system to another . for any practical application of the protocol , it need not be perfect to be useful .
at the risk of being snarky ( each definition is from wikipedia ) . . . comet - a comet is an icy small solar system body that , when close enough to the sun , displays a visible coma ( a thin , fuzzy , temporary atmosphere ) and sometimes also a tail . these phenomena are both due to the effects of solar radiation and the solar wind upon the nucleus of the comet . comet nuclei range from a few hundred meters to tens of kilometers across and are composed of loose collections of ice , dust , and small rocky particles . asteroid - asteroids ( from greek ἀστήρ ' star ' and εἶδος ' like , in form' ) are a class of small solar system bodies in orbit around the sun . they have also been called planetoids , especially the larger ones . these terms have historically been applied to any astronomical object orbiting the sun that did not show the disk of a planet and was not observed to have the characteristics of an active comet , but as small objects in the outer solar system were discovered , their volatile-based surfaces were found to more closely resemble comets , and so were often distinguished from traditional asteroids meteor - a meteoroid is a sand- to boulder-sized particle of debris in the solar system . the visible path of a meteoroid that enters earth 's ( or another body 's ) atmosphere is called a meteor , or colloquially a shooting star or falling star . if a meteoroid reaches the ground and survives impact , then it is called a meteorite . many meteors appearing seconds or minutes apart are called a meteor shower . the root word meteor comes from the greek meteo¯ros , meaning " high in the air " .
the relation $\mathbf{e} = -\nabla v$ holds only in the absence of vector potential , otherwise the electric field changes to $$ \mathbf{e} = -\nabla v-\frac{\partial\mathbf{a}}{\partial t} . $$ the reason for this is that when you introduce vector potential by $\mathbf{b} = \nabla\times\mathbf{a}$ , faraday 's law reads $$ \nabla\times\mathbf{a}+\frac{\partial}{\partial t} ( \nabla\times\mathbf{a} ) = \nabla\times\left ( \mathbf{e}+\frac{\partial\mathbf{a}}{\partial t}\right ) = 0 . $$ this can be solved generally by putting the bracket equal to a gradient of a scalar function $-\nabla v$ which gives the result for electric field in terms of both scalar and vector potentials given above .
yes . for photons in vacuum , the energy per photon is proportional to the photon 's classical , electromagnetic frequency , as $e = \hbar \omega = h f$ . here , we see a connection between two classical properties of light : the energy and frequency . what is surprising is that the relation holds for matter , where there is no classical equivalent of the frequency . nevertheless , in an interferometry experiment , an relative energy shift of $\delta e$ can lead to an observable frequency difference $\delta f$ , so that the phase of an interferometer operated for a time $t$ is $\phi = \delta e\ , t/\hbar$ . this was originally observed in neutrons and has more recently been seen in electrons and atoms . even the rest mass energy $mc^2$ has a equivalent frequency , which is known as the compton frequency $\omega_c = mc^2/\hbar$ . while we can not ( currently ) experimentally measure it , it can be inferred from atom interferometry experiments . the general idea of a matter-wave frequency occurs where it is possible to make and readout a superposition state , which does not occur classically .
you just made some math mistakes . you made a mistake when you did $q = h\int_a kr$ . you got $q = h\pi k r^2$ , but you should have gotten $q = \frac{2}{3} h\pi k r^3$ . notice how this second expression has units of charge while the first one does not . another mistake you make is that you say $\frac{1}{r} \frac{\partial re ( r ) }{\partial r} = \frac{1}{r} ( \frac{\partial e ( r ) }{\partial r} + r\frac{\partial e ( r ) }{\partial r} ) $ . this is not proper application of the product rule . if correct these mistake you will get the right answer unless there are other mistakes i did not find .
i believe that in that text , $i$ refers to the magnitude of the current ( a scalar ) , which is assumed to be in the same direction as the length vector $\vec{l}$ ( a vector ) . there is no need for both $i$ and $\vec{l}$ to be vectors . think of current flowing through a wire—if $i$ were a vector ( $\vec{i}$ ) , then the direction of $\vec{i}$ would always be the same as the direction of the wire , because current always flows along a wire . the direction of the wire is already captured by $\vec{l}$ , so it is not necessary to make $i$ a vector quantity also .
since omega centarui is at a declination of -47.5 degrees , you would need to be at a lattitude of 23.5 degrees north ( i.e. . on the tropic of cancer ) for it to get to 20 degrees above the horizon when it crosses the meridian ( due south ) . in theory , if you had a perfectly flat horizon ( and not atmosphere ) , you could see it just on the horizon as it crosses the meridian at a latitude of 42.5 degrees north . to see it from a hill with a negative horizion the lattitude would depend on the amount of negative horizon you had . this assumes that there is no atmosphere . even at 30 degrees above the horizion you are looking through twice as much air as straight up . at 20 degrees you are looking through 3 times as much air and it gets worse as you go to the horizion , this will effectively limit the minimum angle you can look at . omega cen is a 3.7 magnitude object so it is fairly bright but even so , you will loose it in the atmosphere before you get all the way to the horizon .
they accrete gas from a disk , fed by either a wind or roche lobe overflow from their companion . almost all known millisecond pulsars are in binary systems , but i think some in globular clusters may have been disrupted by three-body encounters , so appear to be isolated .
any nasa image is in the public domain , although it is common practice to provide attribution back to nasa . ( i often see people trying to make copyright claims on images that i know are nasa provided images ) i believe that other images provided by us government funded observatories are also public domain , but you occassionally get a pi institution trying to assert a claim . the issue is going to be how you want to look for the images . the two main places that i know , without going to a specific mission archive are : nasa 's astronomy picture of the day ( apod ) nasa image exchange however , these are often calibrated and cleaned up in ways that make ' em useless for scientific analysis . if you want the really good stuff , you will want to go to use the national virtual observatory ( nvo ) , but then you will need a fits reader , and have to colorize the images yourself to make ' em pretty . update : a few more sources of nasa images : nasa images , a collaboration between the internet archive and nasa google directory : science > technology > space > nasa > images and movies
i tend to think that you are right that the phase - but not the absolute value - may be decomposed into infinitesimal pieces of solid angle . and for each tiny solid angle , the phase you get is the same . it is not hard to see what happens with the $|jm\rangle$ eigenstate under a parallel transport around an infinitesimal solid angle $\omega$ . it simply gets transformed by the phase $$\exp ( i m \omega ) . $$ in your notation , $n_{jm}=m$ . it is because it reduces to exponentials of small rotations around $x , y , -x , -y$ axes , and the commutator boils down to $j_z$ for the usual reasons . for your example , $\omega$ was $4\pi/8=\pi/2$ and $m$ was $1/2$ . that is why you got $\exp ( \pi i /4 ) $ . of course , i realize that this answer seems to contradict the previous problem you posed because $\exp ( 4\pi i m ) $ equals one for any conceivable value of $m$ , so there should be no obstruction ever . so at this moment , i am not sure which answer is the right one . i would guess that my answer to your previous problem is completely wrong - there may exist bundles that can not be obtained simply as products of the non-existing low-spin bundles . and there could also be an issue with the overall sign . note that if the inner products between the two states remain real for any angle , you eventually hit the point where the inner product goes negative , but at this point , the absolute value changes the sign relatively to the inner product . cheers lm
the key idea here is the concept of " power-conjugate " stress and strain-rate measures . for the cauchy stress $\sigma$ , the stress power is given by : $$ \dot w/v = \sigma:d $$ where $d$ is the rate of deformation tensor defined as the symmetric part of the velocity gradient . $$d = sym ( l ) = \frac{1}{2} ( l + l^t ) $$ the quantity $\sigma:d$ gives the stress power per unit volume . therefore , using the explicit time integration scheme : $$ \frac{\delta w}{\delta t} = ( \sigma:d ) v $$ the tensor contraction can be re-written as $\sigma:d = tr ( \sigma^td ) $ . this is most easily observed if you work it out in index notation : $$ \begin{align} \sigma:d and = \sigma_{ij}d_{ij} \\ and = ( \sigma^t ) _{ji}d_{ij} \\ and = ( \sigma^t ) _{ji}d_{ik}\delta_{kj} \\ and = ( \sigma^td ) _{jk}\delta_{kj} \\ and =tr ( \sigma^td ) \end{align} $$ where $\delta_{kj}$ is the kronecker delta . due to the symmetry of the stress tensor , you do not really have to compute $d$ explicitly because $\sigma:d = \sigma:l$ . in short , the off-diagonal terms of the stress tensor do factor into the new energy , but it is just not obvious due to the way the formula is evaluating the stress power .
this interference is unfortunately quite typical as david pointed out in his comment . a typical household microwave oven operates at 2.45 ghz , the 802.11g wireless spectrum lies in the range of 2.412 to 2.472 ghz . this by itself is not a big problem , as the wifi algorithms use sophisticated algorithms to operate even with noise at the same frequencies . the problem is the leaked power that can be much higher than any nearby wifi signal . the shielding is never perfect and usually from the ~800w only a tiny amount will leak out through the seals , the metallic grid in the front window , etc . we analysed the amount of leakage with a calibrated microwave sender/receiver pair during my undergraduate time and found attenuation in the range of 99.75% to 99.98% for different types of metallic grids in the front doors . this means that still 160mw to 2w can leak out of the oven ( i am not sure if 2w of leakage are even allowed today as it was an old microwave oven ) . compared to the allowed output power of wifi of 100mw to 300mw depending on the region/country it is easy to see why a not well shielded oven can drown out the signal completely . you can avoid the wifi interruptions by trying out different channels , using a 5ghz setup or using a better microwave . the biological harm caused my microwave radiation is still under debate to phrase it mildly . there a lots of different effects and long-term influences are very hard to characterize . so if we take the worst case scenario of the 2w leaking microwave it is safe to assume that any thermal effects are tiny as you will not put your head against the oven and from the distance of a couple of centimeters the deposited power per volume is too small to have any effect . the same cannot be said so easily for a cellphone which you have for a long time very close to your head , but after hundreds of studies it is clear that immediate negative effects could not be found and now big long term studies try to characterize effects that only happen after years of exposure .
method the method is based on measuring variations in perceived revolution time of io around jupiter . io is the innermost of the four galilean moons of jupiter and it takes around 42.5 hours to orbit jupiter . the revolution time can be measured by calculating the time interval between the moments io enters or leaves jupiter 's shadow . depending on the relative position of earth and jupiter , you will either be able to see io entering the shadow but not leaving it or you will be able to see it leaving the shadow , but not entering . this is because jupiter will obstruct the view in one of the cases . you might expect that if you keep looking at io for a few weeks or months you will see it enter/leave jupiter 's shadow at roughly regular intervals matching io 's revolution around jupiter . however , even after introducing corrections for earth 's and jupiter 's orbit eccentricity , you still notice that for a few weeks as earth moves away from jupiter the time between observations becomes longer ( eventually by a few minutes ) . at other time of year , you notice that for a few weeks as earth moves towards jupiter the time between observations becomes shorter ( again , eventually by a few minutes ) . this few minutes difference comes from the fact that when earth is further away from jupiter it takes light more time to reach you than when earth is closer to jupiter . say you have made two consecutive observations of io entering jupiter 's shadow at t 0 and t 1 separated by n io 's revolutions about jupiter t . if the speed of light was infinite , one would expect \begin{equation} t_1 = t_0 + nt \end{equation} this is however not the case and the difference \begin{equation} \delta t = t_1 - t_0 - nt \end{equation} can be used to measure the speed of light since it is the extra time that light needs to travel the distance equal to the difference in the separation of earth and jupiter at t 1 and t 0 : \begin{equation} c = \frac{\delta d}{\delta t} = \frac{d_{ej} ( t_1 ) -d_{ej} ( t_0 ) }{\delta t} \end{equation} ( both numerator and denominator can be negative representing earth approaching or receding from jupiter ) in reality more than two observations are needed since t is not known . it can be approximated by averaging observations equally distributed around earth 's orbit accounting for eccentricity or simply solved for as another variable . practical considerations note that you will not manage to see io enter/leave jupiter 's shadow every io 's orbit ( i.e. . roughly every 42.5 hours ) since some of your observation times will fall on a day or will be made impossible by weather conditions . this is of no concern however . you should simply number all io 's revolutions around jupiter ( timed by io entering/leaving jupiter 's shadow ) and note which ones you managed to observe . for successful observations you should record precise time . it might be good to use utc to avoid problems with daylight saving time changes . after a few weeks you will notice cumulative effect of the speed of light in that the average intervals between io entering/leaving jupiter 's shadow will become longer or shorter . cumulative effect is easier to notice . at minimum you should try to make two observations relatively close to each other ( separated by just a few io revolutions ) and then at least one more observation a few weeks or months later ( a few dozens of io revolutions ) . this will let you calculate the average time interval between observations within a short and long time period by dividing the length of the time period by the number of revolutions io has made around jupiter in that period . the average computed over the long time period will exhibit cumulative effect of the speed of light by being noticeably longer or shorter than the average computed over the short time period . more observations will help you make a more accurate determination of the speed of light . you must plan all of the observations ahead since you can not make the observations when earth and jupiter are close to conjunction or opposition . calculations once you collected the observations you should determine the position of earth and jupiter at the times of the observations ( for example using jpl 's horizons system ) . you can then use the positions to determine the distance between the planets at the time the observations were made . finally , you can use the distance and the variation in io 's perceived revolution period to compute the speed of light . you will notice that roughly every 18 millions kms change in the distance of earth and jupiter makes an observation happen 1 minute earlier or later . cost the cost of the experiment is largely the cost of buying a telescope that allows you to see io . note that the experiment takes a few months and requires measuring time of the observations with the accuracy of seconds . history see this wikipedia article for historical account of the determination of the speed of light by rømer using io .
it is mainly an electrostatic interaction through a dipole-dipole interaction . however , the dipole moment can be permanent or induced . depending on its nature the force has a different name : force between two permanent dipoles ( keesom force ) force between a permanent dipole and a corresponding induced dipole ( debye force ) force between two instantaneously induced dipoles ( london dispersion force ) you can also refer vanderwaals force an atom or molecule is usually globaly neutral : ie there is exactly the same number of positive and negative charges . however the center of charge ( barycenter of the qi ) for the positive and negative charges does not always coincide . this gives rise to an electrostatic dipole that can interact with an external electric field . to answer to your first bullet , the electrostaic dipole is not linked to electrons changing orbital . the cohesive forces in solids have two different origin : orbital coupling ( not of electromagnetic nature ) or ionic bonds in ionic crystals ( na+ , cl- ) for instance . in the latter the cohesion of the crystal is due to electrostaic interaction ( not dipolar ) . for the liquids , the electrostatic forces are responsible for the observed properties . polar solutions are made of molecules with a dipolar moment and are able to dissolve ionic crystals .
the spin referred in condensed matter is the spin of the electrons least bound to the atoms ( usually valance electrons ) . the atoms reside on the lattice sites . a spin half problem means the atoms have only one valance electron . but there are other possibilities like spin 1 , 3/2 and all . as qeb has already mentioned it can also be used for nuclear spins also . i just want mention that any two level quantum system can be represented as a effective spin 1/2 problem .
take the right pedal as an example . it uses a right-hand thread , so turning the pedal spindle clockwise ( cw ) relative to the crank will screw the spindle in , counter-clockwise ( ccw ) will unscrew it . say you put the bike in a repair stand , grab the right pedal and gently simulate the motion of someone riding the bike ( always keeping the pedal platform horizontal as if your foot were there ) . the body of the pedal will turn ccw relative to the crank . intuitively you would think this might help unscrew the pedal ! but in reality the clamping torque of the threads will be far , far greater than any friction in the bearings could generate . even if the bearings were to seize , it would be very difficult to unscrew the pedal ( unless it was never tight to begin with ) . now consider someone riding the bike , putting weight on the right pedal . this applies a force perpendicular to the ground , no matter where the foot is in the pedal stroke . relative to the crank arm , this radial force rotates ccw , which - via the process of mechanical precession - creates a cw torque on the pedal spindle ( thus tightening it ) . the rotations on the left side are all reversed , so it must use the opposite threading to prevent the pedal from coming loose . also see " precession " on wikipedia , from where the below illustration was taken ( cc-sa ) .
you will have two forces that act on an elementary mass element $dm$ on the surface . the force in the $x$-direction will be $df_{x}=\omega^{2}xdm$ and in the $y$-direction $df_{y}=gdm$ . also , we know that the slope of a curve is $\tan{\alpha}=dy/dx$ . however , the tangent is equal also to $\tan{\alpha}=df_{x}/df_{y}$ . so from this you have that $$\frac{dy}{dx}=\frac{\omega^{2}x}{g}$$ after integration you get $$y=\frac{\omega^{2}}{2g}x^{2}$$ which is just the equation for a parabola . this is a two-dimensional derivation based on the stagnant interface . a more general solution would be as follows . consider the axis $oz$ along the cylinders axis . in this case , the velocity components will be $v_{x}=-\omega y$ , $v_{y}=\omega x$ , $v_{z}=0$ . taking euler 's equation $$\frac{\partial\vec{v}}{\partial t}+ ( \vec{v}\cdot\nabla ) \vec{v}=-\frac{1}{\rho}\mathrm{grad}p$$ considering that $\partial\vec{v}/\partial t=0$ , the projections on the three axis on euler 's equation are $$x\omega^{2}=\frac{1}{\rho}\frac{dp}{dx}$$ $$y\omega^{2}=\frac{1}{\rho}\frac{dp}{dy}$$ $$\frac{1}{\rho}\frac{dp}{dz}+g=0$$ the general solution of these equations is $$\frac{p}{\rho}=\frac{1}{2}\omega^{2} ( x^{2}+y^{2} ) -gz+c$$ on the free surface , where the pressure is constant , the surface will have the shape of a paraboloid . $$z=\frac{\omega^2}{2g} ( x^{2}+y^{2} ) $$
it depends how good of an approximation you want . if you just want something that looks like starlight to the human eye then it is not too hard - you can buy solar spectrum bulbs at any hardware store . but of course , this is not going to give you a great approximation , and it is only going to be anywhere close in the visible wavelengths . if you want something that is going to be a fairly good approximation across a very wide range of wavelengths then just heat any random object up to between $3600\:\mathrm{k}$ and $50,000\:\mathrm{k}$ , depending on the star . ( those massive blue stars at $50,000\:\mathrm{k}$ will present a challenge , but i think it is within the bounds of experimental possibility . ) this works because stars and other hot object both emit spectra that are close to the ideal black body spectrum . you can get an idea of how close by comparing the curve on this graph to the edge of the yellow region : ( image source . ) if you want to reproduce all those deviations from the ideal black body curve then it is going to be a bit harder , but it is probably doable if you have a good enough reason to bother . i would guess that a good technique would be to surround your black body with gases similar to the star 's corona , in order to reproduce the absorption lines . emission lines will be a bit more tricky , but i guess if there is no other way you could simply heat those gases up to the appropriate temperature . the uniqueness of a star 's spectrum comes mostly from its temperature and its composition , i.e. the gases that make it up , so by using this method you could probably more or less simulate the spectrum of a specific star . this method would simulate the spectrum of light that the star emits , but if you wanted to simulate the spectrum that we actually see it would be much harder . this is because the spectra of distant stars are modified by a redshift , caused by the fact that distant galaxies are moving away from us . the redshift is basically the optical equivalent of the doppler effect , and it causes us to see frequencies lower than what the star emits . if you wanted to simulate this in the laboratory you would have to use a different method than the one i have described , such as the customised diffraction gratings described in rod vance 's answer . of course , if you meant " simulate on a computer " then it is a different question . i think this is probably not too hard - you just need to look up the appropriate emission and absorption spectra and add them up in the right way . i am sure people researching stars ' spectra do this all the time .
this is not true--- you can make superpositions of degenerate energy eigenstates and the are still eigenstates . for example , moving at wavenumber k and -k have the same energy , but the difference is not in the reciprocal lattice . the proper thing to say is that the hamiltonian is translationally invariant with translation operator the lattice translations , and this means that h commutes with lattice translations , so that the eigenvalues of the lattice translations ( the equivalence class of k under the reciprocal lattice ) are each separately sectors where you find eigenvectors of h .
this equation is called the ray equation and it can indeed be derived from fermat 's principle . i guess you can find more about its derivation in , e.g. , born and wolf 's principles of optics or in fundamentals of photonics by saleh and teich .
as is easily checked , fields linear in creation and annihilation operators ( and hence amenable to a particle interpretation ) have zero vacuum expectation value . thus the $\phi$ field with its nonvanishing vacuum expectation value cannot be given a particle interpretation . but the field $\psi=\phi-v$ has such an interpretation as its vacuum expectation value is zero . this works only if $v$ is the vacuum expectation of $\phi$ . note that the field $\phi$ is and remains massless ; it is the field $\psi$ that had acquired a mass term . the 1-loop approximation to a quantum field theory is given by the saddle-point approximation of the functional integral . for that you have to expand around a stationary point , and for stability reasons this stationary point has to be a local minimizer . if the local minimum is not global , the vacuum state is metastable only ; so one usually expands around the global minimizer . a mass term breaks the scaling symmetry of a previously scale-invariant theory . it may or may not break other symmetries . in the above case , the symmetry $\phi\to-\phi~~~$ of the action is broken in the stable vacuum .
this must be impossible , even for lady castafiore with her earthquake voice . for a glass to break by sheer sound you need to produce a tone equal to the glass 's natural frequency - the frequency at which a body vibrates with the least amount of energy . in other words : there you get the most vibration with a minimum of effort . this is also called resonance . however , it is much harder to create resonance in mixed materials because each component has a different fundamental frequency . so two layers of different types of glass will effectively prevent resonance . of course , apart from that the mere thickness of the material will be a problem for our nightingale .
this is a note on why angular velocities are vectors , to complement matt and david 's excellent explanations of why rotations are not . when we say something has a certain angular velocity $\vec{\omega_1}$ , we mean that each part of the thing has a position-dependent velocity $\vec{v_1} ( \vec{r} ) = \vec{\omega_1} \times \vec{r}$ . we might consider another one of these motions $\vec{v_2} ( \vec{r} ) = \vec{\omega_2} \times \vec{r}$ and wonder what happens when we add them . we get $\vec{v_1} ( \vec{r} ) + \vec{v_2} ( \vec{r} ) = \vec{\omega_1} \times \vec{r} + \vec{\omega_2} \times \vec{r}$ . the cross product is linear , so this is equivalent to $ ( \vec{v_1} + \vec{v_2} ) ( \vec{r} ) = ( \vec{\omega_1} + \vec{\omega_2} ) \times \vec{r}$ , so it makes fine sense to add angular velocities by vector addition .
the doppler shift in the light from the star tells you the period of the planet 's orbit and also the velocity the star moves . you need to know the mass of the star , but this can be estimated to good accuracy from the star brightness and type . once you know the mass of the star you can calculate the distance of the planet from it is period using : $$ r^3 = \frac{gm}{4\pi^2}p^2 $$ where $m$ is the mass of the star and $p$ is the period of the oscillation . not that it is directly relevant to your question , but from the velocity of the star 's oscillation we can calculate the minimum mass of the planet , because the velocity of the stars displacement depends on the gravitational force between the two . we can only calculate a minimum planet mass because if the plane of the system it tilted relative to us the true mass is higher than the one we calculate . having said this , these days most extrasolar planets are discovered because they transit their star , and these systems are not tilted relative to us ( otherwise they would not transit ! ) . that means we can calculate an accurate mass for the planet . in practice we normally turn the calculation over to a computer model ( called a bayesian kepler periodogram if you want to google it ) because there are usually several planets and the oscillation is not a simple sine wave . we use a numerical fit to work out how many planets there are and how far from the star they are .
a fluid motion in a vortex creates a dynamic pressure that is lowest in the center increasing radially ( $p \propto r^2$ ) . the gradient of this pressure that forces the fluid to rotate around the axis . this is usually represented by a vector called vorticity , and defined by $\omega = \nabla \times v$ . in simple terms , this means that the fluid is rotating around a certain point . if you placed a small ball on the flow , you would observe that is would rotate about the center and the direction of vorticity vector is given by the right-hand rule . the formation methods are many . for example , in the wake of an engine , there air has been given rotational momentum and will continue to have vorticity . when two opposite flows meet , they can also form as in planetary systems , like tornadoes or jupiter great red spot
the mass $m$ in the formula is not the rest mass $m_0$ and therefore dependent on the velocity : $$ m = \frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}} \equiv \gamma m_0 $$ this means you cannot simply take the normal mass of a nitrogen atom and put it in there , if you assume a speed $v \neq 0$ . the $c$ in the formula does not mean that the particle travels at light speed . it is a factor , nothing more . the actual particle velocity comes into play by the mass $m$ with the above relation . there is also an expression for the energy in terms of the rest mass and momentum : $$ e^2 = p^2c^2 + m_0^2c^4 $$ this might remind you of pythagoras ; ) in fact this equation is even better because it is also valid for massless particles like the photon , where $e = mc^2$ is only valid for massive objects . you can easily see with the help of the first equation that if $v$ approaches $c$ the term for $m$ becomes $\infty$ and therefore $e$ becomes also $\infty$ . this means that it is impossible to accelerate a particel with a rest mass $m_0 \neq 0$ to light speed .
the assumption is , that the spin $s$ is a large parameter . a conjecture that is apparently not valid for $s=1/2$ . the expansion is in $1/s$ , which is assumend to be close to zero . $$ s^+_j = \sqrt{2s-n_j}a^\dagger_j = \sqrt{2s}\sqrt{1-\frac{n_j}{2s}}a^\dagger_j\approx\sqrt{2s}\cdot\left ( 1-\frac{n_j}{4s}\right ) a^\dagger_j$$ the second term , being of order $s^{-1/2}$ , is neglected . assuming $s$ to be large , amounts to a semiclassical approximation . in this limit the relative uncertainty of the spin-operators becomes vanishingly small . ( use the spin-algebra to see this . ) $$ \frac{\delta s^i\delta s^j}{s^2} \longrightarrow 0$$ the working hypothesis is , that low-energy excitations are realized as small deviations around the fully aligned groundstate . for small spin , e . g $s=1/2$ there is no way to only slightly deviate from let 's say $s^z_i=+1/2$ which brings us back to your objection/observation , that the expansion is not valid in the ' small-spin ' limit .
it depends on the situation and interpretation , but it certainly can be the case that your shadow is faster than you . if you imagine a single spotlight shining in front of you , you would cast a shadow behind . if the spotlight moves to behind you , even if you stand still , your shadow will move to the other side . depending on the distance of the object onto which the shadow is cast , this " motion " of your shadow can be very fast indeed ; it could even be faster than the speed of light ! by another interpretation , you are only very slightly faster than your shadow . again , imagine the spotlight in front of you . this time the light stays still , but you hold your arm out and sweep it up . as light from the spotlight passes you , your arm reflects or absorbs some of it ; the wall behind you reflects only the light that falls on it which corresponds to the light that did not fall on your arm , this is the shadow . as you sweep your arm upwards , the pattern on the wall follows the movement . however , because it takes some time for light to travel from the point where it passes you to the wall , the shadow will slightly lag your movement . for real life cases , this lag is miniscule , but if the wall were a sufficiently long distance away , it could be significant . when you consider the added distance from the wall to the observer , the apparant lag time increases .
well , vectors ( 3d vectors ) do not really transform linearly . unlike galilean transformations , you need now know anything " extra " when transforming a vector . here , due to the " mixing " of space and time , you do . to transform displacement , you need to know time , and vice versa . same with energy and momentum . four-vectors , on the other hand , transform linearly . these are four-dimensional vectors which transform linearly via the lorentz matrix ( $\beta=\frac{v}{c} , \gamma=\frac{1}{\sqrt{1-\beta^2}}$ ) : $$l=\begin{bmatrix} \gamma and -\beta \gamma and 0 and 0\\ -\beta \gamma and \gamma and 0 and 0\\ 0 and 0 and 1 and 0\\ 0 and 0 and 0 and 1\\ \end{bmatrix}$$ for example , if you want to transform position and/or time , you use the four-position $$x=\begin{bmatrix} c t \\ x \\ y \\ z \end{bmatrix}$$ this can be compactly written as $ ( ct , \vec x ) $--this just means that you can expand the second " vector " term to get the next three four-vector components . anyway , the four vector transforms as : $$x'=l\times x$$ ( matrix product ) this , expanded , is : $$\begin{bmatrix} c t ' \\ x ' \\ y ' \\ z ' \end{bmatrix} = \begin{bmatrix} \gamma and -\beta \gamma and 0 and 0\\ -\beta \gamma and \gamma and 0 and 0\\ 0 and 0 and 1 and 0\\ 0 and 0 and 0 and 1\\ \end{bmatrix} \begin{bmatrix} c\ , t \\ x \\ y \\ z \end{bmatrix} , $$ which are your normal lorentz transformations . a property of four vectors is that if we are talking about the same four vector $ ( a , \vec b ) $ in two frames , the value of $a^2-|\vec b|^2$ is the same in both . for four-position , you get $c^2t^2-x^2-y^2-z^2=c^2t'^2-x'^2-y'^2-z'^2$ other four vectors are : four-velocity : $ ( \gamma c , \gamma\vec u ) $ four-momentum : $ ( e/c , \vec p ) $ four-current density : $ ( \gamma\rho , \vec j ) $ four-potential : $ ( \frac\phi{c} , \vec a ) $ ( and a few more which i can not remember )
the equation in the green box has the correct dimensions , but you are correct , the two sides of the equation in the blue box must have the same dimensions too ( and should probably be cm/s in the example shown , but as we can not see what m/m is or what units anything else has been given in , the whole thing is baffling . using a binomial expansion when $d \ll l$: $$ h = l - ( l^2 - d^2 ) ^{1/2} = l - l ( 1 - d^2/l^2 ) ^{1/2} \simeq l ( 1 - ( 1 - d^2/2l^2 ) ) = d^2/2l$$ i would then use that in your original expression $$ v = ( 1+ m/m ) \sqrt{2gh} = ( 1+m/m ) \sqrt{2g d^2/2l} = ( 1+m/m ) d\sqrt{g/l}$$
if you are not interested in relativistic effects , the answer to your question is easy to workout . according to wikipedia , alpha centauri is 4.24 ly away ( 4.0114x$10^{16}\mathrm{m}$ ) . so to get there in 60 years ( $1892160000\mathrm{s}$ ) . so your non-relativistic answer is $v = \frac{d}{t} = \frac{4.0114 \times 10^{16}}{1892160000} = 21200000 \mathrm{m}\ , \mathrm{s}^{-1}$ . this is 21200 $\mathrm{km}\ , \mathrm{s}^{−1}$ . the fastest recored space flight was 24,791mph which is around 11$\mathrm{km}\ , \mathrm{s}^{−1}$ which is 0.05% of 21200$\mathrm{km}\ , \mathrm{s}^{−1}$ . this means we have to be able to get spaceships to travel 2,000 times faster than the fastest current spaceship . note , i believe satellites in geostationary orbits do $\approx 17\mathrm{km}\ , \mathrm{s}^{−1}$ .
it is possible to make an estimate of the amount of baryons in the observable universe . but it is more difficult to make an estimate of anything else . it gets particularly more difficult when you consider things like photons , because they can pop in and out of existance , i.e. the number of photons is not constant . actually , that is true for all elementary particles , since they are considered as excitations of quantum fields in most modern physics theories . thus , the number of particles is not constant . but the heavier the particle , the less likely it will pop in existance . and if it is too heavy , it'll decay in lighter , more stable particles . so maybe you will have something a couple of orders bigger than the amount of baryons , but probably not much larger . then , there is dark matter , of which we do not really know much . so , i have no idea if an estimate has been attempted of the amount of dark matter particles . any estimate will be highly dependent on the theory we have for these particles . in his book the emperor 's new mind , penrose estimates the number of baryons in the observable universe to be of the order of $10^{80}$ . this seems to confirm it .
for the sorts of vehicles we are used to , like cars and aeroplanes , there are two contributions to drag . there is the drag caused by turbulence , and the drag caused by the effort of pushing the air out of the way . the streamlining in cars and aeroplanes is designed to reduce the drag due to turbulence . the effort of pushing the air out of the way is basically down to the cross sectional area of whatever is pushing it is way through the air . turbulence requires energy transfer between gas molecules , so you can not get turbulence on length scales shorter than the mean free path of the gas molecules . the wikipedia article on mean free paths helpfully lists values of the mean free path for the sort of gas densities you get in space . the gas density is very variable , ranging from $10^6$ molecules per cm$^3$ in nebulae to ( much ) less than one molecule per cm$^3$ in intergalactic space , bt if we take the value of $10^4$ in the table on wikipedia the mean free path is 100,000km . so unless your spaceship is very big indeed we can ignore drag due to turbulence . a sidenote : turbulence is extremely important in nebulae , and a quick glance at any of the hubble pictures of nebulae shows turbulent motion . however the length scale of the turblence is of the order of light years , so it is nothing to worry a spaceship . so your spaceship designer does not have to worry about the sort of streamlining used in aeroplanes , but what about the drag due to hitting gas molecules ? let 's start with a non-relativistic calculation , say at 0.5c , and use the density of $10^4$ i mentioned above , and let 's suppose that the gas is atomic hydrogen . if the mass per cubic metre is $\rho$ and you are travelling at a speed $v$ m/sec then the mass you hit per second is : $$ m = \rho v $$ suppose when you hit the gas molecules you accelerate them to match your speed , then the rate of change of momentum is this mass times your speed , $v$ , and the rate of change of momentum is just the force so : $$ f = \rho v^2 $$ a density of $10^4$ atoms/cm$^3$ is $10^8$ per m$^3$ or about $1.7 \times 10^{-19}$kg and 0.5c is $1.5 \times 10^8$m/sec so $f$ is about 0.004n per square metre . so unless your spaceship is very big the drag from hitting atoms is insignificant as well , so not only do you not worry about streamlining , you do not have to worry about the cross section either . however so far i have only talked about non-relativistic speeds , and at relativistic speeds you get two effects : the gas density goes up due to lorentz contraction the relativistic mass of the hydrogen atoms goes up so it gets increasingly harder to accelerate them to match your speed these two effects add a factor of $\gamma^2$ to the equation for the force : $$ f = \rho v^2 \gamma^2 $$ so if you take v = 0.999c then you get $f$ is about 7.5n/m$^2$ , which is still pretty small . however $\gamma$ increases without limit as you approach the speed of light so eventually the drag will be enough to stop you accelerating any more . incidentally , if you have a friendly university library to hand have a look at powell , c . ( 1975 ) heating and drag at relativistic speeds . j . british interplanetary soc . , 28 , 546-552 . annoyingly , i have googled in vain for an online copy .
the plummer model has a potential of the form $$ \phi ( r ) =-\frac{1}{\sqrt{r^2+1}} $$ ( obviously ignoring all constants ) . equating the above with the kinetic energy , you get $$ \frac12v_e^2+\phi ( r ) =0\to v_e ( r ) =\sqrt{2}\left ( r+1\right ) ^{-1/4} $$ this velocity is the maximum velocity you can have at a radius $r$ , so we must have that $0\leq v\leq v_{e}$ . in order to get the velocity , we need to know its distribution function ; this can be found in this book . it reduces down to $$ g ( v ) dv\propto\left ( -e\right ) ^{7/2}v^2dv $$ where $e=-v_e^2+\frac12v^2$ . if we let $q=v/v_e$ , then the above becomes $$ g ( q ) =\left ( 1-q\right ) ^{7/2}q^2 $$ with $0\leq q\leq1$ . it then becomes a matter of using either rejection sampling ( which i generally do not recommend ) or the newton root-finding method ( which i usually do recommend ) to find a fit . then you just randomize the angles $\theta , \ , \phi$ and get your x , y , z velocities : you can find more information in this great pdf book .
1 ) you are correct in how you transform the fields , but the condition you derived for scale invariance is incorrect . each piece of the action must be invariant under scale transformations in order that the whole action is scale invariant . you should get $\lambda^{-2-2\delta}=\lambda^{-d}$ and $\lambda^{-k\delta}=\lambda^{-d}$ . you can check you get the expected mass dimensions for a free scalar field in d-dimensions . 2 ) to test invariance under conformal transformations you either have to calculate how your fields transform under special conformal transformations or under inversion . invariance under inversion will imply conformal invariance since k=i*p*i where k is the generator of scts , i is the inversion operator , and p is the translation operator . inversion is a conformal transformation not smoothly connected to the identity and not all conformally invariant theories are inversion invariant ( but these are theories that include spinors which you do not have to worry about here ) .
$$ \delta \rho \cong -\frac{\int_v \ ! \mathrm{d}^3r \phi ( \mathbf{r} ) \sigma_a ( \mathbf{r} ) \phi ( \mathbf{r} ) } {\int_v \ ! \mathrm{d}^3r \phi ( \mathbf{r} ) \sigma_f ( \mathbf{r} ) \phi ( \mathbf{r} ) } $$ it seems like because of a result of one group perturbation theory . page 223 of j . duderstadt nuclear reactor analysis . since the temperature feedback is changing the thermal utilisation , which is the ratio between the absorbed and utilised thermal neutrons .
there is such a theorem about product spaces , it is called the künneth formula . for de rham cohomology , $$h^n ( y \times x ) \cong \bigoplus_{i+j=n} h^j ( x ) \otimes h^i ( y ) . $$ since the cohomology of $s^n$ and $\mathbb r^n$ are both well known , the calculation is now simple . you can find this theorem in the book by bott and tu , differential forms in algebraic topology . you could of course also use a mayer-vietoris sequence with $u$ as a spherical cap slightly south of the equator , and $v$ as a spherical cap slightly north of the equator , both extended for all times . then both $u$ and $v$ have the topology of $\mathbb r^4$ ( because a hemisphere has the topology of $\mathbb r^2$ ) and $u \cap v$ has the topology of $s^1 \times \mathbb r^3$ ( because a band around sphere has the topology of $s^1 \times \mathbb r$ ) . then you need to find the cohomology of $s^1 \times \mathbb r^3$ . you can do this also with the mayer-vietoris sequence . bott and tu use the m-v sequence on $s^1$ as an example , this calculation will be essentially the same . there is a good reason that the calculation will be essentially the same , namely that $s^1$ is a deformation retract of $s^1 \times \mathbb r^3$ . cohomology is invariant under deformation retracts , so we really just need to know the cohomology of $s^1$ . in fact for the original problem of $s^2 \times \mathbb r^2$ has $s^2$ has a deformation retract , so we could have used this from the beginning . however calculating the cohomology of $s^n$ is rather simple with the m-v sequence . you just need to know it for $s^1$ and then you can proceed by induction .
my assumption from hearing that problem is that you do not need to worry about fluid flow and the problem is only asking you to calculate how the mass of water is distributed . if the tank were cylindrical or a right prism , you could simply assume that all the mass was at the midpoint of the height of the tank , and you have to raise it in the gravitational field to get it out of the tank . the work to empty the tank would be $\frac{1}{2} m g h$ , where m is the total mass of water in the tank . but this tank does not have the same cross section from top to bottom . you will need to include the cross section at each height in your integration from the top of the tank to the bottom . is that enough information for you , or do you have some other clues in the problem that suggests that you do need to worry about fluid flow ?
anomalies ( not anamolies ) are a whole subject whose basics are covered by one or several chapters of almost any good enough quantum field theory textbook so it is counterproductive to retype this whole chapter here . but generally , in quantum field theory , anomalies are quantum mechanical effects breaking symmetries that exist in the classical theory – quantum mechanical contributions that are zero in the classical theory because of the symmetry but that are inevitably nonzero in the quantum theory because the quantum completion does not allow all the symmetries to be preserved . an anomaly in a global symmetry is a physical effect that changes the dynamics but keeps it logically consistent ; a gauge anomaly – the quantum mechanical breakdown of a gauge symmetry – renders the theory inconsistent because the gauge symmetry is needed to decoupled the unphysical ( negative-norm ) polarizations of the gauge boson so it should never be broken . anomalies appear because certain divergent integrals cannot be simultaneously set to zero . they appear in theories that admit left-right asymmetry , especially in even spacetime dimensions . the simplest feynman diagrams that quantify the anomaly are $n$-gons , polygons , where $n=d/2+1$ where $d$ is the spacetime dimensions . so importantly enough , anomalies in the $d=10$ dimensional effective theories resulting from superstring theory are calculated using hexagon feynman diagrams . in the undergraduate example of $d=4$ , anomalies are given by $n=3$ i.e. triangle feynman diagrams . a fermion – left-right-asymmetric particle – is running in the loop of the feynman diagram . three gauge bosons are attached . the diagram is proportional to something like $${\rm tr} ( q^3 ) $$ where $q$ is a generator of a $u ( 1 ) $ subgroup of the gauge symmetry , a charge . all such traces of cubic expressions have to vanish for consistency , and they indeed vanish in the standard model – but a cancellation between quarks and leptons is necessary for that . the standard model with leptons only or quarks only would be inconsistent . the mssm has the same chiral spectrum as the standard model with one exception : there are extra higgsinos . they contribute some extra term to the anomalies that are not cancelled by anything . the arguably simplest way to cancel the anomaly is to add not one higgs doublet ( one doublet of superfields ) but two doublets , with mutually opposite values of the charges ( assuming the same chirality ) . so these two mssm higgsinos cancel the anomalies against one another . the two higgs doublets in the mssm are important for another reason : one higgs doublet is only able to give masses to the upper quarks only ; or the lower quarks only . one actually needs both higgs doublets , the up-type and the down-type , to allow all quarks to become massive .
first off , physics tends to provide a very good background for people who move on to study problems in other areas , which is perhaps why there is a lot of cross-over to computer science . however , there are also a number of areas at the interface of computer science and physics which attract people from both sides : computer hardware ( which is generally based on semiconductor physics ) . large scale simulations physics of computation ( quantum computing , reversible computing , etc . ) theoretical computer science etc . of these , perhaps the last one ( tcs ) seems the most surprising . however , in recent years , there has been significant success in applying ideas from thermodynamics and statistical mechanics to problems in computational complexity . an example of this would be the simulated annealing algorithm which works extremely well for optimization problems , as well as work done on phase transitions in 3sat .
dirac 's derivation of the existence of positrons that you described was a totally legitimate and solid argument and dirac rightfully received a nobel prize for this derivation . as you correctly say , the same " sea " argument depending on pauli 's exclusion principle is not really working for bosons . modern qft textbooks want to present fermions and bosons in a unified language which is why they mostly avoid the " dirac sea " argument . but this fact does not make it invalid . the infinite potential charge of the dirac sea is unphysical . in reality , one should admit that he does not know what the charge of the " true vacuum " is . so there is an unknown additive shift in the quantity $q$ and of course that the right additive choice is the choice that implies that the physical vacuum $|0\rangle$ ( with the dirac sea , i.e. with the negative-energy electron states fully occupied ) carries $q=0$ . the right choice of the additive shift is a part of renormalization and the choice $q=0$ is also one that respects the ${\mathbb z}_2$ symmetry between electrons and positrons . it is bizarre to say that dirac missed the lagrangian formalism . dirac was the main founding father of quantum mechanics who emphasized the role of the lagrangian in quantum mechanics . that is also why dirac was the author of the first steps that ultimately led to feynman 's path integrals , the approach to quantum mechanics that makes the importance of the lagrangian in quantum mechanics manifest . it would be more accurate to say that dirac did not understand ( and opposed ) renormalization so he could not possibly formulate the right proof of the existence of the positrons etc . that would also correctly deal with the counterterms and similar things . still , he had everything he needed to define a consistent theory at the level of precision that was available to him ( ignoring renormalization of loop corrections ) : he just subtracted the right ( infinite ) additive constant from $q$ by hand . your sentence the decay of electrons to positrons is then supressed by the u ( 1 ) gauge symmetry of the lagrangian forcing conservation of electrical charge . is strange . since the beginning – in fact , since the 19th century – the u ( 1 ) gauge symmetry was a part of all formulations of electromagnetic theories . it has been a working part of dirac 's theory from the very beginning , too . the additive shift in $q$ , $q=q_0+\dots$ , does not change anything about the u ( 1 ) transformation rules for any fields because they are given by commutators of the fields with $q$ and the commutator of a $c$-number such as $q_0$ with anything vanishes : $q_0$ is completely inconsequential for the u ( 1 ) transformation rules . all these facts were known to dirac , too . the fact that the u ( 1 ) gauge symmetry was respected was the reason that there has never been anything such as a " decay of electrons to positrons " in dirac 's theory , not even in its earliest versions . an electron can not decay to a positron because that would violate charge conservation while the charge has always been conserved . for historical reasons , one could mention that unlike dirac , some other physicists were confused about these elementary facts such as the separation of 1-electron state and 1-positron states in different superselection sectors . in particular , schrödinger proposed a completely wrong theory of " zitterbewegung " ( trembling motion ) which was supposed to be a very fast vibration caused by the interference between the positive-energy and negative-energy solutions . however , there is never such interference in the reality because the actual states corresponding to these solutions carry different values of the electric charge . their belonging to different superselection sectors is the reason why the interference between them can not ever be physically observed . the " zitterbewegung " is completely unphysical .
your equation for the wave is really a vector equation : $$ \psi ( {\bf x} , t ) = a \sin ( \omega t - {\bf k . x} ) $$ this tends to be glossed over when students are first taught the equation , and to be fair in 1d the dot product $\bf k . x$ is simply $kx$ or $-kx$ depending on whether $\bf k$ and $\bf x$ point in the same or opposite directions . anyhow , $\bf k$ is the wave vector and like all vectors has direction and magnitude . its magnitude is the wave number $k$ , and the wave number is equal to $2\pi/\lambda$ . because $k$ is the magnitude of a vector it is always positive , and therefore $\lambda$ is always positive . when you have a wave travelling in the $-x$ direction it is not $\lambda$ that changes sign , it is the direction of $\bf k$ .
no , a massive body is able to bend light around it , which is called gravitational lensing . this has been observed multiple times . edit photons are massless . otherwise , they would not travel at the maximum speed , which is called speed of light . keep in mind , that gravitational lensing is not a part of newtonian mechanics . you need general relativity for that . and in the context of general relativity , it is not mass , which exerts gravitation , but energy . and photons clearly carry energy . so , you could produce a gravitational well ( even a black hole ) entirely without massive particles .
the main effect of an electromagnetic wave is basically that the electric field in the electromagnetic wave shoves charged particles around ( ions and/or electrons ) . that is called " electric dipole coupling " . electric dipole coupling is almost always much stronger than other effects of the electromagnetic wave . for example , the electromagnetic wave has a magnetic field too , which can exert forces on molecules . this " magnetic dipole coupling " is a much smaller effect than the electric dipole coupling . shoving an electron around ( the electric dipole coupling ) does not directly change or rotate the electron 's spin . it only changes the spin a little bit , due to " spin-orbit coupling " , an effect related to special relativity ( in the " rest frame " of the moving electron , the electric fields get converted to magnetic fields , which torques the spin ) . therefore , since spin-orbit coupling is a weak effect ( unless the electron is traveling near the speed of light ) , the coupling between electromagnetic waves and molecules cannot usually involve a change of spin . that means that a spin-singlet molecule absorbing light almost definitely goes into a spin-singlet excited state . conversely , it is exceedingly unlikely for a spin-triplet excited state to emit light while changing into a spin-singlet ground state . by the way , another possibility is for the magnetic field of the light wave to directly torque the spin of the electron . this effect is normally even weaker than the effect of the wave via the spin-orbit effect .
it is true that by noether 's theorem energy is conserved when the action is depednent on position only and not time . however , i think they want you to write down the equation of motion ( use $f=ma$ ) and then use that to show that the total energy is a constant . write down the expression for total energy and take its derivative w.r.t. time .
both definitions are fine as long as you are careful with signs . here 's a derivation of your gravitational potential energy using the idea of the negative work done " by the field . " ( note the initial negative sign . ) $$u ( r ) =-w_\text{by field}=-\int\vec{f}_\text{field}\cdot d\vec{s}=-\int_{r=\infty}^{r=x}\underbrace{\frac{-gmm\ , \hat{r}}{r^2}}_\text{toward origin}\cdot d\vec{s}=\int_{r=\infty}^{r=x}\frac{gmm}{r^2}dr=-\frac{gmm}{x}$$ the negative inside the integral indicates that the force by the field is toward the origin , in the $-\hat{r}$ direction . now , divide by $m$ and you have your expression . here 's a derivation using the work done by the external agent . if the object does not accelerate , the net force on the object is zero . thus , the force exerted by the external agent is equal in magnitude and opposite in direction to that by the field . we indicate this by explicitly writing that this force is away from the origin : $$u ( r ) =+w_\text{ext agent}=+\int\vec{f}_\text{ext}\cdot d\vec{s}=+\int_{r=\infty}^{r=x}\underbrace{\frac{+gmm\ , \hat{r}}{r^2}}_\text{away from orig . }\cdot d\vec{s}$$ we can stop there since we already have the same expression as our first derivation .
the higgs is inferred from its decays - no higgs bosons themselves are actually detected . as such , an experiment can count the number of higgs boson decays to a particular final state , if it can measure the particles in that final state . for that reason , it is impossible to measure $\sigma$ or the branching ratios independently , you can only measure $\sigma \times br$ . to extract a branching ratio from the measurements , one would have to make a model-dependent assumption about the production cross-section , $\sigma$ , or assume that there were no " invisible " decay channels with final states that we could not see . instead , what typically happens , is that the coupling strengths , $\kappa$ , related to lagrangian couplings in an effective field theory approach , are fitted to all of the data , simultaneously . that allows us to extract the lagrangian couplings for the higgs boson . this need not be true at an electron collider , at which the dominant production cross section would be $ee\to hz$ . this production cross section could be measured independently of the branching ratios by analysing the $z$ , because the electron collider environment is cleaner than that of the lhc .
the " other requirements " are the real requirements , the string business is just a gloss , which omits the most important s-matrix assumptions . . there is another question regarding this here : are there strings that aren&#39 ; t chew-ish ? ( and the linked discussion explains some of the issues ) . string theory is not just a theory of strings . in its simplest formulation , it is a theory of strings which can only interact by exchanging other strings , not by exchanging point particles ( or anything else ) . this excludes things like atomic polymers , or strings made out of points , or strings that interact by self-intersection , except to the extent that you can view the special string-theory strings as made out of string bits , like in matrix theory . the historical marginalization of s-matrix theory and its practitioners is the main reason that the string assumption is played up , and the bootstrap assumptions are played down . bootstrap was politically unpopular , and any theory that said " bootstrap " would be ignored in the 1980s and 1990s . a good list of requirements on string theory is this : there are strings , so the spectrum of the theory is the oscillation spectrum of a string worldsheet action . the exchange of strings is the source of all the ( perturbative ) forces in string theory . this means that once you know the string spectrum , you know the interactions are by summing over all intermediate states of the string alone , with nothing else . the scattering is regge-soft scattering in regge limits . this requirement is technical , and hard to state for a general audience , so it is left out . what it says is that the sum over all intermediate string states gives cancelling amplitude from particles of different spins , and that these cancellations lead to an amplitude that falls off faster than a power at very high energies . although each spin-n state gives an amplitude that blows up ever harder at high energies . this is sometimes called the bootstrap assumption , the assumption that everything in the theory is a bound state which is part of a family of related bound states which together give softer scattering than each one individually . the exchange of strings in the t-channel is the same as exchange in the s-channel , and does not require counting the particles separately . this gives the world-sheet picture , from the symmetry properties of tree diagrams in such a theory . that the interactions are by worldsheet just is not derivable from the spectrum alone , without the assumption that the spectrum bootstraps , and does not resolve to something else at short distances . these requirements partially overlap , nobody has made orthogonal axioms . together with the following rule : the string must have a fermionic excitation they should be enough to uniquely determine the perturbative superstring theories . it is clear that there is at least one string theory that people missed completely , this is simeon hellerman 's m-theory-on-a-klein-bottle strings , and there are tons of different vacua which could be thought of as new theories in this formulation , because each s-matrix is a different theory . these perturbative string theories link up non-perturbatively into an ubertheory called m-theory . a theory today would be called part of string theory if it is a description of some configuration of m-theory . there is no full enumeration of these , so it is impossible to say exactly what a configuration is , although there is a partial list , and there is no real arbitrariness .
the wedge is tangent to the sphere . using that it touches at height r/5 you can easily work out the slope of the wedge . the velocity of the sphere follows directly ( 20m/s times slope ) .
anywhere near room temperature , most of the dopants will be ionized . the photoconductivity is indeed caused by electron-hole pair generation . in an extrinsic material ( almost all silicon you will find is extrinsic ) , the question is what is the carrier concentration . in silicon , the intrinsic carrier concentration is a little over 1e10/cc , so " extrinsic " is anything above that . if your background doping is say 1e14 , than your photo-generated carriers can easily be multiple orders of magnitude larger , and be the conduction mechanism for all practical purposes .
there are 3 observations that support the big bang theory , i.e. origin of the universe in a singularity : the redshift of galaxies , as you already mentioned . the cosmic background radiation . the amounts of different nuclei in the universe , notably the preponderance of light elements like hydrogen and helium . each of these alone would probably not be sufficient to support the big bang theory . the redshift of galaxies could be explained by some other theory , some have been suggested by hoyle and narlikar in the past . probably the other two phenomena could be explained independently as well , but it is the conjunction that fits so well with the big bang hypothesis . does that settle the matter once and for all ? short answer is no . since these 3 observations have been made and confirmed , more detailed observations have been added to the mix and this has complicated the story for the big bang model . but that would take us into a longer post . the current model which is the most widely accepted is the so-called lambda-cdm model . as for the problem of the universe starting in a real singularity , instead of a very dense state , this is still an open problem related to a yet to be invented ( or completed ) theory of quantum gravity . our current understanding of singularities in general relativity is going back to the penrose-hawking singularity theorems . they are of the kind " here be dragons ! " in that they delineate the conditions for singularities to form and point where our knowledge ends . more can not be done , because a singularity is basically a failure of the theory .
one should not expect to have a " good " formula for the local isometric embeddings of a constant negative curvature surface in euclidean $\mathbb{r}^3$ . this is due to a little theorem proved by david hilbert around 1901: theorem there does not exist a smooth immersion of the hyperbolic plane into euclidean 3 space . the theorem has been further studied in the years following . in 1961 efimov showed that any complete surface with curvature strictly bounded above ( that is to say , if there exists a negative number $k_0 &lt ; 0$ such that the gaussian curvature is always strictly less than $k_0$ ) cannot admit a smooth ( twice continuously differentiable ) isometric immersion into euclidean three space . that is to say , if you try to " extend " any surface in euclidean 3 space that satisfies constant negative curvature , you are guaranteed to hit a singularity . in particular , you cannnot expect the surface to be described by $f ( x , y , z ) = 0$ where $f ( x , y , z ) $ has a nice algebraic expression ( say , polynomial ) and has smooth level sets . typically the image one usually use to illustrate the notion of negative ( but not constant ) curvature is the graph $$ z = x^2 - y^2 $$ which produces a classical saddle , or the catenoid whose gaussian curvature , while everywhere negative , is not constant . ( though it has constant [ in fact everywhere vanishing ] mean curvature . ) lastly , however , despite the above , it is possible to embed " patches " of hyperbolic plane into euclidean 3 space . there are many ways of doing so ( one can search for the term pseudosphere ; though some people use the same term for the hyperboloid/de sitter spaces embedded in higher dimensional minkowski space ) , but one of the more well-known is the tractricoid . ( see wiki entry here . ) parametrically in cylindrical coordinates $ ( z , r , \theta ) $ the surface can be described by : $$ \mathbb{r}_+\times\mathbb{s}^1 \ni ( t , \omega ) \mapsto \left ( z=\frac{1}{\cosh t} , r=t-\tanh t , \theta = \omega\right ) $$ and has constant negative curvature .
a good question . the discrepancy between the counting for electric and magnetic multipoles appears because in the potentials , the electric field is encoded in the potential $\phi$ while the magnetic field is encoded in the vector potential $a$ . however , to find the electric field $e$ , you take the gradient of $\phi$ combined with the time derivative of $a$ ( both with a minus sign ) . there are no spatial derivatives in the second term so it is the dominant one , in a $1/r$ expansion . to obtain the magnetic field $b$ , you need to take the curl of $a$ - which means its spatial derivatives . there is no way to get a term to $b$ from $\phi$ and $a$ that would contain no spatial derivatives . from a relativity viewpoint , spatial and temporal derivatives are of the same order . however , things are different for the analysis of static objects . it is only the spatial derivatives that produce an extra power of $1/r$ . so the electric field $e$ is of the same order as the vector potential $a$ ( the time derivative does not add any $1/r$ ) while the magnetic field $b$ has an extra $1/r$ relatively to the vector potential $a$ . quadrupoles have one extra $1/r$ relatively to the dipoles - the standard expansion in $1/r$ . so to go from the electric dipoles to the next order , you either add the $1/r$ by switching from the electric to magnetic fields ; or you get the extra $1/r$ by going from electric dipoles to electric quadrupoles . at any rate , the counting of powers is shifted by one between the electric and magnetic multipoles . best wishes lubos
this question is quite a common one for those first learning about capacitors . first , let 's remember that an electric field caused by stationary charges is conservative--this can easily be explained since a single charge creates a conservative field , and superposition of two conservative fields creates another conservative field . so , the field generated by a floating capacitor has to be conservative . the universe is not crazy , so it is probably us missing something ? are there any assumptions that we made while calculating the field ? yes , there are : we assumed that the capacitor was infinite in size , and thus the field became uniform . but , here , we are dealing with the edges of the capacitor . the field is not uniform here , it is more like ( second half of image ) : or : when it comes back out , the x-component of the field will be against the velocity of the particle , slowing it down back to the initial speed . for example , for a positively charged particle , the trajectory is as follows : the green indicates the force on the particle at various points . once the particle exits , it is " pulled back " . the net effect is that the speed stays the same but the direction does not . perfectly in accordance with conservation of energy . ignoring fringe fields can lead to some interesting apparent paradoxi , like the origin of the force that pulls a dipole slab into a capacitor .
starting without the beam splitter , just to establish notation , label the slits 1 and 2 , the laser l and a point on the screen x . then $\langle 1| l \rangle$ is the ( complex ) amplitude to go from the laser to slit 1 . $\langle x| 1 \rangle$ is the amplitude to go from slit 1 to a point on the x on the screen . the amplitude to do that path combination is $\langle x| 1 \rangle\langle 1| l \rangle$ . given that it must land somewhere on the screen we know $$ \sum_x ( |\langle x| 1 \rangle\langle 1| l \rangle + \langle x| 2 \rangle\langle 2| l \rangle |^2 ) =1$$ ( or integral . . . ) if we now place a beamsplitter b right after slit 2 , then instead we have$$|\langle c| b \rangle \langle b| 2 \rangle \langle 2| l \rangle|^2+\sum_x ( |\langle x| 1 \rangle\langle 1| l \rangle + \langle x| b \rangle \langle b| 2 \rangle\langle 2| l \rangle |^2 ) =1$$ now we know a few things about the constituents : $$|\langle 1|l \rangle |^2=0.5 $$ $$|\langle 2|l \rangle |^2=0.5 $$ $$|\langle b|2 \rangle |^2=1 $$ $$|\langle c|b \rangle |^2=0.5 $$ so the first term is $\frac{1}{4}$ ( propagation into the camera ) and the sum of the other terms must be $\frac{3}{4}$ ( propagation to somewhere on the screen ) .
inside the event horizon all timelike paths lead to the singularity . a timelike path is one along which you never travel at a speed greater than c . a static black hole is spherically symmetric ; any asymmetries are radiated away as gravitational waves as the black hole forms . therefore the singularity must be at the centre or it would break the spherical symmetry .
the central point of the question is somewhat ambiguous , but here is an effort to answer it . i am sorry in advance if i have misunderstood it . does light/photons travel ? the question whether light travels from place a to place b or not , can be answered mainly by experience and experiment/observation . when you hold a torch in the dark and you aim it at some point in the background where it is dark , you can see its effects almost immediately . from having being dark , now it is bright and you can see the objects that exist there . that means that light not only travelled there and illuminated the area , it also came back to your eye to give you the information about the objects . this means that light has not always been there , suspended in the air , waiting for you to turn the torch on and make it become reality . i don’t think this is how you envision it . does light “feel” the existence of space ? this type of questions touch on the borders of ontology , somewhat . it is not very easy to formulate answers because one has to talk in terms of metaphysical notions and concepts which , unfortunately , fall outside the scientific method of thinking . but let us take a look at it from this point of view : imagine we send a laser beam from one side of our room to the other . watching it without an apparatus it looks as if light did not have to travel at all , it looks as if the event evolved instantly . a very sensitive apparatus , however , can sense that light has actually taken some time to go there and back . the situation can become more obvious if we try to send the laser beam to the moon and back ( this has been done . ) even we , without any apparatus , can tell that the distance involved must be huge . so space becomes important and even light “feels” the vastness of it . in the experiments you mentioned , the extremely sensitive detectors can distinguish photons arriving with a time difference just a few nanoseconds or less , due to the slightly different paths they take ( space becomes very important in less obvious ways ) light can even “feel” the geometry of space-time , as is demonstrated by the deflection of light-rays passing near the surface of the sun , during a total solar eclipse . light can “feel” the immense density of a bose-einstein condensate by slowing down to incredibly low speed . you can run fast enough and catch up with it ! ! the question whether or not light takes a well defined path to go from a to b involves quantum mechanics , and from your comment i read that it does not interest you at the moment ( ? )
in special relativity , hertz per dioptre is an excellent unit for showing the joint invariance of electromagnetic phenomena in the behavior of all types of lenses , reflective or refractive , under the effects of the lorentz transformation along the axis of motion . i am not aware of any other unit that links those two domains in quite that way . in the case of refractive lenses with chromatic dispersion , the invariance turns out to be non-trivial and a bit surprising , since it asserts that the atomic materials in a lorentz compressed lens must maintain a very specific relationship in how they interact with a spectrum of gamma-shifted light frequencies . here 's how it works for the easier reflective-lens case . first , imagine a sphere 4 meters across with an $f=280$ thz resonant infrared light wave inside . why 4 meters ? well , i am trying to use the correct definition of dioptre . that is the focal length of a refractive or reflective lens , which means the distance it requires to converge parallel light down to a single focal point . in this case , the lens is reflective and has spherical curvature . looking only at a region small enough ( e . g . 2 cm across ) to avoid spherical aberration , the focal length of the $d=4$ m sphere is $l=\frac{1}{2}r=\frac{1}{4}d=\frac{1}{4}4=1$ m . so , a 4 m diameter sphere thus correctly gives a dioptre ( curvature ) of $\delta=1/l=1/1=1$ d , where d $=m^{-1}$ . next , accelerate the sphere along it x axis to a velocity of $v=\sqrt{\frac{3}{4}}$ c , which gives a lorentz factor of $\gamma=2$ . that means that both the sphere and the resonant light pattern within it will be compressed to $\frac{1}{2}$ their original lengths along the x axis , from the perspective of a viewer " at rest " relative to the moving sphere . for the small reflective lens regions around either end of where the x axis crosses the sphere , the pre-acceleration curvature was $\delta_0=1d$ ( the zero subscript indicates the rest frame ) . after acceleration to $\gamma=2$ the sphere becomes an oblate spheroid , and the curvatures of the two reflective lens areas have been reduced to $\delta_1=2d$ , where higher dioptre numbers indicate flatter curves . ( the proof of that is left as an exercise for the reader , but it is not difficult . ) now let 's examine what happens to the frequency of the light within the sphere . the neat thing about special relativity is that physics must remain invariant for both the observer and the observed system . so , if there were n wavelengths of resonant light crossing the sphere along the x axis prior to it being accelerated , there must also be n wavelengths along that same length after the compression . in other words , the wavelengths of the radiation must also be cut in half along x ( only ) , resulting in twice the frequency as before . that transforms the original x-axis $f_0=280$ thz light of the at-rest sphere into $f_1=560$ thz light in the moving sphere . an observer in the rest frame would see this as bright green . observant readers may now be saying " hey , that can not be right ! the lorentz factor also slows time . . . so should not the light in the moving sphere be slower and thus less energetic ? " while it is true that time will pass more slowly within the moving sphere , it is not correct to think that this same light will be slower when viewed from the rest frame . for that situation the geometry of the wavelengths wins , and the light looks green . however , a simpler way to think of it is that since the light is being emitted and reflected by an object traveling at $\gamma=2$ ( or equivalently $v=\sqrt{\frac{3}{4}}$ c ) , the ordinary doppler effect will double its frequency . ( @colink has correctly noted that the above explanation glosses over some important complications . please see his excellent comment for more info . i may try to address that soon . ) now it is time to put this all together . the original light and sphere had an eta factor of : $\eta_0=f_0/\delta_0 = ( 280 thz ) / ( 1 d ) = 280\times{10}^{12}$ hpd where 1 hpd = 1 hz/d ( hertz per dioptre ) . the moving light and sphere has an eta factor of : $\eta_1=f_1/\delta_1 = ( 560 thz ) / ( 2 d ) = 280\times{10}^{12}$ hpd . in other words , the eta factor $\eta$ , which relates the lorentz-transformed electromagnetic waves to the lorentz-contracted physical mirrors from which they reflect , has remained invariant for this example of $\gamma=2$ . it is not an isolated case . it is easy to show that $\eta$ is a universal invariant of special relativity : $\forall{v_i} ( \eta_i=\frac{f_i}{\delta_i}= c ) $ where c is a constant in units of hpd = hz/d = hertz per dioptre . now the remarkable generalization of all of this is that by the same kinds of geometric arguments and application of the " physics must be preserved in both frames " principle , refractive lenses must also fall under the above argument . if a refractive lens has chromatic dispersion ( the colored fringes seen in cheap lenses ) , then the constant c in the above equation will become a frequency-dependent value $c ( f ) $ . yet the eta invariance remains intact ! that is surprising because light dispersion is a pretty complicated phenomenon , yet from the rest frame these messy compressed atoms must nonetheless maintain eta invariance . that is . . . unexpected . thus hpd units not only have real physical meaning , but a meaning that relates directly to the original intent of both the hertz and dioptre units ( versus just being $m/s$ in disguise ) . this meaning in turn provides an easy way to express an invariant relationship in special relativity that links together the electromagnetic and mechanical lorentz transformations in an unexpected and non-intuitive fashion . and finally , despite all the above unexpectedly interesting ( to me at least ! ) sr relationships involved , the hpd unit really did originate as a bit of humor in ( as best i could uncover ) this xkcd discussion posting back in 2007 . so , shrodingersduck from the people 's democratic republic of leodensia , wherever you are six years later , i thank you for inadvertently creating an interesting and quite fun opportunity to explore special relativity in a rather unusual context . addendum 2013-01-31 the generality of the hpd unit in special relativity can i think be stated even more broadly . so , here goes : light frequency , geometric forms , and frequency-dependent refractive indices all change when systems undergo lorentz transformation , so they are not individually lorentz invariant . theorem : if the optical characteristics of an optical system are instead described using hpd ( hertz per dioptre ) and/or its inverse unit dph ( dioptres per hertz ) , the resulting description of its optical properties will remain constant ( "eta invariance" ) regardless of relativistic frame or orientation from which the optical system is analyzed . that is a theorem only . @colink 's excellent observation that the doppler argument i made could be bogus because the shift works differently depending on whether the light is moving with or against the velocity still concerns me . so , i want to look at that a lot more closely and see if i can disprove my own theorem . still , would not it be delightful if a unit defined as a joke turned out to be relativistically invariant when the common units for the same phenomena are not ? the other obvious generalization question is this : does eta invariance ( if it exists ) apply to other wave phenomena ? and finally , @joezeng , i think i misunderstood your question about whether the eta factors ( descriptions of optical components using hpd units ) are related to the velocity of light . well , hpd does have dimensional equivalence to a velocity ( $m/s$ ) , but if there is a meaningful way to re-interpret an hpd value as a velocity , i sure do not see it . intriguing question , though . . .
i put an extra answer , since i believe the first jeremy 's question is still unanswered . the previous answer is clear , pedagogical and correct . the discussion is really interesting , too . thanks to nanophys and heidar for this . to answer directly jeremy 's question : you can always construct a representation of your favorite fermions modes in term of majorana 's modes . i am using the convention " modes " since i am a condensed matter physicist . i never work with particles , only with quasi-particles . perhaps better to talk about mode . so the unitary transformation from fermion modes created by $c^{\dagger}$ and destroyed by the operator $c$ to majorana modes is $$ c=\dfrac{\gamma_{1}+\mathbf{i}\gamma_{2}}{\sqrt{2}}\ ; \text{and}\ ; c{}^{\dagger}=\dfrac{\gamma_{1}-\mathbf{i}\gamma_{2}}{\sqrt{2}} $$ or equivalently $$ \gamma_{1}=\dfrac{c+c{}^{\dagger}}{\sqrt{2}}\ ; \text{and}\ ; \gamma_{2}=\dfrac{c-c{}^{\dagger}}{\mathbf{i}\sqrt{2}} $$ and this transformation is always allowed , being unitary . having doing this , you just changed the basis of your hamiltonian . the quasi-particles associated with the $\gamma_{i}$ 's modes verify $\gamma{}_{i}^{\dagger}=\gamma_{i}$ , a fermionic anticommutation relation $\left\{ \gamma_{i} , \gamma_{j}\right\} =\delta_{ij}$ , but they are not particle at all . a simple way to see this is to try to construct a number operator with them ( if we can not count the particles , are they particles ? i guess no . ) . we would guess $\gamma{}^{\dagger}\gamma$ is a good one . this is not true , since $\gamma{}^{\dagger}\gamma=\gamma^{2}=1$ is always $1$ . . . the only correct number operator is $c{}^{\dagger}c=\left ( 1-\mathbf{i}\gamma_{1}\gamma_{2}\right ) $ . to verify that the majorana modes are anyons , you should braid them ( know their exchange statistic ) -- i do not want to say much about that , heidar made all the interesting remarks about this point . i will come back later to the fact that there are always $2$ majorana modes associated to $1$ fermionic ( $c{}^{\dagger}c$ ) one . most has been already said by nanophys , except an important point i will discuss later , when discussing the delocalization of the majorana mode . i would like to finnish this paragraph saying that the majorana construction is no more than the usual construction for boson : $x=\left ( a+a{}^{\dagger}\right ) /\sqrt{2}$ and $p=\left ( a-a{}^{\dagger}\right ) /\mathbf{i}\sqrt{2}$: only $x^{2}+p^{2} \propto a^{\dagger} a$ ( with proper dimension constants ) is an excitation number . majorana modes share a lot of properties with the $p$ and $x$ representation of quantum mechanics ( simplectic structure among other ) . the next question is the following : are there some situations when the $\gamma_{1}$ and $\gamma_{2}$ are the natural excitations of the system ? well , the answer is complicated , both yes and no . yes , because majorana operators describe the correct excitations of some topological condensed matter realisation , like the $p$-wave superconductivity ( among a lot of others , but let me concentrate on this specific one , that i know better ) . no , because these modes are not excitation at all ! they are zero energy modes , which is not the definition of an excitation . indeed , they describe the different possible vacuum realisations of an emergent vacuum ( emergent in the sense that superconductivity is not a natural situation , it is a condensate of interacting electrons ( say ) ) . as pointed out in the discussion associated to the previous answer , the normal terminology for these pseudo-excitations are zero-energy-mode . that is what their are : energy mode at zero-energy , in the middle of the ( superconducting ) gap . note also that in condensed matter , the gap provides the entire protection of the majorana-mode , there is no other protection in a sense . some people believe there is a kind of delocalization of the majorana , which is true ( i will come to that in a moment ) . but the delocalization comes along with the gap in fact : there is not allowed propagation below the gap energy . so the majorana mode are necessarilly localized because they lie at zero energy , in the middle of the gap . more words about the delocalization now -- as i promised . because one needs two majorana modes $\gamma_{1}$ and $\gamma_{2}$ to each regular fermionic $c{}^{\dagger}c$ one , any two associated majorana modes combine to create a regular fermion . so the most important challenge is to find delocalized majorana modes ! that is the famous kitaev proposal arxiv:cond-mat/0010440 -- he said unpaired majorana instead of delocalised , since delocalization comes for free once again . at the end of a topological wire ( for me , a $p$-wave superconducting wire ) there will be two zero-energy modes , exponentially decaying in space since they lie at the middle of the gap . these zero-energy modes can be written as $\gamma_{1}$ and $\gamma_{2}$ and they verify $\gamma{}_{i}^{\dagger}=\gamma_{i}$ each ! to conclude , an actual vivid question , still open : there are a lot of pseudo-excitations at zero-energy ( in the middle of the gap ) . the only difference between majorana modes and the other pseudo-excitations is the definition of the majorana $\gamma^{\dagger}=\gamma$ , the other ones are regular fermions . how to detect for sure the majorana pseudo-excitation ( zero-energy mode ) in the jungle of the other ones ?
yeah , neils bohr and john von neumann were skeptics : many prominent physicists thought it could not even work , based on their knowledge of physical principles . in quantum mechanics , the uncertainty principle developed by einstein , says that the energy ( and therefore the frequency , by e=hv ) of a photon can not be known to great precision in a short time . in masers , photons last for a very short time . therefore , no less than neils bohr and john von neumann thought it could not work , even after it had been created . the solution to this apparent paradox is that , though the photons all have the same frequency and direction , which atoms do the emitting and when remains unknown . the emitting atoms maintain an anonymity that avoids uncertainty > violation . read more : why was the laser light invented ? | ehow . com http://www.ehow.com/about_5250763_laser-light-invented_.html#ixzz1uic0yare
the weld acts on the rod with force components $b_x$ and $b_y$ as well as a moment $m$ . the equations of motion for the rod are $$ b_x = m_{rod} \left ( - \cos\theta \ , \ddot{q} \right ) $$ $$ b_y = m_{rod} \left ( \sin\theta \ , \ddot{q} + g \right ) $$ $$ m - \frac{l}{2} b_y = i_{rod} \dot{\omega}_{rod} = 0 $$ where $q$ is the distance along the guide of travel and the last equation is the sum of moments about the center of gravity of the rod . gravity is included above as $g$ . the equations of motion for the block are $$ -f\ , \cos\theta + n \sin\theta - b_x = m_{block} \left ( - \cos\theta\ , \ddot{q} \right ) $$ $$ f\ , \sin\theta + n \cos\theta - b_y = m_{block} \left ( \cos\theta\ , \ddot{q} + g\right ) $$ where $n$ is the contact normal force . combined the above is $$ m_{rod} \left ( \ddot{q} + g\sin\theta \right ) - f = -m_{block} \left ( \ddot{q} + g\sin\theta \right ) $$ $$ n-m_{rod} g \cos\theta = m_{block} g \cos\theta $$ with solution $$ n = \left ( m_{rod}+m_{block}\right ) \ , g \cos\theta $$ $$ \ddot{q} = \frac{f}{m_{rod}+m_{block}} - g \sin \theta $$ now going back to moment , $m=\frac{l}{2} b_y$ which you can solve now .
your query is valid : how is voltage a ' wave ' that reflects and creates standing waves ? well , the answer is quite simple when you stop and think about it . all signals travelling across transmission lines are merely electromagnetic waves . now these lines are commonly driven by voltage sources , hence they are ' voltage waves ' . this makes perfect sense : if a simple circuit is driven by a sinusoidal voltage source , you expect that the resultant voltage ( amplitude ) will vary like the sine wave . if you extend your thinking a bit , you will see that the same question can apply to any circuit with a sinusoidal voltage source . we do not usually apply the concept of ' waves ' to a simple circuit ( the concept is only useful for the analysis of tls since reflections can be a bit of nuisance and at radio frequencies are important ) , but its present . ever heard of lc circuits ? these circuits are basically resonators where the energy from the source ( a voltage source ) keeps on changing forms . but how can that happen , if the source is a ' voltage ' ? this analogy would help you understand that all signals are just electromagnetic waves . the reason we call them ' voltage waves ' in rf engineering is simply because at those frequencies the wavelike behavior of the source helps us in the analysis/design of the circuit ( through the use of matching circuits , tuning circuits etc ) . hope this answers your query . feel free to inquire more . ok it seems you have a few more queries . let 's get to them . 1 ) yes , an em wave does not require any medium to travel ( that is how antennas in space work ) . however , ' containing ' an em wave inside a conductor is not a big deal . its a simple matter of creating a potential difference and letting it do all the work . you could , ofcourse , ask the same question when you connect a simple wire to a voltage source . the explanation remains the same . 2 ) now why does the wave reflect at an open circuit ? the answer to this question you will encounter in your course , but i will give a simple version of it here . at the open circuit point the current in the line is zero ( by the definition of an open circuit ) . since charge continues to arrive at the end of the line through the incident current , but no current is leaving the line , then conservation of electric charge requires that there must be an equal and opposite current into the end of the line . essentially , this is kirchhoff 's current law in operation . this equal and opposite current is the reflected current and by ohm 's law , it creates a reflected voltage wave . 3 ) what brandon enright posted about is the tried and tested , age-old analogy between electricity/electronics and hydraulics , which every engineer has used at some point . it is perfectly correct . please refer this link ( http://en.wikipedia.org/wiki/hydraulic_analogy ) to further understand it and relate it to your query .
first of all you should not be using the classic ( incompressible ) bernoulli equation for this kind of problem as you are clearly in the compressible flow regime . you should be using the isentropic flow relations because they are more physically accurate ( $\gamma=1.4$ ) : $\frac{p_t}{p}= [ 1+\frac{\gamma -1}{2}m^2 ] ^\frac{\gamma}{\gamma-1}$ ; $\frac{t_t}{t}=1+\frac{\gamma -1}{2}m^2$ ; $\frac{\rho_t}{\rho}= [ 1+\frac{\gamma -1}{2}m^2 ] ^\frac{1}{\gamma-1}$ plus we will employ the ever-useful adiabatic relation ( which is true even for non-isentropic flows ) : $t_t=t+\frac{v^2}{2c_p} \rightarrow \boxed{t=t_t-\frac{v^2}{2c_p}}$ since you already know the engine massflow rate , the rest of the problem is straightforward . applying ideal gas law and the definition of mach number to the basic massflow equation , we have . . . $\dot{m}_1=\rho_1 v_1 a_1= ( \frac{p_1}{rt_1} ) ( m_1\sqrt{\gamma rt_1} ) a_1=p_1a_1m_1\sqrt{\frac{\gamma}{rt_1}}$ this last bit is the key , since if you know $\dot{m}_1$ , $a_1$ , $m_1$ , and $t_1$ you will easily be able to find $p_1$ . and in fact , this really just boils down to finding $t_1$ because you already know the inlet velocity and $m_1=\frac{v_1}{\sqrt{\gamma rt_1}}$ . the key insight here is that the stagnation properties ( aircraft reference frame ) do not change from the freestream ( $\infty$ ) to the inlet lip ( $1$ ) . thus , $t_t=t_\infty+\frac{v_\infty^2}{2c_p}=t_1+\frac{v_1^2}{2c_p}$ , which implies that $\boxed{t_1=t_\infty+\frac{v_\infty^2-v_1^2}{2c_p}}$ , with $c_p=\frac{\gamma r}{\gamma-1}$ since we know the ambient static temperature , aircraft flight speed ( 245m/s ) and the inlet flow velocity ( 215m/s ) we can solve for the inlet static temperature . everything else follows from this information and you should be able to find everything you need for the rest of the problem . because the flow is decelerating into the engine during cruise , we would expect the inlet static temperature to be slightly higher than the freestream ( and it is ) . another way to compute the answer is to again leverage the fact that the stagnation properties do not change as the flow isentropically decelerates into the engine , except that now we will equate the stagnation pressures instead of temperatures : $p_t=p_\infty [ 1+\frac{\gamma -1}{2}m_\infty^2 ] ^\frac{\gamma}{\gamma-1}=p_1 [ 1+\frac{\gamma -1}{2}m_1^2 ] ^\frac{\gamma}{\gamma-1}$ or $\boxed{p_1=p_\infty\left [ \frac{2+ ( \gamma -1 ) m_\infty^2}{2+ ( \gamma -1 ) m_1^2}\right ] ^\frac{\gamma}{\gamma-1}}$ where $m_\infty=\frac{v_\infty}{\sqrt{\gamma rt_\infty}}$ and $m_1=\frac{v_1}{\sqrt{\gamma rt_1}}$ good luck ! ! ! p.s. your diagram is wrong . the two velocities should be pointing in the same direction . the freestream is approaching the aircraft at 245m/s , and the air actually enters the engine at 215m/s ( it slows down ) . both velocities are relative to the aircraft reference frame and pointed in the same direction . also , your engine massflow is wrong because it assumes $\rho_1=\rho_\infty$ , which is incorrect . you need to use the isentropic relations here as well in a way analogous to the method outlined above .
whatever direction you look , you are looking back to a time closer to the big bang . i am guessing that is what you read . after all , there is no location in space of the big bang ( i can elaborate further on this if desired , but i suspect it has been answered in another question ) , so anywhere you move will not get you any closer to a point that does not exist . as to the second part of your question , sadly the answer is no , not at all . anywhere you can get to by going no faster than the speed of light will be further along in its cosmic evolution than we are here and now . there is no way your future can be in the past so to speak . the whole universe is aging uniformly , and so the stars will eventually die everywhere . [ regarding those star deaths , though , that will not happen for a very long time , even by the universe 's standards . stars somewhat smaller than our sun have lifetimes on the order of 100 billion years . for comparison , the universe is only about 14 billion years old right now . even if stars stopped forming today , solar systems around such stars will go on pretty much unchanged for a very long time . ]
a lot is known about qfts ( including qed ) at finite time . it is tractable approximately ( just like scattering ) . though in 4d no rigorous treatment is available ( neither is there one for scattering ) . one can compute - nonrigorously , in renormalized perturbation theory - many time-dependent things , namely via the schwinger-keldysh ( or closed time path = ctp ) formalism . for example , e . calzetta and b . l . hu , nonequilibrium quantum fields : closed-time-path effective action , wigner function , and boltzmann equation , phys . rev . d 37 ( 1988 ) , 2878-2900 . derive finite-time boltzmann-type kinetic equations from quantum field theory using the ctp formalism . there are also successful nonrelativistic approximations with relativistic corrections , within the framework of nrqed and nrqcd , which are used to compute bound state properties and spectral shifts . see , e.g. , hep-ph/9209266 , hep-ph/9805424 , hep-ph/9707481 , and hep-ph/9907240 . there is also an interesting particle-based approximation to qed by barut , which might well turn out to become the germ of an exact particle interpretation of standard renormalized qed . see a.o. barut and j.f. van huele , phys . rev . a 32 ( 1985 ) , 3187-3195 , and the discussion in phys . rev . a 34 ( 1986 ) , 3500-3501,3502-3503 . approximately renormalized hamiltonians , and with them an approximate dynamics , can also be constructed via similarity renormalization ; see , e.g. , s.d. glazek and k.g. wilson , phys . rev . d 48 ( 1993 ) , 5863-5872 . hep-th/9706149 in 2d , the situation is well understood even rigorously : for all theories where wightman functions can be constructed rigorously , there is an associated hilbert space on which corresponding ( smeared ) wightman fields and generators of the poincare group are densely defined . this implies that there is a well-defined hamiltonian h=cp_0 that provides via the schroedinger equation the dynamics of wave functions in time . in particular , if the wightman functions are constructed via the osterwalder-schrader reconstruction theorem , both the hilbert space and the hamiltonian are available in terms of the probability measure on the function space of integrable functions of the corresponding euclidean fields . for details , see , e.g. , section 6.1 of j . glimm and a jaffe , quantum physics : a functional integral point of view , springer , berlin 1987 . in particular , ( 6.1.6 ) , ( 6.1.11 ) and theorem 6.1.3 are relevant . [ information extracted from the section ''relativistic qft at finite times ? '' of chapter b3: ''basics on quantum fields'' of my theoretical physics faq at http://www.mat.univie.ac.at/~neum/physfaq/physics-faq.html ] [ edit october 9 , 2012: ] on the other hand , a lot is unknown about qfts ( including qed ) at finite time . let me quote from the 1999 article ' 'some problems in statistical mechanics that i would like to see solved'' by elliot lieb http://www.sciencedirect.com/science/article/pii/s0378437198005172 : but there is one huge problem that everyone avoids , because so far it is much too difficult to handle . that problem is quantum electrodynamics , and the problem exists whether we are talking about non-relativistic or relativistic quantum mechanics . [ . . . ] the physical picture that begs to be understood on some decent level is that the electron is surrounded by a huge cloud of photons with an enormous energy . we are looking for small effects , called ' radiative corrections ' , and these effects are like a flea on an elephant . perturbation theory treats the elephant as a perturbation of the flea . [ . . . ] after renormalizing the mass so that the ' effective mass ' ( a concept familiar from solid state physics ) equals the measured mass of the electron we are supposed to obtain an ' effective low energy hamiltonian ' ( again , a familiar concept ) that equals the schroedinger hamiltonian plus some tiny corrections , such as the lamb shift . from there we should go on to verify the levels of hydrogen ( which , except for the ground state , have become resonances ) , stability of matter and thermodynamics and all those other good things . but no one has a clue how to implement this program . [ . . . ] on the other hand matter does exist and the sun is shining , so the theory must exist , too . i would like to see it someday
i did some research and calculations : to summarize : the relativistic rocket will not break apart , uniform acceleration along it is possible . but the observers will measure different accelerations due to the gravitational time dilation . in more detail : let 's assume the observer at the bottom measures $\alpha$ acceleration . so for an inertial observer outside ( who draws the minkowski chart ) this accelerating observer 's motion will be hyperbolic . the semi mayor axis of this hyperbola will be $c^2/\alpha$ . let 's say the length of the relativistic rocket is $h$ this is measured before the launch , and the mechanical stresses during the travel try to keep this value constant in the reference frame of the rocket , otherwise the rocket would break apart ( we assume it do not break apart ) . as the rocket accelerates the plane of simultaneity rotates from the viewpoint of the inertial observer . so the two ends of the rocket will not trace two identical hyperbolas . but the two ends always connect two points on the two hyperbolas whose slope is the same ( you can see this on the rindler chart ) . so all parts of the rocket travel with the same speed at the local frame simultaneously , so the rocket 's acceleration will be uniform and it will not break apart . but the two hyperbolas are different . the bottom traces a hyperbola whose semi-mayor axis is $c^2/\alpha$ , the top traces a hyperbola whose semi-mayor axis is $c^2/\alpha + h$ . the acceleration that corresponds to the second hyperbola is $1 + \alpha h / c^2$ times smaller than $\alpha$ . this a bit paradoxical situation , because i stated that the acceleration is uniform along the rocket , now i state it is different due to the different hyperbolas . this paradox can be resolved if i introduce gravitational time dilation , so i assume that the clock of the observer at the top ages faster with the rate i mentioned above . so the top observer measures less acceleration this way . there is an event horizon at the rindler-horizon where $h = -c^2/\alpha$ , there the time stands still . this is somewhat analogous with the black hole 's event ho+rizon . the gravitational time dilation formula mentioned in the wikipedia and the one mentioned in the comment is the same , but that exponentional formula never reaches zero , that would mean the rindler-horizon does not exist . . . which would be a bit odd . so i still need some research . update : the wikipedia article has been fixed since last update . so the general formula to the gravitational time dilation is $e^{\int^h_0 g ( h ) dh / c^2}$ , where $g ( h ) $ is the measured gravitational acceleration at the given level . for rindler observers $g ( h ) = c^2/ ( h+h ) $ where $h = c^2/\alpha$ . doing the integral gives $e^{ln ( h+h ) - ln ( h ) } = ( h+h ) /h$ . substituting $h$ back i will get the original $1 + \alpha h / c^2$ i mentioned earlier .
" elastic " and " inelastic " mean the same things they do in an introductory mechanics class . in elastic collisions the total mechanical energy is the same after the interaction as before ( in a dark matter interaction this means effective the total kinetic energy ) , while in inelastic interaction that equality can not be counted on . either some mechanical energy goes into another channel in the final state or some energy that was in another form in the initial state appears in the final state . typical possibilities for inelastic interaction would be particle creation , excitation of the struck nucleus , or break up of the target . " isospin violating " means that the ( approximate ) isospin quantum number of the final state is different from that in the initial state . isospin is the quantum number that distinguishes protons from neutrons ( and can be applied to other hadrons as well ) . a collision that transforms a proton into a neutron without changing the isospin quantum number ( if any : it probably does not have one ) of the dark matter component would be isospin violating ( and inelastic because some kinetic energy from the initial state went into raising the mass of the final state ) .
the state $|\phi\rangle$ is a coherent state , which has a non-zero overlap with the vacuum state $|0\rangle$ . the vacuum state is defined by $\hat \phi ( x ) |0\rangle=0$ for all $x$ . the vacuum state is also given by the coherent state $|\phi ( x ) =0\rangle$ for all $x$ . furthermore , one should keep in mind that the coherent state basis is overcomplete , and therefore $\langle \phi|\phi'\rangle\neq0$ for any $\phi$ and $\phi'$ . all this properties solve the op 's problem . to see that , one can look at the simpler case of the harmonic oscillator , and everything should be clear .
here 's an example . let 's consider the case of a free particle in a force field given by a potential $u$ . if $$l=t-u , $$ then the newton 's equations of motion $$\dot {\mathbf p}=-\dfrac{\partial u}{\partial \mathbf r}$$ are equivalent to lagrange 's equation in cartesian coordinates : $$\dot {\mathbf p} = \frac{\partial l}{{\partial \mathbf q}} , $$ with $\mathbf q = \mathbf r$ . the principle of least action states that motion $\gamma$ beetween two points of the generalized configurations space , $ ( \mathbf r _1 , t_1 ) $ and $ ( \mathbf r _2 , t_2 ) $ , is the one that makes the action $s ( \gamma ) =\int _{t_1}^{t_2} l ( \gamma ( t ) , \dot\gamma ( t ) ) \text d t$ stationary , meaning that the linear part ( the so called differential ) of the variation $$\delta s ( \gamma , h ) =s ( \gamma+h ) -s ( \gamma ) $$ is zero in $\gamma $ . this two things are linked by the fact that ( for a sufficiently nice $l$ ) this condition is equivalent to the lagrange 's equation above mentioned . an interesting feature of this principle is that it does not depend on the system of coordinates chosen , which gives a lot of freedom in choosing the more appropriate set of coordinates for the problems . to give an application , suppose that $\mathbf r= ( x , y ) $ are the cartesian coordinates of a point in the plane and let $\pi= ( r , \theta ) :\mathbb r ^2 \to \mathbb r ^2$ be the polar coordinates . suppose that $$\mathbf r ( t ) = ( x ( t ) , y ( t ) ) $$ is a solution of the equations of motion . then we can show easily that $$\mathbf q ( t ) =\pi ( \mathbf r ( t ) ) , $$ which is the projection onto the polar plane of this solution , solves the equations of motion $$\dfrac{\text d }{\text d t} \dfrac{\partial \tilde l}{\partial \dot {\mathbf q}}=\dfrac{ \partial \tilde l}{\partial \mathbf q} , $$ that is , lagrange 's equation are still valid in the new system of coordinates , with a new lagrangian given by $$\tilde l ( \mathbf q , \dot {\mathbf q} ) =l ( \mathbf r , \dot {\mathbf r} ) . $$ in fact by definition of $\tilde l$ , we have $$\int _{t_1} ^{t_2} \tilde l ( \mathbf q ( t ) , \dot { \mathbf q} ( t ) ) \text d t=\int _{t_1} ^{t_2} l ( \mathbf r ( t ) , \dot { \mathbf r} ( t ) ) \text d t . $$ since $\mathbf r$ satisfies lagrange 's equations , it minimizes the action associated to $l$ . so $\mathbf q$ minimizes the action associated to $\tilde l . $ so $\mathbf q$ satisfies lagrange 's equations with the new lagrangian .
the weizsäcker formula and all similar formulae in nuclear physics are formulae for the masses of the nuclei , not atoms . that is true by design : the models behind the individual terms ( droplet , shells etc . ) are models for the nucleus only . at the same moment , the accuracy of similar semiempirical formulae is not that marvelous which means that the errors are comparable to the mass of the electrons near $0.5\ , {\rm mev}$ . so if you interpret the weizsäcker formula as a formula for the whole atomic mass , you will increase the errors just a little bit . now , when we care about the full accuracy , there are several amu units . see e.g. http://www.wolframalpha.com/input/?i=mass+of+proton+in+u http://en.wikipedia.org/wiki/atomic_mass_unit where the second page is on the history . the chemical amu i.e. 1.00732 amu per proton is deprecated , and similarly is the related " dalton " , and your " physical " amu , $m_p=1.0076$ based on $1/12$ of the carbon-12 atom ( including electrons in this sentence , but not in other sentences ) , is very much alive . the figure 1.007825 amu is the mass of the hydrogen atom ( proton plus electron ; the electronic excitations are negligible ) . note that $1.007825\sim 1.007276 ( 1+1/1836 ) $ . so this figure is useful exactly to evaluate the mass of the whole atom – an atom is composed of neutrons and the " proton plus electron " combinations because the number of protons agrees with the number of electrons . with a great accuracy , the mass of the atom may be calculated simply by adding the rest masses of the electrons i.e. by replacing $m_p$ by $m_p+m_e$ in the nuclear formulae . that is why the formulae with $1.007825$ are omnipresent . the error introduced by this approximation is comparable just to the atomic binding energies which are multiples of 1 electronvolt , about 1 million times smaller than the other terms we care about ( and 1 billion times lower than the proton rest mass ) , and are neglected pretty much in any discussion about nuclear physics .
the answer is the there is some reduction in mass whenever energy is released , whether in nuclear fission or burning of coal or whatever . however , the amount of mass lost is very small , even compared to the masses of the constituent particles . a good overview is given in the wikipedia article on mass excess . basically , the mass of a nucleus will in general be a little bit off from the sum of the masses of the protons and neutrons inside it . this is because there is a binding energy holding the nucleus together , and your standard $e = mc^2$ gives the equivalent mass for this energy . in the fission of uranium-235 , $$ {}^{235}_{\phantom{0}92}\mathrm{u} + {}^1_0\mathrm{n} \to {}^{236}_{\phantom{0}92}\mathrm{u} \to {}^{141}_{\phantom{0}56}\mathrm{ba} + {}^{92}_{36}\mathrm{kr} + 3\ {}^1_0\mathrm{n} , $$ the total rest mass of the products is slightly less than of the reactants . this is true even though there are the same number of protons ( 92 ) and neutrons ( 144 ) before and after . so it is not as though an entire nucleus disappears , or even an entire proton or neutron . the lost mass comes from the binding energy . the take-away message is that we are not destroying particles to create energy . even nuclear fusion conserves the total number of protons and neutrons . instead , you should think about the mass-energy equivalence the other way around . the fact that there is potential energy capable of being released in nuclear fission implies that the reactants must be heavier than the products . in the same fashion , a typical battery weighs less after being discharged ( though by an immeasurably small amount ) , even though the nuclei are unchanged and the number of electrons is the same . that is , potential energy in any form adds to the mass of the system as a whole , and is not attributable to any one component .
the rope on the left pulls the weight and the pulley towards each other . the rope on the right pulls the floor and the pulley towards each other . each of these pull downwards on the pulley with force $t$ . the total downward force on the pulley is $2t$ . the pulley does not move because the ceiling pulls upward on it hard enough to keep it motionless . the pulley can only be motionless if the total force on the pulley is $0$ . so the upward force from the ceiling is $2t$ . perhaps the tension in the rope on the right is counter intuitive . the weight is motionless , so the total force on it is $0$ . the two forces are gravity and the tension of the rope on the left . both of these have magnitude $t$ . if the rope on the left is pulling upward on the weight , something must pull downward with force $t$ on the rope on the right . that is the floor , which pulls just hard enough to keep the rope from moving . another way you could arrange for a force $t$ to pull downward on the rope on the left is to hang another weight on the rope on the right . this would create the same tension in the rope on the right as the floor does .
a pendulum of ( long ) length l will tick with a period of $2\pi\sqrt{l\over g}$ , and air resistance can be made negligible for a mm-sized oscillation of a heavy object on a several-meter long rigid arm . you need to determine the location of the center of mass accurately to know the effective value of $l$ , but this can be done arbitrarily accurately by balancing the arm and weight on a fulcrum ( or by accurately finding the cm of each of the parts and measuring the configuration of the parts accurately ) . then you just have to count the number of oscillations over a long enough period of time . this is a practical method that allows the determination of g to 5 significant figures ( assuming the error on l is the signifcant one , the lever is 10m , and the position measurements are at the . 1mm scale ) with no technology .
the next thing you will need is time period of a simple pendulum $$t=2\pi\sqrt{\dfrac{l}{g}}$$
have derived it myself : $${\tau}=\dfrac{\underbrace {\large { i}}_{\text{through inductor at steady state}}}{\underbrace {{di_{\small }}/{dt}}_{\text{initial}}}$$ also $$v_{\text{inductor}}=l\dfrac{di}{dt}$$ where $i$ is current through inductor .
in terms of measurements of the field , we can regard the vacuum state as the state that has the least correlations between measurements of the field in different places . at an elementary level , we can call the vacuum state the zero-quantum field state . the statistics of field measurements in the vacuum state are also highly symmetric , being isotropic , homogeneous , and invariant under lorentz boosts . as we add more quanta to the quantum field state , the correlations between field measurements in different places become more complex and less symmetrical . there are many significant differences between a classical field and a quantum field , but the most basic is that instead of talking just about the field at a particular place , we talk about the probability that the field will take one value or another , at many different places all at once . as a result , instead of describing modulations of a classical field --how a particular configuration of the field is different from the zero field-- , we have to describe modulations of all the correlations of the field --how a particular configuration of the quantum field is different from the vacuum state . a classical field is a function of just one point in space time , something like $\phi ( x ) $ , but a quantum field can best be understood ( imo ) in terms of a function of many points , $w ( x_1 , x_2 , . . . , x_n ) $ , that describes the correlations between the field at $n$ different places . for the vacuum state , these functions are called the vacuum expectation values ( vevs ) , but we can equally well construct a similar function in any state . a quantum field operator $\hat\phi ( x ) $ describes how to construct correlation functions for all different $n$ , in any state that we can construct in the hilbert space of quantum field states . the exact changes that are made to the vevs by the action of a single quantum field operator are in a mathematical sense elementary operations on the discrete structure that ultimately derives from the fact that we do not talk about 2½-point correlation functions , for example , we only talk about 2-point , 3-point , . . . , $n$-point correlation functions . a single quantum of the field is often taken to be associated with a given frequency , but it can in general be associated with an arbitrarily complicated modulation of the correlation functions , at many different frequencies . the significance of this is that a quantum field operator is not just " add one quantum " ( though it is that ) , it also says where in space-time to add the quantum , $\hat\phi ( x ) $ . a single quantum , however , may be described as a superposition of different frequencies , or it may equally well be described as a superposition of different positions . in particular , $\hat\phi ( x_1 ) +\hat\phi ( x_2 ) $ is in many ways as good as a single quantum field operator as is $\hat\phi ( x ) $ . the mathematics of creation and annihilation operators is given in answers to the question you linked to , in very bare form , so if that is not what you want then you must want something different . to put the above into two bullet points , $\bullet$ quantum field operators modulate the correlations that are present in the vacuum state , and $\bullet$ correlations are an intrinsically discrete concept because they are constructed as correlations between 2 , 3 , or any integer number of measurements . there is quite a lot of detail missing from the above , of course ; i would particularly single out the issue of how to understand the consequences of measurement incompatibility at time-like separation . i have attempted not to speak too much beyond your question . if you remember your qm well enough , btw , the quantized simple harmonic oscillator ( sho ) can be regarded as the mathematical foundation of qft ; the quantum field can be constructed as an infinite number of interacting shos ( albeit non-rigorously ) . but you can get that from many of the textbooks .
the quantity that determines day-to-day gravity we feel is the difference is clock-rate at different places , but yes , it is all in the time coordinate , in the sense that the space we see is flat , and only the time coordinate is mismatched from point to point . gravity describes spaces that curve too , distances that change from place to place , but this effect does not matter since it is very small . the reason we see time-component of metric and nothing else is simply because we are so much longer in time than in any other direction--- when you sit still for a few seconds , you extend over a meter or so , but your time extent is one light-second , or 300,000 meters . so on our scales , thing are very very long in time , and short in space , and we can see a slight curvature between different places in the time coordinate , because we go by so much time coordinate .
no , this is not a golden-ratio spiral . its closest relative is the archimedean spiral , whose fundamental equation is $$r=a+b\ , \theta . $$ this is the spiral traced out by the water thrown out by a horizontal sprinkler as it rotates : because its horizontal velocity is constant , the radius $r ( t ) $ of a given drop at time $t$ increases linearly with $t$ , whereas the angle it propagates on is the direction of the sprinkler when it was fired , which also increases linearly with $t$ ; hence , there is a linear relation between $r$ and $\theta$ . image credit : anton croos . i can not find a picture taken from above the sprinkler - apparently people are more careful with their cameras than you had think . in the case of your image , there is the additional action of gravity to deflect the raindrops , so the spiral will not be perfect , but the principle is the same . it is important to note that fibonacci and golden spirals operate on a different principle and they are very hard to sustain over multiple turns , as the radius grows exponentially . this is easy to do with , say , a mollusk that eats more as it grows , but it is hard to accomplish with purely kinematical phenomena . kinematical phenomena do , on the other hand , more or less routinely produce archimedean , or archimedean-like spirals . my favourite is this one , which is produced by shock waves propagating at constant speed through a planetary nebula , and produced by the gas emitted by one of the stars in a closely-orbiting binary pair :
a fresnell lens is essentially a " collapsed " plano-convex lens . if you were to raise each ring of the fresnell lens so the outer diameter of the ring were at the height of the inner diameter of the next larger ring , you had have reconstructed the original plano-convex lens . obviously , there are some diffraction effects and various other higher-order aberrations , but at the simple level i think you are asking , the fresnell lens produces a full image of the object . that is another way of saying yes , it would produce an image of the sun even if the sun were not on the optic axis .
a rule of thumb would be to get rid of unwanted variables . for example , since we are only interested in $\frac{m_1}{m_2}$ and $\theta$ , we can get rid of $f_t$ . $$f_t \sin \theta = m_1 g$$ $$f_t \cos \theta = m_1 a$$ rearrange to get $$\frac{m_1 g}{\sin \theta}=\frac{ m_1 a}{\cos \theta} \hspace{20mm}\text{ . . . eq 5}$$ i do not know whether getting this ' equation 5' is part of the solution , but my point is get rid of unwanted variables .
yes . . . your supervisors formula is incorrect . smart people make mistakes -- it happens . if your supervisor is not open to criticism , then i suggest that you give him or her a few simple examples that illustrate the problem , and then immediately present a solution . your example of a mirror coated over a non-mirror is a great place to start . in order to solve your multiple reflection problem , you can use the transfer-matrix method . unfortunately you will need to invert the result , because you want to calculate the properties of one layer given the properties of several , which is the opposite of what the method is usually set up to do . whether or not you can do this depends on if you have enough information to separate the effects of the different layers . also , note that the reflectance of one layer is actually undefined , because reflection is a property of an interface between two materials , not a property of a single material . water in water does not reflect , and air in air does not reflect , but there is reflection at air-water interfaces . your coatings will reflect different amounts if they are in contact with one another as compared to if they are in contact with air .
assuming that by non-uniform circular motion you mean moving in a circle but at changing speed , then this does not conserve angular momentum so it cannot happen in a central field . there must be so component of the force tangential to the radius , and therefor some component of the acceleration that is not central . this happens for satellites orbiting the earth , and indeed the moon ( as the grail satellites have found ) because the earth , moon and presumably the vast majority of other bodies are not spherically symmetric so field felt by an object orbiting close to them is not central .
answer expected by following author 's hints . \begin{align} \mathbf{l}_{eg} and = \dfrac{1}{4\pi}\int \mathbf{r'} \times \left [ \mathbf{e} \times \mathbf{b} \right ] d^3r'\\ and = \dfrac{1}{4\pi} \int \left [ \left ( \mathbf{b . r'} \right ) \mathbf{e} - \left ( \mathbf{e . r'} \right ) \mathbf{b} \right ] d^3r'\\ and = \dfrac{1}{4\pi} \int \left [ \left ( \dfrac{g}{r'^3}\mathbf{r ' . r'} \right ) \mathbf{e} - \left ( \mathbf{e . r'} \right ) \dfrac{g}{r'^3} \mathbf{r'} \right ] d^3r'\\ and = \dfrac{g}{4\pi} \int \dfrac{1}{r'} \left [ \mathbf{e} - \left ( \mathbf{e . \hat{r}'} \right ) \mathbf{\hat{r}'} \right ] d^3r ' \\ and = \dfrac{g}{4\pi} \int \left [ \mathbf{e . \nabla'}\right ] \mathbf{\hat{r}'} d^3r ' . \end{align} or , let $\mathbf{u}$ and $\mathbf{v}$ be arbitrary vectors : \begin{equation} \left [ \mathbf{u . \nabla}\right ] \mathbf{v} = \left [ \mathbf{u . \nabla}v^i\right ] \mathbf{e}_i , \end{equation} where $ ( \mathbf{e}_i ) _{1\leq i \leq 3}$ denotes the cartesian basis . by integrating by parts we have : \begin{align} \mathbf{l}_{eg} and = \dfrac{g}{4\pi} \int \left [ \mathbf{e . \nabla'}\right ] \mathbf{\hat{r}'} d^3r'\\ and = \dfrac{g}{4\pi} \int \mathbf{e . \nabla'} ( \hat{r}'^i ) d^3r ' \mathbf{e}_i \\ and = \dfrac{g}{4\pi} \int \left [ \mathbf{\nabla ' . } \left ( \mathbf{e}\hat{r}'^i \right ) \mathbf{e}_i - \left ( \mathbf{\nabla ' . e} \right ) \mathbf{\hat{r}}'\right ] d^3r'\\ and = \dfrac{g}{4\pi}\ \left [ \oint \mathbf{\hat{r}'} \left ( \mathbf{e . da}\right ) - \int \left ( \mathbf{\nabla ' . e}\right ) \mathbf{\hat{r}'} d^3r ' \right ] . \end{align} but the field $\mathbf{e}$ vanishes at infinty so it comes : \begin{equation} \mathbf{l}_{eg} = -\dfrac{g}{4\pi} \int \left ( \mathbf{\nabla ' . e}\right ) \mathbf{\hat{r}'} d^3r ' . \end{equation} and finally , using the maxwell equation :$\mathbf{\nabla ' . e} = 4\pi e \delta^{ ( 3 ) } ( \mathbf{r} - \mathbf{r'} ) $ , we get the result : \begin{equation} \mathbf{l}_{eg} = -eg\mathbf{\hat{r}} . \end{equation}
the mirror gets proportionally smaller . the explanation is the similarity of triangles . the eye and the marks on the mirror form a triangle , while the eye and the two points on the image form another triangle . the two triangles are similar , with ratio 1/2 , no matter the distance .
the important factor is not the absolute speed of the atoms but the speed relative to each other . i would guess the article on the rubidium atoms is related to making a bose-einstein condensate , and slow relative speed is needed otherwise your cloud of atoms just disperses instead of making a condensate . increasing the speed of an isolated atom makes absolutely no difference to it , but if you increase the relative speed of atoms in some assemblage the atoms will collide with each other at that speed . as you increase the relative speed from the low levels of the rubidium atoms the collision energy will grow to the point where it ionises the atoms , so the atoms form a plasma . if you carry on increasing the speed up to near light speed the collisions between the nuclei will be so violent that they completely destroy the atomic nuclei and break them into showers of hadrons . we can be confident about what happens in near light speed collisions , because this is exactly what the rhic does . the lhc has also done heavy ion collisions in between finding the higgs boson . it is worth a note that tokamak fusion reactors work by raising deuterium and tritium ions to very high relative speeds so that the collisions cause the nuclei to fuse . the corresponding temperature is about 100 million degrees , though from a quick google i could not find the velocities of the ions . you do not get fusion in the rhic/lhc experiments because the energies are so high that the nuclei are completely blown apart .
as you approach the event horizon , the doppler shift of any electromagnetic signals you send back out approaches infinity . any distant observer will get old and die while monitoring your signals that get slooower and sloooooower . your own experience is that you exist for some finite time ( not very long , for a black hole of a typical size ) , and then you hit the singularity and die . ( you may die earlier because of tidal forces . that depends on the size of the black hole . ) to you , the experience is that the doppler shifts of signals coming in from the outside become greater and greater . however , you do not get to see the arbitrarily distant future of the outside universe . for any location outside the event horizon , there is some latest time at which their signals can reach you before you go splat into the event horizon . i have never liked descriptions of this sort of thing that use phrases like " it seems to you like " or " you see " or " in your frame of reference . " general relativity does not have global frames of reference . it is meaningless to talk about whether something far away is happening " now " as judged " according to you . " all you can do is receive or transmit signals . you know they are just signals . btw , there is a classic science fiction novel , gateway , by frederik pohl , in which the protagonist makes himself really , really miserable by imagining that his lover , whom he abandoned to fall into a black hole , is forever cursing him " now . " a better understanding of general relativity would have been the best therapy for the poor guy -- but i guess that would have spoiled the book .
work done by the electric force is positive and not negative . the change in the potential energy of the electric field $\delta u$ is equal to the negative of the work done $w$ by the electric force . you have $$\delta u = -w$$ $$u_1=u_0-w&lt ; u_0$$ $$\therefore w&gt ; 0$$ and as $$w=\int f\cdot dl&gt ; 0$$ , we can say that $f$ acts in the same direction as $dl$ , i.e. it is attractive .
$\partial_i \frac{f}{1-f^2}=- ( \frac{f}{1-f^2} ) \partial_i ( \frac{1-f^2}{f} ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) \partial_i ( \frac{1}{f}-f ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) ( \partial_i ( \frac{1}{f} ) -\partial_if ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) ( - \frac{1}{f}\partial_i f \frac{1}{f} ) -\partial_if ) ( \frac{f}{1-f^2} ) $ $= ( \frac{1}{1-f^2} ) \partial_i f ( \frac{1}{1-f^2} ) + ( \frac{f}{1-f^2} ) \partial_i f ( \frac{f}{1-f^2} ) $
as you have mentioned , topological insulators ( ti ) are " topological " because they can not be smoothly connected to trivial band insulators without closing the band gap ( and without breaking certain symmetry ) . simply generalize this to the many-body case , we may say that the topologically ordered states are called " topological " because they can not be smoothly connected to the trivial product state without closing the many-body gap . to gain a better understanding , one should realize that " topology " is a complement to " geometry " . by geometry , we mean that there is a sense of measurement of the distance and angle etc . , and the shape and the size of the object matters . while by topology , we mean that one can continuously deform the object , and the shape or the size does not matter . so the topological properties are those properties that can endure continuous deformation of the state ( by continuity we mean without encountering a quantum phase transition ) . to protect the topological properties against deformation , a gap between the ground states and the excited states is always required . so topological property is only defined for gapped quantum matters ( both ti and topological order are within this scope ) . on the other hand , gapless quantum matters do not have topological properties , and their properties are geometrical . the topological/geometrical distinction is also reflected from the mathematical tools we used to study the physics . for gapped quantum matters , we use topological tools like homotopy , cohomology , k-theory , category theory etc . for gapless quantum matters , we use geometrical tools like gravity theory ( ads/cft ) .
a hydrogen atom can be made to show chaotic behaviour if it is excited to very near it is ionisation energy . the maths is somewhat beyond me , but this paper discusses calculations of the hydrogen atom showing the onset of chaotic behaviour . you can find more by googling for " rydberg atom " combined with " chaos " or " chaotic behaviour " .
this answer is only an order of magnitude estimation . the main uncertainty comes from the fact that for a constant current the cooling power is not determined by the superconducting wire but from the quality of the isolation against radiation , convection and conduction of warmer parts of the structure to the cold wire at 4.2k we can use the lhc ring as an estimate of how much helium might be necessary . the cern uses approximately 120 tonnes of liquid helium to cool a ring with a circumference of 27km . to keep the temperature constant 8 compressor stations , each with a power of 18kw are used . comparing this with the diameter of the earth we arrive at 177 777 tonnes of liquid helium . precises estimates of the world wide helium reserves are not easily available , so one might used the data for the us as a first guess . the currently available reserves are estimated to be 147 billion cubic meters ( wikipedia ) . as the main source for helium is natural gas other areas of the world also have large reserves , and this is only a lower bound . now this amount is converted : $$\mathrm{mass_{he}} = 147\cdot 10^9\ , \mathrm{m}^3 \cdot 0.18\ , \mathrm{ kg/m^3} = 2.6\cdot 10^{10}\ , \mathrm{kg}$$ which is much more than the $1.7\cdot 10^{8}\ , \mathrm{kg}$ necessary for a ring with similar specifications as the lhc . you can greatly reduce the necessary amount by using pulse tube refrigerators and relax the specifications a bit by allowing slightly higher temperatures . then the conduction via the metal itself might be enough to cool the wire as nbsn wires are embedded in copper .
grassman $d\theta$ has opposite mass dimension to $\theta$ , which is why the notation is not 100% optimal , it confuses on this issue . but if you know how to evaluate the integral , that it goes like the derivative , then you know how change of scale works , and it is the opposite of normal change of scale : $$\int d ( k\theta ) f ( k\theta ) = {1\over k} \int d\theta f ( \theta ) $$ and this is why the volume determinant for the integration ends up being the reciprocal of the bose case .
the final condition for a realistic theory in physics is that its predictions ( of scattering amplitudes or correlators etc . ) have to agree with observations . this implies that the predictions have to be consistent and obey some general consistency conditions ( unitarity , non-negativity of probabilities , some symmetries , locality or approximate locality , and so on ) . for theoretical theories , it is just the general consistencies that hold . now , the task is to classify all possible theories and learn how to calculate with them . it turns out that the euclidean spacetime or world sheet is simply a simpler , more straightforward , more free-of-subtleties approach to produce a machine that calculates some scattering amplitudes or other observables . at least formally , the euclidean theories may be continued to analytic ones and vice versa . for nontrivial spacetime topologies , the euclidean objects are likely to be more manageable . for example , the world sheets in string theory ( think about a torus or pants diagrams etc . ) are much more well-behaved in the euclidean signature so we may consider this approach " primary " . covariant calculations in string theory are almost always done with the euclidean ones . the result may be continued to the minkowski momenta etc . and some of the consistency conditions above are still guaranteed to hold because of some properties of the complex calculus . in the light-cone gauge , we may work directly with the minkowski-signature world sheets . but we pay the price that the interaction points where strings split or join are singular and the direction of the " future time " is ambiguous . we must also include contact terms , higher-order interaction terms , to deal with some divergences caused by the singular world sheets , but when these things are summed over , we may prove that the resulting amplitudes agree with the covariantly computed ones ( in the euclidean signature ) . gravity in $d\geq 4$ ( and maybe 3 ) suffers from the " negative norm conformal factor " . the euclideanized einstein-hilbert action $\int r \sqrt{g}$ is no longer positively definite . in particular , if you consider scalar waves that scale the metric by an overall number , $g_{\mu\nu}=e^f\eta_{\mu\nu}$ , and derive the kinetic term for $f$ , it will have the opposite sign than the kinetic term for other components of the metric tensor ( the physical polarizations of the gravitational waves , like $g_{xy}$ ) . it follows that the action will be bounded neither from below nor from above , and $\exp ( -s_e ) $ in the euclidean path integral will diverge in some region of the configuration space . in this sense , people believe that the minkowskian path integral must be the " more kosher one " for higher-dimensional gravity . but this is a bit empty statement because at the quantum level , higher-dimensional gravity obtained as a direct quantization of einstein 's equations is inconsistent , anyway . and string theory which is consistent and contains gravity does not give us any tool to directly rewrite the path integral in terms of spacetime fields including the metric ; it is not a field theory in the ordinary sense . so the preference for the " minkowski signature " is a bit vacuous . after all , the minkowskian action is not bounded from either side , either . this is considered " not to be a problem " because the integrand is $\exp ( is ) $ which still has the absolute value equal to one , so it does not diverge . but i would personally say that the unboundedness of the euclidean action is the " same " problem for the minkowskian path integral . quite generally , the wick rotation is extremely important in quantum field theory and it is actually even more important in quantum gravity or places with many spacetime ( or world sheet ) topologies , i.e. in situations where one has many different " time variables " in which we might try to expand things . one should not be afraid but at the end , whatever theory he deals with , he gets some amplitudes whose self-consistency ( and/or consistency with observations ) must be verified . with some " good rules of behavior " while wick-rotating , one may be " pretty sure " that some tests will be passed .
you ask if the region where the field is defined has holes in it , what happens ? well , in that case you can define the vector potential on simply-connected sub-regions $r_i$ whose intersection is the whole non-simply connected region $r$ and such that they differ by only by a gauge transformation on the regions of overlap . this is a physically well-motivated thing to do , because it means that up to gauge transformation , the vector potential can be defined on $r$ . here 's a simple example . let $\ell=\{ ( x , y , z ) \ , |\ , x=0 , y=0\}$ denote the $z$-axis , then the region $r=\mathbb r^2\setminus\ell$ is not simply connected . to see this , simply consider a closed loop enclosing the axis ; there is no way to continuously shrink it down to a point while staying in $r$ . because of this , the there is no $\mathbf a$ defined on all of $r$ . however , let $\ell_+$ denote the positive $z$-axis , and let $\ell_-$ denote the negative $z$-axis , then the regions $r_- = \mathbb r^3\setminus \ell_+$ and $r_+ = \mathbb r^3\setminus \ell_-$ have the property that they are each simply connected and $r = r_+\cap r_-$ . moreover , we can define a vector potential $\mathbf a_+$ on $r_+$ and $\mathbf a_-$ on $r_-$ such that there exists a scalar function $\lambda$ for which \begin{align} \mathbf a_+ ( \mathbf x ) - \mathbf a_- ( \mathbf x ) = \nabla\lambda ( \mathbf x ) , \qquad \text{for all $\mathbf x\in r$} \end{align} in fact , here are the explicit expressions in spherical coordinates $ ( r , \theta , \phi ) $: \begin{align} \mathbf a_{\pm} and = -g\frac{\cos\theta\mp 1}{r\sin\theta}\hat{\boldsymbol \phi} \end{align} i will leave it to you to determine $\lambda$ ; it is a fun exercise . what are the physical consequences of that ? well , in the context of quantum mechanics , these sorts of topological issues are physically relevant ( i am unsure if there are examples in which they are relevant at the classical level , but i do not think so ) . i will not go into the details here ( unless perhaps there is some demand ) , but the very vector potentials i wrote down in the example above come up when discussing magnetic monopoles and the quantization of electric charge ( see dirac quantization ) . these topological issues also become significant in discussing the famous aharonov-bohm effect .
yes , and the formula you already have still works . take z to be the optical path length : refractive index n times physical distance . a gaussian beam in glass diverges in exactly the same way as in free space , only ' squeezed ' in the z direction by a factor of n .
with no resistance , the full voltage is applied to the fan , and you get mechanical work done , at whatever efficiency the fan itself is capable of . never minding the fan itself , so far as the electrical aspect goes , you could say it is 100% efficient . with resistance in the system , for example about equal to the resistance of the fan , you have less current flowing through the system . half as much . same voltage . so the system is using half the power . the fan , now one part of a voltage divider circuit , is getting half the voltage , and getting half the current . it runs slower , doing less , doing about 1/4 the work . 1/4 of the fan action divided by 1/2 power going into the system means that the system is 50% efficient . imagine a lot of resistance in the system , megohms . you will have only a trickle of current , say a microamp . the fan barely moves . the system will be using very little power , but most of that little power is just making the resistor hot . or rather , a small fraction of a degree warmer than ambient temperature . the system efficiency is close to zero .
like any other form of matter gas molecules feel the gravitational pull of the planet they surround , so they are attracted to the planet by gravity . at any temperature above absolute zero the atoms/molecules in a gas have a velocity distribution known as the maxwell-boltzmann distribution . as long as their velocities remain well below the escape velocity of the planet the atmosphere will be bound to it . although this gives the basic idea it is an oversimplification for several reasons . for example the escape velocity decreases as you move up through the atmosphere , however the temperature changes as well so the average gas atom/molecule velocity also changes with height . also even if the average is well below the escape velocity a small fraction of molecules will have high enough velocities to escape . however even then only molecules near the top of the atmosphere are likely to escape as the mean free path near the ground is too short for even an energetic molecule to escape . finally radiation from the sun is an important factor in removing gas molecules from planets . on earth the magnetic field keeps most of the radiation out , but on mars gas loss due to solar radiation is important . for more info you might want to have a look at the wikipedia article on atmosphere loss . i am not sure what your second question is asking as it does not seem relevant to atmosphere loss . if you are asking why planets generally have a non-zero tilt this should be posted as a separate question . in brief , for most planets the tilt is chaotic and varies contnuously , and the planet may even flip over completely . earth 's tilt is stabilised by the moon and varies only slightly with time . re the revised question : at any given temperature the average velocity of lighter gas molecules is greater than heavier gas molecules , so it is the ligher gas molecules that escape most easily . for example the earth loses a few kg of hydrogen per second but almost no oxygen or nitrogen . however if the rate of loss of a light gas is very fast it can carry molecules of heavier gases along as well by colliding with them and transferring momentum to the heavier molecules . this is known as hydrodynamic escape . note that this only happens when there is a rapid loss rate of the lighter gas , so it is not happening to any significant extent on any of the planets in the solar system .
i suspect luboš 's answer may be a bit complex for you so i will attempt a simpler explanation ( if i am wrong just ignore this answer ) . when you say " i think its because the velocity to outward " you are getting close . to show what is going on i have zoomed in on the diagram you posted in your question . let 's start at the moment where the velocity of the object is $u$ . the thing you need to remember is that velocity is a vector so it has both magnitude and direction . acceleration may be a change in the magnitude or a change in direction or both . in the case of circular motion it is the direction of the velocity that is changing not it is magnitude . to see this look at the vector $v$ , which is the velocity of the particle after some short time . so the velocity has changed from $u$ to $v$ . to get the size of the change we need to find what vector has to be added to $u$ to give $v$ . at the top of the diagram i have put the two vectors $u$ and $v$ with their starting point together , so you can see that the vector needed to change $u$ to $v$ is the small vector $a$ , and you can see it points downwards . if i move $a$ to the starting point of $u$ on the circle you can see that $a$ points towards the centre of the circle . the vector $a$ is of course the acceleration ( well it is the acceleration times the short time interval between $u$ and $v$ ) and that is why the acceleration points towards the centre of the circle . if you did not have any acceleration the object would move in a straight line so it would just move away at a tangent to the circle . the effect of the acceleration $a$ is to bend the trajectory of the object so it stays on the circle . finally , you ask " what keeps the direction of acceleration always point inward " . you have mixed up cause and effect . it is not that there is some magical property of circular motion that causes the acceleration to be central . it is that if in some system the acceleration is always central the object will move in a circle ( or strictly speaking in a conic section , but let 's not complicate matters ) . for example suppose i tie a stone to a string and start whirling it round my head . the force causing the acceleration is provided by the string , and obviously the string always runs from the stone to the centre because my hand is at the centre . it is because the force caused by the string is always central that the stone moves in a circle .
the answer to your question is very wide and include many phenomena . there is no single mechanism that converts absorbed energy into heat . i will give you a general overview on this topic . heat is transferred by radiation which is in general an electromagnetic wave ( not light only ) . for example , ir radiation alone provides 49% of the heat provided to earth from sun radiation . have a look at heat section of this page . in electromagnetic spectrum , the wavelength varies significantly , which means that materials respond differently to different ranges of wavelengths . for example ( listing from long wavelength to short wavelength ) : radio waves : the wavelength here is very long such that the materials constituents ( atoms and molecules ) do not sense the waves . thus most solids are transparent to radio waves , which means there is no wave-material interaction . there is no heat generated . the transparency is clear as you can get cellphone reception in your home as the wave simply travel through walls and our bodies . microwaves : the wavelength here is shorter , the photons energy in this level are comparable to the energy levels of the molecular rotational levels , which means that a molecule could absorb a microwave photon and start to rotate . this rotation is a form of kinetic energy of molecules which is translated as heat when you look at the whole ensemble of molecules . this mechanism is the mechanism known for microwave heating in your kitchen . ir : the photon energy in this range is comparable to molecular vibrational levels . so if a molecule absorbed an ir photon it will vibrate , which is translated into heat when you look at the whole ensemble of molecules . visible light : things get complicated here , the photon energy here is comparable to electronic levels within an atom . there is no simple direct explanation of how photon energy is transformed into heat . one can argue for visible light that an electron can absorb a photon and radiate it without increasing kinetic energy of atoms ( no increase oh heat ) . that is true but that is a very simplistic picture of what happens in reality . first because the visible light is continuous spectrum , while the photons that can be absorbed by electrons are discrete . so there are many visible light photons with wavelengths that electrons can not absorb , which could be absorbed by ions or affect polar bonds somehow if they existed in the material . second , some materials like solids ( where heat by radiation is efficient ) have energy bands rather than the simplistic electron levels concept . so the reaction of materials to visible light in this case is complex . but one of the mechanism of transforming photon energy to heat that i can think of here is the absorption of visible light by ion lattice in some crystals . uv : the photons in this range have enough energy to ionize the atoms they hit ( by liberating an electron from its orbital ) . again , there is no simple direct explanation of how photon energy is transformed into heat . one possible route is the inelastic scattering of electrons with materials atoms . for example , have a look at page 5 of this report where it is mentioned that inelastic electron scattering can induce phonons ( lattice vibration , which is a form of kinetic energy of atoms ) x rays : the photon energy in this range is much larger than the energy required to move electron from one level to other . x rays photons can ionize an atom and become lower energy photon through compton scattering . there is no simple direct explanation of how photon energy is transformed into heat . briefly , heating by radiation in general can not be attributed to a simple explanation . the mechanisms through which photons energy is transformed into heat depend on the energy of the photon and the properties of the material . the simplistic picture of a photon being absorbed by electron is narrow to explain conversion to heat because it only describes a single electron in a free atom . that is a narrow view of material properties . have a look here and the nice figure of non-ionizing radiation section of this page hopefully that was helpful
the problem with this question is that static friction and kinetic friction are not fundamental forces in any way-- they are purely phenomenological names used to explain observed behavior . " static friction " is a term we use to describe the observed fact that it usually takes more force to set an object into motion than it takes to keep it moving once you have got it started . so , with that in mind , ask yourself how you could measure the relative sizes of static and kinetic friction . if the coefficient of static friction is greater than the coefficient of kinetic friction , this is an easy thing to do : once you overcome the static friction , the frictional force drops . so , you pull on an object with a force sensor , and measure the maximum force required before it gets moving , then once it is in motion , the frictional force decreases , and you measure how much force you need to apply to maintain a constant velocity . what would it mean to have kinetic friction be greater than static friction ? well , it would mean that the force required to keep an object in motion would be greater than the force required to start it in motion . which would require the force to go up at the instant the object started moving . but that does not make any sense , experimentally-- what you would see in that case is just that the force would increase up to the level required to keep the object in motion , as if the coefficients of static and kinetic friction were exactly equal . so , common sense tells us that the coefficient of static friction can never be less than the coefficient of kinetic friction . having greater kinetic than static friction just does not make any sense in terms of the phenomena being described . ( as an aside , the static/kinetic coefficient model is actually pretty lousy . it works as a way to set up problems forcing students to deal with the vector nature of forces , and allows some simple qualitative explanations of observed phenomena , but if you have ever tried to devise a lab doing quantitative measurements of friction , it is a mess . )
there are a number of mathematical imprecisions in your question and your answer . some advice : you will be less confused if you take more care to avoid sloppy language . first , the term spinor either refers to the fundamental representation of $su ( 2 ) $ or one of the several spinor representations of the lorentz group . this is an abuse of language , but not a bad one . a particularly fussy point : what you have described in your first paragraph is a spinor field , i.e. , a function on minkowski space which takes values in the vector space of spinors . now to your main question , with maximal pedantry : let $l$ denote the connected component of the identity of the lorentz group $so ( 3,1 ) $ , aka the proper orthochronous subgroup . projective representations of $l$ are representations of its universal cover , the spin group $spin ( 3,1 ) $ . this group has two different irreducible representations on complex vector spaces of dimension 2 , conventionally known as the left- and right- handed weyl representations . this is best understood as a consequence of some general representation theory machinery . the finite-dimensional irreps of $spin ( 3,1 ) $ on complex vector spaces are in one-to-one correspondence with the f.d. complex irreps of the complexification $\mathfrak{l}_{\mathbb{c}} = \mathfrak{spin} ( 3,1 ) \otimes \mathbb{c}$ of the lie algebra $\mathfrak{spin} ( 3,1 ) $ of $spin ( 3,1 ) $ . this lie algebra $\mathfrak{l}_{\mathbb{c}}$ is isomorphic to the complexification $\mathfrak{k} \otimes \mathbb{c}$ of the lie algebra $\mathfrak{k} = \mathfrak{su} ( 2 ) \oplus \mathfrak{su} ( 2 ) $ . here $\mathfrak{su} ( 2 ) $ is the lie algebra of the real group $su ( 2 ) $ ; it is a real vector space with a bracket . i am being a bit fussy about the fact that $\mathfrak{su} ( 2 ) $ is a real vector space , because i want to make the following point : if someone gives you generators $j_i$ ( $i=1,2,3$ ) for a representation of $\mathfrak{su} ( 2 ) $ , you can construct a representation of the compact group $su ( 2 ) $ by taking real linear combinations and exponentiating . but if they give you two sets of generators $a_i$ and $b_i$ , then you by taking certain linear combinations with complex coefficients and exponentiating , you get a representation of $spin ( 3,1 ) $ , aka , a projective representation of $l$ . if memory serves , the 6 generators are $a_i + b_i$ ( rotations ) and $-i ( a_i - b_i ) $ ( boosts ) . see weinberg volume i , ch 5.6 for details . the upshot of all this is that complex projective irreps of $l$ are labelled by pairs of half-integers $ ( a , b ) \in \frac{1}{2}\mathbb{z} \times \frac{1}{2}\mathbb{z}$ . the compex dimension of the representation labelled by $a$ , $b$ is $ ( 2a + 1 ) ( 2b+1 ) $ . the left-handed weyl-representation is $ ( 1/2,0 ) $ . the right-handed weyl representation is $ ( 0,1/2 ) $ . the dirac representation is $ ( 1/2,0 ) \oplus ( 0,1/2 ) $ . the defining vector representation of $l$ is $ ( 1/2,1/2 ) $ . the dirac representation is on a complex vector space , but it has a subrepresentation which is real , the majorana representation . the majorana representation is a real irrep , but in 4d it is not a subrepresentation of either of the weyl representations . this whole story generalizes beautifully to higher and lower dimensions . see appendix b of vol 2 of polchinski . figuring out how to extend these representations to full lorentz group ( by adding parity and time reversal ) is left as an exercise for the reader . one caution however : parity reversal will interchange the weyl representations . sorry for the long rant , but it raises my hackles when people use notation that implies that some vector spaces are spheres . ( if it is any consolation , i know mathematicians who get very excited about the difference between a representation $\rho : g \to aut ( v ) $ and the " module " $v$ on which the group acts . )
the question was motivated because i have a suspicion that the electron does not participate to the source of a gravitational field , and eventually not even responds to such a field . in 1908 milikan measured the charge on a single electron . the charge-to-electron mass ratio $q/m_{e}$ was calculated by thomson in 1897 using the angular momentum and the deflection due to a perpendicular magnetic field . i think that this value reflects the inertial mass . i found this old ( 1964 ) doc gravitational and resonance experiments on very low-energy free electrons by fairbank , and this entry experimental comparison of the gravitational force on freely falling electrons and metallic electrons by wittborn and fairbank , 1967 , with the abstract : a free-fall technique has been used to measure the net vertical component of force on electrons in a vacuum enclosed by a copper tube . this force was shown to be less than 0.09mg , where m is the inertial mass of the electron and g is 980 cm/sec2 . this supports the contention that gravity induces an electric field outside a metal surface , of magnitude and direction such that the gravitational force on electrons is cancelled . it seems that the issue remains unsettled . - docs of 1992 and 2007 - ( tests of the weak equivalence principle for charged particles in space ) fairbank , as everyone else back then , and even now , believed that the electron must participate in gravity , contrary to my suspicion , preferring to imagine the existence of an imaginary induced electric force , possibly because in the 50s and 60s existed some hype about possible effects relating electricity and gravity . experimentation is central to advancement of physics , and the solution of this unsettled issue may prove important in the genesis of a sucessful theory encompassing both the so called particles and the gravitation field . i can understand that performing the test of fairbank under microgravity conditions can discriminate if the electron responds to the gravity field . to test if the electron has an active gravitational mass a possible test that may work ( 'i do not know the tecnological limits . . . ' ) could be done using a collimated beam of slow neutrons with a path traversing between the plates of a very large high-voltage capacitor . the possible deflection , or not , of the beam may prove very interesting .
it really is just a definition , as misha said . a crystal is defined as a state of matter where translational invariance is broken to a discrete subgroup . it occurs when you have a simple molecule substance , where units are rigidly arranged in a packing . the densest packings of most non-contrived simple shapes repeat with a particular periodicity . you can arrange for complex molecules to crystallize , but it requires special preparation in aquaous solution , and proper ionic concentrations , which surround the complex molecule in a cage of a certain type . if you get a protein to crystallize , you write a paper , because then you can take an atomic scale picture of it using x-ray diffraction . if you just jam proteins together and cool them down you make amorphous non-translational things , like gels or rubbers . these are heavily tangled polymer chains with no translational subgroup preserved . you do not study them , because they are complicated . they have a ground state entropy , which is caused by the trapping of the gel in metastable states . these systems are the subject of active research , and their theory is not well understood . whether glasses have a sharp liquification transition was debated for decades ( current consensus is that they do ) .
it is not sound propagation but rather momentum diffusion that makes a fish swim . the rate at which momentum diffuses is determined by the kinematic viscosity , which for water is about $10^{-6} m^2/s$ . it takes minutes for momentum to diffuse in water over distances of centimeters , while the time scale over which a fish wiggles is tenths of a second . so , during a wiggle the water surrounding the fish does not ' communicate ' with any wall , and the fish does not notice any anomalous hydrodynamic effects .
this question is really about history and what was known to the protagonists in your tale and at what time . as a principle , relativity was embraced every bit as fully by newton and galileao as it was by einstein - it is just that einstein had a few more experimental results he had to gather into relativistic thinking . as in dgh 's answer the whole point of special relativity is that velocity is a relative concept insofar that the physical laws will seem the same to all inertial observers . this concept was well appreciated by galileo and newton . see for example the quote of galileo 's character salviati in the galileo 's ship thought experiment of 1632 . saviati 's narrative is clearly saying that there is no experiment whereby one could tell whether or not the ship were moving uniformly . so , in spirit , galileo 's and newton 's physics were no less " relative " than einstein 's . the problem is that there is no unique way to make physical laws the same for all inertial observers . see the derivation of the lorentz transformation under the heading from group postulates on the lorentz transformation wikipedia page . here the lorentz transformation is derived from very basic symmetry and homogeneity assumptions about the universe . take heed that the derivation only yields the form of the transformation , it does not yield the universal speed parameter $c$ . so there are a whole family of relativities that fulfill these basic symmetry and homogeneity assumptions - roughly that physics is the same for all uniformly moving observers - and both einstein 's special relativity and galilean relativity belong to this general family ; the latter is simply the limit as $c\to\infty$ . not to detract from einstein 's achievement , einstein was responding to experimental results not known to galileo and newton - namely that speed of light seemed to be constant for all inertial observers - and showing that , against intuition at the time , this observation could still be consistent with the principles of inertial invariance of physics so clearly stated by galileo 's salviati . it is simply that the reference frame transformation laws had to be different . to galileo and newton , the transformation laws implied by inertial invariance would have seemed to be unique ; einstein simply disproved this and showed instead that there was a more general transformation . that transforamtion challenged to notion of absolute time and simultaneity . this was the leap that revealed the non-uniqueness of galilean relativity , which is indeed unique if one insists on upholding the notion of absolute time . einstein 's physics is " more relative " than that of galileo and newton , but only by dint of experimental results . these results forced the forsaking of a principle that galileo and newton had no experimental grounds to forsake - that of absolute simultaneity and time .
a newton is 1 ${\rm kg\cdot \rm m/s^2}$ , not 1 ${\rm kg\cdot s^2/m}$ . also , you need to take the square root at the end .
per wikipedia , the electromagnetic tensor $f^{\mu \nu}$ contributes to the stress energy tensor $t^{\mu \nu}$ by $$t^{\mu \nu} = \frac{1}{\mu_0} \left ( f^{\mu \alpha} g_{\alpha \beta} f^{\nu \beta} - \frac{1}{4} g^{\mu \nu} f^{\gamma \delta} f_{\gamma \delta} \right ) $$ the einstein equations govern how the stress-energy tensor is coupled to spacetime curvature . since the magnetic field is entirely captured by the electromagnetic tensor , the answer is yes , magnetic fields contribute to gravitation .
the fundamental quantity in thermodynamics is entropy , which is a function of $n$-variables $s=s ( x_1 , x_2 , . . . , x_n ) $ . for instance , for a simple mono-component system $s=s ( u , v , n ) $ where $u$ is internal energy , $v$ is volume , and $n$ composition . taking the differential $$\mathrm{d}s = \sum_i \left ( \frac{\partial s}{\partial x_i}\right ) _{j \neq i} \mathrm{d}x_i = \sum_i f_i \mathrm{d}x_i$$ the quantities $f_i \equiv ( {\partial s}/{\partial x_i} ) _{j \neq i} $ are intensive entropic parameters and measure the change in entropy when variables change . for instance the intensive entropic parameter $ ( 1/t ) $ gives the change on entropy due to a change in the energy $u$ . using the thermodynamic theory of fluctuations it can be shown that $$f_i = k \left ( \frac{\partial \ln p}{\partial x_i}\right ) $$ where $p$ is the probability of a fluctuation in the variables near an equilibrium state . using the definition of average $$\langle a \rangle = \int a p \mathrm{d}x_1 \mathrm{d}x_2 \cdots \mathrm{d}x_n$$ the demonstration of the central result of linear nonequilibrium thermodynamics $$\langle f_i \cdot x_j \rangle = -k \delta_{ij} $$ is direct although it needs first the use of $ ( \partial \ln p / \partial x_i ) p = \partial p / \partial x_i$ in the integrand and next integration by parts . what wikipedia makes is to rewrite this central result using the new quantities $x_i \equiv - f_i / k$ but the physically important quantities are the $f_i$ often also called in this context thermodynamic forces or affinities .
i like brandon 's very physically intuitive answer : mine is a little drier . it is simply that three waves $e_j ( t ) ; \ , j=1,2,3$ mix through $n^{th}$ order nonlinearity by way of $n^{th}$ power term $\left ( \sum_{j=1}^3 e_j ( t ) e^{-i\ , \omega_j\ , t} + e_j ( t ) ^* e^{i\ , \omega_j\ , t}\right ) ^n$ in the taylor series for the input to output transfer function . so in the $n^{th}$ order term we get frequencies $$|\sum_j a_{n , j} \omega_j|\qquad ( 1 ) $$ where : $$\sum_j |a_{n , j}| = n ; \quad a_{n , j} \in \{0,1,2 , \cdots n\}\qquad ( 2 ) $$ so you need , as you can guess , at least third order to get terms where all three frequencies are together in the sum . from ( 1 ) you will get : $$\begin{array}{c} |\omega_1 \pm 2\omega_2|\\ |\omega_1 \pm 2\omega_3|\\ |2 \omega_1 \pm \omega_2|\\ |2 \omega_1 \pm \omega_3|\\ |2 \omega_2 \pm \omega_3|\\ |\omega_1 \pm \omega_2\pm\omega_3| \end{array}$$ the last term is the only third order one where all three frequencies combine . you need to go to four wave mixing and higher order to get more " interesting " linear combinations of frequencies of the form being $|2 \omega_1 \pm \omega_2\pm\omega_3|$ and fifth order to get $|3 \omega_1 \pm \omega_2\pm\omega_3|$ .
i think this is a case of the mathematics being designed to model reality . as you say , making the time component of the metric positive would give a space that does not match what we observe . in particular , the negative component for time allows us to disconnect regions of space that are not causally linked . in other words , the fact that the speed of light is finite and a maximum means that we must describe space-time with a shape that keeps causally disconnected regions separate . the necessary shape is reflected in the choice of the sign of $\eta_{00}$ . that is how i understand anyway . . .
if the photon has more energy than the ionization energy , it can ( and often does ) ionize the atom . one nice way to think about this is to put the various energy levels of the atom in an energy level diagram like this : ( taken from this web site ) . all of the bound-state energy levels lie below the line marked $e_\infty=0$ ev . above this line is the region called " free electrons " in the diagram . this corresponds to an ionized atom . in this range , the energy levels are not quantized -- that is , any energy is possible . so any photon with enough energy to put the total energy somewhere in that shaded region is capable of ionizing the atom .
i suppose a lot of what constitutes the " friedmann equation ( s ) " is just up to definitions . however , with the 3 equations you listed , there will be redundancy . here is the normal derivation i usually see ( feel free to skip the first two paragraphs if you are not familiar with tensors/gr ) : given einstien 's equation in the form $r_{\mu\nu}=-8 \pi g s_{\mu \nu}$ , ( where $s_{\mu \nu}$ is related to the stress energy tensor by $s_{\mu \nu}=t_{\mu \nu} - \frac{1}{2}g_{\mu \nu}t^\lambda_\lambda$ ) and with some playing around with the spatial portion of the robertson walker metric ( see weinburg cosmology ) , we can get a riemann tensor $r_{ij}=\tilde{r}_{ij}-2\dot{a}^2 \tilde{g}_{ij}-a\ddot{a}\tilde{g}_{ij}$ ( tilde means the spacial metric and it is curvature tensor ) to $r_{ij}=- [ 2k+2\dot{a}^2+a\ddot{a} ] \tilde{g}_{ij}$ ( where $k$ is the curvature constant ( -1,0 , +1 ) ) . we then decide on a stress energy tensor , using the principles of homogeneity and isotropy ( we do not want there to be some strange asymmetry ) , to get one of the form $t_{00}=\rho$ , $t_{i0}=0$ , and $t_{ij}=a^2p\tilde{g}_{ij}$ . which gives us $s_{ij}=\frac{1}{2} ( \rho-p ) a^2\tilde{g}_{ij}$ and $s_{00}=\frac{1}{2} ( \rho+3p ) $ . using all this , plugging back into the efes , we get two equations ( one for i=j=0 , and one for the rest ) : ( 1 ) $-\frac{2k}{a^2}-2\frac{2\dot{a}^2}{a^2}-\frac{\ddot{a}}{a}=-4\pi g ( \rho - p ) $ ( 2 ) $\frac{3\ddot{a}}{a}=-4\pi g ( 3p+\rho ) $ then , cause the first equation is sorta unwieldy , we can add three times the first equation to the second to get the nicer ( and more familiar ) : ( 3 ) $\dot{a}^2+k=\frac{8}{3}\pi g a^2$ we can also get the following equation from ( 1 ) and ( 2 ) : ( 4 ) $\dot{\rho}=-\frac{3\dot{a}}{a} ( \rho + p ) $ which is really no surprise , as this conservation law is found in any solution to the efes . so really what equations you decide to call the " friedman equations " is up to whatever your doing . ( 1 ) and ( 2 ) are the direct consequences of derivation , which you can then use to derive ( 3 ) and ( 4 ) . it just turns out for most cosmology calculations , we do not want to use ( 1 ) , and the last 3 ( ( 2 ) ( 3 ) ( 4 ) ) are more useful . however there will be redundancy within these 3 equations ( after all , they came from just two equations ) ! edit : just to adress the ops question : lets take ( 2 ) and ( 4 ) : ( 2 ) $\frac{3\ddot{a}}{a}=-4\pi g ( 3p+\rho ) $ ( 4 ) $\dot{\rho}=-\frac{3\dot{a}}{a} ( \rho + p ) $ lets multiply ( 2 ) by $\frac{2a\dot{a}}{3}$ . $\frac{2a\dot{a}}{3} [ \frac{3\ddot{a}}{a} ] =\frac{2a\dot{a}}{3} [ -4\pi g ( 3p+\rho ) ] =&gt ; 2\dot{a}\ddot{a}=-\frac{8}{3}\pi g ( 3p a\dot{a} + \rho a\dot{a} ) $ some tricky algebra here : $=&gt ; 2\dot{a}\ddot{a} = \frac{8}{3}\pi g ( -3 a\dot{a} ( \rho+p ) + 2\rho a\dot{a} ) $ we now substitute in the conservation of energy equation . ( 5 ) $2\dot{a}\ddot{a} = -\frac{8}{3}\pi g ( \dot{\rho}a^2+2\rho a \dot{a} ) $ but hey ! this looks sorta like what you would get if you differentiated the friedmann equation ( 3 ) , note that $k$ is not time dependent ( if it were , that would be crazy ! ) . since your integrating instead of differentiating you will have constants of integration , but you can just tie that into the $k$ term ( which is a sorta interesting way to view what $k$ means ) . edit : doing a similar process will show you a way to derive ( 4 ) to begin with .
same charges repulse each other . so when they are confined in a system , they try to have stable distance among them as much as possible . for a hollow conducting sphere this stable maximum distance is equal distribution of charges in the outer surface . if any charge try to go the inside conducting surface it automatically decrease distance which increase repulsion . that is why charge inside the conducting surface zero .
are you worried that the cables that go to the fukushima reactors will carry radioactivity out ? the answer is no . you should read up a bit on radioactivity and educate yourself , since it is one of the facts of life . in the article you will see that it is atoms that are responsible for radioactivity whereas the current in the cables is due to electrons . the parts of the cables that are in a radiation environment will become radioactive , but that activity will remain in the locality , and the length of cable that was exposed . it in no way can be transmitted away from the region the way the current is transmitted .
the blades are at an angle . as the blade moves down it hits an air molecule and the air molecule " bounces off " toward you . it is just like hitting a ball in tennis/baseball
energy exchange is quantized when moving a electron from one bound state to another bound state . this is not because the exchange is inherently quantized , but because the states the electron may occupy are quantized . thus the standard photo-electric effect in which a photon can not excite an atom unless it has a minimum energy . however , . . . there are multi-photon processes by which sub-threshold light can excite transitions . cross-section for them go by intensity-squared ( or worse ) and are very small for any reasonable light intensity . to study or employ them you get powerful , short pulsed laser systems . where short pulsed means nano-second or faster pulses and powerful means " do not look into beam with remaining eye " . even then you do not get a lot of rate . these processes are utterly negligible for the kind of benchtop experiment we use to teach the photoelectric effect : you just can not get enough intensity . ( see below for how negligible . ) the conceptual model here is that the first photon bumps the electron to a short-lived , unstable state without well defined quantum numbers , and the second comes along before that state decays and finishes the job . we are currently exploring the application of such a process to calibrating light yields , opacities in a large volume of scintillating material . from new j . phys . 12:113024 , 2010 : for gases the one-photon absorption cross-section $\sigma_1$ is typically of the order of $10^{−17}\text{ cm}^2$ , whereas the two-photon and the three-photon cross-sections are of the order of $\sigma_s = w/f_2 \approx 10^{−50}\text{ cm}^4\text{ s}$ and $\sigma_3 = w/f_3 \approx 10^{−83}\text{ cm}^6\text{ s}^2$ , respectively . where $f$ is intensity in photons/second and w is excitation rate in reciprocal seconds .
your situation is unfortunate , but this will become very typical as you get further in school . i will do my best to explain this problem fully : to begin with , in this problem , two interference patterns are formed , each pattern unique to one of the wavelengths provided to you . it is important to understand here that the fringe patterns might overlap , but do $\textbf{not}$ $\textit{interfere}$ with one another ( in terms of wave front interference ) . consequently , it can be concluded that the bright fringes of one wavelength will eventually share locations with the dark fringes of the other wavelength . when this occurs , no fringes will be visible , as there will be no dark bands to differentiate a between a single fringe , and the fringes adjacent to it . in order to make the transition between $\textbf{periods of fringe absence / appearance}$ the mirror 's change in position must produce an $\textbf{integer number}$ of fringe shifts for each wavelength , and the number of shifts for the shorter wavelength must be one more than the number for the longer wavelength . $\textbf{this next bit is imperative for understanding the michelson interferometer:}$ the light travels the length of the apparatus $\textbf{twice}$ , so the change in the position of the mirror must also be accounted for $\textbf{twice}$ . thus , even though a fringe shift possesses a wavelength value of $\lambda$ , we will denote it as $\cfrac{\lambda}{2}$ and the change in position of the mirror $ ( \bigtriangleup x ) $ will become $2 ( \bigtriangleup x ) $ to wrap this problem up , let 's make $\epsilon_n$ the number of fringes produced by a given wavelength . $\epsilon_1$ = $\cfrac{2 ( \bigtriangleup x ) }{\lambda_a}$ and $\epsilon_2$ = $\cfrac{2 ( \bigtriangleup x ) }{\lambda_b}$ , therefore $\epsilon_2$ = ( $\epsilon_1 + 1$ ) from here we can see that $\cfrac{2 ( \bigtriangleup x ) }{\lambda_b}$ = $\cfrac{2 ( \bigtriangleup x ) }{\lambda_a} + 1$ , meaning that $\bigtriangleup x = \cfrac{\lambda_a\lambda_b}{2 ( \lambda_a-\lambda_b ) }$ i will let you plug in your values , you should get a number much less than a meter , but much greater than a nanometer . this was not an easy problem for what i am assuming is an a level physics course . let me know if you have questions . good luck .
your $ t=m_aa $ is wrong . it is $ t - f= m_aa $ . because tension due to $ m_b $ is pulling it forward and friction is trying to resist that force . using $v^2 = u^2 + 2as$ for calculating velocity after moving $ s = 0.03 m $ forward with initial velocity $ u = 0 m/s $ is correct ! .
the random potential is a model for small fluctuations in the local energy of an electron , when there are defects or impurities that raise the energy of an electron at certain spots by a little bit , and lower it at other spots . this is a good model , although it does not look like one at first , because the quantum mechanical solution is not sensitive to the finest details of the random ptential v ( in low dimensions ) , rather the quantum mechanical electron only notices the slowly varying average value of v from region to region . this is a huge field , and i am only giving a very superficial overview . it is one of the most studied problems of the past half-century , and it deserves the attention it gets . the random potential looks superficially intimidating , because the usual definition is that the potential v is gaussian random at every point , and generated by the probability distribution $$ e^{-{1\over 2}\int v^2 d^dx} $$ in d-dimensions . this is an independent gaussian at every point x of d-dimensional space , and the statistical correlation function of v for different picks from this probability distributions is $$ \langle v ( x ) v ( y ) \rangle = \delta^d ( x-y ) $$ you should think of this as follows : the space is made a lattice of size $\epsilon$ , the integral is replaced by a sum , and the delta function by a discrete bump of size $1\over \epsilon^d$ . this is probably the discrete approximation you saw . then the statement is that in the limit that $\epsilon$ goes to zero , you get a sensible theory . the theory is the eigenvalue of the schrodinger operator $$ ( \delta + \lambda v ) \psi = e\psi $$ where $\lambda$ is the disorder strength--- it tells you how strong the randomness is . the point is that the potential energy in each lattice site is a gaussian of width $\lambda\over \epsilon^{d\over 2}$ . the gaussian has a huge variance , and the potential swings from a huge positive value to a huge negative value every few neighboring cells . it does not look like a sensible notion of potential superficially . potentials that are this crazy make no sense classically , since you have turning points everywhere , and classically , you just oscillate around the deepest minimum , and you can not , because there are bumps everywhere blocking your path . for this problem , you have to stop thinking classically completely . in quantum mechanics the particle is tunneling around the minima , going through to other minima , and getting suppressed some whenever it has to go through a high potential region . the main question here is whether the particle can wind its way to infinity , so that the wavefunction is extended , or if the wavefunction of every state decays exponentially at long distances . this is the question of whether the random material localizes the electrons , so that it is an insulator , or delocalizes the electrons , so that it is a conductor . the classic reference for this is the nobel prize winning paper " absence of diffusion in random lattices " by p.w. anderson . despite the title , it is not talking about statistical diffusion , it is talking about this quantum mechanical thing . self-consistency in dimensions &lt ; 4 the first thing to do is to make sure that the problem has a self-consistent small-spacing limit . this is probably what you are interested in doing rigorously , but the heuristics are instructive . consider placing a small wavefunction bump of width a on top of the random potential . the thing to check is that you ( almost surely ) can not gain an infinite amount of energy by placing the bump in the right place , at a point where the average of v over the bump is going to be very negative . dividing the region of size $a^d$ in to $\epsilon^d$ cubes , you find that there are $n= ( a/\epsilon ) ^d$ of them , each random , and so by the square-root law , the typical negative value of v on this bump will have the size $1\over \sqrt{n}$ , while the fluctuations in v on the scale $\epsilon$ blow up as $1\over \epsilon^{d\over 2}$ . the $\epsilon$ parts cancel out , and you find the a scaling of the typical potential energy in a region of size a : $$ v_\mathrm{typ} = - {\lambda\over\epsilon^{d\over 2}} ( {\epsilon\over a} ) ^{d\over 2} = {\lambda\over a^{d\over 2}}$$ as a gets bigger , you average over more independent random v 's , and you get a smaller average , and it shrinks as this power law . as a gets smaller , you can make a more and more negative potential energy without bound , because the fluctuations get bigger ( and you look for the most negative region ) . but there is a cost to making a smaller , the small width means there is a momentum uncertainty of order $1/a$ , and this makes a kinetic enegy which goes as $1/a^2$ . so so long as the kinetic energy beats the potential energy in how it blows up , there will be a good limit . otherwise , the particle 's states will collapse to a point . you probably are asked to prove this rigorously . the condition is that d&lt ; 4 . so the problem makes sense in 1,2,3 dimensions , and maybe in 4 . it also makes sense on fractal shapes of the appropriate fractal dimension strictly less than 4 . the case of 4 dimensions is marginal , and i am not sure if the problem is well posed there , if it depends on the lattice details , or if there are different 4 dimensional fractals where it works and others where it does not . this is the boundary case . to make this rigorous might be somewhat difficult , because you need to prove that with probability 1 , one cannot gain energy by finding very special locations in a configuration of v where the potential energy is much much smaller than the typical energy flucuation size . there might not be good methods in mathematics for doing this rigorously right now , but it is completely obvious on physical grounds ( the number of positions is only growing polynomially with the mesh shrinking , while the number of configurations on the shrinking mesh grows exponentially . the much greater number of possibilities for the microscopic v values makes it obvious that searching through polynomially many positions is not going to get you more than a small factor over the naive estimate above . ) 1 dimension is exactly solvable the one dimensional problem can be solved exactly , and this was done by bert halperin in http://prola.aps.org/abstract/pr/v139/i1a/pa104_1 . one way to treat this is to use the so-called " r-matrix " , which is a 1-dimensional reflection/transmission matrix that tells you how plane-waves scatter/reflect off a bump . the product of the r matrices in a series is the r-matrix for the random potential problem , and in this case , you can analyze the product numerically , and see that you get attenuation for all v 's . this means that all wavefunctions in 1d are localized , they all fall off exponentially at long distances . one surprising ( but true ) prediction is that all wires are eventually insulating . since all materials have a little bit of random potential , all one-dimensional wires will localize eventually . the reason you do not see this is because long metal wires both have a relatively small disorder , and a large thickness , so that the localization length is much larger than you can measure . 2 dimensions is critical the two dimensional problem is fascinating , because it is the critical point for a renormalization group analysis of the problem of localization . at two dimensions , the random potential problem is marginally localizing . this is an involved analysis , and i am not prepared to write about it right now . one thing that this suggested is that 2d is like 1d , in that every $\lambda$ localizes . there were simulations , however , that suggested that this is not so , that there is a localization transition in 2d . this debate went on for 20 years in condensed matter physics , and i have heard that it is considered resolved now , although which way , and with what methods ( numerical or rg ) i would not be able to say . 3 dimensions : anderson transition and weak localization the three dimensional case is very interesting , since it has a second order phase transition between localizing and delocalized states as you tune the parameter $\lambda$ . at $\lambda=0$ , you have complete de-localization , all the states are momentum states , they are spread out and do not decay at infinity . when $\lambda$ is weak , you can treat the random potential as a small perturbation , and calculate the corrections to material properties from a few orders of perturbation theory . this is not so great from a mathematical perspective , because the perturbation is not localized at one spot , so the eigenfunctions are completely different to a mathematician , but for a physicist , the current transmission in the conductor is only altered by a sequence of scatterings from the randomness . these scatterings have the property that if you scatter k-times , and you scatter exactly backwards k-times , you have the same phase for both processes . this leads the particle to want to stay put more than you expect , because of the constructive interference between the paths and their time-reverse . if you break the time-reversal symmetry , by introducing a magnetic field , you then allow the electrons to flow better , because you remove the constructive interference . this effect is called weak localization , and it leads to materials having a resistivity peak at 0 applied magnetic field . this magneto-resistance was a very active subject in the 1980s and 1990s , as was weak-localization in general . one of the nice aspects of this is that it lets you figure out how long electrons maintain phase-coherence in a material , since the effect requires constructive interference of paths that are extended a significant way inside the metal . but the standard story is just the localization . at extremely high $\lambda$ , we know from the localization scaling that the wavefunctions will scruch up small , but we know from the dimensional scaling that they can not go down to $\epsilon$ , but must occupy some intermediate scale . anderson suggested studying this starting from completely localized states . the approaches disorder problems are interesting because they require you to average over the disorder , but not like a dynamical variable that is thermally fluctuating , but like a static thing . physicists call this " quenched " disorder , since it is analogous to quickly quenching a hot material like steel in water and freezing the impurities and disorder in place , without allowing them to come to thermal equilibrium and iron themselves out . for quenched disorder , you want to compute correlation functions , and then average over the disorder , $$ \langle \phi \phi \rangle_v = \sum_v p ( v ) \langle \phi\phi\rangle = \sum_v p ( v ) {\partial\over \partial j}{\partial\over \partial j} \log [ z ( j ) ] |_{j=0} $$ the quantity $z$ is also a sum over configurations , it is the quantum path-integral ( or the quantum statistical path integral ) . the difference is that the average over $v$ has to be done after you take the log of $z$ , since you do not want $v$ to become a dynamical quantum field , or a classical statistically fluctuating field , but you just want to average the results of the calculation over all values of $v$ . traditionally , there are two ways to do the average over $v$ . the first is parisi 's replica trick : the idea here is to perform the sum over $v$ with $n$ copies of the system : $$ \sum_{v\phi_1 . . . \phi_n} e^{-s ( \phi_1 ) - s ( \phi_2 ) . . . - s ( \phi_n ) } $$ this gives $$ \langle z^n\rangle $$ then you take the formal limit $n\rightarrow 0$ , in which the leading scaling is $$\langle \log ( z ) \rangle $$ . this idea is probably the hardest thing to imagine making rigorous , but this replica idea has been immensely fruitful for physics . it bears a resemblance to the notion of renyi entropy in mathematics , but it is more formal , since the actual partition functions are only fully defined for integer $n$ greater than 1 . this is a vast field , and you can google " replica trick " and " replica symmetry breaking " to learn more . this method is indispensible to modern condensed matter physics . the second method is the supersymmetry approach , and it relies on the following fact : in a susy system , the partition function is exactly 1 ! this might look surprising , but it is obvious in a stochastic system with a nicolai map ( see this answer : a certain $\cal{n}=2$ superconformal theory ( or is it ? ) ) . when you have a stochastic system , the partition function is constant--- it only depends on selecting the noise variable , not on the values of the field . if you use this fact , together with the fact that the derivatives of $\log ( z ) $ with respect to sources is the reciprocal of z times the derivative of $z$ with respect to the sources , you get that for a susy system , you can average $\log ( z ) $ just by averaging $z$ . this is the other way of dealing with disorder . the susy method and the replica method are complementary , and have given insight into different problems . the replica method has been more general although less rigorous , and more fraught with worries about whether it works .
it is not an uncommon interpretation that the alcubierre drive acts as a multiplier of an existing subluminal velocity . the trouble is that the alcubierre metric describes the drive in constant motion that does not change with time . it tells us nothing about how the drive accelerated to that speed or decelerated from it . the drive is moving in whatever direction the metric says because that is how the metric was constructed . i have seen very few papers on the alcubierre metric that even mention the question of acceleration , and none that treat it in detail . i would guess the ratio of hardness of the problem to interest in the outcome is too high for most researchers . the paper the alcubierre warp drive : on the matter of matter examines it briefly although the authors ' interest is really on the effect of matter caught up in the drive . so i am afraid there is no answer to your question .
we do not really understand why charge is quantized . nor we do know if there ought to be magnetic monopoles . these two things seem linked . dirac gave an argument for charge quantization in the early days , but this presupposed the existence of a magnetic monopole . in maxwell 's equations , it would be completely natural to imagine the existence of magnetic monopoles , and some high energy theories predict their existence , but we have seen no direct evidence of them yet . and to my knowledge , though it is not my area of expertise , i do not think there have been any other really compelling arguments for charge quantization to compete with dirac 's original proposal . we also do not understand high temperature superconductivity yet . we also have a hard time computing electronic energy bands for complicated structures . current density functional theory techniques have errors on the percent level .
first of all , to have standing waves you must be talking about a wave carrying system with spatial extent : something like a guitar string . such a system has a set of possible vibrational modes . the first two modes are shown in the figure . we can describe each mode shape with a function $\phi_n ( x ) $ where $n$ is a label that indexes the modes , ie . $n$ is an integer going from 0 to $\infty$ . note that for each mode , $n$ is the number of nodes other than the endpoints . figure : first two normal modes of a string . the black line indicates the rest position . the blue solid line is the zeroth ( "fundamental" ) mode . the broken red line is the first mode . suppose the string is in some arbitrary state , where the deflection from rest at each point $x$ is given by a function $y ( x ) $ . it turns out that you can write any $y ( x ) $ like this : $$y ( x ) = \sum_{n=0}^{\infty}c_n \phi_n ( x ) . $$ that sum is called a fourier series , and the point is that you can write the state of a string as a linear sum of the normal modes . what is special about the modes is that if you start the string in exactly one of those modes , it will keep that shape forever ; only the deflection amplitude will oscillate . for example , if we start the string in $\phi_0$ the string 's deflection for all time is $$y ( x , t ) = \phi_0 ( x ) \cos ( \omega_0 t ) $$ where $\omega_0$ is a frequency associated to the $0^{\text{th}}$ normal mode . the fact that this shape is maintained forever means that the mode is a standing wave , by definition . if you start the string in the generic shape given in our first equation above , the string 's shape for the rest of time will be $$y ( x , t ) = \sum_{n=0}^{\infty}c_n\phi_n ( x ) \cos ( \omega_n t ) . $$ to recap : you can always write the shape of a string as s sum of normal modes ( aka standing waves ) each with its own shape and vibrational frequency . now on to your question . . . if you pluck the string in some way such that the initial shape is not one of the normal modes , or in your words , not one of the usual harmonic standing waves , you can still write it as a sum of other modes ( sum of other standing waves ) . the future evolution of the string 's shape is then given by our third equation . so , the answer to your question is basically that if you excite the string in a non-standing wave way , it will vibrate as a sum of standing waves , and that sum is just whatever set of standing waves superimpose to match how you are exciting the string . note : everything said here pertains to linear systems . most things in nature are at least approximately linear as long as the amplitude of the waves is not too big . there are exceptions to that , however . if that does not make sense , please post a comment : )
the geometry in your picture is too classical . once you pass the event horizon , it does not look like a sphere surrounding you anymore , and you do not see it as a special surface anyway . if you look back along a radial direction , you will see the same horizon point ahead of you ( in the past ) and behind you ( also in the past ) , at different affine parameter along the horizon ( this is clear in a penrose diagram ) . but you will not see the horizon as a sphere . when you approach a schwarzschild singularity , there is no way to avoid getting compressed to oblivion , because all the volume you carry is compressed to a tiny volume near r=0 . the radial area is r , and the area of a sphere is $4\pi r^2$ always , and r is time inside the horizon , and you are necessarily drawn to r=0 , which is the singularity . you can not save yourself by conformal mapping , because the actual physical distances are shrunk--- even if you were to conformally get shrunk to zero size , your matter is not conformally invariant , the atoms set a scale . the dr component of the metric does not vanish at the singularity , it is limiting value is ${1\over 2m}$ . this means that you are losing a certain unit of r per unit time as you fall in , which means your radial volume is shrinking to zero quadratically with time . the time part of the metric ( which is spatial now ) goes to ${2m\over r}$ , and so you gain a linearly diverging space in exchange , but the quadratic compression does not make up in volume for the quadratic sphere shrinking . further , this is not a conformal transformation in any reasonable sense , it is spaghettification . the real caveat about black holes is that this whole story assumes the black hole is neutral and nonspinning . for spinning or charged black holes , the interior structure is altered in radical ways , and there is nothing classically wrong with going in and coming out , except for some dubious arguments about what happens when you hit the cauchy horizon in the interior .
at the specified point , there is a certain tangent line to the path . also at that point , the vector field $\mathbf f$ has certain components . we want to know the component of $\mathbf f$ in the direction of the tangent line to the path at the point given . here 's how i would approach this . let 's restrict to the $x$-$y$ plane . the curve on which the object is traveling can be written as a parameterized curve in 2d as $$ \mathbf x ( s ) = ( x ( s ) , y ( s ) ) = ( s , s^2 ) $$ the unit tangent vector to this path at parameter value $s$ is $$ \mathbf t ( s ) = \frac{\mathbf x' ( s ) }{|\mathbf x ( s ) |} = \frac{ ( 1 , 2s ) }{\sqrt{1+4s^2}} $$ the tangential component of a vector field $\mathbf f ( \mathbf x ) $ at a given point along the curve corresponding to parameter value $s$ is therefore $$ f_t ( s ) = \mathbf f ( \mathbf x ( s ) ) \cdot\mathbf t ( s ) $$ i will let you try to figure out the rest . cheers !
" site " is synonymous with " vertex " . they are called " sites " because they are the places where the objects of interest ( particles with a spin or whatever ) are located . so it is a lattice with $n$ vertices .
it may not be the answer you are looking for but i recommend you get a thermos or a well insulated flask . these are what mountaineers use and you do not have to change the chemical composition of water this way .
if the temperature is not much below freezing , the rate of heat transfer from your plants ( and particularly from the earth around their roots ) is low , if there is a lot of water present , the high heat of fusion means that it will take a long time to actually freeze much of it . so maybe the plant makes it through the night without too much damage . note that if it does not warm up enough the next day the second night will kill them because it starts close to freezing .
this is a common question that applies to chemical reactions , batteries , and other common forms of energy conversion . the short answer is yes , but of course the change is negligible . in a chemical reaction you have a set of reactants and a set of products . if you were to take the mass of the reactants and sum them up , you would find them to be more than the sum of the mass of the products . this mass difference time $c^2$ is the energy of the reaction . again , this is by an amount that is so small that it is unmeasurable . the mass change from of nuclear reactor fuel after burning it at a large heat rate for 4 years is barely measurable itself , if you are talking about putting the fuel on a scale and measuring it . now come the qualifiers . if you put air and wood in a completely closed container and let it burn what would happen to the total mass of the container ? it would stay the same . the temperature would increase , and as the molecules move faster the mass increases ( basic relativity ) . if you preformed a chemical reaction , then cooled the products , you could try to measure a mass difference that is theoretically predicted . but even assuming you have a scale that is precise enough , you will have to look into other things like tidal forces . experimentally measuring these mass changes would be a futile effort .
total momentum is always conserved , in both elastic and inelastic collisions , but total kinetic energy is only conserved in elastic collisions . this example seems to be a completely inelastic collision , because at the end the objects merge . there is a formula to calculate the final velocity $v$ of two object with speed $u_1$ and $u_2$ and mass $m_1$ and $m_2$ in a completely inelastic collision , which is : $$v=\frac{m_1u_1+m_2u_2}{m_1+m_2}$$ here 's a simple derivation : since momentum is always conserved , the sum of momenta at the beginning is the same as the end : $$p_{i1}+p_{i2}=p_{f1}+p_{f2}$$ however , since this is a completely inelastic collision , at the end the two objects will merge , and so there will be only one final momentum . the final momentum is simply the sum of initial momenta , like final mass is the sum of initial masses : $$p_{1}+p_{2}=p_f\qquad m_1+m_2=m_f$$ then : $$v=\frac{p_f}{m_f}\qquad v=\frac{p_1+p_2}{m_1+m_2}\qquad v=\frac{m_1u_1+m_2u_2}{m_1+m_2}$$ total kinetic energy however is not conserved , as you can see summing initial kinetic energies and comparing with the final kinetic energy .
when electron " orbits " nucleus it is trapped in potential barrier caused by nucleus : electron needs some energy $e_0$ to escape ( overcome ) that barrier ( $e_0$ is same as work function $\phi$ ) , when photon with frequency $f$ ( energy of that photon is $e=hf$ ) comes and hits electron , it gives it energy ( $e=hf$ ) , and if it is greater than $\phi$ then electron can escape the nucleus ( overcome the potential barrier ) : and it will have some kinetic energy ( $ke$ ) too . so in order to electron escape the nucleus photon which will hit it must have greater energy than $\phi$ ( it is same as greater frequency than $f_0$ ( where $f_0$ is minimum frequency for photon which will hit electron so it will escape the nucleus and you can calculate it using this equation $\phi = hf_0$ and $f_0=\frac{\phi}{h}$ ) ) . but when photon which will hit electron has same energy as minimum energy required for electron to escape the nucleus ( $\phi = e$ or $f=f_0$ ) then electron will just " go up " to the top of potential barrier and then it will " go down " back to the bottom of the potential barrier : and it wont be able to escape the nucleus . ( sorry for my poor drawings )
the first equation , $$\frac{1}{x-x_0+i\epsilon}=p\frac{1}{x-x_0}-i\pi\delta ( x-x_0 ) $$ is actually a shorthand notation for its correct full form , which is $$\underset{\epsilon\rightarrow0^+}{lim}\int_{-\infty}^\infty\frac{f ( x ) }{x-x_0+i\epsilon}\ , dx=p\int_{-\infty}^\infty\frac{f ( x ) }{x-x_0}\ , dx-i\pi f ( x_0 ) $$ and is valid for functions which are analytic in the upper half-plane and vanish fast enough that the integral can be constructed by an infinite semicircular contour . this can be proved by constructing a semicircular contour in the upper half-plane of radius $\rho\rightarrow\infty$ , with an indent placed at $x_0$ , making use of the residue theorem adapted to semi-circular arcs . see saff , snider fundamentals of complex analysis , section 8.5 question 8 . the third one is the kramers-kronig relation , as funzies mentioned .
i agree with @pauljgans on that order/disorder is not a good analogy . moreover , the thermodynamic entropy of your room is the same when the room is ordered that when is not . in a first approximation the entropy of your room is $s = u/t$ with $u$ internal energy and $t$ temperature . entropy will change as $ds = du/t$ if you heat up your room from 15 ºc to 21 ºc . in a more general model you will be considering other changes in entropy due to flows of mass , chemical reactions , diffusion . . . yes , entropy is related to energy , but entropy is not a conserved quantity and its nonconservation is guided by the second law , which says that entropy production is non-negative .
i think you can get a estimate like this . for a semiconductor with no split in the quasi-fermi levels , the electrons and holes take their intrinsic values ( carrier density ) $n_0$ and $p_0$ ( $cm^{-3}$ ) . the charge carriers are in equilibrium with the thermal photons being absorbed and emitted inside the material . so if we calculate the emission rate of thermal photons then we know the time constant for how long the thermally generated carriers will last before recombining ( because at equilibrium upwards rates and downward rates must balance ) . let 's assume perfect bimolecular recombination , then the rate of thermal emission is , $\frac{\partial n}{\partial t} = bn_0p_0$ , where $b$ is the bimolecular recombination coefficient , for gaas , $b=7\times10^{-10}$$cm^{6}s^{-1}$ , and the intrinsic carrier density is , $n_i=n_0=p_0=2\times10^{6}$$cm^{-3}$ . this gives a transition rate of 2800 $s^{-1}$ . this seems a bit slow . but it is correct for the assumptions , namely because we assumed an un-doped intrinsic semicondutor ( the carrier density is very low ) . for more information i recommend ' light-emitting diodes by e . fred schubert’ , search for the vanroobroeck-shockley equation .
if i were thinking about it from my own perspective , i would see it as the need to balance energy . we already knew from relativity that mass carried an intrinsic energy with it , and so if i measure something and energy is missing , either i ( 1 ) did not account for some lossy mechanism or ( 2 ) did not measure all the energy coming out . from here , you are stuck with either a particle that carries energy but has no mass ( the photon , which i can measure ) or something that has mass but has kinetic energy . as you observed , this particle would have to carry no charge because of charge conservation but also because charged massive particles are easy to detect if they exist and do not decay rapidly . assuming you are hunting for photons at a known wavelength and did not detect them , the only choice left to you is a massive chargeless particle . energy conservation and charge conservation are the two most fundamental laws of physics , and we have never observed them violated in fundamental processes .
no . there is no theory of open , oriented strings . any string theory must contain closed strings , while the open strings are optional . if there is a string theory which contains oriented open strings , then it has the problem that the oriented open strings cannot couple to the oriented closed strings . why ? this is my understanding of the explanation given by the by thomas mohaupt in lecture notes " introduction to string theory " : in the closed string spectrum , there is an $\mathcal n = 2a$ algebra and and an $\mathcal n = 2b$ algebra which lead to different string theories . both have 32 supercharges . in each of these , , there are 2 gravitinoes , dilationoes , 1 in the ramond neveu-schwarz sector and 1 more in the neveu-schwarz ramond sector . these 2 gravitinos need 2 different supercurrents to couple to . but the $\mathcal n=1$ supersymmetric algebra with only 16 supercharges clearly cannot allow this ! thus , the open oriented strings would not couple with the closed oriented strings . the solution is to have open unoriented strings instead . this along with the unoriented closed strings is the type i string theory . the " only unoriented closed strings " theory is also inconsistent because of other reasons .
it seems you have done the hard part already , which is to evolve the object 's position as a function of time . and moreover , the simulation seems stable over a number of orbits . ( but eventually things start to go wrong ; you may want to look at an answer i wrote to what is the correct way of integrating in astronomy simulations ? ) so my understanding is all you really need is the full orbit plotted . in that case , there is no need to make it a function of actual time ( that is hard ) . instead , we can fall back to kepler 's first law , which says that the separation $r$ between the bodies obeys $$ r = \frac{r_\mathrm{max} ( 1-e ) }{1+e\cos ( \theta ) } . $$ $r_\mathrm{max}$ is the maximum separation , which i believe is the initial separation in your particular simulation . the eccentricity $e$ ( formula here ) is given by $$ e^2 = 1 + \frac{2er^4\dot{\theta}^2}{g^2m^2m} , $$ where $e$ is the orbital energy , $g$ is the gravitational constant , $m$ is the mass that is so heavy it essentially does not move , and $m$ is the lighter mass . this can more conveniently be rewritten $$ e^2 = 1 + 2 \left ( \frac{rv}{gm}\right ) ^2 \left ( \frac{v^2}{2} - \frac{gm}{r}\right ) , $$ where $v$ is the velocity of the moving particle . note that the convention is for $\theta$ to measure the angle from closest approach ( i.e. . , $\theta = 0$ is the negative $y$-axis in your simulation ) . if you want $\theta$ to increase with time , then the ( nonstandard ) transformation to cartesian coordinates is \begin{align} x and = -r \sin ( \theta ) \\ y and = -r \cos ( \theta ) , \end{align} assuming $r = 0$ corresponds to the massive object . in any event , to plot the orbit all you need to do is calculate the constant $e$ at one point in the orbit , sample $\theta$ with as many points as you like , calculate the separations $r ( \theta ) $ , and convert the $ ( r , \theta ) $ pairs to $ ( x , y ) $ .
assuming there is more flux coming into the open end of the tube than the thermal radiation in the closed part of the piston then yes . radiation pressure doing work is perfectly feasible eg https://en.wikipedia.org/wiki/solar_sail
a band , in itself , does not have charge . a band is just a collection of possible states where electons might be found . the electrons have charge but the states themselves do not . if the valence band is fully occupied , the charge of the electrons in the band will be balanced by the charge of the protons in the lattice , and the overall charge of the material will be neutral . as an aside , since the protons are well-localized in position space , they are very spread out in k-space . if there is an unoccupied state in the valence band , there will be a local ( in k-space ) net positive charge because the nuclear positive charges are not compensated by valence band electrons . this positve charge behaves as if it were attached to a particle , and we call that particle a hole . often the electron removed from the valence band is only moved up to the conduction band , so the net charge in the material may still be neutral , even though there is a hole present . the conduction band particle ( electron ) and valence band particle ( hole ) may move apart in k-space ( and in position space ) so we must keep track of them seperately .
a rigid body has 6 configuration degrees of freedom because its most general configuration can be obtained by translating ( 3 degrees of freedom ) and rotating ( 3 degrees of freedom ) its initial configuration . a mathy way of saying this is that its configuration manifold is $\mathbb r^3\times \mathrm{so} ( 3 ) $ . however , you are right that the phase space of a rigid body is 12-dimensional because each independent configuration degree of freedom corresponds to an independent momentum degree of freedom .
no . the law of the conservation of mass-energy is a purely local law ( i.e. . the divergence of the stress-energy tensor must be zero at any point in spacetime ) . you will find that , as long as your wormhole is a solution to einstein’s equations , this law will hold anywhere in the vicinity of the wormhole or within it . in other words , there is no point at which the mass suddenly disappears . it enters the mouth of the wormhole , pass through its throat , and emerges from its other mouth .
i was wondering if there is an intuitive way to understand the motion of a body influenced by two other massive bodies ( say the moon being influenced by the earth and the sun . no , intuition is not of real use in a three body gravitational problem , more so in many body . in 1887 , mathematicians ernst bruns [ 4 ] and henri poincaré showed that there is no general analytical solution for the three-body problem given by algebraic expressions and integrals . the motion of three bodies is generally non-repeating , except in special cases . [ 5 ] actually the motions of the planets are studied as possible dynamic chaos . planetaria solve the many body problem with numerical approximations .
i would add a humidity sensor , as water vapor is the strongest green house gas . this graph may suggest other gases > breakdown of the anthropic greenhouse gas emissions by gas . source : ipcc , 2007 here is an article on halocarbons .
in principle , the operator $h$ is not a matrix . however , you can write down a matrix representation . for that , you need a basis , which should be complete ( =very large matrix , probably infinitely large ) - for many illustrating cases in quantum mechanics , it is not complete . if your basis consists of $|a\rangle$ and $|b\rangle$ , it is not complete , but you can still describe some effects nicely . you gave the example $h=|a\rangle\langle b| + |b\rangle\langle a|$ , which means that $h$ is ( it is just another way to write it ) $$h=\begin{pmatrix} 0 and 1\\ 1 and 0 \end{pmatrix} , $$ and you can calculate its eigenvalues . for a general 2x2 hamiltonian matrix , the formula is $$h=\sum_{i , j}c_{i , j}|i\rangle\langle j|=\begin{pmatrix} c_{1,1} and c_{1,2}\\ c_{2,1} and c_{2,2} \end{pmatrix}$$ $i$ and $j$ can take the value $a$ and $b$ . the matrix is a 2x2 matrix because the hamiltonian only contains two vectors , $a$ and $b$ . how to expand a hamiltonian operator into a matrix is explained nicely on wikipedia ( first four formulas of the section ) . if it is already in the abstract form $h=|a\rangle\langle b| + |b\rangle\langle a|$ , you can just read it off : the prefactor of $|a\rangle\langle a|$ ( $=0$ ) is the top left entry , the prefactor of $|a\rangle\langle b|$ ( $=1$ ) is the top left entry etc . you do not need to know the explicit form of basis in which the matrix is written . remember that the eigenvalues of a matrix are invariant to unitary transformations .
i can not really know why your professor used c++ , but there are several reasons why you would : performance : scientific computations might require top-notch performance . c++ allows for very low level control over the hardware and has many possibilities for micro-optimization while still providing high-level abstraction . of course , this is also the reason that c++ is quite complicated . historical reasons : c++ has been the de facto general programming language for quite a while . your prof 's code might need to be compatible with other projects . plus , i think there are more libraries for c/c++ than for any other language , your professor might have to make use of such a library . it is also possible that c++ is the standard language for this kind of thing in your professor 's university or field , simply because it used to be the best choice . maybe your professor only knows c++ . . .
there are several ways to solve this . the simplest is to google for the formula for the focal length of two lenses in contact , but let 's instead do the problem step by step . this shows the situation with just one lens . we know th magnification is -3 , so we know that $v = -3u$ so the lens equation tells us ( i am taking +ve to be to the left i.e. $u$ is positive and $v$ is negative ) : $$ \frac{1}{u} - \frac{1}{v} = \frac{1}{f} $$ and since $v = -3u = -3d$ we have $f = 3d/4$ . now use the image from the first lens as the object for the second i.e. $u_2 = -3d$ and we know $f = 3d/4$ , then we have : $$ -\frac{1}{3d} - \frac{1}{v_2} = \frac{4}{3d} $$ and a quick rearrangement gives $v_2 = -3d/5$ and the magnification is $m = -3/5$ . the alternative is to google for the focal length of two lenses in contact , and amongst the hits will be this page telling you that the combined focal length is given by : $$ f = \frac{f_1 f_2}{f_1 + f_2} $$ or when the two lenses are identical : $$ f = \frac{f^2}{2f} = \frac{f}{2} $$ we have already worked out that $f = 3d/4$ , so $f = 3d/8$ , and using the lens equation we get : $$ \frac{1}{d} - \frac{1}{v} = \frac{8}{3d} $$ and solving for $v$ gives $v = -3d/5$ , just as we got before .
i would not call it integration , but knowing that the thermal conductivity is a constant , as well as the boundary temperature , you can calculate $$\frac{dt}{dx}=\frac{t_2-t_1}{\delta x} . $$ and then there is the difference between heat flow and heat flux , the latter being the heat flow per unit area , hence the factor $a$ . formally , you would not know that the temperature profile is linear , but you can derive it from fourier 's law . basically , you assume that the heat flux on point 1 and point 2 are equal , which givees you the steady state form of the heat equation $$\frac{d^2t}{dx^2}=0 , $$ which is integrated twice to obtain the linear profile .
the entropy of the ( density ) matrix $a$ , usually denoted $\rho$ , is evaluated as the supremum ( i.e. . maximum that may never be realized , just arbitrarily closely approached ) of the trace of the product of matrices $ah$ minus the natural logarithm of $\exp ( h ) $ , the exponential of $h$ . the supremum is taken over all hermitean matrices $h$ of the same size as $a$ . in practice , when you try to maximize this expression , you will find out that the best choice is $$ h = \ln ( a ) + c\cdot {\bf 1} $$ in words , the ideal matrix $h$ that gives you the supremum is the logarithm of the matrix $a$ ( there should be a minus sign somewhere to get the right conventions for entropy but i will overlook this detail to agree with the literature below ) . the choice of $c$ does not matter because the piece proportional to the unit matrix gets subtracted . see a proof of this formula e.g. as theorem 2.13 in http://www.mathphys.org/azschool/material/az09-carlen.pdf which also provides you with some background , as much as you need .
for a path integral , the integral $$ \int e^{i\int \lambda ( x ) f ( x ) } d\lambda = \prod_x ( 2\pi ) \delta ( f ( x ) ) $$ where the product is over all points x ( imagine a lattice ) . so that the result is not just a saddle point identity ( saddle points and perturbative approximations are not the same , perturbative is by expanding the exponential in a series ) , it is an exact path integration identity . while the result is not fully rigorous , that is only because of the issue of path integration definition , the limit of continuous space as the lattice gets finer . ignoring this mathematical point , there is no way this identity is avoided in any proper definition of path integration , since it holds without any regulator ambiguity .
first , note that in physics , we consider unitary representations $u$ of the poincare group acting on the hilbert space $\mathcal h$ of the theory because we are interested in a precise formulation of the concept of poincare transformations acting on the quantum mechanical states of the theory as symmetries ( since the laws of physics should be inertial frame-invariant ) ; and by wigner 's theorem , we choose these symmetries to be realized by unitary operators . these observations are related to your #1 and #3 and i think they should be kept conceptually distinct from the notion of a state that represents a single particle state . second , since such quantum field theories are supposed to allow for the emergence of states of particles , and in particular should account for states in which there is a single elementary particle , we expect that there is some subset $\mathcal h_1$ of the hilbert space of the theory corresponding to states " containing " a single elementary particle . given these observations , let 's rephrase your question as follows : what properties do we expect that the action of the representation $u$ will have when its domain is restricted to the subspace $\mathcal h_1$ ? in particular , we would like to justify the following statement the restriction of the unitary representation $u$ acting on $\mathcal h$ to the single-particle subspace $\mathcal h_1$ is an irreducible representation of the poincare group acting on $\mathcal h_1$ . this requires justifying two things : the restriction maps $\mathcal h_1$ into itself . the restriction is irreducible . i think that the justification of the first property is pretty intuitive . if all we are doing is applying a poincare transformation to the state of the system , namely we are just changing frames , then the number of particles in the state should not change . it would be pretty strange if you were to , for example , boost or rotate from one inertial frame into another and find that there are suddenly more particles in our system . the irredicibility requirement means that the only invariant subspace of the single particle subspace $\mathcal h_1$ is itself and $\{0\}$ . the physical intuition here is that since we are considering a subspace of the hilbert space in which there is a single elementary particle , expect that there is no non-trivial subspace of $\mathcal h_1$ in which vectors of this subspace are simply " rotated " into one another . if there were , then the particle would not be " elementary " in the sense that the non-trivial invariant subspace would represent the states of some " more elementary " particle . when it really comes down to it , however , i am not sure if there is some more fundamental justification for why the restriction of $u$ to $\mathcal h_1$ is irreducible aside from the decades of experience we have now had with particle physics and quantum field theory .
the reason your calculation is not right is because the mean energy of the molecules hitting the wall is not the mean number times the mean energy per molecule , because the fast molecules hit the walls more frequently than the slow ones .
no , it does not propagate . a low frequency electric field is a near field - it may depend on time but its distance-dependence is such that this field decays as $1/r^2$ at best . the propagating part of a low-frequency field ( $\propto 1/r$ ) has a magnetic component too . edit : the time-dependent field $e ( t ) $ has two terms : one is a near field and another is a propagating field $$e ( t ) =a_1/r^2+a_2\omega/r . $$ the propagating field strength is proportional to the frequency $\omega$ and is accompanied with a propagating magnetic field of the same strength . they can be small . a near magnetic field is smaller than a near electric field due to low frequency . so it is possible to have a variable electric field without variable magnetic field , but it does not propagate in reality . light is a propagating emf .
two books to get you started in general computational radiation transport : computational methods of neutron transport , written by e.e. lewis , edited by w.f. , jr . miller , isbn 0-89448-452-4 monte carlo particle transport methods : neutron and photon calculations , written by ivan lux and laszlo koblinger , isbn 0-8493-6074-9 the supporting materials for the mcnp ( monte carlo n-particle ) transport code maintained by los alamos national laboratory may also be useful : http://mcnp.lanl.gov/ these references are not targeted at nuclear rockets , instead they focus on the basic principles of computational radiation transport . application to nuclear rockets , or any other particular system , is just a matter of constructing the right set of assumptions and constraints to answer your specific question .
but if this is true , why is hydrostatic pressure perpendicular to the surface it acts on instead of always going down ? because of the properties of a fluid . a fluid will not tolerate directionality in force . if you have a jug of liquid and poke a hole at a surface below the water level , the fluid is ready to flow out in any direction , left , up , down , wherever . it will have the same push in any direction . you confusion might be cleared up by trigonometry . if a surface on the bottom of a tank is slanted , then the force pushing down is the pressure times $\cos{\theta}$ , if $\theta$ is the slope . mentally deconstruct the force vector into two - a downward component and a sideways . the downward component fits the model that you want pressure to fit . it is equal to the weight for fluid above it . the sideways force is also there , but it does not really change things . since it is sideways , it will not affect the weight of a jug of water you are holding . all of the sideways force components will cancel each other out so that the weight felt by the bottom surface is exactly equal to the weight of the fluid in the container .
if i had to give a one sentence answer to this question , it would be as follows : *that the phase and amplitude alone on one plane is enough to wholly define a three-dimensional light field arises from the various uniqueness theorems for maxwell 's equations within a connected volume given the solution on the volume 's boundary ; otherwise put : once you know a solution on a boundary , then the values within must follow from " reasonable " physical assumptions . for simplicity let 's sit with the scalar diffraction theory , so now we are essentially talking about uniqueness theorems for the helmholtz equation $ ( \nabla^2 + k^2 ) \psi = 0$ . uniqueness theorems when $k^2 &gt ; 0$ or when $k^2 \in \mathbb{c}-\mathbb{r}$ are much more complicated than when $k^2\leq0$ . the latter case corresponds to static solutions of the klein gordon equation or to static solutions of the maxwell equations with or without an assumption of a massive photon ; see my answer here for more details . such cases have very strong uniqueness theorems : once a solution 's values are set on a compact volume 's boundary , there is only one possible solution within the volume . this situation even extends to semi-infinite volumes . however the former situation includes $k^2&gt ; 0$ , the case for scalar diffraction in freespace or a lossless dielectric : uniqueness theorems need further strong assumptions about the field to make them work . thankfully , some of these assumptions are reasonable physically . we can restore simplicity to the solutions of the freespace helmholtz equation ( i.e. to the situation we have with a hologram ) by making reasonable physical assumptions such as the sommerfeld radiation condition or that the field is a tempered distribution ; for more information on the latter condition , see my answers here or here . given these assumptions , together with the assumption that the field is propagating purely left-to-right , we can reconstruct a field from the hologram as follows . you begin with the helmholtz equation in a homogeneous medium $ ( \nabla^2 + k^2 ) \psi = 0$ . if the field comprises only plane waves in the positive $z$ direction then we can represent the diffraction of any scalar field on any transverse ( of the form $z=c$ ) plane by : $$\begin{array}{lcl}\psi ( x , y , z ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \left [ \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) \ , \psi ( k_x , k_y ) \right ] {\rm d} k_x {\rm d} k_y\\ \psi ( k_x , k_y ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \exp\left ( -i \left ( k_x u + k_y v\right ) \right ) \ , \psi ( x , y , 0 ) \ , {\rm d} u\ , {\rm d} v\end{array}$$ to understand this , let 's put carefully into words the algorithmic steps encoded in these two equations : take the fourier transform of the scalar field over a transverse plane to express it as a superposition of scalar plane waves $\psi_{k_x , k_y} ( x , y , 0 ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) $ with superposition weights $\psi ( k_x , k_y ) $ ; note that plane waves propagating in the $+z$ direction fulfilling the helmholtz equation vary as $\psi_{k_x , k_y} ( x , y , z ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) $ ; propagate each such plane wave from the $z=0$ plane to the general $z$ plane using the plane wave solution noted in step 2 ; inverse fourier transform the propagated waves to reassemble the field at the general $z$ plane . if you can understand these steps you should be other see how the solution to helmholtz 's equation , i.e. the full three-dimensional scalar light field , is reconstructed from its values on a plane . the latter of course is what a phase and intensity mask hologram encodes . what hinders holography ? i am not up with the latest hologram production techniques , but essentially making a hologram is a kind of interferometry and as such calls for low vibration and building of an interferogram between transmitted and reference light . one can not simply " snap " a hologram like one can with a digital camera ( or even with an older style film camera ) . moreover , the phase masking needed to make the equations above work is highly colour-dependent , so that any kind of colour holography is even more restrictive than the making of one-colour holograms . the holography wiki page gives you a good overview ; the " rainbow " holographic technique is the nearest i know of to colour holography . aside from this technique , most holograms need high coherence in the light source for reconstruction . another interesting technique is the manipulation of light by computer generated holography , where one computes by solving maxwell 's equation the phase and amplitude mask needed for e.g. nulling out the mean aberration from a lens before analysis by an interferometer .
standard deviation adds uncertainties to the measured value : $23.3\pm 0.4\ , {\rm m}$ . one can quickly look at the error ( which has units of ${\rm m}$ in my case ) and think , the value could be as low as $22.9\ , {\rm m}$ or as high as $23.7\ , {\rm m}$ without much thinking . modifying this to being a percentage of the value would be confusing . plus it would be arbitrary . as martin beckett points out , since the celsius and kelvin scales have identical temperature spacing but differing 0-points , making the error ( std . dev . ) scale to the value would be ridiculous : $$20.0\pm0.1\ , {\rm c} \to 20.0\ , {\rm c}\pm0.5\%$$ $$293.0\pm0.1\ , {\rm k}\to293.0\ , {\rm k}\pm0.034\%$$ same value in units systems would lead to differing uncertainties ; this would bring about more confusion .
if want to describe the dynamics of the ball , you need to use the so ( 3 ) matrix which describes the ball 's orientation . this is a 3 by 3 matrix whose transpose is its inverse . these may be parametrized by euler angles , and most of the literature on rigid rotating bodies uses this convention , but i think it is best just to use the matrix entries themselves because euler angles are hard to work with . the orientation matrix itself has a velocity vector which is described by an antisymmetric matrix . the center of mass motion of the ball , ignoring air , will be a straight parabolic arc in the direction of the original motion , and the spin has no effect . the ball will not go off center just by spinning it , unless it collides with some part of the machine going out . there is asymmetric air drag on the ball when it is rotating , which will lead the ball to arc its trajectory a little bit , this is a small effect . you should not use spin to do this , because the main effect of the spin will be on the bounce when landing , which is going to be impossibly wild given the enormous spin you intend to put on the ball . human servers can not put too much spin on the ball , because they are limited by the racket design . you are better off just aiming the ball in a different direction for different serves , and if you want to disguise the direction , do it with a reflector plate which the ball bounces off . you should match to experiment , not theory , because it will be easier to fit the experimental data with a curve than to predict it from mechanics . given your current spin design , you will make inhuman and unanswerable serves . reflection of spinning tennis balls since you keep asking for this , i worked it out . it is somewhat interesting , because there are several issues which one has to keep straight . most of the answer is determined independent of tennis-ball details , but one quantity , the change in rotation in the plane of reflection , is impossible to predict well . consider a tennis ball impacting the z=0 plane , travelling freely with velocity $v_x , v_z$ , and with rotation vector $\omega_x$ , $\omega_y$ , $\omega_z$ . this is the general case , since you choose the z axis perpendicular to the wall , and the x axis parallel to the velocity in the plane of the wall . at the moment of impact , the friction force of the ball will quickly and irreversibly enforce the no-slip condition , before any significant deformation of the ball . the reason is that the total impulse ( momentum transfer ) from the impact is about $2mv_x$ , so that the available friction impulse is of the order of $2\mu mv_x$ , where $\mu$ is the coefficient of friction , which is of order 1 , and this is much bigger than the impulse required to enforce no slip or a reasonable range of $\omega$ , say 10-500 . no-slip-on-contact means that the friction impulse imparted to the ball in the x-y plane $p_x , p_y$ must satisfy the following : $$ v_x + {p_x\over m} = r\omega_y - {\alpha p_x\over m}$$ $$ {p_y\over m} = r\omega_x - {\alpha p_y\over m}$$ these conditions enforce that the velocity and the angular velocity after the impulse are those required for no-slip . this gives the impulse . the constant $\alpha$ is the coefficient of the moment of inertia , $$ i = {mr^2\over \alpha}$$ for a shell , like a tennis ball , $\alpha=3$ . these conditions determine the outgoing velocity in the x , y directions and the outgoing angular velocity in the x , y directions . $$\delta v_x = {r\omega_y - v_x\over 1+\alpha}$$ $$\delta v_y = {r\omega_x \over 1+\alpha}$$ the z direction is executing a reflection independent of the x and y , because once no-slip is established , the elastic process happens as it would for a non-rotating ball . the final z velocity is $\kappa v_z$ in the opposite direction , so that $$\delta v_z = - ( 1+\kappa ) v_z$$ where $\kappa$ is a phenomenological bounce-loss parameter , for a tennis ball , i would guess about . 8 ( from bouncing tennis balls , they go back to about 64% of their original height each bounce ) . the final values of $\omega_x$ and $\omega_y$ are determined by no-slip $$\omega_y^f = {v_x\over r}$$ $$\omega_x^f = {v_y\over r}$$ the undetermined quantity is the final value of $\omega_z$ , the rotation in the plane of the impact wall . this rotation is reduced by the friction force as the ball elastically deforms , and bounces off . i will write this as $$\omega_z^f = q ( v_z , \mu ) \omega_z$$ where $q ( v_z , \mu ) $ is a phenomenological function which you need to parametrize . the friction torque is reduced by the impact area from the friction force , and this is a factor of maybe 1% for a ball at 60 m/s , but the total friction impact available is $2\mu mv_z$ , which is about 100 times the amount needed to stop a reasonable rotation . so these are comparable , and it is not easy to predict how the impact will affect this component of rotation , except that it will reduce it , perhaps by as little as 1% , perhaps as much as 90% per bounce . this needs to be measured for a real tennis ball at real impact speeds .
classical lagrangians of fermions are always constructed out of grassmann numbers . no exception . in both of op 's cases , the mass term is nonvanishing : in the first case the mass term is proportional to $\psi_1^*\psi_2^*-\psi_2^*\psi_1^* = 2\psi_1^*\psi_2^* \neq 0 . $ in the second case , i write in the chiral basis : $$\gamma^0=\begin{pmatrix} and \sigma^0 \\ \sigma^0 \end{pmatrix}\qquad \gamma^0=\begin{pmatrix} and \sigma^2 \\ -\sigma^2 \end{pmatrix} \enspace \rightarrow \hat{c}=\begin{pmatrix}-i\sigma^0\sigma^2 \\ and i\sigma^0\sigma^2\end{pmatrix}$$ so that ( picking the diagonal element of the mass matrix $i=j$ as the off-diagonal terms are obviously non-vanishing . ) $$ \begin{aligned}\psi_r^t \hat{c} \psi_r and = \begin{pmatrix}0 and 0 and \psi_3 and \psi_4\end{pmatrix} \begin{pmatrix}-i\sigma^0\sigma^2 \\ and i\sigma^0\sigma^2\end{pmatrix} \begin{pmatrix}0\\0\\\psi_3\\\psi_4\end{pmatrix} \\ and = \begin{pmatrix}\psi_3 and \psi_4 \end{pmatrix}i\sigma^0\sigma^2\begin{pmatrix}\psi_3\\\psi_4\end{pmatrix}\\ and =\psi_3\psi_4-\psi_4\psi_3 = 2\ , \psi_3\psi_4 \neq 0 \end{aligned}$$ where in the last step i used the grassmann property of anticommutation .
the following is a passage from diffusion in the standard map ( pdf ) by itzhack dana and shmuel fishman : a major difficulty in the analysis of chaotic behavior of hamiltonian systems is the proximity of chaotic and regular orbits on various scales [ 2 ] . thus , for $k \approx 1$ , the phase plane of ( 1 ) is an intricate mixture of regular and chaotic orbits [ 3 ] . for $k \gg 1$ chaotic orbits fill almost the entire phase plane , but " islets of stability " , in particular accelerator modes [ 2 , 7 ] , are known to exist for $k$ arbitrarily large . when a chaotic orbit approaches such an islet , it may wander near it in a " regular " fashion . the area of each islet generally decreases when $k$ increases . distinctive features of a stochastic or random motion , which allow a statistical description of it , are the rapid decay of correlations and diffusion . the decay of correlations in the chaotic region is related to the local instability , measure lyapunov characteristic exponent [ 1 , 2 , 8 ] . for $k \gg 1$ , the decay of correlations is actually exponential , with the decay exponent proportional to the lyapunov exponent [ 8 ] . the existence of a definite characteristic time for the decay of correlations and the resulting statistical independence generally imply diffusion in the unbounded direction of the action $i$ for $k &gt ; k_\mathrm{c}$ [ . . . ] . therefore a definite diffusion coefficient $d$ can be associated with the chaotic motion $$ d = \lim_{n\to\infty} \frac{\big\langle ( i_n-i_0 ) ^2\big\rangle}{n} , $$ where the average is taken over some ensemble of initial positions $ ( i_0 , \theta_0 ) $ within the chaotic region . for large $k$ , where the lyapunov exponent is large , diffusion was verified unambiguously [ 2 ] .
i am not quite sure i understand what you mean , but if you have an amount of energy to spend : $$\delta e = e_2 - e_1 = \frac{1}{2} m_1 v_2^2 - \frac{1}{2} m_1 v_1^2 = \frac{1}{2} m_1 \left ( v_2^2 - v_1^2 \right ) $$ using this , you can calculate $v_2$ if $v_1$ is known . the force to achieve this can be calculated by $$m \delta v = \sigma f \delta t $$ i am not sure this is exactly what you want , but i hope it'll help edit : in the case of energy dissipation with work , as i am not able to sustain large forces for a large period of time , this might be a good approach to model the force delivered by a humon . perhaps you could make the force applied a function of time . if you make it asymptoticaly decreasing over time , then you can use the value $\beta$ to scale it such that the integral will be equal to $e$ . the exact distribution of this energy depends on the problem . you are converting your initial energy budget to useful energy ( work e.g. $f \cdot ds$ ) and useless energy ( the cost of delivering a force ) . the problem is that the ratio between these two is dependent on the situation . if you stumble upon a rock , and no movement occurs it is only useless energy , whereas if not , you convert all your energy to work . i think you will need to consider the instantenous state every time to see what case applies .
for 1d potentials , the sequence of bound state energy eigenvalues $e_n$ cannot grow faster than what happens in the case of an infinite well , i.e. $e_n$ cannot grow faster than $n^2$ .
short answer : this is just feynmann parametrization , so you could demonstrate the formula by recurrence .
first of all , the problem is technically difficult due to the fact that generally unbounded self-adjoint operators like those used in general qm have domain smaller that the whole hilbert space . for this reason i will consider here only bounded self-adjoint operators whose domain , as is well known , is the full hilbert space . proposition . the elements of $g$ are all the operators of this form : $$a= p^\perp b p^\perp + p \tag{1}$$ where : ( i ) $b=b^\dagger$ is every bounded self-adjoint operator ( ii ) $p$ is an orthogonal projector such that $p \geq |\psi\rangle \langle \psi|$ ( i am assuming that $|\psi\rangle$ has norm $1$ ) and $p^\perp := i -p$ . ( in other words $p$ is the orthogonal projector on a subspace including $|\psi\rangle$ and $p^\perp$ is the orthogonal projector on the orthogonal subspace to that space . ) proof . if $a= p^\perp b p^\perp + p$ then $a|\psi\rangle = |\psi\rangle$ by construction ( notice that $p^\perp |\psi\rangle=0$ ) and $a= a^\dagger$ since $b , p , p^\perp$ are self-adjoint so that $$a^\dagger= ( p^\perp ) ^\dagger b^\dagger ( p^\perp ) ^\dagger + p^\dagger = p^\perp b p^\perp + p =a\: . $$ conversely , if $a\in g$ , as it is self-adjoint , let us consider its spectral decomposition : $$a = \int_{\sigma ( a ) } \lambda dp^{ ( a ) } ( \lambda ) \: . $$ as $1 \in \sigma_p ( a ) $ , because $|\psi\rangle$ is an eigenvector with eigenvalue $1$ , the spectral measure satisfies $p^{ ( a ) } ( \{ 1\} ) \geq |\psi\rangle \langle \psi|$ and the integral can be decomposed as : $$a = \int_{\sigma ( a ) \setminus\{1\}} \lambda dp^{ ( a ) } ( \lambda ) + 1p^{ ( a ) } ( \{ 1\} ) \: . $$ where $$c := \int_{\sigma ( a ) \setminus\{1\}} \lambda dp^{ ( a ) } ( \lambda ) $$ admits $ ( p^{ ( a ) } ) ^{\perp} ( \cal h ) $ as invariant space and vanishes on $p^{ ( a ) } ( \cal h ) $ , just for the additive property of the spectral measure $p^{ ( a ) }$ . therefore , it holds , for $p:=p^{ ( a ) }$ , $$p^\perp c p^\perp =c\: . $$ defining $b:=c$ we have again , $a= p^\perp b p^\perp + p$ . qed notice that , for every $a\in g$ , there are many couples $ ( b , p ) $ associated to it via ( 1 ) . however running $b$ throughout the real space of self-adjoint operators ( 1 ) and $p$ throughout the set of orthogonal projectors on subspaces including $|\psi\rangle$ reproduces all elements of $g$ . you also see that $g$ is not a group , with the usual definition in math , with respect to the composition of operators , because if $\cal h$ has sufficiently large dimension you can find two non-commuting elements of $g$ so that their product does not belong to $g$ as it is not self-adjoint . it is not a group eferhring to the sum of operators because $g$ does not include the null operator . the problem with unbounded operators is that $bp^{\perp}$ may be undefined because the range of $p^\perp$ may not belong to the domain of $b$ . however it is possible to re-arrange a similar construction if specifying some detail .
note that it is the residue of the combination $j ( z ) \mathcal a ( z_0 , \bar{z_0} ) $ that we need : the poles come from the ope where the operators come together . now it could be that $j ( z ) \mathcal a ( z_0 , \bar{z_0} ) $ is more singular than $\frac{1}{z-z_0}$ at $z=z_0$ . only the simple poles contribute by the residue theorem . and if there is no simple pole ( maybe $\mathcal a$ is $1$ , and/or the combination is regular ) , then the answer will be zero .
for all intents and purposes , you can use an incompressible equation of state : $$ v = constant $$ that is it . no matter what pressure and temperature , you have the same volume . it is not completely true , but in relation to gasses it is true enough to make it that pressure work is negligible in liquids compared to gasses , and for liquids , you can just deal with the heat content without considering any work done in the expansions and contractions required to change temperature .
you are right about exact results , these depend on your definition of " exact " . the best definition of an exact is if you have a fast algorithm to calculate the result in a reasonable time . the faster the algorithm , the more exact the result . for helium atoms , the answer is yes--- you can use the variational method to produce a result to as good a precision as you like , in short time . you can even do the first approximations with pencil and paper . except for the pencil and paper , this is also true for li , be , b , c , maybe as high as ne . but for atoms like iron , you get another problem which is that the space of all wavefunctions is growing exponentially . this means that describing the energy levels of the atom with all its interparticle entanglements in some highly excited state , is very very difficult . it cannot be solved in general , because solving the general quantum system includes quantum computation , which can solve some things , like factoring integers , which are very unlikely to have an efficient classical algorithm . but for calculating the ground state wavefunction of fermions , and levels near the ground state , you have two methods that are practical : hartree fock--- this assumes the electrons move in a potential which is the nuclear potential , and a central potential due to the other electrons . this is a self-consistent method , and is good enough for practical atomic physics . density functional theory--- here you try to reparametrize the ground state wavefunction in terms of a three dimensional function which represents the electron density , and use tricks to figure out the details of the electronic entanglements inasmuch as these are necessary . both these methods are best for ground states and near-ground state properties . in general , the resources you need grow exponentially in the number of electrons , but these methods are practical for nearly all atoms and molecules you will ever encounter , and they can be refined by higher order approximations ( although this is more practical for dft ) . the bosonic ground state problem is very simple in comparison to the electron problem you are asking about , and requires only monte-carlo on the imaginary time version . this is how quantum fields are simulated .
there is no fallacy , you are just not being particularly careful . you need to include both the electric and magnetic forces of the right magnitude and a covariant result drops out . ( of course historically it went the other way around : people noticed that frame changes were messed up unless the transformation laws were different and this led to the development of special relativity . ) for simplicity let both beams be very thin and have equal uniform charge density $\rho$ in the rest frame and suppose they run exactly parallel separated by a distance $l$ . let the velocity of the beams be $v$ in the lab frame . rest frame : taking the usual gaussian pillbox gives the electric field of one beam at the location of the other as $$ \vec{e}_\text{rest} = \frac{\rho}{2\pi\epsilon_0 l} \hat{r} , $$ where $\hat{r}$ is the unit vector directed away from the source beam . thus the force on a single particle in the second beam ( charge $q$ ) is : $$ \vec{f}_\text{rest} = \frac{\mathrm{d}p}{\mathrm{d}t_\text{rest}} = \frac{\rho q}{2\pi\epsilon_0 l} \hat{r} . $$ lab frame : the charge density of the beam is enhanced by the relativistic $\gamma=1/\sqrt{1-v^2/c^2}$ factor . thus the electric field is : $$ \vec{e}_\text{lab} = \frac{\gamma \rho}{2\pi\epsilon_0 l} \hat{r} . $$ there is also a magnetic field of magnitude $$ b = \frac{\mu_0 \gamma\rho v}{2\pi l} $$ and directed so as to produce an attractive force . plugging these in the lorentz force formula $$ \vec{f}_\text{lab} = q ( \vec{e}_\text{lab} + \vec{v}\times\vec{b} ) = q\left ( \frac{\gamma \rho}{2\pi\epsilon_0 l} - \frac{\mu_0 \gamma\rho v^2}{2\pi l}\right ) \hat{r} = \frac{\rho q}{2\pi\epsilon_0 l}\gamma\left ( 1 - \epsilon_0\mu_0 v^2\right ) \hat{r} . $$ using $\epsilon_0 \mu_0 = c^{-2}$ this reduces to $\vec{f}_\text{lab} = \gamma^{-1} \vec{f}_\text{rest}$ which , on noting the relativistic time dilation $\mathrm{d}t_\text{lab} = \gamma \mathrm{d}t_\text{rest}$ , is exactly right ! note that i have used the fact that the force is orthogonal to the velocity implicity when writing the lorentz transformation law for the force . you can prove the covariance for general motions using the covariant formulation of em . lesson : relativity and electromagnetism go together like hand and glove !
in short , no , at least not at this point . unfortunately i was not able to read the entire article , but what i understood from the abstract is that it was only meant for very small volumes and masses , but if they did manage to levitate a cup of coffee , that is a step in the right direction . but there is another problem with this method - it uses high-intensity sound waves , although i am not sure at exactly what intensity , so it would likely be uncomfortable or possibly even dangerous to levitate a living being in this manner . also , i imagine that the device that would be able to levitate the mass of a human with sound waves would be relatively bulky , and there is the factor of the protective equipment that would likely be necessary . but i do believe it is possible , as just about anything is possible . i just do not think that it is the best way to levitate a human . other non-living materials , sure , it could be a great thing . but i think there will be better ways to do this with new discoveries . as for if you can levitate soon , who knows . there could be a discovery tomorrow that would allow you to levitate , or it could be years . update : i found this bit of information on wikipedia : there is no known theoretical limit to what acoustic levitation can lift given enough vibratory sound , but in practice current technology limits the amount that can be lifted by this force to at most a few kilograms . this was in the article on acoustic levitation that rody posted .
only insofar that the maximum writing speed is limited by the need to unambiguously encode the disk 's surface . if you tried to raise the speed of writing , the bit error rate increases : theoretically as a continuous , monotonic function of writing speed but practically a point is reached where the ber increases abruptly , so the ber as a function of speed tends to look like a step function . the writing data corresponds physically to changing the optical properties of the disk so this takes a nonzero time , and the more time one spends doing it , the more well formed the symbols are . as the ber increases , the error correction procedures , namely the computing of the nearest valid codeword to a corrupted one in coding space and , more importantly , repeat reads if error correction fails , use up more time , so the read gets markedly slower for a disk with a few errors , then fails altogether . this phenomenon is very like the practical example where it gets harder and harder to read someone 's hadnwriting as the writer gets more and more hurried and thus shapes their letters more and more ambiguously .
first of the sum of the forces on the bar have to be zero because the center of gravity does not move ( assuming symmetry and such ) . the only thing you can calculate is the torque requited to maintain the constant speed . i can give an example with a rectangular bar . take the bar and split it into infinitesimal slices . each slice has face area of ${\rm d}a = h\ , {\rm d}r$ where $r$ is the distance from the center and varies from $-l/2$ to $l/2$ , and $h$ is the height of the bar . the liquid-solid force for each slice is ${\rm d}f =\frac{1}{2} \rho\ ; c_d v^2 {\rm d}a$ where $\rho$ is fluid density , $c_d$ is the coefficient of drag and $v$ is the velocity of the bar slice , which is equal to $ v = \omega\ ; r $ . note that the force has to flip signs when $r$ flips signs and thus we have to add a ${\rm sign} ( r ) $ term and integrate over $r$ to get the total force $$ f = \int_{-l/2}^{l/2}\ ; {\rm sign} ( r ) \frac{1}{2}\rho\ ; h\ ; c_d\ ; ( \omega\ , r ) ^2\ ; {\rm d}r = 0 $$ and to total torque $$ m = \int_{-l/2}^{l/2}\ ; {r\ ; \rm sign} ( r ) \frac{1}{2}\rho\ ; h\ ; c_d\ ; ( \omega\ , r ) ^2\ ; {\rm d}r = \frac{\rho c_d h \omega^2 l^4}{64} $$ the main assumption here is that the $c_d$ does not depend on the flow velocity $v=\omega\ , r$ which it does . a more extensive analysis would require finding the reynolds number and the fanning friction factor and deriving the pressure distribution along the bar based on the flow characteristics .
as your calculations show , yes , it does . the reason for this is that the work performed by a force $f$ on an object is proportional to the displacement through which it is applied , $$w=f \delta x . $$ if an object is going faster , then for an given time interval $\delta t$ ( and thus a given impulse $i=f\delta t$ ) the displacement $\delta x=v\delta t$ is bigger , and you must perform more work .
as with many discussions about string theory , it is sometimes good to recall some reality : it was over 50 years ago that the higgs mechanism was proposed . compared to fully-fledged theories such as string theory , the higgs mechanism is a tiny add-on to the observed standard model ( as it was then ) . it took 50 years for experiment to get to the point of seeing it , and in fact so far just a first glimpse of it . for 50 years , the higgs mechanism was speculation not confirmed by experiment . it had all theoretical backing behind it , theory all pointed to it being true , but it could not be checked experimentally for 50 long years . for 50 years , you were free to make tv documentaries about particle physics without mentioning the higgs mechanism , if you thought it was too outlandish a proposal to have a chance of being confirmed . then finally experiment reached its energy scale and there it was . 50 years later . as you all know , there are scenarios thinkable where more beyond-the-standard-model-physics is right around the corner , but nothing to rule out that it takes another 50 years to see the next piece of " new physics " . that is just a fact of our short life . but that is not necessarily as bad as it may sound . while " new physics " may remain specuative for a long time to come , here is a well-kept secret to take note of : even old physics is not fully understood yet . and string theory can help here , and theorists know ( though tv stations may not yet have gotten the message ) . for instance , computation of scattering amplitudes even in the known and confirmed standard model is still a challenge , if only you are ambitious enough . string theory has helped with understanding some subtle points in plain yang-mills perturbation theory . see the links at string theory applied elsewhere -- qcd scattering amplitudes . in particular check out the remarkable story linked to there , told by matthew strassler in his post from string theory to the large hadron collider , which is about how string theory insights into qcd scattering amplitudes helped raise the precision of loop computations to the level that it was possible in the first place to separate signal from background in the lhc . he cites people who were involved as saying that without these string theory insights the higgs might have been produced , but not identified at the lhc . have a look , it is an interesting story . another thing may be worthwhile to remember from time to time : while we are all fond of proclaiming that we understand fundamental particle physics via quantum yang-mills theory , fact is that quantum yang-mills theory is still an open theoretical problem . we know that we do not understand some very fundamental facts about qauntum yang-mills . it is a " millenium problem " yang-mills existence and the mass gap . now , one thing that string theory has become after its " second revolution " is something like a map of the space of yang-mills like-field theories and various " dual " theories . via d-brane physics , kk-reduction , ads/cft , etc . yang-mills like theories appear in various guises in various corners of string theory , and their embedding into string theory geometrically explains subtle equivalences between these , such as electric/magnetic duality , etc . if you have not seen it before , check out at http://ncatlab.org/nlab/show/gauge+theory+from+ads-cft+--+table at least part of this string-theoretic " map " of the space of quantum field theories related to yang-mills theory . while this has not solved the mass gap problem yet , clearly , one may start to feel that the deeper nature of yang-mills theory is slowly but surely being probed here . the punchline here is the following : besides being a framework for models of quantum gravity and gauge unification , string theory is a piece of theoretical physics that sheds light on the nature of quantum field theory as such . while experimentalists and public media are busy with indulging in the higgs physics now that they waited for for half a century , maybe theoreticians can use the time before the next accelerator to step back and think a bit more about the still open more fundamental issues of quantum physics . that is where string theory has already helped , and i think will help in the future . of course you will not see this on public tv . ( generally , it is surprising these days how not only the public media but also the broad community 's attention is consistently attracted to the shallow and ignoring the deep advances that do happen in fundamental physics . for instance there is loads of excitement about , say , the firewall essay contest , but the really interesting advances , such as for instance in genuine mathematical characterization of string theory vacua here remains a topic among a tiny group of specialists . at the same time everybody has an opinion about the " landscape " , and everbody else has the opposite opinion . what is needed instead is more decent theoretical work on the foundations of quantum field theory and , inevitably then , string theory . )
there is a simple reason why we can consider variations on the whole $a$ rather than the quotient $c=a/g$ and the reason is following : all configurations that are $g$-equivalent have the same value of the action $s$ . that is what we mean by the statement that the theory has the symmetry $g$ . so the variation of the action $s$ in the directions that are equivalent to the action of a gauge transformation in $g$ vanish automatically , by the gauge invariance of the action ! the variation of the action $\delta s$ is therefore a combination of the variations $\delta a_\mu$ only of those kinds that are independent of the directions along $g$ . that is also why the equations of motion that we derive from $\delta s=0$ do not determine the evolution of the fields such as $a_\mu$ unambiguously out of the initial conditions : the equations of motion only constrain $f_{\mu\nu}$ and they always allow us to change $a_\mu$ in the future , by a gauge transformation . this ambiguity arises from the " flat directions of the action " . you could be studying $\delta s =0$ on the quotient $c=a/g$ only but it would be cumbersome and $\delta s$ would be physically and literally the same thing as it is on $a$ . it is the very point of introducing degrees of freedom which include one redundant one ( because of the gauge symmetry ) to simplify the picture . in fact , the equations of motion from $\delta s= 0$ on $a$ are manifestly lorentz-covariant etc . if you were trying to parameterize the space $c=a/g$ by some fields , you would probably have to impose some lorentz-breaking or otherwise unnatural conditions , e.g. $a_0=0$ , and the whole formalism would lose the manifest lorentz symmetry even though the actual phenomena , when looked at properly , would still obey the laws of relativity . so yes , the methods on $a$ and on $c=a/g$ are equivalent , and it is the calculus on $a$ that is the smarter one . if the formalism using $c=a/g$ were more convenient from all points of view , we would never talk about the gauge symmetry because it would be a totally counterproductive concept ! be sure that it is a very useful concept . i can not make sense out of the second part of the question . when we discuss infinitesimal variations of quantities such as energy , they should be linear combinations of the infinitesimal variations of the fields , like $du=\vec e\cdot d\vec d$ . but your expression is " doubly infinitesimal " , it is bilinear in $\delta x$ where $x$ is something , and terms this small can be neglected in the usual infinitesimal calculus in which $\delta a$ is sent to zero because they are of higher order . the energy ( and stress-energy tensor ) is gauge-invariant in electrodynamics , too , so its ( linear , first-order ) variation induced by gauge transformations is equal to zero . talking about some second-order " variations " seems completely misguided to me . also , less seriously , i feel uneasy about your usage of the word " orthogonal " . in general , one does not have an inner product ( needed to determine orthogonality ) on the full configuration space and it is really not needed for most questions of this sort . the directions in $a$ away from a slice that may be used as representatives of $c=a/g$ come in two types : those that are pure gauge transformations , in the direction of the $g$ " fibers " , and those that are not . most of them " are not " but any combination of those that are and those that are not " is not " again and no combination is really " fundamentally different " than others . so it is a bit meaningless to look for " orthogonal " directions to the directions along $g$ . finally , you ask whether " a " gauss law is related to the standard gauss law taught at school but you have not really explained what you mean by " a " gauss law . there is only one gauss ' law . it is the equation of motion obtained by varying the action with respect to $a_0$ , and it gives us something like ${\rm div}\ , \vec d = \rho$ . it is always the same law – which may be generalized to more complicated theories than electrodynamics , e.g. yang-mills theory . this equation ${\rm div}\ , \vec d =\rho$ is interesting because it does not contain any time derivatives . so it really does not dictate the time evolution of anything : it already constrains the initial state . one may prove that if the equation holds at $t=0$ , it will hold at any time : the time-derivative of the gauss ' law may be derived as a spatial derivative of other maxwell 's equations involving $\vec d$ . this non-dynamical = constraint character of the gauss ' law ( the fact it does not contain time derivative ) is related to the fact that this equation of motion is derived from the variation of a field that may be interpreted as a completely redundant one in a particular convention how to fix the gauge symmetry . that is why we can identify it with the statement that the states related by gauge transformations are treated as equivalent states by the theory . this is particularly clear in the quantum theory where we may a priori have states not annihilated by ${\rm div}\ , \vec d - \rho$ but only states that are annihilated by this operator , i.e. states that respect the equivalence of the states related by gauge transformations , may be considered physical .
how can i see that infinitesimal rotations commute see the wolfram mathworld article on infinitesimal rotation update : i have recently been made aware that it is bad form to simply " answer " with a link sans summary so i will succinctly summarize the contents of the link . essentially , the infinitesimal rotation matrix is the identity plus an infinitesimal matrix . in the product of two of these , the non-commutative part evaporates as the components are products of infinitesimals . only the sum of the identity ( squared ) and the two infinitesimal matrices remain regardless of the order in the product .
there is actually an extremely nice way to uncover the bloch sphere representation for any density operator . ( pure states are just a special case . ) definition . i am not sure how your ( lecturer ? book ? other learning source ? ) defines the bloch sphere , but the definition that makes the most sense from a fundamental perspective is that , for any density operator &rho ; , the point on ( or inside ) the bloch sphere corresponding to &rho ; is the vector ( r x , &nbsp ; r y , &nbsp ; r z ) such that $$ \rho = \tfrac{1}{2} ( i + r_x x + r_y y + r_z z ) $$ where $i$ is the identity and $x , y , z$ are the ( other ) 2&times ; 2 pauli operators . proof sketch . it is easy to show that the operators $i , x , y , z$ are linearly independent ( what linear combinations of them add to the zero operator ? ) and are hermitian ( each is equal to it is own conjugate-transpose ) . from this you can show that they span the set of all 2&times ; 2 hermitian operators ; and as they are linearly independent , they are actually a basis set for those operators . so any density operator &mdash ; which is also hermitian &mdash ; will decompose into $i , x , y , z$ in a unique way . ( it is possible to show that it is coefficient in $i$ is always &half ; by considering the trace . do you see how ? ) answer . you should try to prove the things i have said above &mdash ; it is not hard , and it is using math that will be useful to you later anyway &mdash ; but for the problem of finding the bloch sphere representation , all you need to do is solve for ( r x , &nbsp ; r y , &nbsp ; r z ) in the equation above . if you like , you can even obtain these coefficients by a simple formula . ( hint : what is the trace of the product of two different matrices chosen from $i , x , y , z$ ? what does this mean for $\mathrm{tr} ( \rho p ) $ for $p \in \{i , x , y , z\}$ ? ) another remark &mdash ; in the future , you do not have to really do any work to find the eigenvalues of a diagonal matrix $d$ . it is easy to show that he standard basis vectors $\mathbf e_j = [ \ ; 0 \ ; \cdots \ ; 0 \ ; \ ; 1 \ ; \ ; 0 \ ; \cdots \ ; 0 \ ; ] ^\top$ are eigenvectors for any diagonal matrix , and that the eigenvalues are exactly the coefficients on the diagonal ( with multiplicity given by how often each is repeated ) . it is also easy to show that $d - \lambda i$ is invertible for any other $\lambda$ , so that these are all the eigenvalues .
the density of states is always uniform in k , because in a box of length l , you have a uniform lattice in k space , and you imagine making the box bigger and bigger . so given $e\propto k^\alpha$ , then you ask how much k volume there is in a band of size de around e . the answer is always proportional to $|k|^{d-1}$ in $d$ dimensions , just from the area of a sphere of radius $|k|$ , times the width of the spherical annulus , $d|k|$ , which is $$d|k| = {de\over ( de/dk ) } \propto {de\over k^{\alpha-1}} \propto {de\over e^{\alpha-1\over\alpha}}$$ so the density of states is $$ \rho ( e ) = e^{ ( d-\alpha ) \over\alpha}$$ which for d=3 and $\alpha=2$ , gives $\rho ( e ) \propto \sqrt e$ .
when physicists say that a particle has electric charge , they mean that it is either a source or sink for electric fields , and that such a particle experiences a force when an electric field is applied to them . in a sense , a single pair of charged particles are a battery , if you arrange them correctly and can figure out how to get them to do useful work for you . it is the tendency for charged particles to move in an electric field that lets us extract work from them . a typical electronic device uses moving electrons to generate magnetic fields ( moving electrons cause currents , and currents generate magnetic fields ) and these magnetic fields can move magnets , causing a motor to turn . what is happening at a fundamental level is that an electric field is being applied ( via the potential across the battery ) that is causing those electrons to move . if i wanted a magnetic field to be generated , i could get one from a single pair of charges , say , two protons placed next to one another . the protons will repel ( like charges repel ) and fly away from each other . these moving protons create a current ( moving charge ) which creates a magnetic field . your author is right when he says that charges attract or repel other charges . to help connect it to more familiar concepts , consider this : the negative end of your battery terminal attracts electrons and the positive end repels them . ( the signs of battery terminals are actually opposite the conventional usage of positive and negative when referring to elementary charges . as a physicist , i blame electrical engineers . ) the repelled and attracted electrons start moving , and these moving electrons can be used to do work .
special relativity makes the existence of magnetic fields an inevitable consequence of the existence of electric fields . in the inertial system b moving relatively to the inertial system a , purely electric fields from a will look like a combination of electric and magnetic fields in b . according to relativity , both frames are equally fit to describe the phenomena and obey the same laws . so special relativity removes the independence of the concepts ( independence of assumptions about the existence ) of electricity and magnetism . if one of the two fields exists , the other field exists , too . they may be unified into an antisymmetric tensor , $f_{\mu\nu}$ . however , what special relativity does not do is question the independence of values of the electric fields and magnetic fields . at each point of spacetime , there are 3 independent components of the electric field $\vec e$ and three independent components of the magnetic field $\vec b$: six independent components in total . that is true for relativistic electrodynamics much like the " pre-relativistic electrodynamics " because it is really the same theory ! magnets are different objects than electrically charged objects . it was true before relativity and it is true with relativity , too . it may be useful to notice that the situation of the electric and magnetic fields ( and phenomena ) is pretty much symmetrical . special relativity does not really urge us to consider magnetic fields to be " less fundamental " . quite on the contrary , its lorentz symmetry means that the electric and magnetic fields ( and phenomena ) are equally fundamental . that does not mean that we can not consider various formalisms and approximations that view magnetic fields – or all electromagnetic fields – as derived concepts , e.g. mere consequences of the motion of charged objects in spacetime . but such formalisms are not forced upon us by relativity .
given that leftaroundabout and vonjd have addressed the fundamental place of the fourier transform in the formalism , let me talk a little about an experimental application . what is the shape and size of a atomic nucleus ? from rutherford we learned that the nucleus is rather a lot smaller than the atom as a whole . now , electron microscopy can just about provide vague picture of a medium or large atom as a out-of-focus ball , but there is no hope of employing that technique to something orders of magnitude smaller . what we do is scatter things off of the component parts of the nucleus . a nice reaction here is $$ e + a \to e + p + b $$ where $a$ represents that target nucleus and $b$ the remnant after we bounce a proton out . ( this is what nuclear physicists call " quasi-elastic scattering " . ) now , if ( 1 ) we are shooting a beam of electrons at a stationary target , ( 2 ) we have a precision measurement of the momenta of the incident and scattered electrons and the ejected proton , ( 3 ) we are willing to neglect excitation energy of the remnant nucleus , and ( 4 ) we assume that $p$ mostly did not interact with the remnant after being scattered , we know the momentum of the proton inside the nucleus at the time it was struck . collect enough statistics on this and we have sampled the proton momentum distribution of the nucleus . now , here 's the fun part : you can show that the spacial distribution of protons in the nucleus is the fourier transform of the momentum distribution . and bingo , a measurement of the size of the nucleus . do it with a polarized target and you can get info on the shape as well .
background ( skip this if you know it all ) ! i too worried about this when i first learned it . basically i think it is easiest to think of the eightfold way quantum mechanically first and worry about qft later . so that is what i will do in this answer . in quantum mechanics ( at least according to wigner ) a particle is a basis vector in some representation of the full symmetry group of the theory ( poincare $\times$ internal ) . fundamental particles are defined to be in the ( anti ) fundamental representation of the internal symmetry group . in the eightfold way we hypothesize that the relevant hamiltonian for our qm theory has an $su ( 3 ) $ symmetry and look at the consequences . we also restrict our attention to just spin 1/2 fermions . this means that by definition there are three fundamental particles ( the up , down and strange quarks ) together with three fundamental antiparticles . now we know from basic qm that multiparticle states are constructed from tensor products of single particle states . a useful mathematical way of enumerating the possible particles is to find all tensor products of the fundamental and antifundamental representations . these decompose into irreducible representations allowing you to easily count the number of degrees of freedom and their properties . how do i decompose a tensor product into a sum of irreps the general procedure is known as clebsch-gordan decomposition . it is completely analogous to the process you go through when adding angular momenta in qm . you can even compute coefficients which tell you exactly how any given tensor product state decomposes for a general symmetry group $su ( n ) $ , see here . of course , in reality this complexity is often not necessary to determine the particle content of the theory . instead you can do the following . to determine the irrep decomposition of $m\otimes n$ plot the weight diagrams of $m$ and $n$ plot the weight diagram of $m\otimes n$ which is obtained by ( vector ) adding the weights in the first two diagrams in all possible ways . check : you should get $mn$ weights find the " highest " weight ( usually the one with largest distance from the origin ) and identify which irrep it belongs to . this involves calculating the higher irreps , or looking up their weight diagrams . note down this irrep . remove all other weights on the diagram which correspond to the irrep for the highest weight you have found . repeat steps 3 and 4 until there are no weights left . the reason this works is fairly transparent - on each iteration of the algorithm you are just identifying an invariant subspace . remembering that irreps are labelled by their highest weights completes the argument . if you want any more detail i recommend you read jan gutowski 's notes particularly section 4.3 . p.s. just read your profile - hope you are having a good start at imperial ! i will be a phd student at queen mary from january , so maybe i will see you around at a london triangle meeting .
first , $u$ is surely not " any non-singular matrix " . for a given basis , $u$ is almost completely determined i.e. unique . it contains $\gamma_2$ because it is derived from the only imaginary pauli matrix . because of the basic dirac algebra $$ \{ \gamma_\mu , \gamma_\nu \} = 2\cdot 1_{2\times 2} \cdot g_{\mu\nu} $$ one may see that $\gamma_0$ is hermitian , $\gamma_0=\gamma_0^\dagger$ , while the spatial ones are anti-hermitian , $\gamma_i=-\gamma_i^\dagger$ . in your identity , you want to relate $\gamma^\mu$ to its transposition $\gamma^{\mu t}$ . up to the sign that depends on the spatial or temporal character of $\mu$ , the transposition is the same thing as complex conjugation . so a related problem is whether the complex conjugate matrices $\gamma^{\mu*}$ can be related to $\gamma^\mu$ by something like a conjugation . and the answer is yes . the main fact behind the exercise is that $\sigma^2$ is the only imaginary pauli matrix , so complex conjugation of pauli matrices is equivalent to the conjugation by $\sigma^2$ with an extra sign . this may be easily generalized if you also include the temporal 0th component and if you use the normal basis . you should check the identity you want to verify in a particular convenient basis , i.e. with an explicit form of the gamma matrices . the verification is most convenient if you write the gamma matrices in block form , with $2\times 2$ blocks being either multiples of pauli matrices or the unit matrix . in a more general representation , the dirac gamma matrices differ from those in the particular basis you will have verified by a conjugation only , and this may only mean that $u$ is changed in the formula , but the essence of the conjugation is unchanged . these equations are important because $c$ is related to the charge conjugation – the replacement of particles by antiparticles ( e . g . exchange of electrons and positrons ) . mathematically , the most important part of the charge conjugation is complex conjugation which is why we needed to express the " complex conjugate gamma matrices as some conjugations of the normal ones " . theories with a symmetry between matter and antimatter are symmetric under c - the charge conjugation symmetry . spinors are mapped to $\psi\to c\psi$ etc . and the only hard part of the symmetry of the lagrangian is a step that requires you to conjugate the gamma matrices by $c$ which is why it is good that we have a way to simplify $c^{-1t} \gamma^\mu c^t$ .
here tension is zero for very sort span of time ( infinitesimally sort time ) , or for an instant only . when ever it moves away from the vertically top position it will fill tension again . so it will move in circular trajectory . and if we consider instantaneous velocity it is tangential . i think this clarify your doubt .
you are correct in stating that the resultant effect on incoming rays and their focussing depend on the two lenses and the distance between them . to reduce the two lens system in general we use the principal planes method and then can describe the situation in terms of two parallel planes only . but when they say that the powers are added or the " resultant " focal length is given by $$1/f=1/f_1+1/f_2$$ is valid only in case of thin lenses very close together such that their combined thickness can be ignored . you can easily get this equation by assuming their ( lenses' ) centres two be coincident and then applying some geometry or by using the general refraction -through-curved-surfaces equation and trace your image based on the object distance . in any case , the finiteness of the thickness has to be eliminated to get this equation . if we assume two very thin lenses in contact , than this equation holds true and the " resultant " power or the focal length is just the power/focal length of the single lens that can replace the two-lens system without any change in its optical properties ( in paraxial approximation ) .
that is a pretty good way to look at it . to be more mathematically explicit , notice that the energy of an electric dipole ( see here ) with dipole moment $\mathbf p$ in an electric field $\mathbf e$ is $$ u = -\mathbf p\cdot\mathbf e = -|\mathbf p||\mathbf e|\cos\theta $$ this expression is minimized when $\cos\theta = 1$ , which is when the angle between the dipole moment vector ( which for a physical dipole points from the negative to the positive charge ) and the field is $0$ . this is precisely the condition for the dipole to be aligned with the electric field as you described . if the dipole is not aligned with the field , then it will experience a torque that tends to align it with the field . you can see why this happens in the physical dipole ; the positive charge feels a force in the direction of the field , while the negative charge feels a force in the direction opposite the field , and these both tend to rotate the dipole to align in with the field .
you do not need to apply steiner 's theorem onto the point mass . the point mass finds itself at a distance ( apparently ) $r$ of the x-axis . since the moment of inertia is an extensive value , you can simply add all moments of inertia . there is the moment of inertia of the solid disk with respect to it is diameter . you have to ' steiner ' that away from a distance $r$ . then you need to add $\frac{5}{4} mr^2$ which is the moment of inertia associated to a point-mass of mass $\frac{5}{4}m$ a distance $r$ away from the rotation axis therefore , $i_{xx} = \frac{mr^2}{4} + mr^2 + \frac{5}{4}mr^2 = \frac{10}{4}mr^2$ from the drawing , i strongly suppose the point mass finds itself at $ ( r , r ) $ , therefore lying on the diagonal . you should be able to conclude what the moment of inertia is then . the minus sign you give for the supposed answer seems highly suspiciou , as the moment of inertia is a sum of positive quantities .
always start with a nice clear diagram/sketch of the problem . it all follows from there . here is a free body diagram i made for you . then you have ( the long detailed way ) : sum of the forces on body equals mass times acceleration at the center of gravity . $\sum_i \vec{f}_i = m \vec{a}_c $ $$ a_x = m a_x \\ a_y - m g = m a_y $$ sum of torques about center of gravity equals moment of inertia times angular acceleration . $\sum_i \left ( \vec{m}_i + ( \vec{r}_i-\vec{r}_c ) \times\vec{f}_i\right ) = i_c \vec{\alpha} $ $$ a_x \frac{l}{2} \sin ( \theta ) - a_y \frac{l}{2} \cos ( \theta ) = i_c \ddot \theta $$ acceleration of point a must be zero . $\vec{a}_a = \vec{a}_c + \vec{\alpha}\times ( \vec{r}_a-\vec{r}_c ) + \vec{\omega}\times ( \vec{v}_a-\vec{v}_c ) $ $$ a_x + \frac{l}{2} \sin ( \theta ) \ddot\theta + \frac{l}{2} {\dot\theta}^2 \cos ( \theta ) =0 \\ a_y - \frac{l}{2} \cos ( \theta ) \ddot\theta + \frac{l}{2} {\dot\theta}^2 \sin ( \theta ) =0 $$ now you can solve for $a_x$ , $a_y$ from 3 . and use those in 1 . to get $a_x$ , $a_y$ . finally use 2 . to solve for $\ddot\theta$ or do the shortcut of finding the applied torque on a and applying it to the effective moment of inertia about the pivot $i_a = i_c + m \left ( \frac{l}{2}\right ) ^2 $ to get $$ \ddot\theta = \frac{m g \frac{l}{2} \cos\theta }{ i_c + m \left ( \frac{l}{2}\right ) ^2 } $$
define the operators $\hat a ( f ) =\sum f_j\hat a_j$ and $ |f_1 , . . . , f_n\rangle:=\hat a ( f_1 ) . . . \hat a ( f_n ) |vac\rangle$ . then $\langle g_1 , . . . , g_m|f_1 , . . . , f_n\rangle$ vanishes for $m\ne n$ and is a sum of the products $\langle g_1|f_{j_1}\rangle . . . \langle g_n|f_{j_n}\rangle$ for all possible permutations $ ( j_1 , . . . , j_n ) $ of $1 , . . . , n$ . note that the $\langle g|f\rangle$ are easy to compute . the formula you requested is a special case of this .
i will be substantially lazy and drop all quadratic terms . keep in mind that i am doing everything to linear order . also , i will suppose we are finding normal coordinates around the point $x^\mu = 0$ . it is trivial to modify the technique for use anywhere else . i will follow your mostly minus metric convention but use $c=1$ . we have the metric $$ \mathrm{d}\tau^{2}=g_{\mu\nu}\mathrm{d}x^{\mu}\mathrm{d}x^{\nu} , \ ( 1 ) $$ and we want coordinates $\tilde{x}^\mu ( x ) $ such that $$ \mathrm{d}\tau^{2}=\eta_{\mu\nu} \mathrm{d}\tilde{x}^{\mu}\mathrm{d}\tilde{x}^{\nu} , \ ( 2 ) $$ ( to linear order ) in a neighbourhood of zero . we write the coordinate transformation as $$ \tilde{x}^{\mu}=ax^{\mu}+\frac{1}{2}b_{\nu\rho}^{\mu}x^{\nu}x^{\rho}+\cdots , \ ( 3 ) $$ where $a$ and $b^\mu_{\nu\rho}$ are constants to be determined . the higher order terms do not influence the construction . without loss of generality we take $b^\mu_{\nu\rho}=b^\mu_{\rho\nu}$ . subbing ( 3 ) in ( 2 ) i get ( exercise ) $$ \mathrm{d}\tau^{2} = \left [ a^{2}\eta_{\rho\sigma}+a\left ( b_{\sigma\lambda\rho}+b_{\rho\lambda\sigma}\right ) x^{\lambda}+\cdots \right ] \mathrm{d}x^{\rho}\mathrm{d}x^{\sigma} . $$ matching this onto ( 1 ) order by order gives the conditions ( exercise ) $$\begin{array}{rcl} a^{2}\eta_{\rho\sigma} and = and g_{\rho\sigma}\left ( x=0\right ) , \\ a\left ( b_{\sigma\lambda\rho}+b_{\rho\lambda\sigma}\right ) and = and g_{\rho\sigma , \lambda}\left ( x=0\right ) . \end{array}$$ in your case this gives the conditions ( exercise ) $$\begin{array}{rcl} a^{2} and = and 1-2\phi^{0} , \\ ab_{t\lambda t} and = and -\phi_{ , \lambda}^{0} , \\ ab_{i\lambda i} and = and \phi_{ , \lambda}^{0} , \text{all the other}\ b^\mu_{\nu\rho}\ \text{vanish} , \end{array}$$ ( where for shorthand $\phi^0 \equiv \phi ( x=0 ) $ and $i=x , y , z$ ) which i am sure you can solve . : ) i get $$ \tilde{x}^{\mu}=\sqrt{1-2\phi^{0}}x^{\mu}-\frac{\phi_{ , \lambda}^{0}x^{\lambda}}{2\sqrt{1-2\phi^{0}}}x^{\mu}+\cdots , $$ though you should check this yourself ( cause i do not feel like it ) in case i made any index/sign mistakes . : )
the main problem with rf energy harvesting is the low power density ( &lt ; 1µw/cm^2 , unless near a transmitter ) . other approaches are generally more useful , though even picowatts can be enough for some applications . some links : overview of energy harvesting systems ( for low-power electronics ) energy scavenging/harvesting ambient rf energy harvesting
i only know one particular reason : the transition responsible for the h1 line is highly forbidden and shows an extreme lifetime ( 10^7 years ) , so the absorption rate in interstellar clouds , which can be very opaque for every other radiation , is very small . looking at the h1 line allows you to see objects which are for example hidden behind dust clouds that absorb a lot of the radiation of the object behind .
temperature is not a concept that has a lot of utility at the level of single atoms because it represents the mean kinetic energy of a group of particles ( to within a coefficient ) . you can define it , it just does not help much . at the level of two atoms you revert to a more fundamental model such as the forces between them . one atom transfers energy to another through electromagnetic forces between them . when that energy manifests as randomized kinetic energy at the microscopic scale we refer to it as " heat " at the macroscopic scale . in a solid it is usually reasonable to treat the forces between individual pairs of atoms as being spring-like ( i.e. . they obey $f_{i , j} = -k ( r_{i , j} - r_0 ) $ ) . starting from there you can build various models of solid behavior . for instance the einstein model of a crystal .
afer the discussions in the comments this would be my solution : the l.h. s in the equations of motion is always simply $m\ddot{x}$ , the r.h. s contains the sum of all acting forces . so you have gravity and the force of the spring . thats it . $m\ddot{x}=mg-k ( x-x_{0} ) $
i think you may be misguided by the concept that we associate $\textbf{observables}$ to self-adjoint operators . they do operate on the hilbert space , but to see them as entities that transform states or prepare them is a little bit tricky . i will describe here self-adjoint operators and preparation of states . 1 ) the true ( physical ) power of self-adjoint operators for describing observables lies in the spectral theorem , and not in its $\psi \mapsto a\psi$ action . physically , what it means ? there is a set called spectrum of an observable , and it is the set of possible outcomes on its measure for given states . for example , a spin observable $s$ on a 1/2-spin system has spectrum $\sigma ( s ) = \{-1/2 , +1/2\}$ , and decomposes as a sum of its spectral projections , $s = +1/2 p_{+} -1/2 p_{-}$ . in general , there is a spectral resolution $e$ , that is , a bunch of projection related to the spectrum , such that the operator can be written as $a = \int_{\sigma ( a ) }\lambda de ( \lambda ) $ . and what are the spectral projections ? those are again ( self-adjoint ) operators , but the whole collection of spectral projections will give you a probability measure when coupled with a state . in the spin system example , if you take a state $\psi$ , then $\langle\psi , p_+\psi\rangle$ would give you the probability of measuring a +1/2 spin , and likewise for -1/2 . now suppose you had a 1/2 spin system with prepared state $\psi$ , and you measure the spin , and get +1/2 . after the measurement , your state collapses to a $|+1/2\rangle$ state . in a more detailed formalism , suppose you have prepared a state $\psi$ and you are going to make a measurement of an observable expressed as $a = \int_{\sigma ( a ) } \lambda de ( \lambda ) $ ( where the $e$ is the spectral resolution of your operator , just think of the 1/2-spin example intuitively ) . then suppose your measurement is on a subset $\lambda \subset \sigma ( a ) $ ( you may think of the set $\{+1/2\} \subset \{-1/2 , +1/2\}$ . your state $\psi$ then collapses to the following state $\phi$: $\psi \rightarrow \phi = \frac{e ( \lambda ) \psi}{\|e ( \lambda ) \psi\|} . $ ( notice that $\phi$ is normalized and well defined , since $e ( \lambda ) \psi=0$ then the probability of the outcome being in $\lambda$ would be zero to start ) . summing up , you do not simply apply a self-adjoint operator on a state , since , as you have seen , it does not have much meaning . this is a point most introductory qm books do not stress as much as i would want . what happens with measurements and collapses and whatsoever uses , as i tried to point out , the spectral projections more than the operator itself . so , as you said about your hamiltonian operator , it does not act like your syrup machine , which we will try to cover up next . 2 ) now what you describe as " tools " , in your example , the putting syrup , is not a measurement per se , it is a preparation of states , which would grab a state without syrup and put syrup in it . the modeling of such procedure is usually ignored , at least to my knowledge . one choice would be just saying " my state now is syrup" , end of discussion . other option is using unitary operators ( $u$ such that $uu^* = u^*u = 1$ ) . those transform state vectors in state vectors . if you would like more sophisticated examples , it starts to get tricky , and i will shut up before i say something very wrong about it . but rest assured this is not easy at all , and your question is really nice . hope to see some other inspiring aswers .
my answer is more of a summary of insight that others have presented on a number of questions on physics . se . it is true that all astronomical bodies larger than a certain mass will take on a nearly spherical shape . the logical process to arrive at this conclusion involves several steps . i will try to enumerate these with the smallest number of non-trivial steps . material strength matters less and less as the scale of your system increases . a material yield strength ( or other definable limit ) has units of pressure ( like mpa ) . when material structure is acting to hold the shape of something against an external force , it offers some some anisotropy in the stress tensor . as we increase the size of a self-gravitating system , the scale on which this anisotropic component can matter becomes less and less . basically on larger scales , even rocky bodies behave more and more like a liquid than a solid . this does not preclude the existence of complex solid structures and mountains on the surface even though the vast majority the vast majority of the planet behaves mostly like a liquid ( see earth ) . when the material strength matters very little , then a single self-gravitating body will either : attain a shape that is consistent with hydrostatic equilibrium , or it will break up into pieces . in the presence of angular momentum , the hydrostatic equilibrium shape is non-spherical . most astronomical bodies ( but not all ) have sufficiently small angular momentum such that they are sphere-like . if the angular momentum is very high , it will break up into pieces , although practically it probably never forms in such a way to begin with . there are relatively few bodies spinning below the break-apart threshold but fast enough to be highly non-spherical for reasons that i do not fully understand . the kepler space telescope is offering new insights into the variety of planets including those with super-fast rotation and will likely shed new light on the subject . i have a hard time understanding what the star wars pictures are even trying to depict , but i will focus on lola sayu since i think i can make out what the picture is showing . the depiction is akin to an apple with a bite taken out of it . ( image license cc-by-sa-3.0 , wikimedia commons ) specifically , here are the various reasons such a shape is unphysical for a planet : the core of the planet is molten , therefore it behaves as a liquid , therefore it assumes a hydrostatic equilibrium shape , and the above shape is not included , qed . now , it would be over-generalizing without some qualifiers . the inner of most rocky planets is molten because of heat left over since its creation and internal heat production . we can potentially think of a sci-fi scenario where both of these will not be present ( just set it for a trillion years in the future ) . we defer to the next reason . even with the entire volume being fully solid , there is a separate , distinct , reason that mountains can not be higher than what the material properties will permit , and a planet with 1/4th of the matter blown away as in the case of lola sayu , the edges of that crater appear like a mountain to gravity . limited material strength cannot , against the gravity force , uphold shapes so severely deformed from the hydrostatic condition . as a final note , the pieces blow off from the planet in the picture are either in orbit , or they will be cleared away within a fairly small amount of time . most objects will probably not remain in orbit since they are ejected from the surface , and it will more than likely return to the surface at some point in the future , per : ( image license cc-by-sa-3.0 , wikimedia commons ) now , obviously you can not tell if something is in orbit from the picture ( since a still picture does not show movement ) , but it leaves plenty of unanswered questions . where did that mass of the planet go ? ! perhaps it blew away faster than escape velocity . either way , i am pretty sure none of the concerns mentioned here were given consideration in the creation of the artwork .
there are a few different ways to establish this correspondence , eg , using lie groups or fourier transform . but , in the end of the day , the notion that this takes one from classical to quantum mechanics is nothing but an ' axiom ' . so , in this sense , it is a bit weird to use the word " derive " the principle : this ' principle ' is used as one of the axioms that defines quantum mechanics . in any case , von neumann was probably the one to first formalize this construction in terms of what he called " transformation theory " , which is the theory of fourier transforms for distributions ( generalized functions ) .
the formula you have specified $$ \delta k = \sqrt{ ( \delta k_1 ) ^2 + ( \delta k_2 ) ^2} $$ is the formula to obtain error of quantity $k$ , as being dependent on $k_1$ and $k_2$ according to the following expression $$ k = k_1 + k_2 . $$ generally , to obtain experimental error of a dependent quantity ( and the expression stated in your question ) , you start with the expression for dependent quantity $$k = f ( k_1 , k_2 , . . . ) $$ and use statistical expression $$\delta k = \sqrt{\sum_i \left ( \frac{\partial f}{\partial k_i} \delta k_i \right ) ^2} . $$ if $$k = \frac{k_1 + k_2}{2}$$ then $$ \delta k = \frac{\sqrt{ ( \delta k_1 ) ^2 + ( \delta k_2 ) ^2}}{2} $$ so the generalized answer might be : you have to divide with $n$ and not $\sqrt{n}$ . however , bare in your mind that the statistical expression above might be used when measured quantities are " independent " of each other . if $k_1$ and $k_2$ are the same quantity measured in two measurements , this is not exactly true , so the exact statistical expression is much more complicated .
for a general system ( unknown equation of state/energy ) this will not be possible . for example a $ ( p , v ) $-curve with points $ ( p , \frac{c}{p} ) $ could be an isotherm because maybe $v ( p ) =\frac{nrt}{p}$ . an isotherm corresponds to a horizontal line in a $ ( t , s ) $-diagram . but if you do not know you are dealing with an ideal gas situation then it might look different . as a general orientation , the stability criteria for reasonable thermodynamic systems and related relations might be helpful to guess the signs of the rates of change . for example in a typical situation , you had expect the compressibility $$\beta=-\frac{1}{v}\frac{\partial v}{\partial p}$$ to be positive . the adiabatic and isothermal compressibilities are defined for curves of constant $s$ and $t$ respectively , so the corresponding adiabatic or isothermal line in the $ ( t , s ) $-diagram will be associated with a curve $ ( p , v ( p ) ) $ in the $ ( p , v ) $-diagram with $$\frac{\partial v ( p ) }{\partial p}&lt ; 0 . $$ you see this in the ideal gas example ( see picture ) , where the isotherm is $v\propto \frac{1}{p}$ and the adiabate is also some negative power of pressure $v ( p ) \propto \frac{1}{p^{\frac{1}{\gamma}}}$ , with $\frac{1}{\gamma}&gt ; 0$ . positivity of the heat capacities $c_v$ or $c_p$ at constant volume or pressure is a dual example , which relates to properties of $s ( t ) $ in similar way . furthermore , the maxwell relations cross correlate the slopes . lastly , there are some principles , following from the laws of thermodynamics , which forbid some curve configurations . e.g. two different adiabates can never cross twice , because that would mean a possibility for work without heat flow . but as i said , besides that rough reasoning regarding the shapes of the cyclic processes , if you do not know $t ( p , v ) $ etc . and you can not compute heat or work , how would you know which line in the $ ( p , v ) $ diagram corresponds to the isotherm or the adiabate ? in the other case , i.e. if know know the equation of state or some other relation , then the dependencies can of course be explicitly computed .
the usual approximation for arithmatic of quantities with uncorrelated uncertainties that for small ( ish ) uncertainties $\delta x_i$ or measurements $x_i$ let 's us write for multiplicative operation $$ \begin{array}~ y and = and \left ( \frac{x_1}{x_2}\right ) \ , \text{ or }\ , \left ( x_1 x_2\right ) \\ \frac{\delta y}{y} and = and \sqrt{ \left ( \frac{\delta x_1}{x_1}\right ) ^2 + \left ( \frac{\delta x_2}{x_2}\right ) ^2 } \end{array} $$ ( i.e. . add relative uncertainties in quadrature ) . you can get this kind of result from a bastardization of the chain the rule . given $y = f ( x_1 , x_2 , \dots ) $ or each input measurement $x_i$ , compute $\left ( \frac{\partial f ( x_1 , x_2 , \dots ) }{\partial x_i}\right ) \delta x_i$ and add all the resulting terms in quadrature . you have also been a little free with your nomenclature here . call $s$ the underling signal and $n$ the random noise ( this can be counting statistics or any other random process such as shot noise in the detector , but not a constant bias which must be subtracted off--our noise is assumed to have a mean of zero ) with a distribution whose width is characterized by $\sigma$ . a single measurement is then $m_i = s_i + n_i$ , and the population has a signal to noise ration of $\frac{s}{\sigma}$ . the win from addition is that the sum of $n$ such measurements is $$ m = \sum_{i=1}^n m_i = ns + \sqrt{n}\sigma $$ meaning that the signal to noise ratio of the sum is $\frac{ns}{\sqrt{n}\sigma} = \sqrt{n}\left ( \frac{s}{\sigma}\right ) $ , an improvement .
you should already be familiar with damping . it simply refers to the fact that if you set a spring going , it eventually stops . the wikipedia article should cover most of what you want to know . any particular spring may be damped for all sorts of reasons . any way the spring can lose energy contributes to damping , so it could be lost internally to heat due to stressing the material , or externally to heat via friction , or externally to an electromagnetic field , to some sort of mechanical dashpot , etc . if you were designing an experiment to study damping , you could be interested in a number of different particular things . you will have to make your own choice about what the most interesting thing to study is . for example , would you like measure the damping ratio ? the overall magnitude of the damping effect ? the time it takes your spring to reduce its amplitude to 1/2 its previous value ? if you have a particular effect picked out , do you want to know how it depends on the load on the spring , or the material of the spring , or whether the spring is in a vacuum , etc . there are many possible things to study , so your question has no definite answer . i am sure you can find something particular you find worthwhile .
see the wiki article on polarized 3d glasses . most likely , you have a pair of circularly polarized glasses . the mirror reverses the circular polarization . the article on circular polarization does it better than i would be likely to achieve in less than an hour or two . or hyperphysics , or google .
let us start by summarising the governing equations of mhd . we have the reduced form of the maxwell equations $$\nabla \times \mathbf{b} = \mu \mathbf{j} , $$ $$\nabla . \mathbf{j} = 0 , $$ $$\nabla \times \mathbf{e} = - \partial \mathbf{b} / \partial t , $$ $$\nabla . \mathbf{b} = 0$$ and the auxillary equations $$\mathbf{j} = \sigma ( \mathbf{e} + u \times \mathbf{b} ) , $$ $$\mathbf{f} = \mathbf{j} \times \mathbf{b} . $$ from the last two equations you get your result , namely that $$\mathbf{f} = \sigma ( \mathbf{e} + u \times \mathbf{b} ) \times \mathbf{b} . $$ note that the above combine to give the induction equation . i hope this helps .
their race is ill-defined . you can not declare a winner if you can not agree on the ordering of events . if they failed to pick a reference frame for the race before starting it , then of course an argument over the winner may ensue . no laws of physics have been violated . if you try and extend this to " malfunctioning machines " , then yes , a machine that was not designed with special relativity taken into account ( like the race was designed without sr ) will give unexpected results . but again , no laws of physics will be broken . as to shooting people . . . that is murder , whether the bullet left the gun before you pulled the trigger or after .
as far as i have understood from this paper , they have given some observational limits to the value of $\omega_{k , 0}$ , but this article concludes asserting that " there is no evidence from planck for any departure from a spatially flat geometry " . taking $\omega_{k , 0}=0$ and the value for $\omega_{r , 0}$ given at this post , one can compute the above integral obtaining $t_0 = 0.947797 \ , h_0^{-1}$ , which , taking $h_0 = 67.3 \ , \text{km} \ , \text{s}^{-1} \text{mpc}^{-1}$ , gives $t_0 = 13.78 \times 10^9$ years .
the doppler shift causes a shift in wavelength at the origin of the wave ( the frequency of the source never changes ) . this results in a shift in frequency for the observer . in the link below you can see the emission of the wave for a moving source causes the wavelength to be shorter in front and longer behind . the actual source is not changing in frequency . so it is a matter of relativity . to the traveling observer ( in the source ) , only the wavelength is changing , to the stationary observer ( experiencing the doppler shift ) both frequency and wavelength have changed . lookang , wikimedia commons . more simulations and applets here .
the book says " if at a moment δt later the angle of the whole object has turned through δθ . . . " a " moment " in this context is meant to convey a vanishingly short period of time , not an indefinitely long one , which means that δθ is taken to be a vanishingly small angle . furthermore , feynman ( and anyone else ) is free to use whatever notations they like ; there is no " law " that dictates dθ has to be used in this context .
entanglement is a quantum correlation between two ( or many ) objects - a correlation means that these two objects ' properties are not independent of each other - which was created in the objects ' common past when they were close to one another i.e. when they were two parts of the same physical system . quantum mechanics changes the character of possible " properties " that objects may have ( the quantities describing objects 's properties are usually called " observables " and they are represented by hermitian operators on the hilbert space ) , as well as the way how these properties are measured and predicted ( just probabilistically ) , so it also changes the character and magnitude of correlations that the objects may exhibit . in particular , quantum correlations may often be stronger - and affecting a large fraction of measurable properties of the objects - than what would be possible according to classical ( i.e. . non-quantum ) physics . in classical physics , correlations have to satisfy e.g. the so-called bell 's inequalities in various situations but quantum mechanics - and the real world - can easily surpass these bounds . technically , objects and their properties in quantum mechanics are described by wave functions . to describe the state of two mostly independent objects , one has to take a wave function from the tensor product $h_1\otimes h_2$ of the hilbert spaces describing the individual objects . the wave function in the tensor product implies probabilistic predictions for any pairs of properties of the first and second object ; in general , they are not independent , and for each combination of the objects ' properties , quantum mechanics ( and the wave function ) may remember an independent probability . any vector in the tensor product that can not be written as a tensor product of vectors from $h_1$ and $h_2$ ( instead , it can only be written as a linear combination of such tensor products of vectors ) is called entangled . in other words , it is non-entangled if it is a simple tensor product of two simpler vectors . if it is a simple tensor product , all probabilities of " coupled properties " of the pair of objects simply factorize to the probability of the first object , and probability of the second object , as you know from probabilities of independent phenomena . the simplest pedagogical example of an entangled state ( as well as the entangled state that is most often found in literature ) is $$\frac{x_1\otimes y_1 + x_2\otimes y_2}{\sqrt{2}}$$ because there are two terms with four different factors , you can not use the distributive law in any way that would allow you to rewrite it as a simple product . the letters $x , y$ refer to the two objects and the labels $1,2$ refer to two different states of each of the two objects . in this state , if the property "1 or 2" is measured on $x$ , one obtains the answers 1 or 2 with 50% probability for each : the coefficient of the wave function is $1/\sqrt{2}$ because these complex coefficients have to be squared to obtain the probability . however , because $x_1$ is " coupled " to $y_1$ and $x_2$ is coupled to $y_2$ , the state and the machinery of quantum mechanics predict that the object $y$ will be measured to have the same property : if $x$ is in 1 , $y$ is in 1 , and the same for the state 2 . linear algebra - which is crucially important for quantum mechanics - allows one to reinterpret the state above as an " identity operator " so the correlation will exist regardless of the type of measurement that we perform both on $x$ and $y$ . for example , if the two states represent spins , the two particles will be correlated so that you will find out that they are polarized with respect to the same axis , if you measure both particles ' polarizations with respect to the same , particular , but arbitrary axis . this would be kind of impossible for two separated particles in classical physics that could only be perfectly correlated for one choice of the axis - but not another axis rotated by 45 degrees , for example - without a communication in between them . however , quantum mechanics predicts that such a 100% correlation " regardless of the axis " is not only possible but guaranteed by the state above and it requires no communication . indeed , one can prove that relativistic theories in quantum mechanics - especially quantum field theory - do not allow one to transmit a single bit of information faster than light even though this would be needed in classical physics to guarantee the perfect correlation that quantum mechanics predicts for these experiments ( and that the experiments confirm ) .
it is a black coating to stick on surfaces of devices of an optical setuo to reduce spurious reflections . so it is very very black ( "99,99%" ) on one side , and possibly sticky on the other side . for an example , see edmund optics : acktar light absorbent foil
it is not poop . it is fly barf . a fly spends about 25% of its time re-digesting and it only can eat liquids . it mixes the eaten food with the appropriate enzyme for digestion . the fly does this by retrieving the eaten food from its digestive system ( a vomit of sorts ) , and drop by drop it is placed on the surface on which the fly is sitting . only then is it sucked back up after they are mixed . the small black dots , that are left in various places , such as the ceiling , are not fly droppings , but actually the remains which are not sucked back up . there are a variety of fluids that are mixed along with oils from the food . this creates a surface film much like when you write with your finger on a mirror . the growth of the spot is just the fluids slowly flattening out and spreading on the surface . the film prevents condensing water from beading to form a lens shape which defuses the light into the cloudy haze you see elsewhere . this is like an anti-fogging agent which works by minimizing surface tension , resulting in a non-scattering film of water instead of single droplets , an effect called wetting . anti-fog treatments usually work either by application of a surfactant film , or by creating a hydrophilic surface . the wetting may appear to spread near the edges of the film during evaporation because the droplets are on the cusp of being completely wetted and have more surface area to evaporate faster which gives the illusion of the spot spreading during evaporation . as for the poop ? well , the droppings fall to the ground and go undetected .
you should get them started whenever they are interested . answer questions , take them out to do astronomy related activities when they want , and so forth . the main thing for me is not to push them faster than they want to go . that causes them to lose interest . as for getting their own telescope , i got mine when i was eight . my neighbor down the street just got one for their eight year old son as well . that is only two data points so you can not really draw many conclusions . however , that seems to be the age when they are responsible enough to take care of it on their own . since you already have a 10" , it may not be as critical as they will probably grow up learning to use yours .
the average of any quantity $s$ is $\frac{\sum\limits_{r=0}^ns_r}{n}$ . if the distribution is continuous , lets say as a function of x , then it becomes $\lim\limits_{n\to\infty}\frac{\sum\limits_{r=0}^ns_r}{n}$ . this can be rewritten as $\frac{\int s ( x ) dx}{\int dx}$ , taking limits as the length of the wire . in your formula , i do not see any $x$ term in the rhs , nor anything that could depend on x , so i do not see how we can proceed . please specify what is constant and what is a function of x . so the final formula is $$\frac{\int t ( x ) dx}{\int dx}$$ if your wire is infinite , you may need to take limits 0 to y , and then limit the expression for average as $y\to\infty$ . update : with the updated formula , assuming the wire spans from x=0 to x=l , $$\langle t\rangle=t_\infty- \frac{\dot{q}}{km^2}\left ( \frac{\tanh ( ml ) }{ml}-1\right ) $$ if the wire spans from 0 to y , $$\langle t\rangle=t_\infty- \frac{\dot{q}}{km^2}\left ( \frac{\sinh ( my ) }{my\cosh ( my ) }-1\right ) $$ . limiting y to infinity gives us an infinite answer . so i am assuming that i have interpreted it correctly in my previous answer .
the annihilation produces gamma photons , whose total energy sums up to the total energy $e_0=\sqrt{p^2 \ , c^2 + m^2\ , c^4}$ formerly contained in the matter / antimatter kinetic energy ( the $p\ , c$ term ) and that " frozen " in rest mass ( the $m\ , c^2$ term ) . so energy is conserved . as for gravity , the einstein field equations " can not tell the difference " between the " matter/anitmatter " and " photons " - they are both pretty much the same and their " effective gravitational mass " is $e_0/c^2$ . look up the stress energy tensor - this is the " source " side of the einstein field equations that begets the " curvature " ( deviation from euclid 's parallel postulate ) of spacetime . there are some subtleties , because the se tensor is a tensor , not a scalar , but if you think of all the " stuff " as a gravitational source of gravitation mass $e_0/c^2$ , this is a good mental picture . so , at the time of the collision , nothing much changes as far as gravity is concerned . the change from matter/antimatter to light will change how the se tensor and thus spacetime curvature evolves with time , because matter / antimatter and light have different propagation equations ( the former propagating at some speed less than that of light ) .
yes , there is investigation . some random names on the field ( more on the physics side , no specific order ) : carl dettmann , tamás tél , ott , ying-cheng lai , adilson motter , celso grebogi , holger kantz , alessandro moura , eduardo g . altmann , etc , etc , etc . a quick search on some of these names should help you to find some recent papers on what is being done ( not restricted to ! ) . some specific topics on the subject with some activity : billiards , transient chaos , hamiltonian systems , quantum chaos , control theory . do not be biased by this information , use it has a shortcut to search more and more . it is not meant to represent anything in terms of importance , quantity or quality of the research .
to the last comment - i disagree . wind turbines operate in a fairly different flow regime , and compare the table fan to the prop of a ship , they are about the same shape . the key difference is moving fluid volume . the larger blades push against the fluid more strongly , which is desirable in the fan case and in the ship case . wind turbines are different as the builder cares nothing about the exhaust wind . anyway . . . 1 . i do not think it is ' functional ' in any sense other than the fact that the motor is behind it . 2 . i do not believe so and i agree with other commentators that the fan would be more simple and less noisy without them . i will withhold comment on the obvious safety considerations . 3 . everything dealing with pumping or pushing a fluid makes noise , and it is quite considerable . you could remove the blades and listen to the electric motor itself and you would find it constitutes only a small fraction of the noise . noise minimization for fans is a major engineering topic and it is more or less impossible to reduce it . obviously , the faster you get the bigger of a problem it is . i believe we are talking about turbulent flow in general , i do not think many pumps are actually laminar . the metal bars will have an influence , but i could not say how much .
this is a very intuitive way of arriving at helicity-1 representations , but it is not totally correct . essentially , you are ignoring the continuous spin representations of the poincare group . when you do induced representations of a semi-direct product like the poincare group , you fix some momenta as a representative element of an orbit of $so ( 3,1 ) $ and then look for the little group that stabilizes this momenta . in the case of massive particles for example you choose $ ( m , 0,0,0 ) $ and find that the little group is su ( 2 ) giving rise to the spin degrees of freedom of massive particles . little group transformations then transform spin states into other spin states . but in the case of a massless representation , choose for example $ ( e , 0,0 , e ) $ . if you play around for a while you will find that a much larger group , $iso ( 2 ) $ , the isometries of the plane , stabilizes this momenta . the representation theory of this group can be obtained as it is for the poincare group ( $iso ( 3,1 ) $ ) . but because of the two " momenta " parameters in $iso ( 2 ) $ most of these representations will be infinite dimensional . this would mean that such a massless particle has infinitely many internal degrees of freedom . since this is not observed , people choose the " representative momentum " ( 0,0 ) i.e. the vacuum state for $iso ( 2 ) $ . this corresponds to setting the $iso ( 2 ) $ " momentum generators " to zero , and the little group here is then $so ( 2 ) $ . irreducible representations of $so ( 2 ) $ are one dimensional , but you have to include two such representations by cpt , giving rise to the two degrees of freedom of the photon . so what i am saying is that the $m \to 0$ limit for massive particles is not really well defined , you could land on the discrete helicity representations or the continuous spin representations . because of this it is safer to just start with the correct representations of the poincare group . say that you want the helicity-1 representation . try to put it in $a_\mu$ with polarizations $\epsilon_\mu$ . note that even if you choose only two polarizations to be nonzero when you do a little group transformation they shift by something proportional to the momenta . interpret this as gauge invariance and make the identification on physical states . also , all representations of the poincare group will be characterized by the casimirs $p^2 , w^2$ . it is a straightforward calculation to show that $w^2=m^2j^2$ for massive states . your idea basically amounts to just setting m=0 in this expression , which allows you to show $w^\mu =\lambda p^\mu$ with $\lambda$ the helicity that labels the states . however , you should actually go back through the calculation and plug $ ( e , 0,0 , e ) $ into the expression for the pauli-lubanski vector . then you will find that to show $w^2=0$ you need certain linear combinations of the lorentz generators to vanish on the states . these are exactly the "$iso ( 2 ) $ momenta operators " that you need to set to zero to get the helicity representations . hope this helps !
i am not quite sure what you mean about " keeping the entropy below maximum " , but most basic application of entropy is to determine , which processes are reversible . as for reversible processes , you actually can add or remove heat in a way that the entropy of the whole universe remains the same . this is possible when you add the heat to the system by isothermal process ( constant temperature ) . in fact , in isothermal process the change in entropy of the closed system will be opposite to the change of entropy to the rest of the universe . all other types of heat exchange increase the entropy of the whole universe . that is the point of the carnot process , in which you have two adiabatic processes ( no exchange of heat ) and two isothermal processes ( constant temperature ) . carnot process is fully reversible , thus it does not change the entropy of the universe . if this is not exactly the answer you are looking for , please be more specific .
the first issue i see : your speed is in miles/hour , and your time is in seconds .
edit : note that i am doing only the first variation , and i am not doing each and every step , mainly those pertinent in understanding how the general shape equation is determined . if you want to see the full derivation , you will need to understand the geometric mathematic primer discussed in sections 2 and 3 of the book . geometric methods in the elastic theory of membranes in liquid crystal phases by zhong-can ou-yang , ji-xing liu , yu-zhang xie , xie yu-zhang $c_{0}$: spontaneous curvature of the membrane surface $k_{c}$: bending rigidity of the vesicle membrane $h$: mean curvature of the membrane surface at any point $p$ $k$: gaussian curvature of the membrane surface at any point $p$ $da$: area element of the membrane $dv$: volume element enclosed by the closed bilayer $\lambda$: surface tension of the bilayer , or the tensile strength acting on the membrane $\delta p$: pressure difference between the inside and outside of the membrane . the shape energy of a vesicle is given by : $$ f = f_{c} + \delta p \int dv + \lambda \int da $$ where $$ f_{c}=\frac{k_{c}}{2} ( 2h-c_{0} ) ^{2} = \frac{k_{c}}{2} ( c_{1}+c_{2}-c_{0} ) ^{2} $$ the variation of $da$ and $dv$ are needed , refer to the book to locate those . next we will calculate the first variation of $f$ . and we can break this into components by starting with the first variation $f_{c}$ . $$ \delta ^{ ( 1 ) }f_{c} = \frac{k_{c}}{2}\oint ( 2h+c_{0} ) ^{2} \delta ^{ ( 1 ) } ( da ) + \frac{k_{c}}{2}\oint 4 ( 2h+c_{0} ) ^{2} ( \delta ^{ ( 1 ) }h ) da $$ where the first order variation of $\psi$ gives us : $$ \delta ^{ ( 1 ) }da = -2h\psi g^{1/2}dudv $$ $$ \delta ^{ ( 1 ) }dv = \psi g^{1/2}dudv $$ $$ \delta ^{ ( 1 ) }h = ( 2h^{2}-k ) ) \psi + ( 1/2 ) g^{ij} ( \psi _{ij}-\gamma _{ij}^{k}\psi_{k} ) $$ note : $\gamma_{ij}^{k}$ is the christoffel symbol defined by ( for reference ) : $$ \gamma_{ij}^{k} = \frac{1}{2}g^{kl} ( g_{il , j} + g_{jl , i} - g_{ij , l} ) $$ and we plug those into the variation of $f_{c}$: $$ \delta ^{ ( 1 ) }f_{c} = k_{c}\oint [ ( 2h+c_{0} ) ^{2} ( ( 2h^{2}-k ) \psi + ( 1/2 ) g^{ij} ( \psi_{ij}-\gamma_{ij}^{k}\psi_{k} ) ) ] $$ $$ = k_{c}\oint [ ( 2h+c_{0} ) ( 2h^{2}-c_{0}h-2k ) \psi + ( 1/2 ) g^{ij} ( 2h+c_{0} ) \psi_{ij} - g^{ij}\gamma_{ij}^{k} ( 2h+c_{0} ) \psi_{k} ] g^{1/2}dudv $$ and there are two relations ( $i , j = u , v$ ) $$ \oint f\phi_{i}dudv = -\oint f_{i}\phi dudv $$ $$ \oint f\phi_{ij}dudv = \oint f_{ij}\phi dudv $$ so then we have : $$ \delta ^{ ( 1 ) }f_{c} = k_{c}\oint \left \{ ( 2h+c_{0} ) ( 2h^{2}-c_{0}h-2k ) g^{1/2} + [ g^{1/2}g^{ij} ( 2h+c_{0} ) ] _{ij} + [ g^{1/2}g^{ij} ( 2h+c_{0} ) \gamma_{ij}^{k} ] _{k} \right \}\psi dudv $$ and we can re-write : $$ [ g^{1/2}g^{ij} ( 2h+c_{0} ) ] _{ij} = [ ( g^{1/2}g^{ij} ) _{j} ( 2h+c_{0} ) ] _{i} + [ g^{1/2}g^{ij} ( 2h+c_{0} ) \gamma_{ij}^{k} ] _{k} \psi dudv $$ and for functions $f ( u , v ) $ , where $u , v = i , j$ , we can directly expand : $$ [ ( g^{1/2}g^{ij} ) _{j}f ] _{i} = - ( \gamma_{ij}^{k}g^{1/2}g^{ij}f ) _{k} $$ a laplacian operator for these surfaces is defined in the book , and is given as : $$ \bigtriangledown^{2} = g^{1/2}\frac{\partial }{\partial i} ( g^{1/2}g^{ij}\frac{\partial }{\partial j} ) $$ so then we have : $$ [ g^{1/2}g^{ij} ( 2h+c_{0} ) _{j} ] _{i} = g^{1/2}\bigtriangledown^{2} ( 2h+c_{0} ) $$ using these methods in the first variation : $$ \delta^{ ( 1 ) }f_{c} = k_{c}\oint [ ( 2h+c_{0} ) ( 2h^{2}-c_{0}h-2k ) + \bigtriangledown^{2} ( 2h+c_{0} ) ] \psi g^{1/2}dudv $$ and now we want the variation of $f$ . $$ \delta^{ ( 1 ) }f = \delta^{ ( 1 ) }f_{c} + \delta^{ ( 1 ) } ( \delta p\int dv ) + \delta^{ ( 1 ) } ( \lambda\int da ) $$ which gives us : $$ \delta^{ ( 1 ) }f = \oint [ \delta p-2\lambda h + k_{c} ( 2h+c_{0} ) ( 2h^{2}-c_{0}-2k ) + k_{c}\bigtriangledown^{2} ( 2h+c_{0} ) ] \psi g^{1/2}dudv $$ and since $\psi$ is a very small , well smooth function of $u$ and $v$ , the vanishing of the first variation of $f$ requires that : $$ \delta p = 2\lambda h + k_{c} ( 2h+c_{0} ) ( 2h^{2}-c_{0}h-2k ) + k_{c}\bigtriangledown^{2} ( 2h+c_{0} ) = 0 $$ which is the general shape equation of the vesicle membrane . $c_{0}$ is a constant unless the symmetry effect of the membrane and its environment varies between each point ( we assume it does not ) otherwise $c_{0}$ becomes a function of $u$ and $v$ . so we can reduce to : $$ \delta p = 2\lambda h + k_{c} ( 2h+c_{0} ) ( 2h^{2}-c_{0}h-2k ) + 2k_{c}\bigtriangledown^{2}h = 0 $$ hope this helps . again i would locate that book to see the full derivations . i do not know if the visible section of the book on google shows you everything that you need to know , but i surely hope this points you in the right direction to understanding the problem .
presumably each axle is rigidly attached to its gear ( no bending or breaking ) . this means that the torque on the gear is the same as the torque on its axle . so you can ignore the axles and just think about the gears themselves . and in that case , you can just use the torque ratio of the gears themselves .
these things do happen . the phenomena is called multipath interference . you do not notice it much in ordinary life becase there is rarely a situation where sound is not being scattered from lots of different surfaces and that the frequency is pure enough so that there exists a single null point for all the sound . note that the nodes and anti-nodes caused by the multipath interference move around as a function of frequency . ordinary sounds contains a wide range of frequencies such that even if one specific tone is partly nulled out , we usually do not notice . you can create situations without a fancy lab where you can observe interference effects yourself . for example , in college we put a speaker at one end of a long hallway and drove it with a frequency generator . at the right frequency , which was 43 hz for that hallway if i remember right , there were nodes and anti-nodes dues to standing waves . you could of course still hear the tone at the anti-nodes , but it was significantly softer than at the nodes . the effect was strong enough that if you were at one of the nodes and started walking toward the speaker , the sound would get softer and you had think it was coming from the other direction . it was great fun to see peoples ' reactions that could not figure out where the sound was coming from . with radio waves you can experience this more easily without a deliberate setup because any one radio transmission is in a very narrow frequency band . particularly with commerical fm ( about 3 meter wavelength ) you can notice this driving around in a car . there will be spots where the reception is very poor , even though a few meters in any direction it will be better .
i take the heart of the heisenberg cut to be the way we calculate probabilities and expected values in qm . for elementary qm , for some measurement described by an operator $\hat m$ , the expected value in a state described by a density matrix $\hat\rho$ is given by the trace $\left&lt ; \ ! m\right&gt ; =\mathsf{tr}\left ( \ ! \hat m\hat\rho\ ! \right ) $ . what we put in the $\hat\rho$ is what is in our model universe . the measurement operator describes our measurement apparatus , which is not in the model universe , but instead describes how our measurement apparatus gets information out of the model universe . there is an almost-symmetry between the ways $\hat m$ and $\hat\rho$ appear ; it is almost as if there is a measurement apparatus universe as well as the model universe . different measurement apparatuses can affect each other in the measurement apparatus universe without changing the model universe , which is called measurement incompatibility ( did i just create an interpretation of qm ? do i know this one ? i guess it is too glib , sadly . ) edit : we can extend the mathematics in many different ways , but one deserves mention because it is of great practical value . we can introduce transformations $\hat t_i$ that operate between the preparation apparatus and the measurement apparatus , in which case we have $\left&lt ; \ ! m\right&gt ; =\mathsf{tr}\left ( \ ! \hat m\hat t_n\cdots\hat t_2\hat t_1\hat\rho\ ! \right ) $ . we can at any time say that $\hat m\hat t_n\cdots\hat t_5$ , say , or any other part of this list ( without changing the order ) , is our measurement . anyway , to some extent we can move stuff from the model universe into the measurement apparatus universe and vice versa , although we may have to get into technical stuff like povms to do it . once we go to quantum field theory , there is a tight relationship between the measurement apparatus universe and the model universe , because we use the same lego blocks to build measurements and states . the separation into states and measurements is absolutely fundamental in qm . it is how the relationship between hilbert spaces and experimental results works , which causes trouble when people want to do cosmology , with everything in the model universe . although i had not previously thought about calculating where one should put the separation , i can see that if one chooses a particular accuracy that one wants one 's model of an experimental apparatus to achieve relative to one 's real apparatus , that might put a limit on where one can put the heisenberg cut . i am not sure , however , that one can not always improve the sophistication of one 's description of a measurement , particularly if one is willing to go to povms . i suppose , however , that putting people inside your model universe is always going to be in the realm of toy models . the separation into states and measurement famously comes under the microscope in bell 's article `against " measurement " ' . incidentally , i see you went to willem de muynck , who is perhaps a little idiosyncratic , but i have often found his a good counterpoint of view .
if you mean that qft operators are matrices $m_{ij}$ whose indices $i , j$ are really points in space ( or spacetime ) , so that the operator is really represented by the function $m ( x , x&#39 ; ) $ , then the answer is no . the actual matrices corresponding to qft operators are much much larger than that . an object expressed by $m ( x , x&#39 ; ) $ is pretty much equivalent to operators in ordinary quantum mechanics of a single particle : $$m ( x , x&#39 ; ) = \langle x| \hat{m} |x&#39 ; \rangle$$ the function $m ( x , x&#39 ; ) $ knows everything about the operator $\hat{m}$ because we have evaluated all matrix elements of this operator with respect to a basis - in this case , the position eigenvector basis . ( it is just space , not spacetime . ) however , quantum field theory has a much larger hilbert space . instead of the simple functions above , you may imagine that an operator is expressed by the following functional : $$m [ \phi ( x , y , z ) , \phi&#39 ; ( x , y , z ) ] = \langle \phi ( x , y , z ) |\hat{m}| \phi&#39 ; ( x , y , z ) \rangle$$ note that the object on the left hand side is a functional - it is a function whose two arguments are functions of 3 variables themselves - field configurations of a klein-gordon field , in this case . again , the left hand side knows everything about the operator $\hat{m}$ that acts on the hilbert space of the klein-gordon quantum field theory . the formula above chose a specific basis - a " truly " continuous basis of field configurations of $\phi ( x , y , z ) $ . however , this prescription may make the operators of qft look more complicated than they really are . any operator in any quantum mechanical theory may be specified by its matrix elements with respect to any basis . and there are usually more intuitive bases . in particular , in free quantum field theories , a simple-to-imagine basis is the fock space basis . $$|0\rangle , a^\dagger_{i}|0\rangle , a^\dagger_{i} a^\dagger_{j}|0\rangle , \dots$$ it is the vacuum and all of its excitations by an arbitrary number of creation operators that add particles into any state . in this way , the hilbert space is represented as an infinite-dimensional harmonic oscillator . the transformation " matrix " from the continuous basis of the field configurations to the fock space basis may be found by copying the same transformation for a normal harmonic oscillator infinitely many times and taking the tensor product . ( you do not have to mechanically repeat the same work infinitely many times , so this prescription may sound more intimidating than it is . ) if you meant that the quantum fields $\hat{f} ( x , y , z ) $ are operators and for each $x , y , z$ , then the answer is almost yes . they are operator distributions - that generalize operators much like the distributions such as the delta-function generalize the concept of a function . however , they are damn large matrices . they are matrices expressed with respect to a basis - for example one of the two bases above . but if this paragraph captures what you meant , then you were just asking whether operators are matrices . indeed , at least morally , they are . but this fact is true in any quantum mechanical theory and is not specific to qft . an operator contains a lot of numbers and may always be represented as a matrix . however , the values of the matrix elements always depend on the basis you chose - sometimes in ways that can not be recognized quickly . however , there is something physical about each operator that does not depend on any basis : you may " feel " an operator . you may learn many of its properties . some of them are more easily understood in one basis ; other properties naturally lead to another basis . so it is useful to think about operators in a more general way than matrices in a particular fixed basis - because the latter viewpoint usually discourages one from gaining the insights that are more obvious in another basis . one should not forget that the transition from one basis to another is just a " trivial " linear algebra that does not contain any special " physics " knowledge .
i think it is not quite true that modern physics tries to " avoid big bang singularities " . among many other things , modern physics tries to determine what is happening in the vicinity of the region that the big bang cosmology views as a singularity . by saying that the big bang singularity is completely avoided , you are already presupposing an answer . this possible answer may be a " working hypothesis " but it may also be incorrect . so it is not a " goal of modern physics " . classical general relativity breaks down - and becomes unpredictive - in the presence of singularities . but that does not mean that the full theory that also knows everything about the short-distance physics has to avoid the singularities completely . in string theory , the big bang singularity has not been understood yet - at least to the satisfaction of most string theorists . but many other singularities have been fully understood and it is not true that all of them disappeared . some of them did not disappear but the physics around them began well-defined , anyway . in particular , time-like singularities - such as orbifold singularities ; orientifold singularities ; and conifold points - have been pretty much fully understood . there are new degrees of freedom and new phenomena that take place in their vicinity but in some sense , physics understands them as well as it understands the smooth space today . from some viewpoint - e.g. according to some " probes " ( objects whose reactions we use to evaluate what is happening in the region ) - the singularity may get regulated , replaced by a regular manifold with some typical length scales . other probes see the singularity replaced by a non-commutative geometry that is also regulated . but some singularities according to some probes still look singular ; the best geometrical description is still a geometry that has a full-fledged singularity at the original point . however , it is not necessarily a problem . despite the singularity , physics at some ( not quite ) manifolds such as the conifold may be shown to be completely equivalent to physics at a completely smooth manifold ( e . g . by mirror symmetry ) . this equivalence shows that physics at singular manifolds may be well-defined and predictive . the space-like singularities - like the singularity inside the schwarzschild black hole and the big bang singularity - remain confusing . some people even think that all questions involving the interior of a black hole or the very beginning of the universe are inevitably ill-defined , at least to some extent , so there will never be a set of sharp observables that can be exactly calculated and discussed . i am personally not sure whether it is the case : it could be . but of course , the working hypotheses what is happening near the big bang - probably before inflation - also include some models with bounces ; cyclic universes ; non-commutative geometry ; a beginning of the universe from " zero " that looks totally smooth after the wick rotation to the euclidean space ( the hartle-hawking state ) ; but also an abrupt tunneling into another universe , and many other things . the collection of possibilities is pretty rich and some of them are more motivated than others . of course , physics still does not know for sure which of the tools are truly relevant for the birth of the universe .
what you are describing is the particle in a box system , and for a 3d box the energy levels of this system are given by : $$ e_{ijk} = \frac{\hbar^2\pi^2}{2ml^2} ( n_i^2 + n_j^2 + n_k^2 ) \tag{1} $$ where $l$ is the size of the box . the numbers $n$ label the energy levels with $ ( 1 , 1 , 1 ) $ being the lowest level and larger values of $n_i$ , $n_j$ and $n_k$ giving higher energy levels . the interesting thing about this system is that even in the lowest energy state , $ ( 1 , 1 , 1 ) $ , the energy is not zero . this is the phenomenon known as zero point energy , and a quick look at equation ( 1 ) shows that the magnitude of the zero point energy is inversely proportional to the size of the box squared : $$ e_{111} = \frac{3\hbar^2\pi^2}{2m}\frac{1}{l^2} $$ so to you shrink the size of the box you have to put energy in by doing work on it . an electron is ( as far as we know ) a point particle and has no volume so to perfectly confine the electron would mean taking $l$ down to zero , and that would mean putting in an infinite amount of energy i.e. it can not be done . the uncertainty principle comes in because the uncertainty in the electron 's momentum scales roughly as the energy , so as $l \rightarrow 0$ the momentum uncertainty $\delta p \rightarrow \infty$ . in fact even in principle we can not get $l$ smaller than about the planck length , because at that point the energy density is so high that the system would form a black hole .
i originally had something about the constellations changing in the sky to show that the earth orbits the sun , but that would still be the case if the sun orbited the earth instead . now that i think about it , there is one thing that conclusively proves that the earth orbits the sun : parallax . over the course of one year many of the stars will move relative to each other . at the end of the year they will be back where they started . this is because the earth moves around in a 2au diameter circle , so that six months from your first observation , you will be standing 2au away from where you were then , and are viewing the stars from a ( slightly , but observably ) different angle . to show that the moon orbits the earth you could observe its location at the same time every night , and see that it moves , and is always nearly the same distance from earth . it never goes into a retrograde motion . assuming the earth is spherical , the only way this could be true is if the moon orbits the earth . you might also take the phases of the moon into account and model the sun-earth-moon system to explain it .
i think your problem is assuming that $a = b$ . your triangle looks isosceles , but it will not usually be . using your terminology : $\cos \alpha = \overrightarrow{ae} / \overrightarrow{ad}$ . if we let $\mathbf{f} = \overrightarrow{ad}$ , then $|\overrightarrow{ae}| = f \cos \alpha$ , and $|\overrightarrow{ae}|$ is the component of the force in the direction of movement . the reason you can not just fix $a=b$ is that it makes $\alpha = \frac{\pi}{4}$ automatically , while in reality it can be anything , and it will depend on the relationship between the force and the direcion of movement .
in axiomatic approaches to quantum field theory , the basic field operators are usually realized as operator-valued distributions . that is what wightman fields are : operator-valued distributions satisfying the wightman axioms . wightman functions are the correlation functions of wightman fields , nothing more . there is a nice theorem that says if you have a bunch of functions that look like they are the wightman functions of qft , then you can actually reconstruct the hilbert space and the algebra of wightman fields from it . neither of these concepts is anything new physically . they are just more precise ways of speaking about things physicists already know . ( thinking of fields as operator-valued distributions instead of operator-valued functions lets you make precise what goes wrong when you multiply two fields at the same point . ) you can talk about opes or scattering theory in this language , but it will not gain you anything , unless you are trying to publish in math journals . if you are interested in the wightman axioms , they are explained nicely in the first of kazhdan 's lectures in the ias qft year .
yes , cavitation creates a " vacuum " . but the liquid will start evaporating the moment the void is created , so it is not a vacuum in the strict sense of " nothing there " . when a cavitation bubble collapses ( which usually happens very shortly after it forms ) , the temperature inside can rise very quickly - due to adiabatic compression of the little gas that has diffused into the bubble in the short time that it existed . the collapse can be so violent that the back side of the bubble , much like a shaped charge , creates a " jet " that can impinge the object that initially created the bubble ( image from http://authors.library.caltech.edu/25017/4/chap3.htm ) . this is roughly the mechanism that is responsible for cavitation damage on say the propellers of a ship : ( image from http://upload.wikimedia.org/wikipedia/commons/e/e6/cavitation_propeller_damage.jpg ) but cavitation is cause by " tearing " at the liquid - you are literally pulling it apart . when a liquid is boiling , you are creating bubbles by forcing the ( water ) vapor out of the liquid phase into the solid phase . definitely not a vacuum there . of course it is only a matter of degree . how low of a pressure do you need before you consider something a " vacuum " ?
in general , the energy is a quantity conserved due to the translational invariance in time or , equivalently , the generator of these translations in time . so depending on what we mean by time , we also have different meanings of the word energy . in the usual treatment of the spacetime , the energy $e=p^0$ is the generator of translations in the usual time $t=x^0$ . in the spacetime light cone gauge , the energy is often meant to be $e= p^-$ and generate translations in $x^+$ , the light-cone-gauge counterpart of " time " that defines the slices in such a gauge . a single string has mass going like $\sqrt{n}$ so this is also its energy $p^0$ in the rest frame where $\vec p=0$ . however , in the light cone gauge , $$p^- = \frac{ ( p^i ) ^2+m^2}{2p^+} $$ note that this is just the usual $p_\mu p^\mu = m^2$ solved for $p^-$ in the light cone gauge , so that the $m^2$ in the numerator goes like $n$ ( the total excitation of the string ) rather than $\sqrt{n}$ . also , the energy may mean the world sheet energy , the generator of translations in the world sheet temporal coordinate $\tau$ . then , for an excited string , the energy goes like $h\sim l_0+\tilde l_0\sim n$ much like in the light cone gauge . however , there are different coefficients in these two interpretations of the energy and one has to treat the nonzero modes and ghosts differently . i am confident that all the standard textbooks make it clear from the context which " energy " they are talking about . concerning the second problem , what was clearly meant was the " long/classical string " and its energy $p^0$ in the rest frame of the whole string . such a long string has the contribution to its rest mass $\delta m^2 = ( t l ) ^2$ where $t$ is the string tension ( the linear energy density ) and $l$ is the length . there are also contributions to $m^2$ going like $n/ \alpha'$ etc . from the string excitations . the world sheet energy or the light-cone-gauge energy have contributions from long wound strings that go like $l^2$ ( because they are linear functions of $m^2$ , not $m$ ) . when you wrote $p^0= ( p^++p^- ) /\sqrt{2}$ , you missed the usual $1/\sqrt{2}$ normalization but this relationship holds in general ( definition of the light cone gauge ) . yes , the energy that was scaling linearly in the length ( and tension ) was $p^0$ . however , you do not gain anything by rewriting $p^0$ in terms of the light cone $p^\pm$ because the energy that is proportional to the length of the string has to be measured in the rest frame of the long string in the spacetime where $\vec p=0$ which also implies $p^+=p^-$ . when you impose this equality between $p^+$ and $p^-$ and the assumption about the right squared mass of the long string , you will also derive $p^0\sim tl$ . but there is no point in setting $p^+=p^-$ etc . because the formula for $p^-$ in the light cone gauge always explicitly contains $1/p^+$ and the formula for $p^-$ may always be seen – once you multiply it by $p^+$ and subtract $ ( p^i ) ^2$ from the result – to be equivalent for the formula for $m^2$ . so in the light cone gauge , to derive the " linear density of the string " , you must still know that $m^2$ goes like $ ( tl ) ^2$ , so you effectively assume ( or precalculate ) what you want to derive , anyway , $m_0\sim tl$ .
the explanation is using an energy argument . that for the normal case of a submerged piece of wood , you can assume that if the wood and a parcel of water above it switch places , then the water ( which is heavier/more massive ) drops in the gravitational field releasing potential energy . this release is not offset by the rising wood since it is not as massive . this energy imbalance goes into the kinetic energy to move the log ( and the water ) . this argument says that in the case of the rotating log , none of the water can move downward to release potential energy . since the water stays in the same place , no energy is released , and there is no energy to move the log . why does not the object just stay where it is ? because forces will combine to move it to a position with less energy . you can do a free-body diagram of a ball rolling downhill to see that the net force is down the hill . but you can also simply say that the ball is going to move in a way that lowers its potential energy ( which is downward in the gravitational field ) . using that argument for the cylinder says that there is no position it is free to move to that lowers the potential energy , so no movement will happen .
a friend recently brought to my attention that this experiment was actually performed 6 months after i posted the question in this site : http://blogs.nature.com/news/2011/11/light_coaxed_from_nothingness.html http://www.chalmers.se/en/news/pages/chalmers-scientists-create-light-from-vacuum.aspx christopher wilson from chalmers ( and his team ) used the same mechanism that i have proposed in here : using a superconducting magnet to oscillate the mirror surface . i am glad to see that the idea really works !
the average height of a molecule is $h$ with your exponential distribution , because $\int_0^\infty dt\ , t\ , \exp ( -t ) =1$ , so its average potential energy is $mgh$ where $m$ is the mass of the molecule . how much does it compare with $3kt/2$ ( or $5kt/2$ etc . ) in the kinetic energy ? and what will happen to $\delta t$ ? the answer is given by the virial theorem ( so in advance , i am telling you that the reduction of $\delta t$ will be of the same order as $\delta t$ itself ) but let me use no pre-derived results . instead , just realize that the exponential decrease in $\exp ( -h/h ) $ is nothing else than the maxwell-boltzmann factor $\exp ( -mgh/kt ) $ which implies that $h=kt/mg$ and the average potential energy is $mgh=kt$ . ( at this stage , i am neglecting the dependence of the temperature itself on the height etc . ) so if you have a monoatomic gas which has $3kt/2$ in the kinetic energy , $kt=2kt/2$ is ultimately given to the potential energy which is $2/5$ of the total energy . as the temperature rises , the scale height will also have to rise , and you can see that $2/5$ of the energy that is initially pumped to the kinetic energy will be converted to the increased potential energy and only $3/5$ of the original increase of the kinetic energy will stay in the kinetic energy . so i think that for the monoatomic gas , the answer is $ ( 3/5 ) \delta t$ . similarly , it will be $ ( 5/7 ) \delta t$ if the kinetic energy of a single molecule is $5kt/2$ , and so on . just to be sure , this is not equivalent to the " global warming " issue because the greenhouse warming is a permanent change of the energy flows ( extra watts per square meter - joule in every single second ) while the energy needed to change the gravitational potential of the gas when its distribution at different altitudes changes is just a one-time event . however , the general observation that the ultimate increase of the temperature ( or anything else ) will be smaller than the most naively calculated one is valid : the extra heat is ultimately redistributed to and consumed by " many consumers " ( in this case , potential energy ) , which means that one particular consumer ultimately gets less . this is the reason why the feedbacks of stable systems tend to be negative - an insight known as the le chatelier 's principle ( in chemistry ) or homeostasis ( generally ) or lenz 's law ( in electromagnetism coupled to mechanics ) etc .
there is an interesting question in here if you look hard enough . first of all , there is nothing special about the resonant frequency of something made of little magnets . it might as well be a piece of ordinary string , a metal bar , or whatever . in fact i think the fact that it is made of separate little magnets stuck together would give it a much lower q factor than , for example , a metal bell , which would make the resonant frequency less clear and harder to measure . but anyway , let 's assume we have some object with a mechanical resonance around 15 hz . how do we measure this frequency with no other equipment ? as a human , there are basically two ways you can observe something 's vibration frequency . if the vibration is slow enough - below about 5 or maybe 10 hz - you can just count the individual cycles . if i am wiggling a jump rope at the resonance frequency and count 300 wiggles per minute , then the frequency is 300/ ( 60 s ) = 5 hz . the other way - which works well between about 50 hz and 10,000 hz - is to listen to the pitch of the vibrations and figure out the frequency of that musical pitch . if i listen to a bell and hear the pitch a above middle c , that means the frequency is 440 hz ( a 440 ) . if , like me , you do not have perfect pitch , you will need some reference pitch to compare it too , such as a piano , but once you figure out the closest note of the musical scale , you know the frequency to within 3% . the interesting thing in this case is that 15 hz is right in the middle of that awkward frequency range which is too fast to count , but too low to hear . so how do you measure a frequency in this awkward range ? if you have some simple lab equipment it becomes easy . for example , just shine a light on a photodiode and arrange it so that the edge of the vibrating object gets in the light path and casts a shadow . then look at the photodiode signal on an oscilloscope and you will see the vibrations at 15 hz . ( interestingly , an ordinary tv camera would be a bad choice here because they take about 30 discrete frames a second , so 15 hz is right around the nyquist frequency and aliasing would make it impossible to tell if it were 13 hz or 17 hz , for example . ) but what if you do not have an oscilloscope , or video camera or anything like that ? i think you would still be able to measure the resonance frequency by hand , you had just have to be inventive . for example , you can try to excite the vibrations using subharmonics . if you give your string of magnets a sudden shove 3 times a second , for example , then your 3 hz excitation signal has harmonics at 6 hz , 9 hz , 12 hz , 15 hz . . . , because it is not sinusoidal . so if you are able to show that you get a strong resonance at 3 hz , but not at 2.9 hz or 3.1 hz , then you know that the resonant frequency is one of those harmonics . if you repeat that process with some different subharmonic ( e . g . 5 hz ) , then you can quickly narrow down the possibilities for the resonant frequency .
almost all objects around us are electrically neutral , meaning they have no excess in negative/positive charges . when we use a plastic comb on our hair , a very small amount of negative charges start accumulating on the comb , making it negatively charged . it is important to note that it is not the friction caused by combing your hair that leads to such charge accumulation , instead it is simply related to maximizing the contact surface between comb-hair , making the charge exchange more likely to occur . this is of course all in simple words . now as for the sounds you hear , they are called electric sparks , which results from the discharge of two initially charged objects . the electric field created by the two , ionizes the air , creating ions from air molecules , which in turn becomes a conductive channel ( temporarily ) , this channel is necessary for the discharge ( exchange of charges until neutralization ) to occur . finally a spark is formed when the electric field strength exceeds the dielectric strength ( polarizability of air ) of air . the discharge in the conductive channel raises the temperature of the surrounding air . since the discharge takes so little time to go from one region to another , the heated air has no time to expand . the heated air is compressed , raising the air 's pressure to much higher values than the normal atmospheric pressure . the sudden expansion of the compressed air creates a loud burst , which we call a spark . on wikipedia there is also a good bit of explanation . just google electric spark for further sources and explanations .
the standard model yukawa interactions must be $su ( 3 ) \times su ( 2 ) \times u ( 1 ) _y$ gauge invariant . the down-type yukawa interaction is $$ \mathcal{l} \supset -y_d \bar q \phi d_r + \text{h . c . } . $$ this is indeed gauge invariant . the $\bar q d_r$ form a colour singlet ( $3^* \times 3$ ) , the $\bar q \phi$ form an $su ( 2 ) $ singlet ( $2^*\times2 ) $ , and the whole thing is neutral under $u ( 1 ) _y$ , because the quantum numbers are $-\frac13$ , $1$ and $-\frac23$ , for $\bar q$ , $\phi$ and $d_r$ , which sum to zero . let 's try writing a similar up-type yukawa $$ \mathcal{l} \supset^ ? _ ? -y_u \bar q \phi u_r + \text{h . c . } $$ is it gauge invariant ? no - it breaks $u ( 1 ) _y$ , because the quantum numbers are $-\frac13$ , $1$ and $\frac43$ , for $\bar q$ , $\phi$ and $d_r$ , such that $y=2$ . to fix this problem , we might try $$ \mathcal{l} \supset^ ? _ ? -y_u \bar q \phi^* u_r + \text{h . c . } $$ this is $u ( 1 ) $ invariant because the hypercharge of $\phi^*$ is $-1$ , so we have $-\frac13-1+\frac43=0$ , but now it is no longer $su ( 2 ) $ invariant ( $2^*\times2^*$ ) . now we use the property that $i\tau_2\phi^*$ trasforms in the same way under $su ( 2 ) $ as $\phi$ finally , we can write $$ \mathcal{l} \supset -y_u \bar q i\tau_2 \phi^* u_r + \text{h . c . } $$ which is indeed fully gauge invariant .
the reason why the aft ends of airplanes are streamlined is to preserve a smooth flow of air . just as the fore ends of airplanes are streamlined to smoothly cleave the air , so too the aft ends are streamlined to smoothly reintegrate the flows . turbulence is bad , regardless of where on the aircraft it occurs . note that there is more turbulence behind the stumpy shape than the longer one , despite the fact that the front ends are shaped the same . that is why the back ends of aircraft are streamlined .
why do you want to renormalize a theory that gives uv finite answers ?
one of the standard texts for this kind of thing ( the quantum mechanics of lasers , without all the technical details you had need to know to design a real one ) is loudon 's quantum theory of light . i have got the 2nd edition , i think he is up to the 3rd . in my edition he does the " pre-quantum " explanation of lasers ( i.e. . in terms of einstein a and b coefficients ) in about 40 pages . over the next 200 pages or so he does the fully quantum treatment of light ( quantization of the field , creation and annihilation operators , etc ) and then revisits the laser . the first 40 pages should cover the " clearly written description " criteria , and - as far as it goes - it is relatively rigorous as well .
momentum is transferred in both cases ( conservation of momentum is absolute ) , but how much ? figure the momentum of the two projectiles . the electron is fully relativistic , so you get $$p^2 = e^2 - m^2 = \left ( 15.0^2 - 0.5^2\right ) \ , \mathrm{mev}^2$$ or $p \approx 1.5\times 10^{7}\ , \mathrm{ev}$ ( in $c = 1$ units , of course ) . the bowling ball is fully classical , so you get $$p^2 = 2me$$ with a mass in the $10^{36}\ , \mathrm{ev}$ range . so $p \approx 6 \times 10^{21}\ , \mathrm{ev}$ . the bowling ball transfers roughly 14 orders of magnitude more momentum than the electron ! the energy from the electron ending in thermal modes because there are not any other places for it to go . figuring it again in mks units is left as an exercise for the interested student .
this is a basic example of a bch formula . there are many ways to prove it . for example , write the exponential as $$ \exp ( \mu x + \mu y ) = \lim_{n\to \infty} \left ( 1 + \frac {\mu x+ \mu y}n\right ) ^n = \dots $$ because the deviations from $1$ scale like $1/n$ , it is equal to $$ = \lim_{n\to \infty} \left [ \left ( 1 + \frac {\mu x}n\right ) \left ( 1 + \frac {\mu y}n\right ) \right ] ^n $$ now , we need to move all the $x$ factors to the left and $y$ factors to the right . each factor $1+\mu x / n$ commutes with itself , and similarly for $1+\mu y/n$ , of course . however , sometimes the $y$ factor appears on the left of the $x$ factor and we need to use $$ \left ( 1 + \frac {\mu y}n\right ) \left ( 1 + \frac {\mu x}n\right ) = \left ( 1 + \frac {\mu x}n\right ) \left ( 1 + \frac {\mu y}n\right ) \left ( 1 - \frac{\mu^2 ( xy-yx ) }{n^2} \right ) $$ plus terms of order $o ( 1/n^3 ) $ that will disappear in the limit . the only thing we need to count is the number of such permutations of the $ ( 1+\mu y/n ) $ factors with the $ ( 1+\mu x/n ) $ factors . it is not hard . an average $ ( 1+\mu y/n ) $ factor stands on the left side from $n/2\pm o ( 1 ) $ of the $ ( 1+\mu x/n ) $ factors , and there are $n$ factors of the form $ ( 1+\mu y/n ) $ , so we produce $n^2/2\pm o ( n ) $ factors of the form $$ \left ( 1 - \frac{\mu^2 [ x , y ] }{n^2} \right ) $$ which is a $c$-number that commutes with everything . now we just collect the factors inside the limit . on the left , we see $n$ factors $ ( 1+\mu x/n ) $ which combine to $\exp ( \mu x ) $ , then we have on the right from them $n$ factors with the $y$ that combine to $\exp ( \mu y ) $ , and then there are $n^2/2$ factors of the form $ ( 1-\mu^2 [ x , y ] /n^2 ) $ which , in the $n\to \infty $ limit , combine to $\exp ( -\mu^2 [ x , y ] /2 ) $ .
the phenomenon of parallax itself is simply the result of different views of one 's surroundings observed from different locations . for binocular vision , we get a different view of our environment from each eye simultaneously , and the visual cortex of our brains learns very early on how to process the two distinct images into an appearance of a three-dimensional image . ( actual physical interaction with the objects in the environment is also important in " educating " the visual cortex about distances to objects . we generally accomplish this during infancy . ) in the case of astronomical parallax , celestial objects are generally so distant that the two ( or more ) different views must be obtained at different times from different positions of the earth on its orbit . ( the exception is the moon , which is close enough to show parallax in simultaneous observations from well-separated points on the earth . as an example , this is why occultations of stars by the moon are not seen by all observers on the otherwise proper side of the earth . ) to note the presence of astronomical parallax , it is necessary to have very precise measurements of the direction in which the object is seen , relative to a much more distant " background " . the fact that this is not observed with the naked-eye was used until the 18th century as an argument against the motion of the earth about the sun , since ( obviously ) the planets and stars would show parallax if the earth were not simply stationary at the center of the universe . one difficulty in measuring parallax for stars is that , as we now know , the change in observed direction , even at diametrically opposite locations on the earth 's orbit , is at most a bit over one second of arc ( 1/3600 of a degree ) for the closest stars and generally hundredths of a second or less for most of the visible stars . but even to discern that requires being able to compare the observed directions of the stars at different times . in the age before photography , this required having precise charting of the stars on the sky , which was the result of a tremendous effort in the centuries after the first use of telescopes in astronomy . it was not until 1838 that the first sufficiently accurate observations of stellar parallax were accomplished . by comparison , the aberration of starlight due to the earth 's motion is an effect about twenty times larger than the largest stellar parallaxes , so it was became possible to detect that by 1725 .
neglecting friction , the force experienced is the centrifugal force $f=\frac{mv^2}{r}$ ( it would be less if you included friction since the car actually slips ) vectorially added to the orthogonal gravitational force $f_g=mg$ , i.e. $f = m\sqrt{\left ( \frac{v^2}r\right ) ^2 + g^2}$ where $g = 9.81 \frac{m}{s^2}$ 1 . divide this by $f_g$ to obtain a result in gs . remember to use si-units , i.e. divide a km/h speed by 3.6 ( 1000 km/m / 3600 s/h ) to obtain m/s . for the 350 m curve and 285 km/h that yields about 2.08 gs only , to obtain the 5.5 gs mentioned a radius of about 118 m is required , or some higher velocity ( 490 km/h for the 350 m curve ) . 1 ) thanks j.h. for this important correction !
mad props for a cool question . i am going to justify essentially the converse of the statement because it does not make much sense to talk of the temperature of a system that is in a pure state . let 's assume that we are talking about a quantum system with disrete energy spectrum ( with no accumuation points ) in thermal equilibrium . let $\beta = 1/kt$ be the inverse temperature . then recall that the boltzmann distribution tells us that the population fraction of systems in the ensemble corresponding to energy $e_i$ is given by $$ p_i = \frac{g_ie^{-\beta e_i}}{z} $$ where $g_i$ is the degeneracy of the energy level . in particular , note that the relative frequency with which energies $e_i$ and $e_j$ will be found in the ensemble is $$ p_{ij} ( \beta ) = \frac{g_ie^{-\beta e_i}}{g_je^{-\beta e_j}} = \frac{g_i}{g_j}e^{-\beta ( e_i - e_j ) } $$ in particular , let $i=0$ correspond to the ground level , then the frequency of any level relative to the ground level is $$ p_{i0} ( \beta ) = \frac{g_i}{g_0}e^{-\beta ( e_i - e_0 ) } $$ notice that since the ground level has the lowest energy by definition , we have $e_i - e_0 \leq 0$ , but zero temperature corresponds to the limit $\beta \to \infty$ , and we have $$ \lim_{\beta\to 0 } p_{i0} ( \beta ) = \delta_{i0} $$ in other words , at zero temperature , every member of the ensemble must be in the ground energy level ; the probability that a system in the ensemble will have any other energy becomes vanishingly small compared to the probability that a member of the ensemble has the lowest energy .
just as your quote says , infinity turns up as part of the answer in almost any simple-minded calculation in quantum field theory that goes past the lowest level of approximation . this does not affect quantum mechanics , but it is been there in the mathematics of qft since the late 1920s . feynman , together with a few other people at the end of the 1940s , introduced a less simple-minded way to calculate in qft that sort-of-bypasses the infinities , but no-one who is mathematically inclined could feel very comfortable with the way it was done at that time . many physicists , however , perhaps most , are content nowadays with the mathematics of what is called the renormalization group . the wikipedia page will give you a taste of this , but i doubt anyone is going to be able to give you a short tutorial on the subject that will make you very happy . the renormalization group lets one organize the calculations rather more nicely . the renormalization group draws on lattice methods from classical statistical physics , which i think contributed to physicists feeling relatively comfortable with the mathematics . there is definitely a group of people who think the current mathematics is still " dippy " , but no-one has yet produced an acknowledged serious alternative . there are also less disgruntled efforts just to improve the mathematics more-or-less incrementally , one strand of which uses hopf algebras , which is quite abstract mathematics that no-one could call careless ; one always wants improvement .
ok , i will start . definition and list ( filled with mistakes ) . please , comment/add/edit/etc . definition ${lim}_{x\to\infty} p ( x ) = p ( x+t ) , $ for $\forall\ ; t &gt ; 0$ $p ( x ) $ is finite i.e. $p ( x ) = \int p ( x ) dx = $ const . clarifying definitions $p ( x ) :=$ " probability density function of measurable $x \in \re_+$" $p ( x ) :=$ " cumulative distribution function " = $\int_{0}^x p ( x ) \ ; dx$ $a &gt ; 0$ $0 &lt ; f ( x ) &lt ; const $ $g ( x ) \ne 0$ long tailed all exponential distributions : $p = f ( x ) exp ( -ax ) $ all power law distributions : $p = x^{-\alpha}$ , where $\alpha &lt ; -1$ all " stretched exponentials": $p = f ( x ) exp ( -ax^{g ( x ) } ) $ non long tailed uniform distribution : p ( x ) = const all " increasing or equal " distributions : $|p ( x ) | \le |p ( x+a ) | $ for $\forall a$ power law distributions with $p ( x ) = x^\alpha$ , where $-1 \le \alpha \le 0$ ( integral not finite )
information about double slit experiment : in double slit experiment , light behaves like a wave when it is not observed ( in simple words you do not know in which slit photon passed through ) , because it is a wave , when two waves meet each other they will interferece and on detector screen you will see interference pattern like this : but if you will put detectors on the slits , you will observe in which slit each photon passed though , and because of that interference pattern will disappear , because by observing light , it no longer behaves like a wave ( in other words it is wavefunction has collapsed ) instead it behaves like a particle ( photon ) so , on a detector screen you will no longer see interference pattern example of quantum eraser : easiest way to do that is to put polarizators on each slit which are opposite of each other ( in other words first polarizator will pass photons which have polarization $\rightarrow$ and second will pass photons which have polarization $\uparrow$ ) : because photons which have passed polarizators have different polarizations they can no longer interference ( see : interference of polarized light ) , so on detector screen you will no longer see interference pattern links for more info : for more information about quantum eraser see this page , this page and this page for a video about quantum eraser see this page for more info about interference see this page
a very sensitive device would be required to measure the minuscule change in the water depth along the glass walls , because the differences in the strength of the gravitational field between each side of the glass are essentially zero . because of this the force exerted on the glass is the same , but due to the small volume of water in a glass as opposed to an ocean it would be very hard to measure any change . however , the difference in the strength of the gravitational field between the side of the earth closest to the moon and furthest to the moon is enough to pull water more towards the side close to the moon . it is also worth nothing that the surface tension , at these small sizes , will have a much more significant effect than tidal forces , further hindering the measurements .
it is the result of a dependence of the pressure with growing depth , due to the gravitational field ( i.e. . the weight of the water ) . you may do an easy calculation with some simple geometrical form , e.g. a cylinder totally submerged in water , to quickly understand how it works . the force due to pressure in each surface element of the curved wall of the cylinder is proportional to the depth of that element , and has the normal direction to the wall , i.e. towards the axis of the cylinder . after an easy integration in polar coordinates , you can see that the resultant force points upwards . that is because the forces in the upper parts are smaller that the ones near the more deeply submerged part of the cylinder . a surprising conclusion is that a golf ball submerged in a tank of water in the space station , would not go upwards . . . or that the bubbles in a coke in the hands of an astronaut remain where they origin . . . i would love to see that .
something not right with your third step , you have : $\ddot{x}=-\frac{1}{2x}\rightarrow\dot{x}\ddot{x}=-\frac{\dot{x}}{2x}\rightarrow\frac{1}{2}\left ( \dot{x}^{2}\right ) ^{ . }=-\frac{1}{2}\left ( \ln\left ( x\right ) \right ) ^{ . }\rightarrow\dot{x}^{2}=-\ln\left ( x\right ) +c\rightarrow t=\int\frac{dx}{\sqrt{c-\ln\left ( x\right ) }}=\left|\begin{array}{c} c-\ln\left ( x\right ) =z\\ dx=-e^{c-z}dz \end{array}\right|=-e^{c}\int e^{-z}z^{\frac{1}{2}-1}dz=-e^{c}\gamma ( \frac{1}{2} ) $ now just apply your boundary conditions .
dear rajesh , the goal as well as main achievement of string theory is not to " achieve spacetime quantization " whatever this phrase is supposed to mean but to allow calculations about spacetime that are compatible with the postulates of quantum mechanics . it is not the same thing . the statement that " spacetime is quantized " , whatever it means , is just a working hypothesis , not a holy grail that should be " achieved " . instead , what is needed is to have a theory that is a quantum theory as a whole , i.e. one that agrees with the uncertainty principle , probabilistic character of predictions , observables ' being expressed by linear operators on the hilbert space , and so on . on the other hand , the spacetime is an approximate concept in string theory . at distances much longer than the characteristic distance scale of string theory , the string scale ( or the related gravitational planck scale ) , classical general relativity is a good approximation - and some of its aspects remain exact at all distance scales . at distances comparable to the string scale ( or planck scale ) , entirely new set of physical phenomena take over . it is not true that the only difference of these new phenomena from classical general relativity is that " spacetime is quantized " . and in fact , it is not true in any sense and it cannot be true in a consistent theory of quantum gravity that spacetime becomes discrete . and in fact , geometric quantities - while remaining continuous whenever it makes sense to use them - may be shown in string theory to be invalid variables to describe physics at very short distances . to say the least , they are incomplete . so if your question is how string theory confirms the prejudices and beliefs that the spacetime quantities survive as good variables up to arbitrarily short distance scales and/or that they become discrete , the answer is that string theory is a way to prove that both of these prejudices are dead wrong . if your question is which quantum phenomena affecting spacetime are predicted by string theory , it is a valid question but it is too broad and a proper answer would require to review all of string theory - because all important insights of string theory , in some sense , show the consequences of the co-existence of quantum mechanics with a dynamical spacetime .
just write the transformation in its differential form and then rearrange . $$ d\eta = \frac{u_e}{\sqrt{\int_0^x\rho_e u_e \mu_e dx}}\rho dy $$ rearranging and integrating yields : $$ y = \frac{\sqrt{\int_0^x\rho_e u_e \mu_e dx}}{u_e}\int_0^\eta \frac{1}{\rho}d\eta $$
it is important to distinguish between three group actions that are named " galilean": -the galilean transformation group of the eucledian space ( as an automorphism group ) . -the galilean transformation group of the classical phase space ( whose lie algebra constitute a lie subalgebra of the poisson algebra of the phase space ) . this is the classical action . -the galilean transformations of the wavefunctions ( which are infinite dimensional irreducible representations ) . this is the quantum action . only the first group action is free from the central extension . both classical and quantum actions include the central extension ( which is sometimes called the bargmann group ) . thus , the central extension is not purely quantum mechanical , however , it is true that most textbooks describe the central extension for the quantum case . i will explain first the quantum case , then i will return to the classical case and compare oth cases to the poincare group . in quantum mechanics , a wavefunction in general is not a function on the configuration manifold , but rather a section of a complex line bundle over the phase space . in general the lift of a symmetry ( an automorphism of the phase space ) is an automorphism of the line bundle which is therefore a $\mathbb{c}$ extension of the automorphism of the base space . in the case of a unitary symmetry , this will be a $u ( 1 ) $ extension . sometimes , this extension is trivial as in the case of the poincare group . now , the central extensions of a lie group $g$ are classified by the group cohomology group $h^2 ( g , u ( 1 ) ) $ . in general , it is not trivial to compute these cohomology groups , but the case of the galilean and poincare groups can be heuristically understood as follows : the application of the galilean group action $\dot{q} \rightarrow \dot{q}+v$ to the non-relativistic action of a free particle : $s = \int_{t_1}^{t_2}\frac{m }{2}\dot{q}^2dt$ , produces a total derivative leading to $s \rightarrow s + \frac{m}{2}v^2 ( t_2-t_1 ) + mvq ( t_2 ) - mvq ( t_1 ) $: now since the propagator $g ( t_1 , t_2 ) $ transforms as $ exp ( \frac{is}{\hbar} ) $ and the inner product $\psi ( t_1 ) ^{\dagger} g ( t_1 , t_2 ) \psi ( t_2 ) $ must be invariant , we get that the wavefunction must transform as : $\psi ( t , q ) \rightarrow exp ( \frac{i m}{2\hbar} ( v^2 t+2vq ) \psi ( t , q ) $ now , no application of a smooth canonical transformation can romove the total derivative from the transformation law of the action , this is the indication that the central extension is non-trivial . the case of the poincare group is trivial . the relativistic free particle action is invariant under the action of the poincare group , thus the transformation of the wavefunction doen not acquire additional phases and the group extension is trivial . classically , the phase space is $t^{*}r^3$ and the action of the boosts on the momenta is given by : $p \rightarrow p + mv$ , thus the generators of the boosts must have the form $k = mvq$ , then the action is easily obtained using the poisson brackets{q , p} = 1 , and the poisson bracket of a boost and a translation is non-trivial {k , p} = m . the reason that the lie algebra action acquires the central extension in the classical case is that the action is hamiltonian , thus realized by hamiltonian vector fields and vector fields do not commute in general . the iwasawa decomposition of the lorentz group provides the answer to your second question : $so^{+} ( 3,1 ) = so ( 3 ) a n$ where $a$ is generated by the boost $m_{01}$ and $n$ is the abelian group generated by $m_{0j}+m_{1j}$ , $j&gt ; 1$ . now both subgroups $a$ and $n$ are homeomorphic as manifolds to $r$ and $r^2$ respectively . to your third question : the limiting process which produces the galilean group from the poincare group is called the wingne-inonu contraction . this contraction produces the non-relativistic limit . its relation to quantum mechanics is that there is a notion of contarction of lie groups unitary representations , however not a trivial one . update in classical mechanics , observables are expressed as functions on the phase space . see for example chapter 3 of ballentine 's book for the explicit classical realization of the generators of the galilean group . this is a case where the full geometric quantization recepie can be carried out . see the following two articles for a review . ( the full proof appears in page 95 of the second article . the technical computations are more readable in pages 8-9 of the first article ) . the central extensions appear in the process of prequantization . first please notice that the hamiltonian vector fields $x_f$ corresponding to the galilean lie algebra generators close to the non-centrally extended algebra , ( because , the hamiltonian vector field of constant functions vanish ) . however , the prequantized operators $\hat{f} = f - i\hbar ( x_f - \frac{i}{\hbar}i_{x_f} \theta ) $ , ( $\theta$ is a symplectic potential whose exterior derivative equals the symplectic form ) close to the centrally extended algebra because their action is isomorphic to the action of the poisson algebra . the prequantized operators are used as operators over the hilbert space of the square integrble polarized sections , thus they provide a quantum realization of the centrallly extended lie algebra . regarding your second question , the wingne-inonu contraction acts on the level of the abstract lie algebra and not for its specific realizations . a given realization is termed " quantum " , if it refers to a realization on a hilbert space ( in contrast to realization by means of poisson brackets , which is the classical one ) .
nothing - that is the correct definition . one little caveat is that small systems are usually in contact with a larger system with a temperature that is more easily controlled or measured ( the " heat bath" ) , and $t$ usually stands for the temperature of the heat bath rather than the small system itself . however , this does not make a lot of difference in practice , because the small system will very rapidly attain the same temperature as the heat bath anyway .
the molecules $o_2$ and $n_2$ are symmetric and have no dipole momentum . that is why they can not interact with emw ( at least within dipole approximation ) . one can say that transitions between the oscillator levels in these molecules are forbidden by symmetry in electrodipole approximation . the molecule $co$ consists of two different atoms . the average positions of positive and negative charges are not the same . this molecule is polar .
thanks for asking this question . it is something we all assume to be obviously trivial and often skip . your question made me think and i was not sure whether the values for luminosities listed in wikipedia were in the optical range , or the bolometric luminosity i.e. the luminosity over all wavelengths . a little bit of googling led me to this page , where this question seems to have been discussed well and also resolved .
no , there is no need for screens in the movie theaters to be mirrors i.e. specular reflectors . quite on the contrary , it is completely necessary for them not to be mirrors i.e. to be diffuse reflectors . if the screen were a specular reflector , the light would return back into the direction of the projector and would never reach the eyes of the viewers who are not sitting on the line in which the projector is directed . if the screen were a mirror , the viewers would only see themselves and the projector but could not see any magnified versions of the objects that are supposed to be in the movie . in reality , each point of the screen – which is a diffuse reflector – effectively becomes a source of light whose intensity depends on the amount of incident light at this point and this source is located directly in the plane of the screen . so these sources of light are not images ( in the sense of real or virtual images of mirrors or lens ) at all . more precisely , the only image of the real " object " – the object on the screen – is formed in the viewers ' eyes . it is important for the projector to sharply illuminate each point of the screen differently , by the correct intensity of light of the right color . this requires precise optics that chooses the right directions of light rays for each point of the movie between the projector and the screen . on the other hand , each point of the screen is a diffuse reflector and much like real objects in the real world , it emits light to all directions so that all viewers may see it , regardless of the location of their chair .
using the notation : $ h = h_0 + v$ and $ \mathbf{k} = \mathbf{k_0} + \mathbf{z}$ the requirement that both free and interacting generators satisfy the poincaré algebra commutaion relations , lead to the following requirements : ( please see the following book by eugene stefanovich ( published in the arxiv ) equations 6.22-6.26 ( page 179 ) : $ [ \mathbf{j} , v ] = [ \mathbf{h_0} , v ] = 0$ $ [ p_i , z_j ] = i \delta_{ij} v , [ j_i , z_j ] = i \epsilon_{ijk} z_k$ $ [ k_{0 [ i} , z_{j ] } ] + [ z_i , z_j ] =0 $ , $ [ \mathbf{z} , h_0 ] + [ \mathbf{k_0} , v ] + [ \mathbf{z} , v ] = 0$ ; now , to verify that these relations are satisfied in the present case , please observe that due to the first given realtion expressing the transformation properties of the interaction hamiltonian density that the action of the free poincaré generators on the hamiltonian density is by means of the well known differential operator realization : [ $h_0 , \mathcal{h} ( \mathbf{x} , 0 ) ] = ( \frac{\partial}{\partial t} \mathcal{h} ) ( \mathbf{x} , 0 ) $ $ [ p_i , \mathcal{h} ( \mathbf{x} , 0 ) ] = i \frac{\partial}{\partial x_i} \mathcal{h} ( \mathbf{x} , 0 ) $ $ [ j_i , \mathcal{h} ( \mathbf{x} , 0 ) ] = i\epsilon_{ijk} x_i \frac{\partial}{\partial x_j} \mathcal{h} ( \mathbf{x} , 0 ) $ $ [ k_{0i} , \mathcal{h} ( \mathbf{x} , 0 ) ] = ( ( t \frac{\partial}{\partial x_i} - x_i\frac{\partial}{\partial t} ) \mathcal{h} ) ( \mathbf{x} , 0 ) = -x_i ( \frac{\partial}{\partial t} \mathcal{h} ) ( \mathbf{x} , 0 ) $ what is left is to perform the substitutions . but in the addition of the given requirements , one must assume that the interaction hamiltonian density vanishes sufficiently rapidly at infinity , and surface terms in integration by parts can be ignored . ( of course these are operators and one must specify the strength of convergence ) . here is a sample calculation of one of the required commutation relations : $ [ j_i , z_j ] = \int x_j i \epsilon_{ilm} x_l \frac{\partial}{\partial x_m} ( \mathcal{h} ( \mathbf{x} , 0 ) ) d^3x$ please observe that the poincaré generators do not act on the free $x_i$ 's in the integrand , because they are only dummy integration variables . thus after integration by parts we get : $ [ j_i , z_j ] = -\int i \epsilon_{ilm} ( \delta_{jm} x_l + \delta_{lm} x_j ) \mathcal{h} ( \mathbf{x} , 0 ) d^3x = + i \epsilon_{ijl} x_l \mathcal{h} ( \mathbf{x} , 0 ) d^3x = i \epsilon_{ijk} z_k$ finally , please let me remark that finding an exact representation of the interacting poincaré algebra would require knowing the exact solution ( hilbert space and operator eigenvalues ) of the interacting quantum field model , which is not known outside perturbation theory , however , the interacting poincaré generators can be deduced form the lagrangian by means of noether 's theorem + canonical quantization . the forms of interacting poincare algebra were already studied by dirac in 1949 .
a knot always requires the rope in the knot to be curved . this increases the stress on the outside of the curved bit of rope , and decreases the stress in the inside . this increase in the stress in a knot means the rope breaks at a lower overall stress than a straight rope would .
yes . let 's assume that the charge density is fixed relative to the surface of a sphere of radius $r$ , then spinning the sphere with angular velocity $\vec\omega = \omega \hat{z}$ would create a surface charge density $\vec k$ given by $$ \vec k ( \theta , \phi ) = \omega r\sin\theta\sigma ( \theta , \phi ) \hat\phi ( \theta , \phi ) $$ where $\theta$ and $\phi$ are the polar and azimuthal spherical coordinates .
the elements of this particular metric tensor do not depend on time . $dx^j$ and $dt$ are treated as constants when you take the time derivative . you should also have $\gamma^{\lambda}_{00} = -\frac{1}{2} g^{\nu \lambda} \frac{\partial g_{00}}{\partial x^\nu}$ .
well the real question should be why is there a °c ( celsius ) . the celsius scale is a " centigrade " scale in that it uniformly divides the temperature range between the boiling point of water , and the freezing point of water into 100 equal parts , and then it arbitrarily calls the freezing point zero °c , and the boiling point becomes 100°c . the kelvin scale is referenced to the triple point of water , not the freezing point , and that temperature is about 0.1°c ( it might be 0.098°c but i am not sure about that ) . quite arbitrarily , it was decided that degrees on the kelvin scale , should be identical in size to celsius degrees , and experimentally the zero on the kelvin scale ( zero kelvins ) is 273.16 celsius degrees below the triple point of water , which makes it also -273.15°c
your intuition is correct : the top loop inductor prevents its current from changing instantaneously , so it starts at zero . qualitatively , the top loop current : starts at zero ( here the inductors block current flow in both loops . ) rises to a max as the top loop uncoupled inductor allows current to flow through it while its source voltage ( the coupled inductor voltage ) simultaneously falls as the bottom loop current rises . decays to zero as the coupled inductor voltage falls to zero . an equivalent circuit model does not contain all 3 inductors in series , but rather two parallel branches ( corresponding to the primary and secondary of the coupled inductor ) , with the " secondary " branch ( corresponding to the top loop ) containing an inductance which blocks initial current flow .
it wont be unboundedly fast e.g. due to the viscosity of the water . in addition , i would like to point out that you will not have the cylinder of water . water will accelerate with gravity as it falls . the density of water is constant . the total flow of the water at different height is constant as well . so the area of the water 's cross-section will decline . for thin stream of water from a kitchen tap , surface tension force is enough to keep the water flowing in one stream , but the thickness of the scream decreases , take a look here . however , as you increase the initial radius of your stream , the water mass ( ~r^2 ) grows while the pressure gradient created by surface tension forces ( ~1/r ) declines . that is why the cylinder will pretty quickly separates into multiple streams , and eventually into droplets . .
one perspective ( heh ) involves the following relation among position vectors : $$\vec{r}_{a\rightarrow c} = \vec{r}_{a\rightarrow b} + \vec{r}_{b\rightarrow c} . $$ these position vectors can be for anything ; object $a$ could be a house , object $b$ an ant , and object $c$ a leaf on the river . here 's a diagram to help : so if you want to know the position of object $c$ relative to object $a$ ( the bold dark arrow ) , you just have to know the position of some other object $b$ relative to those others . to answer your question , you can apply this same idea to the solar system : $$\vec{r}_{\mathrm{sun} \rightarrow x} = \vec{r}_{\mathrm{sun} \rightarrow \mathrm{earth}} + \vec{r}_{\mathrm{earth} \rightarrow x} . $$ or in pictures : the position of planet $x$ relative to the sun ( bold dark arrow , which is what we want ) can be found if we know earth 's position relative to the sun and planet $x$ 's position relative to earth . in this way , measurements of a planet 's position as measured from here on earth can be used to get a map of the solar system . . there is the added complication of knowing distance to planets and coming up with a convenient coordinate system in order to actually come up with values for these position vectors . others may have better information on that .
the $\frac{e^2}{4\pi\epsilon_0}\frac{1}{r_0}$ term appears in the potential for the electron motion , as luboš and vijay point out , to keep the whole energy accounting in place so that the nuclear motion can be properly quantized . the key point is that this potential does not involve the electron coordinates , so that as far as the electronic wavefunction is concerned it acts like a constant and therefore does not affect the solution to the electronic eigenvalue equation . if you do include that term , and write the potential energy of the molecule as $$e_p=-\frac{e^2}{4\pi\epsilon_0}\left ( \frac{1}{r_1}+\frac{1}{r_2}-\frac{1}{r_0}\right ) $$ then you can write the potential for the nuclear coordinates directly as $$e_n=\langle\psi ( \mathbf{r}_1 , \mathbf{r}_2 ) |e_p|\psi ( \mathbf{r}_1 , \mathbf{r}_2 ) \rangle$$ where the total wavefunction is split into electronic and nuclear parts as $$\langle \mathbf{r} , \mathbf{r}_1 , \mathbf{r}_2|\psi\rangle=\langle\mathbf{r}|\psi ( \mathbf{r}_1 , \mathbf{r}_2 ) \rangle\langle\mathbf{r}_1 , \mathbf{r}_2|\phi\rangle . $$ the schrödinger equation for the nuclear coordinates is then $$\left ( \frac{\mathbf{p}_1^2}{2m}+\frac{\mathbf{p}_2^2}{2m}+e_n\right ) |\phi\rangle=e|\phi\rangle . $$
in this phd thesis ( unfortunately in german :-/ , a follow up paper can be found here ) , it is shown how for an action of a field theory containing even and grassman fields , the renormalization group equation can be solved numerically ( after expanding the action in derivatives and the fields ) . to investigate the corresponding renormalization group flow in the respective coupling space , analogous methods known to investigate trajectories in the phace space of a nonlinear dynamic system can be applied in this case too . a trajectory in this coupling space starting with certain inital values of the coupling constants at the high energy cutoff evolving in the course of renormalization time ( which is related to the lenght or energy scale considered ) can be considered as analogue to a trajectory in phase space , starting from an initial condition and evolving in the course of time . as described in this paper , the coupling constants in a renormalizable field theory can not only flow to , bypass , spiral in/out or circle around isolated fixed points , but a chaotic behaviour of the renormalization group flow could occur too . this could have potential applications in different fields such as spin glasses , neural networks , or even string theory .
1 ) since u5 and i5 are given we can use ohm 's law to calcualte r5=90ω . good first step . 2 ) knowing u6 we can use the mesh-current method to determine u7=1v . knowing u6 , you have u7 by inspection - u6 and u7 are identical as these voltage variables are across the same two nodes . 3 ) again i used ohm 's law to calculate i7=80ma with u7=1v from 2 ) i7 is given as 80ma . no calculation required . what you should do here instead is calculate r7 = 1v / 80ma = 12.5 ohms if you need it . 4 ) ohm 's law to determine i6=10ma since we have u6 , r6 given . correct . now , you know i8 since , by kcl , i8 = i5 + i6 + i7 = 100ma . and now the dominoes topple . by kvl , u0 = u6 + u8 -> u8 = 1v . the value of r8 is given by ohm 's law : r8 = 1v / 100ma = 10 ohms . again , by kvl , u0 = u1 + u5 + u8 -> u1 = 0.1v now , since u1 = u2 = u3 = u4 , by inspection , we know i1 , i2 , and i3 using ohm 's law . by kcl , i5 = i1 + i2 + i3 + i4 . solve for i4 to get i4 = 2ma -> r4 = 50 ohms .
i do not know about your specific example of impurities in si , but in general temperature does effect electronic excitations and energy levels . by the equipartition theorem , temperature makes additional energy on the order of $k_b t$ available , which can effectively reduce such things as ionization energies . note , however , that $k_b t \approx 25$ mev for $t = 300$ k is small compared with e.g. the bandgap of si , which is $\sim 1.1$ ev . whether or not this will contribute to the ionization of impurities depends on the binding energies of the valence electrons in those impurities . there are also indirect effects of temperature on electronic structure . in particular , increased temperature typically causes materials such as si to expand . changes in bond lengths are necessarily accompanied by changes in the probability density of the electrons . thus , expanding and contracting bond lengths give an indirect dependence of electronic structure with temperature . this , however , is also typically a " small " effect , unless the material in question goes through a phase transition . temperature dependence as a whole is very often neglected in electronic structure calculations , which typically focus on the ground-state configuration .
permittivity is a macroscopic property of matter - it is a consequence of the way material is polarized in the presence of an electric field . the properties of an atomic bond are determined by the atoms participating in the bond , the molecular structure in which they find themselves , and ( to a lesser extent ) the presence of a magnetic and / or electric field that is strong enough to affect the energy states of the orbitals . if i understand your question correctly , you are asking about the influence of material outside of a ( macroscopic ) rod on the inter-atomic bonds . i believe that if there were any effect at all , it would be impossibly hard to measure . immersing a polymer ( especially nylon ) in a liquid can cause significant dimensional changes due to the absorption of water . but permittivity - no , it will not affect the rod .
i start my answer with the the second question ( similar situations ) . this is eerily similar to the free-particle dispersion relation ( or the relativistic energy-momentum relation ) in natural units ( $c = 1$ and $\hbar = 1$ ) , $$\omega^2 = k^2 + m_0^2$$ this system of units is commonly adopted in particle physics and quantum field theory . it can be clearly discerned that in this system of units , the characteristic '' compton wavelength '' of the the particle would correspond to the inverse of mass ( $\lambda_0 \sim 1/m_0$ ) , or strictly speaking have a factor of $2\pi$ in between . if you write in terms of the wave number $k_0 = 2\pi/\lambda_0 = m_0$ , we recover exactly the equation you have , $$\omega^2 = k^2 + k_0^2$$ , which means that your ''intrinsic length-scale'' is getting decided by the most fundamental ''intrinsic'' property of the particle - its mass . of course , then , we expect these systems to have similarities , when $k &gt ; &gt ; k_0$ , ( short wavelength limit ) , the relation is approximately $\omega^2 =k^2$ , since the second term on rhs is negligibly smaller in comparison to the first . thus , $\omega = k$ ( maybe that is what you mean by linear dispersion ) , hence $v_{\rm phase} = \omega/k = {\ \rm constt}$ , and hence $v_{\rm group} = d\omega/dk = 0$ , which is what you get . when $k &lt ; &lt ; k_0$ ( long wavelength limit ) , the relation is approximately $\omega = k_0$ . then , $\omega/k = k_0/k = $ very large , or infinity in the limiting case . the point here is that large or small depends on how $\lambda$ compares with the intrinsic scale , which is decided by the particle 's mass . so , your system is behaving the more or less the same as a free particle . that may or may not be profound , but nothing more can be said unless you mention the context . hope this helps : )
if you write $$f_{\mu\nu} = \partial_{ [ \mu}a_{\nu ] } $$ then your lagrange density is $${ \mathcal{l}} = \epsilon^{\mu\nu\alpha\beta}\partial_{ [ \mu}a_{\nu ] }\partial_{ [ \alpha}a_{\beta ] }$$ now we want a vector whose divergence is ${ \mathcal{l}}$ . the $\partial_{\mu}$ looks promising , so we ask if we can bring that outside so it acts on the remaining vector , i.e. $${ \mathcal{l}} = \partial_{\mu} ( \epsilon^{\mu\nu\alpha\beta}a_{\nu }\partial_{ [ \alpha}a_{\beta ] } ) \ \ ( 1 ) $$ we do not need to worry that we have lost the antisymmetrization brackets on $\mu$ and $\nu$ because the epsilon symbol forces this . as you pointed out , this is not quite what we started with because we have an additional term of the form $$ ( \epsilon^{\mu\nu\alpha\beta}a_{\nu }\partial_{\mu}\partial_{ [ \alpha}a_{\beta ] } ) $$ however the total antisymmetry of the epsilon symbol means we can treat the $\mu$ $\alpha$ $\beta$ contribution as $$\partial_{ [ \mu}f_{\alpha\beta ] } $$ which vanishes due to the maxwell equations . hence the anzatz ( 1 ) holds . ( the vector whose divergence we take looks like the abelian chern simons current ) .
the objects such as $\hat \phi ( x , y , z , t ) $ in a qft are strictly speaking " operator distributions " . they differ from " ordinary operators " in the same way how distributions differ from functions . only if you integrate such operator distributions over some region with some weight $\rho$ , $$\int d^3 x\ , \hat\phi ( x , y , z , t ) \rho ( x , y , z , t ) =\hat o , $$ you obtain something that is a genuine " operator " . in a free qft , the state vectors may be built as combinations of states in the fock space – an infinite-dimensional harmonic oscillator . but you may also represent them via " wave functional " . much like the wave function in non-relativistic quantum mechanics $\psi ( x , y , z ) $ depends on 3 spatial coordinates , a wave functional depends on a whole function , $\psi [ \phi ( x , y , z ) ] $ . for each allowed configuration of $\phi ( x , y , z ) $ , there is a complex number . yes , one may also integrate over all classical functions $\phi ( x , y , z ) $ . there also exists a dirac delta-like object , the dirac " delta-functional " , and it is usually denoted $\delta$ , $$\int {\mathcal d}\phi ( x , y , z ) f [ \phi ( x , y , z ) ] \delta [ \phi ( x , y , z ) ] = f [ 0 ( x , y , z ) ] $$ i wrote the zero as a function of $x , y , z$ to stress that the argument of $f$ is still a function . the functional integration is a sort of infinite-dimensional integration and the delta-functional is an infinite-dimensional delta-function . one must be careful about these objects , especially if we integrate amplitudes that may have amplitudes and especially if we integrate over curved infinite-dimensional objects such as infinite-dimensional gauge groups etc . – there may be subtleties such as anomalies . yes , the hilbert space of a free qft is still isomorphic to the usual hilbert space : there is a countable basis . but we are talking about the finite-energy excitations only . there are lots of " highly excited states " that are not elements of the fock space – one would need infinite occupation numbers for all one-particle states . physically , such states are inaccessible because the energy can not be infinite . however , when one is changing the energy from one hamiltonian to another ( e . g . by simple operations such as adding the interaction hamiltonian ) , finite-energy states of the former $h_1$ may be infinite-energy states of the latter $h_2$ and vice versa . so one must be careful : the physically relevant finite-energy hilbert space may be obtained from some infinite-occupation-number states in a different , e.g. approximate , hamiltonian . it is still true that the relevant hilbert space is as large as a fock space and it has a countable basis . the " totally inaccessible " states that are too strong deformations have an important example or name – they are " different superselection sectors " . rigor is a strong word . people tried to define a qft rigorously – by aqft , the algebraic/axiomatic quantum field theory . these attempts have largely failed . it does not mean that there is not any " totally set of rules " that qft obeys . instead , it means that it is not helpful to be a nitpicker when it comes to the new issues that arise in qft relatively to more ordinary models of quantum mechanics ; it is neither fully appropriate to think that a qft is " exactly just like a simpler qm model " but it is equally inappropriate to forget that it is formally an object of the same kind . formally , many things proceed exactly in the same way and there are also new issues ( unexpected surprises that contradict a " formal treatment" ) that have some physical explanation and one should understand this explanation . some of these new subtleties are " ir " , connected with long distances , some of them are " uv " , connected with ever shorter distances . the fact that a qft has infinitely many degrees of freedom is both an ir and uv issue . so even if you put a qft into a box , you will not change the fact that you need wave functionals , delta-functionals , and that there are superselection sectors and states inaccessible from the fock space . by the box , you only regulate the ir subtleties but there are still the uv subtleties ( momenta , even in a box , may be arbitrarily large ) . those may be regulated by putting the qft on a lattice . this has some advantages but some limitations , too .
the electric field assigns a single vector quantity to each point in space ( specifically , the direction in which a positive test charge would accelerate if it popped into existence at that point , assuming it did not perturb the setup creating the field in the first place ) . i believe that the difficulty of this question arises from an ambiguity in the problem statement : to which point ( s ) do the two vectors you have drawn assign electric fields ? they cannot both correspond to the single point p_lr at which the drawings of the left and right arrows intersect ( the head of one touching the tail of the other ) because this would imply that a test charge dropped at p_lr would accelerate simultaneously in two different directions ! you could adopt the convention of summing the accelerations a la newton ii , since two forces f1 , f2 that would have caused accelerations a1 , a2 if applied separately actually cause acceleration a1+a2 if applied simultaneously . but this would force you to admit , by the definition of the electric field , that e ( p_lr ) =e_l+e_r which points along the wire and does not violate the rule that the drawing was intended to be a counterexample of . we could adopt a different convention for deciding which points the arrows assign electric fields to ( perhaps taking the point to be the tail or the tip of each arrow ) . but this arrangement does not imply anything about current building up or not building up in either place . let me propose a slightly more formal version of your question : what prohibits the inside of a wire from sustaining a solenoidal ( circulatory ) vector field ? solenoidal vector fields do not converge or diverge ( lead to charge buildup ) anywhere , but they are not necessarily equal to zero . an example would be the velocity field of a 2d whirlpool . the answer : nothing ! you can generate one of these fields by accelerating a magnet through a pipe . the electrons are dragged around the pipe in circles , which produces a magnetic field opposing the field of the falling magnet . if you use an everyday conductive material like copper , a constant acceleration force like gravity will inevitably " win " , since resistance takes away some of the electrons ' ability to build a perfectly opposing magnetic field . as they rotate around the pipe in circles , their potential decreases to 0 , which means that there must have been a circulatory electric field in the wire . however , if you use a superconducting material , circulatory currents are able to perfectly oppose impinging magnetic fields , leading to the meissner effect ( superconducting levitation ) . if you made a superconducting wire and placed a magnet nearby , the wire would have all sorts of circulatory currents on the inside that did not point along the length of the wire , but these would not necessarily be due to electric fields since superconductors do not require electric fields to sustain currents . another possible way of obtaining circulatory fields in the wire would be a high-frequency alternating current ( google " skin effect " if you are interested ) , although the elementary description of the physics is a bit more complicated in that case . i would hazard a guess that the context of this question implicitly forbade arrangements consisting of moving magnets and alternating currents . mathematically , the condition of " no changing magnetic fields " implies that there are no circulatory electric fields . by the helmholtz decomposition ( vector fields can be written as a sum of an irrotational and a solenoidal field ) , the earlier argument that the current ( and therefore e-field , in a non-superconductor ) must be divergence-free suffices to prove that the field is constant , and the fact that it must have no component normal to the wire surface then suffices to prove that the field must be constant and point along the direction of the wire . i do not know what mathematical background you have , so if the talk about helmholtz decompositions did not ring a bell then you will have to wait for vector calculus . summary : the confusion was due either to a misunderstanding of the definition of " vector field " or due to implicit assumptions ( no changing magnetic fields , no changing electric fields ) in the problem statement . hope this helped : )
a black hole is created by stellar core collapse . its event horizon ( if that exists -hawking ) appears . by what mechanism and path , as viewed eternally , can accreted matter enter the black hole proper ? it externally appears to be asymptotically stuck within the event horizon . the inside of a black hole is speculative , certainly at the singularity ( if there is one ) , for the unknown local geometry of spacetime . adding mass to a black hole linearly increases its external radius . the external volume increases as the cube of the external radius , implying that galactic core supermassive black holes have average densities approaching zero . ngc 4889 is 2.1×10^10 solar masses , then $r = ( 2.95 ) ( m/m_{sun} ) $ km . the sun 's average density is 1409 kg/m^3 . calculate ngc 4889 's average external density . consider a kerr ( rotating ) black hole . if mass resides in a central zero-dimensional singularity , what sources the external angular momentum ? $l= r × mv$ . black hole interiors are not well-defined .
taking your question literally , you can see a single barium ion : the triµp group has achieved capturing a single barium ion in a paul trap . the images show coulomb crystals formed by a decreasing number of laser-cooled ions as detected with an emccd camera . this forms an important step towards the planned experiments on single radium ions to measure atomic parity violation and build an ultra-stable optical clock . they are in traps like this one : also , warren nagourney from washingtong university took a picture of a single barium atom scattering light from a laser : single trapped atom , glowing blue photo credit : warren nagourney at the university of washington , c . 2000 what is this ? believe it or not , this is a color photograph of a single trapped barium ion held in a radio-frequency paul trap . resonant blue and red lasers enter from the left and are focused to the center of the trap , where the single ion is constrained to orbit a region of space about 1 millionth of a meter in size . what is the red/blue mess on the sides ? low level out-of-focus laser scatter off of metal trap electrodes and accessories ( atom ovens , electron filaments , etc . ) as seen in this photo . how do we know the dot really is an atom ? when one turns off the red laser , the blue dot vanishes . this is because the scattering process requires both laser colors due to a metastable state in the barium ion . if the blue dot stayed around with the red laser off , we might excuse it as being additional laser scatter off some surface . how was the photo taken ? this is a scanned photo ; the camera was a 35mm nikon ( i believe ) with a wide open 50mm f/1.8 lens . the exposure time was two minutes . several shots were taken at different camera positions and this one caught the ion in the very narrow depth of field . is this how you normally " view " the ion ? no , we use a 50 mm f/1.8 camera lens to image the blue dot onto a photomultiplier tube . we do not require the focus to be so good when using the pmt . where can i see more ? lots of ccd images of one and several trapped ions are found on the monroe group site . only two minutes exposure time , so probably in a dark enough room , someone with good sensitivity could actually see it .
what you are missing is a source that is driving the whole thing . as you wrote it , there is no contradiction : $i_1=i_2=0$ . with a source included , it is no longer true that $i_1^2r_1 = i_2^2 r_2$ . you also need to watch the polarities on your transformer v 's and i 's : remember power in = power out .
this is a situation where knowing the history of the terminology can be helpful . the qft/string theory terminology comes from algebraic geometry , where the term moduli space is used for any space whose points correspond to some kind of geometric object . the projective space $\mathbb{p} ( v ) $ , for example , is the moduli space of lines in the vector space $v$ . likewise , a moduli space of instantons is the space of solutions to a set of instanton equations . and the moduli space of complex curves is what you end up integrating over in perturbative string theory after accounting for the gauge symmetries acting on the worldsheet metric . the word ' modulus ' ( plural ' moduli' ) just means ' parameter ' . moduli spaces were originally thought of as spaces of parameters , rather than as spaces of geometric objects ; mathematicians were interested in how the various ways of parameterizing geometric objects were related and eventually realized these parameters were coordinates on a space . string theorists have resurrected this old terminology by using the term ' moduli field ' to refer to a field which parametrizes a moduli space .
dbrane , aside from " beauty " , the electroweak unification is actually needed for a finite theory of weak interactions . the need for all the fields found in the electroweak theory may be explained step by step , requiring the " tree unitarity " . this is explained e.g. in this book by jiří hořejší: http://www.amazon.com/dp/9810218575/ google books : http://books.google.com/books?id=mnnagd7otlicprintsec=frontcoverhl=cs#v=onepageqf=false the sketch of the algorithm is as follows : beta-decay changes the neutron to a proton , electron , and an antineutrino ; or a down-quark to an up-quark , electron , and an anti-neutrino . this requires a direct four-fermion interaction , originally sketched by fermi in the 1930s , and improved - including the right vector indices and gamma matrices - by gell-mann and feynman in the 1960s . however , this 4-fermion interaction is immediately in trouble . it is non-renormalizable . you may see the problem by noticing that the tree-level probability instantly exceeds 100% when the energies of the four interacting fermions go above hundreds of gev or so . the only way to fix it is to regulate the theory at higher energies , and the only consistent way to regulate a contact interaction is to explain it as an exchange of another particle . the only right particle that can be exchanged to match basic experimental tests is a vector boson . well , they could also exchange a massive scalar but that is not what nature chose for the weak interactions . so there has to be a massive gauge boson , the w boson . one finds out inconsistency in other processes , and has to include the z-bosons as well . one also has to add the partner quarks and leptons - to complete the doublets - otherwise there are problems with other processes ( probabilities of interactions , calculated at the tree level , exceed 100 percent ) . it goes on and on . at the end , one studies the scattering of two longitudinally polarized w-bosons at high energies , and again , it surpasses 100 percent . the only way to subtract the unwanted term is to add new diagrams where the w-bosons exchange a higgs boson . that is how one completes the standard model , including the higgs sector . of course , the final result is physically equivalent to one that assumes the " beautiful " electroweak gauge symmetry to start with . it is a matter of taste which approach is more fundamental and more logical . but it is certainly true that the form of the standard model is not justified just by aesthetic criteria ; it can be justified by the need for it to be consistent , too . by the way , 3 generations of quarks are needed for cp-violation - if this were needed . there is not much other explanation why there are 3 generations . however , the form of the generations is tightly constrained , too - by anomalies . for example , a standard model with quarks and no leptons , or vice versa , would also be inconsistent ( it would suffer from gauge anomalies ) .
with a downwardly $x$-axis , you have the equations of movement : $m_1 \ddot x_1 = m_1g - k_1x_1 + k_2x_2 \tag{1}$ $m_2 \ddot x_2 = m_2g - k_2x_2 \tag{2}$ equilibrium position means that $\ddot x_1 = \ddot x_2 = 0$ , so you have , naming $x_1$ and $x_2$ the equilibrium positions : $0 = m_1g - k_1x_1 + k_2x_2 \tag{3}$ $0 = m_2g - k_2x_2 \tag{4}$ the second equation gives $x_2 = \frac{m_2g}{k_2}$ , and replacing $x_2$ in the first equation gives $x_1 = \frac{ ( m_1 + m_2 ) g}{k_1}$ now , we are interested on oscillations around the equilibrium positions , so we make a change of variables : $ y_1 = x_1 - x_1 , \quad y_2 = x_2 - x_2\tag{5}$ now , we rewrite equations $ ( 1 ) $ and $ ( 2 ) $ using $y_1 , y_2$ : $m_1 \ddot y_1 = - k_1y_1 + k_2y_2 \tag{6}$ $m_2 \ddot y_2 = - k_2y_2 \tag{7}$ we see , that the equilibrium positions correspond to $y_1=y_2=0$ , as wished , and the terms in "$g$" ( gravity ) have disappeared . now , the equation $ ( 7 ) $ has the solution : $y_2 = y_2 ~\cos ( \omega_2 t + \phi_2 ) \tag{8}$ , with $ \omega_2 = \large \sqrt{\frac{k_2}{m_2}}$ knowing $y_2$ , we may search a solution of the equation $ ( 6 ) $ , as : $y_1 = y_1 ~\cos ( \omega_1 t + \phi_1 ) + z ~\cos ( \omega_2 t + \phi_2 ) \tag{9}$ one find : $\omega_1 = \large \sqrt{\frac{k_1}{m_1}}$ and $z = \large \frac{k_2 y_2}{k_1 - m_1 \omega_2^2}$ .
the coulomb energy goes like $z ( z-1 ) $ , so it is typically a very small effect for light nuclei . the coulomb energy would be responsible for the difference in binding energy between 3he and 3h . in general , even-even nuclei are always more bound than odd nuclei . this is due to pairing . apart from pairing , one can usually predict nuclear binding energies quite well at a quantitative level by considering them to be the sum of a liquid-drop energy and a shell correction ( strutinsky 1968 ) . the shell correction for 4 he is large , because it is doubly magic . ( i do not actually know if the strutinsky works well for such light nuclei , but it is definitely good enough to give a correct qualitative explanation . ) “shells” in deformed nuclei . v . m . strutinsky , nucl . phys . a 122 no . 1 ( 1968 ) pp . 1-33 . see also : curvature correction in the strutinsky 's method . p . salamon , a . t . kruppa . j . phys . g : nucl . part . phys . 37 no . 10 ( 2010 ) 105106 . arxiv:1004.0079 . ( a variation on the technique , describes the technique itself . )
there are three processes to take into account : the warming of ice towards the melting point if it was originally below $0^{\circ} c$ . the melting of ice itself the warming of the resulting water the 1 . and 3 . part is addressed by heat capacity of ice and water respectively and the amount of heat will be directly proportional to temperature difference and weight of the water/ice . the proportionality constant ( actually it also depends on the temperature but not very strongly so let 's just ignore that ) is called specific heat . for water it is about twice as large as that of ice at temperatures around $0^{\circ} c$ . as for the 2 . part , this has to do with latent heat . simply put , this is an amount of heat you need to change phases without changing temperature . less simply put , when warming you are just converting the heat into greater wiggling of water molecules around their stable positions in the crystal thereby increasing their temperature . but at the melting point that heat will instead go into breaking chemical bonds between molecules in the ice lattice . now , latent heat is really big ( you need lots of energy to break those bonds ) . to get a hang on it : you would need the same amount of heat to warm water from $0^{\circ} c$ to $80^{\circ} c$ as you would need to melt the same amount of ice . now , presumably you want your drink cold in the end so that temperature for 3 . will be close to $0^{\circ} c$ and also the ice cubes should be pretty warm ( no use in producing ice cubes of e.g. $-50^{\circ} c$ , right ? ) . this means that these processes will not contribute much cooling . it is fair to say that melting of the ice takes care of everything . note : we can also quickly estimate how much ice you need by neglecting the processes 1 . and 3 . say you are starting with a warm drink of $25^{\circ} c$ and you want to get it to $5^{\circ} c$ . so , reusing the argument about the $80^{\circ} c$ difference being equivalent to a latent heat of the same mass , we see that you need four times less ice than water to get the job done .
it is very common to abuse the notation here , so i will try to clarify a bit . the state of a physical system can be described by an abstract vector $\left|\psi\right\rangle$ , which is an element of a hilbert space . the wavefunction , $\psi ( x ) $ is the representation of that vector in the position basis , $\psi ( x ) \equiv\left\langle x | \psi\right \rangle \equiv \left\langle x , \psi\right \rangle$ . in this notation , $\left|x\right\rangle$ is a state which is located at the point $x$ and nowhere else . think of $\left|\psi\right\rangle$ as a column vector whose components are the values of $\psi ( x ) $ at each $x$ . $$\psi ( x ) = \begin{pmatrix}\vdots\\\psi ( x_1 ) \\\psi ( x_2 ) \\\psi ( x_3 ) \\\vdots\end{pmatrix}$$ this vector also has a dual , $\left\langle\psi\right| = \left|\psi\right\rangle^\dagger$ , which is the transpose conjugate . in a different basis , this vector would have different components . another common basis is the momentum basis , with components $\psi_p ( p ) =\left\langle p | \psi\right\rangle$ , typically written as simply $\psi ( p ) $ . just like we could write a 3d vector $\bf{v}$ in the $\bf{i} , \bf{j} , \bf{k}$ basis as ${\bf v} = {\bf v}_1{\bf i} + {\bf v}_2{\bf j} + {\bf v}_3{\bf k}$ , we can write $\left|\psi\right\rangle$ as a sum of $x$ components , $$\begin{align} \left|\psi\right\rangle and = \int^{\infty}_{-\infty}\left|x\right\rangle\ , \psi ( x ) \ , \text{d}x \\ and = \int^{\infty}_{-\infty}\left|x\right\rangle\ ! \left\langle x | \psi\right \rangle \ , \text{d}x \end{align}$$ from this it is clear that $\int^{\infty}_{-\infty}\left|x\right\rangle\ ! \left\langle x \right|\text{d}x$ is an identity , since when it acts on $|\psi\rangle$ it gives $|\psi\rangle$ back . eigenvectors are usually labeled by their eigenvalues , so that if $\hat p$ is an operator with an eigenvalue $p$ , we write $\hat{p}\left|p\right\rangle = p\left|p\right\rangle$ . the eigenvectors of a self-adjoint operator form a complete set of states , which is to say that the sum of all the projection operators is the identity , $\int \left|p\right\rangle\left\langle p\right| \ , \text{d}p $ . this means you could also write $$\begin{align} \langle p\rangle and = \left\langle\psi\right|\hat{p}\left|\psi\right\rangle \\ and =\left\langle\psi\right|\hat{p}\int\left|p\right\rangle\ ! \left\langle p\right|\ , \text{d}p\ , \left| \psi\right\rangle\\ and =\int{p}\ , \langle\psi\left|p\right\rangle\ ! \left\langle p\right| \psi\rangle\ , \text{d}p\\ and =\int p\ , \psi^*\ ! ( p ) \ , \psi ( p ) \text{d}p \end{align}$$ where $\psi ( p ) =\left\langle p | \psi\right\rangle$ . this might make more sense , since it is more obviously an average value of p . the reason that $\hat p = -i\hbar{\partial \over \partial x}$ in the x basis is that a momentum eigenstate ( a state with a single wavelength ) is a plane wave , $\langle x \left|p\right\rangle = {1 \over \sqrt{2\pi}} e^{ipx/\hbar}$ , and that means that in the x basis , $\left\langle x \right| \hat p\left|p\right\rangle = {p \over \sqrt{2\pi}} e^{ipx/\hbar} = {-i\hbar \over \sqrt{2\pi}}{\partial \over \partial x} e^{ipx/\hbar}$ .
the boiling point of liquid oxygen is 90k , so it is easily condensed by liquid nitrogen . i have personally made lox by pumping air through a glass u tube immersed in liquid nitrogen , so i can confirm it works . later : as discussed in the comments , what condenses is a mixture of liquid oxygen and nitrogen rather than pure liquid oxygen . the dew point for air is about 82k , far enough above the boiling point of liquid nitrogen for a condensate to form , and the condensate is about 50% liquid oxygen . this article includes the relevant phase diagram . from personal experience i can say the condensate is blue , though i did not test its magnetic properties or the violence of its reaction with organic materials .
if you have ever swum to the bottom of a swimming pool you will know that in water the pressure increases as you go deeper . at a depth of about 10 metres the pressure is twice what it is at the surface , but the water 10 metres down does not burst up to the surface because it is held down by the weight of water above it . in fact the increase of pressure with depth is exactly the weight of water above . exactly the same is true of the atmosphere . the pressure at ground level is 101,325 pa because each square metre of the ground has about 10,329 kg of air above it ( 10329 kg times the acceleration due to gravity 9.81 m/sec$^2$ = 101325 pa ) . if you could magically remove the 100 km or so of atmosphere that is above some patch of air at ground level that air would indeed immediately expand upwards . incidentally , bernoulli 's principle is unrelated to the problem .
huygens and barrow , newton and hooke by v . i arnold . i havent read it , but i am a fan of the author 's writing style . he is a celebrated russian mathematician and also one of the most highly cited russian scientists . road to reality ( which im currently reading ) would also have been a good suggestion but its pretty much all about recent theories , which you are not interested in .
the clockwise direction is normally defined by the right hand grip rule . when your thumb is pointing away from you , your fingers are curled clockwise . so when you look at a clock the axis of rotation is away from you through the clock . i would guess the downvotes are because people believe your question is not physics related , but in fact this rule is how you determine the direction of the angular momentum vector , so there is a connection with physics .
you can decompose a rank two tensor $x_{ab}$ into three parts : $$x_{ab} = x_{ [ ab ] } + ( 1/n ) \delta_{ab}\delta^{cd}x_{cd} + ( x_{ ( ab ) }-1/n \delta_{ab}\delta^{cd}x_{cd} ) $$ the first term is the antisymmetric part ( the square brackets denote antisymmetrization ) . the second term is the trace , and the last term is the trace free symmetric part ( the round brackets denote symmetrization ) . n is the dimension of the vector space . now under , say , a rotation $x_{ab}$ is mapped to $\hat{x}_{ab}=r_{a}^{c}r_{b}^{d}x_{cd}$ where $r$ is the rotation matrix . the important thing is that , acting on a generic $x_{ab}$ , this rotation will , for example , take symmetric trace free tensors to symmetric trace free tensors etc . so the rotations are not " mixing " up the whole space of rank 2 tensors , they are keeping certain subspaces intact . it is in this sense that rotations acting on rank 2 tensors are reducible . it is almost like separate group actions are taking place , the antisymmetric tensors are moving around between themselves , the traceless symmetrics are doing the same . but none of these guys are getting rotated into members " of the other team " . if , however , you look at what the rotations are doing to just , say the symmetric trace free tensors , they are churning them around amongst themselves , but they are not leaving any subspace of them intact . so in this sense , the action of the rotations on the symmetric traceless rank 2 tensors is " irreducible " . ditto for the other subspaces .
given that there has not been any acceleration to cause these velocities , [ . . . ] as a side issue , even in newtonian mechanics , accelerations do not cause velocities . accelerations are just a measure of how rapidly velocities are changing . what you are running into here is the fact that general relativity does not have any notion of how to measure the motion of object a relative to a distant object b . it is neither true nor false that a and b gain relative velocity due to cosmological expansion . it is neither true nor false that a and b have nonzero accelerations relative to one another . frames of reference in gr are local , not global . it is valid to say that distant galaxies are moving away from us at some velocity . it is also valid to say that everything is standing still , but the space between us and the distant galaxy is expanding . [ . . . ] are there still relativistic effects in play ? that is , is there time dilation between the two frames ? kinematic time dilation is well defined in sr , which means that in gr it is only defined locally . gravitational time dilation is only well defined in gr in the case of a static spacetime , but cosmological spacetimes are not static . so it is neither true nor false that there is time dilation between us and a distant galaxy . concretely , you could measure doppler shifts . if you feel like interpreting these shifts in purely kinematic terms , you can assign a velocity to the distant galaxy relative to us . but this is not mandatory and actually does not really work very well , in the sense that the velocity you get is usually several times smaller than the rate at which the proper distance between the galaxies is increasing . ( proper distance is defined as the distance you would measure with a chain of rulers , each at rest relative to the cmb , at a moment in time defined according to a notion of simultaneity defined by cosmological conditions such as the temperature of the cmb . ) in particular , there are galaxies that we can observe that are now and always have been receding from us at $v&gt ; c$ , if you define $v$ as the rate of change of proper distance . the fact that we can observe them tells us that their doppler shifts are finite and correspond to $v&lt ; c$ . here is a nice popular-level article that explains a lot of this kind of stuff : davis and lineweaver , " misconceptions about the big bang , " http://www.scientificamerican.com/article.cfm?id=misconceptions-about-the-2005-03 it is paywalled , but there are lots of copyright-violating copies floating around on the web . the following is a presentation of the same material at a higher level : davis and lineweaver , " expanding confusion : common misconceptions of cosmological horizons and the superluminal expansion of the universe , " http://arxiv.org/abs/astro-ph/0310808
you have to break up the domain to get rid of the absolute value . then , do the integral by integrating by parts .
gravity is doing that work ! if you observe , the domino is in a position of unstable equilibrium . edit : as pointed out in the comments , this position is of a metastable and not unstable equilibrium . this means that the domino is in a state where it has not achieved the minimum possible energy state yet . the energy i am talking about here is the gravitational potential energy . gravitational potential energy is directly proportional to the height of the center of mass from some reference point . that means that when the domino is standing up it has more energy than when it is laying flat . so when you flick the domino , it tends to fall down . it is just trying to achieve a state of lower potential energy . now when you have a lot of dominoes standing up in a row , the whole system of dominoes can be regarded to be in a position of unstable equilibrium ( metastable equilibrium ) , i.e. their combined center of mass is at a greater height than when every one of the dominoes has fallen . so all you need to do is flick one domino and they will all come tumbling down . you can also think of it this way . the people who arranged those dominoes initially had to do work against gravity in arranging each domino in the upright position . so they had to provide the whole system of dominoes with energy initially , which was then all dissipated due to work done by gravity after the flick . so energy is still being conserved !
hint : $$\langle h|l_1^n l_{-1}^n|h\rangle ~=~ \sum_{i=0}^{n-1} \langle h|l_1^{n-1} l_{-1}^{n-1-i} [ l_1 , l_{-1} ] l_{-1}^i|h\rangle ~=~ \ldots $$ $$~=~ 2\sum_{i=0}^{n-1} \langle h|l_1^{n-1} l_{-1}^{n-1} ( l_0-i ) |h\rangle ~=~\ldots $$ $$~=~ n ( 2h- ( n-1 ) ) \langle h|l_1^{n-1} l_{-1}^{n-1}|h\rangle ~=~\ldots ~=~n ! \prod_{i=0}^{n-1} ( 2h-i ) . $$
your interpretation is partially correct . complex numbers are superimposition of two independent variables . coming to electromagnetic ( em ) waves we consider two components of em wave which are perpendicular to each other . this two components do not interact with each other but they are both part of a single wave . so the vector some of these components is the magnitude of em wave . complex component ( i ) of the em wave is not a dimension , it is used to avoid confusion . i could have written it in terms of simple vectors but these em components do not interact with each other . i can introduce numerous components of an em wave . f ( x ) =a ( x ) + i . b ( x ) + j . c ( x ) + k . d ( x ) + l . e ( x ) the move equation is a higher order complex equation . i have number of complex planes and all are independent of each other . fourth dimension exists in em wave and it is time but not polarization .
what does it mean to integrate $\frac{d\mathbf p}{dt}dt$ ? first , and in scalar form , recall from elementary calculus that $$\int_{x_1}^{x_2} dx = x_2 - x_1 $$ second , recall that $$f ( x + dx ) = f ( x ) + f' ( x ) dx$$ where $$f' ( x ) = \frac{df ( x ) }{dx} $$ denoting the differential of $f$ as $$df = f ( x + dx ) - f ( x ) $$ we have $$df = f' ( x ) dx$$ since $$\int_{x_1}^{x_2} f' ( x ) dx = f ( x_2 ) - f ( x_1 ) = f_2 - f_1$$ it follows that $$\int_{f_1}^{f_2} df = f_2 - f_1 = f ( x_2 ) - f ( x_1 ) = \int_{x_1}^{x_2} f' ( x ) dx$$
rephrased : if you are using an ideal , superconducting solenoid , and you hook it up to a battery , and it creates a magnetic field , where does the energy in the battery go ? the magnetic field that is created has an associated energy density $\frac{1}{2\mu} b^2$ . if the field is set up slowly ( adiabatically ) , then most of the energy will go into this term . however , initially , when $b$ is changing rapidly , it induces an electric field $e$ also , and the solenoid will radiate some energy away . if you are using a superconductor , then once $b$ is established , the current in the loop will continue flowing and $b$ will be maintained . for a resistive loop , you need to continue supplying energy to overcome ohm 's law as you said . as alexander pointed out , if you try to switch off the current , $b$ will start to decrease , and this decrease will induce an emf , and thus a current in the solenoid , resisting this change ( lenz ' law ) . this is the origin of induction , and the way in which the magnetic fields of solenoids can store energy ( e . g . , in rlc circuits )
momentum is conserved in magnitude and direction . so in order to analyze any situation of momentum conservation , you should always start with $$ \sum \mathbf p_{i}=\sum\mathbf p_f $$ where the subscripts denote the initial and final momenta . as to the ball and wall , you are correct that momentum is not conserved if you are only looking at the ball . if you consider that the system includes the wall , then the momentum conservation holds . this does mean that the wall contains a momentum of $2mv$ ( for mass $m$ and velocity $v$ ) . but note that since the mass of the wall is incredible compared to the ball , the velocity is notably imperceptible !
the best reference for this is feigenbaum 's original article , reprinted in " universality in chaos " by cvitanovic . the point is that when you iterate a map , every time you period double , you fold up the function one more time . the behavior is dominated by the solution to the following equation : $$ \alpha g ( g ( x/\alpha ) ) = g ( x ) $$ which says that g iterated with itself and rescaled ( both in the domain and range ) looks just like g . the function $g$ is shifted relative to f , so that it is maximum is at 0 , not at some point between 0 and 1 , which means you do not have to follow the critical point under iteration . you can solve this condition more easily by imposing the symmetry $g ( x ) =g ( -x ) $ and using a taylor expansion , and this gives g and $\alpha$ , and $\alpha$ is the scale exponent . everything about the critical behavior is determined by g , and this is described best in the original article .
an electron 's magnetic field is a dipole field - that is , the field strength is given by source : http://www2.ph.ed.ac.uk/~playfer/emlect4.pdf in this expression , $m$ is the magnetic dipole moment . for an electron , this has the value $$m=-928.476377 × 10^{−26} j/t$$ the magnetic permeability $$\mu_0=4\pi\cdot 10^{-7}\frac{v\cdot s}{ a \cdot m}$$ note - when the spin is up , the magnetic field at the origin points down , because the electron has negative charge . hence the negative sign in the value of $m$
the link does not work for me either , but it is not obvious that this is a typo ( http://cartech.ides.com/imagedisplay.aspx?e=111imgurl=%2fcarpenterimages%2fa-toolanddie%2f23-ts23-micromelta11%2f05_ts23_3point.gifimgtitle=3-point+bend+test , http://www.matweb.com/search/datasheet.aspx?matguid=638937fc52ca4683bc0c3f18f54f5a24 ) . and it looks like this strength is indeed the highest for an alloy ( http://gofygure.hubpages.com/hub/what-is-the-strongest-metal-the-hardest-metals-known-to-man )
calculate the determinant of ( 1.2.16 ) . note that these are determinants of $2\times 2$ matrices , as peter kravchuk told you , so $\det ( cm ) =c^2\det ( m ) $ for a constant $c$ . just to be sure , by the word " constant " , i really mean $c$ is a scalar , not a matrix . this $c$ may still depend on $\tau , \sigma$ ( or other variables if there were any ) . there is a coefficient $c^2$ because two columns ( or two rows ) are multiplied by $c$ each and the determinant is multilinear in the rows ( or columns ) . so the determinant of ( 1.2.16 ) is $$\det_{a , b} ( h_{ab} ) = \det_{ab} \left ( \frac{1}{2}\gamma_{ab} \gamma^{cd}h_{cd} \right ) $$ because $\gamma^{cd}h_{cd}/2$ plays the role of the constant $c$ from my previous general identity , the equation above may be simplified to $$\det_{a , b} ( h_{ab} ) = \det_{ab} ( \gamma_{ab} ) \times \left ( \frac{1}{2} \gamma^{cd}h_{cd} \right ) ^2$$ using the symbols such as $h=\det_{ab} ( h_{ab} ) $ and similarly for $\gamma$ , the square root of the minus equation above ( minus because the determinants are negative in a minkowskian signature ) is $$\sqrt{-h} = \sqrt{-\gamma}\times \frac 12 \gamma^{cd}h_{cd} $$ the second power disappeared again . divide ( 1.2.16 ) by the displayed equation i just wrote down . on the right hand side , $ ( 1/2 ) \gamma^{cd}h_{cd}$ will cancel again and you are left with ( 1.2.17 ) . in none of the arguments above , it is important that $c$ may depend on $\tau , \sigma$ . also , it does not matter at all that/whether $h_{ab}$ may be calculated as the induced metric from $\partial_\alpha x^\mu$ etc . why others can derive the second equation immediately those experienced among us see the result immediately because the left hand side of ( 1.2.17 ) is the tensor proportional to $h_{ab}$ with a normalization constant chosen so that the determinant of this left hand side equals minus one ( the minus can not be eliminated because it is given by the minkowski signature ) . in other words , the normalization factor is chosen exactly to " forget " the normalization . similarly , the right hand side of ( 1.2.17 ) is the tensor proportional to $\gamma_{ab}$ with the right normalization factor to makes the determinant of this product equal to minus one . because both $h_{ab}$ and $\gamma_{ab}$ were converted – by adding $ ( -h ) ^{-1/2}$ factors etc . – to matrices whose determinant is equal to minus one , both sides of ( 1.2.17 ) actually encode the information about the matrices $h_{ab }$ and $\gamma_{ab}$ up to their normalization . in other words , ( 1.2.17 ) is equivalent to saying that $h_{ab}$ and $\gamma_{ab}$ are proportional to one another as matrices . the equation ( 1.2.16 ) also says that they are proportional to each other as matrices – with a coefficient that is explicitly given in ( 1.2.16 ) but not in ( 1.2.17 ) – so ( 1.2.17 ) follows from ( 1.2.16 ) ( but not the other way around ) . this paragraph may sound complicated but it is really obvious for those who have a sufficient experience with tensors , matrices , and determinants .
you want the time . simply put , for that the minimum requirement is position ( and hence the distance ) and velocity . to know the position you need to detect it . once you detect it , you can calculate the trajectory and thus the time you have to settle your issues ( assuming it is on a collision course ) . i got this from cnn : the b612 foundation is building the sentinel space telescope , the world 's most powerful asteroid detection and tracking system , to see the millions of asteroids we can not see today and could pose threats to our planet . also , neossat , the near earth object surveillance satellite , is a micro-satellite launched in february 2013 by the canadian space agency ( csa ) that will hunt for neos in space . tracking systems are recording asteroids even as large as 140 meters . any asteroid with a radius more than 300 meters means an assured global catastrophe . check this out . the size that you are asking about is so big that it will create noticeable gravitational effects ( like perturbation in orbit ) and so we will know about it .
faraday 's law : $\mathcal{e}=-\frac{d\phi}{dt}$ . for a bigger loop , the rate of change of magnetic flux through the loop must be smaller than for a small loop . to go any further/be less hand wavy we need vector calculus . the magnetic field of a dipole is : $$ \mathbf{b} ( \mathbf{r} ) =\frac{\mu_0}{4\pi}\left ( \frac{3\mathbf{r} ( \mathbf{m} . \mathbf{r} ) }{r^5}-\frac{\mathbf{m}}{r^3}\right ) $$ whilst the flux through the loop can be calculated from this , if you can understand the integral you need for this $\int\mathbf{b} . d\mathbf{a}$ then you will hopefully be happy with me just saying that there is another expression for emf:$$\mathcal{e}=\oint\mathbf{f} . d\mathbf{l}$$ where $\mathbf{f}$ is the force producing the current we are concerned with . see griffiths introduction to electrodynamics for a proof of equivalence , for example . i am going to do it this way because most of the working is geometric which you should be able to follow if you have not done vector calc . the magnetic force is $\mathbf{v}\wedge\mathbf{b}$ . if we drop the magnet in " end on " then by symmetry the force at all points on the loop is the same magnitude , just in different direction . then $\mathcal{e}=f ( r ) \oint\hat{\mathbf{f}} . d\mathbf{l}$ . $\mathbf{v}$ is in the vertical direction , and $\mathbf{b}$ is coplanar with $\mathbf{v}$ and the axis of symmetry of the problem . to see why , make a quick sketch of the magnetic field lines of the dipole . then $\hat{\mathbf{f}}$ is the direction of $d\mathbf{l}$ and the integral is just the length of the loop $2\pi r$ . $$\mathcal{e}=2\pi r*f ( \mathbf{r} ) =2\pi r|\mathbf{v}\wedge\mathbf{b}|=2\pi rvb\sin\theta$$ i have thrown a lot of variables around so i will just define what is left in this expression for clarity . $r$ is the radius of the loop . $v$ is the magnitude of the velocity of the loop in the magnet frame . $b$ is the magnitude of the magnetic field at at the loop . $\theta$ is the angle between the velocity vector and magnetic field vector . let 's pick a particular point in time , with the dipole somewhere above the loop , and compare two loops of differing radius . let us assume that the object falls with near constant velocity , then that is common along with $2\pi$ . now we find the modulus of $\mathbf{b}$ $$ b=\frac{\mu_0m}{4\pi r^3}|3\hat{r} ( \hat{z} . \hat{r} ) -\hat{z}| $$ note that $\hat{z} . \hat{r}=\frac{h}{r}$ where $h$ is the height of the magnet above the loop . then $$ b=\frac{\mu_0m}{4\pi r^3}\sqrt{\left ( \frac{3h}{r}\hat{r}-\hat{m}\right ) . \left ( \frac{3h}{r}\hat{r}-\hat{m}\right ) }=\frac{\mu_0m}{4\pi r^4}\sqrt{3h^2+r^2} $$ so $$ \mathcal{e}\sim\frac{r}{r^4}\sqrt{3h^2+r^2}\sin\theta $$ to calculate $\sin\theta$ , consider the scalar product of $\mathbf{b}$ with $\mathbf{v}$: $$ \mathbf{b} . \mathbf{v}=|\mathbf{b}||\mathbf{v}|cos\theta $$ $$\cos\theta = \frac{r}{\sqrt{3h^2+r^2}}\left ( \frac{3h}{r}\hat{r}-\hat{z} \right ) . \hat{z}=\frac{3h^2-r^2}{r\sqrt{3h^2+r^2}} $$ then $$\sin\theta=\sqrt{1-\cos^2\theta}=\frac{3h}{r}\sqrt{\frac{r^2-h^2}{3h^2+r^2}}$$ putting everything together $$ \mathcal{e}\sim \frac{r}{r^4}\sqrt{3h^2+r^2}\frac{3h}{r}\sqrt{\frac{r^2-h^2}{3h^2+r^2}} \sim \frac{3hr}{r^5}\sqrt{r^2-h^2} $$ and using $r^2=h^2+r^2$ $$ \mathcal{e}\sim \frac{3hr^2}{ ( h^2+r^2 ) ^{5/2}}$$ for a given $r$ , where is $\mathcal{e}$ maximum ? $$\frac{\partial\mathcal{e}}{\partial h}=3r^2 ( h^2+r^2 ) ^{-5/2}-15h^2r^2 ( h^2+r^2 ) ^{-7/2}$$ which is $0$ when $h=r/2$ . at $h=r/2$ the emf is $$ \mathcal{e}\sim \frac{3}{400r^2}$$ which is strictly increasing as you decrease $r$ i.e. maximum emf felt by smaller loops is greater ( as long as we drop from a height $&gt ; r/2$ for $r$ of the bigger loop . corrections most welcome . image and tidying to follow .
peter , since your original post on this topic i have encountered " causal perturbation theory " which does distribution-like calculations using a modification of distributions based on scaled test functions . this paper does lots of interaction and s matrix calculations . perhaps you are entirely familiar with this , or it is not suitable but here is the link i have been studying : http://arxiv.org/ps_cache/hep-th/pdf/9710/9710225v1.pdf
there is a critical current density for every superconductor where the superconductor acts as an ordinary conductor and a voltage difference can be measured between its ends .
logarithmic series are a very broad topic . generally speaking , many quantities in qcd can be expressed as power series of the form $$x ( s ) = \underbrace{\sum_n x_{0n} ( \alpha_s\ln s ) ^n}_\text{ll terms} + \underbrace{\sum_n x_{1n}\alpha_s ( \alpha_s\ln s ) ^n}_\text{nll terms} + \cdots$$ the kinds of contributions that enter each set of terms depend entirely on what quantity $x$ is being calculated . i can only address this in more detail in the context of bfkl physics . the bfkl equation governs the unintegrated gluon distribution for a hadron , $\mathcal{f}$: $$\mathcal{f} ( \gamma ) = \mathcal{f}^{ ( 0 ) } ( \gamma ) + \frac{\bar{\alpha}_s}{\omega}\chi ( \gamma ) \mathcal{f} ( \gamma ) = \frac{\bar{\alpha}_s}{\omega} [ \underbrace{\chi_0 ( \gamma ) }_\text{ll} + \underbrace{\bar{\alpha}_s\chi_1 ( \gamma ) }_\text{nll} + \cdots ] \mathcal{f} ( \gamma ) $$ where $\gamma$ is the mellin conjugate to the momentum transfer $q$ . the best reference i have been able to find on what is included and excluded from the ll terms is this paper by gavin salam . he identifies three specific effects that contribute to the nll and higher terms : running coupling the running of the strong coupling ( to one-loop order ) is described by $$\bar{\alpha}_s ( q ) = \frac{\bar{\alpha}_s ( q_0 ) }{1 + b\bar{\alpha}_s ( q_0 ) \ln\frac{q^2}{q_0^2}}$$ at ll order , you can just use a constant $\bar{\alpha}_s ( q_0 ) $ for the coupling , but at nll , you have to incorporate the fact that multiple energy scales are involved in the process , and the difference between $\bar{\alpha}_s$ at these multiple scales leads to nll corrections . splitting function part of the bfkl equation involves a splitting function which is associated with gluon branching ( $1\to 2$ ) in the feynman diagrams . the leading term in the splitting function is $\frac{1}{z}$ , where $z$ is the fraction of momentum taken by one of the final-state gluons . that is sufficient for the ll expression , but when additional terms of the splitting function are taken into account , they produce nll and higher contributions . energy scale dependence this is one effect that is more general than just bfkl physics . recall that when we write $\alpha_s\ln s$ we actually mean $\alpha_s\ln\frac{s}{s_0}$ , where $s_0$ is some arbitrary constant . writing the same quantity for different values of $s_0$ involves differences which are at nll and higher order . but of course , those are just the most prominent , specifically identified terms . there are some other , smaller nll corrections , and of course it is largely unknown what contributions come in at nnll and higher order , so it is impossible to provide a complete list .
unpredictability and special relativity can come from the fact that objects that are at a space like separation from us can influence our future like cone . for example , if alpha centauri exploded in a supernova right " now " in our reference frame , we would not know about it for 4 years since that star is 4 light years away from us . so we can not now predict that 4 years from now we will be hit with the supernova blast wave . as @arnoques succintcly put in his comment , to predict an event in our future light cone , the entire past light cone of that event would need to be known and that would include events that are outside our present light cone .
$\mathbf{f}_m ( \mathbf{r}_m ) =\mathbf{f}_{ext} ( \mathbf{r}_m ) +q_m \sum_{i\ne m}q_i\mathbf{n}_{im}/ ( \mathbf{r}_m-\mathbf{r}_i ) ^2$ , is not it ? or you want magnetic forces too ? edit : for magnetic part , you can use the lagrangian from landau-lifshitz textbook ( §65 ) :
the reason " myopic " people see monroe and others see einstein is that the high frequency information in the image says einstein and the low frequency says monroe . when looking at the image closely , you seen the high frequencies and therefore einstein . by looking at it out of focus ( presumably what is meant by " myopic" ) , the high frequencies are filtered out and you see monroe . here is the original : here are versions successively more low-pass filtered ( the high frequency content was removed ) :
if you can form the problem in such a way as the is a contribution to the kinetic energy that can not change during the time modeled then you can drop that term . why ? because the euler-lagrange equation is only concerned with differentials and a added or subtracted constant does not affect them . the simplest example of this is writing the problem in a pair of reference frames where the system is at rest in one and in constant linear motion in another . the ke of the center of mass does not affect the outcome .
i am guessing you meant manganese rather than magnesium , since manganese has five unpaired electrons but magnesium does not . the answer is that ferromagnetism is not simply a function of having unpaired electrons . the effect is far more subtle than that . you would expect that the unpaired electrons in any material would tend to align themselves in opposition to each other because this is usually the lowest energy configuration . however in addition to the usual charge and magnetic interactions there is an interaction called the exchange interaction ( i have linked to the wikipedia article , but this is a complex area and hard going for the beginner ) . whether a material is ferromagnetic depends on the relative strengths of the exchange interaction with the other interactions , and it is a fine balance . this means even small changes may change a material to ferromagnetic or back . for example , although iron is the best know ferromagnet not all crystal forms of iron are ferromagnetic . the austenitic form of iron is paramagnetic not ferromagnetic , so just changing the crystal structure slightly can switch between ferromagnetism and paramagnetism . in manganese the balance of the interactions prevents ferromagnetism , though note manganese alloys like heusler alloy can be ferromagnetic .
the problem with the phase space flow in hamiltonian mechanics is that the flow itself is non-dynamical , that is , the flow is immediately defined for a given hamiltonian , so there is no independent equation governing its evolution . thus , liouville equation is simply a transport of a scalar variable in a given flow . so , dimensional analysis of the flow would be simply subset of dimensional analysis of underlying hamiltonian structure . similarly , i do not think there is any sense of attempting to find turbulence in the phase space flows . sure , time dependence of hamiltonian can introduce changes in the phase space , including the type of changes associated with transitions to chaos : such as bifurcations , tori destruction . . . but again , the flow itself is not the fundamental object in such transitions . if we are talking about the phase space of kinetic equations , the same arguments apply . even though the flow is ' more dynamical ' especially if taken in the context of self-interacting system of equations such as vlasov-maxwell , in these equations the flow itself again is not a fundamental object , so rarely it is analyzed independently . however , most methods of ( numerical ) solutions for such equations like particle-in-cell method and its many variations do use approaches quite similar to that of hydrodynamics .
let me see if i understand the question correctly : in general our solutions for a system will involve for every wavevector k a set of frequencies $\omega ( k ) $ . when these curves giving $\omega ( k ) $ do not cross , there is in obvious sense in which we can separate our solutions into different modes . but when they do cross how do we assign modes ? i believe the answer is in general we do not . ( or we do but we should not . ) unless the solutions have different symmetry properties , which is common enough , our label do not have meaning except convenience . i do not think this is not entirely about definitions . consider adiabatic motion : we think that if we perturb our solution slowly enough it will just change its frequency and wavenumber , but stay on the same mode . but in the vicinity of a crossing of two modes our solution will evolve to have components in both modes , barring symmetry . so is there is no sense to think of them as separate modes . ( the only case i can think where the labeling of modes actually matters is in topological insulators , a case which requires no [ bulk ] band touching . other than that it is just a convenience or it labels symmetry properties ) apologies if i misunderstood your question
whatever is the nature of the interaction one should take it into account in the hamiltonian only once for each distinct pair of particles : $$ u_\text{int} = \sum_{i&gt ; j} u_1 ( \mathbf{r}_i , \mathbf{r}_j ) = \sum_{i&lt ; j} u_1 ( \mathbf{r}_i , \mathbf{r}_j ) . $$ usually these sums are merged to make the hamiltonian of the system more symmetric . in that case the total sum should be divided by two : $$ u_\text{int} = \frac{1}{2}\sum_{i\neq j} u_1 ( \mathbf{r}_i , \mathbf{r}_j ) . $$ now the total hamiltonian $$ h = \sum_i \frac{p_i^2}{2m} + \frac{1}{2}\sum_{i\neq j} u_1 ( \mathbf{r}_i , \mathbf{r}_j ) $$ can be rewritten as a sum of " one-particle " hamiltonians : $$ h = \sum_i\left ( \frac{p_i^2}{2m} + \frac{1}{2}\sum_{j \neq i} u_1 ( \mathbf{r}_i , \mathbf{r}_j ) \right ) . $$ if you mean this when say " put energy into each particle " then , yes , you should put a half of the interaction energy .
45 degrees is , in fact , the angle for maximum range for a projectile with no air resistance . in the absence of air resistance , the only force acting is gravity , which causes a constant acceleration of g downwards . this determines the amount of time the particle spends in the air , via the formula for the position of a particle with constant acceleration : $y ( t ) = y ( 0 ) + v_y t + \frac{1}{2}a_y t^2$ putting in the relevant parameters ( start and end positions both 0 , acceleration -g ( negative because it is downward ) ) this becomes : $0 = v_y t - \frac{1}{2}g t^2$ which we solve to get : $t = \frac{2v_y}{g}$ this time then goes into the equation for the horizontal position : $x ( t ) = x ( 0 ) + v_x t + \frac{1}{2}a_x t^2$ as there is no horizontal force acting , this reduces to just $x ( t ) = v_x t = \frac{2v_x v_y}{g}$ to get this in terms of the angle , we use the fact from trigonometry that for a velocity $v$ at an angle $\theta$ from the horizontal , the vertical velocity is $v \sin \theta$ and the horizontal velocity is $v \cos \theta$ , giving us : $x = \frac{2v \cos \theta v\sin \theta}{g} =\frac{v^2}{g}\sin 2\theta$ this has its maximum value for $\theta = \frac{\pi}{4}$ , namely , 45 degrees from the horizontal .
there are two separate issues to consider . firstly there is usually an energy barrier to decay . radioactive decay occurs due to quantum tunnelling through the barrier , and the rate therefore depends on the barrier height . one of the very first studies of this was by george gamow back in 1928 , who studied the alpha decay of uranium-238 . even though alpha decay produces about 5mev of energy ( nearly 500 gigajoules per mole ! ! ) the half life of uranium-238 is about the same as the age of the solar system . gamow 's calculation is discussed in this pdf , or google for many similar articles . the decay is slow because there is a barrier of around 25mev that prevents the decay . so while it may be energetically favourable for a nucleus to decay to iron a kinetic barrier may reduce the rate to a negligably small value . secondly , although for example nickel-60 may have a lower binding energy per nucleon than iron-56 $^1$ this does not mean the reaction : $$ \mathrm{^{60}ni \rightarrow {}^{56}fe + \alpha }$$ is exothermic because the $\alpha$ particle also has a lower binding energy per nucleon than iron $^2$ . if you took 56 nickel nuclei , disassembled them into individual nucleons then reassembled them into 60 iron nuclei you might get an overall energy decrease , but this route is not available . decay pathways are limited to $\alpha$ , $\beta$ and fission , and if any step is not energetically favourable the decay process will stop at that step . $^1$ actually , according to wikipedia nickel-62 is the most stable nucleus not iron-56 $^2$ i have no idea whether this reaction is exothermic or not
in renormalization , one considers a family of lagrangian densities , with arbitrary factors for each renormalizable monomial in fields and derivatives ; in case of symmetries only of the symmetric ones . thus it is determined by the field and symmetry content only . for perturbation theory , these factors are then written as the renormalized finite term plus the diverging counterterm . the particular way of generating these factors ( by scaling fields or coupling constants ) is immaterial . as the regularization scale is sent to infinity , the counterterms are left to diverge in such a way that the renormalization conditions give finite results . the family of renormalized theories is now parameterizied by the parameters used in the renormalization prescription .
do optical mode phonons interact differently than acoustic ones ? yes . look at the dispersion curve for any material and you can understand this better . let us take silicon for example . you can see that the gradient of the frequency as a function of wave vector ( aka , group velocity ) is what determines the amount of energy that can be carried by phonons . you can see that the optical branches have a much flatter profile than the acoustic branches . therefore , they do not participate heavily in energy transfer and storage ( thermal conductivity and specific heat capacity ) . you can also see that their frequency is much higher than acoustic branches . this means that they interact with em waves of similar frequcencis . this is why , for example , co2 is a greenhouse gas . also , is there a way i can quantify this ? yes . thier interaction is different from acoustic phonons and were first successfully described by einstein where he assumed a dispersion relations at a single frequency , $\omega_0$ , and independent single frequency oscillator at each atom . the density of states is given by $d ( \omega ) = n\delta ( \omega - \omega_0 ) $ the acoustic phonons are described by debye model and they have a liner dispersion relation $\omega= v_s k$ and are responsible for sound wave . the density of states is given by $$d ( \omega ) = \frac{v \omega^2}{2 \pi^2 v_s^3}$$
there is a " common " chart in most chemistry labs that covers exactly this topic . one version of it can be found here : http://chemistry.kenyon.edu/getzler/research%20files/miscibility_elutropicity.pdf for reference 's sake , this was found by a google image search using the term " miscibility . " a clear case of having to know something exists in order to find it .
doing this kind of regularization in a series of which each term is more divergent than the ones before is rather dangerous . for example , one might compute the following series : \begin{equation} \sum_{n=1}^\infty \frac1{n^2+\lambda} = \frac1\lambda \sum_{k=0}^\infty \left ( -\frac1\lambda\right ) ^k \sum_{n=1}^\infty n^{2k} \stackrel{ ? ? }= 0 \end{equation} due to $\zeta ( -2k ) = 0 \ \forall k\in\mathbb n$ . this is obviously wrong as the correct result is a hyperbolic tangent : \begin{equation} \sum_{n=1}^\infty \frac1{n^2+\lambda} = -\frac1{2\lambda} + \frac\pi{2\sqrt\lambda} \coth\sqrt\lambda\pi \not= 0 \ ; . \end{equation} to better understand , you could , for example , introduce a regulator $e^{-n\epsilon}$ in your sum to make it convergent , and then take the limit $\epsilon\to0$ . you will find the result with naive zeta regularization , plus an infinite series of ever more divergent terms . in order to find the correct result , you will have to somehow deal with this series first . in applications such as quantum field theory we know ( due to renormalizability ) that these divergences cancel in physical quantities , but only if there is finitely many of them ( so no infinite series of them ) .
in an inertial reference frame , there most certainly is not zero net force on a planet . there is a force pulling the planet towards the sun , and that is it . there are no other forces on the planet . you are 100% correct : if there really were no net force on the planet , then the planet would be stationary or travel in a straight line at constant velocity . it would not travel in a circle or ellipse . this is newton 's first law . if your tutor disagrees with newton 's first law , i suggest you find a better tutor !
the resistor will heat up the air ( if assume 100% energy convert rate ) , then the total heat transferred from resistor to air will be given by $q = p\cdot t=i^2rt=iut$ then from the first law of the thermodynamics , we know the total inner energy of air $e$ is $e=q-w$ here $q$ is the heat transferred into the system ( if heat is transferred from out , then $q&gt ; 0$ , if the system transfer heat out , then $q&lt ; 0$ ) , and the $w$ is the work done by the air ( if air do work i.e. air pushes the piston out , then $w&gt ; 0$ , if outside does work to the air , i.e. someone pushes the piston to compress the air , then $w&lt ; 0$ ) . so from this , you know the power from the resistor generate heat , and this heat is transferred from resistor to air . if the piston cannot move , then no work will be done , so the increase of inner energy of air is just the heat transferred or generated by the resistor , that is $\delta e = q$ if the piston can move , the increase in energy will result in the increase of air temperature , and therefore , the air pressure will increase and the air will push the piston out . in this process , air do work to the piston . so the inner energy of air will drop , but this work is done by air .
the covariant derivative of $g$ is zero , but it is ordinary derivative is not . you need to know what is $\ddot{x_\mu}$ , \begin{equation} \ddot{x}_{\mu} = \frac{d^2}{dt^2} ( g_{\mu\nu} x^{\nu} ) = g_{\mu\nu} \ddot{x}^{\nu} + \ddot{g}_{\mu\nu} x^{\nu} + 2\dot{g}_{\mu\nu} \dot{x}^{\nu} \ne g_{\mu\nu} \ddot{x}^{\nu} \end{equation} that is the mistake you made in equation ( 1 ) . to get agreement , you need to know what is the meaning of vector $t_{\mu}$ in your equation . i think that is actually the components of the dual of the velocity vector , \begin{equation} t_{\mu} = g_{\mu\nu} \dot{x}^{\nu} \end{equation} notice that $g_{\mu\nu}$ is not inside the time derivative , that is how we lower indices by musical isomorphism . plug in this definition of $t_{\mu}$ and you will get the standard geodesic equation . in fact , equation ( 2 ) is the euler-lagrangian equation for the action , \begin{equation} s = \int d\tau \frac{1}{2} g_{\mu\nu} \dot{x}^{\mu} \dot{x}^{\nu} \end{equation} whose solution gives the distance minimizing curve ( with affine parameterization ) : the geodesic .
what you are describing is not a symmetry in the sense of noether 's theorem of the ( classical i.e. non relativistic and non quantum ) system , but it is a symmetry of our description of the system . a good example is provided when the hamiltonian can be written as $h=t+v$ where $t$ is a kinetic energy and $v$ is a potential energy . modifying the hamiltonian by adding a constant to the potential energy does not change the behavior of the system in either the classical case or in the non relativistic quantum case . this sort of symmetry is called a " gauge symmetry " and is discussed , in the quantum mechanics case , at length in sakurai 's quantum textbook " modern quantum mechanics": http://www.amazon.com/modern-quantum-mechanics-2nd-sakurai/dp/0805382917 see section ( 2.6 ) page 123 , " potentials and gauge transformations " , which describes the difference between the classical case , where changing the potential has no physical significance , and the quantum case , where changing the potential changes the phase , but does not result in physically observable consequences .
the action $s$ is not a well-known object for the laymen ; however , when one seriously works as a physicist , it becomes as important and natural as the energy $h$ . so the action is probably unintuitive for the inexperienced users - and there is no reason to hide it - but it is important for professional physicists , especially in particle and theoretical physics . the op 's statement that the hamiltonian corresponds to energy is a vacuous tautology because the hamiltonian is a technical synonym for energy . in the same way , one might say that the action intuitively corresponds to wirkung ( a german name ) because it is the same thing , too . because it now has two names , it becomes more natural :- ) and the op could also blame the energy for having " unnatural " units of action per unit time . in other words , the question assumes that energy ( and its unit ) is more fundamental and intuitive than the action ( and its unit ) - so it should not be surprising that using his assumptions , the op may also " deduce " the conclusion that the energy is more fundamental and intuitive than the action . ; - ) but is the assumption = conclusion right ? well , energy is intuitive because it is conserved , and the action is intuitive because it is minimized - so there is no qualitative difference in their importance . of course , the only difference is that non-physicists do not learn to use the action at all . the energy may be imagined as " potatoes " which every can do ; the action is an abstract score on the history that is only useful once we start to derive differential equations out of it - which almost no layman can imagine . if the laymen 's experience with a concept measures whether something is " intuitive " , then the action simply is less intuitive and there is no reason to pretend otherwise . however , physicists learn that it is in some sense more fundamental than the energy . well , the hamiltonian is the key formula defining time evolution in the hamiltonian picture while the action is the key formula to define the evolution in the nicer , covariant , " spacetime " picture , which is why hep physicists use it all the time . what the action is in general otherwise , the main raison d'etre for the action is the principle of least action , http://en.wikipedia.org/wiki/principle_of_least_action which is what everyone should learn if he wants to know anything about the action itself . historically , this principle - and the concept of action - generalized various rules for the light rays that minimize time to get somewhere , and so on . it makes no sense to learn about a quantity without learning about the defining " application " that makes it important in physics . energy is defined so that it is conserved whenever the laws of nature are time-translational symmetric ; and action is defined as whatever is minimized by the history that the system ultimately takes to obey the same laws . the energy is a property of a system at a fixed moment of time - and because it is usually conserved , it has the same values at all moments . on the other hand , the action is not associated with the state of a physical object ; it is associated with a history . there is one point i need to re-emphasize . for particular systems , there may exist particular " defining " formulae for the hamiltonian or the action , such as $e=mv^2/2$ or $s = \int dt ( mv^2/2-kx^2/2 ) $ . however , they are not the most universal and valid definitions of the concepts . these formulae do not explain why they were chosen in this particular way , what they are good for , and how to generalize them in other systems . and one should not be surprised that one may derive the right equations of motion out of these formulae for $h$ or $s$ . instead , the energy is universally defined in such a way that it is conserved as a result of the time-translational symmetry ; and the action is defined in such a way that the condition $\delta s = 0$ ( stationarity of the action ) is equivalent to the equations of motion . these are the general conditions that define the concepts in general and that make them important ; particular formulae for the energy or action are just particular applications of the general rules . in the text above , i was talking about classical i.e. non-quantum physics . in quantum physics , the action does not pick the only allowed history ; instead , one calculates the probability amplitudes as sums over all histories weighted by $\exp ( is/\hbar ) $ which may be easily seen to reduce to the classical predictions in the classical limit . a stationary action of a history means that the nearby histories have a similar phase and they constructively interfere with each other , making the classically allowed history more important than others .
i see two questions here . the first is why self-inductance is not considered when solving faraday 's law problems , and the second is why an emf can ever produce a current in a circuit with non-zero self-inductance . i will answer both of these in turn . 1 . why self-inductance is not considered when solving faraday 's law problems self inductance should be considered , but is left out for simplicity . so for example , if you have a planar circuit with inductance $l$ , resistance $r$ , area $a$ , and there is a magnetic field of strength $b$ normal to the plane of the circuit , then the emf is given by $\mathcal{e}=-l \dot{i} - a \dot{b}$ . this means , for example , that if $\dot{b}$ is constant , then , setting $ir=\mathcal{e}$ , we find $\dot{i} = -\frac{r}{l} i - \frac{a}{l} \dot{b}$ . if the current is $0$ at $t=0$ , then for $t&gt ; 0$ the current is given by $i ( t ) =-\frac{a}{r} \dot{b} \left ( 1-\exp ( \frac{-t}{l/r} ) \right ) $ . at very late times $t \gg \frac{l}{r}$ , the current is $-\frac{a \dot{b}}{r}$ , as you would find by ignoring the inductance . however , at early times , the inductance prevents a sudden jump of the current to this value , so there is a factor of $1-\exp ( \frac{-t}{l/r} ) $ , which causes a smooth increase in the current . 2 . why an emf can ever produce a current in a circuit with non-zero self-inductance . you are worried that emf caused by the circuit is inductance will prevent any current from flowing . consider the planar circuit as in part one , and suppose there is a external emf $v$ applied to the circuit ( and no longer any external magnetic field ) . the easiest way to see that current will flow is by making an analogy with classical mechanics : the current $i$ is analogous to a velocty $v$ ; the resistance is analogous to a drag term , since it represents dissipation ; the inductance is like mass , since the inductance opposes a change in the current the same way a mass opposes a change in velocity ; and the emf $v$ is analogous to a force . now you have no problem believing that if you push on an object in a viscous fluid it will start moving , so you should have no problem believing that a current will start to flow . to analyze the math , all we have to do is replace $-a \dot{b}$ by $v$ in our previous equations , we find the current is $i ( t ) = \frac{v}{r} \left ( 1-\exp ( \frac{-t}{l/r} ) \right ) $ , so as before the current increases smoothly from $0$ to its value $\frac{v}{r}$ at $t=\infty$ .
your sphere is traveling at $10^8 \ , \mathrm{m/s}$ ( 33% the speed of light ) but earth 's escape velocity is $11200 \ , \mathrm{m/s}$ . there is absolutely no way the sphere could ever enter any sort of orbit around the earth , regardless of the interaction between it and the earth 's magnetic field . $$ke_{relativistic} = \left ( \left ( \frac{1}{\sqrt {1 - \frac{v^2}{c^2}}} \right ) - 1 \right ) mc^2 = 8.7 \cdot 10^{16} \ , \mathrm{j}$$ that is 20 megatons of tnt . even if the magnetic field could perform some sort of braking , the amount of heat generated to dissipate that much energy would induce nuclear fusion .
it depends on the knocking technique . do you knock with a finger , or do you knock with a full hand ? i would estimate the released energy with the kinetic energy of the finger/hand . if you approximate a hand with a sphere of 8 cm diameter and density of water , and the velocity with 0.1 m/s . you get $$ e \approx m v^2= 0.25 kg * ( 0.1 m/s ) ^2 = 3 mj$$ which is an absolute upper limit
can we view the infinite deep well , as a infinite barrier ? only from the point of view of r=0 , but the electron is not there in the classical model ; and even in the schrodinger model r=0 is only one infinitesimal point , you would have to considered the expectation value of the energy . what would the wavefunction intersect , does it pass through ( 0,0 ) ? the s-orbitals do have a non-zero probability density at ( 0,0 ) , but again that is only one point . i am quite confused by the idea of having negative energy , does it mean i need infinite energy to get out of the well ? no , the electron does not need infinite energy to get out of the well because it does not start at the bottom of the well in the classical model , and has zero probability of starting at the bottom of the well in the schrodinger because it is only a single point . you need to consider potential energy , kinetic energy and total energy . if you look at the following question and answer you should have a solution to the problem : hydrogen atom : potential well and orbit radii
new answer what you have done here is just dimensional analysis . but you have gone a little too far . in particular , just because two things have the same dimensions does not mean that they are equal . if you want to expand $f/a$ a little more , you can choose your favorite from \begin{equation} f = \frac{d p}{dt} = \frac{d}{dt} ( m\ , v ) = m\ , \frac{d}{dt} v = m\ , a \end{equation} here , $p$ is the momentum , $m$ is some amount of mass you are keeping track of , $v$ is its velocity , and $a$ the acceleration . now , pressure is really defined as being the force on a unit of area . so if you do not choose an area , you can not define pressure . there might be situations where there is some momentum change in a volume without the matter directly hitting a surfce . for example , with an electromagnetic force . so there could be a net force on a volume . but again , you can not call anything a pressure unless you select an area to measure it . old answer ( from before reading your clarifications ) pressure is the force per unit area applied perpendicular to the surface of an object . ( or at least the area over which you are imagining measuring the pressure . ) if i understand you correctly , you are interested in the pressure on the surface of the tube . is this correct ? but you are referring to the mass flowing past the tube . that is , it is flowing parallel to the surface . so , in your simplified model where the matter is flowing straight , there would be zero pressure . of course , in real life , the matter would probably having some internal motions as well , which means it would bounce off the walls of the tube , exerting pressure . in short , we would need more information to calculate the pressure . for your particular setup , you still have not told us about any interactions between particles of the matter . from what you have told me , i can still assume that the matter particles all have the same velocity ( but are scattered at different positions ) . now , as you know , force is proportional to acceleration . since the velocity is constant , there is no acceleration , so there must be no force , which means there is no pressure .
any material between two nodes is displaced by the same direction . so the direction of b and c has to be the same as well as the direction of a and d due to symmetry . in addition , the direction of a must be the opposite of b since they are across from a node . similarly the direction of c and d must be opposite . so the two possible configurations are  A--&gt; &lt;--B &lt;--C D--&gt; (figure d) &lt;--A B--&gt; C--&gt; &lt;--D (figure c)  the correct answer is ( 2 ) .
a spatial fourier transform means a fourier transform in the spatial variable ( $x\rightarrow k$ ) , while a temporal fourier transform is the same transformation , but in terms of the time variable ( $t\rightarrow \omega$ ) . the equation you have written is the ( asymmetric ) temporal fourier transform of $f ( k , t ) $ . the spatial transform looks like some variation of \begin{equation} f ( k , t ) = \frac{1}{2\pi} \int g ( x , t ) e^{i k x} dx \end{equation} where $g ( x , t ) $ is the ( one-dimensional ) van hove function .
for inflation the potential energy of the field dominates the kinetic energy $\dot{\phi} \ll v ( \phi ) $ this limit is referred as slow roll and under such conditions the universe expands quasi exponentially $a ( t ) \propto \exp \left ( h dt\right ) = e^{-n} $ where we define the number of e-folds $n$ as : $dn = -h dt$ so that $n$ is large in the far past and decreases as we go forward in time and as the scale factor $a$ increases . with this we have : $\epsilon = -\frac{\dot{h}}{h^{2}} = \frac{1}{h}\frac{dh}{dn}$ accelerated expansion will only be sustained for a sufficiently long period of time if the second time derivative of $\phi$ is small enough : $|\ddot{\phi}| \ll |3h\dot{\phi}| , |v' ( \phi ) |$ so that the equation of motion for the scalar field is approximately : $3h\dot{\phi} + v' ( \phi ) \simeq 0$ this condition can be expressed in terms of a second dimensionless parameter , defined as : $\eta \cong -\frac{\ddot{\phi}}{h\dot{\phi}} \cong \epsilon + \frac{1}{2\epsilon}\frac{d\epsilon}{dn}$ then $\eta \simeq \frac{1}{8\pi g} \left ( \frac{v'' ( \phi ) }{v ( \phi ) } \right ) $ in the slow regime $\epsilon , |\eta|\ll 1$ , where the last condition ensures that the change of $\epsilon$ per e-fold is small . notice that $\eta$ need not be small for inflation to take place . inflation takes place when $\epsilon &lt ; 1$ , regardless of the value of $\eta$
yes , feynman 's comments apply to this situation – it is really the very situation feynman was referring to . the counters destroy the interference pattern whether or not you bother to look . in fact , the claim that " you cannot look " is only valid in practice , not in principle . in principle , the state of the counters before their destruction inevitably leaves some traces in the environment ( although traces that are hard to observe in practice ) because the information is preserved . by inserting the counters , we are correlating the state of the electron with a state of the counter and this entanglement with some degrees of freedom in the environment ( in the counter ) is the immediate reason why the interference is no longer there . when we say that the right interpretation of the wave function is subjective , it does not change anything about the fact that counters behind slits disrupt the interference pattern ( and similar consequences ) . instead , we may say that without an observer , the whole sealed box evolves into a schrödinger-cat-like superposition of macroscopically distinct states . but when we look at the superposition in your case , it contains contributions from the electron in one slit with one state of the counter ; and an electron in the right slit with another state of the counter . each of these possibilities has a nonzero probability amplitude – and nonzero probability – and once we open the ( partly destroyed ) sealed box , we measure which of the two outcomes has gotten realized . but we may still show that we will measure the strength of the interference pattern to be zero if the which-slit information was detected because the waves from the two slits just could not interfere with one another as they were entangled with different states of the counter . only contributions to a wave in which all the remaining degrees of freedom are found in the same state may interfere .
capacitor-resistor circuit in ac ? it is not clear whether $v_c$ , $v_r$ and $v$ are ac voltages or transient values in a dc circuit . i will assume that the capacitor–resistor system are supplied by an ac voltage $v_{max}$ of frequency $f$ . then the equation holds $v=i ( r-\frac{j}{\omega c} ) $ where $j=\sqrt{-1}$ . so that treating the circuit as a potential divider one can write $v_c=\frac{v}{r-\frac{j}{\omega c}}\frac{1}{j\omega c}$ or $v_c=\frac{v}{j\omega rc+1}$ or in terms of the magnitudes $v_{cmax}=\frac{v_{max}}{\sqrt{1+ ( \omega rc ) ^2}}$ therefore , knowing the maximum values for $v_c$ and $v$ you can solve this equation for c $c=\frac{1}{\omega r}\sqrt{ ( v/v_c ) ^2-1}$ where $\omega=2\pi f$ i hope this helps .
the answer by jkl is sufficient but i want to address particularly the why ? . i am so confused . why does it act the way it does ? if one reads a bit about the history of science and physics in particular , it becomes clear that physics at the ultimate end does not answer the ultimate why ? . physics posits laws and uses sophisticated mathematical tools to theorize from axioms , get equations , and check against the data experimentally . it finds how , from axioms one ends with predictions for measurements that validate them . the why ? questions addressed to the axioms has the only answer : because . when one is validating a theory , as for example newton 's gravitational theory , and one hits a disagreement with the data , new axioms and new theoretical tools are developed to explain the why of the disagreement and the new theory validated for the regime of disagreement . special and general relativity are an example . the history of physics has other examples : thermodynamics , developed theoretically to explain bulk behavior , statistical mechanics , out of classical mechanics emerge out of asking how and assuming axioms to contain the why . each theory with its own regime of validity . within a theory a question with a why is answered by proofs of how finally hitting on the axioms . quantum mechanics is the last in the series of exploring the microcosm . the why you are asking hits against the because of its axioms . there are people who continue the exploration , trying new axiomatic theories of how a quantum mechanical theory can emerge from a smaller regime where we are back to classical concepts , and contain the why in axioms for their new theory . they are not successful except with small models . the bulk of theoretical physicists either ignores their efforts or proves that their new theories would violate a basic validated law as , for example , lorentz invariance . and that is the story of why in physics . ultimately the answer is because .
it is not possible that susy particles are hiding in kev or mev range . in particular , there can not be any new charged particles ( and similarly new color-charged particles ) that would be this light because they would be easily pair-produced and easily detected . the first ( february 2012 ) claims by different authors ( the original ones , rupp and van beveren , who made the conjecture ) were refused by the compass collaboration ( which was used as one of the main pieces of " evidence" ) here : http://arxiv.org/abs/1204.2349 compass says that the patterns that attracted the attention or rupp and van beveren are due to $\pi^0$ , $\eta$ , and secondary interactions in the compass spectrometer . rupp and van beveren responded that the compass critique is internally inconsistent . it seems more likely to me that compass is right . the newest russian experimental paper looks strange to me . for example , it never quotes any confidence levels , as far as i can see , and instead says that there are " almost no errors " in their measurement , a claim that it easily refuted by looking at their chaotic wiggly charts . an extended discussion may be found on my blog .
the classically forbidden region coresponds to the region in which $$ t ( x , t ) =e ( t ) -v ( x ) &lt ; 0$$ in this case , you know the potential energy $v ( x ) =\displaystyle\frac{1}{2}m\omega^2x^2$ and the energy of the system is a superposition of $e_{1}$ and $e_{3}$ . this should be enough to allow you to sketch the forbidden region , we call it $\omega$ , and with $\displaystyle\int_{\omega} dx \psi^{*} ( x , t ) \psi ( x , t ) $ probability you are asked for ,
the surface $\mathrm{s}$ does indeed contain charge $q_1$ , and so will have nonvanishing electric flux . however , $\mathrm{s}$ does not contain charges $q_2$ and $q_3$ , so it will have zero total flux due to those charges .
the electromagnetic processes between atoms and molecules in all phases , solid , liquid , gas , depend on what is generically called " van der waals " fields and subsequent forces . it is well known that the atoms/molecules are neutral , nevertheless there exist for all matter dipole and quadrupole and higher order fields which are mainly attractive and form the chemical bonds which is the way neutral atoms and molecules can bind into solids and liquids and interact as gases . these bonds are quantum mechanical , that means that there exist solutions of the schrodinger equation with energy levels from ground state to continuum , one can model them as repeated over all the mass of the solid , liquid and gas . the unfilled energy levels are close to each other in energy and the continuum of n=infinity ( the radial quantum number ) . at the same time the atoms and solids have pure kinetic degrees of freedom : they can vibrate and rotate in solids , they can move in two dimensions in liquids and in all three dimensions in gases . in gases simple scattering of the molecules transfers the kinetic energy of one molecule to the potential energy of another , i.e. raises an electron to a higher level . the electron goes back to its ground state releasing a specific photon , or a cascade of photons , depending on the energy . remember that the higher levels with respect to n , the radial quantum number , are closely packed . these photons are the ones emitted as black body radiation , and they are a continuum because of the 10^23 molecules per mole and the almost continuous energy levels . the temperature is a function of the average kinetic energy in the gas , the higher the temperature the more energetic the kinetic scattering and the higher the average photon energy . in a solid there are also vibrational and rotational kinematic degrees of freedom that are contributing to the average kinetic energy , i.e. temperature . the kinetic energy of the molecules becomes potential energy for an electron in the lattice which then decays to its ground state or through cascades . the logic is the same as for gases and the same holds for liquids that have some extra kinematic degree of freedom with respect to solids . so it is the quantum mechanica behavior of matter at the micro level which is responsible for the black body radiation , and the infrared catastrophy problem of the classical extrapolations was solved . it is the energy levels that make a difference between infinity and well behaved property in the electromagnetic emmissions . thus the average kinetic energy ( proportional to t ) diminishes by turning into electromagnetic emissions through the stepping of energy levels . does that mean in rarefied conditions where the mean-free-path is relatively large , the rate of ir emissions decreases ( while the intensity is still only dependent on the temperature ) ? when the mean free path is large , the temperature is lower , the average kinetic energy of the atoms is lower and thus the photons produced by the transformation of kinetic to exciting electrons to higher potential energy levels and consequent decay to ground energy levels are all lower and will keep getting cooler if the energy is not replenished . i do not know what you mean by the intensity . this is the black body radiation spectrum . as the temperature decreases , the peak of the black-body radiation curve moves to lower intensities and longer wavelengths . the black-body radiation graph is also compared with the classical model of rayleigh and jeans . if you mean gases in low pressure as at the top of the atmosphere etc , one has to study them separately according to the boundary conditions . there can be gases with very high temperatures as in the atmosphere of the sun .
the container on earth will be cooled by convection currents i.e. it transfers heat to the air around it , and also by black body radiation . by contrast the container in space can only cool by black body radiation , and obviously it will cool down more slowly . you can calculate the cooling in space using the stefan-boltzmann law assuming you know the emissivity ( if you paint the container black the emissivity will be close to unity ) . calculating the cooling in air is harder ; typically you had use newton 's law with empirically derived constants . the final temperature in air is obviously just the temperature of the air around your container . the final temperature in space depends on where your container is . just as the container can lose heat by emitting radiation it can gain heat by absorbing radiation , and space is full of radiation . for example the moon is just a lump of inert rock with little or no internal generation of heat , however by absorbing sunlight the daytime temperature can rise to over 100ºc . however at night , when there is no sunlight the temperature can fall to -150ºc . so the final temperature of your container would be different during the lunar night and day , even though it is in a vacuum in both cases . if you took your container into intergalactic space , well away from any radiation sources , then it would indeed cool to the 2.7k of the cosmic microwave background .
"2 . where did they even get 2sa+sb=0 from ? " from the assumption that the length of the rope does not change . " why did they determine the change in distance this way ? my first assumption was that if block b moved up 2ft , then block a should move down 2ft ( the rope must " move " 2ft too right ? ) . " no , there is a difference between a movable pulley and a fixed pulley , so a moves down 1 ft . "3 . where did 2va=−vb come from ? " from the same assumption as above . however , the direction of va is shown incorrectly ( or , alternatively , the signs in the formula are wrong ) . "4 the fbd for block a is confusing , why is the friction force fa in the direction of the ropes ? i thought it was block b that is going down ? " it is writen in the statement of the problem that block b went up . " am i the only one who had trouble deducting that the pulley and block a are the same object ? " i do not quite understand what this phrase means exactly and how it is relevant . "5 . look at the final answer , how could vb be negative ? the problem says block b goes up . " see the answer to your item 3 .
well , perhaps one should consider reading the hamiltonian formulation of general relativity : myths and reality for further mathematical details . but i would like to remind to you with most constrained hamiltonian systems , the poisson bracket of the constraint generates gauge transformations . for general relativity , foliating spacetime $\mathcal{m}$ as $\mathbb{r}\times\sigma$ ends up producing diffeomorphism constraints $\mathcal{h}^{i}\approx0$ and a hamiltonian constraint $\mathcal{h}\approx 0$ . note i denote weak equalities as $\approx$ . this is first considered in peter g . bergmann and arthur komar 's " the coordinate group symmetries of general relativity " inter . j . the . phys . 5 no 1 ( 1972 ) pp 15-28 . since you asked , i will give you a few exercises to consider ! exercise 1: lie derivative of the metric the lie derivative of the metric along a vector $\xi^{a}$ is $$ \mathcal{l}_{\xi}g_{ab}=g_{ac}\partial_{b}\xi^{c}+g_{bc}\partial_{a}\xi^{c}+\xi^{c}\partial_{c}g_{ab} $$ show that this may be rewritten as $$ \mathcal{l}_{\xi}g_{ab}=\nabla_{a}\xi_{b}+\nabla_{b}\xi_{a} $$ where $\nabla$ is the standard covariant derivative . exercise 2: constraints generate diffeomorphisms recall that the hamiltonian and momentum constraints are $$\mathcal{h} = \frac{16\pi g}{\sqrt{q}}\left ( \pi_{ij}\pi^{ij}-\frac{1}{2}\pi^{2}\right ) -\frac{\sqrt{q}}{16\pi g}{}^{ ( 3 ) }\ ! r , \quad\mathcal{h}^{i} = -2d_{j}\pi^{ij}$$ and $\pi^{ij}=\displaystyle\frac{1}{16\pi g}\sqrt{q} ( k^{ij}-q^{ij}k ) $ with $k_{ij}=\displaystyle\frac{1}{2n} ( \partial_{t}q_{ij}-d_{i}n_{j}-d_{j}n_{i} ) $ . let $$h [ \widehat{\xi} ] = \int d^{3}x\left [ \hat{\xi}^{\bot}\mathcal{h}+\widehat{\xi}^{i}\mathcal{h}_{i} \right ] $$ show that $\mathcal{h} [ \widehat{\xi} ] $ generates ( spacetime ) diffeomorphisms of $q_{ij}$ , that is , $$\left\{h [ \widehat{\xi} ] , q_{ij}\right\}= ( \mathcal{l}_{\xi}g ) _{ij}$$ where $\mathcal{l}_{\xi}$ is the full spacetime lie derivative and the spacetime vector field $\xi^{\mu}$ is given by $$\widehat{\xi}^{\bot}=n\xi^{0} , \quad \widehat{\xi}^{i}=\xi^{i}+n^{i}\xi^{0}$$ the parameters $\{\widehat{\xi}^{\bot} , \widehat{\xi}^{i}\}$ are known as " surface deformation " parameters . ( hint : use problem 1 and express the lie derivative of the spacetime metric in terms of the adm decomposition . ) addendum : i would like to give a few more references on the relation between the diffeomorphism group and the bergmann-komar group . from the hamiltonian formalism , there are a few references : c.j. isham , k.v. kuchar " representations of spacetime diffeomorphisms . i . canonical parametrized field theories " . annals of physics 164 2 ( 1985 ) pp 288–315 c.j. isham , k.v. kuchar " representations of spacetime diffeomorphisms . ii . canonical geometrodynamics " ann . phys . 164 2 ( 1985 ) pp 316–333 the lagrangian analysis of the symmetries are presented in : josep m pons , " generally covariant theories : the noether obstruction for realizing certain space-time diffeomorphisms in phase space . " classical and quantum gravity 20 ( 2003 ) 3279-3294 ; arxiv:gr-qc/0306035 j.m. pons , d.c. salisbury , l.c. shepley , " gauge transformations in the lagrangian and hamiltonian formalisms of generally covariant theories " . phys . rev . d 55 ( 1997 ) pp 658–668 ; arxiv:gr-qc/9612037 j . antonio garcía , j . m . pons " lagrangian noether symmetries as canonical transformations . " int . j . mod . phys . a 16 ( 2001 ) pp . 3897-3914 ; arxiv:hep-th/0012094 for more on the hypersurface deformation algebra , it was first really investigated in hojman , kuchar , and teitelboim 's " geometrodynamics regained " ( annals of physics 96 1 ( 1976 ) pp . 88-135 ) .
dear leandro , it is because the pauli matrices , together with the $2\times 2$ identity matrix , form a full real basis of all the hermitian $2\times 2$ matrices ( note that $2\times 2$ hermitian matrices depend on four real parameters ) , and the identity matrix is irrelevant in a hamiltonian because it is just a conventional energy shift that acts on all vectors equally and does not create any subtleties such as line crossing so one may omit it in any discussion of interesting physical effects . so any $2\times 2$ hermitian matrix that " matters " - and yes , hamiltonians have to be hermitian operators - is a real linear combination of the three pauli matrices . they do not have to be interpreted as a spin of any kind . but they are still a more natural basis than any other basis of the hermitean matrices because the product of any pair of the matrices generates a multiple of another matrix in the basis , thus simplifying all the calculations . ( analogy with the conventional $so ( 3 ) $ generators is a more transparent way to see the power of this basis . ) but even if you chose a less clever basis than the pauli matrices , you could derive the same results . the equations could just become a bit more cumbersome . line crossing etc . is first observed at degeneracy 2 . however , one may also study $3\times 3$ or $n\times n$ matrices . conventional bases of the hermitian matrices are called the gell-mann matrices in the case of $su ( 3 ) $ - because gell-mann generalized the pauli matrices when he studied the strong force where $su ( 3 ) $ enters in two ways . the $n\times n$ hermitian matrices depend on $n^2$ real parameters . one usually takes the identity matrix to be one of the basis vectors ; the remaining $n^2-1$ vectors are " generators of $su ( n ) $" .
if a crystal has a discrete group of point symmetries then the electronic eigenfunctions will be suitably invariant under that group . formally , the symmetry requires that the eigenfunctions of a hamiltonian with symmetry group $g$ belong to the various representations of that group . in the abstract , a representation of a group $g$ is a vector space ( in this case a subspace of degenerate energy ) $v$ and a " recipe " for unitarily transforming the vectors in $v$ with the transformations from $g$ , in the form of a group homomorphism $r:g\rightarrow u ( v ) $ . if one knows the structure of a group ( in the form of its multiplication table ) then there is a lot that can be said about its possible representations , which are typically denoted by some standard notation ( e . g . $e$ , $a$ , $b$ , etc . ) . the wavefunctions are then labelled by the representation type of the subspace they belong to . to bring this back down to angular momentum , the subspaces with different $l$ are the different representation subspaces . the group in question is the rotation group $\text{so} ( 3 ) $ . it has an infinite family of representations of increasing finite dimension , and the index $l$ that labels them is precisely the angular momentum quantum number of those wavefunctions . in group theoretic terms , then , " having definite angular momentum " simply means " belonging to a suitable representation of $\text{so} ( 3 ) $" . thus a crystal with a point symmetry will have electronic eigenfunctions that do have " definite crystal angular momentum " , in the sense that they belong to a certain representation of the point group . added in response to comment : unfortunately , there is no physical quantity that corresponds to this symmetry . this is due to the general fact that discrete symmetries have no generators . while you can write rotations , for example , in the form $e^{i\mathbf{j}\cdot\hat{\mathbf{n}}\theta}$ , where $\mathbf{j}$ is the generator , this is not really meaningful for discrete symmetries . a good comparison for this is parity : if $\pi$ commutes with $h$ then we say parity is conserved , in the sense that the transformation itself is a constant of the motion . for a more general discrete group $g$ ( instead of $g=\{-1,1\}$ for parity ) then the labels $+$ and $-$ are replaced by the group representation . similarly the labels $l$ and $m$ correspond to the group representation and to the eigenvalues of some particular group transformation . both are conserved under $h$ , but there is no generator .
the light from distant galaxies does undergo a frequency change . it is red shifted , and the amount of red shifting is used to work how fast the distant galaxy is receding and therefore how far away it is . however this is not a damping effect . the light red shifts because the spacetime in between us and the distant galaxy is expanding , so although the light 's energy is conserved it is spread over a larger distance . you ask why light is not damped , but why should it be ? since energy is conserved the light wave can not lose energy unless there is some mechanism to carry the energy away . for a light wave travelling in vacuum there is simply no mechanism by which it can lose energy , so it does not . in your last paragraph i think you are getting mixed up with a different effect . when a terrestrial radio station broadcasts it sends the radio waves out as a half sphere ( the half above the ground ) . as you get further away from the transmitter the field strength of the radio waves decreases as the inverse square of the distance because the energy of the transmitted wave is spread over a larger area . however the total energy of the light wave is conserved . this obviously happens with distant galaxies as well because the more distant a galaxy is the fainter it appears . this is not due to damping , it is just the inverse square law dependance of the field strength .
this is a notation from differential geometry that is unusual in physics ( as far as i am aware ) . the operators $\frac{\partial}{\partial y}$ and $\frac{\partial}{\partial z}$ are basis vectors for vector fields ; see e.g. this planetmath article for an introduction ( in particular where it says " in some sense " :- ) if you do not want to worry about differential geometry , you could write the field in good old-fashioned vector notation as $$ e=\pmatrix{0\\e_y ( t-x ) \\e_z ( t-x ) }\ ; , \quad b=\pmatrix{0\\b_y ( t-x ) \\b_z ( t-x ) }\ ; . $$
it is the curvature of a connection on a principal u ( 1 ) bundle over parameter space . in describing the quantum hall effect , we have a hamiltonian , which depends on a number of parameters $h ( r_1 , r_2 , . . r_n ) $ . suppose we have the system in its ground state . we now vary the parameters adiabatically ( slowly ! ) . as we vary the parameters , we can think of tracing out a curve $ ( r_1 ( \lambda ) , ( r_2 ( \lambda ) . . . ( r_n ( \lambda ) ) $ in parameter space . as we twiddle the parameters we actually evolve the state using the schroedinger equation . if we transport it round a closed curve in parameter space , i.e. we return to our starting parameters , we find that the state picks up a phase factor relative to the starting state . ( phase factors live in $u ( 1 ) $ the group of unit modulus complex numbers ) . the mechanism that allows us to go from a state at one set of parameters to a state at another set is called a connection . in this case the connection is provided by the schroedinger equation . saying that the connection has curvature just means that transport of a state round a closed curve using this connection does not quite get you back to the state you started with ( in fact mathematically the object which defines the curvature is obtained by transport round a little closed parallelogram ) .
classical chaotic systems can be used to generate random numbers . specifically , if a system is chaotic then it will have a positive lyapunov exponent and so will be unpredictable . although classical mechanics is deterministic , it is not possible to know the initial conditions to infinite precision . therefore it is not possible to predict the future state of a chaotic system . if the future state cannot be predicted by any means , then it is random . you would have to make a study of the particular system in order to determine the rate at which randomness is produced , and the best way to extract it , but it can be done . chaotic systems are all around us ( the weather , turbulence , various electronic circuits , etc . ) however , in a practical sense , it is not possible to do anything without quantum mechanics since you live in a quantum world . in other words , anything you build in this physical world will , underneath it all , be quantum mechanical . now , with quantum mechanics there is something very nice that you can do . it is in fact possible to build a random number generator in which the numbers produced are certifiably random , even if you do not trust the hardware ( say , the hardware was built by your adversary ) . for more information on this , search for " certifiable quantum dice " by umesh vazirani ( which i have not read ) .
firstly , $tan ( \theta ) = \frac{v^2}{rg}$ means that as you increase the centrepital velocity , $v$ , of the pendulum , the angle between the rope and the vertical increases . now as the pendulum gets faster and faster , the angle gets larger and larger but never exceeds 90 degrees . i will not provide supporting data , as the above equation is all the data you need .
turbulence is indeed an unsolved problem both in physics and mathematics . whether it is the " greatest " might be argued but for lack of good metrics probably for a long time . why it is an unsolved problem from a mathematical point of view read terry tao ( fields medal ) here : http://terrytao.wordpress.com/2007/03/18/why-global-regularity-for-navier-stokes-is-hard/ why it is an unsolved problem from a physical point of view , read ruelle and takens here : http://www.ihes.fr/~ruelle/publications/%5b29%5d.pdf the difficulty is in the fact that if you take a dissipative fluid system and begin to perturb it for example by injecting energy , its states will qualitatively change . over some critical value the behaviour will begin to be more and more irregular and unpredictable . what is called turbulence are precisely those states where the flow is irregular . however as this transition to turbulence depends on the constituents and parameters of the system and leads to very different states , there exists sofar no general physical theory of turbulence . ruelle et takens attempt to establish a general theory but their proposal is not accepted by everybody . so in answer on exactly your questions : yes , solving numerically navier stokes leads to irregular solutions that look like turbulence no , it is not possible to solve numerically navier stokes by dns on a large enough scale with a high enough resolution to be sure that the computed numbers converge to a solution of n-s . a well known example of this inability is weather forecast - the scale is too large , the resolution is too low and the accuracy of the computed solution decays extremely fast . this does not prevent establishing empirical formulas valid for certain fluids in a certain range of parameters at low space scales ( e . g meters ) - typically air or water at very high reynolds numbers . these formulas allow f . ex to design water pumping systems but are far from explaining anything about navier stokes and chaotic regimes in general . while it is known that numerical solutions of turbulence will always become inaccurate beyond a certain time , it is unknown whether the future states of a turbulent system obey a computable probability distribution . this is certainly a mystery .
another proposal involves putting large pipes into the middle of the oceans ( tropical regions ) and pumping cold water from the depths to the surface . here is one paper that analyzes it . i will try to summarize . this is a very multifaceted idea , so i am going to try to keep to pure physics as well as i can . firstly , what is the engineering involved with this ? in order to pipe cold water from deep down to the surface we would have to pump it against the density gradient . hot water in this temperature range is less dense , so it naturally hangs around the surface . this would take energy and very large pipes . the good news , however , is that the conditions in the ocean make this a little easier in several ways . the construction and placement of mile-long ( or so ) pipes would not be that impractical , as you would have a floating thing holding them up and they would just hang there . the pumping could actually be done by the waves themselves , and it moves the pipe up and down . for this deployment , you would mainly require some one-way valve that allows the water to travel up the pipe but not down . this would not be all that difficult . so we have established we can engineer and build these things that would make the ocean surface cooler . studies also seem to indicate that the capital cost would not be prohibitive . ask it of your engineers and they should be able to make it . next , how would this affect climate ? several ways . the decreased surface temperature of the ocean would cause it to be a large carbon sink . the ocean has already demonstrated a measurable increase in ph and a decrease rate of co2 absorption , which is a natural consequence of increasing concentration . decreasing the temperature would cause it to suck up more . the decreased surface temperature would reduce atmospheric temperature . this is a pretty obvious impact , and the paper i reference notes that this could be the most major immediate effect . i would change the ocean life dramatically . obviously , since phytoplankton are one of the most major photosynthesizers in the world , changing their environment would change the rate of co2 capture by the environment . how ? i have no idea . i am not sure the context or motivations in which researchers propose and discuss such ideas , but i have heard some arguments for geo-engineering proposals in general , of which this proposal is one . for one , the idea is entirely doable as i have argued . it could decrease the temperature of the earth and it could be done with our resources today . if the situation on earth became so perilous to human life , it is likely that politicians would order such a thing . with that established , the motivations become awfully warped . many people argue that quantifying the effect of geo-engineering in the future could motivate current negotiations for emissions mitigation to take the necessary steps . almost all geo-engineering proposals have a temporary effect , so after implemented , they risk a return to an even hotter climate , and possibly deleterious effects due to the solution itself that could be even worse than global warming itself . to add my own personal thinking , it is obvious that we are not reducing emissions today and will not in the conceivable future , so if one believes climate change will significantly impact life , then it is very likely that some form of this geo-engineering will be the future , whether we like it or not . here is a picture : http://www.popsci.com/node/9798 ( illustration credit to graham murdoch of popular science , believed to be fair use )
you have probably learned about a quantity called " impulse " - try using that to solve the problem . let $j$ be impulse , defined for constant forces as $j = f t$ where $f$ is the force applied and $t$ is the time for which the force is applied . since $f=ma$ , we can substitute this to get : $$\begin{aligned} j and = ft \\ j and = mat \\ j and = m ( at ) \\ j and = m \delta v \\ j and = \delta ( mv ) \\ j and = \delta p \end{aligned}$$ that is , $f t$ is equal to the change in momentum of the cart . since both carts have the same force applied to them for the same amount of time , the momentum change of both carts must be equal . for part ( b ) , consider conservation of energy . the force does a certain work on the cart , which causes the cart to gain kinetic energy . to find the work done on each cart , simply find the increase in kinetic energy of each one . try this part by yourself , and comment if you need further assistance .
let $a$ hermitian operator corresponding to an observable . if $\psi$ is an eigenfunction of a , then $$ a\psi = \lambda\psi $$ we say : $a$ has the value $\lambda$ on $\psi$ . if it is not an eigenfunction , then $$ a\psi = \lambda_1\psi_1 + \lambda_2\psi_2+\dots $$ that is , after the measurement the state changes and you cannot associate a definite value $\lambda$ to it .
the energy formula $$\tag{39.11} e~=~\frac{1}{2}mv^2 -\frac{1}{2} m ( {\bf \omega} \times {\bf r} ) ^2 + u $$ in ref . 1 ( of a point particle , as seen in a rotating reference frame $k$ ) consists of three terms : kinetic energy : $\frac{1}{2}mv^2$ . centrifugal potential energy : $-\frac{1}{2} m ( {\bf \omega} \times {\bf r} ) ^2$ . other potential energies $u$ . in particular , the minus sign in front of the second term is the correct one . it is a centrifugal potential , so it encourages the system to increase its radial coordinate $r$ . phrased equivalently , it costs work ( against the centrifugal potential ) to reduce the radial coordinate $r$ . references : l.d. landau and e.m. lifshitz , mechanics , vol . 1 , 1976 .
a backlit sensor is not really a new kind of sensor , it is just a different arrangement of the imaging elements to allow more light into the sensor . from wikipedia : a traditional digital camera sensor consists of a matrix of individual picture elements . each element is constructed in a fashion similar to the human eye , with a lens at the front , sensors at the back , and wiring in between . the front of the detectors require an active matrix to be placed on their front surface . the matrix and its wiring reflects or absorbs some of the incoming light , thereby reducing the signal that is available to be captured . a back-illuminated sensor moves this wiring behind the sensors , similar to a cephalopod eye . they are not without issues though : moving the active matrix transistors to the back of the photosensitive layer normally leads to a host of problems , such as cross-talk , which causes noise , dark current , and color mixing between adjacent pixels . backlit sensors improve the low-light performance of a ccd chip , reducing the iso needed for low-light conditions ( and thus reducing noise ) . this is a clear benefit to astrophotographers , allowing them to use shorter exposures with less noise to get equivalent shots .
there are a lot of factors that go into whether or not a planet has an atmosphere . first , the mass and size of the planet . really what it comes down to is the escape velocity . the higher the escape velocity ( v e ) , the easier it is for a planet ( or moon ) to retain any atmosphere it gets as the gases that make up the atmosphere have to be moving faster to escape . now v e is proportional to the square root of mass divided by radius and mass is proportional to density times radius cubed . since the density of the rocky bodies in the solar system are basically the same ( and the icey bodies only differ by a factor of 2-3 ) density is roughly constant and that means that v e is basically just proportional to the radius of the object . so larger objects have a higher escape velocity and are more likely to retain an atmosphere . next is temperature which is just a measure of the average energy of the particles . so the gas molecules that make up the atmopherers of hot planets have a higher average energy than the molecules in a planet that is cooler . since the average energy translates into the speed of the gas particles , gasses on warmer planets are more likely to be close to or over the escape velocity of the planet and as such can fly off into space . so cooler planets have a higher chance of holding on to their atmospheres since the particles are moving slower . next we have atmopheric composition . at a given temperature more massive particles are going to be moving slower . thus if you have hydrogen gas , with a molecular weight of 2 , compared to oxygen gas , molecular weight 32 , the hydrogen will be moving on average 16 times faster and is much more likely to escape . co 2 , with a molecular weight of 44 , will be moving even slower still . thus for a given planet with a fixed mass and temperature , the lighter gasses will escape first . finally we have availability of materials . if there are no gasses there to start with and no way to acquire them , it does not matter how massive or how cold the planet is , there is nothing there to hold on to .
short answer . the relation between newtons and kilograms , with respect to work&nbsp ; /&nbsp ; kinetic energy , is actually just through newton 's second law ! here we are not mixing up newtons and kilograms ( forces and masses ) . the mass plays a role in the kinetic energy equation precisely because mass plays a role in how much force it takes to accelerate an object from rest to a speed v . example . consider for instance an object which has a mass of , say , 0.1019&nbsp ; kg . the gravitational force that this body experiences at the surface of the earth is &minus ; 1&nbsp ; n ( that is , a downward force ) , if we take g &nbsp ; =&nbsp ; &minus ; 9.81&nbsp ; m/s&sup2 ; . if we hold this object from a height of one meter , and allow it to drop , gravity performs work on this object , exerting a force of &minus ; 1&nbsp ; n over a ( downward ) displacement of &minus ; 1&nbsp ; m . as a result , just before impacting the ground , the object will have 1 joule of kinetic energy , as ( &minus ; 1&nbsp ; n ) &nbsp ; &middot ; &nbsp ; ( &minus ; 1&nbsp ; m ) = +1&nbsp ; j ; gravity will have done 1 joule worth of work on the object , which causes it to move with 1 joule worth of kinetic energy . in the examples you give , the correspondence that you are looking for is between mass and the gravitational force that it exerts . when you say that you take "1 newton " and lift it one meter , the ' newton ' you are referring is one newton of force exerted downward by an object with some mass &mdash ; or more to the point , the one newton which would be the minimum necessary to raise it steadily by opposing gravity . derivation we can actually show directly how mass comes into the kinetic energy formula , and pinpoint the reason that it is there . suppose that we exert a net force f on an object with mass m , over some displacement d in the same direction as the force , where the object starts at rest . the amount of work done on the object will be $$ w \ ; =\ ; \mathbf f \cdot \mathbf d \ ; =\ ; fd , $$ taking f to be the magnitude of the force f , and d to be the length of the displacement d . because we are doing net work on the object starting from rest , this work will go directly towards the kinertic energy of the object . the acceleration of the object under this force is $$ \mathbf a \ ; =\ ; \mathbf f / m . $$ if t is the time that it takes for the object to be moved by a displacement of d , we have $$ \mathbf d \ ; =\ ; \tfrac{1}{2}\mathbf a \ , t^2 \ , ; \qquad\implies\qquad t \ ; =\ ; \sqrt{2d/a\ ; } \ ; =\ ; \sqrt{2dm/f\ ; } . $$ taking the magnitude a &nbsp ; =&nbsp ; ||&nbsp ; a &nbsp ; || . the speed that the object is travelling after that amount of time is just $$\begin{align*} v \ ; =\ ; a t \ ; and =\ ; \bigl ( f/m\bigr ) \sqrt{2dm/f\ ; } \\ [ 1ex ] and = \sqrt{2fd/m\ ; } , \end{align*}$$ applying newton 's second law again for the acceleration , and cancelling factors of f and m under the square-root . we may then re-express this equation as $$ v^2 \ ; =\ ; 2fd/m \qquad\implies\qquad \tfrac{1}{2}m\ , v^2 = fd = w , $$ where we just applied the formula for the work done on the object at the end . because the net work is the same as the kinetic energy in this case , it follows that $k = \tfrac{1}{2}m\ , v^2$ . in the derivation above , the only ways we used mass was with newton 's law , a &nbsp ; =&nbsp ; f / m . so , the fact that it occurs in the formula for kinetic energy is not because we are confusing mass with force , but because of the relationship between mass and force .
edit : i am not sure what specifically you are after . i will explain a little more why it is difficult to give you a number . it is also quite possible that the number you are seeking might not be what you think it is . . . . the decay would be similar to any plasma type reaction for a mercury-vapor gas . there is a delay between absorption and re-emission of light photons and a typical electrical system driving the energy is not going to stop instantly . in fact the dominate feature in the decay rate will probably be due to the slow discharge of the ballast system driving the gas . the common household circuit lighting up a tube usually has a large transformer with a big magnetic field to step up the voltage and resonate the gas to force the plates to conduct through the ionized gas and light up . it takes a considerable amount of time for this electrical system to discharge and while it does it is going to continue to drive energy to the lamp causing it to light and effecting your " decay " rate significantly . here is a 12v lamp driver just for you to see what is involved electrically : note : the large cap ( 0.047uf ) and the transformer are going to resonate the lamp energy much longer than the actual natural decay rate of the gas . in addition the 47uf cap is going to supply power to the circuit for a non-trivial amount of time after the 12v is removed . for comparison here is an 120v 60hz ballast design and you will see there are similar issues . if you want to look at just the gas decay rate then you might have to excite it with a laser for more precise measurements . the states that the energy moves through is usually measured and plotted on a jablonski diagram like this : in your case $\tau=\frac{1}{k}$ where k is equal to ( in the de-excitation case ) $k = k_f + k_i + k_x + k_{et} + …= k_f + k_{nr}$ where $k_f$ is the rate of fluorescence , $k_i$ the rate of internal conversion and vibrational relaxation , $k_x$ the rate of intersystem crossing , $k_{et}$ the rate of inter-molecular energy transfer and $k_{nr}$ is the sum of rates of radiationless de-excitation pathways . and a more detailed model can be measured : the method for measuring a gas properly looks like : here is an example of what the data looks like . much of this information was taken directly from the research documented here : www.jh-inst.cas.cz/~fluorescence/support/lectures/ufch_fluor03.pps‎
light has a dual nature , one of photons and the other of waves . but energy does not really travel in waves . so what do the wave represent ? let us be clear in our terminology and the domain to which we apply it . our everyday life is lived with classical mechanics and classical electricity and magnetism ( as long as we do not use the net and transistors and the other paraphernalia of modern life ) . classical theories are well developed mathematically and are applicable in the domain where hbar can be considered practically zero . particularly for light maxwell 's equations have been validated in this domain and describe light as a wave . this wave is a propagating changing electric and magnetic field and is a wave in the four dimensional space time , there are peaks and valleys . it displays the classical wave behavior of interference and dispersion . these carry energy , a universal example is simple sunlight . now you use the word photon . the photon is one of the elementary particles of the standard model . this means that we are no longer in the classical domain but in the quantum mechanical domain when we speak of light as a collection of photons . it means that it behaves sometimes as a particle , ( photoelectric effect ) i.e. it has a specific ( x , y , z , t ) value and sometimes as a probability wave , i.e. according to a quantum mechanical wave function the square of which gives the probability of finding a photon at a specific ( x , y , z , t ) which probability has a wave like variation in space because it is the solution of a wave type differential equation describing the dynamics of the situation . note , it is the probability which shows a wave nature , not the " particle " itself . now the wave nature of maxwell 's equation and the wave nature of the probability function are mathematically reconcilable , so that the frequency of the classical wave is the nu in the energy of the photon in e=h*nu . in addition lubos motl has an interestin article about the collective emergence of classical light from photons in his blog , though it needs a background in physics to understand it .
we have also the same notions of derivation , curl , etc . . . for functions that are less regular . when you write maxwell 's equations , you are writing a system of partial differential equations . to investigate them , you have to specify the type of solution you look for ( in the language of pdes : classic , mild , weak . . . ) and the functional space you set your theory in . a natural space for the electric and magnetic fields is $l^2 ( \mathbb{r}^3 ) $ , because this is the energy space ( where the energy $\int_{\mathbb{r}^3} ( e ( x ) ^2+b ( x ) ^2 ) dx$ is defined ) . also more regular subspaces , such as the sobolev spaces with positive index , or bigger spaces as the sobolev spaces with negative index are often considered . these spaces rely on the concept of almost everywhere , i.e. they can behave badly , but only in a set of points with zero measure . also , the sobolev spaces generalize , roughly speaking , the concept of derivative . i suggest you take a look at some introductory course in pdes and functional spaces . a standard reference may be the book by evans , or also the monumental work by hörmander . comment to the edit : it is not true that the maxwell equations in differential form , will always give a nicely behaved continuous and differentiable vector field solution consider , e.g. the static equation \begin{equation*} \nabla\cdot e=\rho \ ; . \end{equation*} to investigate this equation , you have to give it a precise meaning . what are $e$ and $\rho$ ? let 's assume , as you said , that $\rho$ is some discontinuous function . then it is quite strange to look for solutions of $e$ that are smooth and well behaved ! we have mathematical objects that can behave even worse than discontinuous functions , and are called distributions . in particular , we are interested in the distributions dual to functions of rapid decrease , that are called $\mathscr{s}' ( \mathbb{r}^3 ) $ . without entering into details , all functions in $l^p ( \mathbb{r}^3 ) $ , $1\leq p \leq \infty$ are distributions in $\mathscr{s}'$ , as well as dirac 's delta function and its derivatives . and mathematically , it is perfectly legitimate to look at the divergence equation above in the sense of distributions : i.e. to search a distribution $e\in ( \mathscr{s}' ( \mathbb{r}^3 ) ) ^3$ such that its distributional divergence $\nabla\cdot e \in \mathscr{s}' ( \mathbb{r}^3 ) $ is equal to $\rho\in\mathscr{s}' ( \mathbb{r}^3 ) $ . suppose that equation admits a solution , then this solution would not , in general , be a regular function , but a distribution . it may be , for example , a discontinuous function in $l^1$ , or a sum of derivatives of the delta function . anyways , as i already wrote , it is necessary that you understand better the concept of cauchy and boundary value problems for pdes in functional spaces , and also the concept of classical , mild and weak solutions to understand fully the machinery behind maxwell 's equations , and the mathematical meaning of a solution for such a problem .
re question 1: you had typically measure the abundance ratio by putting a sample of your element in a mass spectrometer and counting how many atoms of the different masses you get . this is exactly how carbon dating works : it counts the c$^{12}$ and c$^{14}$ atoms and the ratio tells you how much c$^{14}$ has decayed . re question 3: it is difficult to find rocks that have been unchanged since the earth was formed because plate tectonics tends to mangle early rocks . the age of the solar sytem , and therefore the earth , is calculated by doing radiometric dating of meteorites . for example the allende meteorite is dated at 4.567 billion years old . we assume that the earth formed at the same time as the meteorites . the oldest rocks on earth are somewhere around 4 billion years old depending on whose claims you believe . to back to your question 2: i do not know of references for the isotopic composition of the early earth , but i imagine it would be easily googled . it is difficult to be sure about the isotopic ratios because we do not know what the ratios were in the dust cloud that formed the earth , however the ratios can be calculated in some cases . for example uranium decays to lead , but of course there is lots of lead around anyway so we can not tell what lead has come from uranium decay and what was there originally . once again we use meteorites . by finding meteorites that contain lead but no uranium we can infer the original isotopic distribution of the lead . then by comparing the isotopic distribution in earth rocks with the distribution from the meteorites we can work out how much lead has come from uranium decay and therefore what the original uranium ditribution was .
as several of us have argued beneath your other question here : special relativity version of feynman 's " space-time approach to non-relativistic quantum mechanics " quantum mechanics combined with special relativity needs one to use quantum field theory ( or a theory that is stronger than that ) . if one wants to study relativistic particle in the presence of slits , one may imitate it simply by putting various ( absorbing ) boundary conditions ( for the fields whose excitations are the interfering particles ) at the places where the boundaries of the material reside . the probability waves in space propagate according to the standard free equations , so interference measures the free particles ' propagators . in reality , the only relativistic particles whose interference may be observed in practice at present are photons . the mathematics of interference of individual photons is equivalent to the interference of classical electromagnetic waves - as they studied it since the early 19th century . the probability density is mapped to the energy density of an electromagnetic wave and the $e , b$ fields may be interpreted as the photon 's wave function . the results for the interference patterns are , up to a normalization that can be determined from the total number of particles , identical to the case of the classical electromagnetic waves . so papers for photons exist and the first ones were written in the early 19th century . papers for other particle species do not exist because they are simple sums of the well-known functions that govern the propagator of the corresponding probability waves in the space ; for example , one only needs the simple , free one-particle dirac equation to calculate the propagation of the electrons at any speeds the interference can only be measured for photons whose mathematical description is a very old story . for example , relativistic electrons must have a wavelength comparable to the compton wavelength of the electron , something like $10^{-12}$ meters , or even shorter ( ultrarelativistic electrons ) . the corresponding inteference pattern would probably be too tiny to be seen cheers lm
the right definition of $\hat n$ has the additive constant under which $\hat n |0\rangle = 0$ . you should view your naive expression for $\hat n$ as an " inspiration " that may be used as a starting point to deduce a meaningful operator but a modification is needed . so just define $\hat n$ as the normal-ordered expression without the infinite additive $c$-number term . yes , you made a mistake . when you commute $\hat n$ to the right side so that it annihilates $|0\rangle$ , you have to permute $b^\dagger_{s'} ( \vec k ) $ through two fermionic operators that appear as factors of $\hat n$ , namely $b$ and $b^\dagger$ , so you pick two minus signs and they cancel .
yes there is a simple explanation . think of a metallic sphere that is not connected to anything ( i.e. . floating ) , move this conducting sphere close to a positive electrode . now what happens is , the negative charges are induced on the surface of the sphere closest to the electrode ; those negative charges leave positive charges behind . just like figure ( a ) here : the charge that accumulated at the surface because of induction by electrode affects the voltage everywhere . in electrostatics , the method of image is used to include the contribution of induced surface charge to the total voltage everywhere . the only parameter that one needs to describe a floating potential conducting sphere is the initial charge . in the figure above the total charge is zero . if the sphere was initially charged by a positive or negative charge , the induced surface charge will be affected depending on the initial charge magnitude and polarity . now speaking of mathematics , the electric flux is related to the volume charge density by gauss law : in the previous equation , d is the electric flux and rho is the volumetric charge density and q0 is the total charge in the volume . the previous equation basically states that the charge enclosed within a volume generates an electric flux that if integrated on the surface of the enclosing volume gives a constant value equal to the charge enclosed . in conductors it is known that there is no volumetric charge , instead there is only surface charge , so for the conducting sphere case gauss law becomes : the difference between the left hand side and the right hand side is that lhs is evaluated anywhere in space where r ( the radial distance ) > r ( the radius of the sphere ) . rhs is only evaluated at the surface of the sphere where surface charge is located . now in your example , first you assumed zero initial charge ( that is why the right hand side of your condition is zero ) ; second you only care about the part of conductor that faces the electrodes , that is why your surface integral is open rather than closed as gauss law dictates . to confirm this explanation try to plot the surface charge along the edge on which you defined a floating potential . i replicated your model , the voltage everywhere is : i plotted the surface charge density along the floating potential boundary : you see , a negative charge was induced close to the positive electrode leaving an identical positive charge behind ( close to grounded electrode ) , which is what happened in figure ( a ) above . if you integrate the surface charge density along the boundary the result is zero ( there is no initial charge ) . if you used non-zero initial charge , the integration result will be equal to that value . keep in mind the accuracy of the solution depends on the resolution of the mesh , so the value in case of zero charge is never equal to zero exactly because of numerical error . i hope that answered your question
the answer is just option 2--- the integral on the two sides are necessarily equal in a compact orientable space . the condition of orientability cannot be omitted , see this example : dirac string on ( periodic ) compact space
you can use a negative charge to test an electric field . you just have to remember that the electric field points antiparallel ( opposite ) to the force on the charge , rather than parallel to it ( in the same direction ) . that is just a convention , though ; we could have defined the electric field to point with the force on a negative charge , and physics would work the same , except for a couple of negative signs in some formulas .
why are you looking for a radial surface . . ? look it as an equipotential surface ( a surface where all points are at same constant electric potential ) as it comes with sphere . hence , you can assume the points a to b as radial to find the potential difference .
the fundamental effect of gravitational waves exploited by all detectors is the same : one tries to detect minute oscillatory changes in distances between different parts of the device . these changes are of the order of 1 part in 10 20 or smaller , so detecting them is a real challenge . different types of detectors do differ significantly when it comes to the techniques they employ to measure these minute changes . interferometers look out for relative changes in phase of two light beams travelling in perpendicular directions . weber bars exploit resonance to magnify vibrations caused by gravitational waves of a given frequency . pulsar timing arrays compare timing of pulsar signals coming from different directions looking out for small differences caused by tiny expansion and contraction of space through which the signals propagate . high frequency detectors examine microwaves circulating in a loop looking out for polarization changes caused by spacetime contraction and expansion in different directions . see this wikipedia article for details . depending on how good given detector 's isolation from unwanted influences is , it may require substantial effort to extract gravitational wave signature from data collected by these detectors . for example , ligo phase loops lose lock every time a freight train passes by . the einstain@home project allows volunteers to donate the computing power of their home computers to this effort .
a rapid change of volume means that there is little time for heat to escape ( adiabatic process ) . when you compress a gas , you are doing $p \delta v$ work on the gas , and the internal energy of the gas must change . there is nowhere for this energy to go and since the internal energy of the gas is proportional to temperature ( equipartition theorem / ideal gas properties ) the temperature must change . if you compress said gas and then it reach thermal equilibrium with its surroundings , then you can again extract some of this energy in the form of heat , so that it reaches room temperature . now , if you let it expand again , it is temperature will further drop . repeating this process you eventually reach a point in its phase diagram where the gas is a liquid .
the gravitational , massless , bosonic sector of the string effective action contains the metric tensor , and at least one more fundamental field φ , the dilaton . by comparing the einstein’s ( d+1 ) dimensional einstein-hilbert action with the effective tree level action mentioned before a relation between the effective string coupling ( also fixing the g constant ) and the dilaton field becomes manifest . the dilaton in string theory also can be rescaled to absorb trivial volume factors associated with compact spaces , but are also present in the non-compacted string models . the dilaton is a fundamental scalar field in closed string theory . the effective gravity equations in string theory includes the gravi-dilaton part that looks very similar to brans-dicke scalar-tensor theory of gravity ( this is valid only at tree level ) . the dilaton field , as mention before , controls the string coupling constant so the genus expansion in string theory is directly related to the dilaton field and to corrections to general relativity . there is also the possibility of a dilaton potential in noncritical dimensions . this creates the possibility that the dilaton field expectation value be fixed at a local minimum ( probably in a non-perturbative regime ) , fixing the coupling between strings . the dilaton field is then an essential component of all superstrings models , and thus of the cosmological scenarios based on effective string actions . chapter 9 in maurizios gasperini’s string cosmology book is a very nice introduction to dilaton phenomenology and its importance in cosmology .
according to this article from the nasa web site , in the vicinity of the earth the temperature in the shade is -156c . the boiling point of oxygen at atmospheric pressure is -183c , so if the pressure in your container is at or less than atmospheric pressure the oxygen will not condense . the nasa article does not say what is keeping the temperature at -156c , rather than the 2.7k of the microwave background . possibly it is thermal radiation from the earth .
the earth receives approximately $6.8\text{mw/m}^2$ of reflected sunlight from the moon ( see below for details of how i calculated that ) . however , the sunlight is also absorbed by the moon and this raise the surface temperature . so the moon also emits thermal radiation towards the earth ( assuming the highest day time temperature of 400k , see comments below for more information ) , $\epsilon_{\text{moon}} ( 1-a ) \sigma ( 400k ) ^4 = 89\text{mw/m}^2$ so the total power received from the moon ( reflected + thermal ) is 10,438 times weaker than sunlight , i.e. $$ \frac{6.8\text{mw/m}^2 + 89\text{mw/m}^2}{1000\text{w/m}^2} = \frac{1}{10438} $$ to answer your question about how much that heats the earth , let 's assume that the average daytime temperature of the earth is 20$^\circ$c and the average nighttime temperature is 10$^\circ$c ( these estimates could be improved , but it does not really change the answer significantly ) . therefore the incident solar energy causes a temperature difference $\delta t=10^\circ$c between night and day . so we know that 1000 $\text{w/m}^2$ ( solar irradiance on the earth surface ) cause a temperature increase of around $10^\circ$c . let 's assume that moonlight will also cause a temperature difference but one that is scaled proportionally by its intensity . moonlight is 10,438 ( reflected and thermal energy ) times weaker than sunlight , the change in temperature of the earth from absorbing moonlight is , $$ \frac{10^\circ c}{10,438} = 958 \mu k $$ good luck measuring that . . . assumptions and method solar irradiance is 1000 $\text{w/m}^2$ at the surface of the moon and the earth . the reflectivity of the moon is about $a=$10% . the solid angle of subtended by the moon in the sky is the same as that subtended by the sun $\epsilon_{\text{moon}} = 6.8\times10^{-5} \text{sr}$ . i say this because during an eclipse they appear to the same size so it is probably quite a good assumption . from 1 and 2 we know that $100\text{w/m}^2$ is reflected at the surface of the moon . from 3 , let 's multiply that by the solid angle subtended by the moon as viewed from the earth as this will give us the amount of the reflected energy that hits the earth . so , $100\text{w/m}^2 \times 6.5\times10^{-5} = 6.5\text{mw/m}^2$ .
to perform canonical quantization of a fermion field , we write the field in creation and annihilation operators . writing the hamiltonian in those creation and annihilation operators , we find that the energy of a field is unbounded from below ( it can be as negative as you like ) . this would be a disaster ; a field could forever decay to lower energy states by e.g. the emission of a photon . that is not what we see . if , however , we insist that the field obeys an anti-commutation rule ( the pauli exclusion principle ) , the energy is bounded from below ( cannot be as small as you like ) . the situation is saved . to summarise : physically , fermions must obey the pauli exclusion principle , because if they did not , they could forever decay to lower energy states . for detail and the mathematics , see any introductory book on quantum field theory .
if the ladder is slipping on the floor as well as the wall , then the point of rotation is where the two normal forces intersect . this comes from the fact that reaction forces must pass through the instant center of motion , or they would do work . in the diagram below forces are red and velocities blue . if the ladder rotated by any other point other than s then there would be a velocity component going through the wall , or the floor . s is the only point that keeps points a and b sliding . this leads to the acceleration vector of the center of mass c to be $$ \begin{aligned} \vec{a}_c and = \begin{pmatrix} \frac{\ell}{2} \omega^2 \sin\theta - \frac{\ell}{2} \dot{\omega} \cos\theta \\ -\frac{\ell}{2} \omega^2 \cos\theta -\frac{\ell}{2} \dot\theta \sin \theta \\ 0 \end{pmatrix} and \vec{\alpha} and = \begin{pmatrix} 0 \\ 0 \\ -\dot\omega \end{pmatrix} \end{aligned}$$ if only gravity is acting , then $$\dot\omega = \frac{m\ , g\ , \frac{\ell}{2}\sin\theta}{i_c+m \left ( \frac{\ell}{2}\right ) ^2} $$
the work is of course equal to the potential energy of the dipole $\vec m$ in the magnetic field , $$ u = -\vec m \cdot \vec b . $$ now , your wording indicates that the magnetic field $\vec b$ is macroscopic so it may be treated as a classical parameter . however , $\vec m$ from a spinning particle is quantum mechanical . for example , for an electron , $$ \vec m = -g_s \mu_b \frac{\vec s}{\hbar} $$ where the spin $g$-factor is $g_s\sim 2.0023$ , $\mu_b=e\hbar/ ( 2m_e ) $ is the bohr magneton , and $\vec s$ is the operator ( or triplet of operators ) of the electron 's angular momentum – that act as $\hbar/2$ times the pauli matrices on the spin-up and spin-down states . so up to a multiplicative constant $\gamma$ , $$ u = \gamma \vec \sigma\cdot \vec b . $$ the work done on the electron is a $q$-number , an operator , which acts on the spin-up and spin-down states . for the up and down states relatively to the direction of the external magnetic field $\vec b$ , the work done has a sharply defined value , an eigenvalue of $u$ . for general linear superpositions , the work done on the electron has a certain probability amplitude to be positive and a certain probability amplitude to be negative . the squared absolute values of these probability amplitudes determine the probabilities that the work will be seen to be one ( positive ) value or the other ( negative ) value , just like everywhere in quantum mechanics . it is however very interesting to consider superpositions of states in this context , especially for $j=1/2$ . for spin-1/2 particles , each superposition of up and down states is equivalent to " up " with respect to a certain axis in space . if we deal with general superpositions , this axis on which the spin is " up " will generally not be aligned with the direction of the magnetic field . if that is the case , the presence of the magnetic field will have the effect of causing " precession " of the axis defining the spin up . the axis at which the particle is polarized " up " will be " turning around " the direction of $\vec b$ like a ninny . this precession results from the time-dependent change of the phases of the wave function – which is opposite for the up and down states , so the relative phase is changing as well – and the fact that the relative phases of the wave functions always matter ( for something ) in quantum mechanics . the classic experiment measuring this spin-dependent magnetic work and exhibiting lots of the quantum properties is called the stern-gerlach experiment . with some probabilities , the particle behaves in one way or another .
the canonical metric on $cp^n$ is the fubini-study metric . the distance between two states $\left| x \right\rangle$ and $\left| y \right\rangle$ is $$\gamma ( x , y ) = \arccos \sqrt{\frac{\left| \left\langle x \middle | y \right\rangle \right|^2}{\left\langle x \middle | x \right\rangle \left\langle y \middle | y \right\rangle}} . $$ the infinitesmal metric is thus : $$ds = \frac{\langle dx | dx \rangle}{\langle x | x \rangle} - \frac{\left | \langle dx | x \rangle \right|^2}{\left | \langle x | x \rangle \right|^2} . $$ notice that for $cp^1$ this reduces to the natural metric on the bloch sphere .
if i am only allowed to use one single word to give an oversimplified intuitive reason for the discreteness in quantum mechanics , i would choose the word ' compactness ' . examples : the finite number of states in a compact region of phase space . see e.g. this phys . se post . the discrete spectrum for lie algebra generators of a compact lie group , e.g. angular momentum operators . see also this phys . se post . on the other hand , the position space $\mathbb{r}^3$ in elementary non-relativistic quantum mechanics is not compact , in agreement that we in principle can find the point particle in any continuous position $\vec{r}\in\mathbb{r}^3$ . see also this phys . se post .
nowadays , rockets use a gimbaled thrust system . the rocket nozzles are gimbaled ( an appliance that allows an object such as a ship 's compass , to remain horizontal even as its support tips ) so they can vector the thrust to direct the rocket . in a gimbaled thrust system , the exhaust nozzle of the rocket can be swivelled from side to side . as the nozzle is moved , the direction of the thrust is changed relative to the center of gravity of the rocket . early rockets had vernier thrusters which uses small rocket engines on either sides , to control the altitude of a rocket . nowadays , they are common in most satellites . in this image , the middle rocket shows the normal flight configuration in which the direction of thrust is along the center line of the rocket and through the center of gravity of the rocket . on the left one , the nozzle has been deflected to the left and the thrust line is now inclined to the center line at a gimbal angle $a$ . as the thrust no longer passes through the center of gravity , a torque is generated about the center of gravity and the nose of the rocket turns to the left . if the nozzle is gimbaled back along the center line , the rocket will move to the left . on the right one , the nozzle has been deflected to the right and the nose is moved to the right . wikipedia says , in spacecraft propulsion , rocket engines are generally mounted on a pair of gimbals to allow a single engine to vector thrust about both the pitch and yaw axes ; or sometimes just one axis is provided per engine . to control roll , twin engines with differential pitch or yaw control signals are used to provide torque about the vehicle 's roll axis . the right and left gimbaling is necessary to direct the rocket to its original path , thereby maintaining its stability . . . this link gives a good explanation regarding the stability of rockets . this essay is also good , but it is somewhat big . . .
let us reformulate the question ( v1 ) as a 1d kinematic mouse-and-cat optimal control problem . the masses are irrelevant for the kinematic problem and hence put to $$m~=~1 . $$ 1 ) consider first the cat . the goal for the cat is to obtain the position and velocity ( ! ) of the mouse as quickly as possible . the cat can accelerate $$|a|~\leq~ a_{0} , $$ where $a_{0}&gt ; 0$ is a maximal acceleration . ( update : this type of problem is in optimal control theory known as a double integrator . see also the textbook by h.p. geering , optimal control with engineering applications , springer , 2007 , section 2.1.4 . ) we want to show that , ideally speaking , there is an optimal strategy so that the acceleration of the cat is at all times either the maximally allowed or none , $$ a ( t ) ~\in~\left\{ -a_0,0 , a_0 \right\} , $$ i.e. , the control parameter $a$ has the bang-bang property . let us define a signed kinetic energy $$k~:=~\frac{mv|v|}{2}~=~\frac{v|v|}{2}~=~t ~{\rm sgn} ( v ) , \qquad \qquad t~:=~\frac{mv^2}{2}~=~\frac{v^2}{2}~=~|k| . $$ it is convenient to consider a $ ( x , k ) $ coordinate system . one may view it as a configuration space ( or phase space ) of the system , because the map $v \leftrightarrow k$ is a bijection : $\mathbb{r}\to\mathbb{r}$ . in particular , one may plot the trajectories of the cat and the mouse in a $ ( x , k ) $ diagram . from the work energy theorem , the slope of the trajectory is ( up to a sign ) the acceleration $$ a~=~ma~=~\frac{dt}{dx}~=~ \frac{dk}{dx} {\rm sgn} ( v ) . $$ thus the cat in initial state $ ( x_0 , k_0 ) $ must proceed within a cone $c ( x_0 , k_0 ) $ as indicated in red in figures 1 , 2 and 3 . the cat can exit the cone $c ( x_0 , k_0 ) $ through the $x$-axis $k=0$ only , and turn around , in order to reach a final state $ ( x , k ) $ outside the cone . $\uparrow$ figure 1 . case $k_0&gt ; 0$ . the red region denotes the cone $c ( x_0 , k_0 ) $ . the black oriented paths indicate optimal strategies for the cat to reach 3 different final states $ ( x , k ) $ . $\uparrow$ figure 2 . the cone $c ( x_0 , k_0 ) $ marked with red in the case $k_0=0$ . $\uparrow$ figure 3 . the cone $c ( x_0 , k_0 ) $ marked with red in the case $k_0&lt ; 0$ . in mathematical detail , the cone $c ( x_0 , k_0 ) $ is $$ c ( x_0 , k_0 ) ~:=~\left\{ \begin{array}{lcc} c_{+} ( x_0 , k_0 ) and {\rm for} and k_0&gt ; 0 , \cr\cr c_{+} ( x_0 , k_0 ) \cup c_{-} ( x_0 , k_0 ) and {\rm for} and k_0=0 , \cr\cr c_{-} ( x_0 , k_0 ) and {\rm for} and k_0&lt ; 0 , \end{array} \right . $$ where we have defined the positive and negative cones as $$ c_{\pm} ( x_0 , k_0 ) ~:=~ \left\{ ( x , k ) \in\mathbb{r}^2 \mid \pm a_0 ( x-x_0 ) \geq |k-k_0| \wedge \pm k\geq 0\right\} . $$ for the cat to go from state $ ( x_0 , k_0 ) $ to state $ ( x , k ) $ , there is an optimal strategy that leads to minimal time consumption $\tau ( x , k ; x_0 , k_0 ) $ , which we have tried to indicate in figure 1 . roughly speaking , the cat should choose a route as far away from the $x$-axis $k=0$ as possible , as it is most costly in terms of time to have small speed . if the final state $ ( x , k ) \in c ( x_0 , k_0 ) $ is in the cone , then two legs are needed ( one maximal acceleration and one maximal de-acceleration ) . it is straightforward to calculate that the minimal time consumption $\tau ( x , k ; x_0 , k_0 ) $ for $ ( x , k ) \in c ( x_0 , k_0 ) $ is $$ \tau ( x , k ; x_0 , k_0 ) ~=~ \frac{2\sqrt{|k|+|k_0|+a_0|x-x_0|} -\sqrt{2|k|}-\sqrt{2|k_0|}}{a_0} . $$ there are similar expressions for $\tau ( x , k ; x_0 , k_0 ) $ in various cases where $ ( x , k ) \notin c ( x_0 , k_0 ) $ but with more legs/terms , which we will leave as an exercise to determine . 2 ) next consider the mouse . let us assume that the full future trajectory of the mouse $t\mapsto x_1 ( t ) $ , $t\geq 0$ , is known to an all-knowing cat . ( there are other possible rules of the game , but this setup seems to be closest to what op is after . ) let the velocity and the signed kinetic energy of the mouse be denoted $$v_1 ( t ) ~=~\frac{dx_1}{dt} \qquad {\rm and}\qquad k_1 ( t ) ~=~\frac{v_1 ( t ) |v_1 ( t ) |}{2} , $$ respectively . for each future time $t\geq 0$ , define the difference $$\delta\tau ( t ) ~:=~ \tau\left ( x_1 ( t ) , k_1 ( t ) ; x_0 , k_0\right ) - t$$ between the time the cat could be at the mouse state $ ( x_1 ( t ) , k_1 ( t ) ) $ ( if the cat made a run for it ) , and the time $t$ the mouse would be there . if the two initial states of cat and mouse are different , $$ ( x_0 , k_0 ) ~\neq~ ( x_1 ( t=0 ) , k_1 ( t=0 ) ) , $$ then $\delta\tau ( t=0 ) &gt ; 0$ . the first instant $t_*$ that the cat can obtain the $ ( x , k ) $ state of the mouse is the first time that $\delta\tau ( t ) $ turns non-positive , $$t_*~=~\inf \left\{ t\in \mathbb{r}_+ \mid \delta\tau ( t ) \leq 0 \right\} . $$ this is the answer to how quickly the cat can obtain the position and velocity of the mouse .
you probably know that the mass of the higgs boson is around $125$ gev , which means the energy it takes to create a higgs boson is around $125$ gev and therefore that the temperature at which significant numbers of higgs bosons will be created will be given by $kt = 125$ gev . one gev is $1.602 \times 10^{-10}$j , so the corresponding temperature is around $10^{13}$k - note that this is an order of magnitude estimate . anyhow , the temperature at the centre of the sun is around $10^7$ k , so it is six orders of magnitude too low to create significant numbers of higgs bosons . even a supernova only gets to a temperature of about $10^{11}$k , which is still two orders of magnitude too low .
part of the problem is you are trying to take a derivative of the delta function . you can do this inside an integral using a test function . also , have you tried $\delta ( x ) = \lim_{\epsilon\to 0^+}\frac{1}{2\pi\epsilon} e^{-\frac{x^2}{2\epsilon}}$ also , do not drag around the $t'$ and $r'$ , because $\frac{d}{dt} = \frac{d}{d ( t-t' ) }$ , so you are just slowing yourself down giving opportunity for mistakes to creep in . when you are done , just restore the $t'$ and $r'$ . finally , note that $\vec\nabla \delta ( t-|x| ) = -\delta' ( t-|x| ) \frac{\vec{x}}{|x|}$ $\nabla^2 \delta ( t-|x| ) = \delta'' ( t-|x| ) -\delta' ( t-|x| ) \frac{2}{|x|}$ this should be sufficient .
a process where the energy is kept constant is called isoenergetic ( or , if you prefer , iso-energetic ) . it also seems from the literature that a flow where the energy is constant when following a fluid particle is usually called an isoenergetic flow . similarly , when the enthalpy is kept constant , the process ( or the flow ) is said to be isenthalpic ( or isoenthalpic ) . and so on . notice that if there is some subtlety and you keep a constant internal energy $u=\text{cte}$ but not a constant energy $e=u+e_{\text{m}}$ , by modifying the mechanical energy $e_{\text{m}}$ , you should refrain from using standard names like isoenergetic and explain precisely what happens .
list of fusion power technologies : http://en.wikipedia.org/wiki/list_of_fusion_power_technologies other more well-designed fusion concepts are not fully tested yet due to lack of funding : http://www.crossfirefusion.com/nuclear-fusion-reactor/crossfire-fusion-reactor.html
people do not immediately compress because the body is more or less a pressure vessel . it is not a very good pressure vessel for dealing with vacuum , but it is something . it is your body 's resistance to pressure that lets you do things like spray bodily fluids ( from your mouth , or from your bladder , or from your arteries ) . when my wife was in labor with my son , one of the sensors monitoring her reported the internal pressure during her contractions in pascals ( though you will have to find an obstetrician to tell you what a typical uterine contraction pressure is , because i was mostly paying attention to other things at the time ) . gases do not have this constriction . nitrogen and oxygen at room temperature have mean molecular velocities following \begin{align} \frac12 mv^2 and = \frac32 kt \\ \frac vc and = \sqrt\frac{3kt}{mc^2} \approx \sqrt\frac{3\cdot 25\ , \mathrm{mev}}{30\ , \mathrm{gev}} \\ v and \approx 1.5\times 10^{-6}c \approx 450\ , \mathrm{m/s} \approx 1000\ , \mathrm{mph} \end{align} when you depressurize a gas volume at room temperature , this is the average speed of the gas that moves into the vacuum . if you depressurize an airlock by letting the air flow through a 2 m$^2$ doorway , you have a lot of momentum to carry things along with you . emilio pisanty gives a nice example in a comment : if your airlock is the size of a bathroom ( ~ 20 m$^3$ ) and it is depressurized rapidly , so that all the air is " suddenly " moving away from the door at a thermal speed , the momentum of the air is comparable to the momentum of a car . you have a good intuition for what it is like to be hit by a car ( hint : it sucks , even at low speed ) ; getting thrown out the door of the airlock is a totally plausible outcome , but not a certainty . keep in mind , if you start to calculate things , that the kinetic energy available , $p^2/2m$ , goes way up if you take the momentum of a car collision and put it into a few kilograms of air . note that in a real airlock , you had change the pressure slowly . if you were on a spacecraft where air was a precious resource , you would probably even pump the air from the airlock into the spacecraft rather that throwing it away .
it means that time is no longer an absolute concept , yes . the time a specific observer experiences in a specific frame of reference , i.e. his proper time depends on the path ( worldline ) he takes through spacetime . in other words , it depends on his state of motion , the way he accelerates . this is the reason for the famous twin paradoxon : the resolution is that both twins move on different worldlines and hence end up having different proper times . this can even be easily understood geometrically : between two points in spacetime , the proper time interval of the shortest path connecting them is the greatest . with this knowledge , the reason for the twin-paradox should be evident from the following picture : clearly , the black lines corresponding to the travelling twin represent a longer part than that of the stationary one . this does not mean , however , that time is not well-defined as a physical quantity . it just means that when you measure it , you may get different results , depending on how you move with respect to somebody else .
your approach is all right but the solution given by the textbook is wrong : ) , at least if no approximation is to be made . let 's go the other way around : start from $$p ( v , t ) = \frac{\gamma c_vt}{v} + \frac{\varepsilon}{2v_0}\left [ \left ( \frac{v_0}{v}\right ) ^5 - \left ( \frac{v_0}{v}\right ) ^3\right ] $$ and then derive the values of $\kappa_t$ and $\beta$ ( by the way , should not it rather be $\chi_t$ ( see here ) and $\alpha$ ( here ) ? ) . as you mentioned , to do this , we need to compute partial derivatives of $p$: $$ \left ( \frac{\partial p}{\partial t}\right ) _v = \frac{\gamma c_v}{v} $$ $$ \left ( \frac{\partial p}{\partial v}\right ) _t = -\left ( \frac{\gamma c_vt}{v^2} + \frac{\varepsilon}{2{v_0}^2}\left [ 5\left ( \frac{v_0}{v}\right ) ^6 - 3\left ( \frac{v_0}{v}\right ) ^4\right ] \right ) $$ which give $$ \frac{1}{\kappa_t} = -v \left ( \frac{\partial p}{\partial v}\right ) _t = \frac{\gamma c_vt}{v} + \frac{\varepsilon}{2{v_0}}\left [ 5\left ( \frac{v_0}{v}\right ) ^5 - 3\left ( \frac{v_0}{v}\right ) ^3\right ] $$ and $$ \beta = \kappa_t \left ( \frac{\partial p}{\partial t}\right ) _v = \frac1t \left ( 1 + \frac{\varepsilon }{2\gamma c_v t}\left [ 5\left ( \frac{v_0}{v}\right ) ^4 - 3\left ( \frac{v_0}{v}\right ) ^2\right ] \right ) ^{-1} $$ clearly , that is not what the textbook gives in the first place , but it is close enough to understand what they did : a taylor expansion at order 3 in $v_0/v$ in booth cases , which gives back the expressions $$ \frac{1}{\kappa_t} \approx \frac{\gamma c_vt}{v} + \frac{\varepsilon}{2{v_0}}\left [ - 3\left ( \frac{v_0}{v}\right ) ^3\right ] $$ and $$ \beta \approx \frac1t \left ( 1 + \frac{\varepsilon }{2\gamma c_v t}\left [ - 3\left ( \frac{v_0}{v}\right ) ^2\right ] \right ) ^{-1} \approx \frac1t \left ( 1 + \frac{\varepsilon }{2\gamma c_v t}\left [ 3\left ( \frac{v_0}{v}\right ) ^2\right ] \right ) $$ it really should have been clearer in the text that you could suppose $v\gg v_0$ ( and not only $v&gt ; v_0$ ) and i do not see how you could derive the term in $\left ( \frac{v_0}{v}\right ) ^5$ from this as it is completely neglected to find back $\kappa_t$ and $\beta$
your mistake is that in the expression $$ q=cv $$ the symbol $v$ here represents the magnitude of the potential difference between the spheres . thus , since $b&gt ; a$ here , you need to switch the order of $b$ and $a$ in the first expression you wrote down for $v$ if you want to plug it into the expression defining capacitance ( in other words , you need to take its absolute value ) .
snell 's law still applies to the curved surface , but you have to measure the angles of incidence and refraction relative to the surface where the light hits . the image is my attempt to show parallel rays of light falling on a curved surface . even though the rays are parallel , the angle of incidence is different for the two rays because it has to be measured relative to the normal at the point the light strikes the surface . hence the angle $i$ is not the same as the angle $i'$ . response to comment : it has become clear from the comments that the problem is that the value of $n$ depends on whether the light is passing from the air to glass or from glass to air . to be precise the two refractive indices are reciprocals of each other i.e. $$ n_{air-glass} = \frac{1}{n_{glass-air}} $$ the refraction of the light ray happens because the speed of light , and therefore the wavelength , changes when the light enters and leaves the glass . the refractive index when a light ray passes from a medium 1 to a medium 2 is : $$ n_{1-2} = \frac{v_1}{v_2} $$ where $v_1$ is the speed of light in medium 1 and $v_2$ is the speed of light in medium 2 . so in our example the refractive index when passing from medium 2 to medium 1 is : $$ n_{2-1} = \frac{v_2}{v_1} $$ i.e. $$ n_{1-2} = \frac{1}{n_{2-1}} $$
( i will assume in my answer that people have read the discussion on the old question , linked to by the op . ) no , it is not like the aether . it is still true that locally , there is no preferred reference frame . you do not even really need to think about spacetime to see what is going on . consider a two-dimensional plane , parametrised by $ ( x , y ) $ , and roll it into a cylinder by identifying $ ( x , y ) \sim ( x + nl , y ) ~\forall~ n$ , where $l$ is some constant . locally , this space is still perfectly isotropic , but globally , the $x$ direction has been picked out by the identification . to see what this means , let 's imagine drawing two straight line segments , each beginning at $ ( x , y ) = ( 0,0 ) $ and ending at $ ( 0 , l ) $ . the first will just be $ ( 0 , t ) ~ , ~ 0\leq t\leq l$ , and the other will be $ ( t , t ) ~ , ~0\leq t\leq l$ ( which ends at a point equivalent to $ ( 0 , l ) $ under the identification , and therefore the same point on the cylinder ) . obviously the length of the first line is just $l$ , but the length of the second line is $\sqrt{2}l$ , by pythagoras . although any small patch of the cylinder is perfectly isotropic , we see here that the rotational symmetry is broken globally by the identification . in spacetime , a similar thing happens , replacing rotational symmetry by boost symmetry . short answer : generally speaking , there is never a twin paradox : in any spacetime , just write down the metric in any coordinate system you like , and calculate the proper times for the two trajectories of interest . this tells you unambiguously which twin is older and which younger .
your assumption is very wrong . gravity is everywhere . or simply , the box has mass . it exerts its very own gravity . if your 1st location has the field , then the 2nd location is also influenced by it unless it is at $\infty$ . you can not just shield anything from gravitational field . so , energy is still conserved when you are moving the box from one place to another . . .
what you observe is the general phenomenon that in relativistic theories time translation is replaced by " affine-parameter-translation " or " wordline translation symmetry " and hence the corresponding hamiltonian becomes a constraint , the constraint that states must be invariant under this symmetry . yes , this works for the relativistic spinning particle and the dirac equation , too . here the translation symmetry on the worldline is refined to translation supersymmetry ( for ordinary spinors even , this has nothing a priori to do with spacetime supersymmetry ) . the odd generator of the worldline supersymmetry turns out to be the dirac operator . again , states are required to be annihilated by it and this gives the dirac eqation . plenty of pointers to details about how this works are here : http://ncatlab.org/nlab/show/spinning+particle
in your solution you seem to make the assumption that the terminal velocity in the y-direction is zero . this produces the wrong answer . this is how i would solve the problem : first , let 's note that the initial velocity in both x- and y-direction are the same ( due to the $45^{\circ}$ angle ) . let 's call it $v$ . the distance traveled in the x-direction , $d$ , when the ball hit is the ground is given by : $$ d=vt $$ where $t$ is the time of flight . when the ball hits the ground , its velocity in the y-direction will be $-v$ . this means that its velocity has changed by $2v$ ( or rather by $-2v$ ) . hence we also have : $$ 2v=gt $$ substituting for $v$ gives : $$ d=\frac{gt^2}{2} $$ which solved for $t$ gives : $$ t=\sqrt{\frac{2d}{g}}=\sqrt{\frac{2\cdot 180}{9.8}}\approx 6.06\ , \rm{s} $$
the depletion region forms due to the equilibrium between drift ( field driven ) and diffusion ( concentration gradient driven ) currents . if you have very low doping , the depletion region will be large because a large volume of depleted semiconductor is needed to generate enough electric field to balance the diffusion current . on the other hand , if you have very high doping a much smaller region is required to balance the diffusion of carriers . i was actually doing this calculation last week , here are some results from my simulation of the poisson-boltzmann equation for a ge/gaas pn-junction for different doping levels . i have indicated the approximate width of the depletion layer with the blue background . the blue and greens lines are the conduction and valance bands , the red and light blue lines are the intrinsic fermi-level and the fermi-level , respectively . the top plot is for light doping $10^{16}\text{cm}^{-3}$ , the bottom is for higher doping $10^{17}\text{cm}^{-3}$ . this width of the depletion with applied bias was discussed here in an avalanche breakdown , where are the electrons that break free from ? , so that might also be interesting .
well , ya gotta be careful . as you noted , there is an acceptance cone angle . now , consider an idealized case where the fiber is perfectly straight and perfectly cylindrical . then , even for skew rays ( per wikipedia , " ray that does not propagate in a plane that contains both the object point and the optical axis . such rays do not cross the optical axis anywhere , and are not parallel to it" ) you can determine the local angle of incidence when they hit the internal $n_1/n_2$ index boundary , and see that the rays may end up with different vector angles $ ( \theta , \phi ) $ ( with respect to the x and y coordinates ) but basically the same range of angle with respect to the optic axis . all this is a long-winded way of pointing out that light paths have to be reversible , so a ray can not exit at an angle greater than the acceptance angle . however , if you feed a beam with angular spread less than the acceptance angle , it is more than likely that multiple skew bounces , and , more important , curvature in the fiber , will lead to exit angles as large as the acceptance angle .
the energy needed to remove an electron from a solid is called the work function . for most metals you would need uv photons ( 300 nm for aluminium ) that rarely reach the earth 's surface . visible light can eject electrons from alkali metals , but the quantum yield ( the probability of electron emission per incident photon ) for pure metals is low ( probably less than 1% ) . materials like cste that are used in photocathodes have efficiency up to 40% ( at certain wavelength ) but they are expensive and difficult to handle in open air . silicon solar cells also utilize photoelectric effect and compared to metals they are efficient and inexpensive .
there are two different questions at work here , that you have kind of mashed together . the first question is " what is the speed at which a change in the electric field propagates ? " the answer to that is the speed of light . in qed terms , the electromagnetic interaction that we see as the electric field is mediated by photons , so any change in an established field ( say , due to shifting the position of the charge creating the field ) will not be felt by a distant object until enough time has passed for a photon from the source to make it to the observation point . the second question is " what is the speed of propagation of electric current ? " this speed is slower than the speed of light , but still on about that order of magnitude-- the exact value depends a little on the arrangement of wires and so on , but you will not be far off if you assume that electrical signals propagate down a cable at the speed of light . this relates to electric field in that the charge moving through a circuit to light a light bulb has to be driven by some electric field , so you can reasonably ask how that field is established , and how much time it takes . qualitatively , the necessary field is established by excess charge on the surface of the wires , with the surface charge being generally positive near the positive terminal of a battery and generally negative near the negative terminal , and dropping off smoothly from one to the other so that the electric field is more or less piecewise constant ( that is , the field is the same everywhere inside a wire , and the field is the same everywhere inside a resistor , but the two field values are not the same ) . when the circuit is first connected , there is a rapid redistribution of the charge on the surface of the wires which establishes the surface charge gradients that drive the steady-state current that will eventually do whatever it is you want it to do . the time required to establish the gradients and settle in to the steady-state condition is very fast , most likely on the order of nanoseconds for a normal circuit . there is a good discussion of the business of how , exactly , charges get moved around to drive a current in the textbook that we use for our introductory classes , matter and interactions , by chabay and sherwood . it does not go into enough detail to let you calculate the relevant times directly , but it lays out the basic science pretty well . ( it is a textbook for a first-year introductory physics class , so it sweeps a lot of condensed matter physics under the metaphorical rug-- there is no discussion of band structure or surface modes , or any of that . it is fairly solid conceptually , though , at least according to colleagues who know more about those fields than i do . )
start with a plane-wave at rest with its spin pointing in the z-direction : $\psi_l=\psi_r=\left ( \begin{array}{c} 1\\ 0\end{array}\right ) $ first rotate the spin to the required direction . $\psi_l \rightarrow \exp\left ( -\tfrac12 i\theta_i\ , \sigma^i\right ) \psi_l ~~~~~~~~~~~~ \psi_r \rightarrow \exp\left ( -\tfrac12 i\theta_i\ , \sigma^i\right ) \psi_r$ ( find the euler angles $\theta_i$ by transforming the spin-vector to the rest-frame ) now boost the rotated spinors in the right direction . $\psi_l \rightarrow \exp\left ( -\tfrac12\vartheta_i\ , \sigma^i\right ) \psi_l ~~~~~~~~~~~~ \psi_r \rightarrow \exp\left ( +\tfrac12\vartheta_i\ , \sigma^i\right ) \psi_r$ ( calculate the rapidity $\vartheta_i$ from the momentum ) finally : note that you can expand : $\exp ( i\phi_i\sigma^i ) \longrightarrow \cos|\phi|+i \phi_i\sigma^i \sin|\phi|\ , /\ , |\phi|$ to get the matrices out of the exponent 's argument hans
i am not sure whether you meant initially at rest relative to the universe , or to the surface of the earth . here are the answers to both versions : universe let the latitude be $\theta_0$ . in the non-rotating reference frame ( of the universe ) , the motion of the stone is in simple harmonic motion . so $r ( t ) = r_0 \sin ( \omega t ) $ , where $2\pi / \omega$ is the time it takes to get to the center of the earth . the latitude is constant . and the longitude $\phi_0$ ( in the non-rotating reference frame ) is also constant . but the earth spins with constant angular velocity $\omega_0 = 2\pi$ radians per 24 hours . so in the rotating coordinate $\phi&#39 ; ( t ) = \phi_0 - \omega t$ . so measured relative to the surface of the earth in spherical coordinates you have the parametric description $$ r ( t ) = r_0\sin ( \omega t ) $$ where $r_0$ is the radius of the earth . ( note : this assumes that the earth is perfectly spherical and of uniform density inside , which is obviously not quite physical . of course , digging a tunnel like that is also not quite physical . . . ) $$ \theta ( t ) = \theta_0 , \phi ( t ) = \phi_0 - \omega t $$ finding $\omega$ from the assumption that earth is perfectly spherical and of uniform density is left as an exercise to the reader . a second interesting exercise is to find the conditions on the density of earth and on the rate of revolution that allows for the stone to travel in a closed orbit ( again , this is much much simpler when considered in the non-rotating reference frame . . . ) surface of the earth again we work in the non-rotating reference frame . we have again , via the constant density assumption , ( and assuming that the mass of stone is 1 ) that the potential energy is $p = \alpha r^2$ . the kinetic energy is $2k = \dot{r}^2 + r^2\dot{\theta}^2 + ( r\cos\theta ) ^2\dot{\phi}^2$ . the conservation of angular momentum means that angular component of the velocity is of size $l / r$ , where $l$ can be computed from the rate of rotation of the earth . so the conserved energy is $$ e = \dot{r}^2 + \frac{l^2}{r^2} + \alpha r^2 $$ this gives an ode for $r$ . similarly using a tilted coordinate system you can solve for the angles using the conservation of angular momentum by integrating an ode and plugging in the solution for $r$ . for the maximum depth , however , you do not need to explicitly solve the ode : the energy is known initially : $e_0 = 0+ \frac{l^2}{r_0^2} + \alpha r_0^2$ , where $l$ depends only on the rate of revolution for earth , and $\alpha$ on the mass of earth ( assuming uniform density ) . $r_0$ is the radius of earth . at the maximum depth , $\dot{r}$ is again 0 . so you are down to finding the " other " positive root of the quartic polynomial $$ e_0 r^2 = l^2 + \alpha r^4 $$ which you can solve explicitly using the quadratic formula $$ r^2 = \frac{e_0 \pm \sqrt{ e_0^2 - 4 \alpha l^2}}{2\alpha } $$ where the + solution is the radius of the earth , and the - solution is the depth . plugging in physical numbers : at the initial drop , $\dot{\theta} = 0$ and $\dot{\phi} = \omega = \frac{2\pi}{86400} s^{-1}$ . the radius of earth we take to be $6.38 \times 10^6 m$ , the mass $6\times 10^{24} kg$ . so $\alpha = g m / r_0^3 = 1.5 \times 10^{-6} s^{-2}$ . the conserved angular momentum is initially $$ l_0^2 = r_0^4 \cos ( \theta_0 ) ^4 \omega^2 = 8.8\times 10^{18}\cos\theta_0^4 m^4 s^{-2} $$ and the conserved energy is initially $$ e_0 = l_0^2 r_0^{-2} + \alpha r_0^2 = ( \cos\theta_0 ) ^4 2.15 \times 10^5 + 6.1\times 10^7 m^2 s^{-2} \sim 6.1\times 10^7 m^2 s^{-2} $$ ( the initial angular momentum contribution is very small ) . now $e_0^2 = 3.7 \times 10^{15}$ and $4\alpha l_0^2 = 5.28\times 10^{13}\cos ( \theta_0 ) ^4$ , so the answer is that the maximum depth is very close to the center of the earth ! using the binomial expansion we get that $$ r^2 \sim \frac{l_0^2}{e_0} \implies r \sim 3.9\times 10^5 \times \cos ( \theta_0 ) ^2 m$$ so if you start at the equator where $\theta_0 = 0$ , the hole will get about 94% to the center of the earth .
addressing just the last two paragraphs of your question : if $t$ is " non-degenerate " ( at every point as a linear transformation from $t_pm \to t_p^*m$ ) , then modulo some issues with the constraint equations $g$ can be solved from $t$ , at least locally in time , after prescribing it in a compatible way on a space-like hypersurface . see this article by de turck . presumeably the issue with degeneracy has more to do with the method involved in solving the problem , then with it being a critical condition , since the evolution for ricci-flat metrics ( einstein vacuum equations ) is also well-posed . the main difficulty is about the " univocal " part : due to diffeomorphism invariance , the " local " problem for prescribing ricci curvature is under-determined . ( the global problem may have topological constraints ; that i am not sure about . ) so one expects that for the local problem the issue is more likely the overabundance of solutions . on the other hand , the prescription of initial/boundary data may be crucial . roughly speaking the prescribing ricci curvature equation is analogous to the inhomogeneous wave equation with a source term . in which case the hyperbolic nature of the equations shows that for any initial configuration there exists a different evolution . so it may be that this will lead to multiple different metrics compatible with your $t$ ( indeed consider the case where you restrict your manifold to be $m = \mathbb{r}^4$ ; the global nonlinear stability of minkowski space , combined with classical results on classical results on the existence of solutions to the vacuum constraint equations shows that on $m = \mathbb{r}^4$ there are many incongruent ( since for these other solutions the weyl tensor is non-vanishing ) solutions to einstein 's equation when $t = 0$ ; i would expect something similar for the case of prescribed $t$ ) .
yes , for example heat energy is transferred through three different means : radiation ( which is wave like ) conduction ( which is not wave like ) convection ( which is not wave like ) of course , according to the standard model , everything has wave-like properties , so in a way there is no escape from waves , but i believe that your question is best answered with classical phenomena .
when applying f = m*a , the mass used is simply the mass of the object that the forces are acting on . in this case , it is indeed the mass of the person . you are right in that the weight of the person ( 650 n ) is equal to the mass of the person times to force felt by him due to gravity . dividing by gravitational acceleration will give you his mass . since the normal force on the passenger from the floor is 620 n ( lower than the normal force he would feel from the floor in a stationary elevator , which would be equal to his weight ) , intuition says that the floor of the elevator is accelerating in the opposite direction of the normal force . yeah , everything looks good to me .
when you take the charge distribution to be independent of $\theta$ , and force the problem to be two-dimensional , you are not letting $\delta_\theta \rightarrow 0$ . you are actually letting $\delta_\theta = 2\pi$ . if that is the only simulation you are going to run , the simplest solution is to set $\delta_\theta = 2\pi$ , and have your cells be rings at each value of $r$ and $z$ . if you later want to allow $\theta$ dependence , you can then let $\delta_\theta = 2\pi/n$ , to allow for $n$ cells per $ ( r , z ) $ ring .
it appears to me the issue is understanding momentum conservation . an even cruder example would be to shine a bright torch out the back of your vehicle . even though the photons have no mass , would not the vehicle move forward ? you also refer to mass in this manner in the paraphrasing of newton 's third law " proportional opposite mass/acceleration ratio in the opposite direction " . it is not the mass that matters here . newton 's third law can be thought of as a statement of conservation of momentum . despite having no invariant mass , photons do have momentum . so a light sail , or a flash light in your case , works because to conserve momentum if light is reflected off your craft or emitted from your craft , your momentum must change to compensate for the change in momentum of the light or the momentum the light carried away . noether 's theorem is an even stronger statement , which shows that if we can describe the physics in a manner that does not depend on position ( ie . you could redo this experiment 1km to the right and it would not effect the results ) , then momentum must be conserved . so this forbids a reactionless drive in special relativity as well . spatial translation symmetry becomes a bit messy in gr , since the spacetime itself is dynamic . so your gravity wave idea could work in principle ( alcubierre drive ) . however it is possible to formulate a type of energy and momentum conservation with pseudo-tensors in gr ( this is how einstein discussed it ) . in this view we can keep track of the energy and momentum of gravity waves as well , and so we can still use momentum conservation for these scenarios as well . so your question : why is it impossible to move a mass in a given direction without a proportional change in the opposite direction ? the answer is : momentum conservation forbids a reactionless drive , and momentum conservation itself follows from the translational symmetry of the physics describing a closed system . so like free energy machines , we do not even need to see the details to know something is a misrepresentation or a scam . we can reject such " inventions " on very general grounds . and indeed , the u.s. patent office explicitly will not review a patent on a perpetual motion machine , or a device that could be used to build such a machine . they may have a similar restriction on " reactionless drives " , but i am not sure of that .
the $\theta$-term is also known as the axion term and it is simply the $f\wedge f$ term known to particle physicists . in a more condensed-matter-friendly language , $$\delta {\mathcal l} = \theta\left ( \frac{e^2}{2\pi h} \right ) \vec b \cdot \vec e $$ i do not know the optimum starting point but you may begin with http://arxiv.org/abs/1001.3179 and its followups and references . more generally , the $\theta$-term means the integral of the anomaly polynomial . note that the anomaly polynomial is a nice gauge-invariant expression - but in higher dimensions . the actual anomaly in the original spacetime is related to it by several operations .
it is a misconception to think that just because the 29 th electron is outside the gaussian surface , it will not have an effect on the electric field inside it . the total flux through the surface is indeed zero , but that does not mean there is no influence : imagine a point charge and draw up a sphere next to it . the electric field goes in on one face and out the other to make a zero flux , but there is definitely a field inside . the situation for copper is slightly more complicated because the electron is in a spherically symmetric s orbital . this does reduce its influence on the inner electrons , not because of gauss 's law but because of a theorem of newton : the electric field due to a spherical shell of charge vanishes inside it . however , the 4s orbital is not simply a spherical shell of charge around the [ ar ] 3d 10 core . even in the most simplistic , hydrogen-based models , the higher s orbitals still have a sizable probability of being inside the core , and have multiple shells of positive probability : source : wikipedia . note that this is a 6s hydrogen orbital with no actual relation to copper ; a 4s orbital has four such spherical shells . because of this , the 4s electron does create a nonzero electric field inside the atomic core , and this does affect the inner electrons . as you have guessed , of course , in real life this is much , much more complicated , and quantum chemists will send you packing if you tell them you need an accurate ab initio description of the whole copper atom all the way to the 1s 2 core . the electrons are highly correlated , and all of them interact strongly with each other , including such lovely job-simplifying features like exchange interactions . quantum mechanically , the bottom line to your question is that it is meaningless to talk about " the 29 th electron , " because all the electrons are indistinguishable and therefore you must consider them all as a whole . ( specifically , " the charge density due to the electron in the 4s orbital " is not a meaningful physical quantity . ) however , the quantumness actually helps . the reason for this is clear if you try to come up with a classical model of a copper atom . even if you preserved the shell structure with the lone 4s electron orbiting well outside them , the inner electrons will be a hugely complicated jumble of electrons whizzing around , and the system will be nowhere near spherically symmetric . while you can still draw a gaussian sphere and get information about the total flux , this does not help at all in describing the electric field at any particular point . in the quantum case , on the other hand , everything is much simpler . you have a bunch of closed shells and a single $s$ electron , and this means that the whole atom is spherically symmetric . electric fields must be radial and uniform , and drawing gaussian spheres does help you calculate the electric field at a given radius .
let 's break the question down to its bare bones . let us define danger first . i think danger falls under following categories : any changes to the body 's ( internal or external ) structure . mental trauma --i would attribute it to the double derivative of the velocity ( i.e. . , jolt ) . but this is highly subjective . some people would attribute it to the impact while others would attribute it to the total energy transferred to the body . this is the part i will leave out from this discussion as it is highly subjective . so , let 's focus on 1 . what can cause changes to a [ human ] body 's structure ? one may think that structure changes with the application of force . intuitively , larger the force applied , more changes it is likely to produce . longer the force is applied , more changes it is likely to produce . but in concrete physical terms , structural changes happen when applied energy is sufficient to break the bonds between atoms/molecules making up the structure . thus , the bond strength and the amount of energy both matter . intuitively , a ball of putty or sponge is more likely to deform itself than a hard leather or cork ball . thus , when you are hit by a sponge ball you take less damage than when you are hit by a hard leather ball of the same mass thrown with the same velocity . how much energy will be transferred ? in a collision , energy is transferred between the objects that collide . some of that energy changes the velocity of the objects while rest of it is absorbed by the structure or released as heat . when no energy is absorbed by the objects colliding they are said to have collided elastically . but , as is more likely in a real world scenario , including the car collision case you described , when the final kinetic energies of the two objects do not sum up to the sum of their initial kinetic energies then it follows that some energy will be " lost " in the collision . that energy is not actually lost and is absorbed by the objects colliding . they are said to have collided inelastically in this case . it is the energy that was absorbed by the pedestrian that would contribute to the change in structure of the pedestrian 's body . now for your actual question --what difference does the mass of the car make ? so let us assume that all other factors ( mass of the pedestrian , angle of the collision , initial velocities , resilience of the car and the pedestrian , co-efficient of restitution etc . ) remain the same . let us say the mass of the pedestrian is $m$ and the mass of the car is $m$ . let us also say that they collide perfectly inelastically and the initial speed of the car is $v$ ( do not care about the direction , that is why i said speed ) . then the energy lost is : $$\frac{ ( mv ) ^2}{2 ( m + m ) }$$ roughly thus ( considering $m &lt ; &lt ; m$ ) , the energy lost is directly proportional to the mass of the car . in partially inelastic collisions , the formula becomes a little complicated but the essence of the discussion remains the same . so , everything same , more mass will mean more damage . you can use the formula above to calculate the energy for yourself but doubling the weight of the car will increase the energy lost substantially --and can mean life/death . doubling the speed will cause even more damage --hence the speed limits in congested areas . does the acceleration matter ? you question also asks if the acceleration matters . to answer it directly , yes but not in the same way as the energy transferred . acceleration by itself will not cause anything more sinister ( other than disturbing the fluids in the pedestrian 's body ) as long as it is uniform . if you experience space variant acceleration ( meaning different parts of the body are accelerated differently ) then it can cause tearing and that is harmful . the greater the difference , more will be the tearing . time dependent acceleration ( or jolts ) can cause a lot of discomfort as the body is stretched and fluids move here and there --that can cause vomiting or trauma or both . very large accelerations start with a significant jolt ( as the initial acceleration is very close to zero ) . for the example case you described , the cars are at the same initial speed . the " final " velocity of the pedestrian is $$\frac{mv}{m + m} , perfectly-inelastic$$ $$\frac{2mv}{m + m} , perfectly-elastic$$ therefore , once again , more massive the car , higher will be the acceleration experienced by the pedestrian assuming collision impact times remain same . but if you can make the impact softer , you can reduce this acceleration ( by increasing the time of impact ) --see next section . other ways to make cars safer to make cars safer for pedestrians , one should structure the car so that it absorbs most of the energy itself ( softer bumpers/hoods ) . on formula one race tracks you can see several layers of rubber tires that are there to absorb energy when a car collides with them . one can even structure the car in the fashion you described , allowing the pedestrian to slide over the car , thereby reducing the impact of the collision . but you definitely want to avoid an elastic collision where the pedestrian is reflected by the car into something more dangerous nearby ( e . g . , road , traffic sign pole , railings , a food stall with knives/boiling oil etc . ) .
the casimir effect is analogous to gravity in only one way--- it has a negative energy which varies as a power of the distance . in all other ways , it is different . the power-law is different , the cause is different , it is a quantum effect , not a classical effect , and the mediator is the electromagnetic field , not the gravitational field . negative energy is not surprising--- it just means that there is an attractive force ( and zero potential at infinity ) , so that the potential energy is less than when the objects are far apart . in this respect , both casimir and gravity have negative energies . this energy is an energy difference between two configurations . the total energy in the casimir system is still positive , it is just less than if you take the two plates to infinity . similarly , the total energy in a gravitating system is also positive , but when objects are close , it is less than the energy than when they are far . this is the negative energy in gravity . for the case of casimir forces , there are two ways to view it . the more primitive way is to consider the fluctuating polarizations in a quantum system that lead to attractive forces . these attractions come in two types , depending on the ratio of the distance to ( c times ) the orbital period of the electrons ( the difference in energy levels converted into a time ) : london forces : when the atoms are significantly closer together than the wavelength of light that they would emit between the first excited state and the ground state , the electromagnetic interaction is instantaneous coulomb interaction . the coulomb interaction correlates the charge fluctuations on the two atoms so that they tend to be polarized in the same direction , and this correlated polarization leads to a $1/r^6$ attractive force between points separated by a distance r , or a $1/r^4$ force between two sheets separated by distance r . van-der-waals forces : when the atoms are further than the typical wavelength of light , the electrostatic force is no longer instantaneous , and the correlated force is weaker than you get from the statically correlated system . the force falls off as $1/r^7$ for points , and $1/r^5$ for sheets . this calculation is harder than the previous one , but when casimir calculated the effect , it revealed itself to be more universal than the london-forces , it looked like it does not care as much about the energy levels , that it only depends on polarizability of each object separately . bohr suggested to casimir that the universality could be understood because the attraction was due to the correlations in the quantum field surrounding the materials , and that he should calculate the force only by the vacuum energy in the surrounding field modes . this calculation works , and produces the correct form of the van-der-waals force , and explains the universality . but the effect is not hard to understand--- the casimir effect is the van-der-waals attraction between two plates separated by a distance large enough so that retardation effects are important . in dirac gauge , you have electrostatic forces and photons , and the electrostatic force is not important , and the photon vacuum energy reduction is just due to the different photon modes that are allowed at different distances . this is not mysterious , and it should not be presented as mysterious . but it is a source of endless pseudoscience about machines that extract vacuum energy . the casimir force does not allow you to extract any more energy than any other attractive effective interaction .
one word : inertia . when you are riding a bike on a level gradient you just need to give it a push to get going , then you can coast for quite a while before friction and air resistance slow you down . the human body does not have wheels that can store kinetic energy , so while running you have to give a good kick to get going , and then another kick to keep going on the next step , and so on . when hills are involved the difference is even more pronounced , since we run downhill the same way we do on the level , by continually pushing ourselves forward ; whereas on a bicycle you can take advantage of the slope and just coast down it . i suspect that raising and lowering your centre of mass is not as inefficient as the other answers have suggested . this is because your legs are springy , so at least to some extent you are just converting energy back and forth between gravitational potential and the spring force in your legs . humans are possibly the most efficient long-distance runners in the animal kingdom . there is a school of thought that says the reason we are bipeds is that we evolved as endurance hunters , chasing our prey until it collapsed from exhaustion rather than trying to outrun it over short distances . whether that is true or not , we probably would not do all that bouncing up and down if there was not a good reason for it . you might ask why , if using wheels is so much more efficient , did not we evolve that instead ? i do not know , but it seems no animal has been able to evolve wheeled locomotion .
habitable by whom ? there are conditions that are uninhabitable by humans , however , many " extremophiles " survive perfectly happy . although , if you are talking about humans , here is a small list ( all the rest are probably more " nice to have " requisites ) : approximately 20% oxygen ( more or less depending on the pressure ) temperatures that allow for liquid water adequate access to water ( and food ) adequate protection from radiation . then a whole host of conditions that would not end human life . such as deadly pathogens on chemicals in the atmosphere . all this said , since we only have a sample size of one currently for planetary life , we really do not know what is possible , or how to bound the problem . anything is just speculation drawn from this one sample . that said , we have found life on our own planet where we never suspected it to be . life has proven to be nearly unstoppable in propagating throughout every niche on this planet . so , the better question i think would be what are the requisites for abiogenesis ? i think once life manages to start on any planet , it will adapt to whatever conditions the planet presents ( to within a reasonable degree ) .
there are two main reasons as temperature is decreased the voltage of a car battery decreases and it is internal resistance increases . this means the battery can supply less current . as temperature is decreased the viscosity of oil in the engine increases so the engine is harder to turn over . in the days before fuel injection there was a third factor because carburettors are less good at vaporising fuel at low temperatures , but this will not be a problem for most modern cars .
your problem has two degrees of freedom , acceleration of the top block $a_1$ and acceleration of the bottom block $a_2$ . if the friction force between them is $f$ then the equations of motion are $$ m a_1 = f \\ m a_2 = p - f $$ if the blocks are stuck then $a=a_1=a_2$ and $f$ are unknown . when they slip then $f=\mu m g$ is known , but $a_1$ , $a_2$ are different unknowns . in each case there are two unkowns and two equations . the tansition occurs when friction tries to be $f \ge \mu m g$ which you need to find first .
the fact that things reduce to a simple equation is greatly helped by the right angle between the sticks . the normal force is the component of the force of gravity ( $m_l g$ ) in the direction of the second stick ( $m_l g sin\theta$ ) . the friction needed to keep the right hand stick from moving is $m_r g cos\theta$ . setting the two equal to each other , you see that $g$ cancels out , and because we have accounted for the directions with the $sin\theta$ and $cos\theta$ terms , we are left with the simple expression for the critical angle : $$tan\theta = \frac{m_l}{m_r}$$ the point here is that while the problem needs vectors in the solution , the final expression is not a vector expression ( because you have computed the components ) . in the end , vector math is usually just a series of equations ( one equation for x , one for y , etc ) . we just reduced it all to one equation - helped by the right angle between the strikes . i do not know how to explain this more clearly for you - let me know if this helps with your understanding ?
the reason for having two prongs is that they oscillate in antiphase . that is , instead of both moving to the left , then both moving to the right , and so on , they oscillate " in and out " - they move towards each other then move away from each other , then towards , etc . that means that the bit you hold does not vibrate at all , even though the prongs do . you might ask why it is that they do that , instead of oscillating in the same direction as one another . the answer is that at first they oscillate in both ways at the same time , but the side-to-side oscillations are rapidly damped by your hand , so they die out quickly , whereas the in-and-out ones are not damped this way , so they ring on long enough to hear them . an excellent illustration of this can be seen in this video of a fem model of a two-pronged fork , which shows you all the vibrational modes separately . ( hat tip to ghoppe , who posted this video in a comment . ) having a third prong would not help very much with reducing damping . there are ( at least ) three different ways a three-pronged fork could vibrate : one with all three vibrating side-to-side in phase with one another , and two where one of the prongs stays still and the other two vibrate in and out . ( at first i thought there would be three of this latter type of mode , but the third can be formed from a linear combination of the other two : $ ( 1,0 , -1 ) - ( 1 , -1,0 ) = ( 0,1 , -1 ) $ . ) the vibrational mode in which everything moves in the same direction would be damped by your hand , and some combination of the other two would continue to sound for a while . as ilmari karonen pointed out in a comment , there would also be a " transverse " $ ( 1 , -2,1 ) $ mode , where prongs vibrate out of the plane of the fork . this mode would not necessarily have the same frequency as the primary in-and-out mode , so it is probably something we had want to avoid . but ultimately there is little reason why , in a three-pronged fork , the vibrations would ring on longer than just two prongs - there would be the same amount of vibrational energy as in a two-pronged fork , but just shared between two or three modes of vibration instead of one .
this is getting complicated . : ) you have to make a lot of assumptions to make progress ; you have listed most of them . i will explicitly add an assumption that we consider only rayleigh scattering ( more-or-less consistent with " clear sky" ) , and that the atmosphere is pure n${}_2$ . since this is homework , i will not spill all the beans . what you need is the cross section for rayleigh scattering : how much light does each molecule remove from the incident light . given that , then you have to figure out what to do with it . this will involve figuring out how many molecules are in the way between the sun and your detector . i will say that there are web sites that present the results you need without your having to do the calculations yourself . of course , this will not give you any of the absorption bands that show up in your chart . and , of course , it will not account for the fact that the light hitting the top of the atmosphere only approximately follows the black body curve . i have not done the exercise myself , but i suspect you will end up reproducing one of the main features of those data . update since we are beyond homework : this wikipedia page gives the rayleigh scattering cross section for n${}_2$ , and a value for molecular density , but it does not say if it is the average value or the value at the earth 's surface . nonetheless , that number can help in making order-of-magnitude estimates . at any rate , rayleigh scattering goes as $\lambda^{-4}$ , so when you take that into account you will multiply your result by $a\lambda^{-4}$ where a is some pre-factor that you do not know yet . you can try to make an estimate of the pre-factor using the data on that wikipedia page , and perhaps some knowledges or guesses about the thickness of the atmosphere . ( or you can simply try different values for $a$ until you find one that works . ) you might be able to model the fact that the solar data matches the b.b. curve for high frequencies , but not at lower frequencies .
although i have not researched it , you can learn some physics and try to get into computational physics . there is some industry for it out there ( e . g the materials sector , though i would doubt they hire anything but phds ) . however , if you are a really great programmer and bring in your ideas and experience creating software , with the additional knowledge of being able to conduct simulations of physical systems and conduct solid quantitative analysis , then why not . there is also a market for physics specific software . for example , you can learn electrodynamics and create a ( hopefully open source or atleast free ; ) ) counterpart to simion . if developing independent software alternative is too much , you can contribute by creating physics modules , writing patches etc . to products like sage . there are more possible places where you can develop , off the top of my mind the root develeopment team at cern has two non-physicists working for them . the best strategy i would recommend is to start learning basic physics , and simultaneously research what is going on at the interface of physics and computation . one guide would be to look at the conferences and seminars that are held on the subject and find out what currently engages physicists . for example , have a look at : physics and computation 2010 and conference of computational physics . look at the titles of talks and submitted papers , the workshops , tutorials etc . find more on the web ( keyword search on the arxiv ) etc , and you will get a rough idea of the status of the field . start working on something , make some contributions to open source initiatives or get your own results , basically get some credentials so that employers might pay attention to you and you might find yourself working along with phds .
that is a hard experiment . the remnant nucleus generally has a very low ( non-relativistic ) velocity{*} , making it difficult to detect and characterize . should one only consider individual events where all decay products have been detected and their individual momenta obtained ? this is the formal definition of an " exclusive " event , but experimentally we ( that is the nuclear and particle physics " we" ) almost always relax it with respect to the remnant nucleus . how do you make sure that a set of decay product events are associated to the same event ? time correlation ? does this mean that the sample size needs to be small enough so overlapping decay events are percentually few and can be filtered out ? where possible you set your event rate slow compared to the daq latch and report rate . so almost all reported daq events are associated with only one physics event . there is a bit of art and science in this . if i was really going to do it , i would consider measuring the transverse direction only at a radioactive beam accelerator{**} . that will not make it easy , mind you , just less horribly difficult . that simplifies your life in the sense that the remnant is now a ionizing particle with a macroscopic track length , so you can id it , but of course you no longer have good information on exactly where the decay occurs and may not be able to extract a good value for the longitudinal component of the momentum due to the decay {+} . {*} momentum of a few mev , and mass of multiple to hundreds of gev . you might ask the amo people about this , they are more used to working with just barely ionizing energies than particle physicists and may have an answer . perhaps it is a good application for a multi-channel plate . {**} they do exist . {+} lost in the noise of the much larger component due to the beam momentum .
yes , infrared radiation which is invisible to human eyes but still radiates heat . this is how they make thermal cameras , they are using your body 's infrared radiation which is detected by the camera and forms an image based on visible light .
the main problem about a rigorous solution to such a scattering proplem is that computations are extremely demanding . just imagine you have a wavelength $\lambda$ of some $400$nm to $700$nm for visible light ( from here ) : now , to do physically meaningful simulations , you will need a sub-wavelength lattice which makes any computational cell above , say $10\ , \mu m^3$ not accessible since you have in the order of one million grid points . approximative approaches but of course there can be ways out of it if you are willing to make some approximations which will largely depend on the characteristics of the particles you are looking at . it is best to assume that we only have spherical particles since we can apply mie theory in this case . large particles first of all , let us consider particles which are much larger than the wavelength . then , the radius $r$ times the wave vector $k=2\pi/\lambda$ is much bigger than one , $$kr\gg1$$ which basically means that one observes reflection at a plane interface . you can implement these particles using geometrical optics ( mixed with fresnel reflection if you like ) since nothing really wave-like will happen as in this image ( taken from here ) : small particles second , the particles should be much smaller than the wavelength , $$kr\ll1\ , . $$ then , everything what is observed is a sum of dipolar responses of the particles in the so-called rayleigh-scattering . then , the intensity of light scattered by a single small particle from a beam of unpolarized light of wavelength $\lambda$ and intensity $i_0$ is given by : $$i=i_0 ( 1+\cos^2\theta ) \frac{ ( kr ) ^6}{2 ( kr ) ^2}\left ( \frac{n_p^2-1}{n_p^2+2}\right ) $$ where i have chosen the variables to be consistent with the used terminology and $r$ is the distance to the object , $\theta$ is the scattering angle and $n_p$ is the sphere 's refractive index . here is an image of such a situation with some metal particles also having quadrupolar excitation ( from here ) : a mean field approach - effective permittivity if you have a lot of these small objects , you may use the clausius-mossotti relation which gives you an effective permittivity $\epsilon_p=n_p^2$ depending on the concentration of the particle in some volume : $$\epsilon_{eff} = \epsilon_p + \frac{n\alpha}{1-\frac{n\alpha}{3\epsilon_p}}$$ where $\alpha$ is the polarizability of the sphere , for details see e.g. electromagnetic mixing formulas and applications by sihvola . this would be something like a mean-field approach . you can make some very neat effects using this effective approach since it allows you to calculate a continuous refraction around some particle streams under water . however , if the particles size is in the order of the wavelength , $$kr\approx 1$$ then you may have to take higher multipole moments into account which may be a very demanding task . for much more on the subject i would recommend bohren and huffmanns classic absorption and scattering of light by small particles . sincerely
good question . i think that you are incorrect to assume the chemical potential is independent of the zero point . i think it is easiest to see that the chemical potential must change by using $\mu = ( \partial u/\partial n ) _{s , v}$ . suppose i first define the zero point as 0 so that the energy is $u ( n ) $ . then i redefine the zero-point energy ( for a particle ) as $\epsilon_0$ , so that $u ' = u + n\epsilon_0$ . now , \begin{equation} \mu ' = \left ( \frac{\partial u'}{\partial n}\right ) _{s ' , v} = \left ( \frac{\partial u}{\partial n}\right ) _{s , v} + \epsilon_0 = \mu + \epsilon_0 \end{equation} then if $\epsilon_r ' = \epsilon_r + \epsilon_0$ , you have that $\epsilon_r ' - \mu ' = \epsilon_r - \mu$ , i.e. the difference is independent of the zero point . in fact , the formula $\mu = -t ( \partial s/\partial n ) _{u , v}$ does not imply that chemical potential is independent of zero point . if you change the zero-point energy , you must adjust the form of $s$ accordingly . for instance , you need to have $s ( u=0 ) = s' ( u ' = n\epsilon_0 ) $ and so you see that $s'$ is different that $s$ .
i ) the question formulation ( v4 ) leaves out some important implicit assumptions$^1$ of theorem 5.2 in ref . 1 . these are , among other things , the following four items . the word topologically equivalent should be replaced by locally topologically equivalent , i.e. in some local neighborhood . the ( vector-valued ) functions $g ( w ) $ and $h ( w ) $ are $o ( w ) $ for $w\to 0$ , where $w= ( u , v ) $ . here $o ( w ) $ refers to the little-o notation . the eigenvalues of the $b$ matrix lie on the imaginary axis . the eigenvalues of the $c$ matrix do not lie on the imaginary axis . ii ) let us first take a step back and put it in context . where did all of this come from ? well , we wanted to analyze the stability of the dynamical system $$ \dot{w}~=~aw + f ( w ) $$ at the point $w=0$ . here $a$ is a constant ( =$w$-independent ) quadratic matrix$^2$ , and the ( vector-valued ) function $f$ is of higher-order , $$f ( w ) ~=~o ( w ) \qquad \text{for} \qquad w~\to~ 0 . $$ we then partially diagonalize the $a$ matrix into sub-blocks $b$ and $c$ with the aforementioned properties ( 3 and 4 ) . after the diagonalization , the $w$-variables are split into $w= ( u , v ) $ , and similarly for the ( vector-valued ) function $f= ( g , h ) $ . iii ) we could furthermore partially diagonalize $c$ into sub-blocks $c_{+}$ and $c_{-}$ depending of whether the eigenvalues have positive or negative real part , respectively . these will correspond to unstable or stable directions , respectively , independently of what the higher-order terms $g$ and $h$ are . ( in a renormalization group jargon , they correspond to so-called relevant or irrelevant deformations , respectively . similarly , the $b$ sub-block corresponds to so-called marginal deformations . ) iv ) the tangent plane $\{w|v=0\}$ at $w=0$ corresponds to marginal directions . we need the higher-order information $f$ to decide if they are stable or unstable directions . if $h\equiv 0$ , we can choose the tangent plane $\{w|v=0\}$ as the center manifold . for general $h$ , the center manifold $c:=\{w|v=v ( u ) \}$ is a deformation of the tangent plane $\{w|v=0\}$ . the tangent plane $\{w|v=0\}$ is only a precise substitute for the center manifold $c$ when $u=0$ . intuitively , the center manifold $c$ by construction becomes curved by encoding the higher-order information $f$ in such a way that points $w\notin c$ in a neighborhood of $0$ are attracted to ( repelled from ) $c$ , for time $t$ going to the future ( coming from the past ) , in accordance with just the linear predictions of the $cw$ term , respectively . in particular , the effects of the $h$ terms in the second equation have already been taking into account in the definition of the curved center manifold $c$ , so that one should now only use the linear equation $$\dot{v} ~ = ~ cv$$ without the $h$-term . the first equation $$\dot{u} ~ =~ bu + g ( u , v ( u ) ) $$ is used to determine the stability for points $w\in c$ on the center manifold $c$ itself in a small neighborhood of $0$ . references : yu . a . kuztnetsov , elements of applied bifurcation theory , 2nd edition , 1998 . -- $^1$ some of the implicit assumptions are explained in the scholarpedia link . $^2$ a finite-dimensional quadratic matrix $a$ is not necessarily diagonalizable . eigenvalues , eigenvectors and eigenspaces should then be understood in the sense of generalized eigenvalues , eigenvectors and eigenspaces .
nope , this is just a quasiparticle excitation , like other quasiparticles in condensed matter physics including spinons , chargons , phonons , and others . if they are right , their particular material allows excitations described by fermionic majorana spinors but they are just emergent fields out of the normal fields that involve leptons , quarks , and the electromagnetic field . moreover , i am not sure whether they produce 3+1-dimensional majorana spinor fields or just lower-dimensional ones . the latter , especially quasiparticles on a wire i.e. in 1+1 dimensions , are of course an easier task . no implications for particle physics .
there are two problems to deal with which must be disentangled to solve problems like these . both angular momentum operators are vector operators , so in some sense they " take values " in $\mathbb r^3$ ; you are being asked for their dot product , which should be taken within that copy of $\mathbb r^3$ . you would have the same problem if you were asked to calculate the dot product $\mathbf r\cdot\mathbf p$ for a single particle without spin . the orbital and spin angular momentum operators act on the two different factors of a tensor product of hilbet spaces . thus any ( operator ) product of a scalar orbital operator with a scalar spin operator should be interpreted as a tensor product . you would have the same problem if you were asked to calculate the product $l^2s^2$ , which would need to be interpreted as $l^2\otimes s^2$ . thus , in your case , you must read $l\cdot s$ as $$ \mathbf{l}\cdot \mathbf{s}=\sum_{i=1}^3l_is_i=\sum_{i=1}^3l_i\otimes s_i . $$ to compute the matrix representation of this , you should begin with the matrix representation of each $l_i$ and $s_i$ . you then compute the tensor product matrices $l_i\otimes s_i$ . finally , you add all of those matrices together to get the final result . this is all much clearer with an example . the $z$ component , for example , is easy , since each matrix is given by $$ l_z=\hbar\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} \quad\text{and}\quad s_z=\frac\hbar 2 \begin{pmatrix}1 and 0\\0 and -1\end{pmatrix} , $$ in the bases $\{|1\rangle , |0\rangle , |-1\rangle\}$ and $\{|\tfrac12\rangle , |-\tfrac12\rangle\}$ respectively . the tensor product matrix , then , in the basis $\{|1\rangle\otimes|\tfrac12\rangle , |0\rangle\otimes|\tfrac12\rangle , |-1\rangle\otimes|\tfrac12\rangle , |1\rangle\otimes|-\tfrac12\rangle , |0\rangle\otimes|-\tfrac12\rangle , |-1\rangle\otimes|-\tfrac12\rangle \}$ , is given by $$ l_z\otimes s_z=\frac{\hbar^2} 2 \begin{pmatrix} 1\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} and 0\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} \\ 0\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} and -1\begin{pmatrix}1 and 0 and 0\\0 and 0 and 0\\0 and 0 and -1\end{pmatrix} \end{pmatrix} =\frac{\hbar^2} 2 \begin{pmatrix} 1 and 0 and 0 and 0 and 0 and 0\\0 and 0 and 0 and 0 and 0 and 0\\0 and 0 and -1 and 0 and 0 and 0\\ 0 and 0 and 0 and -1 and 0 and 0\\0 and 0 and 0 and 0 and 0 and 0\\0 and 0 and 0 and 0 and 0 and 1 \end{pmatrix} . $$ this procedure should be repeated with both the $x$ and the $y$ components . each of those will yield a six-by-six matrix ( in this case ) . to get your final answer you should add all three matrices .
the solid angle is defined as the area on the unit sphere subtended by the angle divided by one unit area . it is a ratio so it is a single dimensionless number . i see why you think it should be a 2d quantity , because the surface of a sphere , and any patch on it , is a 2d manifold and you need two quantities ( traditionally $\theta$ and $\phi$ ) to map it . when you calculate an area on the sphere you are basically calculating a definite integral over $\theta$ and $\phi$ , and the result is of course just a single number . you do lose information in the process - for example you just know the total area not the shape of the patch on the sphere . the solid angle that covers the whole sphere is of course $4\pi$/1 or $4\pi$ .
there is a huge variance in how these coordinates are set up , and very often the coordinate systems are chosen for computational convenience ( having more data points in place where the metric varies a lot , and fewer far from , say , your colliding black holes ) , in addition to more physical choices . once you have run the simulation and have found a solution , however , you can apply math and create any coordinate system you wish for visualizations .
when you use the reduced mass , what you have first done is to go from the variables $ ( r_1 , p_1 ) $ and $ ( r_2 , p_2 ) $ to $ ( r=r_1-r_2 , p/\mu=p_1/m_1-p_2/m_2 ) $ and $ ( m r=m_1 r_1+m_2 r_2 , p=p_1+p_2 ) $ , where $m=m_1+m_2$ is the total mass and $1/\mu=1/m_1+1/m_2$ is the ( inverse of the ) reduced mass . as you said , this is usually introduced in classical mechanics to simplify the two-body problem , and it is not a priori valid in quantum mechanics . but in fact , it is . you just have to show that $\hat r , \hat r , \hat p , \hat p$ have all the expected commutation relations of independent position and momentum operators . this then allows to separate the two-body hamiltonian in two parts $\hat h ( \hat r_1 , \hat r_2 , \hat p_1 , \hat p_2 ) =\hat h_{red} ( \hat r , \hat p ) +\hat h_{cm} ( \hat r , \hat p ) $ .
there are no unique representations , so we will assume that op is mostly interested in normal-ordered expressions between the three operators $a^{\dagger}$ , $a$ and $n:=a^{\dagger}a$ . here $ [ a , a^{\dagger} ] =1$ . the underlying identities are $$ \tag{1} [ a , n ] ~=~a \qquad\text{and}\qquad [ n , a^{\dagger} ] ~=~a^{\dagger} , $$ which lead to $$ \tag{2} f ( a ) e^{zn}~=~ e^{zn} f ( e^{z} a ) \qquad\text{and}\qquad e^{zn}f ( a^{\dagger} ) ~=~ f ( e^{z} a^{\dagger} ) e^{zn} , $$ respectively . here $z\in \mathbb{c}$ is a complex number , and $f:\mathbb{c}\to \mathbb{c}$ is a sufficiently well-behaved function , e.g. an exponential function . so the sought-for commutators read in normal-order form $$ \tag{3} [ f ( a ) , e^{zn} ] ~=~ e^{zn} \{f ( e^{z} a ) -f ( a ) \} , $$ and $$ \tag{4} [ e^{zn} , f ( a^{\dagger} ) ] ~=~ \{f ( e^{z} a^{\dagger} ) -f ( a^{\dagger} ) \}e^{zn} , $$ respectively . more generally , one has $$ \tag{5} e^{zn}f ( a^{\dagger} , a ) ~=~ f ( e^{z} a^{\dagger} , e^{-z} a ) e^{zn} , $$ with corresponding commutator $$ \tag{6} [ e^{zn} , f ( a^{\dagger} , a ) ] ~=~ \{f ( e^{z} a^{\dagger} , e^{-z} a ) -f ( a^{\dagger} , a ) \}e^{zn} . $$
spontaneous emission occurs in all directions , and is akin to a glow . to have a concentrated beam with low divergence ( that is the idea of a laser ) , the photons must all be moving in the same direction--that is where a pair of mirrors come in . the high concentration of photons on this axis directly increases the rate of stimulated emission too . but spontaneous emission cannot be increased so simply--that is why it constitutes only about 1/10th of the output radiation of a typical laser .
like dmckee says , the potential energy of electrons in an atom does not really compare to the energy of the nucleus . since the nucleus is so tightly packed , and ( in the case of uranium ) contains so many protons , they have a lot of potential energy—it takes a lot of work to " push " them together . the strong force holds protons and neutrons together when they are close enough together . it is sometimes compared to a glue in that sense . however , this force disappears very quickly once the nucleons become separated past a certain distance ( on the order of a femtometer ) , so for a large nucleus like that of uranium , the protons on one end of the nucleus are not " stuck " to the protons on the other side , but there is still plenty of electrostatic force pushing them apart . that is why large nuclei are unstable ; especially those with a higher proton-to-neutron ratio , like u-235 ( or u-236 ) . if you can destabilize that nucleus enough ( you could imagine it stretching out like a football ) , its own electric force will tear it apart , and release a tremendous amount of energy as the fragments accelerate apart . it seems that you have already considered that to a degree , but yes , you do not have to worry about electrons . just think about the protons , because the nucleus is definitely not neutral . it is a bit of a problem if you do not know how close together those nuclear fragments start out , but you could probably estimate it by considering the nuclear radius of uranium , which is about 7 or 8 fm ( it is hard to find an exact figure ) , or by using double the radius of the fragments it creates , which , it would seem , would be paladium nuclei . obviously the protons were not in the same place when they start out , but they were very close together .
i can not be sure what the source meant without seeing the context , however i suspect the author meant the following . a $ u ( 1 ) $ gauge transformation acting on a charged scalar field gives : \begin{equation} \phi ( x ) \rightarrow e ^{ i \alpha ( x ) } \phi ( x ) \end{equation} under such a transformation the normalization is invariant since $\phi$ simply gains a phase . this is just the definition of a $u ( 1 ) $ rotation .
$i_{13}$ , $i_{32}$ and $i_{21}$ in eq . ( 3.56 ) are positive , as shown in the sentence below ( 3.57 ) and also in the caption of fig . 6 .
colour theory has a lot to do with how the brain processes the signals from the retina , as well as the physics of how light is detected in the eyes . but broadly speaking , the additive and subtractive properties of colour result from the physics of light and its interaction with pigments , so if we were tetrachromatic we would experience them similarly . the main difference is that a tetrochromat would experience four primary colours rather than three . this would change some aspects of colour theory ( i guess you had have something more like a " colour sphere " than the colour wheel , for example ) but the basics would be the same . imagining we had perfect rgb screen technology , capable of reproducing all the colours within our visible spectrum range ( 400nm—700nm ) . could you , if you had tetrachromatic vision , use the same rgb screen and still see all the colours in your " visible spectrum " of the same range ? such a screen is actually kind of impossible . there are colours that we trichromats can see that cannot be reproduced by an rgb screen . the range of colours a three-colour screen can produce are only a subset of the colours we can see . this is due to the way our cone cells respond to more than one light frequency at once . it is a fundamental thing , it is not just due to deficiencies in the screen . but the subset of colours that an rgb screen can produce is a pretty big one , so we generally do not notice . but aside from that point , no , a tetrochromat could not use a three-colour screen and still see most of the colours in its visible range . there would simply be a primary colour missing . it would be like trying to reproduce all the colours a trichromat can see using only two light frequencies . if you use red and cyan for example , then you can get red , white , cyan and black , but you can not get blue or green . could you use this perfect rgb screen technology if , while trichromatic , your cone cells were " tuned " to different frequencies ( but still covered the identical " visible light " range ) ? ! no , you could not , for similar reasons . most likely , it would still cover a large proportion of the colours you can perceive , but there would certainly be some colours you can see that it would not be able to produce .
even in curved spacetime , you can perform a coordinate transformation at any location ( "move to a freely falling frame" ) such that your metric is locally flat , and takes the form \begin{equation} ds^2 = -c^2 dt^2 + dx^2 + dy^2 + dz^2\end{equation} if you consider a null trajectory where $ds^2$ is set to 0 , then the above equation is the statement that " the differential physical distance traveled along the trajectory , as measured by an observer in a freely falling frame at the location in consideration , is equal to the speed of light times the differential time interval measured by that observer . " from einstein 's equivalence principle , this is precisely the way that light must behave .
how does this connect in any way to my equations ? you know the distance travelled and you know the proper time , $t ' = \tau = 10y$ . using the timelike invariant interval equation , solve for t : $t^2 = \tau^2 + ( \frac{r}{c} ) ^2 = ( 10y ) ^2 + ( 4.4y ) ^2 = 119.36y^2 \rightarrow t = 10.93y$ $u = \frac{r}{t} = 0.403c$ since you insist on doing it the hard way , here 's one approach : from your first transformation equation : $\dfrac{\delta x}{\delta t'} = \gamma u = \dfrac{4.4ly}{10y} = 0.44c$ from your 4th transformation equation : $\gamma \delta t = \delta t ' + \delta x \frac{\gamma u}{c^2} = 10y + 4.4y ( 0.44 ) = 11.94y$ combining these results : $ ( \gamma u ) ( \gamma \delta t ) = 5.25ly = \gamma^2 u \delta t = \gamma^2 \delta x = \gamma^2 4.4ly$ from which we get : $\gamma = 1.093$ $\delta t = \gamma \delta t ' = 1.093 \cdot 10yr = 10.93y$ $u = \gamma u / \gamma = \dfrac{0.44c}{1.093} = 0.403c$ as i attempted to point out in comments , the invariance of the interval is fundamental and useful here . the lorentz transformations guarantee that : $ ( c\delta t ) ^2 - ( \delta x ) ^2 = ( c\delta t' ) ^2 - ( \delta x' ) ^2$ you know that $\delta x = 4.4ly , \delta x ' = 0$ , and $\delta t ' = 10y$ so you just put in what you know and you get $\delta t$ immediately and the speed $u = \dfrac{\delta x}{\delta t}$ immediately follows . there is no need to find $\gamma$ in this problem
this is very much a quick and dirty answer , i havn't though too much about it . i might update my answer if i find time in the weekend , otherwise i hope others will give more precise answers . $v_{\alpha\beta\gamma\delta}$ transforms reducibly under $su ( 2 ) $ , as tensor products of four spin $\frac 12$ representations . using $\bf\frac 12\otimes\frac 12 = 0 \oplus 1$ and $\bf 1\otimes 1 = 0 \oplus 1 \oplus 2$ , we find that $v_{\alpha\beta\gamma\delta}$ decomposes into these irreducible representations $$\mathbf{\frac 12\otimes\frac 12\otimes\frac 12\otimes\frac 12} = ( 2\times\mathbf{0} ) \oplus ( 3\times \mathbf{1} ) \oplus \mathbf{2} , $$ two singlets , three triplets and a spin 2 ( 5-dimensional representation ) . there is an action of the permutation group $s_4$ on the indices of $v_{\alpha\beta\gamma\delta}$ , for $su ( n ) $ it turns out that decomposing this tensor into irreducible representations of the permutation group also corresponds to irreducible representations of $su ( n ) $ . this can be done rather quickly using young tableau , you can find the details in most representation theory books for physicists ( i do not have a relevant book here and do not remember the details . i might add the answer in the weekend ) . in the case of a tensor product $\bf 1\otimes 1 = 0\oplus 1\oplus 2$ , we get the following decomposition $$ t_{ij} = \delta_{ij}\frac{tr ( t ) }3 + \left ( \frac{t_{ij}-t_{ji}}2\right ) + \left ( \frac{t_{ij}+t_{ji}}2 - \delta_{ij}\frac{tr ( t ) }3\right ) . $$ these three terms transform irreducibly as spin $0$ , spin $1$ and spin $2$ representations of $su ( 2 ) $ , respectively . in terms for the permutation group , these are the trivial , anti-symmetric and trace less symmetric representations , respectively . something similar can be done for $v_{\alpha\beta\gamma\delta}$ , by using young tableau or just by playing around with it .
yes . g2 shows up often , starting with atomic physics ( perhaps racah is the first ; see r . e . behrends , j . dreitlein , c . fronsdal , and b . w . lee , “simple groups and strong interaction symmetries , ” rev . mod . phys . 34 , 1 ( 1962 ) . ) . you will find some refences in my 1976 phys rev paper on cns . physics . gatech . edu/grouptheory/refs . i have whole folder of physics g2 papers , but now i see i did not bother to enter g2 history into www.birdtracks.eu. nobody 's perfect . sorry predrag ( for responses , email to dasgroup [ snail ] gatech . edu , i sometimes look at those . pure accident i saw this question . . . )
yes , the $r$-argument really is $r_{ik}:=|r_i-r_k|$ , as he writes two pages earlier at the beginning of " systeme von endlich vielen teilchen " . but then you do not need the force to show the relation , it is just the chain rule , which makes derivatives of $u$ into a two term expression and notice that $u ( r_{ik} ) =u ( |r_i-r_k| ) =u ( |r_k-r_i| ) =u ( r_{ki} ) $ . also , it is not so good , that you write "$\frac{d}{dt}r_{a}\nabla_{a}u_{ab}$" for "$\frac{dr_{a}}{dt}\nabla_{a}u_{ab}$" , because it suggests that you mean "$\frac{d}{dt} ( r_{a}\nabla_{a}u_{ab} ) $" . moreover , the books name is not the autors name . and i would change the title to something readable , and by that i do not mean the problem with the total derivative , but a title which is a sentence , not a formula . e.g. " a problem deriving the energy conservation for radial two particle potentials " .
in fluid dynamics , within a isentropic process , the pression is not constant , you have a law like $\frac{p}{\rho^\gamma} = constant$ , or $\frac{p}{t^{\frac{\gamma}{\gamma -1}}} = constant$ here $p$ is the pressure ( called too a static pressure , $\rho$ is the density , $\gamma$ is the ratio of the specific heats of the fluid . because this is an isentropic process , the pressure $p$ can be called " isentropic pressure " . you have a compressible flow equation , which could be written , in the simplest case : $$\frac{v^2}{2} + \frac{\gamma}{\gamma -1}\frac{p}{\rho} = constant = \frac{\gamma}{\gamma -1}\frac{p_0}{\rho_0}$$ here $v$ is the speed of the fluid , $p_0$ is called the " total pressure " ( or stagnation pressure ) , and $\rho_0$ is called the " total density " . at zero velocity $v=0$ , the notions of isentropic pressure and total pressure coincide . idem for density and total density .
there is a sense in which this is right . if we model the laser as a stream of photons , hitting some surface , then it is the change in the momentum of the photons due to their interaction with the surface that causes the pressure . for example , if the laser shines on a mirror , then the photons will bounce back after hitting the mirror , and there momenta will change by an amount equal to twice the magnitude of their original momentum . however , it is also possible for the photons to be absorbed by the material . in this case , the photons do not bounce back , but their momentum change by an amount equal to the initial magnitude of the their momenta , and this momentum change due to interaction with the surface is what causes the pressure . on the other hand , if you are referring to the object that emits the laser , then it will certainly be the case that this object will feel a force in a direction opposite that of the beam travel direction . by conservation of momentum , if the laser emits a photon of momentum $p$ , then its momentum must increase in the opposite direction by that same amount $p$ as well . the change in momentum of the object then leads to propulsion . see also the wiki and especially the second paragraph in the quantum theory argument section .
a very general discussion-not specific to a system : the internal energy , $u$ , of a system is a function of state , which means that its value only depends on the thermodynamic variables ( $p , v , t ) $ for example , at a given state ( this means for a given set of values of these variables ) . let us make this more concrete : imagine the system is in a thermodynamic state where the thermodynamic variables have the values ( $p_i , v_i , t_i$ ) ( $i$ stands for initial ) . at these values of the thermodynamic variables the internal energy has a value : internal energy at the initial state $i$: $u ( p_i , t_i , v_i ) $ . you can think of a gas at pressure , volume and temperature condition ( $p_i , v_i , t_i$ ) . now imagine you change the thermodynamic variables to these ones ( $p_f , v_f , t_f$ ) ( $f$ stands for final ) . the internal energy now has a new value internal energy at the initial state $f$: $u ( p_f , t_f , v_f ) $ . in this process you have changed the internal energy of the system by an amount : change in u : $\delta u= u ( p_f , t_f , v_f ) - u ( p_i , t_i , v_i ) $ i hope it is clear to observe that the system could have followed an infinitely large set of $ ( p , v , t ) $-points , along an infinitely large number of different paths in order to go from state $i$ to state $f$ . however , these are not , in any way , influencing by how much $u$ will change , you can take which ever path you please to go from state $i$ to state $f$ . so the system has no memory of the intermediate states . in mathematical terminology , this means that the differential change , $du$ , is a perfect differential and this is stated by the simple mathematical expression $\oint_c du=0$ it is very similar to the gravitational potential of the earth , for example , which tells us that the amount of energy we need to spend to lift an object by 3m , does not depend whether we bring it straight vertically up or we follow some other path .
the answer to your question is , sometimes , but it depends on the source and your hypothesised relationship approximately holds in some cases . in three dimensions , your relationship does not hold . if the sound source is small , then its pressure field outside the source 's hardware will be a general multipole scalar field , i.e. a general superposition of spherical waves each fulfilling helmholtz 's equation $ ( \nabla^2 + k^2 ) \psi = 0$ where $k = 2\pi/\lambda$ is the wavenumber at the frequency in question in the medium in question : $$\psi ( r , \theta , \phi ) = \sum\limits_{\ell=0}^\infty \sum\limits_{\nu= -\ell}^\ell \psi_{\ell , \nu}\ , j_\ell ( k r ) p^{|\nu|}_\ell ( \cos ( \theta ) ) e^{i\ , \nu\ , \phi}$$ where $ ( r , \theta , \phi ) $ are the spherical co-ordinates of the point in question , $p^{|\nu|}_\ell$ are the associated legendre functions and $j_\ell$ are the spherical bessel functions of the first kind and order $\ell$ . in the farfield , i.e. when $k r \gg \ell$ we have $j_\ell ( k\ , r ) \approx \sin ( k\ , r - \ell \pi/2 ) / r$ so that the wave 's intensity varies like $1/r^2$ in the farfield . therefore , a factor of 10 increase in the distance from the source leads to a factor of 100 decrease in the intensity ( radiated power per unit area ) , which in decibel terms is a loss of $10\log_{10}100 = 20{\rm db}$ . therefore , to match the intensity of a 50db source at 1m distance , you are going to need a 70db source at 10m and a 90db source at 100m . you can get this result imagining the source is like an isotropic radiator with some , say dipole , radiation pattern . there will be an inverse square intensity dependence on dustance . however , suppose your source is somehow cylindrical . maybe it is like a paper cone loudspeaker but the cone is replaced by a very long paper cylinder radiating in and out . if you are near enough to the cylinder that it can be approximated as being infinitely long , then you have essentially gotten yourself a two dimensional problem , the waves are now cylindrical waves : $$\psi ( r , \theta ) = \sum\limits_{\ell=0}^\infty \sum\limits_{\nu=-\infty}^\infty \psi_{\nu , \ell}\ , j_\nu ( k_\ell r ) e^{i\ , \nu\ , \phi}$$ where now $j_\nu$ is the bessel function of the first kind and order $\nu$ . now in the farfield , $j_\nu ( k_\ell r ) \approx\sqrt{2/\pi}\cos ( k_\ell r -\nu\pi/2 -\pi/4 ) /\sqrt{r}$ so that the wave 's intensity varies like $1/r$ in the farfield and now a factor of ten increase in distance corresponds to a loss of 10db . therefore , to match the intensity of a 50db source at 1m , you would need a 60db source at 10m way or a 70db source at 100m away .
there might be several reasons , some more obvious than others . the quality of the cell ( including its orthogonal quality , aspect ratio , and skewness ) also has a significant impact on the accuracy of the numerical solution . orthogonal quality is computed for cells using the vector from the cell centroid to each of its faces , the corresponding face area vector , and the vector from the cell centroid to the centroids of each of the adjacent cells ( see equation 5–1 , equation 5–2 , and figure 5.22: the vectors used to compute orthogonal quality ) . the worst cells will have an orthogonal quality closer to 0 , with the best cells closer to 1 . the minimum orthogonal quality for all types of cells should be more than 0.01 , with an average value that is significantly higher . aspect ratio is a measure of the stretching of the cell . as discussed in computational expense , for highly anisotropic flows , extreme aspect ratios may yield accurate results with fewer cells . generally , it is best to avoid sudden and large changes in cell aspect ratios in areas where the flow field exhibit large changes or strong gradients . skewness is defined as the difference between the shape of the cell and the shape of an equilateral cell of equivalent volume . highly skewed cells can decrease accuracy and destabilize the solution . for example , optimal quadrilateral meshes will have vertex angles close to 90 degrees , while triangular meshes should preferably have angles of close to 60 degrees and have all angles less than 90 degrees . a general rule is that the maximum skewness for a triangular/tetrahedral mesh in most flows should be kept below 0.95 , with an average value that is significantly lower . a maximum value above 0.95 may lead to convergence difficulties and may require changing the solver controls , such as reducing under-relaxation factors and/or switching to the pressure-based coupled solver . source url
strictly speaking it is a unit of energy . but using $m=\frac{e}{c^2}$ , you can convert energy into mass . operating , we get $1{\rm\ , ev}/c^2 =1.78\cdot 10^{-36}\rm{\ , kg}$ . ( the $c^2$ is usually ommited . )
the first five minima look okay ( accounting for the rescaling of the axes ) . i do not know what is happening with the last minimum . how reproducible is it ? with the flat part you are seeing clipping of the amplifier and/or dac , not a physical effect in the tube . the vapour temperature is fine . 180°c is in the range of temperatures our group tested ( it got boring re-running the experiment dozens of times at different temperatures ! ) and you will not see any interesting thermal excitation effects there . on the other hand , our equipment was not able to go beyond an accelerating potential above about 32 v so we did not see your sixth minimum . it may be physical or may be an error , i do not know . i have attached my old reading list for the franck-hertz experiment . some of these ( particularly the original experiment - it is well worth reading their nobel lectures ! ) used proper professional laboratory equipment when they were not worried about electrocuting students . so one of these papers might have the data you want to compare to . fletcher j . ( 1985 ) . non-equilibrium in low pressure rare gas discharges . j . phys . d : app . phys . , 18 , 221 . franck , j . and hertz , g . ( 1925 ) . physics nobel lectures 1925 . physics . gargioni , e . and grosswendt , b . ( 1971 ) . scattering cross sections for electron transport calculations in matter . physicalisch-technischen bundesanstalt . google scholar search . genolio , r . j . ( 1973 ) . average energy of electrons in a franck-hertz tube . am . j . phys . , 41 , 288–290 . hanne , g . f . ( 1988 ) . what really happens in the franck-hertz experiment with mercury ? am . j . phys . , 56 no . 8 , 696–700 . retrieved from umk . li , b . , white , r . and robson , r . ( 2002 ) . spatially periodic structures in electron swarms : ionization , ndc effects and multi-term analysis . j . phys . d : app . phys . , 35 , 2914 . liu , f . h . ( 1987 ) . franck-hertz experiment with higher excitation level measurements . am . j . phys . , 55 no . 4 , 366–369 . mcmahon , d . r . a . ( 1983 ) . elastic electron-atom collision effects in the franck-hertz experiment . am . j . phys . , 51 no . 12 , 1086 . nicoletopoulos , p . and robson , r . ( 2008 ) . periodic electron structures in gases : a fluid model of the “window” phenomenon . phys . rev . lett . , 100 no . 12 , 1-4 . nicoletopoulos , p . ( 2003 ) . analytic elastic cross sections for electron-atom scattering from generalized fano profiles of overlapping low-energy shape resonances . arxiv:physics/0307081 [ physics . atom-ph ] . rapior , g . , sengstock , k . and baev , v . ( 2006 ) . new features of the franck-hertz experiment . am . j . phys . , 74 no . 5 , 423 robson , r . e . , li , b . , and white , r . d . ( 2000 ) . spatially periodic structures in electron swarms and the franck-hertz experiment . j . phys . b : at . mol . opt . phys . , 33 , 507–520 . sigeneger , f . and winkler , r . ( 2003 ) . on the kinetics of electron trapping in the franck-hertz experiment . xxvi international conference on phenomena in ionized gases . greifswald , germany , 15-20 july 2003 . sigeneger , f . , winkler , r . and robson , r . e . ( 2003 ) . what really happens with the electron gas in the famous franck-hertz experiment ? contrib . plasm . phys . , 43 no . 3-4 , 178-197 .
the total momentum of the whole swimmer+raft system is conserved , not of the raft only . so your equation should be $$p_{i , system}=p_{f , system}$$ $$0=m_{raft}\vec v_{raft}+m_{swimmer}\vec v_{swimmer}$$ you cannot conserve momentum for the raft alone because there is a force on the raft ( the swimmer pushing back with her legs , trying to jump forward ) . also note that $v_{raft}$ and $v_{swimmer}$ are the velocities in the ground frame . the $4.6 m/s$ of the swimmer might be with respect to the raft . so you will have to convert that velocity to the ground frame velocity .
if the mass of the penny is negligible comparing to the hammer , the speed of the hammer at the end of the track should be the same in both cases . with the collision in the first scenario , the speed of the penny should be twice of the hammer if the collision is complete elastic . but , for the second move-along scenario , the speed of the penny will be just the same as the hammer at the end of the track .
there are a few differences between luna and titan . one of the primary mechanisms for atmospheric loss is thermal escape . titan is much colder . the particles which escape are essentially the tail of the maxwell-boltzmann distribution , the portion with velocity higher than the escape velocity . this end of the distribution is dominated by an $e^{-e/kt}$ contribution , so as you had expect , lower temperature means fewer particles with enough kinetic energy to escape . note also that since we care about escape velocity , while the distribution is really about energy , more massive particles will not escape as easily . titan 's atmosphere is mostly nitrogen , while the moon is mostly helium and argon . the helium in the moon 's atmosphere is easily lost , since it is so much lighter . another big cause of atmospheric loss is the solar wind . titan itself does not have a magnetic field to protect it from the solar wind , but it does happen to orbit an enormous planet with a magnetic field . titan is protected from the solar wind by saturn 's magnetosphere . titan orbits at about 20 $r_s$ , while the magnetopause is somewhere between 16 and 27 $r_s$ , so titan is inside the magnetosphere a substantial amount of the time . there are a lot of complications due to passing through the magnetopause , but from what i understand , the net effect is definitely protective . and of course , titan is farther from the sun , so the solar wind is weaker .
the fundamental mechanism is the same . they are both photovoltaics . the cells on satellites are far more expensive and efficient . because satellite launch costs dominate any other cost , you might as well pay for a high-efficiency cell . terrestrial cells are 10-20% efficient , usually made from silicon . the ones in satellites can be double that efficiency , and are made from triple-junction ( recently maybe even quadruple-junction ) tandem cells with dozens of layers grown by an expensive epitaxial process ( usually mocvd ) . the cells on satellites have different design specs . . . weight , radiation-hardness , operating temperature , form factor , etc . they are also optimized for a slightly different spectrum of light , because there is no atmosphere that scatters the blue light etc . despite great efforts to make radiation-hard space solar panels , the radiation in space will certainly hurt their lifetime . but it seems not by much . this press release says that spectrolab has a solar cell that is expected to work almost as well ( 88% relative ) after 15 years in space . by comparison , i know terrestrial silicon solar cells can last well over 30 years , but i do not know exactly how the performance is at the end versus the beginning . by the way , you should keep in mind that the surface of earth can be a pretty tough environment too -- wind , weather , scratches , dirt , reactive oxygen and pollutant gases , heat-cycling , etc .
intuition is usually wrong in 3d dynamics . here are the basics . you start from a local to global 3x3 rotation matrix $r ( t ) $ and the body centered 3x3 fixed inertia matrix $i_{body}$ . then the 3x3 inertia matrix in world coordinates is $i ( t ) = r ( t ) i_{body} r ( t ) ^\top$ . if the linear acceleration vector of the center of mass is $\vec{a}_{cg} ( t ) $ and the angular velocity and acceleration of the body $\vec{\omega} ( t ) $ and $\dot{\vec{\omega}} ( t ) $ then the equations you need to solve are $$ \begin{aligned} \sum \vec{f} ( t ) and = m \ , \vec{a}_{cg} ( t ) \\ \sum \vec{m}_{cg} ( t ) and = i ( t ) \dot{\vec{\omega}} ( t ) + \vec{\omega} ( t ) \times i ( t ) \vec{\omega} ( t ) \end{aligned} $$ where $\sum \vec{f} ( t ) $ are the sum of all forces acting on the body and $\sum \vec{m}_{cg} ( t ) $ the moment of all the forces taken about the center of gravity plus the sum of all moments acting on the body . as i commented above read an introduction to physically based modeling first .
if $z \mapsto \mathrm{e}^{\mathrm{i}\delta}$ , then $z^p \mapsto ( \mathrm{e}^{\mathrm{i}\delta} z ) ^p = \mathrm{e}^{\mathrm{i}\delta p} z^p$ . what we are actually looking at is that the " transformation of the $p$-th power of $z$" is a way to speak of the representation of the circle group $\mathrm{u} ( 1 ) $ labeled by $p$ .
( 1 ) since $u ( \textbf{r} ) = u ( \textbf{r}+\textbf{r} ) $ , we can expand this part in terms of reciprocal lattice vectors , $u_k ( \textbf{r} ) = \sum_\textbf{g}{e^{i\textbf{g}\cdot \textbf{r}}u_\textbf{k-g}}$ . we can therefore write : \begin{equation} \psi_{\textbf k+\textbf k} = e^{i ( \textbf k + \textbf k ) \cdot \textbf r}\sum_\textbf{g'}{e^{i\textbf{g'}\cdot \textbf{r}}u_{\textbf k-\textbf k- \textbf g'}} = e^{i\textbf k \cdot \textbf r}\sum_\textbf{g'}{e^{i ( \textbf{g'}+\textbf k ) \cdot \textbf{r}}u_{\textbf k-\textbf k- \textbf g'}}=e^{i\textbf k \cdot \textbf r}\sum_\textbf{g}{e^{i\textbf{g}\cdot \textbf{r}}u_{\textbf k-\textbf g}} = \psi_\textbf k \end{equation} where $\textbf g = \textbf k+\textbf g'$ . ( 2 ) you can interpret $\textbf p'$ as being equal to $\textbf p$ . this is true because the real space lattice is periodic ; $\textbf k$ is always equal to $\textbf k + \textbf k$ . ( 3 ) the conserved quantity is $\textbf k$ $\textit mod$ $\textbf k$ . you can see that i used this fact in the answer to ( 2 ) . you can read just about any solid state physics textbooks for complete justification though my personal favorite is ziman 's theory of solids .
since the magnetic field lines have to close themselves , when it transverses the superconductor it has to do it in a continuous fashion . this means , since the superconductor expels the field when in the sc state , the field gets trapped because it has no way of transverse the cylinder ring without opening the magnetic field lines ( some geometric imagination is useful here ; ) . hope it is clear enough .
i can think of at least four things going on in this experiment that need pointing out : when you inflate a balloon by mouth , the air is warm : this makes the air inside the inflated balloon slightly lighter than the air it displaced the air inside the balloon has 100% relative humidity at 37c , and condensation will quickly form on the inside of the balloon as the air inside cools down . the air inside the balloon contains carbon dioxide , which has higher density than room air ( molecular mass of 12+16+16 = 44 amu , vs oxygen at 32 amu and nitrogen at 28 amu - ignoring small isotopic effects , and ignoring argon ) . the pressure inside the balloon is larger than outside - this increases the density so how large are each of these effects ? warm air : 37c vs 20c results in drop in density of 0.945x ( 293 / 310 ) or -5.5% moisture : partial pressure of water at 37c is 47.1 mm hg source which is about 0.061 atmospheres . assuming that pressure is constant , this water ( mass 18 amu ) displaces air ( mean mass 29 amu ) , so the density of the air decreases by 0.061 * ( 29 - 18 ) / 29 = 2.3% . if we allow the air outside the balloon to have 60% relative humidity ( with saturated vapor pressure of 10.5 mm hg ) , it would be slightly less dense than dry air ( 10.5*0.6/760* ( 29-18 ) /29 = 0.3% ) making the net difference -2.0% . note that much of this moisture will condense when the balloon cools down - little droplets will form on the inside of the balloon . with the air inside still saturated , its density will be 0.1% lower than on the outside ; the net result amounts to 2.9% of the mass of the air in the balloon . carbon dioxide : the exhaled air has 4 - 5 % carbon dioxide source : wikipedia , with an equivalent drop in oxygen . the density of exhaled air is therefore higher than that of inhaled air by 0.045 * ( 44 - 32 ) / 29 = +1.9% pressure in the balloon : from this youtube video - time point 3:43 i estimate the pressure increase in the balloon at 23 mm hg , resulting in an increase in density of 2.9% summarizing in a table : a freshly inflated balloon will thus have only a slightly lower density than the air it displaced , because the temperature + moisture effect is greater than the other two . after you wait a little while , the temperature will equalize and the density of the air inside the balloon will be greater - by 7.7% , with more than half of that not caused by the pressure in the balloon . . . in summary : the experiment described in your link measures the difference in density between air in a balloon , and ambient air . since the density of the air inside the balloon is higher than the density outside the balloon , one may conclude that the air inside the balloon has finite density . one may not conclude that the medium outside the balloon ( which we believe to be " dry air" ) has any density at all - since nothing in this measurement tells us about the air outside the balloon . if you did the experiment carefully with a balloon initially filled with warm air , and you allowed the air to cool down , you might be able to tell that the balance shifts - in other words , that there must be a change in the buoyancy experienced by the balloon as it cools down . that would be an experiment to demonstrate " air has mass " ( volume of balloon decreases , and it experiences less buoyancy ) . from the experiment as described ( popping the balloon ) , we learn that " exhaled air has mass " . that is not the same thing . if you used an air pump ( balloon pump ) to inflate the balloon , the first three components would go away and you are left with the difference due to the pressure only - 2.9% of the mass of the air in the balloon .
the unique thing about sno was that it was simultaneously sensitive to charged-current and neutral-current interactions , because they used deuterated water . the three main interactions are neutrino capture on deuterium , $\nu + n \to e + p$ , which generates a fast electron and a slow proton . the lepton and the baryon exchange a $w$ boson ( the " charged weak current" ) . only electron-type neutrinos may participate in this interaction ; $\nu_\mu$ and $\nu_\tau$ would have to generate heavier leptons , but solar neutrinos do not carry enough energy to make those more massive particles . deuterium dissociation due to neutrino scattering , $\nu + ( np ) \to \nu + n + p$ . the free neutron will wander around for a while before getting captured on another deuteron and emitting a gamma ray . because the neutrino 's charge does not change this reaction is mediated by the " neutral current " ( the $z$ boson ) and all neutrinos contribute equally . elastic scattering from electrons , $\nu + e \to \nu + e$ . this interaction has both charged- and neutral-current contributions , so neutrinos of all flavors may contribute , but electron neutrinos contribute more heavily than the other flavors . these different interaction channels gave independent measurements of the total neutrino flux and the electron neutrino flux . it is worth noting that the neutral current had only just been predicted in 1967 , and was not discovered until the early 1970s . for the most part the solar neutrino community believed that there was some misunderstood property of neutrino detection that caused everybody to measure one-third the predicted solar neutrino flux . it took many years before the possibility that the misunderstood bit was a property of the neutrino itself was really taken seriously . i do not know for certain , but i would expect that the design discussions for sno began in the early 1990s . there are many technical challenges associated with the detector — not least that they have many tons of heavy water suspended in many tons of light water in a thin , transparent membrane . the heavy water is on loan from the canadian nuclear power industry ; sno has a hefty insurance policy to pay to replace it if the membrane ruptures and the heavy water mixes with the light water and is ruined .
in standard newtonian mechanics , acceleration is indeed considered to be an absolute quantity , in that it is not determined relative to any inertial frame of reference ( constant velocity ) . this fact follows directly from the principal that forces are the same everywhere , independent of observer . of course , if you are doing classical mechanics in an accelerating reference frame , then you introduce a fictitious force , and accelerations are not absolute with respect to an " inertial frame " or other accelerating reference frames - though this is less often considered , perhaps . note also that the same statement applies to einstein 's special relativity . ( i do not really understand enough general relativity to comment , but i suspect it says no , and instead considers other more fundamental things , such as space-time geodesics . )
in vacuum and with only the particles we know about the answer is no . let 's look at the symmetries we know exist in nature : $su ( 3 ) $ colour : confined , only colourless states exist below the qcd phase transition $su ( 2 ) \times u ( 1 ) _y$ electroweak : higgsed to $u ( 1 ) _{em}$ electromagnetism $u ( 1 ) _{em}$: here we have opportunity . see below . . . $u ( 1 ) _{b-l}$: global symmetry in sm , possible gauged symmetry of gut or not a true symmetry at all . if gauged it is broken at a high scale . note that we do not see any massless goldstone bosons for this symmetry , so it can not be a spontaneously broken global symmetry . it must be higgsed or not a symmetry at all . in either case breaking has already happened or never will . breaking of this is relevant to baryogenesis but not to much else . qcd chiral flavour symmetry : only an approximate symmetry . broken by chiral condensates , the pseudo-nambu-goldstones are the mesons . ( i will stop listing approximate symmetries otherwise this list will get very long , but this one is rather important . ) local poincare invariance : exact and " gauged " in general relativity . ( note : there is an ongoing debate about the semantics of whether gravity is a gauge theory . there are important similarities and differences between gravity and the standard yang-mills gauge theories . hence the scare quotes on " gauged . " ) global poincare invariance ( $so ( 1,3 ) \ltimes \mathbb{r}_4$ ) : spontaneously broken by the fact that the universe is expanding and there is stuff in it . this is a symmetry of minkowski spacetime , so it is often used in particle physics , but it is not a symmetry of our actual universe because it is expanding . in general there are no global symmetries or conservation laws in gr , but the usual spacetime symmetries hold to a very good approximation on galactic cluster and smaller scales . my previous language meant to convey this but was sloppy and inaccurate . as far as i can see there are two options for spontaneous symmetry breaking in the current universe : either attack #3 or #6 . what do you need to have to break either of these groups ? you need an order parameter that transforms nontrivially under the symmetries to take a nonzero expecation value . for electromagnetism that means you need a charged condensate , but we do not know of any charged scalars and the chiral condensate is necessarily neutral ( why is that ? good question ; ) ) . in principle one of the $w^\pm$ could serve ( spontaneously breaking lorentz invariance as well ) , but this can not happen because they have large positive mass squared through the higgs mechanism . you would have to generate a negative effective mass squared using some fancy new mechanism that definitely does not exist at the low energies we can see . so you can not break em in vacuum , but you can in a medium where collective motions of many particles serve as the condensate . these exotic materials are called superconductors , and a few people think they are mildly interesting . ; ) that leaves local poincare invariance . this can be broken by a vector or tensor field developing a condensate . people have looked at these sorts of models , but needless to say there is nothing like this in known physics . experiments have demonstrated poincare invariance to an incredible accuracy . given the accuracy of the experiments and the cosmological scale of the transition temperature we are talking about you would need a vector or tensor with a cosmologically small negative mass squared . needless to say this is problematic , especially if you want to identify these with the known gauge bosons or graviton .
the reason for the asseveration if time $t$ , does not appear in lagrangian $\mathcal{l}$ , then the hamiltonian $\mathcal{h}$ is conserved . this is the energy conservation unless the potential energy depends on velocity . is that , from the definition of the hamiltonian as the legendre transformation , $$\mathcal{h}\equiv\sum_i\dot{q}_i\frac{\partial\mathcal{l}}{\partial\dot{q}_i}-\mathcal{l}\hspace{1in} ( \dagger ) $$ and knowing that for any function in phase space , $f=f ( q_i , p_i , t ) $ , $$\frac{df}{dt}=\frac{\partial{f}}{\partial{q}_1}\frac{d{q_1}}{d{t}}+\ldots+\frac{\partial{f}}{\partial{q}_n}\frac{dq_n}{dt}+\frac{\partial{f}}{\partial{p}_1}\frac{d{p_1}}{d{t}}+\ldots+\frac{\partial{f}}{\partial{p}_n}\frac{dp_n}{dt}+\frac{\partial{f}}{\partial{t}}\\=\sum_{j=1}^n\left ( \frac{\partial{f}}{\partial{q}_j}\dot{q}_j+\frac{\partial{f}}{\partial{p}_j}\dot{p}_j\right ) +\frac{\partial{f}}{\partial{t}}\\=\left\{f , \mathcal{h}\right\}+\frac{\partial{f}}{\partial{t}}$$ where $\left\{f , \mathcal{h}\right\}$ is the poisson bracket of $f$ and $\mathcal{h}$ , defined as $$\left\{f , \mathcal{h}\right\}\equiv\sum_{j=1}^n\left ( \frac{\partial{f}}{\partial{q}_j}\dot{q}_j+\frac{\partial{f}}{\partial{p}_j}\dot{p}_j\right ) =\sum_{j=1}^n\left ( \frac{\partial{f}}{\partial{q}_j}\frac{\partial{\mathcal{h}}}{\partial{p}_j}-\frac{\partial{f}}{\partial{p}_j}\frac{\partial{\mathcal{h}}}{\partial{q}_j}\right ) $$ if $\mathcal{l}=\mathcal{l} ( q_i , p_i ) $ , i.e. the lagrangian does not depend explicitly on time , which in turn means , from the definition $\mathcal{l}\equiv{t}-v$ , that kinetic energy $t$ and potential $v$ does not depend explicitly on time , then $\frac{d\mathcal{h}}{dt}=\left\{\mathcal{h} , \mathcal{h}\right\}=0$ . now , a constant of motion is precisely some function $f$ of phase space that is independent of time , i.e. such that $\frac{df}{dt}=0$ , so in this case the hamiltonian would be conserved . now , from the definition $ ( \dagger ) $ , you may verify that the hamiltonian equals the energy , $$\mathcal{h}\equiv{t}+v$$ only if $v=v ( q_i ) $ alone . so if that is the case , then energy would be conserved . so identify that in your lagrangian and get your conclusions , anyway you can always verify it this way for your particular case .
the reference you link to is for objects orbiting sun ( i.e. . comets or asteroids ) . if you are wanting to deal with satellites in low earth orbit , you will need a good book on orbital dynamics or astrodynamics . objects in solar orbit are typically dealt with in celestial mechanics . in both cases , the physics is the same ( two body gravitational interaction ) but the terminology is different . for satellites , one does not speak of " perihelion " or " aphelion " but rather " perigee " and " apogee . " some of the orbital parameters have slightly different names too . for satellites , orbital elements are disseminated in a highly standardized form called tle , which stands for two line element . there are also highly standardized algorithms for taking elements in tle form and turning them into topocentric ra and dec values as a function of time . your best bet , aside from an appropriate textbook on orbital dynamics , is to look at the source code for the algorithms themselves . the free satellite program predict http://www.qsl.net/kd2bd/predict.html stands out as being reliable and well documented . of course , the source code is included in the download . if you google around you may even find the nasa papers that document the sgp4/sdp4 algorithms . here are some c++ and c# implementations of the norad sgp4/sdp4 algorithms ( i have not tested or otherwise evaluated them ) . http://www.zeptomoby.com/satellites/ here 's another site i just found via google that looks potentially useful . http://satelliteorbitdetermination.com
the fundamental representation of a lie group $g$ , as commonly used in this context , is the smallest faithful ( i.e. . injective ) representation of the group . we do not require fermions to belong to the fundamental rep , it is just the case that , in the standard model , they always either belong to the fundamental or the trivial representation ( as that is thoroughly indicated by experimental data ) , so there is rarely a need to look at other representations . to belong to a certain representation $v_\rho$ of $g$ means that the field is a section of the associated vector bundle $p \times_g v_\rho$ , where $p$ is the principal bundle belonging to our gauge theory . equivalently , the field is a $g$-equivariant function $p \to v_\rho$ fulfilling $f ( pg ) = \rho ( g^{-1} ) f ( g ) $ for all $p \in p$ and $g \in g$ . if principal bundles are not talked about , the field is often just taken to be a function $\sigma \to v_\rho$ , though this is , strictly speaking , not the way to do it . every field must belong to a representation of the gauge group ( even if it is the trivial one ) since the gauge transformations must have a defined action upon everything in our theory . it is not required that fields belong to irreducible representations ( again , it is simply often just the case ) , but since every reducible representation can be split up into irreducible ones , it is enough to look at the behaviour of the irreducible representations . ( note though that there are fields transforming in reducible representations - the usual $\frac{1}{2}$-spinors ( not weyl spinors ! ) transform as members of the $ ( \frac{1}{2} , 0 ) \oplus ( 0 , \frac{1}{2} ) $-representation of the lorentz group )
as adam said this is not always true . this statement is only correct if you are in the ground state . if you want a path integral formalism demonstration , you can write a partition function introducing a counting field j in your action : $$ z [ j ] =\int \mathcal d\phi \exp\left ( -i\int dt \phi ( t ) a \phi ( t ) +2i\int\phi ( t ) j ( t ) \right ) $$ with a the propagator . if we introduce c the green function so that $ac ( t , t' ) =\delta ( t-t' ) $ , we can make the chage of variable : $$\tilde\phi ( t ) =\phi ( t ) -i\int dt'c ( t , t' ) j ( t' ) $$ you get $$ z [ j ] =\int \mathcal d\phi \exp\left ( -i\int dt \tilde\phi ( t ) a \tilde\phi ( t ) + -i\iint dt dt ' j ( t ) c ( t , t' ) j ( t' ) \right ) $$ so that : $$ \langle\phi ( \tau ) \rangle=\left . \frac{\delta \ln z [ j ] }{i\delta j}\right|_{j=0}=-2i\lim_{j\to0}\int dt'c ( \tau , t' ) j ( t' ) =0 $$
there is no escaping the lie theory if you want to understand what is going on mathematically . i will try to provide some intuitive pictures for what is going on in footnotes , i am not sure if it will be what you are looking for , though : on any ( finite-dimensional , for simplicity ) vector space , the group of unitary operators is the lie group $\mathrm{u} ( n ) $ , which is connected . lie group are manifolds , i.e. things that locally look like $\mathbb{r}^n$ , and as such possess tangent spaces at every point spanned by the derivatives of their coordinates - or , equivalently , by all possible directions of paths at that point . these directions form , at $g \in \mathrm{u} ( n ) $ , the $n$-dimensional vector space $t_g \mathrm{u} ( n ) $ . 1 canonically , we take the tangent space at the identity $\mathbf{1} \in \mathrm{u} ( n ) $ and call it the lie algebra $\mathfrak{g} \cong t_\mathbf{1}\mathrm{u} ( n ) $ . now , from tangent spaces , there is something called the exponential map to the manifold itself . it is a fact that for compact group , such as the unitary group , it is surjective onto the part containing the identity . 2 it is a further fact that the unitary group is connected , meaning it has no parts not connected to the identity , so the exponential map $\mathfrak{u} ( n ) \to \mathrm{u} ( n ) $ is surjective , and hence every unitary operator is the exponential of some lie algebra element . 3 ( the exponential map is always surjective locally , so we are in principle able to find exponential forms for other operators , too ) so , the above ( and the notes ) contain the answers to your first three questions : we can always represent a unitary operator like that since $\mathrm{u} ( n ) $ is compact and connected , the exponential of an operator means " walking in the direction specified by that operator " , and while $\mathcal{u}$ lies in the lie group , $\mathcal{t}$ lies , as its generator , in the lie algebra . one also says that $\mathcal{t}$ is the infinitesimal generator of $\mathcal{u}$ , since , in $\mathrm{e}^{\alpha \mathcal{t}}$ , we can see it as giving only the direction of the operation , while $\alpha$ tells us how far from the identity the generated exponetial will lie . the physical meaning is a difficult thing to tell generally - often , it will be that the $\mathcal{t}$ is a generator of a symmetry , and the unitary operator $\mathcal{u}$ is the finite version of that symmetry , for example , the hamiltonian $h$ generates the time translation $u$ , the angular momenta $l_i$ generate the rotations $\mathrm{so} ( 3 ) $ , and so on , and so forth - the generator is always the infinitesimal version of the exponentiated operator in the sense that $$ \mathrm{e}^{\epsilon t} = 1 + \epsilon t + \mathcal{o} ( \epsilon^2 ) $$ so the generated operator will , for small $\epsilon$ be displaced from the identity by almost exactly $\epsilon t$ . 1 think of the circle ( which is $\mathrm{u} ( 1 ) $ ) : at every point on the circle , you can draw the tangent to it - which is $\mathrm{r}$ , a 1d vector space . the length of the tangent vector specifies " how fast " the path in that direction will be traversed . 2 think of the two-dimensional sphere ( which is , sadly , not a lie group , but illustrative for the exponential map ) . take the tangent space at one point and imagine you are actually holding a sheet of paper next to a sphere . now " crumble " the paper around the sphere . you will end up covering the whole sphere , and if the paper is large enough ( it would have to be infinte to represent the tangent space ) , you can even wind it around the sphere multiple times , thus showing that the exponential map cannot be injective , but is easily seen to be surjective . a more precise notion of this crumbling would be to fix some measure of length on the sphere and map every vector in the algebra to a point on the sphere by walking into the direction indicated by the vector exactly as far as it is length tells you . 3 this is quite easy to understand - if there were some part of the group wholly disconnected to our group , or if our group had infinite volume ( was non-compact ) , we cannot hope to cover it wholly with only one sheet of paper , no matter how large .
notice : pertubative string theory is defined to be the asymptotic perturbation series which are obtained by summing correlators/n-point functions of a 2d superconformal field theory of central charge -15 over all genera and moduli of ( punctured ) riemann surfaces . perturbative quantum field theory is defined to be the asymptotic perturbation series which are obtained by applying the feynman rules to a local lagrangian -- which equivalently , by worldline formalism , means : obtained by summing the correlators/n-point functions of 1d field theories ( of particles ) over all loop orders of feynman graphs . so the two are different . but for any perturbation series one can ask if there is a local non-renormlizable lagrangian such that its feynman-rules reproduce the given perturbation series at sufficiently low energy . if so , one says this lagrangian is the effective field theory of the theory defined by the original perturbation series ( which , if renormalized , is conversely then a " uv-completion " of the given effective field theory ) . now one can ask which effective quantum field theories arise this way as approximations to string perturbation series . it turns out that only rather special ones do . for instance those that arise all look like anomaly-free einstein-yang-mills-dirac theory ( consistent quantum gravity plus gauge fields plus minimally-coupled fermions ) . not like $\phi^4$ , not like the ising model , etc . ( sometimes these days it is forgotten that qft is much more general than the gauge theory plus gravity plus fermions that is seen in what is just the standard model . qft alone has no reason to single out gauge theories coupled to gravity and spinors in the vast space of all possible anomaly-free local lagrangians . ) on the other hand now , within the restricted area of einstein-yang-mills-dirac type theories , it currently seems that by choosing suitable worldsheet cfts one can obtain a large portion of the possible flavors of these theories in the low energy effective approximation . lots of kinds of gauge groups , lots of kinds of particle content , lots of kinds of couplings . there are still constraints as to which such qfts are effective qfts of a string perturbation series , but they are not well understood . ( sometimes people forget what it takes to defined a full 2d cft . it is more than just conformal invariance and modular invariance , and even that is often just checked in low order in those " landscape " surveys . ) in any case , one can come up with heuristic arguments that exclude some einstein-yang-mills-dirac theories as possible candidates for low energy effective quantum field theories approximating a string perturbation series . the space of them has been given a name ( before really being understood , in good tradition . . . ) and that name is , for better or worse , the " swampland " . for this text with more cross-links , see here : http://ncatlab.org/nlab/show/string+theory+faq#relationshipbetweenquantumfieldtheoryandstringtheory
ds , as a segment of the wire carrying current , produces a magnetic field at " o " from all points that the wire is curved . when the wire is in the aa ' section or the cc ' section ds ( as an arbitrarily small section of the wire ) must now point at the origin and therefore it will not produce a magnetic field at the origin . in your question you probably mistakingly omited the $\hat{r}$ part which is why i asked for clarification .
what level are you at ? when you totally internal reflect light on a surface there is an electric field which extends a very small distance out of the surface - this is called the evanescent wave and excites states on the surface called surface plasmons . the interesting thing is that if you very slightly change the electrical characteristics of the surface you can make a big change in the internally reflected light . so a clever way of making extremely sensitive sensors is to bind some metal nanoparticles to the surface and coat them with some chemical that reacts with what you are looking for . the chemical you are sensing sticks to the nanoparticles , changes the electrical field and changes the reflected light = very sensitive chemical/biochemical detector .
this is not correct ( dissipated power is depending on operation conditions , it is not invariant ) . correct is the value of the current through lamp2 ( it is 1.13a ) . you need to look up what voltage this corresponds to from the lamp2 chart . the voltage v1 is then 5v plus this voltage .
it depends on your definition of annihilation . but microscopically all processes are described by feynman diagrams such as these of which last one describes electron positron annihilation ( if it were not for the typo in the out-going photon ) . but as you can see it is all a simple matter of how you turn your head around and the very same diagram represents emission ( or absorption ) of photon by electron ( or positron ) . does electron annihilate with photon and create a brand new electron ? you can certainly interpret it that way . in other words , it is just a question of terminology and interpretation . actual physics does not depend on how you call the process . it is encoded in the underlying math of quantum field theory ( qft ) . in any case , the punchline is that annihilation ( in the strict sense of particle-antiparticle inelastic collision ) does not have a special place in one 's vocabulary once they learn their qft and particle physics . it is just one particular kind of interaction . so you might as well ask which arbitrary interactions are allowed . and answer to that is : there is quite a lot of them and they are described by standard model . but the basic picture is that particles can be charged under certain charges : either the familiar electromagnetic , or less familiar weak and strong charges . or in more modern language whether some families of particles form a multiplet under some gauge group . for weak force with group su ( 2 ) you get lepton ( e . g . electron-neutrino ) and quark ( e . g . up-down ) doublets . for strong force with group su ( 3 ) you need triplets and these are precisely the red green blue colors of quarks that you probably heard about . in any case , for every multiplet there is a diagram like the ones above where you have two charged particles and one mediating particle between them ( photon , weak bosons or gluons ) . besides this you can also get funnier diagrams with e.g. three or four gluon lines . but that is it , these are all of allowed interactions of standard model .
however ket vectors that are composed of superpositions have multiple possible eigenvalues . which leads me to believe that that equation is only valid for eigenkets which are basis states . the equation \begin{align} \hat a|\psi\rangle = a|\psi\rangle \end{align} holds only for eigenvectors of the operator $\hat a$ . in general , there is a mathematical theorem , the spectral theorem , that says that for any hermitian ( self-adjoint ) operator $\hat a$ acting on a hilbert space $\mathcal h$ , there exists a basis of the hilbert space composed of eigenvectors of $\hat a$ . this tells us that any vector $|\psi\rangle$ in the hilbert space can be written as a linear combination of eigenvectors of any given observable . let 's say , for example , that the basis of eigenvectors corresponding to observable $\hat a$ is denoted by $\{|a_1\rangle , |a_2\rangle , \dots\}$ where the vector $|a_i\rangle$ has eigenvalue $a_i$ . then for any state $|\psi\rangle$ in the hilbert space , we can write \begin{align} |\psi\rangle = \sum_i c_i|a_i\rangle \end{align} is it somehow assumed that every basis state in the position basis corresponds to a single energy eigenstate ? no . an eigenstate of one operator is not necessarily an eigenstate of another operator . if , however , two operators commute , then it is possible to find a basis for the hilbert space comprised of vectors that are eigenstates of both operators ( we usually call these " simultaneous eigenstates " of the two operators ) . is there any kind of useful interpretation of multiplying the eigenket by it is eigenvalue as appears in the above observable equation ? i am not sure what you are exactly looking for here , but one fact is that if $|\psi\rangle$ satisfies the eigenvalue equation , and if the system is prepared in that state , then a measurement of the observable $\hat a$ will return the corresponding eigenvalue with probability $1$ .
from a physics point of view , you have some kind of mass-diffusion problem . it is true that a mathematician will name it " heat equation " because the classical problem with the operator $\partial_t - \delta$ is the heat equation . homogeneous neumann bc is indeed appropriate to model a " no flux " condition at the boundaries , be it a flux of heat or of mass . a dirichlet bc here would mean that your domain is in contact with a large ( infinite ) reservoir of fixed density there .
1 ) the pioneer of the rigorous treatment of thermodynamics is constantin carathéodory . his aticle ( carathéodory , c . , untersuchung über die grundlagen der thermodynamik , math . annalen 67 , 355-386 ) is cited everywhere in this context , but probably you want some newer and more modern things . 2 ) buchdahl wrote a lot of papers about this subject in the 40 's , 50 's and 60 's . he summarized these in the book : h.a. buchdahl , the concepts of classical thermodynamics ( cambridge monographs on physics ) , 1966 . 3 ) there was a recent series of articles on this subject by lieb and yngavason which became famous . you can find the online version of these here , here , here and here : ) . 4 ) finally , i have come across the book t . matolcsi , " ordinary thermodynamics " ( since a few friends of mine went to the class of the author ) , which treats thermodynamics in a mathematically very rigorous way . i hope some of these will help you . greetings , zoltan
if the symmetry axis of the cone lies along the direction of travel , and if you are using the drag equation \begin{align} f_\mathrm{drag} = \frac{1}{2}\rho v^2a \end{align} to compute the drag force , then you should take $a$ to be the area of the base , namely the full area that would be obstructing your vision if you were looking at the object coming toward you . this assertion is confirmed by wikipedia when it defines the drag coefficient which uses the nice term " projected frontal area ; " the reference area depends on what type of drag coefficient is being measured . for automobiles and many other objects , the reference area is the projected frontal area of the vehicle . this may not necessarily be the cross sectional area of the vehicle , depending on where the cross section is taken . for example , for a sphere $a=\pi r^2$ .
if you have a water drop with radius $r$ then the pressure difference between the inside of the drop and the outside is : $$ \delta p = \frac{2\gamma}{r} $$ to calculate the hole size you need to work out the pressure at the bottom of the cone and equate this to the pressure calculated using the expression above . the pressure at the bottom of the cone depends on the depth of the water , not the total volume of water in the cone . if the depth of water in the cone is $h$ then the pressure is $\rho g h$ , where $\rho$ is the density of the water at the temperature you are working at , and $g$ is the acceleration due to gravity ( $\approx$ 9.81 m/sec$^2$ ) . equating this to the first expression gives : $$ \rho g h = \frac{2\gamma}{r} $$ or : $$ r = \frac{2\gamma}{\rho g h} $$ for example at stp $\gamma \approx 7.3 \times 10^{-2}$n/m and $\rho \approx$ 1000kg/m$^3$ , so if the depth of the water in your cone is 10cm the maximum radius of the hole is 0.1mm . note that this is the maximum radius for which there is no flow at all . for holes a bit bigger than this the flow may be so slow it is difficult to measure .
the anti-particle for any particle is obtained by charge c and parity p conjugation . c is the operation that interchanges positive and negative charges and p is the operation that reflects in a mirror . the combined operation of cp must produce a particle of the same mass . this is a theorem of relativistic quantum field theory due to cpt symmetry . this other particle is either the same particle or an antiparticle with opposite charge and/or chirality . some particles such a photons , gluons , z bosons , pions , higgs , graviton etc , do not have anti-particles because they are invariant under the cp transformation . you can say that they are their own anti-particle . this can only happen for particles without electric charge and with no chirality . in principle the qcd color charge is also reversed for an anti-particle . this suggests that a gluon should not be regarded as its own anti-particle , but since colourless states are never seen the distinction cannot really be made in any operational sense . all known particles which are their own anti-particle are bosons , but it is also possible for a fermion to be its own anti-particle if it is a majorana spinor rather than a dirac spinor . no known fermions are of this type ( unless neutrinos are majorana ) but they exist in susy models . observed elementary particles that do have anti-particles include all the quarks and leptons ( except possibly the neutrinos ) and the charged w bosons . any composite particle also has an antiparticle made of the anti-particles of its constituents . this can only be its own anti-particle if all its constituents are ( e . g . a glueball ) , or if it is made of particle/anti-particle combinations as is the case for pions .
it is actually very simple . the general lorentz transformation can be rewritten as $$\left ( \begin{matrix} 1 and 0 \\ 0 and h^\textrm{t} \end{matrix}\right ) \ , \left ( \begin{matrix} ct \\ x \\ y \\ z \end{matrix}\right ) = l_u \ , \left ( \begin{matrix} 1 and 0 \\ 0 and k^\textrm{t} \end{matrix}\right ) \ , \left ( \begin{matrix} ct^\prime \\ x^\prime \\ y^\prime \\ z^\prime \end{matrix}\right ) \ , . $$ this corresponds to aligning the $x$ and $x^\prime$ axes with the direction of the relative velocity , and then applying the standard lorentz transformation .
the zeroth law posits the existence of temperature by stating that if a is in equilibrium with b and a is in equilibrium with c , then b is in equilibrium with c . we can then assign an intensive property to a , b and c that we call " temperature " . they are in equilibrium == they have the same temperature . as soon as they are not in equilibrium , the zeroth law is silent . thus , as you observed in your question , we cannot derive an ordering of temperatures based on the zeroth law alone . here , the second law comes to the rescue . the formulation i am familiar with states the entropy of a closed system never decreases if we have two objects that are not in thermal equilibrium , then when we bring them into contact we expect heat to flow between them . now according to the second law , if we move heat $\delta q$ from $a$ to $b$ ( at temperatures $t_b$ and $t_b$ respectively ) , the change in entropy is $$\delta s = t_a \delta q - t_b \delta q\\ = \delta q ( t_a - t_b ) $$ now if the entropy of the system cannot decrease , then if $\delta q$ is positive we know that $t_a - t_b$ must be positive . this is where we find the ordering of temperature : heat travels from hotter to cooler until thermal equilibrium is reached . thus when we have two objects in unequal states we can tell which is hotter by looking at the direction in which heat flows between them . that direction is always from hotter to colder - and to prove this you need the second law . there is an amusing ( although somewhat dated - 50 years old this year ) song by the duo of flanders and swann that touches on this topic . see http://youtu.be/vnbivw_1fns
superfluids can " climb walls " and whatnot . http://en.wikipedia.org/wiki/superfluid superfluids have zero viscosity , but may have surface tension . those with surface tension creep up walls in a capillary-like fashion ( except here , they can creep up a single wall , whereas capillary action requires a tube ) . i do not know about the immiscible liquids , though . what you are saying may be true for certain anisotropic liquids . but i am not sure of this . or maybe weird stuff will happen if you mix immiscible superfluids . ( speculation follows ) they may actually form alternating bands , when one liquid climbs up the wall , the other liquid climbs up the film of the first liquid on the wall . this may happen for two superfluids with different densities and surface tensions .
to make it fall you need a torque . this torque is provided by the weight force acting on the center of mass of the object and by the offset between the center of mass and the edge of the object . imagine your domino standing upright then tilt it . you are moving the center of mass . when the center of mass ( blue ) is on the right of the edge ( red ) then you have a torque , represented by the triangle . the torque is $\tau = m g d$ so to make it fall you need $d$ greater than zero . if the domino ( of uniform constant density ) as base of width l the center of mass is located at l/2 . for an height h , the center of mass is located at height h/2 . that his other sketch : taking this one in the limit case where $d=0$ you obtain the last sketch . solving the trigonometry you obtain $\alpha = atan \frac{l/2}{h/2}$ . the angle of the domino with respect to the " table " is $90-\alpha$ degrees .
we obviously do not know the shape exactly , but by measuring the positions of visible stars in our neighborhood we can tell that it is a spiral arm galaxy . in fact to the naked eye , the fact that we see a band of diffuse light called the milky way shows that we are in a flat type of galaxy and not in a globular cluster or an elliptical galaxy . there is a lot of evidence that the supermassive black hole that is at the center of our galaxy is located in sagittarius a and we know the distance to that black hole . in addition we can measure the orbital velocity of stars rotating around the galaxy . however , we can only see individual stars that are somewhat close to our position about 2/3 of the way out from the center of our galaxy - there is too much obscuring dust to see to the other side of the galaxy , for example . so we will always be somewhat limited in our ability to map out our galaxy exactly . however we do know enough to confidently say that we are in a spiral armed type of galaxy .
first , i just want to remind readers that it is not true that " more glancing angle always means more reflection " . for p-polarized light , as the angle goes away from the normal , it gets less and less reflective , then at the brewster angle it is not reflective at all , and then beyond the brewster angle it becomes more reflective again : nevertheless , it is certainly true that as the angle approaches perfectly glancing , the reflection approaches 100% . even though the question asks for non-mathematical answers , the math is pretty simple and understandable in my opinion . . . here it is for reference . ( i do not have any non-mathematical answer that is better than other peoples ' . ) the maxwell 's equations boundary conditions say that certain components of the electric and magnetic fields have to be continuous across the boundary . the situation at almost-glancing angle is that the incoming and reflected light waves almost perfectly cancel each other out ( opposite phase , almost-equal magnitude ) , leaving almost no fields on one side of the boundary ; and since there is almost no transmitted light , there is almost no fields on the other side of the boundary too . so everything is continuous , " zero equals zero " . the reason this cannot work at other angles is that two waves cannot destructively interfere unless they point the same direction . ( if two waves have equal and opposite electric fields and equal and opposite magnetic fields , then they have to point the same direction , there is a " right-hand rule " about this . ) at glancing angle , the incident and reflected waves are pointing almost the same direction , so they can destructively interfere . at other angles , the incident and reflected waves are pointing different directions , so they cannot destructively interfere , so there has to be a transmitted wave to make the boundary conditions work . :- )
fermi 's golden rule still applies in the relativistic limit , and can be rewritten in a lorentz invariant fashion . starting with the transition probability $$ w_{i\rightarrow f} = \frac{2\pi}{\hbar} |m_{if}|^2 \rho ( e ) \ , , $$ to have $w$ lorentz invariant we had like both the matrix element $|m_{if}|^2$ and the density of final states $\rho ( e ) $ to be invariant . this can be done by shifting a few terms around . a little bit of handwaving to motivate it : the wave function $\psi$ ( which is in the matrix element ) has to be normalized by $\int |\psi|^2 dv = 1$ , which gives us a density ( of probability to encounter a particle ) of $1/v$ . now , a boosted observer experiences length contraction of $1/\gamma$ , which changes the density to $\gamma/v$ . to obtain the correct probability again , we should re-normalize the wave function to $\psi&#39 ; = \sqrt{\gamma}\ , \psi $ by pulling the lorentz factor out . so we intoduce a new matrix element $$|{\cal m}_{if}|^2 = |m_{if}|^2 \prod_{i=1}^n ( 2 \gamma_i m_i c^2 ) =|m_{if}|^2 \prod_{i=1}^n ( 2e_i ) ^2 $$ ( this is for an $n$-body process ) . now the transition probability ( here in differential form ) becomes : $$ dw = \frac{2\pi}{\hbar} \frac{|{\cal m}_{if}|^2}{ ( 2e_1 ) ^2 ( 2e_2 ) ^2 \cdots} \cdot \frac{1}{ ( 2\pi\hbar ) ^{3n}} \ , d^3p_1 \ , d^3p_2 \ , \cdots \delta ( {p_1}^\mu + {p_2}^\mu + \ldots - {p}^\mu ) $$ the delta function is there to ensure conservation of momentum and energy . now we can regroup the terms : $$ \rightarrow \quad dw = \frac{2\pi}{\hbar} \frac{|{\cal m}_{if}|^2}{ 2e_1 2e_2 \cdot \ldots} \cdot d_\mathrm{lips} $$ the density of states/"phase space " $d\rho$ is replaced by a relativistic version , sometimes called the lorentz invariant phase space $d_\mathrm{lips}$ , which is given by $$ d_\mathrm{lips} = \frac{1}{ ( 2\pi\hbar ) ^{3n}} \prod_{i=1}^n \frac{d^3p_i}{ 2e_i } \delta\left ( \prod_{i=1}^n {p_i}^\mu - {p}^\mu \right ) \ , . $$ the nice thing about the relativistic formula for $dw$ is that , in the case you are scattering particles off one another , it immediately shows us three important contributions : not only the matrix element and phase space , but also the flux factor $1/s$ ( where $s = ( {p_1}^\mu + {p_2}^\mu ) ^2$ is the mandelstam variable , and in case the masses are negligible , $ s \approx 2 e $ ) . this flux factor is responsible for the general $1/q^2$ falling slope when you plot cross section over momentum transfer $q = \sqrt{s}$ , which comes entirely from relativistic kinematics . hope this answers your questions . here is a presentation ( pdf ) that sums it up , with an explicit proof that it is lorentz invariant .
if both firecrackers went off at the same spatial point in the earth frame then you could just use the time dilation formula to get the time interval in the rocket . however they go off at different spatial points and this means you have to do the full calculation . let 's see how this works . we will choose our origin so that in the earth frame the first firecracker goes off at $ ( 0 , 0 ) $ , then the second firecracker goes off at $ ( t , d ) $ , where $t = 2\mu s$ and $d = 300m$ , but let 's keep it general for now . we will choose the coordinate system of the rocket so its origin coincides with the earth frame , then the point $ ( 0 , 0 ) $ is the same in both frames . we just need to find where the point $ ( t , d ) $ is in the rocket frame . the lorentz transformations tell us : $$\begin{align} t ' and = \gamma \left ( t - \frac{vx}{c^2}\right ) \\ x ' and = \gamma \left ( x- vt \right ) \end{align}$$ actually we are only asked for the time difference in the rocket frame so we just need $t'$ , and this is : $$ t ' = \gamma \left ( t - \frac{vd}{c^2}\right ) $$ now if the two firecrackers had been in the same place $d$ would be equal to zero and we would just have : $$ t ' = \gamma t$$ so you could just use the usual time dilation equation . it is because $d \ne 0$ that we can not do this . later : the question provides an interesting example of the easy and hard ways to solve problems . i’ll go through the details in case it’s of interest to anyone . if we choose the origins of the two frames to coincide when the first firecracker goes off , so it’s at $ ( 0 , 0 ) $ in both frames , then in the earth frame the second firecracker is at $ ( 2\mu s , 300m ) $ . the question asks if in the rocket frame the second firecracker is at $ ( t’ , 200m ) $ then what is the value of $t’$ ? the easy way to answer this is to use the invariance of the proper time . the proper time , $\delta \tau$ , between any two spacetime points $ ( t , x , y , z ) $ and $ ( t + \delta t , x + \delta x , y + \delta y , z + \delta z ) $ is given by : $$ c^2\delta \tau^2 = c^2\delta t^2 - \delta x^2 - \delta y^2 - \delta z^2 $$ the proper time is the same for all observers , so in this case the earth observer and the rocket observer must calculate the same value for $\tau$ . for the earth observer we have : $$ \tau_{earth} = c^2t^2 – d^2 $$ where $t = 2\mu s$ and $d = 300m$ , and for the rocket we have : $$ \tau_{rocket} = c^2t’^2 – d’^2 $$ where we’re told that $d’ = 200m$ . setting $\tau_{earth} = \tau_{rocket}$ we get : $$ c^2t^2 – d^2 = c^2t’^2 – d’^2 $$ and a quick rearrangement gives : $$ t’^2 = \frac{ c^2t^2 – d^2 + d’^2}{c^2} $$ and putting in the values for $t$ , $d$ and $d’$ we get : $$ t’ = 1.86 \mu s $$ and this answers the question . the hard way to do it is to use the lorentz transformations . these tell us that : $$ t ' = \gamma \left ( t - \frac{vd}{c^2}\right ) $$ but we don’t know the value of $v$ . to get this we have to use the transformation equation for distance : $$ x ' = \gamma \left ( x- vt \right ) $$ we know that $t = 2\mu s$ , $d = 300m$ and $d’ = 200m$ , so we can solve for $v$ . the reason this is hard is that after much scribbling ( and swearing ) the result is : $$ v = \frac{2dt \pm \sqrt{4d^2t^2 - 4 ( t^2 + \frac{d’^2}{c^2} ) ( d^2 – d’^2 ) }}{2 ( t^2 + \frac{d’^2}{c^2} ) } $$ and putting in the values for $t$ , $d$ and $d’$ we get two solutions ( corresponding to the $+$ and $-$ in the $\pm\sqrt{}$ term ) with the values for $v$ of : $$ v = 2.185 \times 10^8 \text{m/s} \space \text{and} \space 5.148 \times 10^7 \text{m/s} $$ and finally substituting either of these into : $$ t ' = \gamma \left ( t - \frac{vd}{c^2}\right ) $$ gives us $ t’ = 1.86 \mu s$ . so we get the same result as using the simpler method , but after much more pain ! still , using the lorentz equations does tell us that there are two possible velocities for the rocket .
as said in the comments , this is a very broad question , so instead of writing a very long post , i point you to a good article titled " superconductivity and the environment : a roadmap": http://iopscience.iop.org/0953-2048/26/11/113001 . the article lists a lot of emerging technologies that make use of superconductors . the applications of room temperature superconductors would be the same as the applications of normal superconductors , but these applications would just be much easier to realize if cryogenic environment is not needed . many items listed in the article would become preferred over non-superconducting way of doing things if an easy-to-use material with room temperature superconductivity was found . since there is no complete theory as for what causes superconductivity in high temperatures , it is impossible to guess when ( if ever ) a rts is found . finding these materials is basically educated guessing an a lot of trial-and-error . it could be that someone stumbles upon such material tomorrow or it could be that room temperature superconductors do not even exist . there is no way to know .
apparently that is possible . from http://en.wikipedia.org/wiki/formula_one_car: indycars , for example , produce downforce equal to their weight ( that is , a downforce:weight ratio of 1:1 ) at 190 km/h ( 118 mph ) , while an f1 car achieves the same at 125 to 130 km/h ( 78 to 81 mph ) , and at 190 km/h ( 118 mph ) the ratio is roughly 2:1 . from http://www.formula1.com/inside_f1/understanding_the_sport/5281.html: a modern formula one car is capable of developing 3.5 g lateral cornering force ( three and a half times its own weight ) thanks to aerodynamic downforce . that means that , theoretically , at high speeds they could drive upside down .
redgrittybrick 's comment is correct . in order to make a roller turn , there has to be force whose vector does not go through the roller 's axis . it needs to avoid the axis in order to exert a turning moment . the only such forces are the frictional ones from the plank and the ground .
in a new paper , rodejohann and zhang write ( pages 13 to 14 ) that in the standard model ( with massless neutrinos ) , the top yukawa can never rg-evolve to exactly 1 , but that this becomes possible once you have massive neutrinos . then it will grow beyond 1 as you continue to still higher energies . but they also write that attaining the exact value 1 could indicate " the restoration of certain kinds of yukawa unifications or flavor symmetries " . so if you can find a form of symmetry breaking which sometimes occurs when a coupling is exactly unity , and then use it appropriately in a gut or other model of new physics . . . then you will have an explanation .
the stefan-boltzmann law governs the irradiance ( radiant power per unit area ) . the total energy is not sigma*t^4 , but rather the total power $p$ of a body with surface area $a$ and temperature $t$ is given by \begin{equation} p=a\sigma t^4 \end{equation} this result may be surprising , but it is correct . the reason irradiance rises so quickly is because the wavelengths of light decrease with increasing temperature , carrying away more energy with each photon ( on average ) . at the same time , higher temperatures cause the photon emission rate from the surface to increase . it is the increase in the energy of the photons coupled with the increase in photon production rate that gives the $t^4$ dependence .
when you calculate work , you do so along a given path . here , that path has tangent vector $d\mathbf s$ . this is a vector with direction ; the minus sign will ultimately come from choosing the path 's orientation--inward or outward . edit : aha , i think i have found the unintuitive part . the key is in the use of the coordinate $r$ to parameterize the path , in that $r$ is larger at the start of the path and smaller at the end . this runs counter to what you would usually do when parameterizing such a path with an arbitrary parameter . let $\mathbf s_0$ and $\mathbf s_1$ be the starting and ending points of a path $\mathbf s ( \lambda ) = \mathbf s_0 + ( \mathbf s_1 -\mathbf s_0 ) \lambda$ . the work integral is then $$w = \int_{\mathbf s_0}^{\mathbf s_1} \mathbf f ( \mathbf s ) \cdot d\mathbf s= \int_0^1 \mathbf f ( \mathbf s ( \lambda ) ) \cdot \frac{d\mathbf s}{d\lambda} \ , d\lambda = \int_0^1 \mathbf f ( \mathbf s ( \lambda ) ) \cdot ( \mathbf s_1 - \mathbf s_0 ) \ , d\lambda$$ for two finite points , the basic approach is sound , but it breaks down when you have a point at infinity involved . this is the reason that the problem of assembling a configuration is usually attacked with a different basic parameterization . instead , set $\mathbf s ( \lambda ) = \lambda \hat{\mathbf a}$ for some unit vector $\hat{\mathbf a}$ and set the bounds of the integral as being from $ [ \infty , r ) $ . this is the important point : even though the path is being traversed coming in from infinity , the parameterization means that $d\mathbf s/d\lambda = + \hat{\mathbf a}$ , not minus as i originally thought . the path 's still oriented outward ; we are just traversing it backwards . here 's how that integral looks : $$w = \int_{\infty}^r \mathbf f ( \lambda \hat{\mathbf a} ) \cdot \hat{\mathbf a} \ , d\lambda$$ of course , we know the expression for the electric force : $$\mathbf f ( \mathbf r ) = k\frac{qq_0 \mathbf r}{|r|^3}$$ plug in $\mathbf r = \lambda \hat{\mathbf a}$ to get $$\mathbf f ( \lambda \hat{\mathbf a} ) = k \frac{qq_0 \lambda \hat{\mathbf a}}{\lambda^3} = k \frac{q q_0 \hat{\mathbf a}}{\lambda^2}$$ we find that the integrand is then $$w = \int_{\infty}^r k \frac{q q_0}{\lambda^2} \hat{\mathbf a} \cdot \hat{\mathbf a} \ , d\lambda = \int_\infty^r k \frac{q q_0}{\lambda^2} \ , d\lambda = - k \frac{q q_0}{r} &lt ; 0$$ the work is negative , so the change in potential energy $\delta u = - w$ is positive as required . so where is the problem then ? as we have seen , there actually should not be an extra negative sign coming in on line 4 ( as posted in the op 's question ) . this is somewhat obscured because an explicit parameterization of the path is never written down in the first place--usually , you do not have to , but this problem is tricky enough that it helps immensely .
the momentum eigenstate is not normalizable . suppose $x$ is a periodic variable with period $2\pi r$ -- the periodicity is important as momentum is then a `good ' operator . then one has the allowed values of momenta are quantized i.e. , $p_n = \frac{n \hbar}{ ( r ) }$ with $n\in \mathbb{z}$ . for the values of $p$ mentioned above , the momentum eigenstates are normalizable with $c=1/\sqrt{2\pi r}$ . let us call this normalized state $\langle x|n\rangle$ in the coordinate basis . explicitly , one has $$\langle x|n\rangle = \tfrac1{\sqrt{2\pi r}}\ e^{ip_n x/\hbar}\ . $$ ( you should treat the remark in the ucsd page about one particle to mean that your normalize your state to one . ) the eigenstates are orthogonal to each other as one can check explicitly . $$ \langle n | m\rangle =\delta_{n , m}\ . $$ the next step is to go from the box normalizable states to the delta function normalizable states . one needs to take $r\rightarrow\infty$ and convert kronecker delta into the dirac delta function . one uses the following identification which follows from the properties of the dirac delta function and standard integration : $$ \sum_m = \frac{r}{\hbar}\int dp \quad \mathrm{and}\quad \delta_{m , n} = \frac{\hbar}{r} \delta ( p_n-p_m ) \ . $$ with these identifications , we define $$ |p_n\rangle =\sqrt{\tfrac{r}{\hbar}}\ |n\rangle\ . $$ it is now easy to see that the dirac delta normalization implies that in the $x$-basis , one has $$u_p ( x ) :=\langle x|p_n\rangle = \tfrac1{\sqrt{2\pi \hbar}}\ e^{ip_n x/\hbar}\ . $$ this leads to a simple mnemonic : replace $r$ by $\hbar$ to go from box normalizable states to delta function normalization . of course , momentum which was discrete in a box now takes values in the continuum .
for questions about resonances and particles the particle data group is the best reference . one can find the whole delta resonance family and remind oneself what each number is standing for , and thus know how to pronounce the symbol . the number in parenthesis is the mass in mev . the superscript is the charge of the particular resonance displayed on the plot , presumably . s , p , d , . . . are by convention the labels of the angular momentum quantum number " l " , and the two numbers are the numerators of the isospin and j quantum number ( j is the total angular momentum quantum number ) . so the first one is read as : delta zero seventeen fifty ( pee three one ) or ( pee three halves one half ) . etc . ( the superscript of parity is missing in your information . ) the bar over a symbol denotes an antiparticle , antidelta ( 1910 ) zero in the second line . i would try and put the charge next to the main symbol , your first option but the other way is clear also . for similar questions the naming scheme for hadrons would be a help in comprehension as well as pronunciation .
the moon was moving in relation to what ? i had to think about this when you described your experience . because of the distance of the moon compared to objects on the horizon , it should appear to stand still in comparison to everything else closer whizzing past you . but then i thought of your description of " oscillations " and thought " aha ! the window . " if you were travelling exactly west to east , and you were observing the moon through a window exactly facing south ( or north , depending which side of the train you are on and the time of year and where on earth you are , ) it would appear to be standing still in your window . ( neglecting for the moment the comparatively slow apparent progression of the moon through the sky . ) but i reckon your train track was not exactly straight for the duration of your trip and the moon was never exactly perpendicular to your train 's direction . but it was probably quite close to perpendicular . so any slight deviation would cause you to observe the moon travelling towards one edge of the window , and then a slight deviation in the other direction would cause the moon to apparently move towards the other edge of the window . it is simply the winding s-shaped route your train took , heading generally east but never exactly , that caused these oscillations .
i would like to add to the answers already given . indeed , the atmosphere is transparent to shortwave radiation from the sun , but absorbs a lot of the longwave radiation from the earth ; that is why we have the greenhouse effect , that is why the earths has a liveable climate and that is part of the reason why we have the lapse rate we observe . but why is it colder at the tibetan plateau , which is a large , flat area at roughly 4 km elevation ? are not we equally close to the local surface there as when we are at sea level ? let 's assume the tibetan plateau receives the same intensity of solar radiation as lower areas at the same latitude . in reality , it probably receives more due to the dry climate . then it should heat up more , should not it ? but it does not . the system earth-atmosphere can be considered to be in a local radiative-convective equilibrium ( see the diagram from kevin trenberth below ) . this means that the energy flows " in " and " out " cancel out by energy transport due to radiation and convection . in other words : what goes in , must go out ( this is not really true locally , because there are large-scale flow patterns known as wind ) . now , the earth surface emits radiation according to its temperature with $p = \epsilon \sigma t^4$ . some of this radiation is aborbed by greenhouse gases ( or clouds ) in the atmosphere : water vapour , carbon dioxide , methane , and others . then the atmosphere heats up , and again radiates according to $p = \epsilon \sigma t^4$ ; part of this radiation goes into space , and part goes back to the surface . the greenhouse gases keep the surface of the earth warm like a blanket . now at the tibetan plateau , the atmosphere is much less dense , because the elevation is so high . therefore , radiation emitted by the surface is not absorbed much , but mostly exits straight into space . this means that the surface cools down . to return to the blanket analogy : tibet has a much thinner blanket than lower elevations do . now i have made a number of severe simplifications , because in reality it depensd on day/night , on clouds , on atmospheric flow such as wind , on humidity , and on other factors . but whereas the explanation given by others explains why it gets colder higher up in the free atmosphere , i think it does not really explain why it is colder at the tibetan plateau .
the energy differences are not due to the isospin which has little direct impact on the energy . instead the energy difference is due to the spin-spin interaction . however , having a different isospin state impacts that allowed spin states . for the mesons , which are bosons , the states must be symmetric . on the other hand , the baryons , which are fermions , the states must be antisymmetric . consider the $\eta$ and $\pi$ 's . the pions , having a symmetric isospin , must have a symmetric spin . the triplet state is less energetic than the singlet . this makes the pions lighter . a similar arguement can be made for the baryons .
at the risk of telling you how to " suck eggs " ( your level in these things is not altogether clear ) , here goes . ingredients : the essential ingredients to this explanation are : a physical " system " which evolves in and whose " events " happen in some space $\mathcal{u}$ ( ordinary euclidean 3-space or minkowsky spacetime , for example ) ; in physics this space is always a linear ( wontedly it is minkowsky spacetime ) space wherein stuff happens : let 's call $\mathcal{u}$ the " scene " of where stuff we want to talk about happens ; a connected lie group $\mathfrak{g}$ which represents the co-ordinate transformations the a system can undergo : in physics these are all linear transformations $\mathcal{u}\to\mathcal{u}$ of the scene $\mathcal{u}$ . wontedly in physics , $\mathfrak{g} = so^+ ( 1,3 ) $ ( the identity connected component of the lorentz group comprising all rotations and boosts of " physical space " , sometimes called the " proper , orthochronous lorentz group " ( proper = unimodular determinant =1 , i.e. " does not invert space " and orthochronous = does not switch the time direction ) or the poincaré group ; a cover of $\mathfrak{g}$ ; this is almost always ( i have never seen it not so ) the universal cover $\tilde{\mathfrak{g}}$ of $\mathfrak{g}$ as explained in my article " lie group homotopy and global topology " on my website here ; a vector space $\mathcal{v}$ which can be , for example , a quantum state space , possibly infinite dimensional and its group $gl ( \mathcal{v} ) $ of linear endomorphisms , i.e. bijetive , linear maps $\phi:\mathcal{v}\to\mathcal{v}$ . informally , $gl ( \mathcal{v} ) $ is the group of invertible matrices acting on $\mathcal{v}$ . most importantly : this space is different from the physical " scene " $\mathcal{u}$ . the scene is $\mathcal{u}$ spacetime all around us , the state space $\mathcal{v}$ is a hilbert space of quantum states . and , actually , although we talk about a " linear " state space $\mathcal{v}$ , we are a bit sloppy : sure , all quantum states are linear superpositions of the basis for $\mathcal{v}$ but they are always of unit magnitude : the probabilities of a measurement 's " collapsing " the state into one of the basis vectors must all sum up to one - " we have to end up in some state " . so , if we are being precise , we take heed that we are actually talking about the unit sphere within $\mathcal{v}$ as the state of quantum states . this state space is very different in character from spacetime , where there is no obligation for 4-positions of events to be unit magnitude ; representations $\rho : \mathfrak{g}\to gl ( \mathcal{v} ) $ , $\tilde{\rho}:\tilde{\mathfrak{g}}\to gl ( \mathcal{v} ) $ of both $\mathfrak{g}$ and its cover $\tilde{\mathfrak{g}}$ , respectively . recall that a representation of a lie group $\mathfrak{g}$ is a homomorphism from from $\mathfrak{g}$ to $gl ( \mathcal{v} ) $ , i.e. a transformation that " respects the group product " so that , given $\gamma , \ , \zeta\in\mathfrak{g}$ , we have $\rho ( \gamma\ , \zeta ) =\rho ( \gamma ) \ , \rho ( \zeta ) $ . and , as discussed above , the linear transformations of the form $\rho ( \gamma ) , \ , \tilde{\rho} ( \tilde{\gamma} ) \in gl ( \mathcal{v} ) $ for $\gamma \in\mathfrak{g}$ and $\tilde{\gamma} \in\tilde{\mathfrak{g}}$ must be unitary so that the transformed quantum states stay normalised . so we can see that $gl ( \mathcal{v} ) $ is very different from $\mathfrak{g}$ or $\tilde{\mathfrak{g}}$: lorentz boosts most assuredly are not unitary ! we say that $\mathfrak{g}$ or $\tilde{\mathfrak{g}}$ " act on the state space $\mathcal{v}$ through the respresentations $\rho$ , $\tilde{\rho}$" . i am using here more of the mathematician 's description of a representation , because here ( i am not always so fussed ) i believe it is clearer than the physicists because we need to take heed that there are two different classes of representations in our discussion : those whereby the group of co-ordinate transformations $\mathcal{g}$ act on the state space $\mathcal{v}$ and those whereby its cover $\tilde{\mathcal{g}}$ acts on $\mathcal{v}$ . baking instructions : wigner 's theorem and why covers are interesting why are we interested in covers at all ? after all , elements of $\tilde{g}$ are not the " physical " co-ordinate transformation . this is where we meet our baking oven for our ingedients : wigner 's theorem . clearly , when our scene $\mathcal{u}$ undergoes a co-ordinate transformation , then the transformations wrought on the quantum state has to preserve inner products in the quantum state space so that the state stays properly normalised . from this assumption alone , i.e. one does not have to assume linearity , wigner proved that the when the scene $\mathcal{u}$ undergoes a " symmetry " ( a lorentz transformation ) , the state space must undergo a " projective homomorphism " $\sigma$ , i.e. if $\gamma , \ , \zeta$ are two lorentz transformations , then the state space transformation corresponding to their product is : $$\sigma ( \gamma\ , \zeta ) = \pm \sigma ( \gamma ) \ , \sigma ( \zeta ) \tag{1}$$ the fact that we do not get exactly a homomorphism is why we are interested in covers : the image of the representation $\tilde{\rho} ( \tilde{\mathfrak{g}} ) $ ( recall that this is a group of unitary operators in $gl ( \mathcal{v} ) $ acting on the state space ) of the cover $\tilde{\mathfrak{g}}$ contains both the transformations that fulfill the genuine $+$-sign homomorphism in ( 1 ) ( which are simply unitary operators in the image $\rho ( \mathfrak{g} ) $ of the co-ordinate transformation group $\mathfrak{g}$ ) and those that flip the sign . so if we allow representations of the cover , we get every possible unitary transformation ( even without an assumption of linearity - this automatically follows ) that can be wrought on the state space $\mathcal{v}$ when the scene $\mathcal{u}$ is transformed . here 's the punchline . quantum states that transform by the transformations belonging to the image $\rho ( \mathfrak{g} ) $ of $\mathfrak{g}$ under the genuine homomorphism $\rho$ are called vectors . quantum states that transform by the transformations belonging to the image $\tilde{\rho} ( \tilde{\mathfrak{g}} ) $ of the cover $\tilde{\mathfrak{g}}$ under the " projective homomorphism " $\tilde{\rho}$ are called spinors . the above can be intuitively thought of as follows : in quantum mechanics , a global phase $e^{i\phi}$ multiplying a system 's quantum state does not affect measurements we make on the system . so quantum systems " do not care whether a homomorphism is genuine or projective " . the universal ( only ) cover of the lorentz group $so ( 1,3 ) $ is the group $sl ( 2 , \ , \mathbb{c} ) $ . so " spinors " transform by a representation of $sl ( 2 , \ , \mathbb{c} ) $ . vectors transform by a representation of $so ( 1,3 ) $ . the word " spinor " can be pretty vague in my experience : it can refer to the transformation in $sl ( 2 , \ , \mathbb{c} ) $ rather than the quantum state that is transformed by it , and people often speak of the unit quaternions as " spinors": roger penrose 's " road to reality " chapter 11 simply defines a spinor as something that takes a negative sign when rotated through $2\ , \pi$ and comes back to its beginning point after a rotation through $4\ , \pi$ . this is actually a pretty good definition , for that is exactly how elements of a representation of $sl ( 2 , \ , \mathbb{c} ) $ act on the state space $\mathcal{v}$ , and is the essential difference between how elements of a representation of $so ( 1,3 ) $ act on state spaces . forget about " quantities with direction " as a vector 's definition : in physics the word " vector " always talks about how something transforms when our scene $\mathcal{u}$ undergoes a symmetry . remember this is pretty near to the word 's literal meaning vehor ( transliterated as vector ) literally means " i am borne " or " i am carried " in latin , so it is all about how the " vector " is borne , either by a transformation in physics or as a pathogen in biology ( the word 's original english meaning ) .
you can not argue with a black object and a white object alone , as i think you partially understand in trying to build your thought experiment . you need a little bit more to define things properly . see whether the following helps . imagine a black object at a temperature $t_0$ and a white object also at $t_0$ inside a perfectly isolating box full of blackbody radiation at some higher temperature $t_1&gt ; t_0$ ( i.e. without the black and white objects , this radiation is in thermodynamic equilibrium ) . to understand exactly what would happen , you would have to describe the " colour " of your objects with emissivity curves that show emissivity as a detailed function of frequency . so your " black " and " white " would need to be defined in much more detail . you would also have to define the surface areas of the two objects and what they are made of ( i.e. define their heat capacities ) . but all of this only effects the dynamics of how the system reaches its final state , i.e. these details only influence how the system evolves . what it evolves to is the same no matter what the details : the box would end up with everything at the same temperature such that the total system energy is , naturally , what it was at the beginning of the thought experiment . " blacker " as opposed to " whiter in this context roughly means " able to interact , per unit surface area , with radiation more swiftly": the blacker object 's temperature will converge to that of the radiation more swiftly than does that of the whiter object , but asymptotically the white object " catches up " . blacker objects absorb more of their incident radiation its true , but they also emit more powerfully than a whiter object at the same temperature . the one concept emissivity describes the transfer in both directions . think of emissivity as being a fractional factor applied to the stefan-boltzmann constant for the surface as well as being the fraction of incident light absorbed by the surface relative to a perfect blackbody radiator . this description is altogether analogous to that of the situation where $t_0&lt ; t_1$ . begin with $t_0=t_1$ , and you have got thermodynamic equilibrium from the beginning . nothing happens , of course . maybe the following will help thinking about what is a really quite a complex question : it would be a fantastic last question for an undergrad thermodynamics exam btw : you can abstract detail away by saying lets define object $a$ to be blacker than object $b$ if , when both objects are made of the same material , are the same size and shape , the temperature of $a$ converges to the final thermodynamic equilibrium temperature more swiftly than that of $b$ when they are both compared in the box-radiation-object thought experiment above . thinking about this now , i am not sure whether the above definition would hold for every beginning temperature of the radiation . maybe there are pairs of surfaces whose relative blackness is different at different beginning temperatures such that $a$ is blacker than $b$ with some beginning temperature whilst the order swaps at a different beginning temperature . i think it is unlikely , but that is probably a different question altogether . by the way , which pub do you drink in ? i might come along . afterword on a heater 's colour : you ask by implication what is the best colour to paint a heater . this is not a simple question and involves the dynamics of the heater system . it is really an engineering question . i suspect in general it is better for them to be blacker rather than whiter . here 's a glimpse of the kind of factors bearing on the situation . if you can say a heater has a constant nett input of $p$ watts , then at steady state that is going to be its output to the room , altogether regardless of its colour . there may be a materials engineering implication here : if you paint the heater whiter , and if its dominant heat transfer to the room is by radiation ( rather than by convection or conduction ) , then it has to raise itself to a higher temperature than it would were it blacker so as to radiate $p$ watts into the room . so its materials might not be as longlasting , and it might be more of a fire hazard than it would be were it blacker . if the heater is the hot water kind , and again if radiative transfer is significant , then the heating system has to run hotter to output power at a given level if the heater is whiter . at a given flow rate and given temperature of heating water , the heat output of heater is lower if it is whiter . you are trying to design the heater to be an " anti-insulator": you want the heat to leak out of the flow circuit in at the heater , not through the lagging on the hot water pipes outside the building channelling the water from the boiler to the heaters . if the hot water pipes leak heat in the same room , then that is no problem . recall the quartic dependence of the stefan boltzmann law . at room temperatures with a low temperature heater ( the hot water kind ) $\sigma\ , t^4$ is likely to be pretty small compared with other heat transfer mechanisms , in contrast to my idealised scenarios above . so the heater 's colour is likely to be pretty irrelevant .
the fresnel equations describes the portion of the electromagnetic field that is reflected at a surface . for any indefinite and non-engineered material , in order to have refraction you must have an index greater than 1 , and thus you must have reflection at the surface . there are two significant exceptions to this . first , if you create a slab of material such that the reflected wave off of the second boundary is both perfectly out of phase with the wave reflected off of the first surface and of equal amplitude , the reflected waves will destructively interfere causing no reflection , but transmission will still occur . this will lead to refraction but no perceived reflection . secondly , if you were to engineer an isotropic metamaterial such that it had an index of -1 , the surface will have no index mismatch so there will be no reflected wave , but " refraction " will still occur . this is only a theoretical condition , as no isotropic negative index metamaterial has been fabricated . for a brief overview of metamaterials see here .
if you have a copy of griffiths , he has a nice discussion of this in the delta function potential section . in summary , if the energy is less than the potential at $-\infty$ and $+\infty$ , then it is a bound state , and the spectrum will be discrete : $$ \psi\left ( x , t\right ) = \sum_n c_n \psi_n\left ( x , t\right ) . $$ otherwise ( if the energy is greater than the potential at $-\infty$ or $+\infty$ ) , it is a scattering state , and the spectrum will be continuous : $$ \psi\left ( x , t\right ) = \int dk \ c\left ( k\right ) \psi_k\left ( x , t\right ) . $$ for a potential like the infinite square well or harmonic oscillator , the potential goes to $+\infty$ at $\pm \infty$ , so there are only bound states . for a free particle ( $v=0$ ) , the energy can never be less than the potential anywhere , so there are only scattering states . for the hydrogen atom , $v\left ( r\right ) = - a / r$ with $a &gt ; 0$ , so there are bound states for $e &lt ; 0$ and scattering states for $e&gt ; 0$ .
energy is associated with work not with force as work energy theorem states . in other words , force can be exerted without generating any work , in such a case whatever exerted the force does not lose or gain any energy because no work was done . in your example , the rubber slope exerted force ( friction force ) but no work was done ( the metal block did not move ) . so there is no exchange of energy although force is exerted . think of the force in this case as energy-free action . the energy of the slope has nothing to do with the gravitational potential energy of the metal block
after much investigation , simulation and a deep literature search , i have figured out the true answer . you perceive a chirp because you are being hit with the echos of the sharp noise that generated the sound . the times between the arrival of those echos is decreasing inversely with time , so it sounds as if it were a tone with a fundamental frequency increasing linearly in time , hence the chirp . to get a feel for the phenomenon , consider a simulation : above you see a slowed down version of the simulated pressure wave inside a 2d racquetball court . i threw up the generated sound on soundcloud . if you watch the simulation , pick a particular point and watch the reflected sounds go by , you will notice the different instances of the multiple echos arrive faster and faster as time goes on . you can clearly hear the chirps in the generated sound , and if you listen closely you can hear secondary chirps as well . these are also visible in the spectrogram : this phenomenon was studied and published recently by kenji kiyohara , ken'ichi furuya , and yutaka kaneda : " sweeping echoes perceived in a regularly shaped reverberation room , " 　j . acoust . soc . am . vol . 111 , no . 2 , 925-930 ( 2002 ) . more info in particular , they explain not only the main sweep , but the appearance of the secondary sweeps using some number theory . worth reading in full . this suggests that for the best sweep one should both stand and listen in the center of the room , though they should be generic at any location . simple geometric argument following the paper , we can give a simple geometric argument . if you imagine standing in the middle of a standard racquetball court , which is twice as long as it is tall or wide , and clap , your clap will start propagating and reflecting off the walls . a simple way to study the arrival times is with the method of images , so you imagine other claps generated by reflecting your clap across the walls , and then reflections of those claps and so on . this will generate a whole set of " image " claps , located at positions $$ ( m , l , 2k ) l $$ where $m , l , k$ are integers and $l$ is 20 feet for a racquetball court , the time for any particular clap to reach you is $t = d/c$ and so we have $$ t = \sqrt{m^2 + l^2 + 4k^2} \frac{l}{c} $$ for our arrival times . if we look at how these distribute in time : it becomes clear why we perceive a chirp . the various sets of missing bars , which themselves are spaced like a chirp , give rise to our perceived subchirps . details of the 2d simulation for the simulation , i numerically solved the wave equation : $$ \frac{\partial^2 p}{dt^2} = c^2 \nabla^2 p $$ and used impedance boundary conditions on the walls $$ \nabla p \cdot \hat n = -c \eta \frac{\partial p}{\partial t} $$ i used a collocation method spatially , with a chebyshev basis of order 64 in the short axis and 128 on the long axis . and used rk4 for the time integration . i modeled the room as 20 feet by 40 feet and started it of with a gaussian pressure pulse in one corner of the room . i listened near the back wall towards the top corner . i put up an ipython notebook of my code , with the embedded audio and video . i recommend playing with it yourself . on my desktop it takes about minute to do a full simulation of the sound . effect of listening location i have updated the code to generate sound at multiple locations , and generate their sounds . i can not seem to embed audio on stackexchange , but if you click through to the ipython notebook view , you can listen to all of the generated sounds . but what i can do here is show the spectrograms : these are laid out in roughly their locations inside of the room . here the noise was generated in the lower left , but the chirps should be generic for any listening and generation location .
i am not a professional , but i will try to answer anyway . meteor showers occur when the earth passes through the orbit of a comet ( or , in at least one case , an asteroid ) . over time , the debris spreads over the entire orbit of the comet . a shower can last for several days , which is an indication of how wide the debris stream is . assuming a duration of 1 day , and assuming the earth 's orbit is roughly at right angles to the debris stream , that gives a width of very roughly 2.5 million kilometers ( and a length of several hundred million kilometers ) . the earth is only about 12,735 kilometers in diameter . say the comet 's orbit is 1 billion kilometers long ( that is probably shorter than average ) . then multiplying the length of the orbit by the area of a circle 2.5 million kilometers across gives the volume of the stream , and multiplying 2.5 million kilometers by the area of a circle 12,735 kilometers in diameter gives the volume of the stream through which the earth passes ( the hole it punches in the stream ) . the ratio is about 15 million . other factors : earth 's gravity will pull in some debris that would not otherwise have hit it , making its effective diameter a bit bigger ( thanks to ghoppe 's comment ) . the density of the stream is not uniform . there are bound to be clumps of greater density . there is probably also a systematic change of density with distance from the sun . the width of the stream probably varies as well . i have no idea of the details the moon ( and its gravity well ) will also sweep up some debris -- but the moon 's effective area is a small fraction of earth 's . but the blatant errors in my assumptions undoubtedly swamp any such effects , and i am only looking for a rough estimate . so yes , the earth 's passage through a meteor stream will effectively punch a hole in it , but it is a very small hole relative to the size of the entire stream . it could have a significant effect over millions of years . i am making a lot of simplifying assumptions here , but the conclusion seems about right if i have gotten the result within one or two orders of magnitude . reference : http://www.amsmeteors.org/meteor-showers/meteor-faq/#5, plus some of my own extremely rough back-of-the-envelope calculations .
i assume you are asking how a black hole can evaporate due to hawking radiation . the answer is that the hawking radiation does not come from the event horizon , but instead comes from a region just outside the event horizon so time has not stopped at its position . if you were to watch a black hole form then evaporate , you would never see an event horizon form . that is because in your co-ordinates the event horizon would take an infinite time to form . you would see the infalling matter slow and red shift , then be re-emitted as hawking radiation without the event horizon ever having formed .
something i posted on reddit answers this question quite well , i think : " rational " and " irrational " are properties of numbers . quantities with units are not numbers , so they are neither rational nor irrational . a quantity with units is the product of a number and something else ( the unit ) that is not a number . by choosing the unit you use to express a quantity , you can arrange for the numeric part of the quantity to be pretty much any number you want ( though switching units will not let you change its sign or direction ) . in particular , it can be rational or irrational . and choices of units are a human convention , so it would not make any sense to extend the idea of rationality or irrationality to the quantity itself . you can use a natural unit system , where certain physical quantities are represented by pure numbers . for example , if you use the same units to measure time and space , $c = 1$ . in such a unit system , it does make sense to say the speed of light is rational , but that is kind of a special case . that reasoning does not really work with other physical quantities . and you really do have to be using natural units . ( technically , you could make a natural unit system where $c = \pi$ , but it would have very complicated and perhaps even inconsistent behavior under lorentz transforms , so nobody does that . ) by the way , empirical measurements always have some uncertainty associated with them , so they are not really numbers either and are also neither rational nor irrational . a measurement is probably better thought of as a range ( or better yet , a probability distribution ) which will necessarily include both rational and irrational numbers .
in mechanical problems where the hamiltonian does not depend explicitly on time , then the hamiltonian is equal to the total energy $h = e$ . these problems can be solved using the ( inverse ) legendre transform , which allows the computation of the lagrangian from the hamiltonian , without the need to solve a differential equation . the lagrangian can be computed from the hamiltonian $h = e$ as follows : the velocity is given by the hamilton equation of motion ( here $c=1$ ) : $v = \dot{x} = \frac{\partial e}{\partial p} =\frac{p+\frac{2p^3}{m^2}}{\sqrt{p^2+m^2+ \frac{p^4}{m^2}}}$ the lagrangian is given by the ( inverse ) legendre transform $ l = p\dot{x}-h = pv-e = \frac{-m^2+\frac{p^4}{m^2}}{\sqrt{p^2+m^2+ \frac{p^4}{m^2}}}$ the only remaining problem is to express the lagrangian as a function of $v$ . now , $p$ is given implicitly in terms of $v$ in the hamilton equation of motion . in the special case at hand , this equation can be solved in a closed form because it is of the third degree in $p^2$: $ 4 \frac{ ( p^2 ) ^3}{m^4} + 4 \frac{ ( p^2 ) ^2}{m^2} +p^2 ( 1-v^2 ) - m^2v^2=0$ now , please see the following wikipedia page for the exact form of the solution , which is cumbersome and will not be given here . moreover , one must be careful in the selection of the correct root , which will depend on the values of $\frac{m^2}{m^2}$ $\frac{p^2}{m^2}$ . in the following an approximate solution will be given for the case $p^2\ll m^2$ and $m^2\ll m^2$ . in this case we substitute : $ p^2 = m^2\frac{v^2}{1-v^2} + \frac{\delta }{m^2}$ please , notice that the first term is the exact solution of the case $m^2 \rightarrow \infty$ . substituting this solution in the hamilton equation and keeping only the leading terms in $\frac{1}{m^2}$ , we get the approximate solution : $ p^2 = m^2\frac{v^2}{1-v^2} + \frac{m^4 v^4}{m^2 ( 1-v^2 ) 3}$ which can be substituted in the lagrangian
the uncertainty principle applies to any quantum system , and is way more general than just single particle examples . it is defined for any pair of operators ( physical quantities ) $a$ and $b$ , with the system in a state $|\psi\rangle$ $$ \delta a \ ; \delta b \geq \frac{\hbar}{2} \langle \psi| [ a , b ] |\psi \rangle$$ note : the constant factor ( $\frac{1}{2}$ here ) varies in different derivations , depending on how exactly you define $\delta a$ and $\delta b$ , but the essence is the same . in the case of simple quantum systems , you could take $a$ to be the position operator and $b$ to be the momentum operator . in your case , it seems like you would like to consider the whole nucleus as an effective particle and apply these operators on it is wavefunction/state . sure , you could do that , and you will get an uncertainty relation from that .
first , let me restore the complex conjugation that was omitted without a good reason : $$ \rho_{\rm pure}=|\psi \rangle \langle\psi |= \big ( \begin{matrix} |\alpha|^2 and \alpha^* \beta \\ \alpha \beta^* and |\beta|^2 \end{matrix} \big ) $$ now , let us use a prettier ( inverse ) ordering of the tensor factors for the bra vectors . you meant : $$ \rho=|\psi \rangle \langle\psi | \rightarrow |\psi \rangle |r\rangle \langle r | \langle \psi| $$ $$ =|\alpha|^2 |0 \rangle |r_0\rangle \langle r_0 | \langle 0|+\alpha\beta^* |0 \rangle |r_0\rangle \langle r_1 | \langle 1|+\\ +\alpha^*\beta|1 \rangle |r_1\rangle \langle r_0 | \langle 0| +|\beta|^2 |1 \rangle |r_1\rangle \langle r_1 | \langle 1| $$ i think that the only extra step , key step of decoherence , you want to be shown is the partial trace of $\rho$ over the $r_0/r_1$ degree of freedom . we have $$\mathop{\rm tr}^{r_0 , r_1} \rho = |\alpha|^2 |0\rangle \langle 0| +|\beta|^2 |1\rangle \langle 1 | = \pmatrix{ |\alpha|^2 and 0\\ 0 and |\beta|^2} $$ that is it . note that the mixed terms did not contribute anything to the partial trace because $\langle r_0|r_1\rangle=0$ and similarly for its complex conjugate . the main result is that this reduced density matrix , a partial trace , after decoherence has vanishing off-diagonal entries , unlike the density matrix for the pure state $\rho_{\rm pure}$ that we started with . in more detailed calculations of decoherence , we usually take into account the fact that $|r_0\rangle$ and $|r_1\rangle$ that the environmental degrees of freedom evolve into just a moment later are not exactly orthogonal to one another . that means that the off-diagonal elements get reduced but they do not quite vanish instantly . however , the inner product is decreasing faster than exponentially as the same information is being imprinted and copied into additional degrees of freedom of the environment . by an exponentially growing avalanche , $\exp ( ct ) $ of qubits get modified according to the initial decohering bits . each of these environmental degrees of freedom or qubits contributes a factor of order $i\ll 1$ from the inner product to the off-diagonal entries so the off-diagonal entries go like $$ \rho_{12}\sim i^{\exp ( ct ) }=\exp ( -b\exp ( ct ) ) $$ where $b=\ln ( 1/i ) $ . to make the off-diagonal entries of the density matrix vanish or almost vanish after some time , the entanglement with the environmental degrees of freedom is essential . if the two subsystems were not entangled , in other words , if the total pure state were a tensor product $|a\rangle\otimes |r\rangle$ , the tracing over the $r$ degrees of freedom would give you the pure density matrix $|a\rangle\langle a|$ back : the system $r$ would have no effect on the system $a$ because of the lack of entanglement ( lack of correlation ) . you may read about decoherence e.g. at pages 9-16 here : http://www.karlin.mff.cuni.cz/~motl/entan-interpret.pdf
resistivity is the relevant parameter for three-dimensional materials . sheet resistance ( less commonly called " sheet resistivity" ) is the relevant parameter for two-dimensional materials , and its inverse is called " sheet conductance " or " sheet conductivity " . in the novoselov paper you cited , they talk about sheet resistance and sheet conductance . please forgive them for the bad habit of using the words " resistivity " and " conductivity " when they mean " sheet resistance " and " sheet conductance " respectively ! ( when they first define it they do actually use the word " sheet " , but then they start leaving it out to be concise . ) the units of sheet resistance are ohms , and sheet conductance is siemens . ( sometimes the unit of sheet resistance is written $\omega/\box$ , " ohms per square " , so that you do not mistakenly think it is an ordinary resistance . ) for most purposes the sheet conductance or sheet resistance of graphene is a far more relevant parameter than the " bulk " conductivity or resistivity . but if you really have to convert one to the other , you would multiply or divide by the thickness of graphene . [ the thickness of graphene is not very well defined . . . therefore the bulk conductivity and resistivity are not very well defined either . the sheet resistance , on the other hand , is perfectly well defined . ]
if you turn a glass upside down and put it in a sink full of water you will find the air stays in the glass and stops the water filling the glass . this is exactly how the undersea lab works . the lab is sealed apart from the opening in the bottom , so the air inside it can not escape , and the trapped air keeps the water out . the air has to be at the same pressure as the water ( otherwise the water pressure would compress it and allow some water in ) so the pressure is higher than on the surface . that is why the chaps in the lab have to open a valve on the barrels containing the towels before they can open them . you have to be very sure there are no leaks in the undersea lab . any leak would allow the air in the lab to leak out and bubble to the surface , and sea water would rush in to replace the leaked air .
i think it is pretty obviously in your last line . so what are the results of the commutator $ [ p_r , p_\theta ] $ and $ [ p_\theta , p_\theta ] $ ? note that $p_\theta$ is not commute with $\theta$ . then you should get the answer .
no , it would not violate the principle of special relativity , which proscribes information traveling faster than the speed of light . let 's call the farthest object we can see x . let 's call x 's neighbor on the far side from us y . the only way for us to get information about y is for information to travel to x ( at or below the speed of light ) , which would then influence x 's behavior . then , information about x 's behavior would have to travel to us ( at or below the speed of light ) . so , nowhere is information traveling faster than light . the only information we can get from x about y is already " out of date"* enough that special relativity is not violated . *the technical term relativists use is " retarded , " but be careful how you use it in everyday conversation . edit : we can make a distinction between the theoretically observable universe according to relativistic considerations , which is what i was talking about above , and the universe observable with present technology and practical limitations . getting sneaky like inferring about y from x actually does increase the latter meaning of observable universe .
there is an excellent talk by lawrence krauss on precisely this subject . i can not recommend watching it highly enough , you should start watching it before even reading the remainder of this post . in summary , we can model the matter just after the big bang at the time we see the cosmic microwave background and determine the characteristic distance scales of the " lumpiness " of the universe at that point . we can view the lumpiness of the universe then by observing the cosmic microwave background radiation at high resolution . now we have something that we can compare the expected visual size of to the apparent visual size , giving us information about the shape of the universe in between .
the double image is almost certainly because you have the axes misaligned with your eyes . the three adjustments you can make are ( usually ) : physical separation of the two lens assemblies main focus supplementary focus for one lens ( sometimes you have totally independent focus , but the above is more common ) the focus options will make things blurry , but having the barrels too wide apart , or too close together can mean your eyes can not easily resolve to a single image . try adjusting the separation first . if that does not help , it is possible you have a badly aligned pair of binoculars . . .
the mass will play the role in the relaxation time to go from a ballistic regime in the langevin equation to an overdamped regime where only diffusion matters . the bigger the mass , the higher the inertia and therefore the longer the time it takes to reach the overdamped regime . once the overdamped regime is reached or , to phrase it differently , if your time window allows you to only see the overdamped regime in both cases then you will see no difference between the two .
i am no expert on aerodynamics but according to wikipedia a sears–haack body has the lowest theoretical wave drag , where wave drag occurs when moving at transonic and supersonic speeds due to the presence of shock waves . however you are saying that your object would not move faster than $20 ms^{-1}$ , which is far below the speed of sound ( $mach 1\approx 340 ms^{-1}$ ) assuming this would be in earths atmosphere near sea-level . and according to another wikipedia page about nose cone designs : for aircraft and rockets , below mach . 8 , the nose pressure drag is essentially zero for all shapes . the major significant factor is friction drag , which is largely dependent upon the wetted area , the surface smoothness of that area , and the presence of any discontinuities in the shape . for example , in strictly subsonic rockets a short , blunt , smooth elliptical shape is usually best . so elliptical cone would be a good choice , however it is not given which length would give the lowest drag . i would suspect that the length would be larger than the radius , because the drag coefficient ( for $f_d\propto v^2$ ) for a half-sphere ( elliptical cone with length equal to the radius ) is higher that a elliptical cone with a slightly longer length . edit : after a bit of searching i did found some experimental results which are in the region of your maximum speed ( $39.2 mph\approx 17.4 ms^{-1}$ ) . the results from this experiment showed that a long elliptical nose cone had the least drag , which had a length to diameter ratio of approximately 3 . the experiment also contained a shorter elliptical nose cone with a ratio of approximately 1.4 . i would not be able to say that a ratio of 3 would be optimal , but it should at least be bigger than 1.4 .
i would like to know how the string deforms over time as it is pulled from that point ( orange ) . does the end closest to the pulling force move first and straighten up , wrapping around the object ' a ' early on , or does the whole string change shape continuously ? for a better understanding , take a look at this figure ( i have modified the diagram in order to explain things in a better way ) when the string is pulled , the coil of wire ( initially slack ) as shown in region $b$ begins to uncoil . what you will observe is that the portion of the wire in region $a$ begins to move only after the coil of wire in region $b$ gets completely uncoiled ( it remains slack until then ) . try it out with a thread or a chain . this is to show you that the portion of the wire close to the pulling force will move first followed by the other portions of the wire .
all points in the observable universe are " connected " in the sense that they can be acted upon by forces that have an infinite range ( gravity and electromagnetism ) . however , points that are outside of our cosmological horizon ( due to the expansion of the universe ) are no longer causally connected with points in our local vicinity , since they are receding from us faster than light . the same is true of points that are inside the event horizon of a black hole .
in linear electrodynamics ( i.e. . low intensities ) , the dielectric constant and refractive index remain unchanged . a different thing is nonlinear optics ( applying to ed in general as well ) , for more theory see e.g. here . but this happens only for materials with strong non-linear parameters ( usually $\chi^{ ( 2 ) }$ or $\chi^{ ( 3 ) }$ ) and for rather high field intensities . this usually yields effect such as the second harmonic generation , but there is also the effect of self-focusing among others , which stems from the dependence of the refractive index on the field amplitude .
when considering a big enough chunk of matter , as in your experiment , you are dealing with it is macroscopic observed effects , where granular effects of single electrons are smoothed out . so if you add $n$ electrons to the electron sea of the metal the observed effect will be a uniform increase of $-ne\over s$ in the surface charge density of the metal .
you need the clebsch-gordan decomposition , at least in the case $n = 3$ . the reason that we decompose a rank $2$ tensor in the way you describe is that $$\mathbf{1} \otimes \mathbf{1} = \mathbf{2}\oplus \mathbf{1} \oplus \mathbf{0} $$ where the bold numbers denote spin representations . here 's a bit more detail . in quantum physics we are really interested in representations of the lie algebra of $so ( n ) $ namely $\mathfrak{so} ( n ) $ . the most useful case for physical purposes is $n = 3$ , where there is an isomorphism $$\mathfrak{so} ( 3 ) = \mathfrak{su} ( 2 ) $$ the clebsch-gordon result comes from the structure of representations for $\mathfrak{su} ( 2 ) $ . in brief , $\mathfrak{su} ( 2 ) $ has irreps $\mathbf{n}$ for each half-integer $n$ . each irrep has $2n+1$ characteristic labels called weights , evenly spaces between $-n$ and $n$ . physically one interprets these as the component $j_3$ of spin . when you take a tensor product of irreps the weights add up , to give you weights for the tensor product representation . a theorem says that this decomposes into the direct sum of irreps in the only way that uses up all these weights . in case that all sounds absurd , let 's do a concrete example . the tensors you mention are elements of tensor products of the vector representation of $\mathfrak{su} ( 2 ) $ typically denoted $\mathbf{1}$ . we want to prove the result above that $$\mathbf{1} \otimes \mathbf{1} = \mathbf{2}\oplus \mathbf{1} \oplus \mathbf{0} $$ well $\mathbf{1}$ has weights $+1,0 , -1$ so the tensor product will have weights $$-2 , -1 , -1,0,0,0 , +1 , +1 , +2$$ which are all possible ways of adding the weights for $\mathbf{1}$ . now rewrite this list suggestively $$-2 , -1,0 , +1 , +2 , \ \ \ \ \ \ -1,0 , +1 , \ \ \ \ \ \ 0$$ these are just the weights for a $\mathbf{2}$ plus the weights for a $\mathbf{1}$ plus the weights for a $\mathbf{0}$ . now it is not hard to identify $\mathbf{2}$ with the traceless symmetric matrices , $\mathbf{1}$ with the antisymmetric ones and $\mathbf{0}$ with the trace , checking that these all transform correctly under the relevant representations . as an exercise you now have all the tools to prove that $$\mathbf{1} \otimes \mathbf{1} \otimes \mathbf{1} = \mathbf{3}\oplus \mathbf{2} \oplus \mathbf{1} \oplus \mathbf{0}$$ can you identify what these are , in terms of decomposing the rank $3$ tensor ? hint : there exist totally symmetric tracefree tensors , totally antisymmetric tensors , a trace term , and tensors of mixed symmetry . here 's a good reference for the lie algebra stuff . let me know if you need any further details ! p.s. i do not know what one can do for general $n\neq 3$ . the clebsch-gordan niceness is a specific property of $\mathfrak{su} ( 2 ) $ so i expect it becomes quite messy . perhaps somebody else has some expertise here ?
if you place a camera you will not see any interference pattern . so , the answer is yes . the camera will cause the wavefunction to " collapse " . but i do not like the term " wavefunction collapse " , because wavefunction is not really any physical object . what the camera will basically do is cause an abrupt change in the state of the particle . here is the defintion of measurement from landau 's book by measurement , in quantum mechanics , we understand any process of interaction between classical and quantum objects , occurring apart from and independently of any observer . the importance of the concept of measurement in quantum mechanics was elucidated by n . bohr . we have defined " apparatus " as a physical object which is governed , with sufficient accuracy , by classical mechanics . such , for instance , is a body of large enough mass . however , it must not be supposed that apparatus is necessarily macroscopic . under certain conditions , the part of apparatus may also be taken by an object which is microscopic , since the idea of " with sufficient accuracy " depends on the actual problem proposed .
yep , you are right , we can only talk about the phase of matter if we have a collection of molecules . it would not make any sense to talk about a single molecule as being a solid , liquid , or gas because what matters is how the kinetic energy of the molecules ( related to temperature ) compares to the **inter**molecular bonding energy . solid : ke &lt ; &lt ; be liquid : ke &lt ; be gas : ke > > be i think the context that this sentence appeared in adds further clarity , so i have included it below . here 's the full paragraph along with fig . 3: the key distinction between the three classes of molecules is summarized in the bottom portion of fig . 3 . small molecules may appear as solid , liquid , and gaseous phases without decomposition , while rigid macromolecules keep their bonding to nearest neighbors ( their molecular integrity ) only in the solid state . due to internal rotation , flexible macromolecules can attain sufficient intramolecular disorder to melt ( or dissolve ) without breaking strong bonds . this property is at the root of many of the useful properties of polymers ( plastic and rubber–elastic behavior in addition to high strength , light weight , and low melting temperatures ) . the three classes of molecules are thus very distinct in their phase behavior . no macromolecule can be evaporated thermally without decomposition . if one tries to place flexible macromolecules into the gas phase by evaporation of small solvent molecules from a dispersion of droplets of a solution with only one macromolecule per droplet , the macromolecules become microphase particles and collect at the bottom of the container [ 9 ] .
straight from the horse 's mouth : source : bureau international des poids et mesures ( search for " dimensionless " for all guidelines . ) the international bureau of weights and measures ( french : bureau international des poids et mesures ) , is an international standards organisation , one of three such organisations established to maintain the international system of units ( si ) under the terms of the metre convention ( convention du mètre ) . the organisation is usually referred to by its french initialism , bipm . wikipedia
quantum computers can process information in a different way then classical computers the main isuues is how to use classical input to " convince " the qbit to process what we are asking and then give an answer that we can use in a positive way . research is progressing towards a quantum level interface that can account for decoherence between the information we give and a understandable answer coming out . in other words the language barrier between classical people and quantum interpretation . there is no technical limitation to qc 's at this time only a limit of our ability to minipulate qbit s the do what we want . http://www.cra.org/ccc/docs/init/quantum_computing.pdf is a pdf document link to the currently imposed limitations of quantum computers .
so i wonder how do we explain the levorotatory and the dextrorotatory in atom level pov ? it is not easy to predict whether solution of given molecule will be levorotatory or dextrorotatory ; this will depend in a complicated way on the structure of the molecule and also on the frequency of light in question . there are models to calculate indices of refraction and absorption for left and right-polarized light for given frequency and molecular structure , but they are quite involved to explain here . the important idea in these is the observation that most often optically active molecules have three-dimensional structure of nuclei that is not congruent with its mirror image ( we cannot take molecule and by rotating and translating it superimpose it on its mirror image ) . optically active molecules have more spatially separated parts ( at least 4 different atoms to allow for this mirror asymmetry ) and an important role in the calculations of indices of refraction has the way these parts interact .
vacuum in outer space may contain cosmic dust which is composed of a few molecules or atoms left by by stars or other cosmic objects . in fact there are different types of dust such as star dust or intergalactic dust . there is aroud 1 atom per cubic centimeter . now if you are talking about empty space or vacuum in quantum mechanics , vacuum is defined as the state with the lowest possible energy . in fact vacuum is not that empty , since due to heisenberg principle , virtual particles are created and destroyed in pairs .
yes , it will be harmful , because the isotope will travel through your bloodstream and deliver radiation damage to cells all over your body . if you want to know how harmful it is , check the recent case of alexander litvinenko , who was assassinated with polonium-210 .
it was not a black hole because the density was not sufficiently high . the density was lower than what is needed for a black hole because the volume was larger . the volume was larger because the atoms ( mostly hydrogen ) were kept away from each other by the pressure produced by the fusion processes . once the fusion processes stop , this source of repulsion between the atoms disappears , the volume shrinks , the density goes up , and the black hole threshold may be surpassed .
what is a local lagrangian density ? a classical field theory on minkowski space $\mathbb r^{d , 1}$ is specified by a space $\mathcal c$ of field configurations $\phi:\mathbb r^{d , 1}\to t$ , and an action functional $s:\mathcal c\to\mathbb r$ . the set $t$ is called the target space of the theory , and is often a vector space . if there exists a function $l:\mathcal c\times\mathbb r\to \mathbb r$ for which \begin{align} s [ \phi ] = \int_{\mathbb r} dt\ , l [ \phi ] ( t ) , \end{align} then we call $l$ a lagrangian for the theory . if , further , there exists a function $\tilde l$ such that \begin{align} l [ \phi ] ( t ) = \int_{\mathbb r^d} d^d\mathbf x \ , \tilde l [ \phi ] ( t , \mathbf x ) \end{align} then we call $\tilde l$ a langrangian density for the theory . finally , if there exists a positive integer $n$ and a function $\mathscr l$ such that \begin{align} \tilde l [ \phi ] ( t , \mathbf x ) = \mathscr l ( t , \mathbf x , \phi ( t , \mathbf x ) , \partial\phi ( t , \mathbf x ) , \dots , \partial^n\phi ( t , \mathbf x ) ) \end{align} then we say that the lagrangian density is local . in other words , the lagrangian density is local provided its value at a given spacetime point depends only on that point , the value of the field at that point , and a finite number of its derivatives at that same point . an example of a non-local lagrangian density . consider $t = \mathbb r$ , namely a theory of a single real scalar field . let $\mathbf a\in\mathbb r^d$ be given , and define a lagrangian density by \begin{align} \tilde l [ \phi ] ( t , \mathbf x ) = \phi ( t , \mathbf x ) + \phi ( t , \mathbf x+\mathbf a ) . \end{align} this lagrangian density is not local because the value of the lagrangian at a given point $ ( t , \mathbf x ) $ depends on the value of the field at that point and on the value of the field at the point $ ( t , \mathbf x+\mathbf a ) $ . if we were to taylor expand the second term $\phi ( t , \mathbf a ) $ about $\mathbf x$ , then we would see that the lagrangian density depends on an infinite number of derivatives of the field , thus violating the definition of a local lagrangian density . what is the issue with theories with non-local lagrangian densities ? i am no expert on this , so i will divert to another user . i will say , however , that people do study theories with non-local lagrangian densities in practice , so there is nothing a priori " wrong " with them , but they might generically exhibit some pathology that you might prefer not to have . perhaps most relevant , though , if you are taking qft from a high energy theorist for example , is that the lagrangian density of the standard model is local , so there is no need to consider non-local beasts if one is studying the standard model .
1 . x-ray diffraction takes pictures of fourier space as briefly described on pg . 34 of " introduction to solid state physics " 7th edition by kittel , the scattering amplitude for an arbitrarily-shaped object is $$f ( \mathbf{k}_i , \mathbf{k}_o ) =\widehat{n} ( \mathbf{k}_i-\mathbf{k}_o ) $$ where $\mathbf{k}_i$ is the incident wavevector , $\mathbf{k}_o$ is the outgoing ( or scattered ) wavevector , and $\widehat{n}$ is the fourier transform of the material 's scattering density $n$ . since it is often electrons which scatter light , $n$ is often assumed to be the electron density as a function of location in the material . this is a surprisingly simple equation ; to illustrate it is visual meaning , imagine an object is placed inside a hollow sphere whose walls are lined with photographic paper , and through a small hole the object is bombarded with x-rays of fixed wavevector $\mathbf{k}_i$ , which are scattered by the object and strike the interior of the imaging sphere , forming an image . what is this image ? imagine a bubble of radius $|\mathbf{k}_i|$ centered at $\mathbf{k}_i$ in fourier space . the image formed on the photographic paper is the image of $\widehat{n}$ on the surface of that bubble , a fact which is simple to deduce by examining the set of points of the form $\mathbf{k}_i-\mathbf{k}_o$ for a fixed value of $\mathbf{k}_i$ , and noting that $|\mathbf{k}_i|=|\mathbf{k}_o|$ . that is what x-ray diffraction examines ; it literally takes a bubble-shaped slice of an object in fourier space . this bubble is sometimes call the " ewald bubble " , and it is what diffraction takes a picture of . 2 . fourier space of a crystal now , let 's insert the fact that we are looking at a crystal . a very simple approximation of a crystal is a 3d dirac comb ( ie , a dirac delta placed at each unit cell ) , whereupon $$n ( \mathbf{r} ) \approx \mbox{diraccomb} ( \mathbf{ar} ) $$ where $\mathbf{a}$ is the inverse of the $3\times3$ matrix whose columns are the 3 lattice basis vectors for the crystal . one then fourier transforms to obtain $$\widehat{n} ( \mathbf{k} ) \approx\mbox{det} ( a ) ^{-1}\mbox{diraccomb}\left ( \frac{\mathbf{a}^{-1}\mathbf{k}}{2\pi}\right ) $$ which essentially tells you that the fourier transform of the lattice $n$ is also another lattice . this lattice , residing in fourier space , is often referred to as the " reciprocal lattice " of a crystal . 3 . crystal diffraction now let 's combine the previous two results . if a reciprocal lattice point of $\widehat{n}$ happens to reside on the surface of the ewald bubble , then light will be diffracted in that direction in real space . but how do we image the rest of reciprocal space , not just the points that happen to lie on that one bubble-shaped slice ? suppose we rotate the crystal through euler angles $\alpha , \beta , \gamma$ . then $n ( \mathbf{r} ) $ will become $n ( \mathbf{r} ( \alpha , \beta , \gamma ) \cdot\mathbf{r} ) $ where $\mathbf{r} ( \alpha , \beta , \gamma ) $ is the rotation matrix for the crystal rotation . since $$\mathbf{r} ( \alpha , \beta , \gamma ) ^{-1}=\mathbf{r} ( -\gamma , -\beta , -\alpha ) , $$ we see that $\widehat{n} ( \mathbf{r} ) $ becomes $\widehat{n} ( \mathbf{r} ( -\gamma , -\beta , -\alpha ) \cdot\mathbf{r} ) $ ( rotations are unitary so the determinant is 1 ) , and hence rotation of a crystal in real space correspond to a reversed rotation of the crystal 's reciprocal space . thus , by rotating the crystal and imaging it after each rotation , we can sweep out a spherical region of radius $2|\mathbf{k}|$ in reciprocal space . visually , we are rotating the ewald bubble , whose surface is attached to the origin , to sweep out a spherical region whose radius is the diameter of the imaging bubble ( which is $2|\mathbf{k}|$ ) . 4 . powder diffraction with those preliminaries out of the way , powder diffraction is quite simple . a powder is a large number of very small crystals oriented in random directions . if there are a large number of crystals all oriented randomly ( isotropic fine powder ) then we can average over all orientations to get $$\widehat{n}_{avg} ( \mathbf{k} ) \propto\int_0^{2\pi}d\alpha\int_0^{\pi}d\beta\mbox{sin} ( \beta ) \int_0^{2\pi}d\gamma\widehat{n} ( \mathbf{r} ( -\gamma , -\beta , -\alpha ) \cdot\mathbf{k} ) $$ but you do not need to worry about integrating it , because it is visually obvious that each delta function located at a reciprocal lattice point $\mathbf{g}$ will be " rotationally smeared " out to form a series of concentric bubbles of radius $|\mathbf{g}|$ centered at the origin . where these concentric bubbles intersect the ewald bubble , diffraction will occur in that direction . and it is a simple fact of geometry that when one bubble intersects another , the intersection region is a circular ring common to both . as a result , there will be ring-shaped regions on the ewald bubble where constructive interference can occur , and thus , the sample will emit cone-shaped beams of light . and that is how the cone-shaped beams of light in the picture above come to fruition .
choose $\alpha$ as the generalized coordinate , so $y_\text{com}=\frac{1}{2}l \sin\alpha$ , and $x_b=l\cos\alpha$ . then $\delta y_\text{com}=\frac{1}{2}l \cos\alpha \ , \delta\alpha$ , and $\delta x_b=-l\sin\alpha \ , \delta\alpha$ . substitute into the equation .
this paper claims that consistent histories is an extension of the feynman path integral approach .
the total voltage difference across the resistors ( $v_3$ ) is , by design , a constant 5.4 volts 1 ( because it is being supplied by a power supply that pretty well approximates a constant voltage source ) . this total voltage difference must be dropped across the two resistors $r_1$ and $r_2$ in series : that is , $$v_1 + v_2 = v_3 . $$ as the resistors are in series , and there are no other paths the current might take , 2 the same current must flow across both resistors : $$i_1 = i_2 . $$ by ohm 's law , the current across a resistor equals the voltage divided by the resistance : $$i = \frac{v}{r} . $$ combining these equations , we see that $$\frac{v_1}{r_1} = i_1 = i_2 = \frac{v_2}{r_2} , $$ which we can rearrange to get $$\frac{v_1}{v_2} = \frac{r_1}{r_2} . $$ that is to say , the ratio $v_1 / v_2$ of the voltage drops across the resistors is equal to the ratio $r_1 / r_2$ of their resistance . since the total voltage $v_1 + v_2$ dropped across the resistors is fixed , this means that , when $v_1$ goes up , $v_2$ must go down , and vice versa . in particular , this means that , when you keep $r_1$ fixed and decrease $r_2$ , you are increasing the ratio $r_1 / r_2$ , and thus $v_1 / v_2$ . since the sum of $v_1$ and $v_2$ is constant , increasing the ratio means that $v_1$ must increase and $v_2$ must decrease by the same amount . for example , if , as in your first experiment , $r_1$ equals $r_2$ , then $v_1$ also equals $v_2$ , and thus both must be half of the total voltage drop $v_3 = v_1 + v_2$ . 3 similarly , if $r_1$ is twice $r_2$ , as in your second experiment , then $v_1$ will also have to be twice $v_2$ . thus , $v_1$ will be two thirds of the total voltage drop $v_3$ , while $v_2$ will be one third of $v_3$ . note that the individual values of $r_1$ and $r_2$ do not matter , only their ratio does : you had get the same results with $r_1 = 100\ ; \omega$ and $r_2 = 50\ ; \omega$ as with $r_1 = 10\ ; \omega$ and $r_2 = 5\ ; \omega$ . of course , the current passing through the resistors ( and the heat dissipated by them ) would be ten times greater for the second case as for the first , but this would not affect the voltages in any way . 4 1 ) you say in the text that it is supposed to be 6 volts , but the measurements say 5.4 , so i am going with that . see note 3 below . 2 ) the voltage sensors do leak some tiny bit of current , but they are designed to make this leakage current as small as possible , so we can normally safely neglect it . 3 ) the difference between the calculated values and your experimental results is presumably due to experimental inaccuracies , e.g. in the readings of your voltmeters or in the actual resistance values of your resistors . 4 ) that is , it would not affect the voltages as long as the experimental assumptions remained valid . if you reduced the resistances too far , either the power supply voltage would start to drop , or the increased current would cause your components to overheat . conversely , if you increased the resistor values too far , at some point the minuscule amounts of current leaking though the voltage meters would start to affect the results measurably .
the answer is no , as you do not know the moment of inertia , and changes te situation a lot : suppose the distribution of mass is that such that all mass is concentrated in the extreme points , and another case in which is uniformly distributed along the longitude : both cases generate the same center of mass and distances , but the torque will be different as mass distribution is different .
because of the practical considerations , we can ignore the relativistic calculations for now . the acceleration of a spacecraft depends on its mass ( including unburned fuel it is carrying ) and the engine it uses . and the amount of time it can accelerate depends on how much fuel it has . some of the total consequences of this can be found in the tsiolkovsky rocket equation for modern rockets , accelerations near $1g$ require large amounts of fuel . the fuel capacity at such accelerations can be measured in minutes , not years . the fuel is exhausted long before very high speeds are reached . you can use the rocket equation to find out what it would take for a rocket to reach a particular speed ( or more properly how much change in speed it can undergo ) . if we take one of our most efficient engines ( an ion thruster with an isp of $10000s$ ) and see how much fuel is required to reach 0.1c : $$\delta v = i_{sp}\text{ }g_0 \text{ } ln \frac{m0}{m1}$$ $$ln \frac{m0}{m1} = \frac{\delta v}{i_{sp}\text{ }g_0}$$ $$ln \frac{m0}{m1} = \frac{3.0\times 10^{7} m/s}{10000s ( 9.81m/s^2 ) }$$ $$ln \frac{m0}{m1} = 305.6$$ $$\frac{m0}{m1} = 5\times 10^{132}$$ so with a fantanstic engine , the craft would have to have a fuel mass that is over one hundred orders of magnitude greater than the non-fuel mass ( the engines , the structure , the equipment , etc . ) that is simply not possible .
notation $w^{-} , w^{+}$ may confuse in a sense that it may seem that here are two different particles which are not connected by charge conjugation . but of course , $w^{+}$ is only $ ( w^{-} ) ^{\dagger}$ , so it is an antiparticle to $w^{-}$ . so term $ ( w^{-} \cdot w^{+} ) $ is simple $|w|^{2}$ ( which is standard for the mass-term ) , and , of course , both of particle and antiparticle have the equal masses . also before making substitution $$ \tag 1 w^{\pm}_{\mu} = \frac{1}{\sqrt{2}} ( w_{\mu}^{1} \mp iw_{\mu}^{2} ) $$ you can see that both of fields $w^{1} , w^{2}$ have equal masses . so of course that their linear combinations $ ( 1 ) $ also have equal masses .
space_cadet mentioned already work about deriving spacetime as a smooth lorentzian manifold from more " fundamental " concepts , there are a lot of others -like causal sets , but the motivation for the question was : the reason for my interest in this regards one of the mysteries of quantum mechanics , that of quantum entanglement and action at distance . i wondered whether , if space is imagined as having a topology that arises from a notion of neighbourhood at a fine level , then quantum entanglement might be a result of a ' short circuit ' in the connection lattice . i am not convinced that such an explanation is possible or warranted , the reason for this is the reeh-schlieder theorem from quantum field theory ( i write " not convinced " because there is some subjectivity allowed , because the following paragraph describes an aspect of axiomatic quantum field theory which may become obsolete in the future with the development of a more complete theory ) : it describes " action at a distance " in a mathematically precise way . according to the reeh-schlieder theorem there are correlations in the vacuum state between measurements at an arbitrary distance . the point is : the proof of the reeh-schlieder theorem is independent of any axiom describing causality , showing that quantum entanglement effects do not violate einstein causality , and do not depend on the precise notion of causality . therefore a change in spacetime topology in order to explain quantum entanglement effects will not work . discussions of the notion of quantum entanglement often conflate the notion of entanglement as " an action at a distance " and einstein causality - these are two different things , and the first does not violate the second .
actually the terms you interesting in is vanishing after integration over full space-time domain . it is easy to see if you use fourier transform : $$ \mathbf{b}_{t , \mathbf{x}}=\int\frac{d^{4}k}{\left ( 2\pi\right ) ^{4} }\ , \mathbf{b}_{\omega , \mathbf{k}}\ , e^{-i ( \omega t-\mathbf{k}\cdot\mathbf{x} ) } . $$ such that $$ \mathbf{b}_{\omega , \mathbf{k}}^{\star}=\mathbf{b}_{-\omega , -\mathbf{k}} $$ thus your term takes the form : \begin{align*} i and =\int dt\int d^{3}x\ , \mathbf{\dot{b}}\cdot\left ( \nabla^{2}\right ) ^{-1}\left ( \mathbf{\nabla}\times\mathbf{b}\right ) =\int d^{4} k\ , \frac{\omega}{\mathbf{k}^{2}}\ , \mathbf{b}_{-\omega , -\mathbf{k}} \cdot\left [ \mathbf{k}\times\mathbf{b}_{\omega , \mathbf{k}}\right ] =\\ and =\int d^{4}k\ , \frac{\omega}{\mathbf{k}^{2}}\ , \mathbf{b}_{\omega , \mathbf{k} }^{\star}\cdot\left [ \mathbf{k}\times\mathbf{b}_{\omega , \mathbf{k}}\right ] . \end{align*} since $i$ is real , i.e. , $i^{\star}=i$ , we find : $$ i=i^{\star}=\int d^{4}k\ , \frac{\omega}{\mathbf{k}^{2}}\ , \mathbf{b} _{\omega , \mathbf{k}}\cdot\left [ \mathbf{k}\times\mathbf{b}_{\omega , \mathbf{k}}^{\star}\right ] =-\int d^{4}k\ , \frac{\omega}{\mathbf{k}^{2} }\ , \mathbf{b}_{\omega , \mathbf{k}}^{\star}\cdot\left [ \mathbf{k} \times\mathbf{b}_{\omega , \mathbf{k}}\right ] =-i , $$ hence $i=0$ . it is also easy to show that $i=0$ without using fourier transform . as the first step , one can utilize the following property : $$ \int d^{3}x\ , a\left ( \nabla^{2}\right ) ^{-1}b=\int d^{3}x\ , b\left ( \nabla^{2}\right ) ^{-1}a , $$ thus $$ i=\int dt\int d^{3}x\ , \mathbf{\dot{b}}\cdot\left ( \nabla^{2}\right ) ^{-1}\left ( \mathbf{\nabla}\times\mathbf{b}\right ) =\int dt\int d^{3}x\left ( \mathbf{\nabla}\times\mathbf{b}\right ) \cdot\left [ \left ( \nabla^{2}\right ) ^{-1}\mathbf{\dot{b}}\right ] . $$ secondly , for the sake of convenience , one can use component notations : $$ i=\epsilon^{ijk}\int dt\int d^{3}x\nabla_{i}b_{j}\left ( \nabla^{2}\right ) ^{-1}\dot{b}_{k}=\epsilon^{ijk}\int dt\int d^{3}xb_{i}\left ( \nabla ^{2}\right ) ^{-1}\nabla_{j}\dot{b}_{k} , $$ where i use integration by parts ( ibp ) with respect to spatial dimensions and change the order of indices in $\epsilon^{ijk}$ . then i would like to use ibp with respect to time : $$ i=-\epsilon^{ijk}\int dt\int d^{3}x\dot{b}_{i}\left ( \nabla^{2}\right ) ^{-1}\nabla_{j}b_{k}=-i , $$ hence $i=0$ .
as far as i can see this is just going to be the uncertainty of the mean of your dataset . to determine this you must know the uncertainty in the individual data points . for the simple case where you can consider the uncertainty in the data to be constant for all points then the the uncertainty of the mean is $$ u_c=\frac{u}{\sqrt n}$$ for the case where the errors are different then $$ u_c=\frac{\sqrt{\sum u_i^2}}{n} $$ the general formula for combining uncertainties is $$ u_c^2 = \sum \left ( \frac{\partial f}{\partial x_i}\right ) ^2 u_i^2 $$ however , i would also point out there are several flaws with your approach . firstly , if the peak is asymmetric the centroid will not be aligned with the peak position . similarly , the method treats all points with equal weighting when determining peak position . therefore , the result can be effected by noise far from the peak which likely has no relevance to the actual uncertainty of the peak position . see this http://terpconnect.umd.edu/~toh/spectrum/peakfindingandmeasurement.htm for a better approach to peak finding .
since $$\cos\left ( x-\frac{\pi}{2}\right ) =\sin x , $$ using $\cos$ or $\sin$ does not matter , it depends on the choice of initial conditions . in addition , in general , there will be a initial phase $\phi$ , so sinusoidal wave is written like $$ y ( x , t ) =a \cos ( kx-\omega t+\phi ) . $$
if you read wikipedia page about corium , they say that critical mass can be achieved locally . but if you are concerned about a critical mass allowing a nuclear explosion , the difficulty in nuclear weapon design , as told here , is to achieve the criticality fast enough . if you do not achieve criticality fast enough , your material heats and its interaction with neutrons decreases , slowing the chain reaction down . and that is with pure ²³⁵u . so basically what happens if criticality happens in a melting nuclear reactor is the release of a lot of heat and radiation , but not in an explosive manner as in an atomic bomb .
it is not so much as one single particle will be seen with different masses as it is that that one type of particle will be seen as having multiple different masses when it is detected multiple times . for example if the extra dimension is like a rolled up microscopic cylinder , the particle can have an infinite number of discrete masses starting from the mass which has no extra kinetic energy in the rolled up cylindrical direction , to masses that have 1 , 2 , . . . units of momentum in the extra rolled up cylindrical direction . these momenta are quantized since only whole wavelengths of the particles wavefunction are allowed around the cylindrical dimension and each unit of extra momentum around the cylinder will be seen as additional rest mass . these are called the kaluza-klein tower of excitations of the basic particle . kaluza-klein theory was developed in the 1920s in an attempt to unify gravitation ( general relativity ) and electromagnetism ( maxwells equations ) . by the way , according to this source : you may also be interested to know that the original 1921 theory has evolved into today 's string theory , as both share the idea of using multiple extra space dimensions to describe the world . the most advanced version of strings is known as m-theory , which utilizes an 11-dimensional spacetime having seven compactified extra space dimensions – a far cry from kaluza 's original single extra dimension ! by the way , kaluza originally came up with his idea in 1919 and communicated it to einstein in hope that the great scientist would recommend it for publication . but einstein , who expressed great admiration for kaluza 's idea , sat on it for two years before recommending it . i am sure this did not sit well with kaluza .
the wikipedia article on color can clear the confusion on color perception . the electromagnetic spectrum in the visible range corresponds to colors as the human eye observes them , but color perception is more general and depends on the phsyiology of the eye and brain . one can make color photographs using for illumination only two frequencies , for example , as polaroid inventor e . land demonstrated . a hot iron rod is red because the upper end of the infrared spectrum , which is the heat you are feeling/measuring , not seeing , goes into our perception of red . the overwhelming majority of the energy is in the infrared for which we have no retina sensors , only skin ones . a book may be red because it is reflecting the red frequency , or we may perceive it as red because of the cones in our retina ( read the article of color in wiki ) . the cover absorbs the non reflected part of the spectrum but that is a very small amount of energy to be converted to heat from room light , so we do not perceive the difference in temperature .
you are absolutely correct - objects do not need to ever reach earth 's escape velocity of 11.2 km/s , and many spacecraft that leave orbit , do not . that being said , note that escape velocity depends on where you are : the velocity that a cannonball 1000 km above the earth 's surface would need to escape is substantially lower than that needed by a cannonball on the surface . this is particularly obvious when you envision a cannonball fired at escape velocity from the surface : once it reaches 1000 km , it must be going slower , but it must still be at " escape velocity " . in this sense , all objects intended to leave earth 's orbit do reach escape velocity , but only at a great distance .
the container will not suddenly accelerate to $v_2$ . when you suddenly change the acceleration to $a_2$ , you are not providing an impulse to the container , so the velocity of the container just after the change will still be $v_1$ and not suddenly accelerate to $v_2$ . the velocity of the container will increase now too but by rate $a_2$ and not $a_1$ ( note that it was increasing before the change too , due to acceleration $a_1$ , in case you missed that ) . the water in the container will also accelerate by $a_2$ , and you you will not experience a sudden splash effect . what would happen to the water is that it will oscillate periodically about an equilibrium position ( infinitely if there is no damping ) . the equilibrium position of water level in an accelerated container is shown by this image:- when you change from $a_1$ to $a_2$ , the equilibrium position changes its inclination $\theta$ , so the fluid level starts oscillating about this level .
i agree with jwenting but in some sense , i feel that he is not answering the question : why there is no " combined $\alpha$ plus $\beta$ decay in which a nucleus emits e.g. a helium atom ? well , let me start with the $\beta$-decay . nuclei randomly - after some typical time , but unpredictably - may emit an electron because a neutron inside the nuclei may decay via $$ n\to p+e^- +\bar\nu$$ which may be reduced to a more microscopic decay of a down-quark , $$ d\to u + e^- +\bar\nu . $$ this interaction , mediated by a virtual w-boson , is why a nucleus - with neutrons - may sometimes randomly emit an electron . so the $\beta$-decay is due to the weak nuclear force . on the other hand , the $\alpha$-decay is due to the strong nuclear force : the nucleus literally breaks into pieces , with a very stable combination of 2 protons and 2 neutrons appearing as one of the pieces ( helium nucleus ) . the two processes above are independent , and each of them can kind of be reduced to a single elementary interaction whose origin is different . this independence and different origin is why the " combined " decay , with an emission of both electron ( or two electrons ) and a helium nucleus , is extremely unlikely . such an emission of a whole atom ( which is electrically neutral but it is surely not " nothing " ! ) could only occur if several of the elementary decay interactions would occur at almost the same time which is extremely unlikely .
i have noticed that none of these answers actually answer the question . the simplest explanation of string theory i can think of : particles we currently consider " point particles " ( electrons , quarks , photons , etc . ) are actually tiny pieces of string with each a characteristic vibration . they interact in a sort of harmony that results in/manifests as the physical laws we observe . if anyone with more knowledge in the field can correct me , i ask for improvements . this is just how i personally explain it to people who ask , and i would hate to give out false information .
the short answer , as karsus ren says , is that it is electromagnetic induction that generates the electricity : the relative movement of a conductor and a magnetic field . as vladimir kalitvianski points out , strictly speaking , windmills do not generate electricity - wind turbines generate electricity , and windmills mill grain . still , it is become reasonably common to refer to wind turbines as windmills . as crowley says , a wind turbine takes the wind 's horizontal motion , turns it into rotary motion , which in some ( but not all ) turbines then goes through a gearbox . the motion then ( depending on the nacelle design ) either : a ) rotates either a conducting coil ( typically copper ) through a magnetic field ; or b ) rotates magnets around a coil ; either way , this generates electricity through electromagnetic induction . the magnets may be permanent magnets ( typically a neodymium alloy ) or electromagnets . the theoretical limit on how much of the wind 's energy can be captured by the blades is called the betz limit , and is an efficiency of ~59.3% ( 16/27 ) . the danish wind-turbine manufacturer vestas has some very accessible stuff on their e-learning site at http://www.vestaselearning.com/ it might seem banal at times , but do not be fooled - there is a lot of decent material in there .
you were onto it in the comments , so i might be late to offer anything new here . the pressure is irrelevant in this problem ; it is a trick , i guess . a reversible adiabatic process is one in which there is no heat flow in or out of the gas , so all of the work done in the expansion/compression goes into the temperature change . just calculate the change in energy like you were saying ( $u_2-u_1$ ) and that is the work done ( on it , not by it ) ! good job figuring it out ! also , as with all physics problems , make sure that the negative signs match your intuition . i can not always keep $-w$ and the like straight on paper , especially since i often mix up whether $w$ represents the work done by the gas or the work done on the gas . i am pretty sure it is the former . so , if the gas expands , it does work , and $w&gt ; 0$ . if the gas contracts , work is done on it , so the work the gas does is negative .
the fundamental reason why energy is conserved , is invariance of the physical laws by a time translation $t \to t + t_0$ , in a lagrangian formulation . this is a particular case of the noether theorem , which states , that if a lagrangian has a continuous symmetry , there is a corresponding conserved quantity . now , if we look in detail , conservation of a charge $q$ may be expressed as a local conservation law implying a ( electric ) current $j_\mu$ , that is $\partial^\mu j_\mu =0$ this is the relativistic notation , with $\mu =0$ is the time coordinate ( $j_0=\rho$ ) , and $\mu =1,2,3$ are the spatial coordinates ( $j_1 = -j^x , j_2=-j^y , j_3=-j^z$ ) . upper and lower indices are related by the metric matrices $\eta^{\mu\nu}$ and $\eta_{\mu\nu}$ which are simply diagonal matrices ( $1 , -1 , -1 , -1$ ) . so $\partial^\mu j_\mu =0$ is simply $\frac{\partial \rho}{\partial t} + \vec \nabla . \vec j =0$ now , for the conservation of momentum/energy $p_\nu$ , you see that ( comparing to $q$ ) you have a supplementary indice $\nu$ ( for instance the energy is $p_0$ , while the momentum coordinates are $p_1 , p_2 , p_3$ ) , so the corresponding local " current " has now $2$ indices : $t_{\mu \nu}$ , and the local conservation law is simply $\partial^\mu t_{\mu\nu} =0$ the " current " $t_{\mu\nu}$ is more known as the stress-energy tensor . so , the current for the energy $e=p_0$ is $t_{\mu0}= t_{0\mu}$ ( the stress-energy tensor is symmetric in its indices ) , and the local law conservation for energy may be written : $\partial^\mu t_{\mu0} =0$
the core of perturbative string theory has a mathematically rigorous formulation . in fact much of mathematical physics and mathematical insight into quantum field theory as such has been gained from the study of the low-dimensional qfts that constitute the worldvolume theories of the string and the various branes . for instance the axiomatization of qft in the “fqft” flavor ( roughly dual to the aqft picture ) historically originates in insights gained in the study of ( topological ) string ( namely the moore-seiberg axioms ) . on the other hand , the attempted implementations and applications of core string theory are vast and numerous , and when it finally comes to string phenomenology the usual level of rigor is just that common among practicing quantum field theorists . on the far end , deep aspects of string theory that are felt by many researchers to be of metaphysical relevance , such as the “landscape of string theory vacua” have led and are leading to speculations that are not anymore backed up by any disciplined reasoning . more in detail : the quantization of the string sigma-model may be obtained cleanly via the mathematical sound process of geometric quantization , see the references on the nlab at string – symplectic geometry and geometric quantization . the famous weyl anomaly of the string is formally understood in terms of anomalous action functionals , see for instance ( freed 86 , 2 . ) . various other obstructions to quantization ( quantum anomalies ) in the background fields for the string sigma-model such as notably the freed-witten-kapustin anomaly , have been understood in fine detail in terms of obstructions in differential cohomology , see for instance ( distler-freed-moore 09 ) . particularly well analyzed are the two special sectors of first quantized string theory , that of rational conformal field theory , which contains the example of strings propagating on lie group manifolds – the wess-zumino-witten model ; as well as the example of topological strings . rational conformal field theories indeed stand out as one non-trivial and rich class of qfts which have been subject to complete mathematical classification ( in the same sense in which mathematicians for instance do the classification of finite simple groups ) . for details on this classification see on the nlab at frs formalism . for the topological string much more is true . the topological string has effectively become a subject in pure mathematics , with its rigorous axiomatization via the tcft version of the cobordism hypothesis-theorem , its formulation as mathematical homological mirror symmetry , its relation to geometric langlands duality etc . but the fqft-axiomatics that serves to mathematically formalize the topological string is not restricted to the topological sector , it also applies to the physical string . for instance huang’s theorem shows that the familiar description of physical string via vertex operator algebra is an instance of the fqft-formalization . indeed , in frs formalism these two formalizations , vertex operator algebras ( via their modular tensor categories of representations , and tqft combined via the rigorous ads3-cft2 and cs-wzw correspondence give the classification of rational cft ) . ( in particular this says that in this low dimensionl holography and ads-cft duality is rigorous , of course this is far , far from true in higher dimensions . ) in summary this is a level of rigour with which the worldsheet 2d qft of the string is understood which is well beyond of what one typically encounters for non-trivial interacting ( non-free ) qft . and this is full non-perturbative quantum field theory ( on the worldsheet ! ) , not just the approximation in perturbation theory . from here on , also string field theory ( its action functional , that is ) , has a completely rigorous formulation in terms of operads and l-infinity algebras ( lie n-algebras for n→∞ ) . a snapshot of the state of the art of rigorous foundations of string theory as of 2011 is in ( sati-schreiber 11 ) . the above text with hyperlinks for all technical terms is also on the nlab at string theory faq -- is string theory mathematically rigorous ? .
i think your reasoning is sound . however , being in melbourne , australia ( which is heading for 35 c today ) , i can think of better applications of eating ice than burning calories . you would certainly experience a greater drop in body temperature by consuming equal amounts of ice versus water at 0 c .
that statement , that space and time are equivalent , is not really correct . in special relativity there is a distinction between spacelike and timelike events , those are events that cannot or can ( respectively ) be causally connected . this replaces the notion of " simultaneous " and " before or after " to something all inertial observers can agree on . in general relativity , this distinction is made locally - causal influence propagates locally in speeds less than c , but that constraints changes from point to point according to the local metric ( which encodes the force of gravity ) . all of this is encoded in the statement that the metric is indefinite , with a specific signature that supports one time direction . while the mathematics of gr can be generalized to other signatures , the physics cannot - the lorentzian signature is essential in correctly interpreting the theory . anyone can play all kinds of mathematical games , but those are meaningless unless you are very clear on how the calculations you perform are related ( at least in principle ) to physically observable quantities .
when an electron is promoted from the valence band to the conduction band it leaves a hole in the valence band . when the electrons falls back down from the conduction band to the valence band ( to fill the hole ) energy is released , but not necessarily as light . a photon can be emitted only when the material has a direct band gap i.e. when the electron in the lowest energy state in the conduction band and the hole in the highest energy state in the valence band have the same momentum . if this is not the case ( and for most semiconductors it is not ) the electron must interact with the crystal lattice , and energy is lost to the lattice rather than as a photon . it actually takes careful design to make diodes emit light ! because the conduction band is technically still an orbital , which means it can have " holes " , right ? no , by definition you can only have a hole in an energy band that is full e.g. the valence band . a hole is what is left behind when you remove an electron from a full band . re your last question , it depends on how localised the electron wavefunctions are . you are quite correct that the inner electrons on the silicon atoms are localised and can not jump from atom to atom . when the average distance of the electron from the nucleus is comparable to the interatomic spacing the electrons feel an " average " force from all the nuclei in the crystal and they may form delocalised wavefunctions . exactly when this happens will depend on the fine detail of the atoms and the crystal structure .
for a plane wave , all cartesian field component space and time dependencies are $exp ( i\ , \vec{k}\ , \vec{x}-i\ , \omega\ , t ) $: time derivatives $f\mapsto \mathrm{d}_t f$ become $f\mapsto\ , -i\ , \omega\ , f$ and vector curls $\vec{f}\mapsto\nabla\times\vec{f}$ become $\vec{f}\mapsto i\ , \vec{k}\times\vec{f}$ . so now we recallampère 's law in time-harmonic form : $$\begin{array}{lclcl} and \nabla\times h and = and \vec{j}+\partial_t\vec{d}\\\rightarrow and \vec{e} and = and \frac{i}{\sigma -i\ , \omega\ , \epsilon}\vec{k}\times\vec{h}\end{array}\tag{1}$$ so you see from this that $\vec{e} , \ , \vec{k} , \ , \vec{h}$ , in that order , form a right handed triple of mutually orthogonal vectors . on the incidence side of the boundary , assumed normal to $\vec{k}$ , there are two such triples : an incident wave with electric field $\vec{e}^+$ and a reflected wave $\vec{e}^-$ ( this one has wavevector $-\vec{k}$ . on the transmission side there is only one triple with electric field $e^t$ . now we simply equate electric fields at the boundary ( they are all in the same direction , so we only need signed scalars henceforth ) , since tangential $\vec{e}$ must be continuous across it : $$e^+ + e^- = e^t\tag{2}$$ and use ( 1 ) to equate the magnetic fields at the boundary so as to fulfill the boundary condition that $\vec{h}$ must be continuous across the boundary . we note that the magnetic fields too are all in the same direction ( orthogonal to the $\vec{e}$s ) , so signed scalars work here too : $$\frac{-1}{\omega\ , \epsilon_0} ( e^+ - e^- ) = \frac{i}{\sigma -i\ , \omega\ , \epsilon}\ , e^t\tag{3}$$ note the minus sign on the lhs between the two $e$s : $e^+ - e^-$: this is because $\vec{k}$ is in the opposite direction for the reflected wave . you should now be able to find $e^-$ and $e^t$ in terms of $e^+$ from ( 2 ) and ( 3 ) and then use maxwell 's relationship $\epsilon = n^2$ to simplify .
if you continued to read on , it goes on to say : actually , all bodies are electrified , but may appear not to be so by the relative similar charge of neighboring objects in the environment . an object further electrified + or – creates an equivalent or opposite charge by default in neighboring objects , until those charges can equalize . therefore , since all bodies are electrified or can be electrified , your statement is correct .
this page says : however , the surface madelung constant computed in the ( 100 ) plane is 96 % of the bulk value for crystals with a sodium chloride structure . in the ( 110 ) planes in crystals with sodium chloride and cesium chloride structures , the surface madelung constant is 86% and 90% respectively , of the bulk value . seems high , but i do not have access to the cited reference ( p . h . citrin and t . d . thomas , j . chem . phys . 57 , 4446 ( 1972 ) ) to check .
not only is the experiment possible , a version of the experiment at the atomic level was done by einstein and de haas [ 1 ] . einstein and dehaas showed that the angular momentum inhering in the aligned electron spins in a ferromagnet can be exhibited on a macroscopic scale when the sample is demagnetize . apparently , his is the only experiment einstein ever performed himself [ 2 ] . references 1 . einstein and de haas paper [ pdf ] http://www.dwc.knaw.nl/dl/publications/pu00012546.pdf wikipedia on the einstein de haas effect https://en.wikipedia.org/wiki/einstein%e2%80%93de_haas_effect
let 's start , as per the op 's comments , with the convention that when we take two systems a and b , and a is doing work on b , then : from the perspective of a , the work is positive ; and from the perspective of b , the work is negative . now , from the perspective of the windmill . the work that the wind does is indeed negative : the wind is adding exergy to our windmill system . and most of that work will then transfer into positive work done by the windmill through the rotating shaft . however , these transfers of exergy do not come for free : some heat is generated in the process , from friction in the system . as to exactly where the heat transfers are , will depend on the initial temperatures of the surroundings and the windmill , and the amount of heat generated by friction . but typically , the friction would raise the windmill 's temperature above the temperature of the surroundings , so heat would get transferred from within the windmill system , to outside it .
see for instance the comments on are w and z bosons virtual or not ? . basically the claim is that the observed particle represents a path internal to some feynman diagram and accordingly there is a integral over it is momentum . i am not a theorist , but as far as i can tell the claim is supportable in a pedantic way , but not very useful .
there is a number of interesting points to this . the passage from 1 . to 2 . is not trivial . if you do the calculation , you will see that the laplacian $\nabla^2\vec{e}$ from the wave equation gives rise to the term in $\left ( \nabla\chi\right ) ^2$ you mention as well as a term in $\nabla^2\chi$ . this second term only goes away in the small $\lambda$ limit and it is the essence of the eikonal approximation . it is not a calculation you should wave away : work it out in full and implement the approximation , noticing that locally $\chi ( \vec{x} ) =\vec{k}\cdot\vec{x}+\text{slow factors}$ , where $\vec{k}$ is large . ( you will of course need to quantify " slow " . ) ( the calculation that $\vec{s}=\frac{c^2}{n^2\omega}\nabla\chi$ , on the other hand , is trivial . ) as kdn mentioned , the integral curves of $\vec{s}$ and its unit vector $\vec{s}$ are the same . this follows from the definition of integral curves : they are curves such that the vector field is tangent to them throughout . this is independent of the length of the vector . ( in terms of the curve it corresponds to a reparametrization of the " time": it changes the speed but not the direction of the velocity . ) using a unit vector means that light rays will be parametrised by path length . one can simply define light rays to be the integral curves of $\vec{s}$ and be happy about it , though of course that is simply missing the physics . the key fact about the light rays , so defined , is that they are everywhere normal to the surfaces of constant $\chi$ , i.e. the surfaces of constant phase , i.e. the wavefronts . plane waves propagate in straight lines normally to the wavefronts in free space , and so do light rays ( so defined ) . it is the normal to the wavefronts that matters when working out fresnel equations , and therefore the ( so defined ) light rays will obey snell 's law . ultimately , proving 3 . is a matter of definition : what are light rays ? write down any defining property and you will be able to prove the integral curves of $\vec{s}$ obey it . it is important to note that in isotropic media $\vec{s}$ is not only the local unit poynting vector , but it is also the local unit wave vector . ( essentially , this is the same point as above . ) intuitively , light rays ought to follow wave vectors because it is wave vectors that tell light waves where to go . in a birefringent ( not isotropic ) medium the phase propagation direction ( wave vector ) and the energy propagation direction ( poynting vector ) are not necessarily the same ( and the snell law does not apply ) . proving 4 . is an interesting exercise ( i.e. . do it ! ) but it is essentially trivial . it relies on the identity $\frac{d\vec{x}}{d\tau}=\vec{s}$ , which defines light ray curves $\vec{x} ( \tau ) $ , on judicious use of the total derivative $\frac{d}{d\tau}= ( \frac{d\vec{x}}{d\tau}\cdot\nabla ) $ , and some interesting vector calculus manipulations . ( hint : prove $ ( \nabla\chi\cdot\nabla ) \nabla\chi=\frac12\nabla\left ( \nabla\chi\right ) ^2$ . ) presumably you know by now that what you get is called the ray equation , what it means , and how to use it , or you would not have stopped there ; ) . this looks like enough to get you going but if you have more questions , do ask .
just because $f^{\mu\nu}$ has two indices does not mean that it represents a spin-2 particle . note that the metric $g^{\mu\nu}$ is a symmetric two indexed object while the em field strength $f^{\mu\nu}$ is antisymmetric . in fact , the metric $g^{\mu\nu}$ is analogous to potential $a^\mu$ in em and the field strength of gravity is the four indexed riemann tensor $r_{\mu\nu\rho\lambda}$ . what spin the field represents depends on the symmetries of the indices and the field equations that it obeys . in particular , the physical degrees of freedom of a massless field of spin $ ( a/2 , b/2 ) $ can be written in terms of dotted and undotted indices $g_{\alpha_1 , \dots , \alpha_a , \dot\beta_1 , \dots , \dot\beta_b}$ totally symmetric in the dotted and undotted indices . in order for it to be a representation of the poincare group it must satisfy the supplementary conditions $$ \begin{align} \partial^{\gamma\dot\gamma}g_{\alpha_1 , \dots , \alpha_{a-1}\gamma , \dot\beta_1 , \dots , \dot\beta_b} and = 0 \\ \partial^{\gamma\dot\gamma}g_{\alpha_1 , \dots , \alpha_{a} , \dot\beta_1 , \dots , \dot\beta_{b-1}\dot\gamma} and =0 \ . \end{align}$$ these supplementary conditions imply that each component of $g$ satisfies the klein-gordon equation . ( the only exception is the scalar case where there are no supplementary conditions because there is no indices . in this case there is just the kg equation $\box g = 0$ . ) it is also shown that these conditions imply that the field has a definite helicity of $ ( a-b ) /2$ and there is only one degree of freedom in such a field . physical fields are made of the sum of two such fields of opposite helicity this is clearly the case for the em field strength $f_{\mu\nu}$ when written in the form $f_{\alpha\beta\dot\alpha\dot\beta}= ( \sigma^\mu ) _{\alpha\dot\alpha} ( \sigma^\nu ) _{\beta\dot\beta}f_{\mu\nu}=2\varepsilon_{\alpha\beta}\bar{f}_{\dot\alpha\dot\beta} + 2\varepsilon_{\dot\alpha\dot\beta}f_{\alpha\beta}$ , so the field strength decomposes into the sum of two massless fields carrying helicity $\pm1$ . there is a good description of the field representations of the poincare group in section 1.8 of ideas and methods in superspace and supergravity . section 1.8.3 deals with the massless representations applicable in the two cases raised in your question . section 1.8.4 has the examples of massless scalar , spin-1/2 , em ( spin-1 ) , spin-3/2 and linearized gravity ( spin-2 ) .
i think the answers in the duplicate have covered most of the key points . i will just add to them the feeling of weight , strain , stress etc . is due to the differential force acting on different points of the body . even while floating in curved space-time you may feel strain , weight etc . because your body is not point size , the particles of your body will be accelerated in different directions relative to each other . to maintain the rigidity of your form , the body exerts a force to hold itself together . this force is weight , strain , stress etc . so irrespective of your state of motion from any frame of reference , if the particles of body are being forced in different directions relative to each other , your nervous system will register this as weight , stress , strain in the respective parts of the body . the cause of relative acceleration between parts of your body is twofold- tidal acceleration due to the structure of space time , the action of non gravitational forces on the different particles in your body . so even in non-inertial frames without the effects of curved-space you might feel this weight , strain etc . the only time you will feel absolutely weightless , is if the relative acceleration between all the particles in your body is equal to 0 . due to the fact that the earth is curved and that you are not a point particle , there is a slight inward lateral strain even when you free fall in vacuum on earth .
i decided to look at whether the estimate you arrive at gives a reasonable-seeming result . 0.000145 parsecs ( 30 au , about the radius of neptune 's orbit ) is a close encounter indeed . this closeness made me think at first that 50 million years seemed to often . we do not have evidence of giant planets passing that nearby that often . so then i looked at the mass distribution of these ' nomad planets ' . in the article it says that they are " ranging from the size of pluto to larger than jupiter " , or $0.002 m_e$ to $&gt ; 300 m_e$ . we can assume that there are many more dwarf-planet sized bodies than gas giants . now we ask , does this seem right ? about every 50 million years a pluto-sized body passes as close as neptune 's orbit ? and i have to say yes , this does seem reasonable . a body the size of pluto would not produce much perturbation at that distance , almost certainly not enough to significanly effect the planets ' orbits and only sometimes enough to disturb a few oort cloud and kuiper belt objects . ( a jupiter-sized body might pass that close far less frequently - on time scales of a billion years or so ; you could imagine that this could help explain some anomalies like the distribution of the giant planet 's orbits , but that is just speculation . ) this is similar to how asteroids fairly commonly pass closer than the distance of the moon to earth . it is interesting and notable , but not catastrophic , which makes me want to say based on intuition that your estimate is reasonable .
we have to be careful in stating exactly what we are going to allow ourselves to assume here . we need some sort of principle of relativity -- that the laws are the same for both observers . but we do not want to assume anything else a priori , right ? for instance , we do not want to assume at first that rulers have the same length for both observers -- we need to prove that . let 's work in one dimension for simplicity . suppose that observer b is moving at constant velocity v relative to observer a . suppose some object is moving along with some speed $u_b$ as measured by b . we want to show that the speed as measured by observer a is $u_a=u_b+v$ . consider the position of the object at two different times , separated by a small amount $dt$ . since time is absolute ( all observers use the same $dt$ ) , what we want to show is equivalent to $$ dx_a = dx_b+v\ , dt $$ ( multiplying the original equation through by $dt$ ) . here $dx_a$ means $x_a ( t+dt ) -x_a ( t ) $ , that is , the change in the position of the object at the two times , as measured by a , and similarly for b . here 's a useful fact : if both observers measure the distance between two points at an instant of time $t$ , they must get the same answer . the reason is symmetry . if the two disagreed , then one would have to get a bigger answer than the other . but for a measurement of this sort , there is nothing to break the symmetry between a and b -- that is , we can just change the sign of $v$ , and consider b to be stationary and a to be moving , and that should not affect the answer . i think that is enough to get us there . suppose that observer b sets of a firecracker at his location at time $t$ , and another at time $t+dt$ . the two observers must agree on the distance from b to the object at the time the first firecracker went off , and they must agree on the distance from b to the observer at the time the second firecracker went off . the difference between these these two numbers is $dx_b$ . but the difference between these two numbers is also $dx_a-v\ , dt$ , since observer a knows that observer b traveled a distance $v\ , dt$ during that time interval . the conclusion follows .
one may prove for an arbitrary rigid body ( and wrt . to an arbitrary choice of pivotal point for the rigid body ) that the three moments of inertia $i_x$ , $i_y$ , and $i_z$ , around the three principal axes ( which we will call $x$ , $y$ , and $z$ ) satisfy the triangle inequality , $$\tag{1} i_x +i_y ~\geq~ i_z , \qquad i_y +i_z ~\geq~ i_x , \qquad i_z +i_x ~\geq~ i_y . $$ in other words , if a semi-positive definite symmetric real $3\times 3$ matrix with non-negative eigenvalues $i_x$ , $i_y$ , and $i_z$ does not satisfy the triangle inequality ( 1 ) , it does not represent a physically possible distribution of mass . ( the proof follows straightforwardly by writing down the definition of moment of inertia . ) moreover , one may show that such three eigenvalues $i_x$ , $i_y$ , and $i_z$ that satisfy ( 1 ) may be reproduced by a solid ellipsoid with a unique choice of non-negative semi-axes $a$ , $b$ , and $c$ ( unique up to the scaling of the total mass $m$ ) . $$ \frac{2}{5}m a^2~=~i_y +i_z -i_x~\geq~0 , $$ $$ \frac{2}{5}m b^2~=~i_z +i_x -i_y~\geq~0 , $$ $$ \tag{2} \frac{2}{5}m c^2~=~i_x +i_y -i_z~\geq~0 . $$
to see if a process will take place you need to calculate it is gibbs free energy $\delta g$ . this is defined as : $$\delta g = \delta h - t\delta s$$ the quantity $\delta h$ is the helmholtz free energy and for liquids and solids is roughly the amount of energy released ( $\delta h$ is negative if energy is released and positive if energy is absorbed ) . $\delta s$ is the entropy change for the process . for precise definitions of these quantities have a look on wikipedia . the equilibrium constant for the process is the exponent of the gibbs free energy . if $\delta g$ is large and negative the process goes to approximately 100% completion while if $\delta g$ is large and positive the process hardly takes place at all . anyhow , for the mixing of two liquids the entropy change is always large and positive so it tends to make $\delta g$ large and negative and this favours mixing . for two liquids to be immiscible the molecules of the two liquids must repel each other so strongly that the enthalpy of mixing overcomes the entropy . as an example take water and mineral oil . water hydrogen bonds very strongly to other water molecules but hardly at all to oil molecules . to mix oil and water you have to put energy in to break all those hydrogen bonds but you get no energy back when the water and oil mix . that makes $\delta h$ large and positive and this disfavours mixing . the surface tension arises from the same physics as the immiscibility . suppose you want to increase the area of the interface . to do this you need to move water molecules from the bulk to the oil/water boundary . but this costs energy because in moving a water molecule to the bounday you break water-water bonds and replace them with water-oil bonds . since it takes energy to increase the area of the boundary this means there is a force , and this is the surface tension . the surface tension is a force per unit length ( because it is an energy per unit area ) and it is independant of the area of the interface . it depends only on the two liquids at the interface . the walls of the container can have an effect because the liquids will generally interact differently with the container , but normally the surface tension is dominant and it would only be in very thin tubes that you had see an effect from the walls . you asked about articles on this subject , and as usual wikipedia has some excellent articles . have a look at http://en.wikipedia.org/wiki/entropy_of_mixing and http://en.wikipedia.org/wiki/surface_tension
let start with newton 's second law in one dimension , then we get $$\sum f_z = m \frac{\partial^2\zeta}{\partial t^2} $$ there are two forces acting on the particle a downward force due to gravity : $m g = \rho ( z ) v g$ an upward buoyant force : $m_{displaced} g = \rho ( z+\zeta ) g$ fill things in to get : $$\rho ( z ) \frac{\partial^2\zeta}{\partial t^2} = - ( \rho ( z ) -\rho ( z+\zeta ) ) g $$ you can approximate the last term with a linearisation using a taylor expansion $$\rho ( z+\zeta ) \approx\rho ( z ) +\frac{d\rho}{dz}\zeta+\mathcal{o} ( \zeta^2 ) $$ you see that the term containing $\rho ( z ) $ will cancel out , thus $$\rho ( z ) \frac{\partial^2\zeta}{\partial t^2} = \frac{d\rho}{dz}\zeta g $$ or , two rewrite it in the simple harmonic oscillator language $$ \frac{\partial^2\zeta}{\partial t^2} - \left ( \frac{g}{\rho ( z ) } \frac{d\rho}{dz}\right ) \zeta = 0 $$
actually in electrostatics energy density of e-field is not a physical observable . as you say , only when charges move will there be any work done . while the two ways of calculating total energy end the same , you cannot distinguish whether energy is stored on the charges or in the field . even e-field itself is more of an abstract mathematical entity , without which everything can be calculated in terms of coulomb law . the physical reality of e and b fields ( and the energy density associated ) becomes apparent only in non-static cases . for example , in electromagnetic radiation , fields can propagate in free space without associating with charges and currents , and the radiation may do work on non-charges ( for example , light pressure ) . because from maxwell equations we can derive a general formula of energy density $$\rho = \frac{\epsilon_0}{2} |\vec e|^2 + \frac{1}{2\mu_0} |\vec b|^2$$ which coincides with the electrostatic case , we deduce that even in electrostatics energy is indeed stored in the fields .
there are three main reasons . 1 ) while venus is orbiting the sun at 35.02 km/s , the earth is also orbiting the sun in the same direction at 29.78 km/s . this factor will decrease the relative transit velocity of venus as seen from earth . 2 ) venus is travelling at 35.02 km/s an elliptical orbit . hence the actual distance traveled by venus during the transit will be slightly more than its diameter because it is travelling on a curved path and not a straight line . this factor will increase the actual transit distance covered by venus . however the contribution of this is negligible and can be ignored except of high precision calculations . 3 ) there will be a small but measurable impact because of the surface velocity of earth 's rotation at 0.434 km/s ( at the equator ) about its axis . notice that the tangential velocity of an observer on earth due to the rotation of the earth about its axis will be in opposite direction to the tangential velocity of both the venus and earth around the sun . this factor will increase the relative transit velocity of venus as seen from earth . my calculation , using kepler 's law differ slightly from that of nathaniel but it is essentially same in spirit . we obtain the transit time of 19 mins 56 seconds which is accurate enough . $$ t \approx \frac{d_v}{v_v\{1 - ( t_v/t_e ) ^{2/3}\} + v_e} = 19 \min 56 \sec $$ where $d_v$ = diameter of venus , $v_v$ = orbital velocity of venus , $v_e$ = orbital velocity of earth , $t_v$ = orbital period of venus , $t_e$ = orbital period of earth , $v_e$ = rotation velocity of earth .
by using pulleys , levers or hydraulics . . . a person must do a certain work in order to lift a car and increase its potential energy . now , since a maximal force that a person can exert is limited , the idea is to increase a displacement / spatial dimension , i.e. $$w = \vec{f} \cdot \vec{s} . $$ system of pulleys or lever or hydraulics do exactly that . you need smaller force , but you exert it on larger displacement / spatial dimension .
yes , lift is nothing but the force in the upward direction . so to keep a 150 pound person aloft , you need a lift of 150 pounds ( 150 pound-force ) .
the german-language wikipedia 's page on c.f. gauß lists a german translation of this work by rudolf h . weber , titled allgemeine grundlagen einer theorie der gestalt von flüssigkeiten im zustand des gleichgewichts ( publication date : 1903 ) , available on the internet archive ( abstract , pdf ) .
you can not integrate the right hand side because $f=f ( x_i ) $ and you have got a differential on $t$ . as for a ) , if you rearrange terms , you can verify that $$m\frac{d\dot{x_i}}{f ( x_i ) }=g ( t ) \ , dt$$ so that now you can not integrate the left hand side because $f$ depends un $x_i$ and you have got a differential on $\dot{x}_i$ .
because the largest earthquakes are caused by the motion of faults in the earth 's lithosphere , the upper limit to earthquake magnitude is going to be related to the contact area between tectonic plates , not the size of the entire earth . some geophysicists believe the magnitude 9.5 earthquake in chile was likely close to the maximum size possible . largest earthquakes in the world since 1900
the continuous stream of air that you are blowing in , it does not enter the pipe continuously . when the stream of air hits the hard edge in an organ pipe , it flaps in and out of it due to the difference in the density of the air outside and inside the pipe . this oscillation of the air in and out , it will be a periodic energy supply for the standing wave in the pipe . think about it as a skipping rope game . the rope is an standing wave and we move the edge up and down to keep it going . the frequency of the stream of air going in and out of the mouthpiece depends on the geometry of the mouthpiece , mainly the length of the mouthpiece that finishes with the hard edge . the frequency of the standing wave in the instrument depends mainly on the length , but also on other factors like the the diameter , temperature , humidity . . . it is interesting to notice that if you blow harder , the frequency of the flapping in the mouthpiece will increase . this will increase the excitation of the standing wave in the pipe and it will jump to the next resonance frequency . this will be double the frequency if the instrument has open ends in both sides . increasing the speed of the air of your input you can excite higher resonance frequencies of the pipe . this also happens with the skipping rope game where you can move the rope up and down faster and you will see standing waves of higher frequencies with more nodes . if you want a more detailed explanation , here is a link with a chapter of a book by thomas d . rossing : woodwind instruments
i am guessing they started with inscribing a regular hexagon in a circle . next draw the three diagonals , which are also diameters of the circle , and construct for each the perpendicular line through the center . voila , you have divided the circle into 12 equal sectors . alternatively , constructing an inscribed regular dodecagon in a circle is not all that hard either : by this rationale , 10 = 2*5 would not have been a likely choice because the method of construction of a regular pentagon was not discovered until the time of euclid , and 14 = 2*7 would be most inconvenient because the regular heptagon is not constructable with compass and straightedge . image source : http://upload.wikimedia.org/wikipedia/commons/2/2b/regular_dodecagon_inscribed_in_a_circle.gif
time zones moving on from the calendar to time , we recommend the abolition of all time zones , as well as of daylight savings time , and the adoption of atomic time—in particular , greenwich mean time , or universal time , as it is called today . like the adoption of a modern calendar , the embrace of universal time would be beneficial . for example , the adoption of universal time would give new flexibility to economic management in the vast east-west expanse of russia : everyone would know exactly what time it is everywhere , at every moment . opening and closing times of businesses could be specified for every class of business and activity . if thought desirable , banks and financial institutions throughout the country could be required to open and to close each day at the same hour by the world time . this would mean that bank employees in the far east of russia would start work with the sun well up in the sky , while bank employees in the far west of russia would be at their desks before the sun has risen . but , across the country , they could conduct business with one another , all the working day . ( this would have a second benefit : at least in the far east and far west , the banks would be open either early , or late , convenient for those who are working “sunlight hours , ” such as farmers . ) with universal time , agricultural workers , critically dependent on the position of the sun , could rise with the sun , without producing any impact on other aspects of cultural and economic life . the readings on the clocks , and the date on the calendar , would be the same for all . but , times of work would be attuned with precision to russia’s local and national needs . china already has adopted a single time zone for the same purposes . and all aircraft pilots , worldwide , use universal time exclusively , for exactly the same reason that we are advocating its broad adoption—plus avoiding collisions . moscow could introduce both a simplified calendar , identical each year ( harmonized with the seasons by rare full-week adjustments at year’s end ) , and universal time , which would abolish the international date line , making the date and the time identical everywhere , including alaska and the farthest eastern regions of russia . there , and also in the center of the pacific ocean , the date would change at 00:00:00 , just as the sun passed overhead . source and context . also see this and this for discussion . dst one of the biggest reasons we change our clocks to daylight saving time ( dst ) is that it reportedly saves electricity . newer studies , however , are challenging long-held reason . a report was released in may 2001 by the california energy commission to see if creating an early dst or going to a year-round dst will help with the electricity problems the state faced in 2000-2002 . you can download an acrobat pdf copy of the staff report , effects of daylight saving time on california electricity use , publication # 400-01-13 , ( pdf file , pages , 5.2 megabytes ) . the study concluded that both winter daylight saving time and summer-season double daylight saving time ( ddst ) would probably save marginal amounts of electricity - around 3,400 megawatt-hours ( mwh ) a day in winter ( one-half of one percent of winter electricity use - 0.5% ) and around 1,500 mwh a day during the summer season ( one-fifth of one percent of summer-season use - 0.20% ) . winter dst would cut winter peak electricity use by around 1,100 megawatts on average , or 3.4 percent . summer double dst would cause a smaller ( 220 mw ) and more uncertain drop in the peak , but it could still save hundreds of millions of dollars because it would shift electricity use to low demand ( cheaper ) morning hours and decrease electricity use during higher demand hours . the energy commission has also published a new report titled the effect of early daylight saving time on california electricity consumption : a statistical analysis . publication # cec-200-2007-004 , may 27 , 2007 . ( pdf file , 592 kilobytes ) a more recent study concludes that daylight saving time in indiana actually increases residential electricity demand . that study titled " does daylight saving time save energy ? evidence from a natural experiment in indiana " . ( pdf file ) looked at the electricity use when portions of the state finally started to observe dst . before the new extended dst , portions of indiana did not observe dst . some have wondered whether this study would be true for the entire united states . initial analysis by staff of the california energy commission says a similar study may not yield the same results for california because : the use of residential air conditioning is relatively low in indiana , and the saturations are low . where as california has high usage of air conditioning in the summer . heating use is relatively high in indiana , while it is relatively low in california . the diurnal variation in temperature is low while california is very high . indiana is located in western edge of the same time zone as maine and florida , but the sun actually comes up at an earlier time than those other two states . indiana 's north-south location will affect how long the days are in the summer and might very well lead to different results in different areas . so , while the analysis is of interest to indiana , it is conclusions may not be totally correct for california or the rest of the country . the first national study since the 1970s , was mandated by congress and was done by the u.s. department of energy . the doe study can be downloaded at : http://www1.eere.energy.gov/ba/pba/pdfs/epact_sec_110_edst_report_to_congress_2008.pdf ( pdf file , 285 kb ) [ actually : here ] the key findings in the report to congress are : the total electricity savings of extended daylight saving time were about 1.3 tera watt-hour ( twh ) . this corresponds to 0.5 percent per each day of extended daylight saving time , or 0.03 percent of electricity consumption over the year . in reference , the total 2007 electricity consumption in the united states was 3,900 twh . in terms of national primary energy consumption , the electricity savings translate to a reduction of 17 trillion btu ( tbtu ) over the spring and fall extended daylight saving time periods , or roughly 0.02 percent of total u.s. energy consumption during 2007 of 101,000 tbtu . during extended daylight saving time , electricity savings generally occurred over a three- to five-hour period in the evening with small increases in usage during the early- morning hours . on a daily percentage basis , electricity savings were slightly greater during the march ( spring ) extension of extended daylight saving time than the november ( fall ) extension . on a regional basis , some southern portions of the united states exhibited slightly smaller impacts of extended daylight saving time on energy savings compared to the northern regions , a result possibly due to a small , offsetting increase in household air conditioning usage . changes in national traffic volume and motor gasoline consumption for passenger vehicles in 2007 were determined to be statistically insignificant and therefore , could not be attributed to extended daylight saving time . source and further references and context
here 's part of my answer to the derivvation of the em tensor for the ghost action . it does not match the expression you gave , but i may have made a mistake . can you check my work ? we start with the action \begin{equation} \begin{split} s_{gh} and = - \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} \nabla_\mu c^\beta \\ \end{split} \end{equation} let us now vary the action w.r.t. metric . we get \begin{equation} \begin{split} \delta s_{gh} and = - \frac{i}{2\pi} \int d^2 \sigma \left ( \delta \sqrt{g} \right ) g^{\alpha\mu} b_{\alpha\beta} \nabla_\mu c^\beta \\ and ~~~~~~~~~~~~~~~~~- \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} \left ( \delta g^{\alpha\mu} \right ) b_{\alpha\beta} \nabla_\mu c^\beta \\ and ~~~~~~~~~~~~~~~~~- \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} \delta \left ( \nabla_\mu c^\beta \right ) \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} \left [ b_{\alpha\mu} \nabla_\beta c^\mu + b_{\beta\mu} \nabla_\alpha c^\mu - g_{\alpha\beta} b_{\rho\sigma} \nabla^\rho c^\sigma \right ] \delta g^{\alpha\beta} \\ and ~~~~~~~~~~~~~~~~~ - \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} c^\lambda \delta \gamma^\beta_{\mu\lambda} \\ \end{split} \end{equation} we now use \begin{equation} \begin{split} \delta \gamma^\beta_{\mu\lambda} = \frac{1}{2} g^{\beta\rho} \left [ \nabla_\lambda \delta g_{\rho \mu} + \nabla_\mu \delta g_{\rho \lambda} - \nabla_\rho \delta g_{\mu\lambda}\right ] \end{split} \end{equation} note that in particular , it is a tensor . the last term then becomes \begin{equation} \begin{split} i and = - \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} c^\lambda \delta \gamma^\beta_{\mu\lambda} \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} b^{\mu\rho} c^\lambda \left [ \nabla_\lambda \delta g_{\rho \mu} + \nabla_\mu \delta g_{\rho \lambda} - \nabla_\rho \delta g_{\mu\lambda}\right ] \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} b^{\mu\rho} c^\lambda \nabla_\lambda \delta g_{\rho \mu} \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} \nabla_\lambda \left ( b_{\alpha\beta} c^\lambda \right ) \delta g^{\alpha\beta} \end{split} \end{equation} we then have \begin{equation} \begin{split} \delta s_{gh} and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} \left [ b_{\alpha\mu} \nabla_\beta c^\mu + b_{\beta\mu} \nabla_\alpha c^\mu - g_{\alpha\beta} b_{\rho\sigma} \nabla^\rho c^\sigma + \nabla_\lambda \left ( b_{\alpha\beta} c^\lambda \right ) \right ] \delta g^{\alpha\beta} \end{split} \end{equation}
mushroom clouds are formed in explosions ( not necessarily nuclear - see picture ) as a result of the rayleigh-taylor instability . for given density contrast $\frac{\rho_{cold}-\rho_{hot}}{\rho_{cold}+\rho_{hot}}$ between the hot cloud and the cold atmosphere , the timescale $t_{rt}$ for this instability scales with the length scale $l_{rt}$ according to : $$t_{rt} \approx \sqrt{\frac{l_{rt}}{g}\frac{\rho_{cold}+\rho_{hot}}{\rho_{cold}-\rho_{hot}}}$$ with $g$ the gravitational acceleration . for $\frac{\rho_{cold}-\rho_{hot}}{\rho_{cold}+\rho_{hot}}\approx 0.1$ and $l_{rt} = 1 km$ we find $t_{rt} \approx 30 s$ . for a cup sized ( $l_{rt} = 0.05 m$ ) explosion with the same density contrast we find $t_{rt} \approx 0.2 s$ . the ' mushroom ' has disappeared before it really takes shape . bottom line is that you need fairly large explosions to observe a mushroom .
provided its a general situation , the heat will circulate through the pipe ( and the water of course ) but most of it will be radiated out . at no point will a measurable amount of water reach a $1000 ^\circ c $ . even if some of the water reaches that temperature or close to it , it will lose it to the colder surroundings . in the end at some point this heat leak out to the surroundings . assuming that somehow there is no heat loss , eventually the whole pipe will approach $1000^\circ c$ as there is an inflow of energy but no place for it to escape to .
to elaborate on marek 's ( correct ) answer , since it seems that math is the issue that @casebash is having : start with an integral representing the time elapsed on the moving observer 's clock in terms of the stationary observer 's coordinates ( i am supressing g and c below . feel free to replace all $t$ 's with $c\ , t$ 's , and all $m$ 's with $\frac{gm}{c^{2}}$ 's ) : $$\begin{equation} \delta \tau = \int d\tau \sqrt{{\dot t}^{2} ( 1-\frac{2m}{r} ) - \frac{{\dot r}^{2}}{1-\frac{2m}{r}}- r^{2} {\dot \theta}^{2} - r^{2}\sin^{2}\theta {\dot \phi}^{2}} \end{equation}$$ where $ ( t , r , \theta , \phi ) $ are all considered to be functions of $\tau$ denoting the moving observer 's position at time $\tau$ . our problem amounts to looking for the path beginning at $ ( t_{0} , r , \theta , \phi ) $ and ending at $ ( t_{f} , r , \theta , \phi ) $ that maximizes this integral , subject to the constraint that the quantity under the square root will be equal to $1$ when the calculation is complete . to make our calculations easier , note that the square root is a monotonic function over all its domain , so we might as well maximize $$\begin{equation} \frac{1}{2}\int d\tau \left ( {\dot t}^{2} ( 1-\frac{2m}{r} ) - \frac{{\dot r}^{2}}{1-\frac{2m}{r}}- r^{2} {\dot \theta}^{2} - r^{2}\sin^{2}\theta {\dot \phi}^{2}\right ) \end{equation}$$ which is considerably simpler . it would take a whole page to carefully vary ( meaning that we basically , but not quite , take the derivative of the function with respect to ) this with respect to the four independent functions . so , i am going to just give you a taste of the task by varying with respect to $\theta$ . the path of maximum " moving clock " time ( heretofore called ' proper time' ) will be the one for which these variations are zero . the other four functions follow just as easily . the variation of this integral with respect to $\theta$ gives us $$\begin{align} \delta \delta\tau |{}_{\theta} and =\int d\tau\left ( -r^{2}{\dot \theta}\delta {\dot \theta}-r^{2}\sin\theta \cos \theta {\dot \phi}^{2}\delta \theta\right ) \\ 0 and =\int d \tau \left ( -\frac{d}{d\tau}\left ( {\dot \theta} r^{2}\delta \theta\right ) + \left ( {\ddot \theta}r^{2} + 2 r {\dot r}{\dot \theta}-r^{2}\sin\theta \cos \theta {\dot \phi}^{2}\right ) \delta \theta\right ) \\ and =\left ( \dot \theta ( t_{0} ) r ( t_{0} ) ^{2}\delta \theta ( t_{0} ) -\dot \theta ( t_{f} ) r ( t_{f} ) ^{2}\delta \theta ( t_{f} ) \right ) \\ and \quad + \int d\tau\ ; r^{2}\left ( {\ddot \theta} + \frac{1}{r}2{\dot r} {\dot \theta}- \sin \theta \cos \theta {\dot \phi}^{2}\right ) \delta \theta \end{align}$$ the first term , that we obtained by integrating the total derivative , vanishes since the variation of $\theta$ is zero at the endpoints $t_{0}$ and $t_{f}$ ( our space of states is paths beginning and ending at a fixed value of $\theta$ . if the variation will be zero , then the remaining stuff under the integral must turn out to be zero if it is going to be a minimum for an arbitrary fixed-point variation ( if the integrand is not zero , just imagine making the variation a hundred kajillion only at the point where it is nonzero--clearly this is not an extremum ) . therefore , the $\theta$ variable of the maximum proper time path must satisfy ${\ddot \theta} + \frac{1}{r}2{\dot r} {\dot \theta}- \sin \theta \cos \theta {\dot \phi}^{2}=0$ . it turns out that this equation is exactly the same equation you get for geodesic motion of a particle , which is the path followed by an observer being acted upon only by gravity . a simple calculation of the first integral for any other path ( and the fact that it will come out higher ) will then show you that this , in fact , is the maximum time path to travel .
the person hits the ground at the same speed in both scenarios . once you are in the air , you fall towards the ground with a constant acceleration of about 10 m/s^2 . everything falls the same way - rocks , cannonballs , people - regardless of size or somersaulting . there may be some small effects from air resistance , but not enough to be noticeable . the sommersaulter may also land with a different orientation so that his center of mass falls a further distance . then he could hit the ground with slightly higher speed . but basically , everything falls the same way . check out this youtube video of a feather and hammer falling simultaneously on the moon , which shows that heavy and light objects fall the same way absent air . also see this youtube video of the tv show mythbusters dropped a bullet straight down and firing one from a gun . the bullets fall in the same amount of time , regardless of their horizontal speed .
the electrons ( torn from one surface ( electrode ) through , say , electron field emission , just move in vacuum in electric field to the other surface ( electrode ) .
i think adam 's answer is excellent , and i would rather make this a comment but can not as i just signed up in order to answer . while i agree with most of what adam said , there are cases where large-n works well for $n=1$ . the case i am familiar with is the large-n expansion for the " spin ices " dy$_2$ti$_2$o$_7$ and ho$_2$ti$_2$o$_7$ , which have ising spins on a pyrochlore lattice . the best reference for this is sergei isakov 's phd thesis ( university of stockholm 2004 ) , specifically section 4.3 and subsection 4.8.2 . it is noted in that thesis that we should not expect large-n to work for pyrochlores below $n=3$ ; the expansion is singular at $n=2$ owing to order-by-disorder . extensive checking against monte carlo simulations and experiment verify that the expansion is good for $n=1$ ( references given in the thesis ) , but we do not know why this is . i found myself needing to prove the equivalence of large-n and mft at high temperatures . i apologise if it is bad etiquette to reference one 's own work , but i provide a mathematical proof in section 2.4 of my master 's thesis which can be found here ( perimeter institute 2011 ) . i believe the main working difference between large-n and mft is that mft always contains a non-zero critical temperature $t_c$ , but that $t_c\sim 1/n$ with $n$ the number of degrees of freedom at each lattice site . this means that large-n , with $n\rightarrow\infty$ , has $t_c=0$ to zeroth order in the $1/n$ expansion ( at which one generally works ) .
there are many different software programs that will do what you want , but planetarium software is very complex and takes a long time to learn before you can use it at its best . i am a technical writer and software support person for starry night software and , as part of my job , write a weekly article for space . com which is almost always illustrated using starry night software . i have been using various versions of starry night for over a decade and can generally get it to do what i want for my illustrations , though i sometimes need to fudge things a bit with other software . i doubt whether you will ever find one program which will do everything you want , so my suggestion is to do what i did : find a program that does most of it , and then work with that program until you can get the best from it .
i would like to create a planar ( rectangle shaped ) map of the entire ( both hemispheres ) sky , with stretching anything ( keeping all constellations etc . as seen in the sky ) . this is a physical impossibility . you simply cannot map a spherical entity ( the celestial sphere ) onto a plane without introducing some distortion . cartographers have developed many different projections in their efforts to solve this problem , but none is perfect . all of them are forced to introduce distortion at some point . the mercator projection suggested in another answer is notoriously inaccurate as you get closer to the poles , making greenland as large as south america , and stretching antarctica into an impossible shape . the best solution is to forget about mapping the whole sky , but to concentrate on smaller areas where the distortions are not as severe . this is what most star atlases and planetarium software programs do .
let 's see two charges $q_1$ and $q_2$ , we have to find electric field at some point between them . let 's assume $d$ to be the distance between the charges , which is constant and $x$ ( from the center of the line joining the two charges , $\frac{d}{2}$ from the chagres ) to be the distance where we want to measure electric field at . now , $\large e_{q_1x} = \frac{1}{4 \pi \epsilon} \frac{q_1}{\left ( \frac{d}{2}+x\right ) ^2}$ and $\large e_{q_2x} = \frac{1}{4 \pi\epsilon} \frac{q_2}{\left ( \frac{d}{2}-x\right ) ^2}$ total electric field at $x$ , $$e_{x} = \frac{1}{4 \pi \epsilon} \frac{q_1}{\left ( \frac{d}{2}+x\right ) ^2} + \frac{1}{4 \pi\epsilon} \frac{q_2}{\left ( \frac{d}{2}-x\right ) ^2}$$ i tried to make it as generalized as possible so you do not have to face problems with such type of problems in future . i hope it helps you and good luck for your exam ! i tried to attach a picture but because of low reputation i could not , sorry .
you misunderstood the classification i believe . let 's take an example . in class d and 1d , the classification tells you there are two possible vacua ( you understood this apparently ) . this is the famous $\mathbb{z}_{2}$ ensemble in the classification . next the classification tells you also that : at the boundary between the two gapped vacua , a majorana mode emerges . more precisely , an evanescent , localised mode emerges at the boundary between the two vacua , since they are gapped . for superconductors this emergent mode is a majorana one , thanks to the particle-hole symmetry . the construction is topological in essence : you map the problem of your differential equation to a group language , you recognise some properties ( the cartan class for instance , above the d one ) which allow you to classify your problem ( using equivalence relation , the more complicated part being to choose relevant criterion ) . then these classes have additional properties , like the boundary term discussed above . to be a little bit more clear , it is topological because you want to understand how the local solutions to your problem are gluing to some other local ones in order to make global ones . think about the möbius stripe that is locally differentiable , but not globally ( check out fiber bundle also , this is the mathematical object describing the property you are looking for ) . for the class d in 1d , you can find the two solutions in both vacua , but you can not glue them continuously without making a majorana , continuity here means making a global continuous solution , or wave-function . nevertheless , the topological property tells you that something strange exists , it will never give you the wave-function . you can think about group classifications in quantum mechanics : the groups of your molecule/lattice-cell tell you which interaction terms exist , they never give you their strength ( or the energy band splitting if you prefer to see the problem that way ) . a microscopic calculation is always required to get microscopic details . now , there are some tricks : since the characteristic length inside a superconductor is the coherence length $\xi=\hbar v_{f}/\delta$ ( $\delta$ gap parameter , $v_{f}$ fermi velocity ) , you can think as a decaying wave-function as $\psi_{\text{maj . }}\sim e^{-x/\xi}$ at the $x=0$ interface . this is cheating because this is not a wave-function , but it nevertheless gives the correct estimate for the real wave function which should be something like $\psi_{\text{maj . }}\sim e^{-x/\xi}\sin k_{f}x$ for instance ( $k_{f}$ fermi wave-vector ) . next , some limitations of the classification : it works for non-interacting systems only ( the coulomb interaction is not taken into account for instance ) it works for pure clean system only ( sometimes this issue is not trivial : for ( so-called topological ) $p$-wave superconductor the superconductivity itself is destroyed by disorder ; so perhaps the topological properties are conserved below the gap , but the gap vanishes due to disorder . . . ) so in practise it can never be applied to condensed matter problem . some people infer the topological property to avoid discussing these points . i think these points are the most relevant one , though .
it is true that all even-even nuclei ( hundreds of such isotopes have been measured ) have spin-0 in the ground state . this is due to what is often called the pairing effect . protons and neutrons are spin-½ particles , and they have a tendency to respectively pair up in proton-proton and neutron-neutron pairs so that their spin ( and orbital ) angular momentum adds to zero . this is a pillar of the nuclear shell model which says that we can predict many properties of a nucleus by examining the " unpaired " nuclei . the spin and parity of odd-even and even-odd nuclei are generally determined by the " valence nucleon " ( cf . the valence electron in the atomic shell model ) that is left when all pairing has occurred . odd-odd nuclei are not commonly found in nature , which we can describe to its tendency to convert the odd proton into a neutron ( or vice versa ) via $\beta$ decay to gain binding energy through the pairing force . as to " why ? " , that is a larger question . this effect is an empirical observation in nuclear physics , and it is seen to be helpful in predicting how nuclei behave over a large range of isotopes . the " pairing term " is a part of the semi-empirical mass formula , which does a fairly good job of predicting nuclear properties , at least among heavier and stable nuclei . the links to the nuclear shell model and the semi-empirical mass formula are probably good further reads .
i think you are referring to asteroid 2005 yu55 which is making an approach on november 8 . this article on nasa jpl 's asteroid watch site gives some details including : the asteroid 's surface is darker than charcoal at optical wavelengths . amateur astronomers who want to get a glimpse at yu55 will need a telescope with an aperture of 6 inches ( 15 centimeters ) or larger . which to me implies that it would not be visible with the naked eye . it is estimated size is 1,300 feet ( 400 meters ) . jpl has also posted an hour-long video here discussing the asteroid . another nasa site , the near earth object program , has better technical information which may help locate the asteroid . some excerpts : . . . the object will reach a visual brightness of 11th magnitude and should be easily visible to observers in the northern and southern hemispheres . the closest approach to earth and the moon will be respectively 0.00217 au and 0.00160 au on 2011 november 8 at 23:28 and november 9 at 07:13 ut . . . . the best time for new ground-based optical and infrared observations will be late in the day on november 8 , after 21:00 hours ut from the eastern atlantic and western africa zone . to get the exact coordinates , you can try nasa 's solar system dynamics site . the page for asteroid 2005 yu55 has orbital elements with a link to generate ephemeris .
it is everywhere ! astronomers have found that supermassive black holes live in the heart of galaxies and pull stars with incredible speeds , but that they are not strong enough to hold all the stars in the gigantic galaxies together . so what does hold them together in the spiral ? swiss astronomer fritz zwicky wondered why galaxies stayed together in groups , so he concluded that there must be something that no one detected before , and he named it " dark matter " . to prove it , scientists build virtual galaxies in computers , with virtual stars , and virtual gravity . astronomers were looking forward to this simulation experiment returning the integrated galaxy similar to our galaxy . instead , they found it disintegrated ! the gravity of the galaxy is not enough to hold it together , so scientists added virtual gravity from virtual dark matter to the simulation experiment . and it solved the problem , the gravity from dark matter held the galaxy together . so we can say that " dark matter is the master of the universe " . this is a short story about dark matter . to explain it in detail , i would need lots of books .
let $v^{\mu}=\frac{dx^\mu}{d\lambda}$ . then , $\epsilon=-v_\mu v^\mu$ . to see the conservation of this quantity along the geodesic we have to look at the covariant derivative along the curve , that is $$v^\nu\nabla_\nu\epsilon=-v^\nu\nabla_\nu ( v_\mu v^\mu ) =-v^\nu v^\mu\nabla_\nu v_\mu -v^\nu v_\mu\nabla_\nu v^\mu . $$ but metric compatibility implies that $\nabla_\nu v_\mu=g_{\mu \sigma}\nabla_\nu v^\sigma$ . also using the geodesic equation $v^\nu \nabla_\nu v^\mu=0$ , we find $$v^\nu\nabla_\nu\epsilon=0 , $$ which shows that $\epsilon$ is a conserved quantity .
quantum field theory is a general framework . there are many different kinds of field theories , the most well-known of which are qed ( quantum electrodynamics ) and qcd ( quantum chromodynamics ) . qed has been shown to agree incredibly well with experiment . the anomalous magnetic dipole moment of an electron , as computed using qed , agrees with experiment to 10 significant figures , the best agreement between theory and experiment of any physical theory in history . the large coupling constant of the strong force makes qcd much more difficult to deal with theoretically , and confinement makes it difficult probe experimentally .
yes , but one first has to generalize the classical 2-point brachistochrone problem $a \to b$ where the initial speed $v_a$ traditionally is zero , to the case where the initial speed $v_a$ may be non-zero but fixed . the solution to this initial speed brachistochrone problem ( assuming no friction ) is still a cycloid . now consider the 3-points brachistochrone problem $a \to b\to c$ with initial speed $v_a$ . the speed $v_b$ is given by energy conservation alone . thus the two segments $a \to b$ and $b \to c$ are completely decoupled , and they can be optimized as two independent 2-point brachistochrone problems with initial speeds $v_a$ and $v_b$ , respectively , leading to two corresponding cycloids $a \to b$ and $b \to c$ .
yes the limit of the velocity of light in vacuum can be described as the corner stone of special relativity . if it goes , special relativity goes , but it is important to keep in mind about context , and what " goes " means . in general physics theories are able to absorb small corrections to up to then absolute norms , and also in general , any new theories which emerge will include the thousand of successes of describing data by the use of special relativity . ( if this measurement is not due to a systematic error ) . after all special relativity is irrelevant to the physics of driving one 's car . the mechanics within the accuracies of speeds attained are newtonian and special relativity in the low speed limits becomes newtonian smoothly . this new measurement has a special context ( neutrino behaviour ) and a very small effect , so we wait for validation and see what theorists will devise to describe the situation . as for 1 ) . special relativity is one of the foundation stones of modern physics , not the only . another is quantum mechanics , then we go to field theories , general relativity , etc . 1 ) is not accurate . special relativity is a necessary but not sufficient condition for part of the modern theoretical framework . an equally important necessary condition for the physics theories we use to describe data is quantum mechanics . ( not sufficient ) since it is only in this one subset of thousands of reactions that a possible break down of the speed of light limit might be seen to be broken , certainly the damage is not as great as taking away a corner stone from a building .
they trust the method used by gps for geodesy , where the claim is they can go down to picosecond and cm accuracy if necessary ( for military use ) . gps error analysis takes even general relativity into account . in gps signal propagation ( pdf ) the systematic errors of a simple gps setup are given , but the opera experiment has more sophisticated use of four satellites . there has been no analysis of the gps systematics further in the paper published in the archive , that is why i say they trust it . among other corrections , gps corrects back to the velocity of light in vacuum . the meter is defined as a fraction of the velocity of light in vacuum ( the second is defined by the caesium clock at normal temperature and pressure ) . in my opinion , it is possible that some of the very sophisticated corrections of gps values might be systematically off , with an end result of effectively redefining the meter . this would not show up in navigation or geodesy because the lengths probed by the opera experiment are very large ( 732&nbsp ; km ) and the errors very small , 20&nbsp ; cm . a tiny systematic offset of what a meter is for gps would not show up in the normal world use of it , but it would show up in measuring the neutrino speed with this method .
the forces are never balanced , as there is only ever one force - gravity . the key is to remember newton 's second law : $f = ma$ . force and acceleration are paired , not force and velocity . knowing just an object 's current velocity tells you nothing about what forces are acting on it . there are two ways to see how the velocity goes to zero . either the initial impulse ( instantaneous transfer of momentum ) is depleted by a force applied over time , $$ m \delta v = \int f \ \mathrm{d}t , $$ or the initial kinetic energy is depleted by a force applied over a distance , $$ \frac{1}{2} m \delta ( v^2 ) = \int f \ \mathrm{d}x . $$ in both cases , the force of gravity is acting continuously to slowly cancel the initial velocity , and there is nothing that turns off this force at the apex of the trajectory .
first remove the " process " step from your diagram so that you are comparing two beams of light with $\cos ( \omega t ) $ . let 's say , for the sake of simplicity , that they can be considered plane waves , and your interferometer combines them at a small angle , so that you see a series of stripes ( bright and dark fringes ) on your screen . now add the " process " step , being careful not to disturb any other part of the interferometer . this causes a phase difference $\phi$ between the beams . the positions of the bright and dark fringes will shift . say $d$ is the distance between two crests or troughs ( brightest parts of two bright fringes or darkest parts of two dark fringes ) , and $\delta x$ is the distance by which the fringes shifted when you introduced the phase difference . then , $$ \phi\bmod{2\pi} = 2\pi\delta x / d$$ note that you will not be able to tell if $\phi &gt ; 2\pi$ .
the term you are looking for is premelting or " surface melting . " it is an observed phenomenon ( which could explain how ice skating works ) with some thermodynamic descriptions . basically what happens is the system is separated into two distinct phases , a solid ( ice ) and a vapor ( air ) . there is a surface energy associated with this interface . if it happens that the surface energy of ice to air , $\gamma_{i-a}$ , is more than the added surface energy of ice to liquid , $\gamma_{i-l}$ , and liquid to air , $\gamma_{l-a}$: $$ \gamma_{i-a}&gt ; \gamma_{i-l}+\gamma_{l-a} $$ then it is thermodynamically " cheaper " for the ice to melt on the surface , creating the separation layer . this layer is technically a quasi-liquid with the difference between the quasi-liquid and liquid primarily due to crystalline ordering ( i.e. . , the quasi-liquid still retains some structure ) .
i think you can estimate the maximal surface charge density as follows . the energy needed to remove an electron from a solid to a point immediately outside the solid is called work function $w$ . for aluminum $w$ is about $4.06-4.26$ ev . the thickness of the charged layer on the surface of a conductor is about several fermi lengths $$ \lambda_{f}=\left ( 3\pi^{2}n_{e}\right ) ^{-1/3} , $$ where $n_{e}=n/v$ is the total electron number density for the conductor . i think that the charge starts to drain from the surface of a conductor when $e\lambda_{f}$ is of the order of the work function , where $e=4\pi\sigma$ is the electric field near the surface : $$ ee\lambda_{f}=4\pi\sigma\left ( 3\pi^{2}n_{e}\right ) ^{-1/3}\sim w , $$ hence $$ \sigma\sim\frac{w}{4e}\left ( \frac{3n_{e}}{\pi}\right ) ^{1/3} . $$ about the question yrogirg . i think that the question is not quite correct . the charge are distributed on the surface of a conductor in such a way that the electric potential is a constant in the body of the conductor . the «stability» of charge on the surface is greatly dependent on the geometry of the object in question . sharp points require lower voltage levels to produce effect of charge «draining» from the surface , because electric fields are more concentrated in areas of high curvature , see , e.g. st . elmo’s fire .
typically one thinks of the sources as being at infinite past , and the detection at infinite future ; then a reversible s-matrix description applies . for photons , a corresponding treatment of sources and detectors can be found in mandel and wolf 's treatise on quantum optics . but their treatment does not give any hint on irreversibility . [ edit ] detection is always irreversible ; nothing counts as detected unless there is an irreversible record of it . there is no really good account from first principles how an irreversible detection event is achieved . from the 1999 article ' 'some problems in statistical mechanics that i would like to see solved'' by elliot lieb http://www.sciencedirect.com/science/article/pii/s0378437198005172 : the measurement process in quantum mechanics is not totally understood , even after three quarters of a century of thought by the deepest thinkers . at some level , the problems of quantum mechanical measurement are related , distantly perhaps , to the problems of non-equilibrium statistical mechanics . several models ( e . g . the laser ) indicate this , but the connection , if any , is unclear and i would like to see more light on the subject . but see http://arxiv.org/pdf/quant-ph/0702135 http://arxiv.org/pdf/1107.2138 a field theoretic discussion of irreversibility necessitates a statistical mechanics treatment . this more detailed modeling is done in practice in a hydrodynamic or kinetic approximation . they treat sources as generators of beams with an extended distribution in space or phase space , respectively . the dynamics of both descriptions is irreversible , and may be computed in terms of $k$-particle irreducible ( $k$pi ) feynman diagrams for $k=1$ and $k=2$ , respectively . the kinetic description is based on the kadanoff-baym equations in the 2pi schwinger-keldysh ( ctp ) formalism . the kadanoff-baym equations are dynamical equations for the 2-particle wightman functions and their ordered analoga , and are used in practice to model high energy heavy ion collision experiments . see , e.g. , http://arxiv.org/abs/hep-th/9605024 and the discussions in good reading on the keldysh formalism what is known about quantum electrodynamics at finite times ? the hydrodynamic description is based on the simpler 1pi approach , but it is ( to my knowledge ) used mainly theoretically ; see , e.g. , reviews of modern physics 49 , 435 ( 1977 ) and the papers http://arxiv.org/pdf/hep-ph/9910334 http://arxiv.org/pdf/hep-ph/0101178 http://arxiv.org/abs/gr-qc/9805074
in absence of a numerical value for that height , we shall call it $h$ . then , the energy necessary to lift $20 \textrm{ kg}$ at ‘normal earth conditions’ ( namely $9.81 \frac{\textrm{n}}{\textrm{kg}}$ acceleration due to gravity ) , is given by $$ e = m \times g \times h = 20 \textrm{ kg} \times 9.81 \frac{\textrm{n}}{\textrm{kg}} \times h $$ with your new height of fifty metres , we can then plug this into our equation and get $$ e = 9810 j\quad . $$ this energy is independent of the speed at which you lift the mass ( unless it has extra speed left over when reaching the height of $50 \textrm{ m}$ ) . furthermore , we are able to directly compute the minimum force necessary to lift an item of $20 \textrm{ kg}$ against the earth’s gravitational field , namely $f_{\textrm{min}} = 196.2 \textrm{ n}$ . a larger speed then requires a larger force to accelerate the mass to this speed . note that the above calculations assume newtonian gravity , and , more importantly , a frictionless system . while the assumption of newtonian gravity usually holds at the surface of the earth , i am looking forward to see a real world lifting device working without losses due to friction .
this is a good challenge ! here is maybe a solution : ordering a 1 meter long si rod with the correct doping level . it seems they are already able to make 2 mm diameter rods of 1 meter long out of pure silicon . ( http://www.goodfellow.com/catalogue/gfcat4j.php?ewd_token=5iqzqhxaluzqcqfmm0hg7c5opn6zxsn=o76p2nzwunxfcv3bozryzd6ivaflft )
the centre of a rainbow is where your shadow would fall . when you are standing on level ground your shadow is obviously on the ground so you only see part of the arc . if you are in an aeroplane or on a mountaintop or somewhere where your shadow would be above ground , you may be able to see the whole circle .
the two cars each have their own degrees of freedom , such that if the spring force is $f_s$ then $$ \begin{aligned} m_1 \ddot{x}_1 and = +f_s - c_1 \dot{x}_1 \\ m_2 \ddot{x}_2 and = -f_s - c_2 \dot{x}_2 \end{aligned}$$ you already mentioed that the spring force is $f_s = -k \left ( x_1-x_2 \right ) $ so now you have your ode .
i will not get into theoretical details -- luboš ad marek did that better than i am able to . let me give an example instead : suppose that we need to calculate this integral : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3}$ here $y_{lm}$ -- are spherical harmonics and we integrate over the sphere $d\omega=\sin\theta d\theta d\phi$ . this kind of integrals appear over and over in , say , spectroscopy problems . let us calculate it for $m_1=m_2=m_3=0$: $\int d\omega ( y_{30} ) ^*y_{20}y_{10} = \frac{\sqrt{105}}{32\sqrt{\pi^3}}\int d\omega \cos\theta\ , ( 1-3\cos^2\theta ) ( 3\cos\theta-5\cos^3\theta ) =$ $ = \frac{\sqrt{105}}{32\sqrt{\pi^3}}\cdot 2\pi \int d\theta\ , \left ( 3\cos^2\theta\sin\theta-14\cos^4\theta\sin\theta+15\cos^6\theta\sin\theta\right ) =\frac{3}{2}\sqrt{\frac{3}{35\pi}}$ hard work , huh ? the problem is that we usually need to evaluate this for all values of $m_i$ . that is 7*5*3 = 105 integrals . so instead of doing all of them we got to exploit their symmetry . and that is exactly where the wigner-eckart theorem is useful : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3} = \langle l=3 , m_1| y_{2m_2} | l=1 , m_3\rangle = c_{m_1m_2m_3}^{3\ , 2\ , 1} ( 3||y_2||1 ) $ $c_{m_1m_2m_3}^{j_1j_2j_3}$ -- are the clebsch-gordan coefficients $ ( 3||y_2||1 ) $ -- is the reduced matrix element which we can derive from our expression for $m_1=m_2=m_3=0$: $\frac{3}{2}\sqrt{\frac{3}{35\pi}} = c_{0\ , 0\ , 0}^{3\ , 2\ , 1} ( 3||y_2||1 ) \quad \rightarrow \quad ( 3||y_2||1 ) =\frac{1}{2}\sqrt{\frac{3}{\pi}}$ so the final answer for our integral is : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3}=\sqrt{\frac{3}{4\pi}}c_{m_1m_2m_3}^{3\ , 2\ , 1}$ it is reduced to calculation of the clebsch-gordan coefficient and there are a lot of , tables , programs , reduction and summation formulae to work with them .
aqwis , it would help in the future if you mentioned something about your background because it helps to know what level to aim at in the answer . i will assume you know e and m at an undergraduate level . if you do not then some of this explanation probably will not make much sense . part one goes back to dirac . in e and m we need to specify a vector potential $a_\mu$ . classically the electric and magnetic fields suffice , but when quantum mechanics is included you need $a_\mu$ . the vector potential is only defined up to gauge transformations $a_\mu \rightarrow g ( x ) ( a_\mu + \frac{i}{e} \partial_\mu ) g^{-1} ( x ) $ where $g ( x ) =\exp ( i \alpha ( x ) ) $ . the group involved in these gauge transformations is the real line ( that is the space of possible values of $\alpha$ ) if electric charge is not quantized , but if charge is quantized , as all evidence points to experimentally , then the group is compact , that is it is topologically a circle , $s^1$ . so to specify a gauge field we specify an element of $s^1$ at every point in spacetime . now suppose we do not know for sure what goes on inside some region ( because we do not know physics at short distances ) . surround this region with a sphere . we can define our gauge transformation at every point outside this region , but now we have to specify it on two-spheres which cannot be contracted to a point . at a fixed radial distance the total space of angles plus the gauge transformation can be a simple product , $s^2 \times s^1$ but it turns out there are other possibilities . in particular you can make what is called a principal fibre bundle where the $s^1$ twists in a certain way as you move around the $s^2$ . these are characterized by an integer $n$ , and a short calculation which you can find various places in the literature shows that the integer $n$ is nothing but the magnetic monopole charge of the configuration you have defined . so charge quantization leads to the ability to define configurations which are magnetic monopoles . so far there is no guarantee that there are finite energy objects which correspond to these fields . to figure out if they are finite energy we need to know what goes on all the way down to the origin inside our region . part two is that in essentially all models that try to unify the standard model you find that there are in fact magnetic monopoles of finite energy . in grand unified theories this goes back to work of ' t hooft and polyakov . it also turns out to be true in kaluza-klein theory and in string theory . so there are three compelling reasons to expect that magnetic monopoles exist . the first is the beauty of a deep symmetry of maxwell 's equations called electric-magnetic duality , the second is that electric charge appears to be quantized experimentally and this allows you to define configurations with quantized magnetic monopole charge , and the third is that when you look into the interior of these objects in essentially all unified theories you find that the monopoles have finite energy .
you may always add the numbers in front of the units , and if the units are the same , one could argue that the addition satisfies the rules of dimensional analysis . however , it still does not imply that it is meaningful to sum the temperatures . in other words , it does not mean that these sums of numbers have natural physical interpretations . if one adds them , he should add the absolute temperatures ( in kelvins ) because in that case , one is basically adding " energies per degree of freedom " , and it makes sense to add energies . adding numbers in front of " celsius degrees " , i.e. non-absolute temperatures , is physically meaningless , unless one is computing an average of a sort . this is a point that famously drove richard feynman up the wall . read judging books by their covers and search for " temperature " . he was really mad about a textbook that wanted to force children to add numbers by asking them to calculate the " total temperature " , a physically meaningless concept . it only makes sense to add figures with the units of " celsius degrees " if these quantities are inteprreted as temperature differences , not temperatures . as a unit of temperature different , one celsius degree is exactly the same thing as one kelvin . if you interpolate or extrapolate a function of the temperature , $f ( t ) $ , you do it as you would do it for any other function , ignoring the information that the independent variable is the temperature . results of simplest extrapolation/interpolation techniques will not depend on the units of temperatures you used .
first , it turns out that there are no uniform gravitational fields so the equivalence principle holds only locally . but , for the sake of argument , let 's assume that a uniform gravitational field can exist . now , consider the situation where an astronaut is in a rocket and the rocket 's accelerometer reads a constant , non-zero value . according to the equivalence principle , the following two perspectives are equivalent and thus indistinguishable by experiment : ( 1 ) there is no gravitational field and the rocket is absolutely accelerating ( due to some type of motor ) ( 2 ) the rocket is stationary ( due to the same motor ) in a uniform gravitational field note that in case 2 , the stars are freely falling in the uniform gravitational field and thus , their speed relative to the stationary rocket increases with time just as in the first case . so , it is not the case that you can distinguish the two perspectives . in both cases , the stars " in front " of the rocket will become more and more blue-shifted with time .
the quantity $mv^2$ is not constant . you can argue this by an example . consider a ball in a uniform gravitational field . if you drop the ball , the quantity $mv^2$ changes as it falls , so it is not constant . or , more simply , $mv^2$ is twice the kinetic energy . think of any system where the kinetic energy changes , and you have shown that the quantity can not be constant . so what is wrong with your mathematical argument ? well , you write $u=\int_c \nabla t\cdot d\vec r$ , but that is not true . first , you have a sign error . ( try replacing $\nabla t$ with $-\nabla u$ and you will see why . ) if you correct the sign error , you had get $u=-\int_c \nabla t \cdot d\vec r = -t + k_1$ . now if you add $t$ to both sides you get $u+t=k_1 , $ which at least is not wrong , but does not really gain you anything since $k_1 = e$ . the definition of change in potential energy for a conservative force $\vec f_c$ is $\delta u = -\int \vec f_c \cdot d\vec r$ . i suppose you might sometimes see this written without the negative sign , but in that case , the force in the integrand would be the " minimum " force exerted by an external agent in order to overcome the conservative force . ( this is a sloppy explanation , and best left for a separate physics . se question . ) the bottom line is , if you are working with some conservative force field $\vec f_c$ , you need the negative sign .
water at 20c and atmospheric pressure has a higher symmetry than ice vii at 100c and 10gpa pressure . you may well point out this is cheating because the pressure is different in the two cases . however this makes the point that temperature is not the only variable . if you are looking at a phase transition between a disordered and ordered phase then you need to consider the gibbs free energies of the two phases . the phase with the lower gibbs free energy is the one that will form . the gibbs free energy is defined as : $$ g = h - ts $$ and a negative gibbs free energy change means the phase change occurs ( give or take a few kinetic barriers ) . generally speaking , for a change from ordered to disordered the entropy increases . i.e. $\delta s$ is positive . if you consider an isothermal change you get a negative $-t\delta s$ contribution that gets more negative as the temperature gets higher , so you would expect that in general changes from ordered to disordered will occur at increased temperature . this does not mean it is impossible to get a disorder to order transition with increasing temperature , but the enthalpy , $h$ , would have to have an odd temperature dependance to outweight the entropic effect .
planck units $\hbar=g_n=c ( =k_b ) =1$ see f.e. http://www.scholarpedia.org/article/bekenstein-hawking_entropy equation ( 11 ) where $t=\kappa/ ( 2 \pi ) $ ( in planck units )
poincaré invariance is a fundamental requirement of relativistic ( quantum ) physics . in particular if $u_g : {\cal h} \to {\cal h}$ represents the ( non-necessarily linear ) action of a poincaré transformation $g$ on ( normalized ) vectors $\psi$ of the hilbert space $\cal h$ of the considered system , transition probabilities have to be preserved : $$|\langle u_g ( \psi ) | u_g ( \phi ) \rangle|^2 = |\langle \psi | \phi \rangle|^2 \quad \forall \psi , \phi \in {\cal h}\: , ||\psi||=||\phi||=1\: . \quad ( 1 ) $$ a celebrated theorem due to wigner establishes that a ( bijective ) map $u : {\cal h} \to {\cal h}$ verifying ( 1 ) must necessarily be linear and unitary or anti linear and anti unitary , depending on the physical nature of the transformation . concerning representations of poincaré group $\cal p$ , by definition they have to satisfy , in addition to ( 1 ) , ${\cal p} \ni g \mapsto u_g$ with $u_g u_h = u_{g\cdot h}$ ( $^*$ ) and $u_e = id$ , where $\cdot$ is the group product in $\cal p$ , just in view of the definition of group representation . in principle each $u_g$ has to be unitary or anti unitary . if $g\in \cal p$ belongs to the proper orthochronous subgroup ${\cal p}_+^\uparrow$ , it can always be decomposed as $g= h\cdot h$ where $h$ still belongs to the same subgroup . therefore $u_g= u_{h} u_{h}$ , thus $u_g$ must be unitary ( even if $u_h$ is anti unitary , as the composition of a pair of anti unitary operator is always unitary ) . we conclude that the orthochronous poincaré group ${\cal p}_+^\uparrow$ ( and consequently the orthochronous lorentz group $so ( 1,3 ) ^\uparrow$ ) can only be represented by unitary operators in quantum theories , when the action of the group is on states . non unitary representations arise dropping the last requirement . for instance dealing with dirac or weyl spinors . ( $^*$ ) actually a phase could take place , since states are represented by normalized vectors up to phases : $u_gu_h = e^{i\alpha ( g , h ) }u_{g\cdot h}$ , however it does not change the result of the subsequent reasoning . as a matter of fact , it is possible to prove that continuous ( projective ) unitary representations of ${\cal p}_+^\uparrow$ are not affected by such phases , differently form representations of galileo 's group where those phases play a crucial role .
key is to notice that your steps provide you with a unit length as well as a unit time . so , let 's measure distance in $steps$ and time in $ticks$ , with your speed being $1 \ step/tick$ . the length of the train is $x$ steps , and its speed is $v \ steps/tick$ ( $v&lt ; 1$ ) . it follows that $$x \ + \ 18 \ v \ = \ 18 $$ $$x \ - \ 11 \ v \ = \ 11 $$ adding 11x the first equation to 18x the second yields $29 x = 396$ . the train is $396/29 \ steps$ long . you also need to check if indeed $v &lt ; 1 \ step/tick$ . leave that to you to demonstrate .
an orbit is stable because of conservation of angular momentum . suppose we start with an object in an exactly circular orbit and slow it down slightly . that means it is moving at less than orbital velocity so it starts to fall inwards . however as its distance to the sun decreases the tangential component of its velocity has to increase to conserve angular momentum . so as the object nears the sun it moves faster and faster , and at its closest approach to the sun it is moving at well above orbital velocity so it starts to move outwards again . you end up with an elliptical orbit : ( this diagram shamelessly cribbed from google images ) it is actually very difficult to get something orbiting a star to fall into it , because you have to reduce the tangential velocity to zero . at the distance of the earth from the sun the orbital velocity is 108,000 km/h . you would have to slow the earth by this amount to make it fall into the sun , and fortunately no meteorite is likely to do that . on a side note , nasa recently sent the messenger spaceship to study mercury , and getting the ship to mercury was hard because of the need to shed all that orbital velocity . even though mercury is a lot closer to the sun than the earth is you can not just fall there . messenger had to use several gravity assists to shed enough speed to allow it to orbit mercury .
notice that $a$ is a linear operator on $\mathbb r^n$ . suppose that $a$ is singular , namely $\det a= 0$ , then the kernel of $a$ is nontrivial . in other words , there exists some nonzero $v\in\mathbb r^n$ for which $av=0$ . it follows that the kinetic energy vanishes for this nonzero $v$ . there is nothing " wrong " with this mathematically speaking , but it is physically pathological because the kinetic energy represents energy due to the magnitude of the motion of the object , and we therefore expect that any state of the object for which $\dot q_i\neq 0$ for some $i$ should be assigned a nonzero kinetic energy .
i know black conducts heat while white reflects it . the correct term is " black absorbs light while white reflects it " . we have named colors of light we see in the visible spectrum . white reflects most of the energy falling from the visible spectrum , black absorbs it . when the energy of light is absorbed it turns into heat . any material painted black will absorb this heat further and its temperature will be raised but it will depend on the material how far the heat is transferred . if it is metal painted black , metal is a good conductor of heat and will distribute the energy fast on the whole body . but they are colors after all . they change the surface properties of materials on which they are painted thus changing the ability of absorption and emission of radiation . the energy coming from the sun covers a much larger electromagnetic spectrum than the visible . the visible has about half of the energy coming from the sun on the surface , as seen in the link . so a metal door in the sun will transfer the heat of the visible spectrum to the interior if painted black , will reflect it back and keep the interior cooler if painted white . it is a good reason for painting roofs and walls white in hot countries . a white car is also better in hot countries for this reason . it is not always sure that the color properties ( absorption/reflection ) are followed by the invisible part of the sun spectrum , infrared or ultraviolet . each paint has to be studied as far as its response to the impinging radiation to be used efficiently for thermal protection .
yes , it is ok , but it is an explanation that has been stripped down to bare bones , and leaves out quite a bit . here 's a little more to help prop up the explanation . first , it is important to realize that in a condensed phase like a solid or liquid the light is not interacting with molecules in isolation . light is interacting with all of the molecules . this makes a big difference . light absorbed and re-emitted by a single molecule can go off in ( almost ) any direction . next , remember that a photon is an excitation of a complete electromagnetic field . it is , unfortunately , often not helpful to think of a photon as a particle that exists at a particular place in space . like all particles in quantum mechanics , there is a chance that it could exist anywhere . individual interactions , on the other hand , can occur with a particular molecule at a particular location . it is best to start thinking about your question in the realm of classical physics , and then modify it later to include quantum mechanics . classically , when light interacts with a molecule , the electron is set into vibrational motion . the energy that the field gave to the molecule will stay with the molecule for a while while this oscillation occurs . the molecule , then , is a radiator and can generate its own light . that is the classical picture of the delay experienced by light during an interaction . after this re-emission , the light will travel at $c$ . how does it know to go straight ? here 's where we need all the other molecules in the liquid . the incident field has a particular spatial pattern , say a sine pattern , and it excites in the molecules an oscillation pattern that exactly matches . the incident light was traveling in a particular direction with a well-defined phase front , and so too does the pattern of oscillation in the liquid . the light re-emitted from the molecules will also share that same pattern , although it will be delayed in time relative to the incident light . the re-emitted light adds to the portion of the incident light that passes unaffected . but the phase fronts , and hence the direction of travel , is parallel to that of the incident wave . it goes off in the same direction . photons : almost the same story . recall that a photon is an excitation of the light field . instead of exciting the molecules into oscillation , the molecule temporarily absorbs a quantum of energy from the field ( the " photon" ) , and the molecule is raised to an excited state . the molecule lives in this state for a short period of time , and then the energy is returned to the field ( "photon emission" ) . but the business about phase fronts and directions still holds . the energy is added to a field propagating in the original direction , slightly delayed . ( another detail we are leaving out . the absorption and re-emission i describe is a very fast process called " virtual transition " . it is fast enough that the uncertainty principle $\delta e \delta t \leq \hbar/2$ allows energy conservation to be temporarily violated . so the frequency of the incident light does not have to match an absorption frequency of the molecule in this process , and the explanation works for transparent media . ) note carefully that i refer to a photon as an excitation of the field , rather than as a particle . when the interaction between light and something else occurs at a particular place , as when it hits a particular pixel in a digital camera , it looks for all the world like a particle hit the pixel . but a more useful way of thinking about it is that the light field exists everywhere , but the interaction occurs at a particular place . hope that helps !
update to address new questions . the answer to this question is no . at least if you take the question purely formally . only theories such as classical field theory , quantum field theory and continuum mechanics are field theories ( you generally recognize them by having continuous degrees of freedom ; also they usually have the word field in the title :- ) ) . but physically , lots of different theories may be equivalent , or may be approximations of some other theory , so there are many connections among them ( this is the point i was trying to illustrate , but maybe i overemphasized it ) . difference between qm and qft is essentially the same as between classical mechanics and classical field theory . in the mechanics you have just a few particles ( or more generally , small number of degrees of freedom ) , while fields have an infinite number of degrees of freedom . naturally , field theories are a lot harder than the corresponding mechanics . but there is a connection i already mentioned : you can see what happens when you let the number of particles grow arbitrarily large . this system will then essentially behave as a field theory . so in a sense , we can say that field theory is a large $n$ ( number of degrees of freedom ) limit of the corresponding mechanical theory . of course , this view is very simplified , but i do not want to get too technical here . field theory is a theory that studies fields . now what is a field ? i suppose everyone should be familiar with at least some of them , e.g. gravitational or electromagnetic ( em ) field . now , how do you recognize that object is a field ? well , essentially , you look at how complicated the object is . to make this more precise : main objects of study of classical mechanics are point particles . all you need to keep track of them is just few parameters ( position , velocity ) . on the other hand , consider the em field : you need to keep track of the data ( electric and magnetic field vector ) in every point of the universe , so there is infinitely many parameters of this system ! this is what i meant by system being large : you need a lot of data to describe it . now , it might seem that something is amiss . you do need a lot of data to describe real objects ( just think of how many atoms there are in the grain of sand ) . so are ordinary objects fields ? yes and no , both answers are correct depending on your point of view . if you consider a massive object as essentially being described by few parameters ( like center of mass velocity and moment of inertia ) and completely ignore all information about atoms then it is clearly not a field . nevertheless , at the microscopic level , atoms wiggle around and even the grain of sand really is as complicated object as any em field ( not to mention that atoms themselves produce em field ) , so it is certainly correct to call them that . now let us see where our definition of field takes us . let 's talk about quantum mechanics for a while . what about two quantum particles ? is it a field ? well , clearly not . what about three ? still not . and what if we keep adding particles so that there will be a huge number of them ? well , it turns out that we will get a quantum field ! this is precisely the correspondence between e.g. photons and quantum em field . you can either look at em field as being described by vector of electric and magnetic field at every point as in the classical case , or you can instead reorganize your data so that you keep track of what kind of photons you have . it is useful to carry both pictures in head and use the more appropriate one . there is also a subject of continuum mechanics . there you can also start with particles ( describing atoms in some real object , e.g. water ) and because there are so many of them , you can again reorganize your data , consider the object as being essentially continuous ( which real objects surely are at least unless you look at them with a microscope ) , and instead describe them by parameters such as pressure and temperature at every point . to summarize : the field theory is essentially about dealing with large objects . however , when we are looking at the problem with particle hat on , we usually do not say it is a field . for instance , when describing real objects as consisting of atoms , we are usually talking about statistical mechanics , or condensed matter physics . only when we move to the realm of continuum mechanics , we say that there are fields . there is much more to be said on the topic but this post got already too long so i will stop here . if you have any questions , ask away !
i suppose that " periodic movement " need not be oscillatory . for example , consider an object that moves right for one minute , stops for one minute and repeats . this would be periodic movement in the sense of occuring at regular intervals . and , while oscillation implies back and forth movement ( if we are still talking about movement ) , it need not be periodic .
your equation is the right solution to schrödinger 's equation in the momentum-energy representation . however , it is only that simple for schrödinger 's equation with no potential , $v ( x , y , z ) =0$ . if it is zero , the solution ( or , similarly , the reformulation of the equation ) is as easy as the algebraic relationship you wrote - but it is also uninteresting for the same reason . the interesting cases have e.g. the coulomb potential $k/r$ or the harmonic oscillator potential $kx^2/2$ and they can not be " solved " in the simple way you sketched . for a nonzero potential , the problem is genuinely equivalent to a partial differential equation . however , that does not mean that it is the only way in which the problem may be formulated or solved . both the harmonic oscillator and the hydrogen atom may be solved ( i.e. . their spectrum may be found ) algebraically , by the creation and annihilation operators in the harmonic oscillator case , or by a hidden $so ( 4 ) $ symmetry in the hydrogen atom case . a general schrödinger 's equation in quantum mechanics is really an ordinary differential equation for the state vector ; the " spatial derivatives " only appear as the action of particular operators on the hilbert space . some of these operators - e.g. the momenta in the position representations - are conveniently represented as partial derivatives with respect to spatial coordinates but that is only the case if we use a " continuous basis " for the hilbert space .
i know this is an old question , but for the benefit of people visiting here wondering what the answer was , here it goes : a droplet can stay at rest on an inclined plate because of small heterogeneities on the surface . this can either be a small roughness ( of the order of nano/micrometers ) or `dirty ' spots where the surface chemistry is locally different . the existence of these heterogeneities allows the droplet to have a different , bigger , contact angle at the downhill side then at the uphill side ( this means that a perfectly smooth , clean surface will not be able to hold any droplets , this can most easily be shown by numerical simulation , because a perfectly smooth and clean surface is very hard to find/make experimentally ) . this difference in contact angle , thus surface energy , translates into a force pointing upwards , which is therefore able to counteract the gravitational force . because there is a certain maximum to the front and rear contact angles ( which depends on the surface roughness/dirtyness , higher roughness/dirtyness will give a larger difference between the two ) the droplet will at some point start to move , at which point you get into the paradoxical discussion that ron maimon was talking about . the balance that you get looks like this : $$ \rho v g \sin \alpha = k \gamma w ( \cos\theta_u - \cos \theta_d ) $$ where , in order of appearance , you have the density of the liquid , the volume of the droplet , the gravitational acceleration , the sin of the tilt angle , a `fudge ' factor of o ( 1 ) depending on the detailed shape of the droplet , the width of the base of the droplet and the cosines of the uphill contact angle and the downhill angle . the phenomenon of the difference in downhill and uphill contact angle is called contact angle hysteresis . if you want to know more about it , you can google for it : there are plenty of good sources out there . to summarize : local heterogeneities on the surface allow a difference in surface energy at the front and rear of the droplet , thus introducing a force that can counteract gravity .
the non-relativistic expression for the wave operators $$ ω±=lim{_{t→∓∞}}u ( t ) {_{full}}u{_{0}} ( t ) , $$ needs revision in field-theoretic situations since usually the free and interacting fields act in different hilbert spaces . an early example is given in " asymptotic conditions and infrared divergences in quantum electrodynamics " by p . p . kulish and l . d . faddeev . theoretical and mathematical physics 1970 , volume 4 , issue 2 , pp 745-757 . thus i expect that there is no simple answer to this question .
it is not particularly meaningful , in this case , where the energy ' comes from ' . it is called the zero-point energy and it is simply there , with not a lot to be done about it . there are some ways to justify it , of course . say you have a square potential well , whose bottom has $v=0$ , and say you try to put a particle with zero kinetic energy in the centre of the well . having zero kinetic energy means having zero momentum , and de broglie 's relation then demands that the particle 's wavefunction have infinite wavelength . however , having such a large wavelength means that with some probability the particle is in regions with nonzero potential energy , and therefore its total energy is positive . there is just no way to place the particle in a state with zero total energy . the way to avoid this is , of course , having a wavepacket just small enough that it will fit comfortably inside the well . however , having a limited spatial extent means having a finite wavelength , and therefore ( by de broglie 's relation ) a nonzero momentum and kinetic energy . you note rightly that if you were to manually ( and adiabatically ) decrease the size of the well , you would have to perform work to push the particle in , and this would increase its zero-point energy . if you have two particles in different wells , their difference in energy can best be thought of as a potential to perform work : there exists a physical transformation of the narrower well ( i.e. . adiabatically allowing it to expand ) which takes it to the state of the other particle and at the same time allows it to perform work . you are no doubt aware of the casimir effect , in which the effects of the zero-point energy are measurable . this is not simply some mathematical result that is an oddity within the theory ; you can indeed perform work using simply the vacuum energy .
your observation is linked to the " optical window in biological tissue " . like you already suspected , the absorption of blue light in tissue is higher than the absorption for red light . best read the related wikipedia article , where all relevant effects are nicely illustrated . http://en.wikipedia.org/wiki/optical_window_in_biological_tissue
the radius of a non-rotating black hole is $$r_s = \frac{2gm}{c^2} \tag{1}$$ where $m$ is the mass , $g$ is newton 's constant , and $c$ is the speed of light . this is the distance from the center of the black hole to the event horizon . the event horizon is the surface that traps light and objects , it separates inside and outside the black hole . anything that passes inside the event horizon can never escape , even light . this is why it is said that the escape velocity at the surface of a black hole is $c$ . it is also the case that any object with $r &lt ; r_s$ is a black hole . but since the mass should be roughly proportional to the volume , and the volume is proportional to $r^3$ , anything heavy enough will form a black hole . therefore from ( 1 ) the proper answer to your question is : because they are very massive , not because they are small .
hard disks use magnetic storage , http://en.wikipedia.org/wiki/magnetic_storage i.e. the information is stored in the south/north orientation of small pieces of the magnetic medium ( also the case for tapes in the old tape recorders ; and credit cards ; these applications differ by the " geometry " how the bits are arranged but not by the essence how a bit is stored ) . a stronger magnet in the head is able to remagnetize the pieces of the surface ; a weaker magnet is able to " feel " in what way the pieces are magnetized . usd flash disks and ssds use flash memory , http://en.wikipedia.org/wiki/flash_memory which evolved from eeprom memory chips . flash memory is composed of flash memory cells which primary contain the floating-gate transistors ; such cells have a control gate ( cg ) and de facto isolated floating gate ( fg ) . fg can keep electric charge for years and the electric field from that affects the threshold voltage of the cell ( voltage at which the cell becomes conductive ) . some extra operations are needed to erase and rewrite the chips which can not be done indefinitely ; after less than a million of erasures , the cell breaks down . there are many other physical mechanisms in which the information is being stored . all the " relatively easily " rewritable technologies use some kind of variable electric or magnetic fields that may be produced by matter . compact disks ( cd/dvd ) use optical information and so on .
a quick google will find lots of analyses of interplanetary travel under constant acceleration . the best one i found is here , and gives results for travel between earth and mars . it even provides matlab code to do the calculation , and you could easily modify this to calculate travel between different planets . we are not supposed to just give links without discussion , but i am not sure how much there is to say . unsurprisingly there is no simple analytical solution to the problem so a numerical solution is necessary . the trajectory ends up looking like an s . i have nicked one of the pictures from the site to show this : green shows the earth 's orbit , cyan shows the orbit or mars , the red line is the constant outward acceleration and the blue line is the constant deceleration . the journey takes around 6 days .
you state that the source produces a pulse per second . a hum , on the other hand , is a sound with a frequency of several hundreds of cycles per second . if that source is particularly violent it may be able to get a sound going in a very good resonator . ( example : i have a guitar , and i have noticed that when i sneeze and the guitar happens to lie right next to me sometimes one or two strings are audibly triggered . ) anyway , to have a chance the resonator must be one that is very responsive . that is , the resonator must be shaped ideally for one particular frequency , so that all of the energy that it does aborb goes to that one frequency . the cube that you describe is definitely a very poor resonator . i think you would have a hard time getting the sound from any source to resonate in it at all . for comparison , bottles tend to have a narrow resonance response . you blow over the top of an open bottle , and when you hit the right angle of blowing just over it , and with the right airflow , you get a resonance in that bottle . hum that same note to the bottle and your humming gets amplified . that amplification shows the resonator 's responsiveness . summerising , to have a chance of producing some effect you need a resonator that is very , very responsive .
i am not sure what your level is , but i think you will find that material on accelerator structures generally requires upper division e and m . however in terms of concepts , this overview and these pictures ( from illinois tech ) will go a long way toward answering the questions you have lined out . in a drift-tube linac , the drift tubes sit inside a larger rf cavity and act as shields . thus , the electric field inside a drift-tube can be approximated as zero . the gaps between drift tubes will still have an e-field , however , which varies sinusoidally in the direction of travel . when the particle is in the gap between drift tubes , it will experience an e-field and undergo acceleration . let 's say we are accelerating protons . we want to accelerate them in a particular direction , but half the time , the e-field in the gap is pointing in the wrong direction . hence we can not make a continuous beam with this particular technology ; we have to break the beam up into " bunches " . this is by-and-large a limitation of all rf accelerating structures . a truly comprehensive reference for linacs is the slac " blue book " , which is now freely available from slac . it is somewhat dated , but covers all aspects of building a linear accelerator in great detail . there are also a good number of slac publications that deal with linacs , such as slac-pub-7802 . you may also consult material presented by the us particle accelerator school . finally , there is the rf linac textbook by wangler .
you must first rewrite the old partial derivatives in terms of the new ones . a priori , they are some linear combinations with coefficients that could depend on the spacetime coordinates in general but here they do not depend because the transformation is linear . the rules $$ t'=t , \quad x'=x-vt , \quad y'=y $$ get translated to $$ \frac{\partial}{\partial t} = \frac{\partial}{\partial t'} - v \frac{\partial}{\partial x'}$$ $$ \frac{\partial}{\partial x} = \frac{\partial}{\partial x'}$$ $$ \frac{\partial}{\partial y} = \frac{\partial}{\partial y'}$$ if you write the coefficients in front of the right-hand-side primed derivatives as a matrix , it is the same matrix as the original matrix of derivatives $\partial x'_i/\partial x_j$ . if you do not want to work with matrices , just verify that all the expressions of the type $\partial x/\partial t$ are what they should be if you rewrite these derivatives using the three displayed equations and if you use the obvious partial derivatives $\partial y'/\partial t'$ etc . if you simply rewrite the ( second ) derivatives with respect to the unprimed coordinates in terms of the ( second ) derivatives with respect to the primed coordinates , you will get your second , galilean-transformed form of the equation . i have verified it works – up to the possible error in the sign of $v$ which only affects the sign of the term with the mixed $xt$ second derivative . i guess that if this explanation will not be enough , you should re-ask this question on the math forum .
from the dutch national science quiz 2006 ( my translation ) : question 14: you put a duvet cover together with smaller laundry in the washing machine . why , at the end of the program , all smaller laundry has twisted itself in the duvet cover ? due to the left-and-right cycling of the drum washing machines predominantly cycle one way . to loosen the laundry , the drum sometimes abruptly turns the other way . due to this opposite movement , suddenly a few liters of water bumps very forcefully into the laundry , and therefore the opening of the duvet cover will come to lie completely open . smaller laundry falls in one piece at a time . as soon as the machine goes into centrifuge mode , the smaller laundry pieces are being pushed in further . it is very difficult for laundry that is in the duvet cover to get out . nb : the false answers were ( my translation ) : because small laundry pieces are more sensitive to water vortices than large items due to the area ( size ) difference between the duvet cover and smaller laundry i would like to add , from personal experience , that it is very rare that all smaller laundry ends up in the cover . sometimes half of it will get in there , and sometimes just the odd sock . but perhaps that is just me and my machine .
i think you have got the whole reason . we have good evidence that meteor showers are caused by lots of tiny pieces of fluffy comet detritus , which has no chance of reaching the ground ; a bright bolide , however , is much more likely to have a substantial rocky or metallic mass .
measuring $w$ is actually what i do for a living . the current best measurements put $w$ at $-1$ but with an uncertainty of $5\%$ , so there is a little room for $w \ne -1$ models , but it is not big and getting smaller all the time . indeed , we had all be thrilled if , as measurements got more precise , $w \ne -1$ turns out to be the case because the $\lambda$cdm model ( where dark energy is just a scalar correction to general relativity ) is just horribly arbitrary and we had much rather get evidence for something new and exciting . you ask if models can reproduce a $w \approx -1$ and then ask about free parameters in such models . you are on the right track with asking about free parameters . there are some ways in which you can explain that $\omega_\lambda$ and $\omega_m$ are fairly close to each other today ( quintessence models do this with tracker models , inhomogeneity inducing corrections to the friedmann equations can do this because the inhomogeneities get stronger as structure grows ) while the $w \approx -1$ value is not a particularly unnatural value for many models ( it is for some , and those are already ruled out ) . so you are in an odd situation where you could certainly imagine , say , a quintessence model with say $w \approx -1$ , but you are not too tempted to because $\lambda$cdm is just much simpler . to get out of this impasse , you are on the right track when you ask about free parameters ( or loose ends as you call them ) . quintessence for example would have a total of $4$ , which represents the number of free parameters in your quintessence field that have an impact on the rate of evolution , whereas $\lambda$cdm has only one . this is why , for the time being , we go with $\lambda$cdm : our current probes can only constrain a single parameter , namely the current effective equation of state , and it is consistent with $\lambda$cdm , which then plays the role of a sort of " placeholder " model given that it requires the least number of degrees of freedom . current constraints on the evolution of dark energy properties are a joke , and you might as well consider them non existent . the image below is a confidence contour in the $w$ vs $w_a$ plane given the latest cmb , bao , and supernova data . $w_a$ is a somewhat arbitrarily chosen parameter ( $w ( a ) = w_0 + ( 1 - a ) w_a$ where $a$ is the scale factor ) but it gives you an idea of your probe 's sensitivity to the evolution of $w$ . future probes may be able to meaningfully constrain $w_a$ , and if it turns out to be different than $0$ , then that'll rule out $\lambda$cdm and open up a whole new ball game . remember , $\lambda$cdm corresponds to a static $w = 0$ . it'll be hard , since we will need to go to much higher redshifts while maintaining similar levels of accuracy in our measurements , if not improvements because given its current properties ( and whatever its evolution ) dark energy is weaker in the past ( $\omega \propto a^{-3 ( 1+w ) }$ , so while a $w \approx -1$ dark energy maintains a similar density , matter density , which corresponds to $w = 0$ , increases as you look back in time ) . being able to better constrain this evolution is the name of the game if you are interested in ruling out the current standard model . the classification of possible explanations that your review suggests is indeed the best way to broadly categorize the proposed solutions . a popular model which modifies the right hand side is quintessence . quintessence employs a common solution ( or common attempt at a solution ) to many problems : when in doubt , throw in a scalar field . no known field satisfied the required properties , so it would most certainly lie outside the standard model of particle physics . in general , one can consider the impact on expansion of the addition of any fundamental field to the lagrangian of particle physics . the only condition this field would have to satisfy for this field to be a candidate for dark energy is $w &lt ; -1/3$ . for a quintessence field $\phi$ with a potential field $v ( \phi ) $ , $p$ and $\rho$ are such that $w$ is given by : \begin{eqnarray} w_\phi and = and \frac{p}{\rho} and \mbox{ ( by definition ) } \\ and = and \frac{\frac{1}{2} \dot{\phi}^2 - v ( \phi ) - \frac{1}{6a^2} ( \nabla \phi ) ^2}{\frac{1}{2} \dot{\phi}^2 + v ( \phi ) + \frac{1}{2a^2} ( \nabla \phi ) ^2} and \mbox{ ( $p$ and $\rho$ for quintessence fields ) } \\ and = and \frac{\frac{1}{2} \dot{\phi}^2 - v ( \phi ) }{\frac{1}{2} \dot{\phi}^2 + v ( \phi ) } and \mbox{ ( use spatial homogeneity to get rid of $\nabla \phi$ terms ) } \\ and \stackrel{\text{$\dot{\phi} \rightarrow 0$}}{\approx} and -1 and \mbox{ ( for a slow rolling field ) } \end{eqnarray} the couple of $\nabla \phi$ terms in there disappear when you invoke a homogeneous universe ( using the comological principle ) . hence , many models with a $v ( \phi ) $ that yields a $w_\phi &lt ; -1/3$ have been considered simply by constructing the proper $v ( \phi ) $ , and it is not hard nor particularly unnatural to construct models where $w \approx -1$ today , simply by having a slow rolling field where $\dot{\phi}$ is small and therefore $w_\phi \approx -1$ . the difficulty lies in making such models distinguishable from the standard case of a $w = -1$ that is static in time , given our experimental insensitivity to the time evolution of the field . the field has not necessarily been in slow roll for the history of the universe . on the whole this is actually a whole class of models , and many models and couplings with matter can be invoked to explain all sorts of things about dark energy ( like the rough similarity of $\omega_\phi$ and $\omega_m$ today , and why this happens when the field is slowly rolling etc etc ) . again though , we are not yet sensitive to any of the observational implications of any of this . in the near future , we may be able to constrain 2 parameters of freedom with regards to the evolution of expansion , and not the 4 that are required to fully characterize a quintessence model , meaning that even then we will only be able to , at best , favor quintessence models over others , but it'll still be a stretch before we can definitively prove them . another right hand side explanation is k-essence , but this is essentially a modified version of quintessence where the kinetic energy term is not the canonical one , and rather than play around with $v ( \phi ) $ it is thought that it is the properties of this kinetic term that could lead to the required properties . recall that our own kinetic term was $\frac{1}{2} \dot{\phi}^2 - \frac{1}{6a^2} ( \nabla \phi ) ^2$ , and that we got rid of it entirely by assuming $\nabla \phi$ homogeneity and that $\dot{\phi}$ was small . the idea for k-essence is not to get rid of these terms , but to come up with a new form for the kinetic energy such that the k-essence field will have to desired properties . the novelty here ( relative to quintessence ) is that we do not need a very low upper bound on the mass of field to make it slow rolling . but observationaly , this remains just as hard to prove because the properties of the field ( your review gives a very comprehensive overview of this ) leave open a number of degrees of freedom as well . $f ( r ) $ models modify the left hand side . here , again , it is best to think of this as a class of models . as the name indicates , $f ( r ) $ gravity works by generalizing the einstein-hilbert action : $$ s [ g ] = \int \frac{1}{16 \pi g c^{-4}} r \sqrt{-|g_{\mu \nu}|} d^4x \rightarrow s [ g ] = \int \frac{1}{16 \pi g c^{-4}} f ( r ) \sqrt{-|g_{\mu \nu}|} d^4x $$ thus the action now depends on an unspecified function of the ricci scalar . the case $f ( r ) = r$ represents classical general relativity . it is this function that we tune in these models to get the desired properties . to test the general case rather than thest each $f ( r ) $ individually , you can taylor expand $f ( r ) = c_0 + c_1 r + c_2 r^2 + . . . $ and constrain your expansion to however many degrees of freedom you think your observations can constrain . just having $c_0$ obviously corresponds to a cosmological constant , and $c_1 = 1$ is , as we have said , the usual gr . note however , that because this modifies gravity as a whole , and not just cosmology , other observations could be brought into the fold to constrain more parameters , and we are not limited here by the number of degrees of freedom cosmological probes can constrain . if cosmological tests of gr agree with smaller scale tests of gr in a manner consistent with an $f ( r ) $ modified gravity , these models could be proven to be onto something . here of course we must look at all the tests of gr to get a fuller picture , but that is another review in and of itself . here we will simply note that more generally , the challenge when modifying the left hand side is to only change cosmology , but remain consistent with the smaller scale tests . the theory of general relativity is well tested on a number of scales . the accelerated expansion of the universe is the only potential anomaly when comparing general relativity to observations . to modify it such that the friedmann equations gain an acceleration term while all other tests remain consistent is tricky business for theorists working in the field . the $\lambda$ correction works because it essentially maintains the same theory except for adding a uniform scalar field in the theory . hence , while its impact on the global metric is significant , it has no impact on local tests of gravity because it does not introduce a gradient to the energy density field . other theories run into tuning problems precisely because they cannot naturally mimic this behavior . chameleon theories get around this problem by adding a field that becomes extremely massive ( i.e. . an extremely short scale force ) when surrounded by matter . hence , its local impact on gravity is negligible in all the contexts in which general relativity has been tested . ideally , your model would remain consistent with pre-existing tests , but still offer hope for some testable differences ( ideally with a rigorous prediction set by the scale of acceleration ) beyond cosmological acceleration so as to make your model more believable . finally , we can consider if the assumptions that went into the friedmann equations are correct . one approach ( not the one you mentioned ) is to consider the impact of inhomogeneities . while it was always obvious that homogeneity did not hold below certain scales , the traditional approach has been to simply conclude that while the friedmann equations will therefore not accurately describe the " local " scale factor , it would describe the " average " scale factor so long as we only considered it to be accurate above the inhomogeneous scales . in other words , they describe the expansion of the " average " scale factor . mathematically speaking , this is not the rigorous way to get around this problem . ideally , what one would do instead , is to calculate the average of the expansion of the scale factor . some theorists have proposed methods of doing this in ways that would add an acceleration term to the friedmann equations . for a matter dominated universe these become : \begin{eqnarray} \left ( \frac{\dot{a}}{a} \right ) ^2 and = and \frac{8 \pi g}{3} \left ( \left\langle \rho \right\rangle - \frac{k}{a^2} \right ) - \frac{1}{2} q ( t ) \\ \frac{\ddot{a}}{a} and = and \frac{- 4 \pi g}{3} \left\langle \rho \right\rangle + q ( t ) \\ \mbox{where } q ( t ) and = and \frac{2}{3} \left ( \left\langle a^2 \right\rangle - \left\langle a \right\rangle^2 \right ) - 2 \left\langle \sigma^2 \right\rangle \end{eqnarray} where $\sigma$ depends on the level of structure formation of the universe . because this model builds on the effect of inhomogeneities , it could explain why acceleration is a late time effect . earlier in the universe 's history , structure fromation was in its infancy and could not provide the necessary level of inhomogeneity . as the universe grows , structure formation kicks in , and so too does this effect become more significant . the biggest challenge for these models is providing a definite value for $q$ . this has proven to be a difficult task because estimating $\left\langle a^2 \right\rangle - \left\langle a \right\rangle^2$ analytically is extremely difficult , and simulating this sort of gr on such a large scale presents problems of its own . another potential effect of these inhomogeneities is the loss of energy as light enters large potential wells , only to exit them after they have significantly expanded . in this view , the observations would be explained by what happens to the photons along the line of sight , and no actual acceleration would take place . in some ways , this is equivalent to a sort of physically motivated tired light model . so far , these models have not been able to explain the observations assuming only a matter dominated universe . it is , however , entirely possible that a small portion of the observed effect is due to this . keep in mind that this explanation would only affect supernovae measurements , and hence could not explain the now independent confirmation of other probes regarding acceleration . at best , they could one day reduce tension between supernovae and other probes . the lemaitre-tolman-bondi model assumes that homoegeneity is just not true at all , and that we live inside a gravitationaly collapsing system , near its center . it is a little out there , but the thing about cosmology is that it is rather easy to make far fetched models and go " hey , you never know " , because hey , we do not :p it is a big universe out there and we only have access to a tiny portion of it , so why not consider i guess . the key to disproving this one will be to increase the agreement between the various probes . the metric it leads to does not have the same impact on all the different distance probes in the same way that $\lambda$cdm does . so it is disprovable , and for the time being can be tuned to explain the observations , but a little far fetched to begin with . i would guess that you can change the toy model they use to model the inhomogeneous shape of the universe and almost always get something that'll fit the data , but the more you do that the less convincing your theory becomes . as a side note , though this is not your main question , it is not odd that we are using an effective fluid model . it is not counting on dark energy having properties similar to a fluid in the traditional sense , it is just telling you that you are using certain symmetries to simplify you $t_{\mu \nu}$ tensor when solving for the friedmann equations . essentially , if you are in the reference frame where the momenta of the universe sum to $0$ then $t_{\mu \nu} = diag ( \rho , p_x , p_y , p_z ) = diag ( \rho , p , p , p ) $ , and $w$ is just the ratio of of $\rho$ to $p$ . it is just a way to simplify equations and clarify your degrees of freedom .
the centre of mass is the point at which our collection of objects will balance if we put a pivot there . let 's call this point $\bf r$ . the vector joining the point $i$ to $\bf r$ is simply $\bf r_i - \bf r$ . the force acting at this point is $m_i \bf g$ , so the torque at the point $i$ is : $$ \bf t_i = m_i \bf g \times ( \bf r_i - \bf r ) $$ the total torque must sum to zero , because that is how we define the centre of mass , so : $$ \sum m_i \bf g \times ( \bf r_i - \bf r ) = 0$$ and the cross product is distributive over addition so we can take it outside the sum : $$ \bf g \times \sum m_i \bf ( \bf r_i - \bf r ) = 0$$ and this can ony be satisfied if : $$ \sum m_i \bf ( \bf r_i - \bf r ) = 0$$
there are no bound states for this potential , for any positive $\alpha$ . all energies $e&gt ; 0$ are allowed , and all of those are unbound states , in which $\phi\sim e^{ikr}$ for large $r$ , with $e=k^2/2$ . the easiest way to prove that there are no states with $e&lt ; 0$ is to show that the expectation values of both kinetic energy and potential energy , $\langle t\rangle$ and $\langle v\rangle$ , are positive for any valid wavefunction . for an energy eigenfucntion , $e=\langle e\rangle=\langle t\rangle+\langle v\rangle&gt ; 0$ . the fact that these are not bound states -- that is , that they go as $e^{ikr}$ rather than as a decaying function of $r$ at large distances -- is most easily seen from the fact that , at sufficiently large $r$ , the potential term is always negligible in comparison to the right-hand side . so at large $r$ we have $-\phi&#39 ; &#39 ; /2=e\phi$ .
the individual streamline with velocity zero may not make much sense on its own , but it often does when you consider the bulk of the fluid as a whole . in the case of the surface of a body immersed in a fluid , you could trace a streamline starting at a point infinitesimally close to the surface , where the velocity would be infinitesimally small , but non-zero . the streamline on the surface would be the limit of the streamlines as your starting point moves towards the boundary . such analysis cannot be performed on stagnant fluid , i.e. it makes no sense talking of streamlines in the bulk of a stationary fluid .
if you use a plate glass window instead of a wall you will find that the rubber and iron balls bounce by a similar amount ( though be careful throwing iron balls at windows :- ) . it is a basic principle in physics that energy cannot be lost . the rubber ball starts off with kinetic energy , hits the wall , and rebounds moving with about the same kinetic energy . so no energy is lost . if the iron ball does not bounce it must mean that the energy it originally had has been transferred to the wall . rubber balls are soft , so they decelerate relatively slowly and they deform and spread out as they hit the wall . this means that the pressure they exert on the wall while they are bouncing is relatively low . by contrast an iron ball is very hard so it stops very suddenly and all the force it exerts on the wall is concentrated on a small area . that means the pressure is high enough to damage the wall . it might cause a visible dent , or it might just cause cracks within the wall that you can not see . in both cases energy is used in damaging the wall , and this energy comes from the motion of the ball . that means little energy is left for the iron ball to bounce back . i started by saying the iron ball would bounce off plate glass . this is because plate glass is very rigid and provided you do not shatter it the glass is not damaged by the iron ball . since no energy is absorbed by the glass , the iron ball bounces back just as the rubber ball does .
you can apply the lorentz force equation $f=qv\times b$ at the microscopic level , since the magnet is made out of charged particles . however , it is not practical to do this for a ferromagnetic object . the electrons in the ferromagnetic material also have intrinsic spin 1/2 and an intrinsic dipole moment , and they therefore experience an additional force in a field gradient . this force is not described by the lorentz equation . a complete , realistic calculation is going to be extremely difficult . you could solve maxwell 's equations numerically , putting in the correct permeability . note that you are going to have hysteresis effects .
moshe is answer gets the main point across . if you are interested in trying to learn something more detailed , here 's a thought about where you might start . ( i am not even close to expert on these matters , but this is something that i have found readable and worthwhile . ) there is an interesting thread of literature including http://arxiv.org/abs/hep-th/9309145 by susskind and http://arxiv.org/abs/hep-th/9612146 by horowitz and polchinski , along with other papers that those might lead you to . the idea is that free strings have a certain entropy , in the sense that for a given mass , there are many different combinations of oscillator modes you can turn on to find a string state of that mass . the string energy is proportional to its length , and a typical string of a given energy looks roughly like a random walk of a particular length . once you turn on a coupling , at some point some of these states will become black holes , because their energy is contained in a region smaller than their schwarzschild radius . for a given mass , a black hole state also has some entropy . there are various consistency checks you can do on the way these things scale , to see that it is sensible to talk about a sort of smooth transition between string states and black hole states if you fix the energy and vary the coupling . there are more technical and more precise papers in the literature on black hole entropy and microstate counting , but for a non-expert like me this particular thread of literature seems interesting because it relies on parametric scaling laws that are pretty readily comprehensible , and paints a relatively clear picture of the physics .
verified it with fea , it is correct . also , taking into account the " rocking " effect requires the piece-wise description : $$ j\ddot{\theta} + \bigg\lbrace\begin{matrix} -m g r \sin ( \alpha + \theta ) and \theta &lt ; 0 \\ m g r \sin ( \alpha - \theta ) and \theta \geq 0 \end{matrix} = m a r \cos ( \alpha - \theta ) $$
you have got the right idea , but are just missing a few details i think . for concreteness , this is a 1d classical sound wave problem with a stationary receiver and a source having velocity $v_\mathrm{s}$ ( positive toward the receiver ) . the medium may have a velocity , but we will lump that in with the sound propagation speed , so there is an effective speed $c$ for the wave to propagate from the source to the receiver . 1 first , consider a pulse emitted at time $t_0$ . if the separation between source and receiver is initially $d_0$ , the travel time will simply be $d_0/c$ , and so we can map emission time to arrival time : $$ t_0 \to t_0 + \frac{d_0}{c} . $$ now suppose another pulse is emitted a time $\delta t$ later . the distance is now $d = d_0 - v_\mathrm{s} \delta t$ , and so we have another emission $\to$ arrival mapping $$ t_0 +\delta t \to t_0 + \delta t + \frac{d_0-v_\mathrm{s}\delta t}{c} . $$ let an arbitrary time be denoted $t$: $t = t_0 + \delta t$ . then eliminate $\delta t$ from the above map : $$ t \to t + \frac{d_0-v_\mathrm{s} ( t-t_0 ) }{c} . $$ call this mapping $y$: $$ t_\mathrm{r} \equiv y ( t_\mathrm{s} ) = t_\mathrm{s} + \frac{d_0-v_\mathrm{s} ( t_\mathrm{s}-t_0 ) }{c} $$ so far this matches what you describe . but how does this get a frequency ? well , you observe a sinusoid of the form $\sin ( f_\mathrm{r}t_\mathrm{r}+\phi_\mathrm{r} ) $ , where $f_\mathrm{r}$ is the received frequency and $\phi_\mathrm{r}$ is an arbitrary offset that does not depend on $t_\mathrm{r}$ . this is in some sense the definition of $f_\mathrm{r}$ . if you want to know the value of the received wave at time $t_\mathrm{r}$ , this must match the value of the source at time $$ t_\mathrm{s} = y^{-1} ( t_\mathrm{r} ) = \frac{c}{c-v_\mathrm{s}} t_\mathrm{r} - \frac{d_0+v_\mathrm{s}t_0}{c-v_\mathrm{s}} $$ note the inverse , since we are now mapping received times to emitted times . the emitted wave was of the form $\sin ( f_\mathrm{s}t_\mathrm{s}+\phi_\mathrm{s} ) $ , and equating the source and received waves with the correct time correction leads to 2 \begin{align} \sin ( f_\mathrm{s} t_\mathrm{s} + \phi_\mathrm{s} ) and = \sin ( f_\mathrm{r} t_\mathrm{r} + \phi_\mathrm{r} ) \\ f_\mathrm{s} t_\mathrm{s} + \phi_\mathrm{s} and = f_\mathrm{r} t_\mathrm{r} + \phi_\mathrm{r} \\ f_\mathrm{s} \left ( \frac{c}{c-v_\mathrm{s}} t_\mathrm{r} - \frac{d_0+v_\mathrm{s}t_0}{c-v_\mathrm{s}}\right ) + \phi_\mathrm{s} and = f_\mathrm{r} t_\mathrm{r} + \phi_\mathrm{r} \\ \left ( \frac{c}{c-v_\mathrm{s}}\right ) f_\mathrm{s} t_\mathrm{r} + \phi_\mathrm{s} - f_\mathrm{s} \left ( \frac{d_0+v_\mathrm{s}t_0}{c-v_\mathrm{s}}\right ) and = f_\mathrm{r} t_\mathrm{r} + \phi_\mathrm{r} . \end{align} from this we can read off $$ \phi_\mathrm{r} = \phi_\mathrm{s} - f_\mathrm{s} \left ( \frac{d_0+v_\mathrm{s}t_0}{c-v_\mathrm{s}}\right ) , $$ and more importantly $$ f_\mathrm{r} = \left ( \frac{c}{c-v_\mathrm{s}}\right ) f_\mathrm{s} . $$ this is the correct doppler shift formula for the case considered . 3 1 note this means $c$ will be a different value for sound propagating away from the receiver if the medium has a bulk velocity . 2 note in going from the second to the third line , we got rid of the sines . technically , either they should stay in for the whole calculation , or you should add an arbitrary integer multiple of $2\pi n$ . but really the sines have nothing to do with the problem , and are something of a distraction . any waveform undergoes the same doppler shift . 3 though some authors may use a different sign convention . you can check that the sign makes sense in that a source moving subsonically toward you in a subsonic medium ( $0 &lt ; v_\mathrm{s} &lt ; c$ ) will have a higher frequency .
for a general potential $v ( r ) $ you can use the variable-phase method . i think you will find the following book very useful : calogero , f . ( 1967 ) , the variable phase approach to potential scattering . academic press , new york . interesting to note is that calogero emphasizes in his 1967 book that the numerical calculation of phase shifts using the variable-phase equation is well within the power of a simple desk calculator !
this is a heavy question , that contains many topics in it that are worthy of their own questions , so i am not going to give a complete answer . i am relying mainly on this excellent review paper by nayak , simon , stern , freedman and das sarma . the first part can be skipped by anyone already familiar with anyons . abelian and non-abelian anyons anyons are emergent quasiparticles in two dimensional systems that have exchange statistics which are neither fermionic nor bosonic . a system that contains anyonic quasiparticles has a ground state that is separated by a gap from the rest of the spectrum . we can move the quasiparticles around adiabatically , and as long as the energy we put in the system is lower than the gap we will not excite it and it will remain in the ground state . this is partly why we say the system is topologically protected by the gap . the simpler case is when the system contains abelian anyons , in which case the ground state is non-degenerate ( i.e. . one dimensional ) . when two quasiparticles are adiabatically exchanged we know the system cannot leave the ground state , so the only thing that can happen is that the ground state wavefunction is multiplied by a phase $e^{i \theta}$ . if these were just fermions or bosons than we would have $\theta=\pi$ or $\theta=0$ respectively , but for anyons $\theta$ can have other values . the more interesting case is non-abelian anyons where the ground state is degenerate ( so it is in fact a ground space ) . in this case the exchange of quasiparticles can have a more complicated effect on the ground space than just a phase , most generally such an exchange applies a unitary matrix $u$ on the ground space ( the name ' non-abelian ' comes from the fact that these matrices do not in general commute with each other ) . the quantum dimension so we know that the ground space of a system with non-abelian anyons is degenerate , but what can we say about its dimension ? we expect that the more quasiparticles we have in the system , the larger the dimension will be . indeed it turns out that for $m$ quasiparticles , the dimension of the ground space for large $m$ is roughly $\sim d_a^{m-2}$ where $d_a$ is a number that depends on $a$ - the type of the quasiparticles in the system . this scaling law is reminiscent of the scaling of the dimension of a tensor product of multiple hilbert spaces of dimension $d_a$ , and for this reason $d_a$ is called the quantum dimension of a quasiparticle of type $a$ . you can think of it as the asymptotic degeneracy per particle . for abelian anyons we have a one-dimensional ground space no matter how many quasiparticles are in the system , so for them $d_a=1$ . although we used the analogy to a tensor product of hilbert spaces , note that in that case the dimension of each hilbert space is an integer , while the quantum dimension is in general not an integer . this is an important property of non-abelian anyons that differentiates them from just a set of particles with local hilbert spaces - the ground space of non-abelian anyons is highly nonlocal . more details on anyons and the quantum dimension can be found in the review paper cited above . the quantum dimension can be generalized to other systems with topological properties , maintaining the same intuitive meaning of asymptotic degeneracy per particle . it is in general very hard to calculate the quantum dimension , and there is only a handful of papers that do ( most of them cited in the paper by kitaev and preskill that inspired this question ) . relation to entanglement i can also try and give a handwaving argument for why the quantum dimension would be related to entanglement . first of all , the fact that the entanglement entropy of a bounded region depends only on the length of the boundary $l$ and not on the area of the region is very clearly explained in this paper by srednicki , which is also cited by kitaev and preskill . basically it says that the entanglement entropy can be calculated by tracing out the bounded region , or by tracing out everything outside the bounded region , and the two approaches will yield the same result . this means the entanglement has to depend only on features that both regions have in common , and this rules out the area of the regions and leaves only the boundary between them . now for a system with no topological order the entanglement would go to zero when the size of bounded region goes to zero . however for a topological system there is intrinsic entanglement in the ground space which yields the constant term $-\gamma$ in the entanglement . the maximal entanglement entropy a system with dimension $d$ has with its environment is $\log d$ , so in an analogous manner the topological entanglement is $\gamma=\log d$ where $d$ is the quantum dimension . again this last argument relies heavily on handwaving so if anyone can improve it please do . i hope this answers at least the main concerns in the question , and i welcome any criticism .
plenty ! the cuo$_2$ planes of cuprate superconductors are perhaps the most famous example , but in fact a lot of layered oxides are going to coordinate in this fashion . the lieb lattice is essentially a two-dimensional counterpart of the " perovskite " structure , which is ubiquitous in nature .
it is not very clear to me if you are asking about energy or momentum . you should also ask about a specific interaction process as there are many , this is required especially to answer your last , quantitative , question . however , generally speaking , a $\gamma$ photon cannot give some of its energy to anything else : it is all or nothing . even in the compton scattering , in which you get a less energetic photon , the initial photon is destroyed . the momentum must be conserved as well , so yes : when a photon hits another particle this is accelerated , you can even generate some measurable pressure with a very intense radiation !
1 . ) as the box moves to the left , the photon moves to the right , and their momenta is conserved . since the masses are moving proportionally and opposite to one another , the center of mass of that system remains fixed . 2 . ) it is the same as the center of mass of a system consisting of a large gymnasium and a tennis ball inside the gymnasium , if that helps make it clearer . it is just that photons are very , very , very , ( very ) " small " - but the idea behind it is the same . 3 . ) yes , it does mean that . the box has moved , but so has the photon , so the center of mass of the box-photon system has not moved , they have just shifted relative to one another . 4 . ) it means that the mass must be non-negligible , so that it is accounted for in calculating the center of mass of the system , so that 1 . ) is true . i hope this helps answer your questions , but please follow up if anything is unclear .
the canonical momenta do not change if you add a total derivative to the lagrangian . the particular total derivative you wanted to add to the lagrangian as well as the lagrangian itself has free $i , j$ indices . you surely meant something else because the lagrangian should have no free indices like that . let me assume that you meant both expressions to be summed with the sum and prefactor $\sum_{ij} c_{ij}$ . of maybe you really meant the lagrangian to be a monomial for fixed values of $i , j$ . but that is not the issue here . the error relevant for your question is that you considered a phase space that has coordinates $\psi_j$ , $\bar\psi_i$ , $\pi_{\psi_i}$ , and $\pi_{\bar\psi_j}$ , and you think they are independent coordinates on the phase space . that would be too many phase space coordinates for such a limited system . well , they are not independent . the right derivation , using any form of the lagrangian you want , will give you $\pi_{\psi_i}=-\bar \psi_i$ ( without one-half ; and equations that may be obtained by simple conjugations from this one ! ) so it means that the " same " non-differentiated $\psi$ 's are their own momenta , too . if you rewrite the lagrangian in such a way that the redundant notation is eliminated , i.e. you do not think that coordinates that are dependent are actually independent ( this is the error that made you end up with the canonical momenta being 1/2 of their right value ; for example , you incorrectly used $\partial\dot{\bar\psi_i} / \partial \psi_j = 0$ , which is not true , in the first momentum you mentioned ) , you will see that $$\frac{\partial l}{\partial \dot\psi_j }=-\bar\psi_i$$ if i use your confusing non-summation over $i , j$ . there is no factor of 1/2 . indeed , to derive this thing without problems , it is helpful to first rewrite the lagrangian as $\bar\psi_i\dot\psi_j$ by adding the appropriate total derivative . this form is unique because it contains no $\dot{\bar\psi_i}$ and no $\psi_j$ , so it is only expressed as a function of the independent 1/2 of the degrees of freedom . needless to say , the hamiltonian is zero if the fermionic lagrangian only contains the kinetic term with the time derivative .
dear dbrane , $\lambda_{\rm qcd}$ is the only dimensionful parameter of pure qcd ( pure means without extra matter ) . it is dimensionful and replaces the dimensionless parameter $g_{\rm qcd}$ , the qcd coupling constant . the process in which a dimensionless constant such as $g$ is replaced by a dimensionful one such as $\lambda$ is called the dimensional transmutation : http://en.wikipedia.org/wiki/dimensional_transmutation the constant $g$ is not quite constant but it depends on the characteristic energy scale of the processes - essentially logarithmically . morally speaking , $$ \frac{1}{g^2 ( e ) } = \frac{1}{g^2 ( e_0 ) } + k \cdot \ln ( e/e_0 ) , $$ at least in the leading approximation . because $g$ depends on the scale , it is pretty much true that every value of $g$ is realized for some value of the energy scale $e$ . instead of talking about the values of $g$ for many specific values of $e$ , one may talk about the value of $e$ where $g$ gets as big as one or so , and this value of $e$ is known as $\lambda_{\rm qcd}$ although one must be a bit more careful to define it so that it is 150 mev and not twice as much , for example . yes , it is the characteristic scale of confinement and all other typical processes of pure qcd - those that do not depend on the current quark masses etc . in most sentences about the qcd scale , including your quote , the detailed numerical constant is not too important and the sentences are valid as order-of-magnitude estimates . however , given a proper definition , the exact value of $\lambda_{\rm qcd}$ may be experimentally determined . with this knowledge and given the known lagrangian of qcd - and the methods to calculate its quantum effects - one may reconstruct the full function $g ( e ) $ .
hint : young 's modulus is given by the ratio of tensile stress over extensional strain ; the formula : $$y=\frac{ ( f/a ) }{ ( \delta l /l_0 ) }=\frac{fl_0}{a\delta l}$$ where $f$ is the force applied , $a$ is the cross-sectional area , $l_0$ the original length and $\delta l$ the extension or change in length of the object . in your case , $l_0=2 \ , \mathrm{m}$ . as stated in the question , the diameter of the wire is $d=0.0008 \ , \mathrm{m}$ . therefore the radius $r=d/2$ , and the area is given by , $$a=\pi r^2 = \frac{\pi}{4} d^2=5.027 \times 10^{-7} \ , \mathrm{m}^2$$ as we have assumed a cross-section of the wire is a perfect circle . the force applied is given by newton 's second law , $f=ma$ . can you take it from here ?
well , your lecturer certainly should not have put it like this , however it is true that you have got a lot wrong here . it is stuff you definitely will need to understand better if you are studying power engineering . first , you seem to think that electrons are attracted by magnetic north poles . they are not ; in fact stationary charges and magnetic fields are not concerned with each other in any way at all 1 ! next , you are talking about electrons in circular orbits about the nucleus . that is roughly the bohr model , which kind-of-sort-of-works , but not really . you want to familiarise yourself to the orbital model , which describes very well how bound electrons actually behave . even in an orbital , you might be inclined to talk about " the nucleus is off the center by a distance proportional to the voltage " . that is again kind-of-sort-of-right since the nucleus lies in a locally-harmonic potential which can be read as " pertubation by an electric field ( which in a fixed capacitor is proportional to the voltage ) will cause a proportional displacement of the nucleus " , but the way you phrase it it is still nonsense . voltage " is " not a distance , it is a potential ( i.e. . energy ) . anyway , this is not actually relevant to understanding rotating-magnet phenomena , i.e. inductance in coils . these are concerned only with conduction electrons , which are not bound to any particular atom at all but " move " through the entire conductor , which is why there can be currents . it is these moving electrons that experience a significant force in the presence of a magnetic field . what current actually is is the number and " speed " with which these electrons move through the conductor , while even a strong displacement of the bound ( valence ) electrons would not consolidate a current 2 . now , all of this seems to say there is not any such thing as inductance . sure there is ! only , it is rather more complicated : electrons at rest are not affected by stationary magnetic fields , but in the same way that moving electrons are affected by such fields , moving magnetic fields ( or , more generally , time-varying magnetic fields ) also cause a lorentz force upon resting electrons . so , effectively , what you are saying about electrons being moved around by moving magnetic fields is not all that wrong again , it only works quite a bit differently . a moving magnetic field will in fact " push resting conductance electrons " through a wire a bit , i.e. induce a voltage . but that voltage really can not be read as anything displacement-like , it is a fundamental electrodynamic phenomenon . in fact , the voltage in its pure , exact value can only be measured if you prevent the conductance electrons from moving , as otherwise they would themselves cause a magnetic field cancelling the inductance etc . pp . . as you see , the whole subject is quite a bit more complicated than you thought . i am sure you are capable of understanding it , but probably not in a few minutes , which is why your lecturer can not really be blamed for not trying to explain it right away . 1 actually , electrons are also small magnets themselves ( they have an instrisic quantum-mechanical spin ) and therefore are attracted to inhomogenic magnetic fields , but that is quite another issue . 2 actually , it would . . . but that is mostly relevant in the high-frequency-regime , i.e. bound electrons that jiggle back and forth very quickly .
you want your parachute not to be a parachute , but a wing . the difference is that it has horizontal velocity , and the air flows smoothly over the top and bottom surfaces . in addition , you want to minimize drag , because sink rate is proportional to drag . the main way to minimize drag is to minimize speed . so it needs forward speed , but no more than necessary . check out paragliders , because that is what they do . i have seen these things in action . they stay up a long time .
i will start this with right hand grip rule for solenoids . . . " the coil ( solenoid ) is held in the right hand so that the fingers point the direction of current through the windings . then , the extended thumb points the direction of magnetic field " . ( which would be along the axis of the coil ) the higher the current , the more the magnetic field would be produced . . . for your example , let us assume the aluminium ring as a circular coil . when the uniform magnetic field is produced , there is a change in magnetic flux ( such as this increase in magnetic field ) along the axis of the ring , according to faraday 's law , induced current flows through the ring whose direction is given by lenz 's law . this induced current in the ring flows in a direction such that it opposes the magnetic field in the solenoid ( the one which actually produces it ) . ( but , the magnitude of induced magnetic field is always lesser than the field in the solenoid ) . anyways , there is a repulsion . with the maximum repulsive force produced , the ring is thrown off from the solenoid . this force always depends on the magnitude of $b$ in the solenoid .
the discrepancy results from the fact that the first approach implicitly assume distinguish-ability while the second does not . let 's take a simple example : $n=1$ , $n=2$ , and there are two cells ( $v=\frac{1}{2}v$ ) , what is the probability to have 1 particle in the first cell ? now label the particles by $1$ and $2$ ( and remove the label in the end ) and exam your two approaches . there are four configurations , $ [ ( 1,2 ) , ( ) ] $ , $ [ ( 1 ) , ( 2 ) ] $ , $ [ ( 2 ) , ( 1 ) ] $ , $ [ ( ) , ( 1,2 ) ] $ . you treat them with equal probability $\frac{1}{4}$ . there are two configurations with only 1 particle in the first cell . so the probability is $\frac{1}{2}$ . there are only three configurations , because $ [ ( 1 ) , ( 2 ) ] $ and $ [ ( 2 ) , ( 1 ) ] $ are the same physical states . the probability is therefore $\frac{1}{3}$ . the second approach is correct , because only different physical states are equally likely . edit feb . 7 , 2014: my conclusion was wrong , although particles should be indistinguishable . op and i implicitly assume there is only 1 microstate in each cell , which is not realistic . suppose there are $m$ sub-cells in the unit cell with volume $v$ ; equivalently there are $m$ different states in each cell $v$ . let $\frac{v}{v} = c$ , then there are $ ( c-1 ) m$ sub-cells outside $v$ as shown in the figure . there are $\binom{m+n-1}{m-1}$ ways to distribute $n$ indistinguishable particles into $m$ sub-cells in $v$ . similarly there are $\binom{ ( c-1 ) m+n-n+1}{c ( m-1 ) -1}$ ways to distribute the rest $n-n$ particles into $ ( c-1 ) m$ sub-cells outside $v$ . so the total number of configurations for exactly $n$ particles in $v$ is \begin{equation} \gamma ( n ) =\binom{m+n-1}{m-1} \binom{ ( c-1 ) m+n-n-1}{ ( c-1 ) m -1} \end{equation} while the total number of configurations is the ways to distribute $n$ particles into $cm$ sub-cells , \begin{equation} \sum_{n=0}^{n}\gamma ( n ) = \binom{cm+n-1}{cm-1} \end{equation} the probability in question is then \begin{equation} p_n = \frac{\gamma ( n ) }{\sum_{i=0}^{n}\gamma ( i ) } = \binom{n}{n} \frac{ ( m+n-1 ) ! }{ ( m-1 ) ! } \frac{ [ ( c-1 ) m+n-n-1 ] ! }{ [ ( c-1 ) m-1 ] ! } \frac{ ( cm-1 ) ! }{ ( cm+n-1 ) ! } \end{equation} then we take the dilute limit , which means the number of sub-cells $m$ are large compared even with $n$ . these factorials can be simplified , \begin{equation} p_n \approx \binom{n}{n} ( \frac{1}{c} ) ^n ( \frac{1-c}{c} ) ^{n-n} = \binom{n}{n} ( \frac{v}{v} ) ^n ( 1- \frac{v}{v} ) ^{n-n} \end{equation} oh god , i got the same answer as the first approach . so there is no inconstancy for these two approaches after we take $m\rightarrow \infty$ ; that means particles even in $v$ can only occupy a small fraction of sub-cells , which somehow makes them effectively distinguishable .
from what i have read in " american prometheus : the triumph and tragedy of j . robert oppenheimer " teller was the first one to express this concern before the trinity test . also quoting from : http://www.sciencemusings.com/2005/10/what-didnt-happen.html physicist edward teller considered another possibility . the huge temperature of a fission explosion -- tens of millions of degrees -- could fuse together nuclei of light elements , such as hydrogen , a process that also releases energy ( later , this insight would be the basis for hydrogen bombs ) . if the temperature of a detonation was high enough , nitrogen atoms in the atmosphere would fuse , releasing energy . ignition of atmospheric nitrogen might cause hydrogen in the oceans to fuse . the trinity experiment might inadvertently turn the entire planet into a chain-reaction fusion bomb . robert oppenheimer , chief of the american atomic scientists , took teller 's suggestion seriously . he discussed it with arthur compton , another leading physicist . " this would be the ultimate catastrophe , " wrote compton . " better to accept the slavery of the nazis than run a chance of drawing the final curtain on mankind ! " oppenheimer asked hans bethe and other physicists to check their calculations of the ignition temperature of nitrogen and the cooling effects expected in the fireball of a nuclear bomb . the new calculations indicated that an atmospheric conflagration was impossible . " bethe apparently then convincingly showed that the atmosphere would not be set on fire by a nuclear bomb .
for the $\theta : 2\theta$ goniometer , the x-ray tube is stationary , the sample moves by the angle $\theta$ and the detector simultaneously moves by the angle $2\theta$ . at high values of $\theta$ , small or loosely packed samples may have a tendency to fall off the sample holder .
a couple of point that might help clarify the situation . when you say " the action of constant force . . . " you really mean the action of a force that is being intentionally harnessed for it is motive power . that is you have given only a partial description of the situation . essential all travel on earth occurs in fluid ( liquid or gas ) environments and much of it occurs in contact with solid surfaces . these facts have physical consequences in the form of dissipative processes ( friction in many guises ) . friction in a force , and it always acts to oppose motion . so what does this imply about the full description of the situation ?
quadrupole magnet quadrupole magnets are mostly used for beam focusing . http://en.wikipedia.org/wiki/quadrupole_magnet
i would like to add a little to lubos 's answer : first a historical note : this is what einstein proposed as a way of understanding quantum mechanics in 1919 or thereabouts , in the paper " do gravitational fields play a role in the composition of the elementary particles ? " einstein was of the opinion that a complicated enough classical theory , like general relativity , would lead continuous waves to collapse into standard-size soliton-like particles and these particles he felt might then bang around along the wave in such a way to reproduce quantum mechanics . this idea reappears several times in the literature , but it demonstrably does not work . a field theory , like gr , is a classical theory , and it therefore is local hidden variables ( the variables are not even hidden in this case ) . this is ruled out by bell 's theorem--- the correlations in quantum mechanics do not allow local fields to carry the data that determines the experimental outcome , not without conspiracy ( superdeterminism ) or nonlocal equations ( faster than light changes in the variables ) . neither works in a straightforward field theory like gr . secondly , gr is not as badly understood as all that , although it is not as well understood as one would like , mostly because numerical methods are in their infancy , and one 's intuition must come laboriously from analyzing exact solutions when these are available . the example i gave of particles oscillating into and out of an extremal black hole is not really new ( do not give me too much credit ) , the new thing there is the holographic interpretation , namely that the coming-out is an ordinary coming out event in this universe . the oscillations of particles into and out of a near-extremal black hole were appreciated in the 1960s , but each oscillation takes you to a disconnected branch classically , because crossing a horizon takes an infinite amount of t-time . this is not possible quantum mechanically , since this disconnected maximally extended thing is not compatible with unitarity . the nice thing about the in-out solution for geodesics in the extremal reissner nordstrom is that if you replace the test particle with a little charged black hole , you can make nonrelativistic oscillations if both black holes are near extremal . the external field of the two black holes does not have a full merger , the little black hole , now not considered as a test particle , but as a solution to gr proper , just smears out on the horizon , then bounces back . i did not calculate this in detail yet , but it can be solved completely with an analysis along the lines of atiyah and hitchin in their famous paper on slow soliton scattering ( the atiyah hitchin space ) , except here , unlike the other case , i am not optimistic there will be a simple geometrical solution , rather one has to bite the bullet and trace the bouncing behavior in the solution either by numerical integration or solving for the near-static phase-space geometry of the two extremal black holes . causalities and ctc 's the basic idea you are giving is that perhaps hidden variables plus closed time-like curves can reproduce bell inequality violations . i will give some sentences about why this is extremely unlikely . quantum mechanics has entangled wavefunctions . what this means is that the wavefunction for k particles is in 3k dimensional space , not in 3 dimensional space . the growth in dimensions means that quantum mechanics packs a stronger computational punch than classical mechanics , and you can not simulate quantum mechanics of k-particles with less than exponentially much classical information . this is why quantum computation works in pure quantum mechanics . so the structure of quantum mechanics is exponentially big and has the entanglements that violate bell 's inequality . if you wish to reproduce this from something like gr , you need gross nonlocality and some way of reproducing nonlocality . so if you have a pair of electrons that bind to an atom ( so that their spins anti-align ) , and then you knock out the nucleus , and do bell measurements on the two outgoing electrons , you need to reproduce the nonlocal correlations from ctc 's in gr . this means that the electron needs to have ctc 's " inside " which go back in time and magically alter the attributes of the other electron . this only became required once you put them together in an atom , and let the photons radiate , and during this process the two point electrons did not necessarily come close to each other ( assuming they are classical and described in space ) . how do ctc 's help correlate them ? to make this work , you would have to go all the way back in time to where the two electrons were created from the inflaton field , and correlate them back then . this type of back-and-forth in time description is utterly conspiratorial , and very unconvincing . there is also no shred of a hint that this will reproduce anything like qm , it is just not ruled out , because you are postulating little tiny internal back-in-time paths on all electrons , something we have no evidence for . there are no real ctc 's in physical exterior solutions of gr . the ctc 's in the intepretation i gave of oscillations into and out of extremal black holes are unphysical--- they are only closed in time because of the wrongness of the classical picture of the horizon . the ctc 's in the interior of a kerr solution can only occur when you wind around the ring singularity , and then it should be possible to unwrap the interior so that it has a pure-causal description , simply by including the winding number of your path around the ring . i do not know the interior kerr well enough to see how to do this , and this must work in any number of dimensions , not just 4 , so i hesitate to say it is what happens , but there must be a reconciliation of causality and kerr interior , because you can set up fields at the horizon of kerr , and let them traverse the interior , and the evolution equation should not have additional constraints , as come from ctcs . all in all , the form of the two theories , gr and qm , is completely different , the descriptions are of a different computational complexity , and the causality notion is totally different in the two schemes , so it is implausible in the highest degree that gr can explain qm . what is more , today we have a good quantum version of gr , string theory , which subsumes and extends the classical theory , so that it is a mistake to pretend that this progress does not exist , and to work as if we were living in 1926 . within string theory , you give a full accounting of all gr effects on flat and ads backgrounds in principle , from an ordinary unitary quantum theory . this quantum gr means that we know how gr and qm are reconciled ( in perturbations to flat and ads backgrounds ) , and the classical limit where gr is reproduced is just not quantum , it is an ordinary classical field theory .
after a lot more searching , i have found the answer to my question ! :d below is a summary of the information i found . there is no specific webpage i can link to because i relied on sources who quoted other sources which no longer exist , but maybe this information can be useful to someone else someday . most of what i learned comes from professor lou bloomfield who currently teaches physics at the university of virginia . edit : none of this is quoted material : all information posted below has been completely reworded , and the analogies ( aside from the guitar string ) are mine . when surrounded by normal matter , a light wave 's electric field will cause electrons to jiggle at a rate equal to the frequency of the light wave : the electric component of the light wave will alternately attract and repel charged particles . when electrons in a material transparent to a certain frequency are excited by a light wave of that frequency , this takes energy away from the light wave . but surprisingly , no photons are absorbed : since the material is transparent to the frequency of the wave , there is no higher orbital which matches exactly the energy level an individual photon would impart to an electron . this means the energy transfer can not involve a real particle interaction . so what happens ? instead of absorbing one or more photons , the electrons enter a virtual quantum state : a temporary excitation that does not exactly match one of the states that the electron can occupy . this is very much like vibrating a guitar string by aiming sound at the string . if the sound you aim at the string matches a frequency that the string can vibrate at , it will cause the string to vibrate . if the sound you use is the wrong frequency , the string will wiggle a little bit as though trying to vibrate , then stop when the sound passes . that is what happens to the electrons : they borrow energy from the light wave , wiggle a little , and then return the energy . a virtual quantum state is very limited in duration , and does not count as a particle interaction . the light wave and the electron remain unentangled and continue to act as probability waves . the electron can only play with the light wave 's energy for a brief period before returning it . the characteristics of the light wave remain unchanged because there was no real particle interaction . so the light does not ricochet off of atoms , nor does it get emitted in the usual sense by the electrons which play with it . even though the interactions are all virtual , electrons are matter and they take time to jiggle . as this happens over and over and over again , it slows the progress of the wave . you might think of this like a kind of friction which acts against the progress of the wave . consider a car whose wheels turn at a constant speed , and imagine it encounters a series of large bumps that slow it down slightly . the speedometer is based on the wheel rotation , so it would say the car has not changed speed at all : it is just as fast as it was on flat terrain . the car will , however , cover less ground per time interval because some of the wheel-turning is used to surmount the humps . these humps are akin to the process of electrons temporarily borrowing energy from the light wave . so is the light wave truly slowed , or is the light still moving at c and only its progress is slowed ? this is not actually a well-formed question , and for all practical purposes the answer does not matter . however , i find it easier to think about it as slowing the wave 's progress . this means the characteristic that " light moves at speed c in all reference frames " still holds true , which makes it much easier for me to reason about relativistic effects . additionally , i was incorrect about different frequencies slowing by the same amount : lower frequencies are slowed less than higher frequencies . when the frequency is lower , even though the wave has less energy , the electrons will need to jiggle over a wider area ( they are pulled for a longer period , then pushed for a longer period ) . since the electrons remain bound to their atoms in this interaction , they can not be pulled out of the atom by a virtual excitation . so the slower the frequency is , the " more virtual " the excitation must be , and the less time the electrons have to play with the light . is this information useful ? if so , is there a way i could make it more accessible ? just curious , as i am very new to se .
the crucial word is " beam " , in " beam splitter " . beam means an ensemble , in contrast to " photon " which is an individual particle . a light beam is an ensemble of photons and if it is of a single frequency $\nu$ , all photons have energy $e= h*\nu$ . a light beam can be split in a beam spliter , i.e. the ensemble of photons can be split into two streams of photons : the intensity of the beam goes down , but the individual photons still have frequency $h*\nu$ . now one can think of impinging photons one by one on a beam splitter . a photon is described by a wavefunction which when squared will give the probability of finding the photon in a particular ( x , y , z ) . it will go either where one stream went or the other according to the probabilities , but it will be seen as a whole photon of energy $e=h*\nu$ .
in an atomic clock , the reference is the d2 line of cesium , i.e. the energy difference between a ground state and an excited state . this is an absolute value up to the heisenberg limit . so , if you had perfect conditions , you are only limited by physics . what i mean is that there is a minimum uncertainty in being able to measure the energy difference between the ground state and the excited state . however , to get to the heisenberg limit , several technical errors have to be overcome . " line broadening " ( i.e. energy measurement uncertainty ) occurs due to many factors such as : laser issues : phase noise , intensity noise , frequency drifts . for the most part , these can be controlled very precisely i.e. to about ~10hz/1thz . " light shifts " can be controlled by lowering the intensity . frequency shifts are compensated by " locking " the laser to the resonance line using active feedback . optical feedback lowers the laser linewidth itself . magnetic fields : this is one of the more nasty problems as you have to actively compensate for ac and dc fields . since the setup involves a mot ( magneto-optical trap ) , measurements are made after the trapping magnetic fields are turned off , so hysteresis or residual fields can perturb the system by the zeeman effect . this problem has also been taken care of by the pioneers of the mot system . apart from this , there are a ton of other things that have to be monitored . so yes , in essence , these errors are of a technical nature as opposed to some random occurrence .
mass is only conserved in the low-energy limit of relativistic systems . in relativistic systems , mass can be converted into energy , and you can have processes like massive electron-positron pairs annhillating to form massless photons . what is conserved ( in theories obeying special relativity , at least ) is mass energy--this conservation is enforced by the time and space translation invariance of the theory . since the amount of energy in the mass dominates the amount of energy in kinetic energy ( $mc^{2}$ means a lot of energy is stored even in a small mass ) for nonrelativistic motion , you get a very good approximation of mass conservation . out of the energy conservation .
let 's assume that the question is telling us to keep the component of the speed in the plane perpendicular to the central rod constant . then in the case with upward acceleration , newton 's second law in the $x$ ( horizontal ) and $y$ ( vertical ) directions reads \begin{align} t_a\sin\theta_a and = m\frac{v^2}{\ell\sin\theta_a}\\ t_a\cos\theta_a and = m ( g+a ) \end{align} where $t_a$ is the magnitude of the tension for given upward acceleration $a$ and $\theta_a$ is the corresponding angle . this is two equations in two unknowns $t_a$ and $\theta_a$ . solving for $t_a$ ( i used mathematica out of laziness fyi ) $$ t_a = \frac{m v^2}{2\ell}\sqrt{1+\frac{4 ( g+a ) ^2\ell^2}{v^2}} $$ note that for fixed $v , \ell , m$ the tension necessarily increases . note . i had previously only analyzed the vertical component of the tension which op pointed out was not sufficient to answer the question .
this cannot be answered fully today--- this is the question of what is the bit-content of the relevant computation that is going on in the body . if you know the bits to simulate the computation efficiently , excluding the bits that are effectively random , you could say . whenever you have a biological system , some bits are doing important things , like the bits that tell you what domains are bound to an active protein , and some bits are useless , like the bits that tell you the precise orientation of a subpart of the cytoskeleton , or the bit that describes the orientation of some water molecule . when you want to store the data in a person , you are generally speaking about the biologically relevant data . if you store this data , and destroy the organism , and then restore the organism from new molecules , arranged according to the instructions in this irreducible data , the organism will behave in a statistically indistringuishable way from the original . one can say some general things about the size of this data : it is not infinite there are some people who claim that the computation is like an analog computer ( or as close as one can come given quantum limitations ) . this means that there are molecules or large objects in the cell that store analog data in their positions . this is not true at all , and this fallacy is persuasive enough that one must argue against it . when you have a system in a thermal bath , there is always diffusion going on between interactions . if you have a molecule which is storing data in some way relevant to the biology , it must store this data in a way that can be retrieved by the remaining computation effectively . if this data is randomized by diffusion , it is not effective storage , and this bit may be discarded and replaced by a random number generator . if you have a diffusing protein with interactions with other proteins every time $\delta t$ on average , and with diffusion constant d , the protein will randomized into a gaussian of size $\delta x= d\sqrt{\delta t}$ before the next interaction . this means that it is pointless and wasteful in terms of storage to specify the position to more accuracy than this size , and the number of possible positions is on a lattice of size $\delta x$ . the number of points grows as the log of $\delta x$ , so the number of bits a position can encode is bounded by the log of this , and it only grows logarithmically . this means that no matter how absurdly quickly you try to make the protein interact , the number of bits it can store in the position is never very high . it would be better off adding 5 binding domains rather than trying to localize more precisely . the result applies to all other continuous storage mechanisms you can dream up--- the position of untethered molecules diffuses , the angles of proteins randomize , the concentrations of atoms are only relevant to the extent that a localized channel or protein can discriminate between different concentrations . in all biological systems , the spatial resolution cutoff for all motions is coarse enough to make the dominant information storage mechanism molecular binding . it is not that large the binding of molecules at first glance includes a large number of bits , since each protein is a long sequence of amino acids . but this is also a false bit content . in order to be dynamical data , capable of computing , the data has to change in time . even if you have a very complicated protein , if it is only action is to bind to a ligand , then it has exactly two states , and carries one bit of information . if it binds to a polymer , then it can have many different bits , but these bits should be associated to the binding sites of the polymer . the protein has two states , the polymer has many different states of protein binding . to store this protein state efficiently on a computer , i just have to name the proteins with a unique name ( this only takes a few bits ) , and give the state of each one of each type . the proteins which carry 1 bit of information will be fully specified by the number of 1-state proteins and the number of 0-state proteins . to specify this number only requires log growing information , so it has negligible bit content . this is absurd , of course . if you include rough position information , you will always need about 10 bits per proteins ( to name a billion locations in the cell where it could be ) to really specify the state . so there are not going to be 1 bit proteins which are not tethered to one spot . but the point here is that the dynamical data that is doing the computation is far smaller than the data encoded in the protein amino-acid sequence , because this data is rom , it is not ram , and it can be specified ahead of time , you do not need to simulate to know the types of proteins that are running around it is very small in proteins there is a simple formalism ( see here , i authored this : http://arxiv.org/abs/q-bio.mn/0503028 ) which allows you to estimate the bit-capacity of binding proteins . the formalism is also useful for describing how proteins bind . it turns out to be related to d . harel 's higraphs , but it extends the formalism nontrivially to include polymerization ( harel was only interested in finite state automata , and did not consider binding molecules ) . the point of this formalism is to easily estimate the bit-capacity of protein networks . the estimate is simply by multiplying the bit-capacity of a typical protein by the total number of proteins . excluding proteins of known function , metabolic stuff , and so on , you find that there is a range of bit-values in a human cell from as low as 10kb to as high as 1mb , but the higher end is extremely optimistic , and assumes that every different binding state is functionally discriminatable , so you can tell apart every phosphoryllation state of p53 from every other just by looking at the future dynamics . this is clearly false , and i tend to believe the lower estimates . it is rather big in rna on the other hand , rna strands in the cell can do much more . rna is self-binding in complementary pairs , and to predict the future behavior of a strand , you need to know the sequence . this is because the sequence can find another complementary sequence and bind , and this bound sequence can attach to a protein , and so on , in a closed loop computation . in order for this information to contribute to the bit-content , one must assume that there is a vast undiscovered network of interacting rna . i will assume this without any compunctions . this explains many enduring mysteries , which i will not go into . the memory capacity in an rna computer in the cell is easy to estimate , it is twice the number of nucleotides . unlike for proteins , each nucleotide is carrying ram , not rom , if the interactions with other rna is sufficiently complex . this is not true of mrna , this is not true of trna , it is not true of ribosomal rna in isolation , so it requires more roles for rna than were ever imagined . this is a prediction which does not surprise biologists anymore . it all hinges on how much rna computing is going on in the cells the estimate for rna is the memory content of dna , which is on the order of 10^9 bits . the rna can be an order of magnitude greater , two orders of magnitude greater , but no more , since you will run out of space . this is 10^11 bits per cell . the biggest thing is the brain in the brain , there is more genetic material than anywhere else . if you just take the weight of the brain and consider that it is all rna , you get a reasonable estimate of the bit content of a person . it is approximately 10^22 bits per person , or $10^9$ terabytes , a billion terabytes in rna , much less in everything else ( so the wikipedia estimate is probably ok for everything else ) . this is the correct estimate of the memory capacity of a person , since it is riduclous to think that the information in the vast unknown rna in the brain is decoupled from the neuron activity , considering that the neuron activity is otherwise completely computationally pathetic . i described this idea in more detail here : could we build a super computer out of wires and switches instead of a microchip ? .
you can gain some intuition from looking at the density distribution function in momentum space which for the $|bcs\rangle$ is given by $n_k=v^{2}_k$ . in the bcs limit one finds approximately the filled fermi sphere , while in the bec limit $n_k\sim 1/ ( 1+ [ ka ] ^2 ) ^2$ which is proportional to the square of the fourier transform of the dimer wave function . for this reason in the bec limit the state $|bcs\rangle$ describes a condensate of dimers . you can find a little bit more about this question in http://arxiv.org/abs/0706.3360
the short answer ( and likely one you are not going to like ) to your question is that you are going to need as many zernike terms as it is experimentally found are needed to model the aberration accurately . zernike polynomials are normalized so that they contribute equally to the mean square phase error , and this latter , to the first order , is what sets the strehl ratio , which is effectively what you are after here . the simplest analysis of the relation between aberration and phase is explored in sections 9.2 and 9.3 of born and wolf " principles of optics " ( mine is the sixth edition ) ; in 9.3 the maréchal criterion is discussed , which is limit on the total mean square aberration that is maréchal 's definition of " diffraction limited performance " . you may be able to narrow your description down to a few key zernike polynomials , but there is no fundamental physics or mathematics to tell you which ones : either you must determine this experimentally or sometimes you can hazard a good guess if you can find any details of the production process that builds your mirrors out and understand the production process 's " symmetries " . as for the ellipticity of your problem ( wrought by the 45 degree beam folding in your " periscope " arrangement ) , i believe it can be handled by simple co-ordinate transformations as detailed below that make the mirror system nominally axisymmetric . so your zernike analysis will be the " wonted " or " normal " one , but you transform your elliptical domain into a circular one with the $x$-direction shrink embodied in equations ( 2 ) and ( 3 ) below . thus your situation will have the following special symmetry considerations : spherical aberration terms become astigmatic for example , astigmatic terms will tend to become spherical - likewise , the symmetry class of all aberrations on the mirror surfaces will be changed by the shrink embodied by ( 2 ) and ( 3 ) ; your mirrors will themselves are ellipsoidal rather than spherical ( which i am sure you understand ) to yield the focusing power when 45 degree tilted rather than orthogonal to the optical axis . i am pretty sure ( without further analysis ) the nominal contours on the ellipses are going to be of the form $\frac{1}{2} x_m^2 + y_m^2 = r^2$ ( see ( 2 ) and ( 3 ) - so astigmatic zernike terms are going to be especially important . there could be " azimuthal " misalignment between the two principal axes of the mirrors : i.e. the major and minor axes of one mirror may not be quite parallel to one another . this could lead to zernike terms of high azimuthal symmetry classes : tetrafoil , heaxfoil and octofoil ( $\cos$ or $\sin ( 4 \theta ) $ , $\cos$ or $\sin ( 6 \theta ) $ , $\cos$ or $\sin ( 8 \theta ) $ ; are you diamond turning these mirrors , btw ? diamond turning is extremely accurate , even in non axisymmetric components - the biggest errors are likely to be spherical ( or spherical in the transformed co-ordinates in your case ) so i am guessing the major aberrations are going to arise from misalignements of the principal mirror axes . now we get onto how the analysis looks . given $d \gg \lambda$ , fraunhofer diffraction applies , so the input field undergoes the equivalent of the following three steps : diffract to mirror system so that the wave has a diverging spherical wavefront of curvature radius $d$ ; mirror system , through weak but intended ellipsoidal shape , imparts phase to the diffracting spherical wavefront so that now the wavefront has the conjugate phase ( intended function , so that it becomes a converging rather than diverging wave ) together with an unintended phasing $\exp ( i\ , \phi ( x , y ) ) $ , where $\phi ( x , y ) $ is the unintended aberration as a function of the rectangular mirror surface co-ordinates $x$ and $y$ ( to be defined so as to take account of the " folding " owing to the two 45 degree slanted mirrors ) ; diffraction " back " to the image plane . i am thus thinking of this problem as a more complicated version of a diffracting field bouncing off a spherical mirror aligned to the spherical wavefront so that the field focusses back at its initial positing in the ideal case . the transformation undergone by a field $\psi ( x , y ) $ in the object plane to reach the image plane is thus : $$\psi \mapsto \mathfrak{f}^{-1}\ , \exp ( i\ , \phi ( x , y ) ) \ , \mathfrak{f}\ , \psi\quad\quad ( 1 ) $$ where $\mathfrak{f}$ is the unitary two dimensional fourier transform . the inverse transform comes from the fact that my equivalent system imparts the nominal conjugate phase to the wavefront and sends it " backwards " to the image plane . as long as we deal with measurements in the image plane alone , we cannot tell which of the pair any aberration comes from , so we may as well represent the combined effect of the " periscope " pair by the one aberration function $\phi ( x , y ) $ . this aberration is the thing whose effect you wish to quantify . now for the definitions of $x$ and $y$ , which is where the 45 degree tilting is accounted for : $$x = \frac{k\ , x_m}{\sqrt{2}\ , d}\quad\quad ( 2 ) $$ $$y = \frac{k\ , y_m}{d}\quad\quad ( 3 ) $$ here $x_m$ and $y_m$ are the physical distances measured along the surface of the effective mirror , $i . e . $ imagine the mirrors flattened out taking away their by their nominal ellipsoidal curvature designed to offset the field curvature arising from the first diffraction . the residual deviations from this design goal are then added together to get the aberration function $\phi$ . here the $x$ direction is along the plane containing the system chief ray 's nominal folded path : i.e. if the tube ( line joining the two mirror centres ) of the " periscope " is horizontal , then the $x$-axis is horizontal and likewise if the periscope tube is in any other direction . the square root of two factor accounts for the mirror tilt : the beam spreads out further on the mirror in the $x$ direction than it does in the $y$ owing to the effective mirror 's intersecting the beam at the slant . so now we are left with the effect of $\phi ( x , y ) $ . let the input field be : $$\psi_i\left ( x , y\right ) = \frac{1}{\sqrt{\pi}\ , \sigma} \exp\left ( -\frac{x^2+y^2}{2\ , \sigma^2}\right ) \quad\quad ( 4 ) $$ where $\sigma$ is the input field 's spotsize . the first diffraction sends $\psi$ into the $x-y$ space by : $$\psi_i\left ( x , y\right ) \mapsto \psi ( x , y ) =\frac{1}{2\ , \pi\ , \sqrt{\pi}\ , \sigma} \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty \exp\left ( -i \left ( x\ , x +y\ , y\right ) \right ) \ , \exp\left ( -\frac{x^2+y^2}{2\ , \sigma^2}\right ) \ , \mathrm{d}x\ , \mathrm{d}y = \frac{\sigma}{\sqrt{\pi}} \exp\left ( -\frac{1}{2}\ , \sigma^2\ , \left ( x^2+y^2\right ) \right ) \quad\quad ( 5 ) $$ and now the output field is : $$\psi_o\left ( x , y\right ) =\frac{\sigma}{2\ , \pi\ , \sqrt{\pi}} \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty \exp\left ( -i \left ( x\ , x +y\ , y\right ) \right ) \ , \exp\left ( -\frac{1}{2} \sigma^2 \left ( x^2+y^2\right ) \right ) \ , \exp ( i\ , \phi ( x , y ) ) \ , \mathrm{d}x\ , \mathrm{d}y\quad\quad ( 6 ) $$ now we can generalize the above result by including the effects of a defocus by noting that the diffraction through axial distance $\delta z$ of a unidirectionally propagating scalar field fulfilling the helmholtz equation undergoes the transformation defined by : $$\psi \mapsto \mathfrak{f}^{-1} \ , \exp\left ( i\ , \delta z \sqrt{k^2 - x^2 - y^2}\right ) \ , \mathfrak{f} \ , \psi \approx e^{i\ , k\ , \delta z}\ , \mathfrak{f}^{-1}\ , \exp\left ( -i\ , \delta z \frac{x^2+y^2}{2\ , k}\right ) \ , \mathfrak{f}\ , \psi\quad\quad ( 7 ) $$ where $\psi$ is the field at the input plane and is transformed to the field on a parallel plane a distance $\delta z$ in the direction of the field 's propagation . the first fourier transform $\mathfrak{f}$ splits the field into plane wave components with $x$-wavenumber $x$ and $y$-wavenumber $y$ , then the phasing term sandwiched between the two fourier transforms imparts the right phase delay for each plane wave component ( if a plane wave has $x$ and $y$ wavevector components $x$ and $y$ , then the $z$ component of the wavevector must be $\sqrt{k^2 - x^2 - y^2}$ where $k$ is the wavenumber ) , then the last inverse fourier transform assembles all these delayed plane wave components into the diffracted field . the approximation assumes a low numerical aperture field , so that $x$ and $y$ are small compared to the wavenumber $k$ and the plane wave components all make small angles with the axial direction . so now we can combine ( 1 ) and ( 7 ) to get a version of ( 6 ) generalized to where the light field is transformed by the whole system , followed by a defocus of $\delta z$: $$\psi_o\left ( x , y\right ) =\frac{\sigma}{2\ , \pi\ , \sqrt{\pi}} \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty \exp\left ( i \left ( \phi ( x , y ) - x\ , x -y\ , y - \delta z \frac{x^2+y^2}{2\ , k}\right ) \right ) \ , \exp\left ( -\frac{1}{2} \sigma^2 \left ( x^2+y^2\right ) \right ) \ , \mathrm{d}x\ , \mathrm{d}y\quad\quad ( 8 ) $$ the strehl ratio is the intensity ratio of the above quantity to its aberration free value ( which is $\frac{1}{\sqrt{\pi}\ , \sigma}$ ) , assuming the output field 's peak amplitude is at position $ ( x , y ) $ . so now we look carefully at the total aberration term that lowers the peak amplitude : $$\phi ( x , y ) - x\ , x -y\ , y - \delta z \frac{x^2+y^2}{2\ , k}\quad\quad ( 9 ) $$ and rewrite that in polar co-ordinates $ ( \rho , \theta ) $ . to begin applying zernikes to this aberration , we must define the " outer radius " in the normalized mirror co-ordinates $ ( x , y ) $ ; the gaussian $\exp\left ( -\frac{1}{2} \sigma^2 \left ( x^2+y^2\right ) \right ) $ is well " contained " within a radius of , say , $r = 4 / \sigma$ ; you can rework the following for any value of $r$ , although i am suggesting a working value is going to be very like what i have just suggested . with polar co-ordinates normalized so that $x^2+y^2 = r^2$ corresponds to the whole aperture and therefore to $\rho = 1$ ( i.e. $\rho^2 r^2 = x^2+y^2$ ) , our total aberration is : $$\phi ( \rho , \theta ) - x\ , r\ , \rho\ , \cos\theta -y\ , r\ , \rho\ , \sin\theta - \frac{\delta z\ , r^2}{4\ , k} ( 2\rho^2 - 1 ) + \frac{\delta z\ , r^2}{4\ , k}\quad\quad ( 10 ) $$ here you see the two tilt and defocus zernike functions $\rho\ , \cos\theta$ , $\rho\ , \sin\theta$ and $2\rho^2 - 1$ and they correspond to sideways system misalignments ( corresponding to tilts on your mirror , which i assume you have the freedom to impart for the sake of alignment ) and small errors in the axial position of the output plane : again , i assume you have the freedom to impart small axial displacements to the mirror system in your alignment controls . the net aberration , after all these adjustments have been optimized , will be your mirror aberration $\phi ( \rho , \theta ) $ with the defocus and tilt zernikes removed . let $\tilde{\phi} ( \rho , \theta ) $ be the net aberration after tilt and defocus zernikes have been removed . then the strehl ratio is : $$\begin{array}{lcl}\mathcal{s} and = and \left ( \frac{\sigma^2\ , r^2}{2\ , \pi} \int\limits_0^\infty\int\limits_0^{2\pi} \rho\ , \exp\left ( i \tilde{\phi} ( \rho , \theta ) \right ) \ , \exp\left ( -\frac{r^2}{2} \sigma^2 \rho^2\right ) \ , \mathrm{d}\theta\ , \mathrm{d}\rho\right ) ^2 \\ and \approx and \left ( 1 - \frac{\sigma^2\ , r^2}{2\ , \pi} \int\limits_0^r\int\limits_0^{2\pi} \rho\ , \frac{\left ( \tilde{\phi} ( \rho , \theta ) \right ) ^2}{2} \ , \exp\left ( -\frac{r^2}{2} \sigma^2 \rho^2\right ) \ , \mathrm{d}\theta\ , \mathrm{d}\rho\right ) ^2\\ and \approx and 1 - \frac{\sigma^2\ , r^2}{2\ , \pi} \int\limits_0^r\int\limits_0^{2\pi} \left ( \tilde{\phi} ( \rho , \theta ) \right ) ^2 \ , \rho\ , \exp\left ( -\frac{r^2}{2} \sigma^2 \rho^2\right ) \ , \mathrm{d}\theta\ , \mathrm{d}\rho\end{array}\quad\quad ( 10 ) $$ the last expression is the one i believe you are looking for to describe the degradation of the mode peak . the last two steps came from assuming small aberration so that we expand $\exp ( i\tilde{\phi} ( \rho , \theta ) ) $ as a taylor series to quadratic terms , and then to note that the integral of the linear taylor term vanishes because we have stripped away the mean value of $\tilde{\phi} ( \rho , \theta ) $ when we removed the tilt and defocus zernike terms . recall that the constant $r$ is the domain radius in your normalized co-ordinates defined by ( 2 ) and ( 3 ) and so has the dimensions of inverse length . this analysis is not too different from that in section 9.3 of born and wolf . the main difference here is the co-ordinate transformations ( 2 ) and ( 3 ) which map the physical mirror surface co-ordinates into the normalized co-ordinates suitable for zernike analysis , and the " lopsided " imparting of the $\sqrt{2}$ factor in ( 2 ) accounts for the tilted beam path in the " periscope " , so you will need to transform your datasets by ( 2 ) and ( 3 ) before doing your zernike analysis . mathematica encodes zernike radial functions by zerniker [ n , m , r ] ( radial class $n$ , azimuthal symmetry $m$ , $r$ the independent variable ) . most wavefront analysis softwares that come with interferometers can do the kind of linear transformation on datasets you want - the one i use is 4d technology "4sight " . the software " durango " has a free evaluation version but i have not used it . failing all that , i have my own c++ library for doing zernike analysis so if you need to use this , contact me by my email on my user page . lastly , for the sake of theoretical completeness , one strictly should not use zernike functions for situations where beams are gaussian apodised like yours . one should use functions that are orthogonal with respect to the weight function $\rho \ , \exp\left ( -\frac{r^2}{2} \sigma^2 \rho^2\right ) $ ; the zernikes are orthogonal with respect to the weight function $\rho$ . but this should not make a great deal of difference as long as you choose your " cutoff " radius $r$ well .
i can understand the identity qualitatively now : if the function f ( t ) has a well defined value at $t = \pm\infty$ , then for large values of $|t|$ the function f is essentially constant with values $f ( -\infty ) $ for $t &lt ; 0$ and $f ( +\infty ) $ for $t &gt ; 0$ . for tiny $\epsilon$ the exponential factor $e^{-\epsilon |t|}$ is essentially equal to $1$ for $|t| &lt ; 1 / \epsilon $ , and almost zero for large $|t| &gt ; 1 / \epsilon$ . we then have approximately : $\epsilon\int_{-\infty}^\infty \mathrm{d}t \ , f ( t ) e^{-\epsilon |t|} \approx \epsilon\int_{1 / \epsilon}^0 \mathrm{d}t \ , f ( -\infty ) + \epsilon\int_0^{1 / \epsilon} \mathrm{d}t \ , f ( +\infty ) = f ( -\infty ) + f ( +\infty ) $
the pressure increases , not decreases , to $10^{-2}$ torr . as there is ( almost ) nothing in the chamber except oxygen , it contributes all the pressure . note that $10^{-6}+10^{-8} \neq 10^{-2}$ the flow does not matter at all .
i have not made the transition myself , and it certainly depends on what aspect of finance you go into , but the answer is likely " absolutely . " if you were to end up doing computational finance , i assume some of the techniques are similar . in computational astrophysics , we often simulate a physical problem by breaking it up into discrete chunks and evolving some partial differential equations in each of those chunks . this usually done in a multidimensional framework of up to ~7 ( 3 space + 3 momentum + time ) dimensions if you are doing radiation hydrodynamics solving the boltzmann transport equation . i am a bit unfamiliar with computational finance , but i imagine you have a similar problem in that you have some differential equations telling you how each of the important variables changes with respect to your dependent variables . the problem here is likely to be similar to wanting to minimize some " energy " ( read : maximize some profit ) function in some very high-dimensional phase space - certainly greater than 7 . nevertheless , their certainly exists an overlap in techniques . more importantly , the ability to think about and solve problems in the discrete manner used in computational astrophysics will certainly be applicable in areas outside of astronomy , such as finance .
start from the beginning . why constraint relations ? why are they there ? let me emphasize : let 's take origin at top pulley which is at rest . note that length of top rope is constant : $a+b=k\implies a''+b''=0 \implies a''=-b''$ also length of second rope is constant : $ ( c-b ) + ( d-b ) =k\implies c''+d''=2b''$ note that $d$ is a constant as the top pulley and ground is rest : $c''=2b''$ hence , $c''=-2a''$ as stated in comments . also , everything we have done is futile and the block $m_2$ will hit the ground very quickly .
formula 1 is the contribution to the magnetic field from the small segment of the coil of length $dl$ shown in the lower portion of figure 1 . that piece of coil has current $di = n i\ , dl/l$ flowing through it where $n$ is the number of turns in the coil , $i$ is the current per turn , and $l$ is the length of the coil . from the hyperphysics site ( or by integrating the biot-savart law ) , the axial field from this loop is : $$b_z = \frac{\mu_0}{4\pi} ( 2\pi r ) di\frac{ r}{ ( r^2+z^2 ) ^{3/2}} = \frac{\mu_0 r^2 di}{2 y^3}$$ where $y= ( r^2+z^2 ) ^{1/2}$ is the distance from the measurement point to the rim of the loop and $r$ is the coil radius . substituting the formula for $di$ gives you the first equation . the second equation is the integral of the above over the length $l$ . the integral is first converted to an integral over angle $\theta$ via $\sin \theta = r/y$ and $\sin\theta\ , dl = y\ , d\theta$: $$b = \int\frac{\mu_0 r^2 di}{2 l y^3} = \frac{\mu_0 n i}{2 l} \int \frac{r^2 dl}{y^3} = \frac{\mu_0 n i}{2 l} \int_{\theta_1}^{\theta_2} \sin \theta \ , d\theta$$
what the rocket does is holds the propellant in a lower-pressure , easier to store form , with higher energy density . basically you have a lot of that kinetic energy you will need in the form of chemical energy . then you can pipe that fuel to the combustion chamber and ignite it there . at that point all that chemical energy is released in the form of expanding gases and heat . so , instead of having a very heavy container holding all that hot pressurized gas , you just have a smaller container holding your fuel .
the crux of the argument is that we can treat complex analytic ( holomorphic ) functions as functions in 2d , and their real and imaginary parts ( separately ) are solutions of laplace 's equation ( $\nabla^2 \psi = 0$ ) , due to the cauchy-riemann condition . conformal maps such as the one you cite map analytic functions to analytic functions , i.e. generate new solutions from old ones . thus , by knowing a trivial solution ( such as around the cylinder ) , we can generate the flow around a new object by finding a conformal map to it . relevant details : laplace 's equation solves potential flow problems : incompressible , inviscid , curl-free flow ( though we are allowed rotational flow around finite objects --- the resulting singularity is technically outside of the domain ) . we can somewhat relax the need for fully holomorphic functions . such mappings tend to mess up your boundary conditions at infinity --- so it may be quite hard in general to find such mappings .
the string that the eggs hang from is allowed to move in the same direction as the eggs when swung perpendicular . this is not the case when you swing in parallel . if you hold the top string still and swing the egg perpendicular you will see that almost none of that energy transfers to the other egg and the egg you pushed will continue to swing . basically the main string only has one degree of freedom and moving parallel is not the direction that the string moves .
i agree that this idea can seem weird . there are two was you can resolve the " paradox " you raise . you can think of the coating as a multiple reflexion : this is done both in born and wolf " principles of optics " i section 7.6 in the sixth edition and in hecht " optics " ( which i do not have before me ) . the first wave bounces off the coating 's outer interface as you say , but it is soon joined and mostly cancelled by the coating 's inner , out-of-phase bounce . one gets an infinite sequence of bounces at different phases that sum up as an infinite geometric sequence . so , when the light wavefront first reaches the antireflexion coating , the loss you talk about indeed does happen , but it is only for a fleetingly short time . all the bounces progressively add up over time so that the steady state behaviour is reached and the initially big losses are cancelled . we can analyse the behaviour of light by using a fourier transform to split it up into its perfectly time-harmonic components . so we get system transmission $t ( \omega ) $ and reflexion $r ( \omega ) $ co-efficients as functions of frequency $\omega$ and use these to analyse the behaviours of the the frequency components separately and then use the inverse fourier transform to build up the total , transient system response . now think for a moment about a single frequency component of light . it has no beginning , no end and no cause and effect behaviour : it is simply " there " , delocalised over all space and you solve maxwell 's equations for its propagation subject to all boundary conditions everywhere at once . so the seemingly non-causal ( or acausal ) behaviour is not paradoxical for an entity like a lone frequency wave . the causal behaviour of the time-limited light pulse is a phenomenon that emerges when we gather all the acausal , lone frequency behaviours and build them up into the pulse that fulfills the initial time conditions ; the first approach gives you what you need more directly , but the second i think is more interesting . maxwell 's equations are invariant with respect to a time flip i.e. $t\mapsto-t$ . every retarded wave solution to maxwell 's equations , for example , the spherical wave electric potential pulse $v ( x , y , z ) = f ( \sqrt{x^2 + y^2 + z^2} - c\ , t ) /\sqrt{x^2 + y^2 + z^2}$ can be replaced by the corresponding advanced wave solution wherein we replace $t$ by $-t$ and still have a valid solution . causality must be " put into our solutions by hand" ; we do this by choosing retarded waves over advanced waves when we add up time-harmonic solutions to fulfill our initial time conditions . in the first explation above , we do this by assuming that the electromagnetic field has an abrupt front which travels towards the substrate from the surrounding air and reaches the coating first . there is no requirement for this order of process to get solutions of maxwell 's equations and this order does not arise from maxwell 's equations , we are imposing this order grounded on our causal intuition . indeed , wheeler feynman absorber theory was an intriguing early attempt by feynman and wheeler to get rid of the self interaction of the electron ( which causes divergences in quantum electrodynamics ) by making all solutions to maxwell 's equations symmetric in time as well as maxwell 's equations themselves . both the anitcausal advanced and causal retarded solutions are present with equal weight in such solutions , but wheeler and feynman showed how causality can be preserved when all the electric charge of the universe is considered as a whole .
this might be more of a math question . this is a peculiar thing about three-dimensional space . note that in three dimensions , an area such as a plane is a two dimensional subspace . on a sheet of paper you only need two numbers to unambiguously denote a point . now imagine standing on the sheet of paper , the direction your head points to will always be a way to know how this plane is oriented in space . this is called the " normal " vector to this plane , it is at a right angle to the plane . if you now choos the convention to have the length of this vector ( "the norm" ) equal to the area of this surface , you get a complete description of the two dimensional plane , its orientation in three dimensional space ( the vector part ) and how big this plane is ( the length of this vector ) . mathematically , you can express this by the " cross product " $$\vec c=\vec a\times\vec b$$ whose magnitude is defined as $|c| = |a||b|sin\theta$ which is equal to the area of the parallelogram those to vectors ( which really define a plane ) span . to steal this picture from wikipedia 's article on the cross product : as i said in the beginning this is a very special thing for three dimensions , in higher dimensions , it does not work as neatly for various reasons . if you want to learn more about this topic a keyword would be " exterior algebra " update : as for the physical significance of this concept , prominent examples are vector fields flowing through surfaces . take a circular wire . this circle can be oriented in various ways in 3d . if you have an external magnetic field , you might know that this can induce an electric current , proportional to the rate of change of the amount flowing through the circle ( think of this as how much the arrows perforate the area ) . if the magnetic field vectors are parallel to the circle ( and thus orthogonal to its normal vector ) they do not " perforate " the area at all , so the flow through this area is zero . on the other hand , if the field vectors are orthogonal to the plane ( i.e. . parallel to the normal ) , the maximally " perforate " this area and the flow is maximal . if you change the orientation of between those two states you can get electrical current .
the short answer is that it can , if $m = 1 = m^{-1}$ . in this way of looking at it , all quantities in planck units are pure numbers . the longer answer is that there are two different ways of thinking about natural unit systems . natural unit systems in terms of standard units one of them , and perhaps the easier one to understand , is that you are still working in a " traditional " unit system in which distinct units for all quantities exist , but the units are chosen such that the numerical values of certain constants are equal to 1 . for example , if you want to set $c = 1$ , you are not literally setting $c = 1$ , you are actually setting $c = 1\ , \frac{\text{length unit}}{\text{time unit}}$ . length and time do not actually have the same units in this interpretation ; they are equivalent up to a multiplication by factors of $c$ . in other words , it is understood that to convert from , say , a time unit to a length unit you multiply by $c$ , and so that is left implicit . in order to do this , of course , you have to choose a length unit and time unit which are compatible with this equation . so you could not use meters as your length unit and seconds as your time unit , but you could use light-seconds and seconds , respectively . if you want to set multiple constants to have numerical values of 1 , that constrains your possible choices of units even further . for example , suppose you are setting $c$ and $g$ to have numerical values of 1 . that means your units have to satisfy both the constraints $$\begin{align} c and = \frac{\text{length unit}}{\text{time unit}} = \frac{\ell_g}{t_g} and g and = \frac{ ( \text{length unit} ) ^3}{ ( \text{mass unit} ) ( \text{time unit} ) ^2} = \frac{\ell_g^3}{m_gt_g^2} \end{align}$$ where i have introduced $\ell_g$ , $t_g$ , and $m_g$ to stand for the length , time , and mass units in this system , respectively . you can then invert these equations to solve for $\ell_g$ , $t_g$ , and $m_g$ in terms of $c$ and $g$ - but as you can probably tell , the system of equations is underdetermined . it still gives you the freedom to choose one unit to be part of your unit system , such as $$\text{kilogram} = \text{mass unit} = m_g$$ having made that choice , you can now solve for $m_g$ , $\ell_g$ , and $t_g$ in terms of $c$ , $g$ , and $\text{kilogram}$ ( or whatever other choice you might have made ; each choice gives you a different unit system ) . running through the math for this gets you $$\begin{align} m_g and = 1\text{ kg} and \ell_g and = \frac{g ( 1\text{ kg} ) }{c^2} and t_g and = \frac{\ell_g}{c} = \frac{g ( 1\text{ kg} ) }{c^3} \end{align}$$ now you can plug in values of $g$ and $c$ in , say , si units , and get conversions from si ( or whatever ) to this unit system . note that , as i said , length does not literally have the same units as time or mass , but you can convert between the length unit , time unit , and mass unit by multiplying by factors of $g$ and $c$ , constants which have numerical values of 1 . in a sense , you can consider this multiplication by $g^ic^j$ as analogous to a gauge transformation , i.e. a transformation that has no effect on the numerical value of a quantity , and the units of length , time , and mass are mapped on to each other by this transformation just as gauge-equivalent states are mapped on to each other by a gauge transformation in qft . so it is more proper to say $l \sim t \sim m$ ; the dimensions are not equal , just equivalent under some transformation . if you do the same thing but setting $c = \hbar = 1$ instead , remember what you are really doing is specifying that your units must satisfy the constraints $$\begin{align} c and = \frac{\text{length unit}}{\text{time unit}} = \frac{\ell_q}{t_q} and \hbar and = \frac{ ( \text{length unit} ) ^2 ( \text{mass unit} ) }{ ( \text{time unit} ) } = \frac{\ell_q^2m_q}{t_q} \end{align}$$ ( $q$ is for " quantum " because these are typical qft units ) , and then running through the math , again with $m_q = 1\text{ kg}$ , you get $$\begin{align} m_q and = 1\text{ kg} and \ell_q and = \frac{\hbar}{ ( 1\text{ kg} ) c} and t_q and = \frac{\ell_q}{c} = \frac{\hbar}{ ( 1\text{ kg} ) c^2} \end{align}$$ again , the units are not literally identical , but $\ell_q \sim t_q \sim m_q^{-1}$ under multiplication by factors of $\hbar$ and $c$ . of course , your third constraint does not have to be a choice of one of the fundamental units . you can also choose a third physical constant to have a numerical value of 1 . to obtain planck units , for example , you would specify $$\begin{align} c and = \frac{\text{length unit}}{\text{time unit}} = \frac{\ell_p}{t_p} \\ \hbar and = \frac{ ( \text{length unit} ) ^2 ( \text{mass unit} ) }{ ( \text{time unit} ) } = \frac{\ell_p^2m_p}{t_p} \\ g and = \frac{ ( \text{length unit} ) ^3}{ ( \text{mass unit} ) ( \text{time unit} ) ^2} = \frac{\ell_p^3}{m_pt_p^2} \end{align}$$ you can tell that this is no longer an underdetermined system of equations . solving it gives you $$\begin{align} m_p and = \sqrt{\frac{\hbar c}{g}} and \ell_p and = \sqrt{\frac{\hbar g}{c^3}} and t_p and = \sqrt{\frac{\hbar g}{c^5}} \end{align}$$ here , since you have set three constants to have numerical values of 1 , your three fundamental planck units will be equivalent up to multiplications by factors of those three constants , $g$ , $\hbar$ , and $c$ . in other words , multiplication by any factor of the form $g^i\hbar^jc^k$ is the equivalent to the gauge transformation i mentioned earlier . you can tell that all these units are equivalent under such a transformation , but more than that , all powers of them are equivalent ! in particular , you can convert between $m$ and $m^{-1}$ by multiplying by constants whose numerical value in this unit system is equal to 1 , and thus it is not a problem that $m \sim m^{-1}$ here . unit systems as vector spaces another way of understanding unit systems , which is kind of a logical extension of the previous section , is to think of them as a vector space . elements of this vector space correspond to dimensions of quantities , and the basis vectors can be chosen to correspond to the fundamental dimensions $l$ , $t$ , and $m$ . ( of course you could just as well choose another basis , but this one suits my purposes . ) you might represent $$\begin{align} l and \leftrightarrow ( 1,0,0 ) and t and \leftrightarrow ( 0,1,0 ) and m and \leftrightarrow ( 0,0,1 ) \end{align}$$ addition of vectors corresponds to multiplication of the corresponding dimensions . derived dimensions correspond to other vectors , like $$\begin{align} [ c ] = lt^{-1} and \leftrightarrow ( 1 , -1,0 ) \\ [ g ] = l^3m^{-1}t^{-2} and \leftrightarrow ( 3 , -2 , -1 ) \\ [ \hbar ] = l^2mt^{-1} and \leftrightarrow ( 2 , -1,1 ) \end{align}$$ in this view , setting a constant to have a numerical value of 1 corresponds to projecting the vector space onto a subspace orthogonal to the vector corresponding to that constant . for example , if you want to set $c = 1$ , you project the 3d vector space on to the 2d space orthogonal to $ ( 1 , -1,0 ) $ . any two vectors in the original space which differ by a multiple of $ ( 1 , -1,0 ) $ correspond to the same point in the subspace - just like how , in the previous section , any two dimensions which could be converted into each other by multiplying by factors of $c$ could be considered equivalent . but in this view , you can actually think of the two dimensions as becoming the same , so that e.g. length and time are actually measured in the same unit . since in planck units you set three constants to have a numerical value of one , in the dimensions-as-vector-space picture , you need to perform three projections to get to planck units . performing three projections on a 3d vector space leaves you with a 0d vector space - the entire space has been reduced to just a point . all the units are mapped to that one point , and are the same . so again , $m$ and $m^{-1}$ are identical , and there is no conflict .
welcome to the confusing world of coordinate systems used in astronomy ! the two coordinate systems relevant to your problem are the international celestial reference system ( icrs ) , and the ecliptic coordinate system . the first one is really well-defined , the latter is " simply " derived from that . put simply , the icrs is based on extending the earth 's equatorial plane out to infinity . the system places the x-axis in line with the sun-equinox line . the z-axis points to earth 's north pole , and y completes the right-handed system . needless to say , this system is not the most natural choice when viewing the solar system from afar . a more natural choice there is the ecliptic coordinate system . this system follows the same definition as the icrs , except that the ecliptic ( roughly the plane the earth 's orbit lies in ) is extended out to infinity , not the equatorial plane . it is fairly straightforward to convert ecliptic coordinates to equatorial coordinates and back . do not forget to convert angular distances ( ra , dec ) to euclidian coordinates , with the distance to the pole star equal to 1 ( this facilitates computations and provides a nice check ) .
at the start of the launch , the rocket has the largest mass of its entire flight . any rocket that can make it to orbit necessarily is fairly big , making its fully loaded mass enormous . the combination of large size and large mass makes its relative air drag smaller than compared to a smaller and less massive rocket . the rocket 's speed is also a consideration . at maximum acceleration , the rocket becomes supersonic only after it has reached the very upper limits of the troposphere . this means it only start moving really fast after it has climbed above the most dense parts of the atmosphere . since air resistance depends quadratically on speed , but the air density drops roughly exponentially with altitude , air resistance is hardly a consideration at all ( for large rockets ) . naturally , if there is no air at all , there is no air resistance , so less propellant would be required in all . but with regard to fuel efficiency : for any rocket that can make it to orbit , removing the whole atmosphere would be less effective than launching that rocket from the top of mt . everest : )
strictly according to double slit experiment calculations , yes infinite fringes are possible if ( 1 ) the slits are infinitesimal and placed infinitely apart , but actually , the fringes will be very close to each other so your eyes will not be able to differentiate and secondly each of the fringe will have almost $0$ intensity . so for obtaining infinite fringes you also need to have an infinite screen . the above results come from the fact that for a fringe : $\frac{dsin\theta}{\lambda}=n$ now for $n$ to be grow without bound , $d$ should be grow without bound . also replying on your suggestion that a screen placed infinitely far from the sources , you seem to assume that the distance between the fringes will get small , but this is not the case as the distance between two consecutive fringes is $\frac{\lambda d}{d}$ so if you get $d \rightarrow \infty$ the fringes also get infinitely apart , thus even if you will have an infinite screen length , you will only be able to say a finite number of fringes on the screen as $\frac{\infty}{\infty}$ will be finite in this case as both are dependent on exponent $1$ of length hence are of same order , here the ratio of infinities is taken to denote number of fringes on the infinite screen . intuitively , you may think that the $sin\theta$ of the maxima gets smaller , as the screen is getting farther away but what also is happening is that $n$ and $\lambda$ are finite so to get to second maxima , you have to travel an infinite distance on the screen to change$\theta$ by an appreciable amount . edit : it actually if you see carefully is a case of similar triangles . if the distance from the source gets infinity , then the fringe width gets infinity , if you draw out the case of two screens such that one is behind other , you will see because for fixed $n , d , \lambda$ $\rightarrow$ $\sin\theta$ is constant .
the difficulty here is one of definitions . in the pre-einstein-ian view there is a privileged frame ( typically taken as that of the " fixed " stars ) , and whichever body is experiencing the smallest acceleration with respect to that frame ( which is to say the earth by a large margin ) would have a stronger claim to being " still " thus the apple accelerates and the ground is still . in general relativity it is inertia frames that are special , and you can tell if you are in one by setting a test mass next to and letting go of it . if it stays there you are in an inertia frame . in that view , the apple is still and the ground comes rushing up to it . note that the earth as a whole is free falling and is therefore in an inertial frame , but object on the surface are not . at the kinds of energies that apply to falling apples on earth you can do physics correctly in either view . we teach the former in physics 101 , but the latter has a pretty strong claim to being more fundamental .
the mass of the ring is wrong . the ring ends up at an angle , so its total width is not $dx$ but $\frac{dx}{sin\theta}$ you made what i believe was a typo when you wrote $$\text{d}m = \frac{m}{4\pi r^2}\cdot 2\pi \left ( r^2 - x^2 \right ) \text{d}x$$ because based on what you wrote further down , you intended to write $$\text{d}m = \frac{m}{4\pi r^2}\cdot 2\pi \sqrt{\left ( r^2 - x^2 \right ) }\text{d}x$$ this problem is much better done in polar coordinates - instead of $x$ , use $\theta$ . but the above is the basic reason why you went wrong . in essence , $sin\theta=\frac{r}{r}$ so you could write $$\text{d}m = \frac{m}{4\pi r^2}\cdot 2\pi \frac{r}{sin\theta} \ \text{d}x \\ = \frac{m}{4\pi r^2}\cdot 2\pi \frac{r}{\frac{r}{r}} \ \text{d}x\\ = \frac{m}{4\pi r^2}\cdot 2\pi r \ \text{d}x\\ = \frac{m}{2 r} \ \text{d}x$$ now we can substitute this into the integral : $$i = \int_{-r}^{r} \frac{m}{2 r} \cdot \left ( r^2 - x^2\right ) \ \text{d}x \\ = \frac{m}{2r}\left [ {2r^3-\frac23 r^3}\right ] \\ = \frac23 m r^2$$
depends on the type of engine . if its a space engine , watch out for the exhaust . the rocket speed can be computed solely by its mass lost , but if the torus absorbs that then uhoh . also , the rocket engine speeds up the torus whilst accelerating , so finding when the two are equal is kind of tough . it is a fair assumption that the angular veloctiy added by the car to the torus is negligible though . the answer is yes , just imagine a frictionless car attached to the rail as you said , where the rocket is stationary compared to the earth or perhaps rather co-rotating with it . the rocket can not help being pulled down . i am a little worried with what you mean by 1g , since at the equator objects are both farther away from the center of the earth ( it is an ellipsoid ) and spinning more quickly . orient your torus parallel to the equator and perhaps mean corotating as in rotating at the same speed ( but not frequency ) as the equator . realize that equal frequencies will result in greater tangential speed for larger circles . wish i could bust out some equations but i am pretty bad at more than conceptual methods of viewing things . finally , about absolute rotation , realize that that is a thing . there is a difference between a spinning bucket and a non-spinning one , as newton showed ( just look at the water bulging ) . just because i am spinning with the bucket does not mean it will be the same reality , in fact it will be different . special relativity states that physics are the same in inertial reference frames . the answer is yes though , if you do not worry so much about the inverse square law of gravity , which i do not think you were . good question . thinking about these frames of reference will help your understanding of special relativity . remember-accelerating frames are not equal to inertial ones .
gamma emission is emission of a photon upon a nucleus transitioning from an excited state to a lower or ground state of the same nucleus . the number of neutrons and protons in the nucleus is exactly the same before and after the gamma photon is emitted . beta decay results from a nucleus having too few or too many neutrons relative to the number of protons to be stable . if there are too many neutrons , a neutron becomes a proton , an electron and an anti electron-neutrino . if there are too few neutrons , a proton may become a nuetron by positron emission or electron capture . whether beta decay is favorable can be calculated based upon the energies of the parent and daughter nuclei , as well as the energies of other particles . alpha decay is only observed in heavy nuclei , with at least 52 protons . iron ( 26 protons , 30 neutrons ) is the most stable nucleus . the semi-empirical mass formula may be used to determine if alpha decay is energetically favorably , but even if it is , the rate of decay may be extremely slow . there is a potential energy barrier to the particle 's escape from the nucleus . see this reference for further information .
electricity ( and electromagnetic fields in general ) will invariably follow the " easiest " path . the entire point of a faraday cage is to provide the electricity from whatever discharge you are facing with an " easy " path that does not involve the person inside the cage . holes will not change a thing . why would the electricity even try to get through the holes if there is so much conducting chicken wire for it to go through ? electricity loves chicken wire , it hates air . so if there is chicken wire , it just goes through it . nonetheless it is perfectly possible to get electric shock inside a faraday cage . just install an electric socket inside the cage , linked to the electric grid outside , and then do something stupid like cram your fingers into the socket . . . ( mark the the frequency of any external discharge will not change a thing . electricity prefers conducting material to isolators like air and people , so the faraday cage will shield you from all external discharges . )
if you actually discuss with people working on quantum memories , you will notice ( at least i did ) that they share a vague definition : " a quantum memory is something which stores a quantum state " better than a classical memory could do . beyond that , they have vastly different ideas on how to implement a quantum memory ( single qubits , collective degrees of freedom , array of qubits impelmenting a topological error correction code . . . ) what to do with a quantum memory ( ram for a quantum computer , store states to reconstruct them later , store states to measure them later ) how to evaluate the quality of a quantum memory ( fidelity , quantum capacity , cheating probability in a noisy-storage model based quantum cryptography protocol . . . ) note that the same kind of differences also apply on classical memories , between a sheet of paper , a magnetic tape , an ecc ram or a group of neurons in my brain . i am convinced however that it is possible to give a generic definition of a quantum memory . in a paper ( shameless plug ) on a specivic kind of continuous variable quantum memory , i wrote a quantum memory , by definition , stores informations about a quantum state for a given time interval , and it should do it better than any classical memory ( i.e. . classical-states based memory ) . since an a priori known quantum state has a complete classical description ( its density matrix ) , it can be reconstructed with an arbitrarily high fidelity by a setup only storing this description in a classical memory . more specifically , following the noisy storage model literature , a quantum memory can be defined by a quantum channel , which itself can be described by a time-dependent completely positive ( cp ) map $\mathcal t_t$ . if the quantum memory has a classical-output ( e.g. if it is used for a delayed measurement ) , it can be modelled by a cp map followed by a measurement . it gives us a straightforward criterion to distinguish a classical memory from a quantum memory , since a classical memory supplemented by measurement and preparation can only implement entanglement breaking channel . if the memory output is quantum , it can be said to be quantum memory iff $\mathcal t_t$ is not entanglement breaking . if the output is classical , one has to show that the outputs cannot be obtained by direct measurements of the input state . the question whether the memory uses a ( cloud of ) atom ( s ) to store a photonic qubit or topologic error correcting codes to store the state of the nuclear spin of 5 nv-centres is irrelevant for the definition . in the same way that the ram of my computer differs vastly from a poetry book . both are classical memories . then , by definition , the classical capacity cannot be a figure of merit to characterize a quantum memory . but many figures of merits are possible , depending on the application , as with quantum channels . the quantum capacity seems a natural figure of merit , but a memory storing bound entangled state would be excluded by this figure of merit .
excellent question . there are forces for which a force-carrier particle is not it is own antiparticle ( eg : strong force or the weak force ) so if we accept your explanation , then we might have to abandon newton 's 3rd law for those forces , which is implausible . edit : maybe i am being obtuse . i do not think really matters that the antiparticle is not the same particle , so long as it can be interpreted as some particle with the opposite momentum . we never care about what kind of particles carry the force -- if there are many , we just sum over them anyways . so , maybe the reason that you gave does get to the heart of the matter . also , in my explanation below , the derivation of the potential from the propagator depends on the feynman pole prescription which is the interpretation that every particle has a corresponding antiparticle . so i think you hit the nail on the head :- ) if i were to take a shot , i would say that the law " comes from " the fact that only the relative position matters , when two objects exerting a force on each other . think of the energy in the system and the force as $- \frac{\partial u}{\partial x}$ . the " potential " is obtained by inverting the propagator must depend only on the distance between the charges and nothing else ( by rotational invariance ) . distance $= |x_1 - x_2|$ . physically , you can think of either charge in the potential of the other -- so the energy must be symmetric in their charges . this will give us a force law similar to what we have seen before $$u ( q_1 , x_1 ; q_2 , x_2 ) \sim \frac{q_1 q_2}{|x_1 - x_2|} e^{- m |x_1 - x_2|}$$ where $m$ is the mass of the force-carrier particle . for a photon or graviton $m=0$ so we get the usual coulomb/newtonian potential . once we have this expression for the energy , newton 's 3rd law is purely a result in classical mechanics . by computing $f_1 = -\frac{\partial u}{\partial x_1}$ and $f_2 = -\frac{\partial u}{\partial x_2} = - f_1$ we can show that the forces on the two charges are equal and opposite .
forget the friction part , unless you are always peeling rubber or screeching the brakes . the acceleration is the combined sum of forces - air drag , engine thrust at the wheels , slope of the road , etc . , divided by mass ( weight ) . you are right . those things change all the time , so the acceleration and speed are always changing . the cruise control is a feedback system that tries to adjust the throttle to stay at one speed , but only approximately . cars are designed with a property called driveability , which basically means they are not too jerky in acceleration , so they do try to limit the 3rd derivative . the 2nd derivative , acceleration , is limited by engine power and the amount of available tire friction . forget the higher derivatives . you can take as many derivatives as you want . it is a continuous system and infinitely differentiable .
a quantity is local if it is a finite linear combination $\sum_k g_k p_k ( x ) ~~$ of products $p_k ( x ) $ ( or other pointwise functions , such as $\sin \phi ( x ) ~$ for sine-gordon theory ) of field operators or their derivatives at the same point $x$ . a quantum field theory is local if its classical lagrangian density is local . ( by abuse of terminology , an action or a lagrangian may also be called local if the corresponding lagrangian density is local . ) since in qft fields are only operator-valued distributions , a local quantum field product is not well-defined without a renormalization prescription , which involves an appropriate limit of nonlocal approximations . in 1+1d , normal ordering is sufficient to renormalize the field products , while in 3d and 4d more complicated ( mass and wave function ) renormalizations are needed to make sense of these products .
you may consider every excited state of an atom $\psi_n$ as another particle . then their number may be infinite even in this simple example . the higher energy of the system , the higher $n$ . in qft each particle has occupation numbers . during reactions some occupation numbers decrease , some other increase . factually qft equations are the balance equations governing the occupation numbers while interactions .
the deal here is that you have to be very careful about what quantity you are interested in and what you have . assumptions you have measurements $f_i$ of the fraction of stuff ( does not really matter what ) during time interval $i$ . you may also have measurements of $o_i$ of the total output of the medium in which stuff makes up a fraction . alternately you may only know $\bar{o}$ the average output or the total output over the entire time range $\mathcal{o}$ . note that for $n$ measurements at uniform spacing $\mathcal{o} = n * \bar{o}$ . case 1: you want to know " how much stuff " totaled over several time periods and you have both fractional values and outputs for every period . this is as good as it gets . you need to add up the daily stuff . the daily amount of stuff is $s_i = f_i o_i$ . that is just the definition of $f_i$ . so assuming you have the daily outputs you get : $$ \mathcal{s} = \sum_i s_i = \sum_i f_i o_i \quad . $$ case 2: you want to know " how much stuff " totaled over several time periods , but you have the total output or average output without periodic output values . the best you can do is $$\mathcal{s} \approx \sum_{i=1}^n f_i \bar{o} = \mathcal{o} \frac{1}{n}\sum_{i=1}^n f_i = \bar{o} \bar{f}$$ where $\bar{f}$ is defined by this relation as the mean fraction . in this case it makes sense to take a mean of the fractional reading , but only because you do not have enough data to get the right answer . this will be approximately correct if ( 1 ) the $o_i$s have small variance or ( 2 ) the $f_i$s have a small variance or ( 3 ) you have a lot of data and the $f_i$s and $o_i$s are uncorrelated . case 3: you want to show that the fraction is uniformly above or below some limit . then you just need the $f_i$s , and computing their mean and variance is fine as long as ( 1 ) the answer is a long way from the limit and ( 2 ) the $f_i$ are uncorrelated with the $o_i$s . case 4: you want to show that the stuff is uniformly above or below some limit . then you need the daily values to do it right . in the absence of the daily output you can fall back on the average output again , but it will only be reliable if all three conditions listed for case 2 apply . side note : you were concerned in the comments that 2 million parts per million does not make sense . and you are partially right . you can not have a fractional concentration of 2 million ppm , but there are cases when that value is meaningful . you probably already understand this . people are happy to talk about 200% in some cases , but " percent " is literally " per one hundred " . two hundred pars per hundred is the number 2 and it makes sense in cases involving changes but not in cases involve the fraction of people who qualify for something .
yeah , definitely . one example is an inelastic collision , where both masses will have the same velocity after colliding . in this case , let 's say a bullet of mass $m$ and speed $v_0$ hits a stationary rock of mass $m$ and they stick together and move with a final speed $v$ . intuitively , you can already tell that their velocity must be in the same direction as the bullet 's initial velocity . but let 's make it explicit : from conservation of momentum , we have that $$mv_0=mv+mv$$ $$mv_0= ( m+m ) v$$ since mass must always be positive , $v$ must have the same sign as $v_0$ , which means that the bullet moves in the same direction as before-keeps moving forward ( although at a slower speed ) -and is not ' reflected ' by the rock . edit ( to respond to additional question ) . under what conditions does this happen ? essentially , the bullet would have to stick ' into ' the rock .
depends on what you mean by ' central force ' . if your central force is of the form ${\vec f} = f ( r ) {\hat r}$ ( the force points radially inward/outward and its magnitude depends only on the distance from the center ) , then it is easy to show that $\phi = - \int dr f ( r ) $ is a potential field for the force and generates the force . this is usually what i see people mean when they say " central force . " if , however , you just mean that the force points radially inward/outward , but can depend on the other coordinates , then you have ${\vec f} = f ( r , \theta , \phi ) {\hat r}$ , and you are going to run into problems finding the potential , because you need $f = - \frac{\partial v}{\partial r}$ , but you will also need to have $\frac{\partial v}{\partial \theta} = \frac{\partial v}{\partial \phi} = 0$ to kill the non-radial components , and this will lead to contradictions . it is logical that a field of this form is gong to be nonconservative , because if the force is greater at $\theta = 0$ than it is at $\theta = \pi/2$ , then you can do net work around a closed curve by moving outward from $r_{1}$ to $r_{2}$ at $\theta = 0$ ( positive work ) , then staying at $r_{2}$ constant , going from $\theta =0 $ to $\theta = \pi/2$ ( zero work--radial force ) , going back to $r_{1}$ ( less work than the first step ) , and returning to $\theta = 0$ ( zero work ) .
try looking at this problem microscopically : you can imagine the ball consisting of a number of smaller pieces of matter . the total kinetic energy of the entire ball is the sum of the kinetic energies of its pieces : $$t=\frac{1}{2}\sum_im_iv_i^2$$ now , if it were to just fall straight down , all these little pieces would have velocity vector $v_i=v$ and you could write : $$t=\frac{1}{2} ( \sum_im_i ) v^2 = \frac{1}{2}mv^2$$ where $m$ is the total mass . but if it is rotating , the small pieces have an additional component in the velocity : they do not just move downwards , but have also to move around the center of rotation of the body . so in addition to the downward falling component of the motion , the particles have : $$t_{rot}=\frac{1}{2}\sum_im_iv_{\text{rot} , i}^2 = \frac{1}{2}\sum_im_i ( \omega r_i ) ^2 = \frac{1}{2}\omega^2\sum_im_ir_i^2 = \frac{1}{2}\omega^2 i$$ so the total energy is : $$t_{tot}=\frac{1}{2}mv_{cm}^2+\frac{1}{2}\omega^2 i$$
some definitions might be useful : potential : the potential energy per unit charge , $v = \frac{u}{q}$ . potential depends only on the environment and the location , not on what is placed at that location . voltage : a difference in potential , $\delta v$ , between two points in the same environment . you can think of this as the change in potential energy per unit charge for a test charge moving between the two points . so the first answer i would give you is that potential energy depends on the test charge ( $+q$ in your example ) , but voltage does not , because it is per unit charge . but i think what you really mean to ask is , why is not the potential energy of a capacitor , $\frac{1}{2}cv^2$ , the same as the potential energy of a charge moving across the capacitor , $qv$ ? that is because the potential energy of a capacitor represents the energy that had to be put in to move all the charges that are already in the capacitor . the first charge $q$ to be moved from one plate to the other did not need any energy to do it , because at the beginning , the plates were uncharged , and thus at the same potential . after one charge had been moved , there was a potential difference $v_1 = q/c$ . in order to overcome that potential difference , the next charge needed energy $qv_1 = q^2/c$ . after the second charge had been moved , there was a potential difference $v_2 = 2q/c$ . so the third charge needed energy $qv_2 = 2q^2/c$ . . . . and so on . adding all these up gives $$\frac{q^2}{c} + \frac{2q^2}{c} + \frac{3q^2}{c} + \cdots + \frac{nq^2}{c} = \frac{ ( n^2 + n ) q^2}{2c} \approx \frac{ ( nq ) ^2}{2c}$$ where $nq$ is the total charge on the capacitor . ( $n$ particles , each of charge $q$ . ) of course in practice , we consider infinitesimal elements of charge , and do an integral instead : $$\int_0^{q_\text{total}} \frac{q}{c}\mathrm{d}q = \frac{q_\text{total}^2}{2c}$$
a simple google-book search of " solar declination " lead me to this google-book preview of solar energy engineering : processes and systems by soteris a . kalogirou . this book gives the spencer formula as equation ( 2.6 ) , on page 55 .
no , it is not . your system will go through the same point twice in every oscillation , once moving in each direction , and the friction force will be reversed in each pass , so your approach does not work . what you need to consider is the velocity , not the displacement , so $$ma=-kx - \mathrm{sign} ( v ) f_{\mathrm{fric}} . $$ this is not all that helpful in actually figuring out the motion , and to solve that equation you will have to break it down into several parts . also , if static and dynamic friction are different , your mass will stop at its maximum elongation , and you will then have static friction again . this is what causes stick-slip vibrations .
these are basically ways of describing which leading diagram contributes the largest term to the cross-section ; they are named after which of the mandelstam variables characterizes the 4-momentum of the virtual particle . in t- and u-channel processes the 4-momentum of the exchange particle is space-like ( has a negative norm ) . in the s-channel the 4-momentum of the exchange particle is time-like ( has a positive norm ) . this necessitates the annihilation of the incident particles .
why is 100k rpm an issue ? dental drills works at around 10 khz , with is about 600k rpm . :- if you look at the seiko patent for the design of the kinetic , the drive-train steps up the rotation of the spinning weight by 100 fold for the generator . when i charge my watch i shake it at around 5 - 10 hz , which means 0.5-1 khz for the small pinion , which gives 30 - 60k rpm indeed . so i do not think the wikipedia article is that far off the mark . when things are really small and light and precision made , i think you do not have to worry as much about heat dissipation and wear . ( a typical hard-drive disk is about a couple inches in diameter . the pinion can not be more than a 10th of an inch , if you allow two steps to up the rotation speed . )
i think the question you are really trying to ask is : what makes a set of microstates work for thermodynamics . in quantum mechanics , for a set of microstates , you can just take a set of orthogonal states of the system . for classical mechanics , you need to take a set of microstates which all have equal volume . of course , what equal volume means depends on the measure you put on the system . this generally is not covered in popular texts . ( i do not know why ; maybe because people who are popularizing physics think that measures are stupid , unnecessary , and overly complicated mathematical notions . ) for thermodynamics of ideal gasses , you can choose a microstate to be one which contains equal volume in terms of the x and p ( position and momentum ) variables of the particles . if you chose an alternate measure , where the states had equal volumes in terms of x 2 and p 2 variables , i believe that thermodynamics would not work with this measure . for other kinds of systems , if you know enough physics , it is fairly easy to see what the proper generalization of the variables x and p to these system ( hint : position and momentum are complementary variables ) . i do not know what the proper way to explain this is , though .
some good things to remember for basic circuits are that parallel pathways have the effect of increasing the area available to current flow , and because of this always lower resistance -- at least in the basic circuits i know about . another thing i remember is adding inverses always amounts to the product over the sum : ( 1/r1 + 1/r2 + 1/r3 + . . . ) ^-1 = r1r2r3 . . . /r1+r2+r3 . . . } this is a relationship seen in many areas of physics , most notably ( imo ) for the reduced mass of a system , which can make many calculations much simpler . since other people have basically already answered your question ( the angle reveals the length ) , i thought i would give my two cents , i hope it moves your studying along a bit faster . also , unless i missed something too , i think the lower length should be 5 pi/3 . if the path from a to b is a circle , then depending on how you write your fractions , the sum should be 6 pi / 3 or 2 pi . does this also suggest how , simple as it may be , one of my suggestions might speed things up ?
i will take a swing at this , but bear in mind that you probably will not get definitive answers because you are asking about two active and difficult areas of research ( pop iii star formation and re-ionization ) . i will answer the particular questions , but i am hoping you get a feel for the fact that we do not have clear-cut answers yet . during what range of years after the big bang did the stars form ? the consensus is somewhere in the range $20&lt ; z&lt ; 50$ , which corresponds to 50 myr $&lt ; t&lt ; $ 200 myr . the spread is quite large because star formation depends broadly on gas density and temperature . because the big bang produces some big density perturbations and some small ones , they will form stars at different times . what is the expected range of masses of these stars . . . this is really , really hard to answer . until about a year ago , consensus was settled on a very heavy mass distribution , with many ( if not most ? ) stars in a range 100-1000 $m_\odot$ ( see e.g. the 2004 review by bromm and larson ) . this is argued on the grounds that the smallest gravitationally unstable mass in a homogeneous isothermal gas , the jeans mass , runs like $t^{3/2}$ and primordial gas cannot cool as much as metal-polluted gas . the difference in temperature is about a factor of 30 , so pop-iii stars would naively be about 100 times heavier than modern stars , whose mass distribution seems to peak around 0.5 $m_\odot$ . however , recently ( as in , articles in science in the last few weeks ) have reported very detailed simulations of early star formation where the stars stop their own growth as they start to radiate . the result is stars that are a few times 10 $m_\odot$ . still very big by modern standards , but not as big as previously thought . so the jury is out , imo . . . . and what is the expected lifetime before they supernova ? well , it depends on how large they are , but broadly stars of about 40 solar masses last about a million years or less . how they evolve is unclear because it is difficult to compute the rate at which they might lose mass from their surfaces . i assume these stars resulted in the re-ionization of ism ( interstellar medium ) , so what is the evidence and estimates of the age of the re-ionization era ? the role of pop iii stars in reionization is not at all clear . this is slightly outside of my own work , but from my own knowledge i think it is quite well known that ionization was complete by about redshift $z\approx6$ , which is about a billion years after the big bang . remember that many pop iii stars could have already been born , evolved , and died , producing enough metals to produce pop ii stars in the next generation . working out just how much radiation had been poured out is theoretically very difficult . however , as far as i know , observations of a $z=7.085$ quasar have given us some idea that the intergalactic medium around it was not re-ionized . so its more likely that stars after pop iii ( and maybe agn/quasars ? ) had more to do with re-ionization because it happened broadly later than they were born . that is just wild speculation on my part , though .
let $q$ denote the set of all possible configurations of the system ( the configuration manifold ) . consider a point $q_0\in q$ . for the sake of conceptual clarity , and to make contact with physics notation , let 's work in some local coordinate patch around $q_0$ . suppose that $q_0$ represents the position of the system under consideration at time $t_0$ . at a given time $t$ later , the system will be at some position say $q ( t ) $ that is determined by the evolution equations ( the euler-lagrange equations if we are doing lagrangian mechanics ) , and the quantity \begin{align} q ( t ) - q ( t_0 ) = q ( t ) - q_0 \end{align} would be the displacement of the system after a time $t$ . suppose , instead we consider some other curve $\gamma ( s ) $ in the configuration space which starts at the point $t_0$ ; \begin{align} \gamma ( s_0 ) = q_0 , \end{align} and suppose that we compute the displacement \begin{align} \gamma ( s ) - \gamma ( s_0 ) = \gamma ( s ) - q_0 \end{align} that would result from moving along this other curve of our choosing . we call this displacement the virtual displacement after a time $t$ corresponding to moving along the curve $\gamma$ . it is called virtual because it is the displacement in the position of the system that would occur if the system were to move along the curve $\gamma$ of our choosing -- a " virtual " curve as opposed to the " real " curve along which the system travels according to the lagrangian evolution of the system . note . i used the parameter $s$ for the curve $\gamma$ instead of $t$ to emphasize that moving along that curve does not correspond to time-evolution . now what about virtual " infinitesimal " displacements ? well , recall that the term " infinitesimal " in physics essentially always refers to " first order " approximations , see , e.g. this se post : rigorous underpinnings of infinitesimals in physics so when we are discussing a virtual infinitesimal displacement , what we have in mind is taking the virtual displacement $\gamma ( s ) - q_0$ , taylor expanding it to first order in $s$ , and extracting only the first order term . let 's do this : \begin{align} \gamma ( s ) - q_0 = \gamma ( s_0 ) + \dot\gamma ( s_0 ) t + o ( s^2 ) - q_0 \end{align} using the fact that $\gamma ( s_0 ) = q_0$ , we see that the taylor expansion of the virtual displacement is \begin{align} \gamma ( s ) - q_0 = \dot\gamma ( s_0 ) t + o ( s^2 ) , \end{align} and now we notice that to first order in $s$ , the size of the virtual displacement is controlled by the coefficient of $s$ , namely $\dot\gamma ( s ) $ . in other words , virtual infinitesimal displacements ( meaning we just keep the first order contribution in $s$ ) , are determined by the velocity vector of the chosen " virtual curve " at $s_0$ . but if you have taken a differential geometry course , then you know that velocities of curves on a manifold are simply tangent vectors to that manifold ! so virtual infinitesimal displacements can be associated with tangent vectors to the configuration manifold . the intuition to keep in mind here as that a virtual displacement just tells us how far we would get away from a certain point on the manifold if we were to travel on a certain curve of our choosing that may not coincide with the actual motion of the system determined by time evolution . the " infinitesimal " part and identifying this part with tangent vectors comes simply from considering what happens only to first order .
the static air pressure seen by the aircraft does not change with the aircraft 's velocity . your confusion is from a common misinterpretation of bernoulli 's principle . it is not true that a fluid 's pressure will decrease simply by virtue of flowing faster . after all , this violates the idea that physics should be the same in all inertial frames . here is a simple counterexample to the typical interpretation of the bernoulli principle . consider a tube of infinite length and uniform diameter with some gas sitting in it . now consider various coordinate systems with a velocity in the direction of the tube . in these different coordinate systems , the velocity of the gas will be different , but we expect the force on the walls of the tube due to the fluid 's pressure to be the same in all cases . ( the tube is not going to rupture simply because of a choice of coordinate system ! ) instead , bernoulli 's principle says that , in a given flow ( say , along a streamline ) , a local increase in velocity is associated with local decrease in pressure . the canonical example is fluid flow through a tube with a constriction ( a venturi ) . quoting from the wikipedia article for bernoulli 's principle : bernoulli 's principle can be derived from the principle of conservation of energy . this states that , in a steady flow , the sum of all forms of mechanical energy in a fluid along a streamline is the same at all points on that streamline . this requires that the sum of kinetic energy and potential energy remain constant . . . . if a fluid is flowing horizontally and along a section of a streamline , where the speed increases it can only be because the fluid on that section has moved from a region of higher pressure to a region of lower pressure ; and if its speed decreases , it can only be because it has moved from a region of lower pressure to a region of higher pressure . consequently , within a fluid flowing horizontally , the highest speed occurs where the pressure is lowest , and the lowest speed occurs where the pressure is highest . note the emphasis on relative changes occurring on a streamline . the specific flaw in your argument is here : now the fluid is accelerated along a streamline , and hence the static pressure should drop according to the relation given above . in the wind tunnel , something has to do work to accelerate the air to the wind tunnel velocity , adding energy to the flow , which violates the conservation of energy assumption in bernoulli 's principle .
let 's take your last question first . let the stress tensor at a point ( x , y , z ) in the fluid be given as $\sigma$ . you can pick a cartesian basis $\{ e_1 , e_2 , e_3 \}$ and express the components of the tensor in that basis $$ \begin{bmatrix} \sigma_{xx} and \sigma_{xy} and \sigma_{xz} \\ \sigma_{xy} and \sigma_{yy} and \sigma_{yz} \\ \sigma_{xz} and \sigma_{yz} and \sigma_{zz} \end{bmatrix} $$ the normal stresses are simply $\sigma_{xx} , \sigma_{yy}$ and $\sigma_{zz}$ . it is important to realize that these stresses will have different values in another basis . clearly , you can not attach too much physical significance to things that are basis dependent . however , it is a theorem of continuum mechanics that you can always find at least one basis in which the off-diagonal ( shear terms ) are zero . in this basis , the tensor components are $$ \begin{bmatrix} \sigma_{1} and 0 and 0 \\ 0 and \sigma_{2} and 0 \\ 0 and 0 and \sigma_{3} \end{bmatrix} $$ these numbers have actual physical significance . $\max ( {\sigma_1 , \sigma_2 , \sigma_3} ) $ is the largest principal normal stress at the point . similarly , $\min ( {\sigma_1 , \sigma_2 , \sigma_3} ) $ is the smallest normal stress at that point . it is not too hard to realize that $\sigma_1 , \sigma_2 , \sigma_3$ are the eigenvalues of the stress tensor . on the other hand , the pressure is ( -1/3 ) times the trace of the stress tensor , i.e. $$ p = -\frac{1}{3} \sigma_{jj} $$ the trace is an invariant of the stress tensor , so if you take the sum of the diagonals of the stress tensor in any basis you will get the same value . mathematically , $$ tr ( [ \beta ] [ \sigma_{ij} ] [ \beta ] ^t ) = tr ( \sigma_{ij} ) $$ so you see that the pressure and the normal stress are very different entities indeed . in particular , the pressure is isotropic - it has no preferred direction . now consider a state of pure shear in a fluid . to keep matters simple , we will assume planar flow and ignore out of plane components . the stress tensor for pure shear in our standard basis looks like this $$ \begin{bmatrix} 0 and \tau \\ \tau and 0 \\ \end{bmatrix} $$ looks like the normal stresses are zero , right ? not so fast . as this is a symmetric real tensor , you can always find another basis in which you have normal stress components ! in fact , if you solve the eigenvalue problem setting $\det ( \sigma-\lambda i ) =0$ , you get principal normal stresses of $\pm \tau$ . so , in a coordinate system with basis vectors $e'_1 = \{ ( \frac{1}{\sqrt{2}} , \frac{1}{\sqrt{2}} ) \}$ and $e'_2 = \{ ( -\frac{1}{\sqrt{2}} , \frac{1}{\sqrt{2}} ) \}$ rather than $\{ ( 1,0 ) , ( 0,1 ) \}$ , you get a stress tensor from a situation of " pure " shear that looks like this $$ \begin{bmatrix} \tau and 0 \\ 0 and -\tau \\ \end{bmatrix} $$ you can easily verify this by carrying out the change of basis yourself . so what looks like " pure shear " in one basis is biaxial normal stresses in another basis . since the signs are different , you have both tensile and compressive normal stresses in your fluid .
you should be able to avoid the non-continuous derivative at l/2 by splitting the integral into two parts 1 ) from 0 to $l/2 - \epsilon$ and 2 ) from $l/2 + \epsilon$ to l . then take the limit as $\epsilon-&gt ; 0$ of your expectation value for p . this will give you $&lt ; p&gt ; $ , which is nonzero . you will also need $&lt ; p^2&gt ; $ ( which you should find to be zero ) . the uncertainty in momentum can then be calculated from $&lt ; p&gt ; $ and $&lt ; p^2&gt ; $ .
the equations hold due to the non-trivial property of the uniqueness of the taylor expansion . consider the left and right hand side of an equation as a function , \begin{equation} f ( \lambda ) =g ( \lambda ) \end{equation} now taylor expand both sides , \begin{equation} f_0 + \lambda f_1 + \lambda^2 f_2^2 + . . . =g _0 + \lambda g _1+ \lambda^2g _2^2 + . . . . \end{equation} since the taylor expansion is unique " both ways " of taylor expanding are equivalent and we must have \begin{equation} f_i = g_i \end{equation} for all $i$ . intuitively you can understand the statement as follows . the equation must hold at all $\lambda$ so it must hold at $\lambda=0 \rightarrow f_0=g_0$ . now suppose that you increase $\lambda$ a tiny bit . in this case all the $\lambda^2 , \lambda^3 , . . . $ terms are going to be very small . at this point we have , \begin{equation} \lambda f_1\approx \lambda g_1 \end{equation} so we must have $f_1=g _1$ ( since we can make $\lambda$ arbitrarily small ) . you can continue the process iteratively to show that you can equate coefficients of a taylor expansion to all orders .
it is not necessarily true . for a zero potential $v_2$ you have $p_2=0$ , whereas if $v_1$ is a rectangular pit , in general , $p_1&gt ; 0$ .
the opposite sign of the shifts is due to the conservation of the location of the center-of-mass or , equivalently , momentum conservation . at least for the simple system of 2 bodies , the animation on the page is being observed from the inertial frame of the center of mass . one may check this fact by seeing that the trajectories are periodic ( ellipses ) even if the masses are comparable and the initial velocities and locations are generic ( but allowing a bound state ) . in the center-of-mass frame , the total momentum of both bodies is zero . so if one of them moves in one direction , the other is moving in the opposite direction . their trajectories are really ellipses that are similar to each other ( one obtains one from another by scaling , multiplication by a negative number ) . it follows that if the apsis of one body is on one side from the center of mass , the apsis of the other body must be on the other side .
( 1 ) the first law is written in form of differentials themselves , so i think there may be no escape from using differential equations . ( 2 ) the way most commonly the first law is written is , $du=dq-dw_{\text{work done by the system}}$ . here dw is work done by the system . however , in subjects other than physics , more important quantity is the work done by the experimenter . ( this is quite common in chemistry ) as the process in thermodynamics are most " quasistatic" ( http://en.wikipedia.org/wiki/quasistatic_process ) , the container/piston is always in equilibrium . so , $\vec{f_{ext}}=-\vec{f_{int}}$ ( they are equal and opposite ) , then we have , $dw_{\text{by the system}}=-dw_{\text{on the system}}$ . and so the first law can be written as : $du=dq+dw_{\text{work done on the system}}$ . here dw is work done on the system . as the two $dw$s have different meanings we will not a different answer . ( 3 ) the internal energy of a gas is state variable/state function ( http://en.wikipedia.org/wiki/functions_of_state ) . $u$ depends only on the final and initial states of the system and not on what process was used to get from initial to the final state . so we can use a constant volume process to get $du$ which then can be used in any process without any modification .
your calculations are correct , provided the cylinder is indeed ohmic . the constant $e$ you are getting is the difference in electric field between both terminals . as for the current flowing from inside to outside , as you said the cross sectional area will be different , and so will the length . the length $l=r_b-r_a$ , but the cross sectional area is not uniform , because at the beginning of the wire ( the interior ) , $a=2\pi r_al$ , and at the " end " of the wire ( the exterior ) , $a=2\pi r_bl$ . so you will have to treat each portion of the wire as its own infinitesimal resistor $dr$ , and the total resistance is the series combination of them : $$dr=\rho\frac{dl}{2\pi ll}$$ $$r=\int_{r_a}^{r_b}\rho\frac{1}{2\pi ll}dl$$
there does not need to be an magnetic field in the inductor for there to be " back emf " ( i would prefer " induced emf" ) . the induced emf is the consequence of a changing magnetic field and not of a magnetic field itself and hence there can be a changing magnetic field even at zero magnetic field ( something like a positive acceleration downwards for a ball thrown upwards , momentarily at rest . velocity is zero but the rate of change is not ) . the induced emf is given by $e=-\frac{d\phi}{dt}=-l\frac{di}{dt}$ , where $\phi$ is the magnetic flux through the circuit ( inductor ) . as a matter of fact , in a simple ac generator , which works on the principal of electromagnetic induction , the value of the induced emf is maximum when the magnetic flux through the loop of the generator is zero . now , to derive an equation of the current as a function of time , at any time t:- $$e-ir=l\frac{di}{dt}=-e_i$$ where $e$ is the emf of the ideal battery and $e_i$ is the induced emf . rearranging the equation and integrating:- $$\int_0^t dt=\int_0^{i_s}\frac{l}{e-ir}$$ where $i_s$ is the current at infinite time , i.e. at steady state where there is no longer changing magnetic fields and hence no induced emf . this is given by $i_s=e/r$ since the inductor has no effect at steady state . solving the above equation gives us:- $$i ( t ) =i_s ( 1-e^{-\frac{t}{\tau}} ) $$ where $\tau=l/r$ is called the time constant . at time $t=0$ , the current is zero but the rate of change of magnetic field is non zero and hence the induced emf is equal to the battery emf ( the maximum value ) . as time passes , the induced emf reduces slightly , and the current starts slowly and rises steadily till it reaches the steady state at $t\rightarrow \infty$ . you can get the expression for the induced emf as $$e_i=-l\frac{di}{dt}=i_sre^{-\frac{t}{\tau}}$$ the back emf acts as an opposing emf ( principally like a battery of varying emf fixed in an opposing direction to the original battery ) , and its value is maximum at the beginning ( equal to $e$ ) and hence there is zero current , and its value starts dropping as the rate of change of magnetic field starts dropping exponentially , and becomes zero at steady state ( $t\rightarrow \infty$ ) where the rate of change of magnetic field is zero .
mass is not always first . for example we write newton 's law for the force between two objects as : $$ f = \frac{gm_1m_2}{r^2} $$ i do not think there are hard and fast rules . i suspect conventions have arisen over the years and we have all got used to what we learned at school , which was taught by teachers who are used to what they learned at school and so on . we tend to put constants first , as in the case above where newton 's constant $g$ is first , and in many cases the mass is a constant . for example when we write : $$ f = ma $$ in the vast majority of cases $m$ is constant and that is probably why we put it first .
no , its temperature will not stop rising . substances have both a temperature , and a density . remember in chemistry you had two different heat capacities , constant pressure , and constant volume . the water will still heat up , although the heat capacity is the constant volume heat capacity . at some temperature , it becomes a gas , with the same density as it started with .
there are very important differences between this two approaches , that can be summarized by noting that the lippmann-schwinger is the ( formal ) solution of a one-body problem ( scattering of a particle by an external potential ) whereas the dyson equation gives the solution of a many-body problem . i focus here on the non-relativistic many-body case ( it is also the case of the scattering problem ) . it is only in the case where the system is in an external potential and non-interacting ( or empty , assuming conservation of the number of particle ) that the two approaches are equivalent ( or more precisely the dyson equation gives back the lippmann-schwinger equation ) .
the light you see as the image of the sun on the sky is basically undeflected . http://en.wikipedia.org/wiki/diffuse_sky_radiation says it is 75 % when the sun is high and the sky is clear . the frequency dependency is due to rayleigh scattering . for the cloudy sky the fraction is much smaller , up to many orders smaller than unity ( maybe 1 millionth part as a wild guess ) .
you ask : i wonder how e.g. radiation can be a power or a newtonian force . we can get radiation from the creation of the universe but we can not really call that radiation a newtonian force or a power from the big bang or can i say that the big bang is causing newtonian movements today ( ? ) e.g. is radiation a newtonian movement ( ? ) radiation classically carries energy and if the radiation interacts with a body , then power is transmitted , where power is the rate of change of energy . for example there is radiation pressure in vacuum , in this link . radiation does transfer energy in a newtonian way , where the force can be estimated from dp/dt , the instantaneous change of momentum . the big bang is not a simple story to be expounded here and used as an example . have you read the wiki article ? but can there be energy that is energy and no force and no power yes , as said in the comments to your question only interactions need the concept of force and power .
i would rephrase your question as : what is the experimental signature of a black hole . if there exists a definite experimental signature of a celestial body that defines a black hole , your question is answered . i found the following paper that clarifies the issue : classical black holes are solutions of the field equations of general relativity . many astronomical observations suggest that black holes really exist in nature . however , an unambiguous proof for their existence is still lacking . neither event horizon nor intrinsic curvature singularity have been observed by means of astronomical techniques . this paper introduces to particular features of black holes . then , we give a synopsis on current astronomical techniques to detect black holes . further methods are outlined that will become important in the near future . for the first time , the zoo of black hole detection techniques is completely presented and classified into kinematical , spectro-relativistic , accretive , eruptive , obscurative , aberrative , temporal , and gravitational-wave induced verification methods . principal and technical obstacles avoid undoubtfully proving black hole existence . we critically discuss alternatives to the black hole . however , classical rotating kerr black holes are still the best theoretical model to explain astronomical observations . from this it is evident that in contrast to most physics subjects where first there is experimental evidence needing explanation and then theory arrives , black holes are an " artefact " of general relativity theory , i.e. the concept carries all the baggage of gr . nevertheless , what is called a " black hole " in gr has some experimental signatures which any other competing gravitational theory would have to account for . it might not be in the enticing format of a " black hole " , but some data are there . these at the moment are consistent with the theoretical black hole expected from gr . so in a sense your question has as answer : " yes the definition of a black hole is within the terminology introduced by general relativity " and may not be defined outside it ; but also " no the experimental signatures do not depend on general relativity in order to exist , just their interpretation and attribution as a black hole " may be in question , if an alternative theory to gr succeeds to describe reality .
if the universe is infinite there is obviously an infinite number of ways of arranging the matter within it , so there is no requirement for the universe to repeat at large scales . what the article is suggesting is more subtle than this . suppose we take a finite volume . this could be as small as you , or as large as the observable universe , but in both cases there is a finite number of ways of arranging the matter in a finite volume . the reason there is only a finite number of ways is that we assume separations smaller than the planck length can not be distinguished . so our finite volume is made up of a large but finite number of planck volumes , and there is only a finite way of arranging the matter between this finite number of planck volumes . depending on how you do the calculation the number of ways of arranging the matter in the observable universe is around $2^{10^{118}}$ . so if you assume the universe is completely random then the probability of a randomly selected volume the size of the observable universe looking just like ours is 1 in $2^{10^{118}}$ . this is obviously very unlikely , but in an infinite universe there are an infinite number of observable universe sized volumes , so somewhere there will be an exact replica of our observable universe . in fact there will be an infinite number of such exact replicas . none of this has anything to do mith multiple universes . the argument above is just that there must be repeating regions within our universe .
( 1 ) the equation $pv=nrt$ is a " proportionality " type of equation , which necessarily has a linear dependence when only one variable is independent . the class of problems your are dealing with now has only one independent ( you control the change ) and one dependent ( the equation tells you the change ) variable , while the others are held constant . and from a pure math perspective , for it to be a non-linear dependence with only one independent variable there would need to be operators other than multiplication/division ( e . g . exponent , logarithm , sin , cos , etc . ) . ( 2 ) when you are equating two gas states you are really solving the equation : $$ \frac{p_1 v_1}{nrt_1}=\frac{p_2v_2}{nrt_2} $$ and for the case you are looking at $n_1=n_2 $ the variable $n$ is on both sides and cancels . so in this case you are not multiplying either side by $n$ . when you are only looking at one gas state , you are not setting equations equal to each other and there is no cancellation , so you still have the variable $n$ .
the problem is that your coordinates are not well defined at $\theta=0$ and $\phi=\pi/2$ . note in particular that $$ u|_{ ( 0 , \frac{\pi}{2} , \gamma ) } = \begin{pmatrix}1 and 0\\0 and 1\end{pmatrix} $$ for any value of $\gamma$ . a simpler choice is $$ \tilde{u} = \begin{pmatrix} x+iy and z+iw \\ -z+iw and x-iy \end{pmatrix} , $$ with $$ x = \sqrt{1 - y^2 - z^2 - w^2} . $$ differentiating this you find $$ d\tilde u = i\begin{pmatrix} dy and +i\ , dz + dw \\ -i\ , dz + dw and -dy \end{pmatrix} - \frac{y\ , dy+z\ , dz+w\ , dw}{\sqrt{1-y^2-z^2-w^2}}\begin{pmatrix}1 and 0\\0 and 1\end{pmatrix} $$ from which you can read off the pauli matrices at the point $ ( x , y , z , w ) = ( 1,0,0,0 ) $ .
the degrees of freedom of a diatomic gas are as follows : 3 translational : the molecule can move in x , y and z-direction . 2 rotational : the molecule can in principle rotate around each axis . but consider rotations around the molecular axis ( connecting the h atoms ) : in this case , the physics does not change . another way of thinking about it is that the axial rotation mode only can store a vanishing amount of energy , compared to the others . the rotational modes are only available at higher temeratures , since the molecule has a moment of inertia that has to be overcome to start rotating . 2 vibrational : the atoms can wiggle together and apart , which is one degree of freedom . but there is also another one , which is harder to see : think of the molecule as an harmonic oszillator with kinetic and potential energy . if both atoms are displaced towards the middle , the molecule has a higher potential energy . the equations of the harmonic oscillator would normally fix the kinetic energy of the atoms in this case . but in a gas with its random kinematics it is totally possible that they have the " wrong " kinetic energies for their relative displacement . so , for the purposes of statistical mechanics , these are 2 further dofs , making seven .
since electrons do not interact through the strong interaction , an electron-quark " atom " is on the face of it the same as an electron-proton atom . ( except maybe weak interaction decays , i am not entirely sure . ) however : a free quark has never been observed in experiments , and it is widely believed - but not proved - that the theory of strong interactions does not allow free quarks at all . instead , quarks bond in pairs or triplets . the most famous such particles are of course neutrons and protons . so in a sense not only can quarks form composite particles , they most likely have to . bound pairs of quarks have energy levels like atoms , but often the difference in energy is very large . it is the strong interaction after all . the energy difference can be so large that particle physicists have several names for particles that have the same constituent parts .
it is a very , very , very , speculative question . however , simple to settle , dark matter must exist before any black hole is formed . one usually need cold dark matter in the interval around the recombination so the perturbations can grow enough to form the structures that we see now . note also that it needs to be cold to not spoil the growth of structures , therefore , your tachyons in some way must also act like a gas of cold particles .
as in physics in general , a suitable choice of coordinates makes our life so much better . time dilation in this problem is somewhat a more trivial effect , and the transformation of gravitational field is somewhat a more complicated phenomenon . with this in mind , let me reformulate slightly the two situations : case 2 . pendulum is at rest with respect to the earth ( and some observer moves with respect to them , observes time dilation etc etc ) case 1 . pendulum is set above the earth , which moves relativistically below it ( and some observer moves with the earth , observes time dilation etc ) so , let us settle the physics first , and the observer effects last . case 2: classical physics problem , nothing to settle . case 1: from the pendulum 's point of view , the gravitational field is generated by a moving body ( => the field is unknown ) . from the earth frame , a relativistic body moves in a gravity field ( => the equations of motion are unknown ) . one might transform the energy-momentum tensor of the earth from the earth rest frame to the pendulum frame , but special care should be taken about the fact that the earth ceases to be spherical in the new frame ( though its density does increase as $\gamma^2$ ) . additionally , it is not clear appriori that the motion of the earth does not cause any additional forces . i propose to use a straightforward yet more secure method of transforming the metric tensor from the earth frame to the pendulum frame , and hence obtain the gravity , acting on the pendulum . in the earth rest frame the metric tensor is known to be $$g_{\mu\nu}=\left ( \begin{array}{cccc} and 1-2u and 0 and 0 and 0 and \\ and 0 and 1-2u and 0 and 0 and \\ and 0 and 0 and 1-2u and 0 and \\ and 0 and 0 and 0 and -1-2u and \\ \end{array} \right ) , $$ where $u$ is the newtonian potential of the earth . this expression corresponds to the so called weak field limit , when the metric tensor is nearly flat . we use the standard notation of mtw ( $c=1$ , signature $ ( +++ - ) $ , einstein 's summation rule etc ) and refer to this book for further details on linearized gravity . transformation of the field to the pendulum frame : lorentz tranformation matrix is given by : $$ \lambda_{\mu&#39 ; }^{~\mu}=\left ( \begin{array}{cccc} and \gamma and 0 and 0 and \beta \gamma and \\ and 0 and 1 and 0 and 0 and \\ and 0 and 0 and 1 and 0 and \\ and \beta\gamma and 0 and 0 and \gamma and \\ \end{array} \right ) , $$ with $\beta=\dfrac{v}{c} , \gamma= ( 1-\beta^2 ) ^{-1/2}$ and $v$ being the relative velocity of the pendulum with respect to the earth rest frame . the transformed metric tensor is obtained by : $$g_{\mu&#39 ; \nu&#39 ; }=\lambda_{\mu&#39 ; }^{~\mu}\lambda_{\nu&#39 ; }^{~\nu} g_{\mu\nu}=\left ( \begin{array}{cccc} and 1-2u\dfrac{1+\beta^2}{1-\beta^2} and 0 and 0 and -\dfrac{4 u \beta}{1-\beta^2} and \\ and 0 and 1-2u and 0 and 0 and \\ and 0 and 0 and 1-2u and 0 and \\ and -\dfrac{4 u \beta}{1-\beta^2} and 0 and 0 and -1-2u\dfrac{1+\beta^2}{1-\beta^2} and \\ \end{array} \right ) $$ in the pendulum frame ( further primes in the indices are omitted ! ) : it is known that only the term $g_{44}$ determines the newtonian potential . one can see that by writing out the lagrangian for the pendulum : $$ \mathcal{l}=\dfrac{1}{2}g_{\mu\nu} u^\mu u^\nu=\\ =\dfrac{1}{2} ( ( u^1 ) ^2+ ( u^2 ) ^2+ ( u^3 ) ^2- ( u^4 ) ^2 ) -\\ -u ( ( u^2 ) ^2+ ( u^3 ) ^2+4 u^1 u^4 \beta \gamma^2+ ( ( u^1 ) ^2+ ( u^4 ) ^2 ) \dfrac{1+\beta^2}{1-\beta^2} ) $$ here $u^\mu$ is the 4-velocity of the pendulum . as the latter moves non-relativistically ( in its own frame ) , we may consider $u^4\gg u^1 , u^2 , u^3$ and $u^4\approx \mathrm{const}$ , which leaves : $$ \mathcal{l}=\dfrac{1}{2} ( ( u^1 ) ^2+ ( u^2 ) ^2+ ( u^3 ) ^2 ) -u ( u^4 ) ^2\dfrac{1+\beta^2}{1-\beta^2} $$ if the pendulum as a whole did not move with respect to the earth , we would have $\beta = 0$ and $$ \mathcal{l}_0=\dfrac{1}{2} ( ( u^1 ) ^2+ ( u^2 ) ^2+ ( u^3 ) ^2 ) -u ( u^4 ) ^2 $$ effectively , therefore , the pendulum in its rest frame experiences the gravitational field magnified by the factor of $\dfrac{1+\beta^2}{1-\beta^2}$ . the pendulum frequency is thus magnified by $\dfrac{ ( 1+\beta^2 ) ^{1/2}}{ ( 1-\beta^2 ) ^{1/2}}$ . remarks : the neglected terms in the lagrangian are either $\dfrac{v}{c}$ or $ ( \dfrac{v}{c} ) ^2$ smaller than the kept leading terms . hence , up to $\dfrac{v}{c}$ accuracy the direction of motion does not affect the pendulum frequency . finally , lets add time dilations to get the final answers . let the period of the pendulum in the case when observer , the earth , and the pendulum do not move with respect to each other be $t_0$ . then : case 1: in the pendulum frame , as we have seen it has the period of $\dfrac{ ( 1-\beta^2 ) ^{1/2}}{ ( 1+\beta^2 ) ^{1/2}} t_0$ . then in the observer frame , due to time dilation , the period is $\dfrac{1}{ ( 1+\beta^2 ) ^{1/2}}t_0$ . case 2: in the pendulum frame the period is $t_0$ . in the observer frame the period is $\dfrac{t_0}{ ( 1-\beta^2 ) ^{1/2}}$ . to conclude , the two cases are quite different due to the different physics happening . in one case the observed period changes due to the change of the reference frame , whereas in the other there is an additional factor due to the fact that the gravity of a moving source is not the same as that of a still source .
if you take the classical analogy of a charge generating field lines then the force at some point can be taken as the density of field lines at that point . in 3d at some distance $r$ the field lines are spread out over a spherical surface of area proportional to $r^2$ so their density and hence force goes as $r^{-2}$ - so far so good . the trouble with the strong force is that the interactions between gluons cause the field lines to attract each other , so instead of spreading out they group together to form a flux tube or qcd string . in effect all the field lines are compressed into a cylindrical region between the two particles so the field line density , and hence the force , is independant of the separation between the quarks . this means it does not matter what the dimensionality of space is , because the field lines will always organise themselves along the 1d line between the quarks . the quarks woould be confined in any dimension space . annoyingly i can not find an authoritative but popular level article on qcd flux tubes , but a google will find you lots of articles to look through .
i think that the most prominent example of " prediction before observation " in statistical physics is the bose-einstein condensate . it was predicted in ~1925 by , well , bose and einstein , obviously . then after more than ten years it was proposed as an explanation for superfluidity and superconductivity . and the actual bec of atoms ( as a new state of matter ) was obtained only in 1995 .
if $\cal h$ is a complex hilbert space , and $a :d ( a ) \to \cal h$ is linear with $d ( a ) \subset \cal h$ dense subspace , there is a unique operator , the adjoint $a^\dagger$ of $a$ satisfying ( this is its definition ) $$\langle a^\dagger \psi| \phi \rangle = \langle \psi | a \phi \rangle\quad \forall \phi \in d ( a ) \: , \forall \psi \in d ( a^\dagger ) $$ with : $$d ( a^\dagger ) := \{ \phi \in {\cal h}\:|\: \exists \phi_1 \in {\cal h} \mbox{ with} \: \langle \phi_1 |\psi \rangle = \langle \phi | a \psi\rangle \:\: \forall \psi \in d ( a ) \}$$ the above densely-defined operator $a$ is said to be self-adjoint if $a= a^\dagger$ . a densely-defined operator satisfying $$\langle a \psi| \phi \rangle = \langle \psi | a \phi \rangle\quad \forall \psi , \phi \in d ( a ) $$ is said to be symmetric . it is clear that the adjoint of $a$ , in this case , is an extension of $a$ itself . a symmetric operator is essentially selfadjoint if $a^\dagger$ is self-adjoint . it is possible to prove that it is equivalent to say that $a$ admits a unique self-adjoint extension ( given by $ ( a^\dagger ) ^\dagger$ ) . the operator $-i\frac{d}{dx}$ with domain given by schwartz ' space ${\cal s} ( \mathbb r ) $ ( but everything follows holds also if the initial domain is $c_0^\infty ( \mathbb r ) $ ) is symmetric and essentially self-adjoint . both $-i\frac{d}{dx}$ and the true momentum operator $p:= \left ( -i\frac{d}{dx}\right ) ^\dagger$ are not bounded . both operators do not admit eigenvalues and eigenvectors . the spectrum of $p$ is continuous and coincides with the whole real line . passing to fourier-plancherel transform , the operator $p:= \left ( -i\frac{d}{dx}\right ) ^\dagger$ turns out to coincide with the multiplicative operator $k \cdot$ . concerning the issue of unboundedness of most self-adjoint quantum operators , the point is that a celebrated theorem ( one of the possible versions of hellinger–toeplitz theorem ) establishes that : a ( densely-defined ) self-adjoint operator $a :d ( a ) \to \cal h$ is bounded if and only if $d ( a ) = \cal h$ and almost all operators of qm , for various reasons , are not defined in the whole hilbert space ( unless the space is finite dimensional ) . these operators are not initially defined on the whole hilbert space because they usually are differential operators . differential operators need some degree of regularity to be applied on a function , whereas the generic element of a $l^2$ space is incredibly non-regular ( it is defined up to zero-measure sets ) . the subsequent extension to self-adjoint operators exploits a weaker notion of derivative ( weak derivative in the sense of sobolev ) but the so-obtained larger domain is however very small with respect to the whole $l^2$ space . addendum . in view of a remarkable andreas blass ' comment , i think it is worth stressing a further physical reason for unboundedness of some self-adjoint operators representing observables in qm . first of all the spectrum $\sigma ( a ) $ of a self-adjoint observable represented by a self-adjoint operator $a$ has the physical meaning of the set of all possible values of the observable . so if the observable takes an unbounded set of values , the spectrum $\sigma ( a ) $ must be an unbounded subset of $\mathbb r$ . secondly , if $a$ is a self-adjoint operator ( more generally a normal operator ) , the important result holds true : $$||a|| = \sup\{ |\lambda| \:|\: \lambda \in \sigma ( a ) \}$$ including the unbounded cases where both sides are $+\infty$ simultaneously . so , if an observable , like $p$ or $x$ or the angular momentum , takes an unbounded set of values , the self-adjoint representing it has necessarily to be unbounded ( and therefore defined in a proper dense subspace of the hilbert space ) .
the density of water is , nominally , 1g/ml , but varies depending on several factors impurities ( dissolved ions , salts , other solutes will change the density ) salts and ions are typically higher mass atoms/molecules than the 18atm h$_2$o molecule , as such any dissolved ions increase the mass of the liquid in the same volume and therefore increase the density , ocean salt water can be 3-5% more dense than freshwater at the same temperature , for instance . salt water also depresses the freezing point and this affects the temperature dependence of the density . temperature water density varies greatly with temperature , fresh water near boiling can be as low as 950 g/l ( . 95g/ml or 950kg/m$^3$ ) increasing as temperature decreases to a maximum at 4$^\circ$c ( 1g/ml ) before decreasing slightly to freezing . for water room temperature and below the variation is less than $ . 5$% . this is by far the largest contribution to variation in density for fresh water at normal earth conditions . external pressure liquids are relatively poorly compressible , so at atmospheric levels the atmospheric pressure will contribute negligibly to the density of water , however freezing temperatures get depressed with changes in atmospheric pressure and can affect the density vs temperature . at higher pressures the effects are more pronounced for all of these conditions , there are really no equations to determine the density , and those that exist are typically quantitatively based on experimental measurement and apply to a certain set of conditions . the answer is that you have to look it up based on your conditions . ( most tables for density vs temperature online are given for fresh water at 1 atm ) depending on the accuracy you desire you can analyze all these properties without measuring its mass to determine the density with greater precision . however for most daily uses , an estimate of 1g/ml is close enough
your example has a tricky issue involving angular momentum ( see below ) , but i can address the spirit of the question using a much simpler example . let us imagine we have a chamber containing two gases , $a$ and $b$ , such that $a$-$a$ interaction terms are equal to both the $b$-$b$ interaction terms and the $a$-$b$ interaction terms . ( it does not hurt to imagine the molecules as red and blue spheres of the same size and weight . ) now we imagine swapping some of the $a$ molecules with $b$ molecules in such a way that the $a$ molecules all end up on one side of the chamber and all the $b$ molecules on the other . due to the assumption above , swapping two particles does not change the total energy , but by doing this we have created an ordered system from which work can be extracted . ( to extract work from this system you need a piston that is permeable to $a$ molecules but not $b$ molecules , and another piston that is only permeable to $b$ but not $a$ . see here for details on how , as well as some other relevant stuff about the relationship between work and entropy . ) now , since we can extract work from this system , will its weight change due to the $e=mc^2$ relation ? perhaps surprisingly , the answer is no . this is because the relevant $e$ is the internal energy of the system ( usually written $u$ in thermodynamics ) , and that has not changed . the work that can be extracted from a system at a constant temperature is given by $u-ts$ . by reordering the atoms we have reduced $s$ but kept $u$ constant . when we extract work from this ordered system , its internal energy $u$ also stays constant , but the energy of the environment reduces . effectively , we take heat out of the environment and convert it to work . normally this is not allowed by the second law , because converting heat into work would cause a reduction in entropy - but through a clever use of semi-permeable pistons we can offset that reduction in entropy by an increase in the entropy of the gas mixture . the point is that the $s$ term represents the disorder ( or , more correctly , it represents the opposite of information - see the paper linked above ) and the $u$ term represents the energy . the mass of an object depends only on the $u$ term and not on the $s$ term , so order and mass/energy are actually quite independent things . the tricky thing with your example is that although you keep the energy constant , changing the velocities in the way you describe adds angular momentum to the system . when momentum is involved , $e=mc^2$ is no longer strictly valid , and you have to use the full energy-momentum relation $$ e^2 = ( pc ) ^2 + ( mc^2 ) ^2 , $$ where $p$ is the relativistic momentum . i do not know how to do this for your example . it may or may not be the case that the effective mass would change . however , if it does change it is because the momentum has changed and not because the system has become more ordered .
it sounds as if you are describing flow through an orifice plate . in that case see this wikipedia article for how to calculate the flow rate or google for something like " gas flow orifice " . there are various web sites with flow rate calculators e.g. this one . we are not supposed to just give links as an answer , but the formulae for calculating the flow are quite complicated and i am not sure what would be achieved by copying and pasting them here .
1 ) notice that by inserting a complete set of position states we can write $$ \hat p \psi ( x ) = \langle x|\hat p|\psi\rangle = \int dx'\langle x|\hat p|x'\rangle\langle x'|\psi\rangle =\int dx'\langle x|\hat p|x'\rangle \psi ( x' ) $$ so if we set $$ \langle x|\hat p|x'\rangle = -i\hbar \frac{\partial}{\partial x}\delta ( x-x' ) =i\hbar \frac{\partial}{\partial x'}\delta ( x-x' ) $$ then we can use integration by parts to obtain $$ \hat p \psi ( x ) =i\hbar \int dx'\frac{\partial}{\partial x'}\delta ( x-x' ) \psi ( x' ) = -i\hbar \int dx'\delta ( x-x' ) \frac{d \psi}{dx'} ( x' ) = -i\hbar \frac{d\psi}{dx} ( x ) $$ so your expression is correct . the derivative of a delta function is essentially defined by the integration by parts manipulation that i just performed ; in fact derivatives of distributions in general are defined in an analogous way . see this lecture for example . hope that helps ; let me know of any typos ! cheers !
there is no unique way to " further decompose " the deformation into the " rigid transformation " and " others " because whatever " rigid part " you choose , you may always calculate " others " as a simple difference ( that is because there is really no global constraint on the " other " part ) . so the " rigid part " may be anything you want .
in the representation $|\psi\rangle = a|0\rangle + b|1\rangle $ we must have $|a|^2+|b|^2=1$ , so that gives us one constraint . the second is that an overall global phase does not make any difference . we can use these two freedoms to chose $a$ and $b$ in a specific way . traditionally we choose them such that $$|\psi\rangle = \cos \theta|0\rangle+\exp ( i\phi ) \sin \theta|1\rangle $$ see this wiki article on the bloch sphere for details .
just use the jacobian of the coordinate system transformation . if your cartesian coordinates are $\mu$ and $\nu$ and your cylindrical coordinates are $\mu ' , \nu'$ , then there is a jacobian ${f_\mu}^{\mu'}$ that allows you to write $$f^{\mu ' \nu'} = f^{\mu \nu} {f_\mu}^{\mu'} {f_\nu}^{\nu'}$$ where the jacobian is given by $${f_\mu}^{\mu'} = \frac{\partial x^{\mu'}}{\partial x^\mu}$$ now that is all well and good , but you might be thinking it is a bit abstract , and . . . it is . there is another way to do this instead , using what is called geometric algebra . in geometric algebra , the em tensor is called a bivector , taking on the form $$f = f_{tx} e^t \wedge e^x + f_{ty} e^t \wedge e^y + \ldots = \frac{1}{2} f_{\mu \nu} e^\mu \wedge e^\nu$$ where $e^\mu$ represent basis covectors . what we have used here is called a wedge product , and orthogonal basis vectors will anticommute under it . to extract the components in a new basis , you have a couple choices : ( 1 ) you can write the basis covectors in terms of the cylindrical basis and simplify . so that would entail writing $e^x$ and $e^y$ in terms of $e^\rho$ and $e^\phi$ . this is equivalent to finding the inverse jacobian . however , there is another choice ( 2 ) , which is to simply take the inner product of the basis vectors $e_\rho \wedge e_t , e_\phi \wedge e_t$ and so on with $f$ . this requires a little more knowledge of geometric algebra , but you can write $e_\rho \wedge e_t$ in terms of $e_x \wedge e_t , e_y \wedge e_t$ , and so on , which may be an easier computation . i will do the latter here to demonstrate the technique . see that $e^\rho = e^x \cos \phi + e^y \sin \phi$ . we can then find $f^{t \rho}$ as : $$\begin{align*}f^{t\rho} and = f \cdot ( e^\rho \wedge e^t ) \\ and = f \cdot ( e^x \wedge e^t \cos \phi + e^y \wedge e^t \sin \phi ) \\ and = f^{tx} \cos \phi + f^{ty} \sin \phi \end{align*}$$ this is no more exotic that finding the components of a vector in a new basis by finding the projection of the vector on each new basis vector .
the intensity of the light from the sun at the orbit of the earth is around 1.4 kilowatts per square metre . for comparison , a domestic heater is usually around 3 kw , so a satellite with a 2m surface area ( admittedly this is bigger than most satellites ) facing the sun needs to dissipate as much energy as used to heat your living room . this is in addition to the waste heat produced by the electronics on the satellite . for most satellites it is keeping cool that is the problem , and that is why they are wrapped in reflective foil . where you have a satellite that needs to stay really cool , such as the herschel space observatory , the satellite has to carry a supply of liquid helium to cool itself . in fact the herschel space observatory ran out of liquid helium at the end of april last year and can no longer operate . it is certainly true that if you can stay out of sunlight space is a pretty cold place . if you could avoid all reflected light then in principle you could cool to the 2.7k temperature of the microwave background , and this is cold enough to use superconductors . however this is not a practical way to operate most satellites .
interference of sound waves might work for you . one experiment that comes to mind is to have two speakers playing identical periodic sounds . ( i do not think they have to be pure sinusoidals , but " simple " so it sounds like a note . ) depending on where you stand relative to the speakers , you will hear " dull " areas where the sound is not loud , demonstrating deconstructive interference . depending on the patience of your group , you could even have them walk around and map out the destructive areas . ( i actually think mapping it out would be quite hard ; test it before doing it . ) the number of destructive areas and their areas can be adjusted by tuning the frequencies of your source and/or the speaker separation . there is a phet simulation here .
i have seen bubbles made with hydrogen . this is a popular trick with the various lecturers who do fireworks related lectures because the bubbles make a satisfying pop if you ignite them . a bubble is mainly stabilised by layers of surfactant adsorbed at the gas/water interface . as the bubble wall thins , the adsorbed surfactant layers at the opposite gas/water surfaces come into contact and prevent further thinning . this is a purely kinetic barrier as the gas/water surface tension is still greater than zero ( i.e. . you had reduce the overall energy by reducing the surface area ) but the rate of desorption of surfactant from the surface is slow . in principle the gas will affect the adsoption of the surfactant at the gas/water interface and possibly affect the stability , but in practice all common gases are so different from water that the relatively minor differences between gases makes little difference . bubbles can even be blown with steam as long as you keep the gas phase temperature above 100c . however , over medium timespans the gas inside the bubble will diffuse out through the water film and cause the bubble to shrink . the rate at which this happens will depend on the solubility of the gas and its diffusion rate in water . i am sure there will be differences between hydrogen and air , though i do not know of anyone who has actually measured it . i found papers reporting diffusion rates here and here , though both are behind paywalls . i had better luck with solubility figures . both hydrogen and helium are about a factor of ten less soluble in water than nitrogen , which would make their bubbles more stable than air bubbles though their greater diffusion rates will counteract this to some extent .
first of all , bernoulli 's law is applicable only to inviscid flow , while poiseuille 's flow is for the viscous fluid . the fact that pressure is constant along the orthogonal cross section of the pipe could be derived from the assumption that the flow is parallel , that is everywhere inside the pipe the velocity field has only z-component ( assuming the cylindrical coordinate system , with pipe oriented along the z-axis ) . then the r-component of navier-stokes equation is then reduced to $0 =- \frac1{\rho}\frac{\partial p}{\partial r}$ ( all terms containing velocity components are equal to zero here ) , which gives the pressure independent of radial coordinate . the assumption that the flow in the pipe would be parallel in derivations of poiseuille 's flow is just an ansatz , compatible with the symmetries of the problem , that is later justified by producing correct solution of navier-stokes equation . however one should remember that such flow would be stable only for relatively large viscosity of the fluid ( that is for reynolds number not exceeding the certain critical value ) .
two objects that are initially at rest with respect to each other have initially parallel world lines . however , the curvature of spacetime means that world lines that are initially parallel do not remain so . this is called geodesic deviation . ( image credit ) in the above image , the geodesic segments are parallel at the equator but , nonetheless , converge at the pole . imagine the time direction ( and remember , every object is relentlessly " moving " forward through the time direction even when " at rest " in space ) is along lines of longitude and the spatial direction is along lines of latitude . if the surface were , instead , a plane , the two geodesics would remain parallel and the objects associated with those world lines would not move towards or away from each other .
a conducting body can have a potential , and it need not be zero . potential can be arbitrarily set , depending upon your reference potential . the only difference in the tratement of conducting bodies is that they must be equipotential , i.e. , they must have constant potential at all points inside them ( but not necessarily points inside cavities ) . the potential of a metal shell due to its own charge $q_1$ is $\frac{kq_1}{r_{shell}}$ if you add a point charge $q_2$ at the center , then the potential becomes $\frac{kq_1}{r_{shell}} + \frac{kq_2}{r_{shell}}$ . remember , potential at a point is can be defined with respect to the work required to get a test charge there from infinity . if the field at a point is zero , that does not imply that the field is zero all the way to infinity . it just means that you can jiggle a test charge in the neighborhood of that point without doing work .
you are right about the equation of motion for an object in free fall with air resistance ( well , almost right : your $c$ is not the usual definition of the drag coefficient ) , but when you integrate it , you do not go from $v^2$ to $s^3/3$ . that only works when the thing being squared is actually the variable of integration : $\int t^2\mathrm{d}t = t^3/3$ , but $\int f ( t ) ^2\mathrm{d}t \neq f ( t ) ^3/3$ . to properly solve the equation , you will need to start by finding speed as a function of time . you can write the equation as $$\frac{\mathrm{d}v}{\mathrm{d}t} = g - \frac{c}{m}v^2$$ this is a separable differential equation , so you can put everything involving the independent variable $t$ on one side and everything involving the dependent variable $v$ on the other side , $$\frac{\mathrm{d}v}{g - \frac{c}{m}v^2} = \mathrm{d}t$$ this can be integrated over $t$ , giving $$t = \int_{v ( 0 ) }^{v ( t ) }\frac{\mathrm{d}v}{g - \frac{c}{m}v^2} = \sqrt{\frac{m}{cg}}\tanh^{-1}\biggl ( \sqrt{\frac{c}{mg}}v\biggr ) $$ ( assuming $v ( 0 ) = 0$ ) . then you can solve this for velocity , $$\frac{\mathrm{d}s}{\mathrm{d}t} = v = \sqrt{\frac{mg}{c}}\tanh\biggl ( \sqrt{\frac{cg}{m}}t\biggr ) $$ which is another separable equation , $$\int_{s ( 0 ) }^{s ( t ) }\mathrm{d}s = \int_0^t\sqrt{\frac{mg}{c}}\tanh\biggl ( \sqrt{\frac{cg}{m}}t\biggr ) \mathrm{d}t$$ the result of that integration is $$s ( t ) = s ( 0 ) + \frac{m}{c}\log\cosh\biggl ( \sqrt{\frac{cg}{m}}t\biggr ) $$ i have written a blog post about a ( possibly ) interesting " application " of this calculation . the math above is basically a summary of part of that post .
let 's speak about 1d particles for simplicity . what should be understood first of all is that for indistinguishable particles configuration space is not the same as for distinguishable ones . for two distinguishable spinless 1d particles configuration space is a square : one side is for $x_1$ , another for $x_2$ . but if the particles appear indistinguishable , then half of the space is redundant : states obtained by exchange of particle coordinates are identical . so , for two spinless particles configuration space is really a triangle : here lower purple triangle is the config space . now , schrödinger equation for two indistinguishable spinless 1d particles becomes trivially to obtain from that for distinguishable particles : we just have to impose boundary conditions for $x_1=x_2$ line . for bosons wavefunction must be symmetric , and this implies homogeneous neumann conditions . for fermions wavefunction must be antisymmetric , so it has a node at the line of interparticle collision , and this means homogeneous dirichlet conditions . now for fermions this automatically gives us the correct eigenstates , which indeed obey pauli exclusion principle — they just can not have nonzero values on $x_1=x_2$ line . here're first several 1 : if we now want to return to our full ( square ) configuration space , we should just append zeroes to missing triangle , and then subtract the same wavefunction but with exchanged particles from it . it will automatically appear differentiable at $x_1=x_2$ line : now let 's include spin in our picture . thanks to spin being a discrete degree of freedom with finite range of values , for single particle we can just concatenate its configuration space parts corresponding to different spin values . for example , a state of a spin-$\frac12$ particle in infinite box in state $\left|1\uparrow\right\rangle+\left|3\downarrow\right\rangle$ could look like this : here left part corresponds to state of spin-up component of wavefunction , and right one is for spin-down component . note that the hamiltonian should not try to differentiate the wavefunction at the joint — as far as we neglect spin-dependent interactions , its matrix should just be a diagonal block matrix of two spinless hamiltonians . now for two indistinguishable particles nothing changes except we now use 4 times bigger full configuration space ( than in case without spin ) , or 2 times bigger correct triangle-shaped space . here 's how full configuration space would look : here spin states are denoted as $\left|s_1\right\rangle\left|s_2\right\rangle$ where $s_i$ is spin of particle $i$ . lines of interparticle collisions are denoted by slight dark color . now the truncated symmetrized configuration space : the upper-left rectangle is now merged with lower-right one . how — it will depend on the state . also note that there is still possibility of interparticle collision — inside this lower-right rectangle — which we have not symmetrized , and in fact must not do ( there is no reason to do it in this case ) . now it is easy to see that the states with identical spins are forced to have antisymmetric orbital — their orbital parts of configuration space are triangular . the states with non-equal spins may be symmetric or antisymmetric ( or asymmetric at all ) . this corresponds to known classification of two-particle states into spin-singlets and spin-triplets . just looking at the wavefunction , one could immediately say what type of state we have : triplet states will have a node along interparticle collision line — this means the orbital is antisymmetric . let 's now look at first several states : here we can see : spin-singlet states — numbers 1,2,6 , and triplets — numbers 3,4,5 . for more than two particles it seems quite straightforward to generalize : just truncate the configuration space so that it is no longer possible to exchange particles , and impose the correct boundary conditions on the hypersurfaces which appear after these cuts . for many particles this may even give some computational resource savings : hypervolume of configuration space would reduce by $n ! $ where $n$ is number of particles , as compared to space for distinguishable particles . 1: i used simplified way of computing these states , just putting very large potential barrier in the upper-left triangle . to decouple different spin parts of wavefunctions i also added thin barriers . in real computation one should , of course , use the opportunity to remove extra data from processing and more correctly define the hamiltonian matrix .
that is a really good question . you are right that measuring the tranverse velocity is a very difficult measurement , mostly due to andromeda 's distance from the sun . the problem can be tackled in two ways : directly , and indirectly . direct measurements mean actually tracking a positional change between andromeda and even more distant objects assumed to be essentially at rest , like quasars . the recent discovery of water masers mentioned above should make this possible ; a transverse velocity of ~100 km/s is an angular shift on the order of 10 microarcseconds per year . this is much smaller than is possible with optical telescopes ; the extreme baselines of radio telescopes like the very long baseline array , however , do make direct measurements feasible . these observations are currently taking place , and we should have a published measurement within a couple of years . indirect measurements of andromeda 's transverse velocity use a few different techniques . the loeb et al . ( 2005 ) paper made their estimate based on the fact that m33 , a neighboring galaxy to andromeda , shows no sign that its stellar population has been disturbed by passing nearby andromeda . this constrains the possible range of directions and speeds of andromeda 's velocity . they combine this with data on m33 's orbit , plus simulations of how close the galaxies would have to be to show an effect , and estimate both a direction ( mostly eastward ) and speed ( $100 \pm 20$ km/s ) of andromeda 's proper motion . a second indirect method was published by van der marel and guhathakurta in 2008 ; they used information on the orbits of satellite galaxies orbiting m31 to estimate the center of mass ( or barycentre ) of our local group . since the position and velocity of the local group barycentre depend partially on m31 's orbit , they also estimated a transverse velocity . their result is -78 km/s w , -38 km/s n . the upcoming direct measurement of m31 's proper motion should answer which ( if either ) of these other estimates are correct . in addition , we are looking forward to answering several interesting questions regarding both the past and future of our local group of galaxies . stay tuned !
yes . it turns out that your $t_l$ is equal to $-t/\omega$ , where $\omega$ is the angular velocity and $t$ is the usual temperature . we normally work with the reciprocals of such quantities , and in the language of non-equilibrium thermodynamics we say that a gradient in $-\omega/t$ is the " thermodynamic force conjugate to " a flow of angular momentum . within the formalism of thermodynamics itself there is indeed nothing special about energy . ( there is , however , quite a lot that is special about energy when it comes to mechanics . ) however , the usual terminology and notation obscures this quite a bit . we usually write the fundamental equation of thermodynamics with the energy on the left-hand-side , like this : $$ du = tds - pdv + \sum_i \mu_i dn_i . $$ this equation can be extended with many other terms , including $\phi dq$ ( electric potential times change in charge ) and $\omega dl$ ( angular velocity times change in angular momentum ) . however , the " special " quantity here is the entropy , $s$ , which is non-decreasing while all the other extensive quantities are conserved . we can rearrange this to put the special quantity on the left , and to get $$ ds = \frac{1}{t} du + \frac{p}{t}dv - \sum_i \frac{\mu_i}{t}dn_i + \dots - \frac{\phi}{t}dq - \frac{\omega}{t} dl . $$ this observation is the basis of non-equilibrium thermodynamics . it follows immediately from this that $$ \frac{\partial s}{\partial l} = -\frac{\omega}{t} . $$ it also follows that angular momentum cannot be spontaneously transferred from one body to another while keeping all other quantities constant unless the second body has greater $-{\omega}/{t}$ . however , that " while keeping other quantities constant " is a bit tricky . in just about any reasonable situation , adding angular momentum to a system will also change its energy . the same is true of changes in volume , chemical composition or charge : changing these things will , generally speaking , also change the energy . this is probably the main historical reason why energy is seen as special in thermodynamics : it is the only thing you can practially change while keeping everything else constant . ( we call this " heating up " or " cooling down " a system . ) so while it is quite possible to define angular-momentum analogues of heat , free energy and the carnot limit , these do not tend to have the same immediate practical applications as the energy-based versions . nevertheless , i think the existence of such quantities is an enlightening and often-overlooked observation . i would encourage you to keep on thinking along these lines , since understanding the symmetry between energy and the other conserved quantities leads to a deeper understanding of thermodynamics as a whole .
given the rather large volume of the universe , i suppose it is possible . not as an initial condition as far as i can tell though because of the conservation of angular momentum . however , given the right circumstances of impact events on a rogue planet ( with no other bodies to perturb its non-rotation ) , i suppose it is possible . highly unlikely , but theoretically possible . as to why planets rotate , cornell ( the home of carl sagan ) has a great explanation . what i am saying is that there will be no planets if there was no initial angular momentum in the primordial solar nebula . if a nebula with absolutely no rotation collapses , then there will only be a central non-rotating star and there will not be any planets . planets form out of a protostellar disk , which itself forms only because of the initial angular momentum of the cloud . the dynamics of a rotating body is of course controlled by forces like gravity . kepler 's laws are a direct consequence of gravity .
the rule is that the total momentum of an isolated system is constant . in the example with the car and the ferry , the isolated system is the system consisting of the car and the ferry . the system is isolated because it is assumed that there is no interaction ( such as drag ) between the ferry and the water . then you could imagine that the car and ferry are stationary with respect to each other . then as the car begins to move and gains momentum in one direction , the ferry gains momentum in the other direction . conservation of momentum guarantees that the car and ferry momenta add to zero so that the total momentum is zero . it is important to notice that the rule is not that total momentum is zero , but that total momentum is conserved . so if the car and ferry were initially drifting with some speed , then when the car starts moving across the ferry , the car will gain momentum and the ferry will lose momentum but the total momentum will remain at its initial non-zero value . also notice that in a more realistic model , you would include the drag force from the water . then the momentum of the car/ferry system would not be conserved ( which is allowed because it is no longer an isolated system ) . but if you add in the momentum of the water , you will find that still the total momentum is conserved .
it can be argued that there are two primary challenges associated with quantum computing . one is a coding challenge , and the other is a purely mechanical ( or in some minds physical ) challenge . as ron indicated , viable quantum error correction code ( qecc ) is probably the largest breakthrough that makes quantum computers possible . peter shor was the first to demonstrate a viable quantum error correction code that can also be characterized as a decoherence free subspace . at this point , it can be argued that most of the problems associated with fault tolerant quantum computing are resolvable in terms of theory . however , their is a question of scaling as it relates to the number of qubits that are required to make the system viable , and the physical size of the system that is required to support those qubits . most arguments that say there is a needed breakthrough in physics can be traced to this question of scalability . in many cases , the difficulties are related to decoherence times and the speed at which the computers can perform calculations , which would allow the qecc time to operate . at this point those are now considered largely engineering challanges . as far as physical breakthroughs , it is likely that those conversations are in regards to topological quantum computers , which rely upon the construction of anyons in order to operate . currently these are largely viewed as pure mathematical constructions , however , the construction and use of stable anyons , even as quasiparticles , as part of topological quantum computer would be a major physical breakthrough .
you need to know the rate of heat given by the flame to the water . suppose the flame transfers $h$ kj/s to the water . the latent heat of evaporation of water is $2260$ kj/kg for energy balance , the heat given to the water must be equal to the amount of heat required to convert water into steam . $$ h = \dot{m} \times 2260 \\ \therefore \dot{m} = \frac{h}{2260} \text{ kg/sec} $$ if you say the rate of heat transfer does not matter ( ignore the flame and assume the water is at a certain temperature and stays at that temperature throughout the experiment ) , $$ \dot{m} = \frac{\theta a ( x_s - x ) }{3600} \text{ kg/s} $$ where $\theta = ( 25 + 19 v ) $ is the evaporation coefficient ( $kg/m^2\cdot hr$ ) . this is an empirical equation , so you can not derive it from first principles . $v$ is the velocity of air just above the surface of the water ( $m/s$ ) $a$ is the surface area of the water ( $m^2$ ) $x_s$ is the humidity ratio in saturated air at the same temperature as the water surface ( kg h2o in kg dry air ) $x$ is the humidity ratio in the air ( kg h2o in kg dry air ) it is fairly straightforward to find $x$ and $x_s$ from the relative humidity and the mollier chart
the set $g$ gives the representation of the identity and generators of the abstract group of quaternions as elements in $sl ( 2 , \mathbb c ) $ which are also in $su ( 2 ) $ . taking the completion of this yields the representation $q_8$ of the quaternions presented in the question . from the description of the symmetry group as coming from here , consider the composition of two $\pi$ rotations along the $\hat x$ , $\hat y$ , or $\hat z$ axis . this operation is not the identity operation on spins ( that requires a $4\pi$ rotation ) . however , all elements of $d_2$ given above are of order 2 . this indicates that the symmetry group of the system should be isomorphic to the quaternions and $q_8$ is the appropriate representation acting on spin states . the notation arising there for $d_2$ is probably from the dicyclic group of order $4\times 2=8$ which is isomorphic to the quaternions .
gold foil is quite easy to hold you just hang it from a paperclip . the only difficulty is if there is a lot of static electricity in the air which makes it stick to things . ( this is the main reason for the cold damp cambridge 's supremacy in early particle physics ) photographic film at the time was not sensitive and so in marsden and geiger 's experiments they used a tiny target coated with phosphorescent crystals which emitted a spark of light when hist by a particle and viewed them through a microscope . this involved sitting in a dark cupboard for hours straining to see and count rare pinpoints of light while staring down a primitive microscope - this is why the job was given to grad students . edit : there is a very good book , the fly in the cathedral which describes this period , the science and the experimental techniques . high vacuum stuff is a pain the ___ even with modern quick connects and helium leak detectors , doing it with string and sealing wax ( literally ) was quite a challenge .
there are tons of papers on the connection between quantum processes and probability theory ( though i do not understand why you single out coherent states - they do not play a special role in this connection ) . the theory of stochastic processes and the theory of quantum processes are the commutative and noncommutative side of the same coin , with many similarities . see , e.g. , the books by gardiner ( handbook of stochastic processes ) or barndorff-nielsen ( quantum independent increment processes : structure of quantum lévy processes , classical probability , and physics ) online is the following article by barndorff-nielsen http://www.jstor.org/stable/10.2307/3647584
i think i understand what you mean when you say that you are not satisfied with the “nontrivial bulk topology argument” when it comes to thinking about edge states . the chern number ( for time-reversal breaking ) and $\mathbb{z}_{2}$ invariant ( for time-reversal symmetric ) systems , as danih suggested , does indeed give you information about the edge states ; the chern number and $\mathbb{z}_{2}$ invariant give the number and parity of edge states respectively . but these computations once again rely on directly dealing with the nontrivial bulk topology . it seems that you are more interested in explicitly seeing what is happening at the edge . there could ( possibly ) be many ways of doing this ; one very popular one that i am aware of is the jackiw-rebbi solution . i know you want a simple argument without any calculations ; do not worry , the below calculations are only there to make a point in the end . consider a 2d dirac model with a spatially varying mass term : $$h=-iv_{f}\left ( \sigma_{x}\partial_{x}+\sigma_{y}\partial_{y}\right ) +m ( x ) \sigma_{z}$$ where $\lim_{x\rightarrow\pm\infty}m ( x ) =\pm m_{0}$ and the sign of $m ( x ) $ on either side of $x=0$ stays the same ; in that case we must have $m ( 0 ) =0$ . if you consider the analogy of this generic dirac model to topologically nontrivial systems , you would have a topologically nontrivial ( trivial ) system for $x&lt ; 0$ $ ( x&gt ; 0 ) $ . using the same boundary conditions you described $k_{y}$ is still a good quantum number . therefore in the above hamiltonian we can replace $i\partial_{y}\rightarrow k_{y}$ ; writing explicitly in matrix form we get $$h=\left ( \begin{array}{cc} m ( x ) and -iv_{f} ( \partial_{x}-k_{y} ) \\ -iv_{f} ( \partial_{x}+k_{y} ) and -m ( x ) \end{array}\right ) . $$ you can solve for the solutions $\psi ( x ) =\left ( \psi_{1} ( x ) , \psi_{2} ( x ) \right ) ^{t}\equiv\left ( u ( x ) , v ( x ) \right ) ^{t}e^{ik_{y}y}$ with energy $e ( k_{y} ) =v_{f}k_{y}$ as $$\left ( \begin{array}{cc} m ( x ) and -iv_{f} ( \partial_{x}-k_{y} ) \\ -iv_{f} ( \partial_{x}+k_{y} ) and -m ( x ) \end{array}\right ) \left ( \begin{array}{c} u ( x ) \\ v ( x ) \end{array}\right ) =e\left ( \begin{array}{c} u ( x ) \\ v ( x ) \end{array}\right ) . $$ looking at the zero energy solution ( by picking the most convenient $k_{y}=0$ ) we get a set of first-order coupled differential equations $$m ( x ) u ( x ) -iv_{f}\partial_{x}v ( x ) =0$$ and $$-iv_{f}\partial_{x}u ( x ) -m ( x ) v ( x ) =0$$ the equation for $v ( x ) $ after elimination is $$\partial_{x}^{2}v ( x ) =\left ( \frac{m ( x ) }{v_{f}}\right ) ^{2}v ( x ) +\frac{1}{m ( x ) }\partial_{x}v ( x ) \partial_{x}m ( x ) . $$ the general solution would be $$v ( x ) =c_{1}\sinh\left ( -\frac{1}{v_{f}}\int dx\ ; m ( x ) \right ) +c_{2}\cosh\left ( -\frac{1}{v_{f}}\int dx\ ; m ( x ) \right ) . $$ implementing the physically relevant boundary conditions ( $\lim_{x\rightarrow\pm\infty}v ( x ) =0$ ) we have $$v ( x ) \propto\exp\left ( -\frac{1}{v_{f}}\int dx\ ; m ( x ) \right ) . $$ for the simple ( but slightly unphysical ) case of $m ( x ) =m_{0} ( 2\theta ( x ) -1 ) $ it can be verified that we get a simple expression $$v ( x ) \propto\exp\left ( -\frac{m_{0}}{v_{f}}|x|\right ) . $$ showing the state localized at the edge . you can get a more physical expression for $v ( x ) $ ( i.e. . one that is smooth at $x=0$ ) by choosing an $m ( x ) $ which changes less abruptly at $x=0$ . i realize that you are looking for a simple mathematical argument to see the existence of edge states without solving the model . although i performed some trivial calculations above , the conclusion is that when a parameter ( in this case $m ( x ) $ ) in the model crosses its critical value ( critical point in the phase diagram ) at a certain point in real space you are expected to see an edge state in the vicinity of that point . this is in no way a proof ; i provided only one example ! i have one last comment on “ i believe there should be a reason if the edge mode is robust . ” whether the robustness of the edge states can be determined from the model alone depends on the complexity of the model . for example , the robustness of the edge states in topological insulators comes from time-reversal symmetry . when you write down the hamiltonian for ( say ) the hgte/cdte quantum well using the bernevig-hughes-zhang ( bhz ) model as a $4\times4$ matrix ( which hold approximately at the $\gamma$ point ) you are constructing your model such that it respects time-reversal symmetry ; it is not the other way around where the robustness is a consequence of the model . when dealing with the bhz model , the robustness of edge states can be argued by enforcing kramer 's theorem on the dispersion of the edge states . you can read more on this in : what conductance is measured for the quantum spin hall state when the hall conductance vanishes ? . scroll all the way down until you see the question in the block quote “ also : why is there only a single helical edge state per edge ? why must we have at least one and why can not we have , let 's say , two states per edge ? ”
@brandon is correct . you can compute the average kinetic for any free particle using the equipartition theorem , which gives $\langle e \rangle = \frac{1}{2}k_b t$ per quadratic degree of freedom , where $t$ is the temperature and $k_b$ is boltzmann 's constant . for free particles in 3d this gives $\langle e \rangle= \frac{3}{2} k_b t$ ; equating $\frac{1}{2} m_e v^2 = \frac{3}{2} k_b t$ ( in a classical approximation ) shows that the root-mean-square velocity $v$ of free electrons is temperature dependent . $^1$ in the above , the velocities are measured with respect to the environment at temperature $t$ -- we do not even need to consider the fact that the electrons you are talking about are presumably on earth , being accelerated around the sun in centripetal motion . even light will change its speed $c=c_0/n$ in an explosion , because the index of refraction $n$ of the surrounding medium will become inhomogeneous and fluctuate : it is only the speed of light $c_0$ in vacuum that is constant . $^1$note , as also pointed out by brandon , electrons often move at relativistic speeds , so $\frac{1}{2}m_e v^2$ is a poor approximation . $v$ in this formula can exceed the speed of light $c$ , for example . to quantitatively calculate the velocity you need the correction from special relativity . this does not qualitatively change the answer though .
in general physics course we assume maxwell 's equations as the result of many experiments . after that , in field theory we build the lagrangian which satisfies the maxwell equations . we also can build non-linear field theories of em interactions , but there is a requirement to getting of the maxwell equations in the limit of weak fields . so these methods are not connected with the derivation . but we can derive the equations by using some postulates , which generalize experimental facts . the number of postulates should be reduced to a minimum . maxwell 's equations can be earned from the coulomb 's law , special relativity theory and superposition principle ( more details you can see in my answer on this question ) . the other question is why do we need derivation of equations instead of postulating them . they satisfy the experiments , so that is enough for using them in practical cases . the postulating them is not much worse then deriving in this situation .
the question seems a bit odd because " time of maximum spring compression " is an odd concept . the spring compression is a function of time and the time of maximum spring compression is zero because it is an instant not a time interval . maybe the question means the time interval from the time the car first touches the spring to the time of greatest compression . assuming this is the case , and bearing in mind that because this is a homework question we are only allowed to give hints , the trick to doing this question is to realise that the spring behaves as a simple harmonic oscillator i.e. the compression of the spring from the moment the car touches it will be : $$d = a sin ( \alpha t ) $$ where $a$ and $\alpha$ are some constants that you need to calculate . the problem simplifies a lot if you think about the relation between the period of a harmonic oscillator and the amplitude of oscillation .
because you have $d$ coordinates , so you get $d$ choices for choosing coordinates -- i.e. , i choose eastern standard time , and cylindrical coordinates centered on the sun , measured in kilometers and radians . this would make the solar system metric go from having ten independent components to six .
i think this problem is known in the literature as ' soft landing ' . a nice article which may interest you is ( 1 ) and the references therein . ( 1 ) : liu , xing-long , guang-ren duan , and kok-lay teo . " optimal soft landing control for moon lander . " automatica 44.4 ( 2008 ) : 1097-1103 .
it is not often that dmckee and i differ ( mainly because he is usually right :- ) but we differ on this on . or at least we differ if i have correctly understood what you are asking . in a hydrogen atom the 1s , 2s , etc wavefunctions are ( subject to various approximations ) good descriptions of the single electron and have well defined angular momentums . in multielectron atoms it is convenient to think of electrons populating successive 1s , 2s , etc levels , but this is only a conceptual model and not an accurate representation . you are quite correct that while there is a well defined angular momentum for the whole atom , you cannot define the angular momentum of individual electrons . in the old days ( maybe it is still done ) we had calculate atomic structure using a hartree-fock method with individual electron wavefunctions as the basis , and as dmckee points out , atoms have spectral lines that can often be approximately thought of as exciting a specific electron between individual electron wavefunctions . however what you are really doing is labelling the whole atom as an $l , m$ state and not an individual electron .
what you want is not really possible . the reason is that the angular momentum of a particle may have a spin component , or there may be other particles for which you must also include the angular momentum . more specifically , all you may conclude from symmetry arguments is that in a rotationally invariant theory there exists a pseudovector operator $\hat{\mathbf{j}}$ whose commutation relations with the position and momentum components of any one particle in the system are the ones you posted . it will typically include the orbital angular momentum of the particle , $\hat{\mathbf{l}}$ , as well as other operators such as spin which will commute with all position and momentum operators . this , of course , obviates the fact that if you only do have the orbital degrees of freedom of a single particle , then there is nothing there that will have more angular momentum , and the equality you want should follow . the way to go about proving that is the following . you begin with a total angular momentum operator $\hat{\mathbf{j}}$ about which you know only its commutation relations with $\hat{\mathbf{r}}$ and $\hat{\mathbf{p}}$ . you then construct the orbital angular momentum operator $\hat{\mathbf{l}}$ and show that it has the same commutation relations with $\hat{\mathbf{r}}$ and $\hat{\mathbf{p}}$ as $\hat{\mathbf{j }}$ . this means , then that $\hat{\mathbf{j }}-\hat{\mathbf{l}}$ commutes with all components of $\hat{\mathbf{r}}$ and $\hat{\mathbf{ p}}$ . now , if your system really does consist of a single particle , then it carries the direct sum of three irreducible representations of the heisenberg group , the algebra of which is of course spanned by the components of $\hat{\mathbf{r}}$ and $\hat{\mathbf{p}}$ . this means that you can apply schur 's lemma to conclude that each component of $\hat{\mathbf{j }}-\hat{\mathbf{l}}$ must be a multiple of the unit operator . finally , in an isotropic system , such a vector would break the global rotational symmetry , and must therefore be zero .
physics is independent of our choice of units and for something like a length plus a time , there is no way to uniquely specify a result that does not depend on the units you choose for the length or for the time . any measurable quantity belongs to some set $\mathcal{m}$ . often , this measurable quantity comes with some notion of " addition " or " concatenation " . for example , the length of a rod is $l \in \mathcal{l}$ a measurable quantity . you can define an addition operation $+$ on $\mathcal{l}$ by saying that $l_1 + l_2$ is the length of a the rod formed by sticking rods 1 and 2 end-to-end . the fact that we attach a real number to it means that we have an isomorphism $$ u_{\mathcal{m}} \colon \mathcal{m} \to \mathbb{r} , $$ in which $$ u_{\mathcal{m}} ( l_1 + l_2 ) = u_{\mathcal{m}} ( l_1 ) + u_{\mathcal{m}} ( l_2 ) . $$ a choice of units is essentially a choice of this isomorphism . recall that an isomorphism is invertible , so for any real number $x$ you have a possible measurement $u_{\mathcal{m}}^{-1} ( x ) $ . i am being fuzzy about whether $\mathbb{r}$ is the set of real numbers or just the positive numbers ; i.e. whether these are groups , monoids , or something else . i do not think it matters a lot for this post and , more importantly , i have not figured it all out . now , since physics should be independent of our choice of units , it should be independent of the particular isomorphisms $u_q$ , $u_r$ , $u_s$ , etc . that we use for our measurables $q$ , $r$ , $s$ , etc . a change of units is an automorphism of the real numbers ; given two units $u_q$ and $u'_q$ , the change of units is $$ \omega_{u , u'} \equiv u'_q \circ u_q^{-1}$$ or , equivalently , $$ \omega_{u , u'} \colon \mathbb{r} \to \mathbb{r} \ni \omega ( x ) = u'_q ( u_q^{-1} ( x ) ) . $$ therefore , $$ \omega ( x+y ) = u'_q ( u_q^{-1} ( x+y ) ) \\ = u'_q ( u_q^{-1} ( x ) +u_q^{-1} ( y ) ) \\ = u'_q ( u_q^{-1} ( x ) ) + u'_q ( u_q^{-1} ( y ) ) \\ = \omega ( x ) + \omega ( y ) . $$ so , since $\omega$ is an automorphism of the reals , it must be a rescaling $\omega ( x ) = \lambda x$ with some relative scale $\lambda$ . consider a typical physical formula , e.g. , $$ f \colon q \times r \to s \ni f ( q , r ) = s , $$ where $q$ , $r$ , and $s$ are all additive measurable in the sense defined above . give all three of these measurables units . then there is a function $$ f \colon \mathbb{r} \times \mathbb{r} \to \mathbb{r} $$ defined by $$ f ( x , y ) = u_s ( f ( u_q^{-1} ( x ) , u_r^{-1} ( y ) ) . $$ the requirement that physics must be independent of units means that if the units for $q$ and $r$ are scaled by some amounts $\lambda_q$ and $\lambda_r$ , then there must be a rescaling of $s$ , $\lambda_s$ , such that $$ f ( \lambda_q x , \lambda_r y ) = \lambda_s f ( x , y ) . $$ for example , imagine the momentum function taking a mass $m \in m$ and a velocity $v \in v$ to give a momentum $p \in p$ . choosing $\text{kg}$ for mass , $\text{m/s}$ for velocity , and $\text{kg}\ , \text{m/s}$ for momentum , this equation is $$ p ( m , v ) = m*v . $$ now , if the mass unit is changed to $\text{g}$ , it is scaled by $1000$ , and if the velocity is changed to $\text{cm/s}$ , it is scaled by $100$ . unit dependence requires that there be a rescaling of momentum such that $$ p ( 1000m , 100v ) = \lambda p ( m , v ) . $$ this is simple -- $10^5 mv = \lambda mv$ and so $\lambda = 10^5$ . in other words , $$ p [ \text{g} \ , \text{cm/s} ] = 10^5 p [ \text{kg} \ , \text{m/s} ] . $$ now , let 's consider a hypothetical situation where we have a quantity called " length plus time " , defined that when length is measured in meters and time in seconds , and " length plus time " in some hypothetical unit called " meter+second " , the equation for " length plus time " is $$ f ( l , t ) = l + t . $$ this is what you have said - $10 \text{ m} + 5 \text{ s} = 15 \text{ " m+s"}$ . now , is this equation invariant under a change of units ? change the length scale by $\lambda_l$ and the time scale by $\lambda_t$ . is there a number $\lambda$ such that $$ f ( \lambda_l l , \lambda_t t ) = \lambda_l l + \lambda_t t $$ is equal to $$ \lambda f ( l , t ) = \lambda ( l+t ) $$ for all lengths and times $l$ and $t$ ? no ! therefore , this equation $f = l + t$ cannot be a valid representation in real numbers of a physical formula .
the left parentheses are equal to zero due to $u_{\rho}u^{\rho}=-c^2$ . this is true for timelike vectors in the ( -1,1,1,1 ) signature .
summary : the energy change is negligible and if it is not , the energy difference comes from a frequency change of the photon . one must realize that unless the plate was already quickly rotating before the experiment , the energy stored in the rotation of the plate at the end is negligible relatively to the energy of the photon for the same reason why the electron carries most of the kinetic energy in the hydrogen atom even though both electron and proton are orbiting around the shared center-of-mass . in the latter case , the energy of a particle goes like $p^2/2m$ , so for a fixed value of $p^2$ - and indeed , the proton 's and electron 's $p$ only differ by a sign - the lighter particle carries much more kinetic energy . this is totally true for the rotational motion as well . the kinetic energy of rotation is $j^2/2i$ where $j$ is the angular momentum and $i$ is the moment of inertia . for a single photon , the angular momentum relative to the direction of the photon 's motion , $j_1=-\hbar$ , is changed to $j_2=+\hbar$ . the overall change is $j=+2\hbar$ which becomes the angular momentum of the plate . when you square it , you clearly get a negligible number - that is divided by an $o ( 1 ) $ value of the moment of inertia . so the energy obtained in the form of the increased rotation , caused by a single photon 's change of the polarization , is tiny . if you study where this small amount comes from , you will find out that the photon 's frequency is decreased by a tiny fraction so that its reduced energy exactly compensates the increase of the kinetic energy of the plate . but we can only see this change as nonzero if we allow the plate to have an arbitrary angular frequency before the experiment . it is not too difficult to explicitly check that this statement works : if $j$ jumps by $2\hbar$ , the energy $j^2/2i$ jumps by $j\times \delta j/i=2j\hbar/i$ . because $e=\hbar\omega_\gamma$ for photons , the energy conservation law requires that the frequency of the photon decreases by $\delta \omega = -2j/i$ where $j$ is the plate 's angular momentum before it changed the photon 's polarization . but indeed , $-2j/i=-2\omega$ where $\omega$ is the angular frequency of the plate before the experiment . that is exactly the frequency change that the photon experiences . why ? because the periodicity of the photon is conserved if measured relatively to the rotating plate - so its angular frequency changes from $|-\omega_\gamma|$ to $|+\omega_\gamma|$ . but relatively to the inertial system , it is changed from $|-\omega_\gamma-\omega|$ to $|+\omega_\gamma-\omega|$ i.e. it drops by $2\omega$ . it is just like comparing sidereal days and solar days . what would happen with a single photon ? i was working with a single-photon case from the beginning because it is a reasonable approach . a large electromagnetic wave may always be represented as a coherent state of many photons , so one just multiplies the results for a single photon . of course , for a single photon , the " electromagnetic wave " must actually be represented as a wave function determining the probability amplitudes . at any rate , it is true that such wave plates may guarantee that every single photon that enters with a particular polarization leaves with another polarization . if you measure the " exact " polarizations of the final photon , you will get the right one with probability of 100 percent . if you measure a different one , you may get odds between 0 and 100 percent . every photon will be ultimately detected at a single point ; the wave function knows about the odds .
the gregorian calendar that we use today is a compromise between having leap years spaced as evenly as possible and keeping the calculations relatively simple . ( the older julian calendar , which had a leap year every 4 years , was also such a compromise , one with a greater bias toward simplicity . ) in the gregorian calendar , every 4th year is a leap year , except that every 100th year is not a leap year , except that every 400th year is a leap year . the cycle repeats every 400 years , with 97 leap years in each cycle . but they are not distributed as evenly as they could be ; most successive leap years are 4 years apart , but some of them are 8 years apart . the gregorian calendar assumes that a year is 365.2425 days . in fact , it is slightly less than that ; wolframalpha says it is 365.242190419 days . what you are suggesting , i think , is a calendar in which leap years are always either 4 or 5 years apart , and are as evenly distributed as possible , so that the difference between the calendar and the astronomical year are minimized . to do this , start with a 365-day year , and keep track of the offset between the calendar the astronomical year . this offset increases by 0.242190419 days every year . year 0: offset = 0 year 1: offset = 0.242190419 days year 2: offset = 0.484380838 days year 3: offset = 0.726571257 days year 4: offset = 0.968761676 days year 5: offset = 1.210952095 days add a leap day , recompute offset = 0.210952095 days . . . every time the offset exceeds 1 day , add a leap day and subtract 1 day from the offset . this will keep the offset as small as possible over time , but at the expense of easy predictability ; it becomes much more difficult for those who are not mathematically inclined to understand when the next leap year is going to be . and the calendar would presumably have to be adjusted as the number of days in a solar year is computed more precisely , or as it changes as the earth 's rotation rate changes slightly . assuming that the figure of 365.242190419 is correct , your calendar would repeat itself , not every 400 years , but every billion years . an alternative would be to keep the gregorian calendar 's assumption of 365.2425 days per year , and just distribute the leap years more evenly through the 400-year cycle . for example , starting from 2000 , you have leap years in 2000 , 2004 , 2008 , 2012 , . . . , 2128 , 2133 , 2137 , 2141 , . . . , 2265 , 2270 , 2274 , . . . . in either case , most of the time leap years would not be years that are multiples of 4 , which is a nice property of the julian and gregorian calendars . that is the math , but it is absolutely trivial compared to the politics . i am afraid there is very little chance that such a system would be widely accepted . the gregorian calendar , which was a clear improvement over the julian calendar , was introduced in 1582 , but it was not fully adopted worldwide until the late 1920s ( and there are still pockets of resistance ) . there are tremendous advantages in the fact that almost the entire world uses the gregorian calendar , and it will remain within a day or so of the astronomical year for the next several thousand years . your proposed calendar has the advantage of remaining very slightly closer to the astronomical year , but the costs of universal adoption would be tremendous , and the costs of partial adoption would be even worse .
define " best " . as always , there is no one-size-fits-all answer . are you just a casual observer , looking mostly for naked-eye objects ? or are you looking through a telescope for deep-space stuff ? is your scope a go-to that can be interfaced with and controlled from the phone ? here are some examples , look at the features and decide what is best for you : skysafari 3 , either plain , or plus , or pro . my favorite . a lot of folks hauling big dobs and whatnot use it . star walk stellarium - good for casual naked-eye gazing but not much else . starmap 3d , either plain or plus pocket universe
the saturn v payload mass to leo was 118,000 kg . wikipedia has a decent comparison of super-heavy launch systems with a payload mass to leo of 50,000 kg or more . none are in current use , and only two systems are in development . there is also a " heavy " lift launch system list which includes the delta iv and ariane 5 you mentioned . the top operational system is the atlas v hlv with a mass to leo of 29,420 kg and a mass to gto of 13,000 . however , it has never been launched and the united launch alliance claims it needs a 30 month lead-time to produce the heavy launch vehicle variant of the atlas v . next on the list with mass to leo/gto : delta iv heavy : 22,950/12,980 kg , 3/4 successful launches . proton : 21,600/6,360 kg ( comparatively lower gto due to launch location ) , 295/335 successful launches ariane 5: 21,000/10,050 kg , 54/58 successful launches . so the answer is there are no currently operational launch systems which approach the saturn v mass to leo capability .
it is a matter of flux . two factors enter the ability to detect gammas . the flux of the gammas , i.e. . how many per meter square per second , and the crossection of interaction . the crossection is dependent on the energy , and for a given energy is the same for extraterrestrial and terrestrial gamma rays . the flux is not . gamma rays no matter how high was the flux when they were created reach us from large to enormous distances , the flux spreading like 1/r^2 from the distant point ( for us ) source . terrestrial gamma rays are close to the satellites in comparison and the flux much higher than the extra galactic one . the probability of finding terrestrial gammas is measurable , a substantial number survives the trip through the atmosphere . extra terrestrial ones are few and the probability of surviving to the surface very small to zero .
it is only for a spherically symmetric shape that you can treat an extended body as if it were a point mass at the com . the ringworld is stable against axial displacements after which it will gently bob back and forth around the star . unstable against transverse ones because the gravitational attraction of the near-side is large than that of the far-side .
the math is quite abstruse but if you want you can go to this calculator and punch in numbers to find what you need . http://virtualtrebuchet.com/trebuchet.aspx it is the best simple trebuchet calculator i have seen .
i personally think the text is misleading . it is blindly applying gauss ' law while not considering its subtleties . here 's a more cause-and-effect way to look at it . after this , we will get to gauss ' law . let 's take a look at the positively charged plate . yes , the surface charge density on one side doubles . but the surface charge density on the other side goes to zero . a known result about infinite sheets of charge is that distance from the sheet does not affect the electric field . so , in effect , the region to the right of the positive plate is not affected by the redistribution of charge on this plate . the total surface charge density ( if you were to flatten the plate and look at it as truly two dimensional ) did not change . so , the positive plate 's effect on the electric field is not changed when the negatively charged plate is brought near . instead , it is the presence of the second negatively charged plate that doubles the electric field in the region between the plates , not the doubling of the surface charge density on one of the plates . there is only one doubling going on . in fact , if you were to somehow take two infinite sheets each with $\sigma$ , and separated them by some small distance , the electric field outside of the region between the plates would be identical to a single infinite sheet with $2\sigma$ . this is a crucial idea . i hope it is clear . why , then , is the text saying it has to do with the surface charge density ? they are applying gauss ' law to a gaussian pillbox with the left face in the middle of the positive conductor and the right face in the vacuum between the plates . with this choice of gaussian surface , there is only a flux through the right face . this is convenient for calculations . however , the electric field that gives rise to this flux is not due only to the enclosed charge , even though we mathematically calculate it like that . rather , the electric field one should use in gauss ' law is the total electric field due to all charge distributions , including charges outside the gaussian surface . this is one of the subtle but amazing facts of gauss ' law : to calculate the flux through a surface , you only need to mentally worry about the enclosed charge , but the result you get for the electric field ( if you can indeed extract the field from the flux , usually in a highly symmetric geometry ) is due to all of the charge , not just the enclosed charge . so , for your particular problem from h and r , the flux through the right face of the gaussian surface is caused by the superposed electric field from both plates . neither of these fields alone changed , but their effects superpose , causing a doubling of the net electric field . but when applying gauss ' law for this problem , we usually do not worry about what actually causes this electric field . the doubling of the electric field is " accounted for " mathematically by the doubling of the surface charge density on the positive plate . however , viewing this doubling of the charge as the cause of the now-doubled electric field is not correct in my opinion .
if we assume a circular orbit , the equation relevant to your question is given by the equality of gravitational to centripetal force : $$g\frac{mm}{r^2}=\frac{mv^2}{r} , $$ where $m$ is the mass of the satellite , $m$ the mass of the planet , $g$ the gravitational constant , $r$ the distance between the centers of mass of both bodies and $v$ the tangential velocity . you can solve this equation for $v$ and end up at $$v=\sqrt{g\frac{m}{r}} . $$ as you can see , there is one solution to this equation . it is determined by two variables : the mass of the planet and the radius .
it is not true that \begin{align} e_n = h+\hbar\omega_0 . \end{align} why ? well , $h$ is a linear operator while $e_n$ and $\hbar\omega_0$ are real numbers ; an operator cannot equal a real number . you might try to fix this by multiplying each of the real numbers by the identity operator and then claim that \begin{align} e_n i = h+\hbar\omega_0 i \end{align} this is still not correct . you can immediately tell that it can not be correct because the left hand side depends on $n$ , a non-negative integer , by the right hand side does not . so what is going on ? as essentially mentioned in the comments , the following statement is true : if $\psi$ is an eigenvector of the harmonic oscillator , then there exists a non-negative integer $n$ for which \begin{align} h\psi = \left ( n+\frac{1}{2}\right ) \hbar\omega\psi \end{align} where here we are using notation in which the harmonic oscillator hamiltonian is given by \begin{align} h = \frac{1}{2m} p^2 + \frac{1}{2}m\omega^2x^2 \end{align} where $p$ and $x$ are the position and momentum operators respectively . in other words , if you act the hamiltonian on an eigenvector , then it acts simply by multiplying that eigenvector by a real number , the corresponding eigenvalue . but when the hamiltonian acts on a vector that is not an eigenvector , this does not happen .
i assume with energy you mean electricity , and your generators sound like rechargeable batteries to me . in that case you have a series connection , which will simply double the voltage available . no , this would not generate infinite energy . whatever you do , each generator needs fuel that contains chemical energy . in the best ( =impossible ) case it would be entirely converted into electrical energy . whether you temporarily store a part of g1 's energy in g2 or not does not change anything about the fact that the total energy cannot exceed the chemical energy of your fuel ( unless of course there are other influences , e.g. a solar panel and the very energetic sun which will not run out of fuel for some billion years . . . )
they are stating that as part of the question , not stating that it is necessarily true . you often see things like " ignoring air resistance " , a " frictionless plane " or " massless spring " as part of questions to allow a simple analytical answer . in reality for a car moving at 40 m/s air resistance is likely to be the major source of drag and this is proportional to velocity^2 , but this makes the resulting equations trickier
first of all , velocity has a sign . after rebounding the velocity is in the opposite direction so $\delta v = ( 25 - ( -22 ) ) = 47 m/s$ $47 m/s / 0.0035 s = 1,342.9 m/s^2$ [ correction $13,429 m/s^2$ ]
there are no derivatives of the delta function in your integral . performing the integration with one of the delta functions you recover energy-momentum convervation in the vertices and overall energy-momentum conservation in the form of a leftover $$ \delta^4 ( p_1 + p_2 - p_3 - p_4 ) $$ for further simplification you need to consider the absolute matrix element squared with sums over the particle spins ( you average over the incoming and sum over the outgoing ) : $$\frac{1}{4} \sum_{s_1 , s_2 s_3 s_4} \vert \mathcal m \vert^2 = \sum_{s_1 , s_2 s_3 s_4}\vert \text{your integral} \vert^2 $$ and use completeness relations like $$ \sum_s u^s ( p ) \bar u^s ( p ) = p_\mu \gamma^\mu - m $$ $$ \sum_s v^s ( p ) \bar v^s ( p ) = p_\mu \gamma^\mu + m $$ good luck !
any reshaping of the droplet will require flow of water inside the droplet and there will be viscous losses . presumably the energy would come from an increased torque on whatever motor was moving the droplet and substrate .
the determinant is fairly easy to calculate . you know already , essentially , the eigenvalues of the stiffness matrix ; more accurately , you know the eigenvalues of the matrix $\mathbf{m}^{-1}\mathbf{k}$ , because the $\omega_i$ are zeros of the equation $$0=\det ( \mathbf{m}^{-1}\mathbf{k}-\omega^2 ) . $$ ( the more aesthetically minded would replace $\mathbf{m}^{-1}\mathbf{k}$ with $\mathbf{m}^{-1/2}\mathbf{k}\ , \mathbf{m}^{-1/2}$ to get a hermitian matrix , but no matter . ) if you express the second determinant in the corresponding eigenbasis , you get $$ \det ( \mathbf{k}-\mathbf{m}\ , \omega^2 ) =\det ( \mathbf{m} ) \det ( \mathbf{m}^{-1}\mathbf{k}-\omega^2 ) =\frac{m^3}2\det\begin{pmatrix} \omega_1^2-\omega^2 and 0 and 0 \\ 0 and \omega_2^2-\omega^2 and 0 \\ 0 and 0 and \omega_3^2-\omega^2 \end{pmatrix} , $$ which gives your textbook 's expression . more generally , this is an expression of the principle that a matrix 's determinant is the product of its eigenvalues . the adjunct , on the other hand , does not ( to my knowledge ) satisfy any such nice relation ; in any case it is a nasty beast to deal with and i think few people judiciously substituting in the definition $k=m\omega_2^2/2$ of $\omega_2$ instead of $k$ .
infinity is a mathematical term , very useful , but the history of physics has shown us that when we make mathematical extrapolations that lead to infinities of one sort or another , a different mathematical model will eliminate those infinities ( call me quantum mechanics ) . in thermodynamics the black body radiation leads to the ultraviolet catastrophe , and quantum mechanics saves the day . in classical electromagnetism , a point like electron would tend to an infinite potential at ( 0,0,0 ) as it goes with 1/r . quantum electrodynamics saves the day . that is because quantum mechanics has inherent probabilistic indeterminacies when sizes become of order of h ( the planck constant ) . even though elementary particles are postulated as point particles , they are not classical particles , the wave/particle duality saves the day , so the minimum volume would be of dimensions compatible with h in the variables examined and the measurement methods used . once gravity is quantized , the set will be complete , taking care of minimum black hole volumes too , in a similar way .
this is because you are doing a rotation transformation , which is an su ( 2 ) on the state vector ( the vector the matrices act on ) , but when you do the transformation of the pauli operators , it is a regular vector rotation of all three , so it preserves the length . this is a special case for 2 state quantum mechanics , any su ( 2 ) transformation preserves the length of the coefficients of the pauli matrix in the expansion of any 2 by 2 matrix , the pauli matrices make an operator vector .
most of the subducted crust is sea floor , which does not normally contain oil or gas . at this point someone is bound to mention north sea oil/gas , but the north sea floor is actually more like continental rock than basalt sea floor , and it is not being subducted . anyhow , even if gas or oil was subducted it is not going to explode because there is not any oxygen available for it to burn . if it happened i would guess that the oil/gas would get dissolved in molten rock and eventually emerge from volcanos .
from looking at the pictures of the grids provided on that website , i think i understand what they are doing . it is not actually very complicated . it seems easiest to explain this if we consider only one of the hexagonal " tubes " formed by the grid . normally , the light from a flash spreads out over some ( relatively wide ) angle , but now think of what happens to the light when it is confined to this tube : the light that would normally head off at a steep angle is intercepted by the walls of the tube , while light that was already propagating parallel ( or at least close to parallel ) to the tube axis is passed without obstruction . this results in a more confined range of angles over which the light can spread . of course , the confinement is not perfect , as i will explain below , but you can imagine how the angular spread of the beam would be limited to consist only of those rays which can pass cleanly through the tube . this is simple geometry , and you could adjust this angle by varying the length and diameter of the tube . however , the light that is intercepted by the tube walls is not completely eliminated . instead it scatters into a random direction . it is this scattering that gives the resulting beam a nice soft edge . instead of being sharply confined to the angular range allowed by the tube , the scattering allows it to " bleed " a little bit . i should add that the hexagonal shape of the tubes probably has very little effect on the resulting illumination pattern . more likely a hexagonal shape was chosen for some other reason , like easy manufacturing or more mechanical stability . optically , a square grid or even an array of circular tubes , like a bundle of straws , would work just fine .
the easiest ( and roughest ) way to to do it would be to convert your running " work " into a vo2 score . the american college of sports med 's equation is vo2= resting component + horizontal component + vertical component or vo2= 3.5 + ( 0.2 x speed ) + ( 0.9 x speed x elevation gain ) so , using your example of 8.67 mph ( speed in the equation is in meters per min ) 3.5 + ( . 2*232.67 ) + ( 0.9*232.67* . 045 ) = 59.5 thus running on flat ground should give you a speed of 280 m/min or 10.44 mph 5 min 44 sec per mile ( i am an exercise scientist , not a physicist )
there have been experiments with single photons at the time which display interference even after knowing which slit the photon went through . i do not think that the disturbance of the field by a single electron would be detectable , including correlated with which slit it went through . possibly a variant of the experiment in the link , using entangled electrons might show the same effect , interference even if the path is known . this is what one expects from quantum mechanics because the fringe pattern is dependent on the probability , i.e. the square of the wave function and there is no reason to believe that a particle is spread out over all its wave function , but a lot of experimental consistent results that tell us it is just a probability to find the whole particle at a specific point on the screen . in my opinion , the experiments that destroy the interference pattern when attempting to check the slit the particle went through is due to the introduction of a new wavefunction that differed a lot from the undisturbed one . after all we are talking quantum mechanics . any change in the boundary conditions reflects in the mathematical formula of the wavefunction .
yes , it is possible . a static setup like this will work as long as any small motion of the parts would increase the potential energy . in this case , it looks like there is only one possible motion - rotation of the entire ruler-hanger-hammer piece about the axis where the ruler touches the table . if the ruler were to rotate down a little bit , the entire hammer would go down some , decreasing its potential energy . however , it would also rotate , raising the head , where most of the mass is , and thus increasing the potential energy . when these two effects cancel , the system will be in equilibrium . to figure out when the effects will cancel , we could find the system 's center of mass . if the center of mass is directly under the contact point , the system 's in equilibrium . this is because as the system rotates , the center of mass moves in a circle . to be in equilibrium , it must be at the bottom of that circle . the center of the circle is at the contact point , so the bottom of the circle is directly beneath it . there are other , equivalent ways of doing the same problem . for examples check out these questions : why does the weighing balance restore when tilted and released equilibrium and movement of a cylinder with asymmetric mass centre on an inclined plane
short answer : unknown slightly longer answer : the situation you describer would obtain if neutrinos were majorana particles ( and thus not dirac particles ) . it is favored by theorists because it feeds into a nice explanation of why the neutrinos are so light by comparison to the other massive particles . experiments are underway that might settle the question by detecting so called " neutrino-less double beta decay " reactions which are forbidden for dirac neutrinos but allowed for majorana neutrinos . june 2012: one of the neutrino-less double beta decay experiments , exo , has announced modestly significant limit on the possible masses of the neutrinos if they are majorana particles . this limit excludes some of the theoretically favored scenarios . right now the significance is too low for a lot of fanfare , but this might be a the first indication of a surprise . january 2013: kamland-zen has reported a slight improvement on the exo result as of the end of their first run , with purification of the working material planned before resuming data taking . july 2013: exo released another paper last month , setting a much beter measurement of the decay with neutrinos : $$ t_{1/2}^{2\nu\beta\beta} = \left ( 2.172 \pm 0.017\text{ ( stat ) }\pm 0.060\text{ ( sys ) } \right ) \times 10^{21} \text{ years}\ , , $$ but they do not update their limit for neutrino-less double beta decay , so still no definitive answer on majorana/dirac . august 2013: gerda has published a report on a low significance null result in the july 17 issue of phys . rev . lett . ( also as arxive:1307.4720 ) . like the exo and kamnland-zen results this is not a strong result by itself , but the combination of all three is starting to look pretty significant .
this paper details the mathematical model behind what you are doing . by far the most difficult aspect is not the balloon itself but collisions with the environment : http://arxiv.org/pdf/physics/0407003.pdf
yes--- the electric charge is unbroken , from the zero mass of the photon , so b would have to be unbroken . but b commutes with all the generators of the su ( 2 ) , so all the electroweak doublets would have the same electric charge . but this is impossible , as the only things in a family with a given electric charge are unique--- they can not make an su ( 2 ) doublet with anything else .
let me begin with the second question where you do not change the dimensionality , just the volume . the entropy never decreases when you actually compress gas . the compression means that the walls are mostly moving against the colliding molecules which means that they are recoiled backwards at higher velocities . the molecules ' kinetic energy increases so they occupy a larger volume in the momentum space ( in macroscopic language , a gas heats up while being compressed ) which at least compensates the decrease of the volume in the position space . the other answer is incorrect . the second laws says not only that systems exhibit some activity indicating that they do not like a decreasing entropy ; instead , it says that whatever activity physical systems display , they will never achieve a macroscopic decrease of the entropy . it is just impossible . to compress gas by 70% is possible , to decrease the entropy by a macroscopic amount is not . now , the interesting first question . if you could change the effective dimensionality , it would still be true in any consistent theory that the entropy can not decrease . so if your theory were just able to add dimensions like that while keeping a molecule in a sphere of the increasing dimension , the second law of thermodynamics would imply that such an addition of dimensions is not physically possible – it would be another , more sophisticated example of the perpetual motion machine of the second kind . in some sense , it is true that the second law encourages physical systems to lose the dimensions ( a way to increase the entropy , given your formula for the higher-dimensional spherical volumes ) . when the energy dissipates , the energy per degree of freedom effectively goes down which allows us to use a lower-dimensional " effective " description . for example , a gas full of kaluza-klein particles probing ( moving in ) extra dimensions will tend dissipate its energy and decay to many lower-energy quanta which are effectively living just in 3+1 dimensions .
you should not think of the schrödinger equation as a true wave equation . in electricity and magnetism , the wave equation is typically written as $$\frac{1}{c^2} \frac{\partial^2 u}{\partial t^2} = \frac{\partial^2 u}{\partial x^2}$$ with two temporal and two spatial derivatives . in particular , it puts time and space on ' equal footing ' , in other words , the equation is invariant under the lorentz transformations of special relativity . the one-dimensional time-dependent schrödinger equation for a free particle is $$ \mathrm{i} \hbar \frac{\partial \psi}{\partial t} = -\frac{\hbar^2}{2m} \frac{\partial^2 \psi}{\partial x^2}$$ which has one temporal derivative but two spatial derivatives , and so it is not lorentz invariant ( but it is galilean invariant ) . for a conservative potential , we usually add $v ( x ) \psi$ to the right hand side . now , you can solve the schrödinger equation is various situations , with potentials and boundary conditions , just like any other differential equation . you in general will solve for a complex ( analytic ) solution $\psi ( \vec r ) $: quantum mechanics demands complex functions , whereas in the ( classical , e and m ) wave equation complex solutions are simply shorthand for real ones . moreover , due to the probabilistic interpretation of $\psi ( \vec r ) $ , we make the demand that all solutions must be normalized such that $\int |\psi ( \vec r ) |^2 dr = 1$ . we are allowed to do that because it is linear ( think ' linear ' as in linear algebra ) , it just restricts the number of solutions you can have . this requirements , plus linearity , gives you the following properties : you can put any $\psi ( \vec r ) $ into schrödinger 's equation ( as long as it is normalized and ' nice' ) , and the time-dependence in the equation will predict how that state evolves . if $\psi$ is a solution to a linear equation , $a \psi$ is also a solution for some ( complex ) $a$ . however , we say all such states are ' the same ' , and anyway we only accept normalized solutions ( $\int |a\psi ( \vec r ) |^2 dr = 1$ ) . we say that solutions like $-\psi$ , and more generally $e^{i\theta}\psi$ , represent the same physical state . some special solutions $\psi_e$ are eigenstates of the right-hand-side of the time-dependent schrödinger equation , and therefore they can be written as $$-\frac{\hbar^2}{2m} \frac{\partial^2 \psi_e}{\partial x^2} = e \psi_e$$ and it can be shown that these solutions have the particular time dependence $\psi_e ( \vec r , t ) = \psi_e ( \vec r ) e^{-i e t/\hbar}$ . as you may know from linear algebra , the eigenstates decomposition is very useful . physically , these solutions are ' energy eigenstates ' and represent states of constant energy . if $\psi$ and $\phi$ are solutions , so is $a \psi + b \phi$ , as long as $|a|^2 + |b|^2 = 1$ to keep the solution normalized . this is what we call a ' superposition ' . a very important component here is that there are many ways to ' add ' two solutions with equal weights : $\frac{1}{\sqrt 2} ( \psi + e^{i \theta} \phi ) $ are solutions for all angles $\theta$ , hence we can combine states with plus or minus signs . this turns out to be critical in many quantum phenomena , especially interference phenomena such as rabi and ramsey oscillations that you will surely learn about in a quantum computing class . now , the connection to physics . if $\psi ( \vec r , t ) $ is a solution to the schrödinger 's equation at position $\vec r$ and time $t$ , then the probability of finding the particle in a specific region can be found by integrating $|\psi^2|$ around that region . for that reason , we identify $|\psi|^2$ as the probability solution for the particle . we expect the probability of finding a particle somewhere at any particular time $t$ . the schrödinger equation has the ( essential ) property that if $\int |\psi ( \vec r , t ) |^2 dr = 1$ at a given time , then the property holds at all times . in other words , the schrödinger equation conserves probability . this implies that there exists a continuity equation . if you want to know the mean value of an observable $a$ at a given time just integrate $$ &lt ; a&gt ; = \int \psi ( \vec r , t ) ^* \hat a \psi ( \vec r , t ) d\vec r$$ where $\hat a$ is the linear operator associated to the observable . in the position representation , the position operator is $\hat a = x$ , and the momentum operator , $\hat p = - i\hbar \partial / \partial x$ , which is a differential operator . the connection to de broglie is best thought of as historical . it is related to how schrödinger figured out the equation , but do not look for a rigorous connection . as for the hamiltonian , that is a very useful concept from classical mechanics . in this case , the hamiltonian is a measure of the total energy of the system and is defined classically as $h = \frac{p^2}{2m} + v ( \vec r ) $ . in many classical systems it is a conserved quantity . $h$ also lets you calculate classical equations of motion in terms of position and momentum . one big jump to quantum mechanics is that position and momentum are linked , so knowing ' everything ' about the position ( the wavefunction $\psi ( \vec r ) ) $ at one point in time tells you ' everything ' about momentum and evolution . in classical mechanics , that is not enough information , you must know both a particle 's position and momentum to predict its future motion .
classical electrodynamics generally makes a continuity approximation for bulk materials . we are not interested on variation in the fields at the scale of the distances between atoms , so we just average them away . 1 with that approximation , the conduction electrons are acted on by the mean field . they also contribute to the mean field but we treat the two bits separately because there are so many conduction electrons that we can pretend that they are a continuous fluid instead of a set of discrete parts . 2 finally the " zero field in a conductor " argument ( and related ones like " field perpendicular to the surface " ) are contingent on having reached a quiescent state ( sometimes called the " steady state " ) . anytime things change there will be a ( usually brief ) period when this is not true . that is referred to as the " transient behavior " . 1 there are situation in which that trick is not appropriate , but until you have a strong foundation you do not have the tools to look closely at them . 2 this is often a good approximation even at the microscopic scale because at that scale the electrons need to be treated with the ( quantum ) methods of condensed matter physics .
i have never seen a paper where the calculation is performed in a manifestly covariant manner . however , i have posted a set of reference notes on my website ( http://jacobi.luc.edu/notes.html ) that contains the variations needed to carry out the calculation . let me summarize the calculation here . the action for gravity on a compact region $m$ with boundary $\partial m$ is $$i_{eh} + i_{ghy} = \frac{1}{2 \kappa^2} \int_{m}d^{d+1}x \sqrt{-g} r + \frac{1}{\kappa^2} \int_{\partial m} d^{d}x \sqrt{-h} k ~ . $$ the metric on $m$ is $g_{\mu\nu}$ , and $r = g^{\mu\nu} r_{\mu\nu}$ is the ricci scalar . the induced metric on the boundary $\partial m$ is $h_{\mu\nu} = g_{\mu\nu} - n_{\mu} n_{\nu}$ , where $n^{\mu}$ is the ( spacelike ) unit vector normal to $\partial m \subset m$ . now consider a small variation in the metric : $g_{\mu\nu} \to g_{\mu\nu} + \delta g_{\mu\nu}$ . the quantities appearing in the einstein-hilbert part of the action change in the following manner : $$ \delta \sqrt{-g} = \frac{1}{2} \sqrt{-g} g^{\mu\nu} \delta g_{\mu\nu}$$ $$ \delta r = -r^{\mu\nu} \delta g_{\mu\nu} + \nabla^{\mu}\left ( \nabla^{\nu} \delta g_{\mu\nu} - g^{\nu\lambda} \nabla_{\mu} \delta g_{\nu\lambda} \right ) $$ thus , the change in $i_{eh}$ is $$\begin{aligned}\delta i_{eh} = and \frac{1}{2\kappa^{2}}\int_{m} d^{d+1}x \sqrt{-g} \left ( \frac{1}{2} g^{\mu\nu} r - r^{\mu\nu} \right ) \delta g_{\mu\nu}\\ and + \frac{1}{\kappa^2} \int_{\partial m} d^{d}x \sqrt{-h} \frac{1}{2} n^{\mu} \left ( \nabla^{\nu} \delta g_{\mu\nu} - g^{\nu\lambda} \nabla_{\mu} \delta g_{\nu\lambda}\right ) ~ , \end{aligned}$$ with the boundary term coming from the volume integral of the total derivative in $\delta r$ . the variations of the quantities in the ghy term are a bit more complicated to work out , but they all basically follow from standard definitions and this result for the variation of the normal vector : $$\delta n_{\mu} = \frac{1}{2} n_{\mu} n^{\nu} n^{\lambda} \delta g_{\nu\lambda} = \frac{1}{2} \delta g_{\mu\nu} n^{\nu} + c_{\mu}~ . $$ in the second equality i have introduced a vector $c_{\mu}$ that is orthogonal to $n^{\mu}$ ; it is given by $$c_{\mu} = - \frac{1}{2} h_{\mu}{}^{\lambda} \delta g_{\nu\lambda} n^{\nu} ~ . $$ the reason i have introduced this vector is that the variation in the trace of the extrinsic curvature can be written as $$\delta k= - \frac{1}{2} k^{\mu\nu} \delta g_{\mu\nu} - \frac{1}{2} n^{\mu}\left ( \nabla^{\nu} \delta g_{\mu\nu} - g^{\nu\lambda} \nabla_{\mu} \delta g_{\nu\lambda} \right ) + d_{\mu} c^{\mu}$$ where $d_{\mu}$ is the covariant derivative along $\partial m$ that is compatible with the induced metric $h_{\mu\nu}$ . so , the change in the ghy part of the action is $$\delta i_{ghy} = \frac{1}{\kappa^2} \int_{\partial m} d^{d}x \sqrt{-h}\left ( \frac{1}{2}h^{\mu\nu} \delta g_{\mu\nu} k + \delta k \right ) ~ . $$ combining this with $\delta i_{eh}$ we see that the several terms cancel , leaving $$\begin{aligned} \delta i = and \frac{1}{2\kappa^2}\int_{m} d^{d+1}x \sqrt{-g}\left ( \frac{1}{2} g^{\mu\nu} r - r^{\mu\nu} \right ) \delta g_{\mu\nu}\\ and + \frac{1}{\kappa^2}\int_{\partial m} d^{d}x \sqrt{-h}\left ( \frac{1}{2} ( h^{\mu\nu} k - k^{\mu\nu} ) \delta g_{\mu\nu} + d_{\mu} c^{\mu} \right ) ~ . \end{aligned}$$ we discard the term $d_{\mu} c^{\mu}$ , which is a total boundary derivative .
assume that radius and charge density for outer ring and inner ring is $r$ , $\lambda$ and $r$ , $\lambda'$ respectively . because $r&lt ; &lt ; r$ you can assume the magnetic field on the whole surface of inner ring to be : $$b=\frac{\mu_0i}{2r}$$ now , $i$ is equal to $\lambda v$ and $v=r\alpha t$ , so $$b=\frac{\mu_0\lambda r\alpha t}{2r}$$ and $ ( r&lt ; &lt ; r ) $ $$\phi=\pi r^2b$$ now we find induced electric field around the first ring . according to faraday 's law : $$e ( 2\pi r ) =\frac{d\phi}{dt}$$ so $$e=\frac{\mu_0\lambda r\alpha}{4}$$ now we find total torque exerted on inner ring : $$\tau=r\lambda ' e ( 2\pi r ) =\frac{\mu_0\lambda \lambda'\pi r^3\alpha}{2}$$ so you can find the angular acceleration by having it is moment of inertia ( for a ring $i_0=mr^2$ ) : $$\alpha'=\frac{\tau}{i_0}=\frac{\mu_0\lambda \lambda'\pi r^3\alpha}{2mr^2}=\frac{\mu_0\lambda \lambda'\pi r\alpha}{2m}$$
well , the answer is yes and no . the band inversion between the $s$-like ( conduction ) band $\gamma_6$ and $p$-like ( valence ) band $\gamma_8$ in hgte is primarily responsible for its topologically nontrivial band structure . the bulk band structure of hgte with ( right ) and without ( left ) spin-orbit coupling is shown in the figure below . there are a total of eight bands ( including spin ) shown in both figures . since we’re interested in the physics close to the $\gamma$ point , we can approximately ignore bulk inversion asymmetry . under this assumption the spin up and down bands are degenerate as clearly seen from the figure . from this point on i will not consider spin explicitly when talking about bulk band structure ; i.e. there are a total of four bands ( ignoring spin ) in the figures below . note : please don’t focus on the quantitative details of the left figure . it is a hypothetical scenario introduced purely for pedagogical purposes . you can notice that , in the figure on the left ( without spin-orbit coupling ) , the heavy hole ( hh ) and light hole ( lh ) bands are degenerate . when you turn on spin-orbit , the $\gamma_6$ and $\gamma_8$ bands reverse their order , the $\gamma_8$ band splits its degeneracy , and the lh band gets inverted . the fermi energy sits at the intersection point of the lh and hh bands . but notice that , despite lh and hh acting as the conduction and valence bands ( right figure ) respectively , there is no gap between them ! you cannot get a topological insulator without a bulk gap . if you could somehow induce a gap between the lh and hh bands ( say ) by straining hgte then it could , in fact , be turned into a 3d topological insulator ! now , there were several ( experimental ) advantages in creating a cdte/hgte/cdte quantum well . first of all , since it’s a quantum well you would have sub-bands ( not bands unlike bulk materials ) due to quantum confinement in the out-of-plane ( say $z$ ) direction . as a result , a single band in the bulk will split up into several sub-bands , each corresponding to a different quantized $k_z$ , as you shrink the thickness of the material in the $z$-direction . now , you can notice ( in the figure below ) , unlike the bulk , the electron ( conduction ) and hole ( valence ) sub-bands do have an energy gap . this plot obviously shows the minima ( electron ) or maxima ( hole ) of these sub-bands ; they still disperse in k-space . and as you may know the inversion of the sub-bands will occur when you cross the critical thickness ( as shown in the figure below ) . another very important advantage of using a quantum well structure in doing your experiments is that , unlike a bulk sample , you can electrically tune your fermi energy using a gate . you could both tune your fermi energy to intersect the electron ( or hole ) sub-band or keep it in the gap , and observe the change in conductance . when you are in the quantum spin hall regime you will never stop conducting as your fermi energy goes from the electron ( or hole ) sub-band to the gap ; this is due to the topologically protected ( due to time-reversal symmetry ) edge states inside the bulk gap ( here bulk means not on the edge of the well ) . in a bulk sample ( bulk meaning not quantum confined ) you would probably have performed some sort of controlled doping ( assuming the gap has already been induced somehow ) to control your fermi energy . in that case you would probably have to fabricate different samples for different values of fermi energy ; that’s certainly very inconvenient . in summary , you need to somehow induce a gap in hgte , by either quantum confinement or induced strain to turn it into a 2d or 3d topological insulator . cdte is not responsible for the key physics , i.e. band inversion , which gives rise to a topologically nontrivial band structure in hgte . it is interesting to note that the hgte quantum well was not the first proposal by bernevig , hughes , and zhang . the experimental difficulty of working with strained hgte led them to revise their proposal and predict a topological insulator in the quantum well instead ! this was back in 2006 ; people have now managed to experimentally create 3d topological insulators out of strained hgte .
your physical intuition is correct , it is indeed sum over all admissible paths . there is a problem with viewing $dx ( t ) $ as a measure on the function space , because it is not well defined ( e . g . infinite at some ' points ' $x ( t ) $ ) . this is one of the big ( and as far as i know open problems ) in the mathematical formulation of path integrals . often one absorbs the kinetic part of the hamiltonian in the measure to get an exponential dumping factor , e.g. $d x ( t ) exp ( -\frac{i}{\hbar} \int t [ x ( t ) ] $ . salmhofer considers in renormalization : an introduction many of the mathematical questions . ( if you know other good sources , i would be happy to hear about them ! ) path integrals are a nice way to ' visualize ' many calculations ( e . g ' i sum xyz over all possible paths ) , but are hard to compute . indeed , the only calculations i know are based on breaking the path in linear segments ( and even this gets clumsy ) . often one performs some kind of taylor expansion and only considers the first orders . there are then rules how to calculate often recurring terms ( see feymann diagrams ) .
what you say is correct in principle , but ignores the important fact that practical car engines are horribly inefficient , and their effeciency changes quite a bit over the range of speed and power required to move the car . note that this is the point of transmissions . at best they do not loose any power , but they make the overall process more efficient by allowing the gasoline engine to operate at a more efficient point . in one way , you can look at a hybrid as having a wide-ranging finely adjustable transmission , but there is more to it than that . the efficiency of a gasoline engine is in part related to what fraction of peak power it must put out . if the gas engine is the only mechanical output in the car , then it must be sized to supply peak power . however , most of the time much less than peak power is needed , so the engine often runs at a inefficient point . with a electric motor available to fill in the when peak power is demanded , the gas engine can be sized smaller and it is easier to make it more efficient over most of the normal operating range . it also allows for the option of not using the gas engine at all at very low power levels where it would be very inefficient . instead it can effectively be run in bursts of more efficient operation . for example , if the gas engine is 3% efficient at 500 w , but 6% efficient at 1 kw , then you are better off running it at 1 kw half the time instead of at 500 w all the time . with a hybrid , you have this option . with just a gas engine , it is stuck having to produce whatever power is demanded at the moment , regardless of how efficient that is . i have a honda civic hybrid , and i can tell you this stuff really works . i routinely get 50 miles/gallon minimum on the highway , often substantially more . the engine is physically small for the size car , and it has been specially designed to be easily shut down and restarted . going down a hill , even at highway speeds , the engine often turns off . if the hill is steep enough , the motor is run as a generator and charges the battery . when i get to the bottom of the hill , i can see that for a little while the control system uses the electric motor to keep the car going at the set speed ( this is all with cruise control engaged ) , then eventually gives up and switches on the gas engine . i can feel a slight klunk when that happens , and the charge indicator goes abruptly from discharge to charge .
you might want to have a look at the work of gavin crook ( http://threeplusone.com/gec/ ) , especially the first two chapters of his phd thesis ( to be found on his website ) are quite revealing . i will quickly summarize his main result : assume a system is what he calls microscopically reversible , that is , the probability of a trajectory through phase space is related to the probability of the system taking the reverse trajectory by a simple function of the heat ( eq . 1.10 in his thesis ) . it is initially in equilibrium . then you drive it out of equilibrium by some ( time-reversible ) protocol . now for an arbitrary function $f$ depending on the path of the system through phase space , it holds that \begin{equation} \langle f \rangle_{\mathrm{f}} = \langle \hat f \exp ( -\beta w_\mathrm{d} ) \rangle_{\mathrm r} \end{equation} where $\langle \ldots \rangle$ denotes an average over all possible paths the system can take through phase space and f / r denotes the forward / reverse non-equilibrium process . $w_{\mathrm{d}}=w_{\mathrm{tot}} - w_{\mathrm{r}}$ is what he calls dissipative work ; it is just the total work minus the minimum amount of work required ( reversible work , that is , the free energy difference ) . $\hat f$ is the time reversal of $f$ . and now it comes : this holds regardless of the strength of the perturbation ! by choosing $f=1$ ( or any other constant ) , one obtains the jarzynski equality \begin{equation} \langle \exp ( -\beta w ) \rangle = \langle \exp ( -\beta \delta f ) \rangle \end{equation} ( $\langle \ldots \rangle$ again denotes an average over all possible realizations of the non-equilibrium process ) which relates the work performed during a non-equilibrium process to the free energy difference by an equality ( ! ) instead of the inequality resulting from the second law . with more sophisticated $f$ 's one also obtains other relations like the transient fluctuation theorem , the kawasaki response ( which gives you a probability distribution for a non-equilibrium ensemble ) . there is a lot more literature ; i also recommend the 1997 papers of christopher jarzynski ( sadly no free access ) . at the moment , i am learning about all these things myself , so the above might not be 100% waterproof explained , but i hope , one gets the idea .
that prof müller said it : shock waves in addition of the usual things . the picture in your link is rather different from the thing shown in the video , for the time being , i do not understand really what is the new thing . from thermodynamics it is clear , that they ( hope ? ) to have a higher effective deltat , obviously without having higher temperatures at machiney parts ( which makes them expensive and/or short-lived ) i assume that those shock waves can be transformed into working pressure without the high temperatures of the combustion shock wave touchng machine parts . i hope we will hear more from prof müller in near future . edit : this link is somewhat more detailed and less press-release-silly . in general it says what i surmised ( by application of thermodynamics basics ) http://www.zdnet.com/blog/emergingtech/wave-disk-engines-to-make-hybrid-vehicles-cheaper-more-efficient/1887
it would crash into the earth because the earth 's gravitational field is not uniform and , even if said ring were to be perfectly positioned , ignoring the effects of wind , strikes from cosmic debris ( not a lot that low in the atmosphere ) , change in mass of the ring ( e . g . corrosion ) , change in shape of the ring ( due to e.g. gravitational forces , heat deformation from sunlight ) , etc . , the earth 's gravitational field distribution changes over time and the ring would eventually fall out of its perfect position and crash into the surface .
starting at ${position}_z$ = $z$ = 0 and $v ( z ) = 0$ and by tracking multiple acceleration values either with a time interval or at fixed intervals , $t$ , then you can get the position . . . . somewhat . it will drift over time . also , your device cannot rotate whatsoever , or else you need a gyroscope to track that and then use trigonometry to properly orient the x y and z values from the accelerometer . assuming it is always oriented such that the $a ( z ) $ is always perfect vertical acceleration ( if you are in a vehicle that is always flat , in which case z does not matter , or you are on a vertical guide rail ) , $$p ( z ) = \int_0^t v ( z ) ~dt = \iint_0^t a ( z ) ~dt $$ also , from here : short answer : forget about it . longer answer : unless you are on a perfectly straight rail , you will not achieve what you want to do without ( a ) a set of gyros ; and ( b ) far more accurate sensors than what you have . accelerometers measure acceleration in the body fixed reference frame , whereas you need some displacement in an earth-fixed frame . therefore , you need not only to integrate the accelerometers , but rotate them into the earth-fixed frame before doing the integration . this is assuming perfect sensors . mems sensors are far from perfect - i have written up a post on some of the errors here . consider two errors : 1 . a bias on the accelerometer . 2 . an initial attitude ( tilt ) error . in addition to whatever acceleration signal there is , integrate a bias and you get a ramp error with time . integrate the ramp and you get a quadratically increasing error with time . this will add up really , really quickly . consider a tilt error . you will now be measuring some of the gravity vector in the forward ( or whatever ) direction . integrate this error twice and you will have the same problem as the bias . so , my advice again is do not ! find another method . also , check this book out for more detailed designs , or use whatever sensors and algorithm these guys are on : http://www.youtube.com/watch?v=6ijarke8vku if you still want to give this a shot , use the trapezoidal method in excel , it is pretty easy . there is an explanation page here with a sample , but here 's a more complete way :
the name of the property is itself a clue here : enthalpy of vaporization . by nature , enthalpy does take into account the work required to push the atmosphere . you can see the impact of increasing the pressure on the enthalpy of vaporization on a mollier diagram . increasing the pressure has the overall the effect of reducing the enthalpy of vaporization , until it becomes zero at the critical point . at this stage , there is no longer a phase change associated with vaporization .
your problem is ill-posed , becaus the state of a beam is not described by a state vector $\psi$ , but by the corresponding density matrix $\psi\psi^*$ , which determines $\psi$ only up to a phase . thus it is meaningless to talk about ''the'' superposition of two beams . if the two beams are known to have a fixed , but unknown , relative phase , the question becomes well-defined . the only way to gain this knowledge is to check that the beams have been coherently generated from a common source . in this case , a half-silvered mirror will produce the required superposition , and one can change the relative phase by a rotator ( as described in the book by mandel and wolf ) . for preparing arbitrary states of a collection of beams from a single coherent beam see my paper http://de.arxiv.org/abs/quant-ph/0306123 . see also m . reck , a . zeilinger , h.j. bernstein and p . bertani , experimental realization of any discrete unitary operator , phys . rev . lett . 73 ( 1994 ) , 58–61 . these papers specify how to create appropriate transformations ; to prepare a particular state , find a transformation that creates it from a state you know how to prepare , and then implement this transformation .
basically you have a train with an observer a inside who emits a beam of light to the left which is reflected off a wall . . . let 's call that wall w , for reference below . . . at distance $d$ from a . . . . let 's call that distance $d := aw$ ( anticipating that distances between certain other participants will have to be considered , and distinctly named , below ) . the time it takes for the beam to get back to the observer is $t_0 = \frac{2 \ , d}{c}$ which is the proper time . . . . a.k.a. " ping duration " $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c}$ . so far , so good . now consider an observer b outside the train . the train is moving at a velocity $v$ to the right relative to b . ( also : b as well as everyone at rest wrt . b are moving at velocity $v$ relative to a and w ; from a towards w . ) thus the time it takes for the light to hit the wall is $\frac{d}{c + v}$ that is eventually incorrect . let 's try to be more precise : b and a were passing each other ( which is supposed to be visible by everyone else ; a.k.a. " emitting a light signal " ) , there exists some participant ( let 's call it p ) who was and remained at rest wrt . b , who therefore of course was passed by w " sometime " , and specificly : whose indication of being passed by w was simultaneous to b 's indication of being passed by a , and there exists some participant ( let 's call it q ) who was and remained at rest wrt . b ( as well as p ) and who was passed w just as q and w observed ( together , at their meeting ) that b and a had passed each other . therefore $\frac{pq}{bq} = \frac{v}{c}$ , and $\frac{bp}{bq} = \frac{pq}{bq} + 1 = 1 + \frac{v}{c} = \frac{c + v}{c}$ . of interest is then b 's duration from the indication of being passed by a until the indication simultaneous to q 's indication of being passed by w ( and observing that b and a had passed each other ) . this is of course half of the ping duration $\mathop{\delta}\limits_{\text{ping}} \tau_b [ q ] $ , i.e. half of $\frac{2 \ , bq}{c}$ ; thus $\frac{bq}{c}$ which is in turn equal to $\frac{bp}{c + v}$ . and the time it take for it to return to a is $\frac{d}{c - v}$ . arguing similarly to the above , this corrsponding duration of b is more precisely equal to $\frac{bp}{c - v}$ . now , in order to compare a 's ping duration $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c}$ with the sum of the corresponding durations of b , i.e. with $\frac{bp}{c + v} + \frac{bp}{c - v} = \frac{2 \ , c \ , bp}{c^2 - v^2} = \frac{2 \ , bp}{c} \frac{1}{1 - \beta^2}$ we still need to establish value of the distance ratio $\frac{aw}{bp}$ . that means we are now left with having to derive " length contraction " ! but that is not difficult , given all of the explicit setup and named participants that were introduced above already : we should consider one more participant , j , who was and remained at rest wrt . a and w , and whose indication of being passed by b was simultaneous to w 's indication of being passed by q ( and observing that b and a had passed each other ) . therefore $\frac{aj}{aw} = \frac{v}{c}$ , and $\frac{jw}{aw} = 1 - \frac{aj}{aw} = 1 - \frac{v}{c} = \frac{c - v}{c}$ . considering the two explicit requirements of simultaneity above , the corresponding ratios of distances should be equal : $\frac{bp}{aw} = \frac{jw}{bq}$ . inserting expressions from above : $\frac{bp}{aw} = \frac{jw}{aw} \frac{aw}{bp} \frac{bp}{bq} = \frac{c - v}{c} \frac{aw}{bp} \frac{c + v}{c} = \frac{aw}{bp} \frac{c^2 - v^2}{c^2} = \frac{aw}{bp} ( 1 - \beta^2 ) = \sqrt{ 1 - \beta^2 }$ . consequently : $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c} = \frac{2 \ , bp}{c} / \sqrt{ 1 - \beta^2 }$ . calling b 's corresponding duration $\frac{2 \ , bp}{c} \frac{1}{1 - \beta^2} = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_w}$ therefore $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_w} \sqrt{ 1 - \beta^2 }$ , as may have been expected . last week in class we derived the formula for time dilation using light clocks well , should not that have been pretty much the derivation i just sketched ? . edit for completeness , and to emphasize a particular point in the following , here 's also the derivitation involving light clocks " perpendicular to the direction of motion " ( which seems to have been mentioned in passing in the op 's question ) : expanding on the setup described above , with the principal protagonists a and b and suitable auxiliary participants ( w and j at rest wrt . a ; p and q at rest wrt . b ) , and all of them " sitting or moving in one line " , we now also consider participant f ( at rest wrt . a , j , w ) with distance ratios $\left ( \frac{af}{fj} \right ) ^2 + \left ( \frac{aj}{fj} \right ) ^2 = 1$ , and ( without loss of generality , but just to re-use setup relations from above ) with $\frac{aw}{fj} = 1$ , therefore $\frac{af}{fj} = \sqrt{ 1 - \left ( \frac{aj}{fj} \right ) ^2 } = \sqrt{ 1 - \left ( \frac{aj}{aw} \right ) ^2 } = \sqrt{ 1 - \beta^2 }$ ; and participant g ( at rest wrt . b , p , q ) with distance ratios $\left ( \frac{bg}{gp} \right ) ^2 + \left ( \frac{bp}{gp} \right ) ^2 = 1$ , and such that g and f met each other in passing . importantly , the entire region containing the setup is of course supposed to be flat . therefore it can be demonstrated ( what otherwise may be glanced over for seeming " too obvious to even point out" ) , that f 's indication of having been passed by g was simultaneous to a 's indication of having been passed by b ; and vice versa that g 's indication of having been passed by f was simultaneous to b 's indication of having been passed by a . then , by the same argument that was used above for comparison of distance ratios between pairs of participants who were not at rest to each other , we set : $\frac{af}{bg} = \frac{bg}{af}$ , and therefore $\frac{af}{bg} = 1 . $ with $\mathop{\delta , \tau_a}\limits_{\text{ping trip } b_g} = \frac{2 \ , fj}{c} = \frac{2 \ , af}{c} / \sqrt{ 1 - \beta^2 }$ and $\mathop{\delta}\limits_{\text{ping}} \tau_b [ g ] = \frac{2 \ , bg}{c}$ follows $\mathop{\delta}\limits_{\text{ping}} \tau_b [ g ] = \mathop{\delta \ , \tau_a}\limits_{\text{ping trip } b_g} \sqrt{ 1 - \beta^2 }$ . finally , as can be shown explicitly , it holds symmetrically that $\mathop{\delta}\limits_{\text{ping}} \tau_a [ f ] = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_f} \sqrt{ 1 - \beta^2 }$ .
there are 3 actions of the galilean group on the free particle : on the configuration space , on the phase space and on the quantum state space ( wave functions ) . the galilean lie algebra is faithfully realized on the configuration space by means of vector fields , but its lifted action on poisson algebra of functions on the phase apace and on the wave functions ( by means of differential operators ) is the central extension of the galilean algebra , known as the bargmann algebra in which the commutator of boosts and momenta is proportional to the mass . the reasoning is given in the following arguments 1 ) the action on the configuration space : $q = \{x^1 , x^2 , x^3 , t\}$: here the translations and the boost operators act as vector fields and their commutator is zero : translation : $x^i \rightarrow x^i+c^i$ , generating vector $p_i = \frac{\partial}{\partial x^i}$ boost : $x^i \rightarrow x^i+v^i t$ , generating vector $g_i = t \frac{\partial}{\partial x^i}$ this is a faithful action of the galilean group : $ [ p_i , g_j ] = 0$ . 2 ) the lifted galilean action to the phase space $q = \{x^1 , x^2 , x^3 , p_1 , p_2 , p_3\}$ the meaning of lifting the action is to actually write the lagrangian and finding the noether charges of the above symmetry : the charges as functions on the phase space will generate the centrally extended version of the group . an application of the noether theorem , we obtain the following expressions of the noether charges : translation : $p_i = p_i$ boost : $ g_i = p_i t - m x^i$ . the canonical poisson brackets at $t=0$ ( because the phase space is the space of initial data ) : $\{p_i , g_j\} = m \delta_{ij}$ the reason that the lifted action is a central extension lies in the fact that that the poisson algebra of a manifold itself is a central extension of the space of hamiltonian vector fields , $$ 0\rightarrow \mathbb{r}\overset{i}{\rightarrow} c^{\infty} ( m ) \overset{x}{\rightarrow} \mathrm{ham} ( m ) \rightarrow 0$$ where the map $x$ generates a hamiltonian vector field from a given hamiltonian : $$x_h = \omega^{ij}\partial_{j}h$$ ( $\omega$ is the symplectic form . the exact sequence simply indicates that the hamiltonian vector fields of constant functions are all zero ) . thus if the lie algebra admits a nontrivial central extension , this extension may materialize in the poisson brackets ( the result of a poisson bracket may be a constant function ) . 3 ) the reason that the action is also extended is that in quantum mechanics the wave functions are sections of a line bundle over the configuration manifold . a line bundle itself is a $\mathbb{c}$ bundle over the manifold : $$ 0\rightarrow \mathbb{c}\overset{i}{\rightarrow} \mathcal{l}\overset{\pi}{\rightarrow} m\rightarrow 0$$ thus one would expect an extension in the lifted group action . line bundles can acquire a nontrivial phases upon a given transformation . in the case of the boosts , the schrödinger equation is not invariant under boosts unless the wave function transformation is of the form : $$ \psi ( x ) \rightarrow \psi' ( x ) = e^{\frac{im}{\hbar} ( vx+\frac{1}{2}v^2t ) }\psi ( x+vt ) $$ the infinitesimal boost generators : $$\hat{g}_i = im x_i + \hbar t \frac{\partial}{\partial x_i}$$ thus at $t=0$ , we get : $ [ \hat{g}_i , \hat{p}_j ] = -im \hbar\delta_{ij}$ thus in summary , the galilean group action on the free particle 's configuration space is not extended , while the action on the phase space poisson algebra and quantum line bundle is nontrivially central extended . the classification of group actions on line bundles and central extensions may be performed by means of lie group and lie algebra cohomology . a good reference on this subject is the book by azcárraga , and izquierdo . this book contains a detailed treatment of the galilean algebra cohomology . also , there are two readable articles by van holten : ( first , second ) . group actions on line bundles ( i.e. . quantum mechanics ) is classified by the first lie group cohomology group , while central extensions are classified by the second lie algebra cohomology group . the problem of finding central extensions to lie algebras can be reduced to a manageable algebraic construction . one can form a brst operator : $$ q = c^i t_i + f_{ij}^k c^i c^j b_k$$ where $b$ abd $c$ are anticommuting conjugate variables : $\{b_i , c_j \} = \delta_{ij}$ . $t_i$ are the lie algebra generators . it is not hard to verify that $q^2 = 0$ if we can find a constant solution to the equation $q \phi = 0$ with $\phi = \phi_{i j} c^i c^j$ which takes the following form in components , we have $$ f_{ [ ij|}^k \phi_{k|l ] } = 0$$ ( the brackets in the indices mean that the indices $i , j , l$ are anti-symmetrized . then the following central extension closes : $$ [ \hat{t}_i , \hat{t}_j ] = i f_{ij}^{k} \hat{t}_k + \phi_{ij}\mathbf{1}$$ the second lie algebra cohomology group of the poincaré group is vanishing , thus it does not have a nontrivial central extension . a hint for that can be found from the fact that the relativistic free particle action is invariant under poincaré transformations . ( however , this is not a full proof because it is for a specific realization ) . a general theorem in lie algebra cohomology asserts that semisimple lie algebras have a vanishing second cohomology group . semidirect products of vector spaces and semisimple lie algebras have also vanishing second cohomology provided that there are no invariant two forms on the vector space . this is the case of the poincaré group . of course , one can prove the special case of the poincaré group by the brst method described above .
your use of the no-crossing idea is correct - we do not expect level crossings in two dimension to appear unless protected by symmetry . the symmetries in this case are the symmetries of the honeycomb lattice and time reversal . the protection of level crossings by symmetry is ubiquitous in solid-state . i should add that the existence of these dirac point is actually slightly more robust than would be implied by simple symmetry arguments . the band structure will still have dirac cones if one applies any perturbation that does not violate parity , time reversal and is not extremely strong [ 1 ] . this is because of the interplay of the berry 's curvature and the dirac point , which i could find a reference for if you would like . [ 1 ] extremely strong means that if i imagined increasing the strength of this perturbation up from zero it would drag the dirac cones from the $k$ , $k'$ points into each other . this would mean a perturbation of energy about the bandwidth , which is several electron-volts .
since this is a homework question , here 's a bit to get you started : i will assume you mean that the qubit is in the pure state $$ |\psi\rangle = \sqrt{p}|+\rangle + \sqrt{1-p}|-\rangle $$ the bloch sphere representation of pure states can be written as follows : $$ |\psi\rangle = e^{i\phi}\sin ( \theta/2 ) |+\rangle + \cos ( \theta/2 ) |-\rangle $$ post more of your work if you are still stuck and need guidance .
$pdv$ is boundary work . $vdp$ is isentropic shaft work in pumps ( as you have identified above ) , gas turbines , etc . now you must realize that even in a pump or turbine the mechanism of work is still $pdv$ , i.e. , the gas pushing on the blade out of its way . but , then there is work required to maintain the flow in and out of the device/control volume , which requires flow work $pv$ so the net reversible work from a steady-flow device turns out to be shaft $vdp$ . why flow work $pv$ ? to push a packet of fluid with volume $v$ forward into a device you have to do work against the pressure of the fluid already in the device , i.e. , overcome the back force of that fluid . this implies the work you do in pushing your new packet of length $l$ and cross-section area $a$ into the device is : \begin{align*} \int fdx = \int_{0}^{l}padx = pv \end{align*} it must be noted that in a steady-flow device ( unlike in a piston ) the back pressure $p$ is constant . now consider the device ( e . g . , turbine to be a control volume ) . the energy of the fluid going in is its internal energy and the work invested into the fluid to enter the device : $u_{entry}+p_{entry}v_{entry}=h_{entry}$ . similarly for exit from the device . the net change across the device is $\delta h$ . for a differential device ( or across a small change ) this is $dh$ . the work output from the shaft of then device is the $\delta w= dh$ . now if the device is isentropic , i.e. , adiabatic-reversible . the gibbs equation provides : \begin{align*} and dh=tds+vdp=vdp\\ and \delta w =dh=vdp \qquad ( \text{isentropic}\ ; ds=0 ) \end{align*} therefore $vdp$ is isentropic shaft work from a flowing device . important points : 1 ) both internal energy and enthalpy are state variables , therefore can be measured for a system static or flowing . this is why sometimes there is a tendency to use $u$ and $h$ incorrectly . the true purpose of $h$ is to capture the work required to push/maintain a flow against a back pressure , i.e. , it incorporates the $pv$ part . therefore when you write an energy balance with flows coming in and out , the energy crossing boundary is not just $u$ but $h$ and this distinction must be kept in mind . 2 ) $vdp$ is isentropic steady-flow shaft work . the isentropic is key here .
a supersymmetric supermultiplet - that is , a set of fields that are related by supersymmetry transformations - must contain a equal number of fermionic and bosonic degrees of freedom . a majorana fermion has two ( on-shell ) degrees of freedom . the majorana 's supersymmetric scalar partner must , therefore , be a complex scalar with two degrees of freedom . a complex scalar field can be neutral under a $u ( 1 ) $ symmetry . because the $u ( 1 ) $ transformation , $$ \phi\to\exp ( iq\theta ) \phi , $$ makes no sense if $\phi$ is a real field , all real scalar fields are necessarily neutral under a $u ( 1 ) $ symmetry ; however , not all scalar fields that are neutral under a $u ( 1 ) $ are necessarily real fields .
the simplest way to explain the christoffel symbol is to look at them in flat space . normally , the laplacian of a scalar in three flat dimensions is : $$\nabla^{a}\nabla_{a}\phi = \frac{\partial^{2}\phi}{\partial x^{2}}+\frac{\partial^{2}\phi}{\partial y^{2}}+\frac{\partial^{2}\phi}{\partial z^{2}}$$ but , that is not the case if i switch from the $ ( x , y , z ) $ coordinate system to cylindrical coordinates $ ( r , \theta , z ) $ . now , the laplacian becomes : $$\nabla^{a}\nabla_{a}\phi=\frac{\partial^{2}\phi}{\partial r^{2}}+\frac{1}{r^{2}}\left ( \frac{\partial^{2}\phi}{\partial \theta^{2}}\right ) +\frac{\partial^{2}\phi}{\partial z^{2}}-\frac{1}{r}\left ( \frac{\partial\phi}{\partial r}\right ) $$ the most important thing to note is the last term above--you now have not only second derivatives of $\phi$ , but you also now have a term involving a first derivative of $\phi$ . this is precisely what a christoffel symbol does . in general , the laplacian operator is : $$\nabla_{a}\nabla^{a}\phi = g^{ab}\partial_{a}\partial_{b}\phi - g^{ab}\gamma_{ab}{}^{c}\partial_{c}\phi$$ in the case of cylindrical coordinates , what the extra term does is encode the fact that the coordinate system is not homogenous into the derivative operator--surfaces at constant $r$ are much larger far from the origin than they are close to the origin . in the case of a curved space ( time ) , what the christoffel symbols do is explain the inhomogenities/curvature/whatever of the space ( time ) itself . as far as the curvature tensors--they are contractions of each other . the riemann tensor is simply an anticommutator of derivative operators--$r_{abc}{}^{d}\omega_{d} \equiv \nabla_{a}\nabla_{b}\omega_{c} - \nabla_{b}\nabla_{a} \omega_{c}$ . it measures how parallel translation of a vector/one-form differs if you go in direction 1 and then direction 2 or in the opposite order . the riemann tensor is an unwieldy thing to work with , however , having four indices . it turns out that it is antisymmetric on the first two and last two indices , however , so there is in fact only a single contraction ( contraction=multiply by the metric tensor and sum over all indices ) one can make on it , $g^{ab}r_{acbd}=r_{cd}$ , and this defines the ricci tensor . the ricci scalar is just a further contraction of this , $r=g^{ab}r_{ab}$ . now , due to special relativity , einstein already knew that matter had to be represented by a two-index tensor that combined the pressures , currents , and densities of the matter distribution . this matter distribution , if physically meaningful , should also satisfy a continuity equation : $\nabla_{a}t^{ab}=0$ , which basically says that matter is neither created nor destroyed in the distribution , and that the time rate of change in a current is the gradient of pressure . when einstein was writing his field equations down , he wanted some quantity created from the metric tensor that also satisfied this ( call it $g^{ab}$ ) to set equal to $t^{ab}$ . but this means that $\nabla_{a}g^{ab} =0$ . it turns out that there is only one such combination of terms involving first and second derivatives of the metric tensor : $r_{ab} - \frac{1}{2}rg_{ab} + \lambda g_{ab}$ , where $\lambda$ is an arbitrary constant . so , this is what einstein picked for his field equation . now , $r_{ab}$ has the same number of indicies as the stress-energy tensor . so , a hand-wavey way of looking at what $r_{ab}$ means is to say that it tells you the " part of the curvature " that derives from the presence of matter . where does this leave the remaining components of $r_{abc}{}^{d}$ on which $r_{ab}$ does not depend ? well , the simplest way ( not completely correct , but simplest ) is to call these the parts of the curvature derived from the dynamics of the gravitational field itself--an empty spacetime containing only gravitational radiation , for example , will satisfy $r_{ab}=0$ but will also have $r_{abc}{}^{d}\neq 0$ . same for a spacetime containing only a black hole . these extra components of $r_{abc}{}^{d}$ give you the information about the gravitational dynamics of the spacetime , independent of what matter the spacetime contains . this is getting long , so i will leave this at that .
your equivalence is not an equivalence . from the way you write , i think that your mistake comes from a problem with index notation . for the levi-civita connection , we have $$\nabla_a g_{\mu\nu} = 0 . \tag{1}$$ but this does not mean that the $a$-derivative of the $\mu\nu$ component of $g$ is $0$ . it means that the $a\mu\nu$ component of $\nabla g$ is $0$ . thus from ( 1 ) we cannot conclude that $$\nabla_b g_{ct} \overset{ ? }{=} 0 \tag{2} . $$ since in this equality $t$ is not an " index to be filled in " , but is shorthand for $g_{c\mu} t^\mu$ . we see that ( 2 ) holds iff $t^\mu$ is covariantly constant . not surprising , since it is the definition of $t_c$ . it is easy to become confused by this when doing or presenting concrete calculations . thus you really have to compute $$k_{ [ a} \nabla_b k_{c ] }$$ and check when it vanishes . however since the frobenius theorem has a formulation in terms of differential forms , one can use coordinate-independent , index-free differential forms language to solve your problem , and this could be faster . first , we have $$k_{ [ a} \nabla_b k_{c ] } = ( k \wedge dk ) _{abc} . $$ ( you may object that on the left we have the covariant derivative and on the right the exterior , and the former involves the metric but the latter does not . but it is easy to check that the christoffel symbols cancel on the left , and we get the formula for $d$ on 1-forms . this and $\nabla_a f = ( df ) _a$ for scalars implies that for an arbitrary antisymmetric tensor $t_{\mu\nu\cdots}$ , we have $\nabla_{ [ a} t_{\mu\nu\cdots ] } = ( dt ) _{a\mu\nu\cdots}$ . ) we have $$k_a = ( 1- \frac{2mr}{\sigma} ) dt + \frac{2j\sin^2\theta}{\sigma} d\phi . $$ i have not done the calculations myself from this point , but unless $j = 0$ , $\sigma$ depends on $\theta$ , so probably we will get a $d\theta \wedge d\phi$ term in $dk$ , that will not cancel .
but when they are pushed in the same direction , they create what we call an uniform electric field movement of electrons in the same direction does not create a uniform electric field . two infinite parallel plates with an electric potential difference between them , and no movement of electrons or other charged particles , would set up a uniform electric field therebetween .
if dark matter interacts only gravitationally , then the cross section for producing it in the e+e- machines is inherently too low to be detected . i am discussing e+e- machines because those are the ones that can give a closed enough system to be able to detect missing mass and energy cleanly . the cross section at the y ( about 10gev in mass ) is something like 10^-2 millibarn . now the coupling constant in front of the calculations ( squared ) is the electromagnetic one , which is orders of magnitude larger than the gravitational one . this will affect to practically zero both the magnitude and the width of any reaction producing the hypothetical 7 gev particle , either in some pair production , or associated production . there was some talk of finding more positrons than electrons associated with the measurements reported . in that case there exists a coupling between electromagnetic fields and these proposed particles , but a specific model would be needed to say at what level the production of these would be excluded by the existing world data from e+e- machines . there are limits given assuming super symmetry is the valid theory . see this aleph thesis which gives limits over 40 gev .
i am not an expert in algebraic topology by the stretch of anyones imagination , but hopefully i can shed some light on this . the starting point is , as you mention , the maxwell equations themselves . cast into a geometric language the curvature 2-form $\bf{f}$ , which you can think of as the faraday tensor for $u ( 1 ) $-maxwell theory ( there are generalisations for yang-mills theory ) , can be written $\mathbf{f}$ $= e \wedge d \sigma + b$ and we have the equations : $d \bf{f} = 0$ $\ , \ , \ , \ , \ , \ , ; \ , \ , \ , \ , \ , \ , $ $d^{\star} \bf{f} = \bf{j}$ the ' self-dual ' here is referring to hodge duality apearing in the above , since in vacuum we have an obvious symmetry here . a good physical description can be found here ( hodge star operator on curvature ? ) . so , we have a space-time manifold $\mathcal{m}$ that has a curvature 2-form satisfying some properties ( maxwell equations ) . what can this tell us about the topological structure on $\mathcal{m}$ ? the theory of de-rham cohomology is essentially the study of differential forms on manifolds . the idea is that by analysing the way in which $p$-forms behave one can deduce some global structure properties . this makes physical sense to me , since if certain classes of functions are behaving in very specific ways it must say something regarding the curvature of the manifold , right ? herein the link lies and why things can be said regarding the $\it{second}$ homology group , since $\bf{f}$ is a 2-form . a little more mathematical : if a $k$-form $\omega$ satisfies $d \omega = 0$ it is called closed . if one can write $\omega = d \lambda$ for some $ ( k-1 ) $-form $\lambda$ , $\omega$ is called exact . cohomology is the study of whether or not these two notions are interchangeable . the idea is analogous to gauge potentials for maxwell 's equations in $\mathbb{r}^{4}$ wherein we have the identity $\nabla \times ( \nabla a ) = 0$ for any function $a$ , which we of course know as the vector potential associated with $\bf{f}$ . consider the space of closed $k$-forms : $z^{k} ( \mathcal{m} ) = \{ \omega \in c^{k} ( \mathcal{m} ) : d \omega = 0 \}$ so if $\omega$ is closed then so is $\omega + d \tau$ . so , we have a very natural equivalence relation on $z^{k}: \omega \sim \omega'$ iff their difference is exact . the $k$-th de-rham cohomology $h^{k} ( \mathcal{m} ) $ is defined as a quotient of $z^{k}$ by the space of exact forms : $b^{k} ( \mathcal{m} ) = \{d \lambda : \lambda \in c^{k-1} ( \mathcal{m} ) \}$ as $h^{k} ( \mathcal{m} ) = z^{k} ( \mathcal{m} ) /b^{k} ( \mathcal{m} ) $ the dimensions of $h^{k}$ are called the betti numbers $b_{k} = \dim h^{k} ( \mathcal{m} ) $ . which are a topological invariant of the space . the euler characteristic is defined in terms of them also : $\chi = \sum ( -1 ) ^{k} b_{k}$ , which is an important curvature invariant . on manifolds it tells you whether your space is compact if it vanishes , for example , and relates to the hodge dual contracted riemann tensor . edit ( some more ) : in essence , this means we have a specific way to test the homological structures since , by definition of the maxwell equations , we have $\bf{f}$ $\in z^{2} ( \mathcal{m} ) $ . as a last note , the hodge dual gives a canonical way to associate ( using poincaré duality ) $h^{k} ( \mathcal{m} ) $ with its dual space . so the maxwell equations really give some deep insight into both ( co ) -homological groups in vacuum . a good reference is this paper by dotti and kozameh ( http://www.famaf.unc.edu.ar/~gdotti/1.pdf ) .
but that is exactly the deeper meaning ! setting things up so that all coordinates are in the same units ( besides being a reasonable requirement for $x^\mu$ to be considered a four-vector ) is a constant reminder that time is really not that different from space . in fact , if it were not for that sign in the metric , spacetime would be completely symmetric in its coordinates . this is made even more apparent when we switch to a system of units in which $c = 1$ . now we do not even have to make a decision to set $x^0 = ct$ , because by measuring both time and space in meters ( or seconds if you prefer ) , we have completely abandoned the notion of time being a distinct entity which is somehow independent from space .
at the galactic center , there is an object called sagittarius a* which seems to be a black hole with 4 million solar masses . in 1998 , a wise instructor at rutgers made me make a presentation of this paper http://arxiv.org/abs/astro-ph/9706112 by narayan et al . that presented a successful 2-temperature plasma model for the region surrounding the object . the paper has over 300 citations today . the convincing agreement of the model with the x-ray observations is a strong piece of evidence that sgr a* is a black hole with an event horizon . in particular , even if you neglect the predictions for the x-rays , the object has an enormously low luminosity for its tremendously high accretion rate . the advecting energy is pretty " visibly " disappearing from sight . if the object had a surface , the surface would heat up and emit a thermal radiation - at a radiative efficiency of 10 percent or so which is pretty canonical . of course , you may be dissatisfied by their observation of the event horizon as a " deficit of something " . you may prefer an " excess " . however , the very point of the black hole is that it eats a lot but gives up very little , so it is sensible to expect that the observations of black holes will be via deficits . ; - )
you can see accurate visualizations of ( simulated ) eddy currents in multiple papers . one example is : efficient solvers for nonlinear time-periodic eddy current problems
you get a rise in a capillary tube because it reduces the energy stored in the surface tension at the air-water and air-glass interface . the water rises until the reduction in the surface tension energy is balanced by the increase in the gravitational potential energy of the water . but it is not at all obvious how you could extract energy from this . if you evaporate water from the top of the tube then you will certainly pull up more water to replace the water lost by evaporation . i suppose this is analogous to a tree pulling up water , though my limited memory of biology i think the sap is driven up the tree by osmotic pressure in the roots as well as by capillary action . i suppose you could put a microturbine at the bottom of the capillary tube then heat the top and extract energy as the water rises up the tube to replace the water that is evaporated . however i doubt this would be as efficient as just using the same amount of heat in steam engine . were you wondering if there was a way to make the water rise up the tube , then fall back , then rise up again , generating energy with each cycle ? the only way you could do this was if there was some way to change the air-water or air-glass surface tension in some reversible way . you can easily reduce the air-water surface tension by adding surfactant , and this will make the water drop , but you had need to get the surfactant back out to make the water rise again .
just adding some precisions to @alfredcentauri ' s answer . when you use a ket notation $|p\rangle$ , or $|e\rangle$ , or $|x\rangle$ , by definition , these kets are eigenvectors of their corresponding operators : $p|p\rangle=p|p\rangle , \quad h|e\rangle = e |e\rangle , \quad x|x\rangle = x|x\rangle$ it is not a calculus , it it a definition . so , if you try to find a solution of the schrödinger equation by $\left| \psi \right \rangle = \left| \lambda \right \rangle e^{-iet/ \hbar}$ , where you do not know $|\lambda \rangle$ , the first lines of your question prove that $|\lambda \rangle = |e\rangle$ , thanks to the definition of $|e\rangle$ finally , starting from the definition of the ket $|p\rangle$ : $p|p\rangle=p|p\rangle$ , we have obviously $\large \frac{p^2}{2m}|p\rangle=\frac{p^2}{2m}|p\rangle$ . in the case of a free particle , you have $h = \frac{p^2}{2m}$ , so finally $ h|p\rangle=\frac{p^2}{2m}|p\rangle = e_p|p\rangle$ . this shows that $|p\rangle$ is a eigenvector of $h$ , with the eigenvalue $e_p=\frac{p^2}{2m}$ in a one-dimensional problem , for one value of $e$ , you have to possibilites for $p$ , such as $e=e_p$ , these are $ p\pm = \pm \sqrt{2me}$ .
some more misconceptions : the chemguide website quoted above might be a useful reference for " uk-based exam purposes " as stated there , but it certainly does not help in solving the question . the arguments given above that followed the comments on chemguide are inaccurate . a simple quantum chemistry calculation of gold in its ground state will give you that the electron in the s orbital ( a1g ) is the most energetic in this atom . hence , ionization will most easily be accomplished by removal of this electron , and not of d electrons , and this is easily proved by another computation for ionized gold , which will show you that the 5d orbitals will remain filled while the 6s orbital is no longer occupied . actually , it is known that if the most external d shell is filled , the energies of these orbitals will be effectively lowered , and there is a very high probability that the ionized electron will not come from it , but from more energetic s or p orbitals . ( i have just done a few of these calculations in order to make sure this point is right ) in order to analyze why some metals are more inert than others , various effects come into play . relativistic effects , such as the contraction of s orbitals , for example , are a major factor in making gold less reactive than silver , and in lowering the oxidation potential of gold . so , in addition to looking at chemical potentials when discussing the inertness of metals in different environments , it is better not to reduce the arguments to simple electron configuration trends which usually work quite well for main group elements , since , though they might generate insights for the understanding of the behavior of metals , these insights might be either right or wrong .
for generality , we work with the function supplied by the op , but with arbitrary constants , $$f ( t ) =\ln [ \cosh ( at ) ] $$ applying the chain rule , we may compute the first derivative , $$f' ( t ) =\frac{1}{2\cosh ( at ) }\frac{d}{dt}\left ( e^{at} +e^{-at}\right ) = a \tanh ( at ) $$ to compute higher order terms in the taylor expansion , we require higher derivatives analogously computed using repeated application of the chain rule : $$f'' ( t ) =a^2 \mathrm{sech}^2 ( at ) \quad \quad f''' ( t ) =-2a^3\tanh ( at ) \mathrm{sech}^2 ( at ) $$ $$f^{ ( 4 ) } ( t ) =2a^4 ( \cosh ( 2at ) -2 ) \mathrm{sech}^4 ( at ) $$ the taylor series of the original $f ( t ) $ is given by the usual expression , $$f ( t ) =\sum_{n=0}^{\infty}\frac{f^{ ( n ) } ( p ) }{n ! } ( t-a ) ^n$$ if we choose to center our series at $p=0$ , after some algebraic manipulations , we obtain , $$f ( t ) =\frac{1}{2}a^2 t^2 -\frac{1}{12}a^4 t^4 + \mathcal{o}\left ( t^6\right ) $$ for small $|t| \leq 1$ , as the powers of $t$ increase , the corrections will become smaller and smaller . hence higher order terms may be truncated , and the approximation , $$f ( t ) \approx \frac{1}{2}a^2 t^2$$ would be sufficient . plot of various taylor series orders : black : to order $\mathcal{o} ( t^2 ) $ , blue : to order $\mathcal{o} ( t^4 ) $ , red : to order $\mathcal{o} ( t^6 ) $ . the behavior of a taylor approximation after a finite region around the center $p$ is common ; i believe it is known as runge 's phenomenon .
as witten explains in his notices of the ams article ( please see also his more recent lecture ) , the fully quantum string theory is characterized by two coupling constants ( or in the language of deformation quantization : two deformation parameters ) . the string coupling $g_s$ and the string tension $\alpha^{'}$ . in perturbation theory , one gets dependence of the string amplitudes on powers of $g_s$ ( or equivalently in $\hbar$ ) through the genus expansion . the dependence of the amplitudes $\alpha^{'}$ is obtained once one takes into account that in the presence of background fields , the string lagrangian is not free , it is described by a sigma model . if we compute the quantum correction to this sigma model we get terms with more and more derivatives multiplied by more powers of the of the string tension ( as in chiral perturbation theory ) . when the quantum corrections to the trace of the energy momentum tensors are calculated then here also terms depending on powers of $\alpha^{'}$ will appear and the condition of vanishing of the beta function will results einstein 's equations with correction terms proportional to $\alpha^{'}$ . please see equations 3.7.14 . in polchinsky 's first volume , where the beta functions are given to the first power of $\alpha^{'}$ . witten explains that for a while , the work on string theory concentrated on finding candidates of $\alpha^{'}$ deformed theories ( as conformal field theories ) , then $\hbar$-quantize them as in ordinary quantum theory . but , as witten explains , after the discovery of the full set of string dualities and the role of membranes , it was realized that in order to fully quantize string theory , the two quantizations or two deformations ( $\hbar$ , $\alpha^{'}$ ) must be perfomed together . this route has profound consequences , for example , it leads to the conclusion that the string full quantum theory should be in the realm of noncommutative geometry , because in the presence of a $b$-field and brane boundary conditions , the position-position commutation relations will obtain $\alpha^{'}$ deformation and become noncommutative . as a consequence , the ordinary uncertainty relation will get $\alpha^{'}$ deformation and turns into a generalized ( minimal scale ) uncertainty , in which the position uncertainty has a nonvanishing minimum : $\delta x \geqslant \frac{\hbar}{\delta p}+ \alpha^{'}\frac{\delta p}{\hbar}$
as you noted , the ising model has spins that are $\pm 1$ whereas in a full quantum model such as the heisenberg model , the spins are represented by pauli matrices . this means that they are not interchangeable . the biggest difference is that at zero temperature , there are no spin fluctuations in an ising model , whereas there are fluctuations in the heisenberg model . the ising hamiltonian can be written as $$h = j\sum_{\langle i , j\rangle} \sigma_i \sigma_j$$ and all the $\sigma_i \in \{-1,1\}$ . whereas for quantum spins , we had have $$h = j\sum_{\langle i , j\rangle} \vec{s_i} \cdot \vec{s_j}$$ and the $\vec{s_i}$ are vectors with elements determined by the pauli matrices . this product can then be expanded as $$\vec{s_i} \cdot \vec{s_j} = s_i^z s_j^z + \frac{1}{2}\left ( s_i^+ s_j^- + s_i^- s_j^+\right ) $$ where $s_i^+$ and $s_i^-$ are the spin raising and lowering operators . thus , we get one term that looks just like the ising term , because $s_i^z$ can be either $-1$ or $1$ , but we also get terms that describe how the two spins can flip : they start with opposite spin and then both of them flip . for a ferromagnet in the ground state , this is not important , because there the spins are all in parallel and thus the flip terms give zero contribution , but in an antiferromagnet , it makes the ground state highly complicated , whereas in an ising model the ground state would just have neel order .
concerning the second question , the planck length is the planck length and not newton 's length ( yes , the op has asked this question ) . newton did not know planck 's constant which was only discovered 2+ centuries later so he could discuss neither planck 's constant nor the planck length and other natural units which are functions of planck 's constant . max planck realized the importance of planck 's constant $h$ – usually used in the form of the reduced planck constant i.e. dirac constant $\hbar=h/2\pi$ – for the black body formula he derived ( it appeared previously in high-energy approximations of the black body formula ) . he was also able to figure out that any quantity ( with any units ) has a natural unit which may be written as a product of powers of three fundamental universal constants , $\hbar , c , g$ . so he derived all the natural units or planck units such as the planck length , planck mass , planck time , and products of their powers . what you wrote is the unique product of such powers of $\hbar , c , g$ that has the unit of length : you should be able to verify it is approximately $1.6\times 10^{-35}$ meters . because it is a length that is calculated purely from constants that are totally natural , and " naturally equal to one " , in quantum mechanics ( $\hbar$ ) , relativity ( $c$ ) , and a theory of gravity ( $g$ ) , it is a length that is likely to play a prominent role in any theory that addresses quantum mechanics , relativity , and gravity , i.e. any theory of quantum gravity . string theory is a theory of quantum gravity ( the only known theory of quantum gravity that is free of internal contradictions , in fact ) , so the planck length is important in string theory , too . well , because string theory contains extra dimensions , a more fundamental constant could be a " higher-dimensional planck length " which is comparable to the usual planck length you wrote down – but could also be very different if some of the extra dimensions of space are either hugely warped or very large ( relatively to the planck length ) . previous question about the planck length and my answer about its relevance is here : how to get planck length
one does not have to set $g=1$ or $8\pi g=1$ in a quantum theory of gravity ; it is just one possible choice that may be convenient . if one wants to study relevant and irrelevant operators in general relativity and its extensions , it is useful not to set $g=1$ or $8\pi g=1$ because by doing so , we would make all quantities dimensionless . instead , it is a good idea to reformulate general relativity in the same way as other quantum field theories . quantum field theories with a weakly coupled classical limit are usually described by lagrangians $$ {\mathcal l} = {\mathcal l}_\text{free} + {\mathcal l}_\text{interactions} $$ the fields are normalized and redefined so that the kinetic terms ( those with 2 derivatives in the case of bosons , 1 derivative in the case of fermions ) have the usual normalization , schematically $ ( \partial_\mu \phi ) ^2/2$ for bosonic fields and $\bar\psi \partial_\mu \gamma^\mu \psi$ for fermionic fields . this is possible for general relativity , too . note that its lagrangian is the integrand of the einstein-hilbert action $$ {\mathcal l}_{\rm gr} = \frac{1}{16\pi g} r $$ and it is proportionally to ricci scalar that schematically contains terms $g \partial^2 g$ , among others . we may expand the metric around a background , in the simplest case the flat background $$ g_{\mu\nu} = \eta_{\mu\nu} + \sqrt{8\pi g} \cdot h_{\mu\nu} $$ the $\eta$ tensor is the flat minkowski metric ; $h$ is the perturbation away from it which carries the " operator character " . i have conveniently added the coefficient in front of $h$ because when we insert it to the einstein-hilbert action , the leading term will generate $${\mathcal l} \sim ( \partial h ) ^2 $$ and the coefficients involving $g$ will cancel ; let me be sloppy about the numerical coefficients of order one . however , the nonlinear einstein-hilbert action will also produce terms that are of higher order in $h_{\mu\nu}$ but each $h$ will appear together with a factor of $\sqrt{g}$ , too . so the cubic and higher interaction vertices in general relativity are weighted by $\sqrt{g}$ and its higher powers . because $g$ has a positive dimension of length for $d\gt 2$ , the interactions in general relativity are irrelevant ( non-renormalizable ) . that is true even for $d=3$ . however , gravity in $d=3$ has no local excitations , it is kind of vacuous , so the non-renormalizability problem may be , to some extent , circumvented . at any rate , for $d=4$ and higher , even the leading interaction is irrelevant and leads to non-renormalizable divergences already at 2-loop level ( and even in ${\mathcal n}=8$ supergravity , which offers as many supersymmetric cancellations as possible , there are new divergences requiring counterterms at the 7-loop level ) . so the theory breaks down at a cutoff scale that is not too far from the planck scale , $m_{\rm pl} = g^{1/ ( d-2 ) }$ . that is where a consistent theory of quantum gravity i.e. string/m-theory has to cure the problems by providing the theory with new states ( strings , branes , and – more universally – black hole microstates ) and new constraints . only if we try to study distances shorter than the planck distance ( a sort of a meaningless exercise for many reasons ) or energy scales higher than the planck scale ( where the typical " particles " really look like ever larger black holes ) , we find out that the rg flows break down and become meaningless as a methodology . however , at distances much longer than the planck length , the rg flows for gr are just fine and behave as they do in any non-renormalizable theory . the naively quantized einstein 's theory is not predictive at the planck scale but at much lower energies , one may systematically add new corrections , with a gradually increasing number of derivatives , to make an effective field theory ever more accurate . some people , most famously steven weinberg , have speculated that there could be a " zero-distance " ultraviolet limit that could run to general relativity at long distances . i think that the research attempting to find evidence for this conjecture remains inconclusive , to say the least . most folks in quantum gravity are actually convinced that this can not work not only because of the apparent absence of the required scale-invariant field theory describe the uv ; but also because such a picture of gravity would contradict holography and black hole thermodynamics .
i thought i might just start with an introduction first . : ) the basic principal behind free electron laser is that of synchrotron radiation . when electrons or charged particles are made to change momentum ( like being bent in a in an arc where the force is radially inwards ) they emit electromagnetic radiation . if the particles are relativistic then the electromagnetic radiation the lab observer relative to the electron will observe the electromagnetic radiation being emitted in a cone in the direction of motion . ( i will post figures if people really want ! ) in the case of the free electron laser you need to have magnetic arrays which are simply dipole magnets aligned such that the electrons will see as its travelling in a straight line , alternating vertical magnetic fields . this causes the electrons to " undulate " horizontally . with relativistic electrons ( which is not hard to do ) you will emit synchrotron radiation like a pencil beam . the wavelength from an undulator is given by lambda = undulator_period/ ( 2gamma^2 ) * ( 1+a^2/2 ) , where gamma is the relativistic gamma factor , a = e*b*undulator_period/ ( 2*pi*electronmass*speedlight ) where b is the strength of the magnetic field and e the electric charge . but the radiation from all the electrons are not very temporally coherent ! now as to why its called a laser ! the fundamental feature of why lasers are powerful is because the waves emitted by all sources are emitted in a temporally coherent fashion . i.e. all the electric fields add constructively so the total power scales like pn^2 where p is the power emitted by a single source and n is the number of sources . in a fel the undulation is small enough that the electrons are irradiated by its own synchrotron radiation and is therefore travelling in an optical field as well as the magnetic field from the dipole magnets . the optical field causes the electrons to micro-bunch and cause further amplification of that particular wavelength ( micro-bunching effect ) and is referred to as self-amplified spontaneous emission ( sase ) . this positive feedback generates powers that give an additional gain of n . typical bunches of electrons are 10^11 electrons , so you can get orders of magnitude more power . i presume you can reduce the size of the magnetic arrays to just a stretch of material with carefully oriented domains however it will be difficult to reach the fields required . to measure thats the field of spectroscopy which i am less familiar . for uv you will have to have some material with dispersion properties like a prism of sorts and a detector that will respond to uv radiation . for x-rays you can use monochromators that use the bragg principal and determine the energy ( calibrated to know emission lines from elements ) . the energy loss along the undulator is a small factor . the emitted photons are very small fractions of its total energy . for example for 10 to 100 nm wavelengths you would need something like 300 mev electrons ( rest mass of an electron is 0.5 mev ) so beta = 0.9999986 , and 10 nm photons are only 10s of ev . i thought the military were focusing on uv fels like you said , to reduce water absorption . xrays are much harder to generate with enough power to cause damage . that was more than i initially intended to say . hope its useful .
the claim is consistent with page 37 of this and page 11 of this . the $\beta$-function of $u ( 1 ) $ depends on the charge of the degrees of freedom , but since there are none below the scale $a$ , it is zero . as for hep-th/9707133 , i suppose that the coupling there is given for non-vanishing charges .
yes , hydrogen ( and anything ) evaporates/sublimes in vacuum
the essence of a glide is that the aircraft is descending . just like a car rolling down a moderate grade , it is trading potential energy to replenish the kinetic energy lost to drag . whether the nose points up or down only relates to the angle of attack , which only relates to speed . an aircraft traveling at slow speed has a higher angle of attack , so its nose will point up , compared to when it is traveling at high speed . one of the things you learn in flight training is how to handle a loss of power . there is a mnemonic for that : abc a : trim for the airspeed ( 65 kts in a c172 ) that gives you the best glide range . this is fairly slow and nose-high . ( there is even a somewhat slower speed that gives you less range but more time aloft . ) b : look for the best landing site , be it a field , road , or if you are lucky , an airport . c : look in the cockpit for what you can do , like trying to restart the engine , and calling on the radio . so , under a , you can see that a slow glide is relatively nose-up , even while the aircraft is descending .
such operators are ill-defined in an interacting theory because whatever counterterms we try to subtract , their expectation value in any finite-energy state will diverge . the closest operators that are well-defined are densities of charge – number operators with signs labeling antiparticles – because the divergent contributions naturally cancel for them . in free quantum field theories , you may define the number operator and write it as an integral but the integrand will not really be commuting with itself at other points so the attribution of the particles into different points will be misleading . in the non-relativistic limit of quantum field theory , all these problems go away under some extra assumptions .
the $\delta$ function is not continuous , so it is a priori not differentiable . in fact , it is not even well-defined as an ordinary real-valued function , but can be made so in terms of distributions - linear maps on a space of test functions given by $f\mapsto\int\delta f=f ( a ) $ . it is possible to sensibly define derivatives of distributions by looking at representations as limits of functions : if $\delta_i$ is a family of functions so that $\lim_{i\rightarrow\infty}\int\delta_i ( x ) f ( x ) \mathrm dx=f ( a ) $ for any test function $f$ , then it can be considered a representation of the dirac delta . now , if we take the family of derivatives $\frac{\mathrm d}{\mathrm dx}\delta_i$ we arrive at $$ \int\left [ \frac{\mathrm d}{\mathrm dx}\delta_i ( x ) \right ] f ( x ) \mathrm dx=-\int\delta_i ( x ) \left [ \frac{\mathrm d}{\mathrm dx}f ( x ) \right ] \mathrm dx $$ through integration by parts and using the fact that $f$ has by definition compact support ( which makes the boundary term vanish ) . as the derivative is linear as well , this defines another linear map $f\mapsto-\int\delta f'$ on the space of test functions , which we call the derivative of our distribution . symbolically , $$ \left [ \frac{\mathrm d}{\mathrm dx}\delta ( x-a ) \right ] f ( x ) =-\delta ( x-a ) f' ( x ) $$ which you can just plug in into your formula above without any need for actual computation as it holds true by definition .
entropy is a concept in thermodynamics and statistical physics but its value only becomes indisputable if one can talk in terms of thermodynamics , too . to do so in statistical physics , one needs to be in the thermodynamic limit i.e. the number of degrees of freedom must be much greater than one . in fact , we can say that the thermodynamic limit requires the entropy to be much greater than one ( times $k_b$ , if you insist on si units ) . in the thermodynamic limit , the concept of entropy becomes independent of the chosen ensembles - microcanonical vs canonical etc . - up to corrections that are negligible relatively to the overall entropy ( either of them ) . a single particle , much like any system , may be assigned the entropy of $\ln ( n ) $ where $n$ is the number of physically distinct but de facto indistinguishable states in which the particle may be . so if the particle is located in a box and its wave function may be written as a combination of $n$ small wave packets occupying appropriately large volumes , the entropy will be $\ln ( n ) $ . however , the concept of entropy is simply not a high-precision concept for systems away from the thermodynamic limit . entropy is not a strict function of the " pure state " of the system ; if you want to be precise about the value , it also depends on the exact ensemble of the other microstates that you consider indistinguishable . if you consider larger systems with $n$ particles , the entropy usually scales like $n$ , so each particle contributes something comparable to 1 bit to the entropy - if you equally divide the entropy . however , to calculate the actual coefficients , all the conceivable interactions between the particles etc . matter .
the energy stored in the spring is equal to the work done in compressing it . the force needed to compress the spring to the maximum is only a small part of this calculation . suppose the spring is completely relaxed , and you apply a small force compressing the spring slightly . the energy stored is the product of the force and the distance , measured in newton-metres , or joules . now you increase the force slightly , and the spring compresses a short distance more . the additional energy stored in this new force multiplied by this next compression distance . this continues , until the spring is totally compressed . so to calculate the energy stored , you need to know how far the spring compresses , and how the compression force changes as the spring compresses . edit to rely to comment : suppose the spring is quite long in its relaxed state , with a spring constant of 40 lbf/in . the spring will need to compress by 50 in , to reach the winch maximum of 2000 lbf . . so the energy stored in the compressed spring would be ( with suitable conversion to si ) $$e_s=\frac12kx^2=\frac127005\times1.27^2=5650 \text{ joules}$$ so this would be the energy available to loft a projectile . the problem would be to transfer as much as possible of this energy to a light projectile . there are inherent , internal losses in the rapid extension of a spring that make it difficult to efficiently launch a missile . usually , the spring pushes slowly , with huge force on the short end of a lever , while a light mass accelerates quickly at the long end of the lever . google " trebuchet " , or even " punkin chuckin " . spelling is correct ! the equation in the comment would give the maximum height possible , with no losses in the spring .
goldstein 's " closed " means the orbiting body will eventually return to some point with the same velocity it had there previously ; i.e. that the path will repeat itself exactly . note that this can occur even in the case of precession : if the ratio between radial period and angular period is rational , the orbit will be closed . there is no precession only if this ratio is unity . what goldstein is saying is that in this point in the text , all he has shown is that the objects ' separation $r$ will satisfy $r_1 &lt ; r &lt ; r_2$ for all time , and that this does not necessarily imply that the orbit will ever repeat itself . for example , we have not ruled out the possibility that $r$ could be periodic with period $\pi$ , while $\theta$ could repeat every 3 units of time . other authors use " closed " or " elliptical " to mean goldstein 's " bounded , " and " open " or " hyperbolic " to mean " unbounded . " part of the degeneracy in terms probably comes from the fact that for two ideal point masses in newtonian mechanics , bounded orbits will be closed in goldstein 's sense . see the application of bertrand 's theorem to the inverse square law .
it is not clear whether you mean a pulse of a wave , i.e. a short section of a wave , or whether you mean a top hat function , but in both cases the principle is the same . if you fourier transform a pulse of a wave or a top hat function you get the frequencies that make it up . if you decrease the length of the pulse or reduce the width of the top hat function you will find that the width of your fourier transform increases i.e. it spreads across more frequencies . that means it is harder and harder to pin down what you mean by the frequency of the pulse . in the limit of reducing your pulse to a delta function you find the fourier transform now inludes all frequencies from zero to infinity at an equal amplitude so it is impossible to define even an average frequency . this is the sense in which $\delta k$ becomes larger . in real life we often find the fourier transform is approximately gaussian and we can define $\delta k$ as the 1/e half width ( i.e. . the standard deviation ) .
the problem is with your first calculation and also with the somewhat misleading equation that you have found . it is true that $$\frac{i_2}{i_1}=\left ( \frac{d_1}{d_2}\right ) ^2$$ but units are important here . in that formula , $i_1$ and $i_2$ would properly be expressed as power values . to compute with decibels , which are logarithmic quantities , one would instead use $$i_1 + 10\log\left ( \frac{d_1}{d_2}\right ) ^2 = i_2$$ or equivalently , $$i_1 + 20\log\left ( \frac{d_1}{d_2}\right ) = i_2$$ , where $i_1$ and $i_2$ are decibels and $d_1$ and $d_2$ are in identical linear units ( feet or meters , for example ) . with your particular numbers we get $$\begin{eqnarray} i_2 and = and 213\text{ db} + 20 \log\left ( \frac{75}{45000}\right ) \\ and = and 213\text{ db} + 20\log\left ( \frac{1}{600}\right ) \\ and \approx and 213\text{ db} + 20 ( -2.78 ) \\ and \approx and 213\text{ db} - 55.56 \\ and \approx and 157.4\text{ db} \end{eqnarray}$$ estimating manually you have correctly remembered that -3db is half the power . that is , $$\frac{1}{2}p = -3\text{db}$$ . another easily remembered fact is $$\frac{1}{10}p = -10\text{db}$$ . both are very commonly used in engineering for rough estimations . so in this case , because it is an inverse square law , we have $$\begin{eqnarray} \left ( \frac{75}{45000}\right ) ^2 and = and \frac{1}{600^2} \\ and = and \frac{1}{360000} \\ and \approx and \frac{1}{400000} \\ and \approx and \frac{1}{2^2\cdot 10^5} \\ and \approx and -6\text{db} - 50\text{db} \\ and \approx and -56\text{db} \end{eqnarray}$$ so this would give $213\text{db} - 56\text{db} = 157\text{db}$
the way i understand your description , the motion of the light and the spring tension are perpendicular to the direction of acceleration . consider the problem in the ( instantaneous ) rest frame of the mirrors and spring . let 's use coordinates where x is the direction of the photon 's motion ( in the unmoving rest frame ) , and y is in the direction of acceleration ( and motion ) of the mirrors . the relevant equations are spring force $f = kx$ , photon momentum in the mirror ( x ) direction = $p_x = \hbar k_x$ , photon rate $1/t$ where $t$ is the time between photons , and the photon force = change in momentum per unit time = $2\hbar k_x / t$ . the requirement for no change to the mirror is that $kx = 2\hbar k_x/t$ . clearly the spring is just a spring in the moving rest frame and so there is no change to the spring force $kx$ . as for the photons , their momentum perpendicular to the direction of velocity ( or acceleration ) is unchanged , so there is no change to the momentum any single photon imparts to the mirror $2\hbar k_x$ . uh , note that since the photon reverses its direction , it imparts twice its momentum to the mirror . another way of saying the same thing is to note that stationary and moving observers agree on the number of photon wavelengths $\lambda_x$ between the two mirrors . they also agree on the distance between the two mirrors . therefore they agree on the wave number $k_x = 2\pi/\lambda_x$ for the photon 's momentum in the x direction . so what is left is the rate at which the photons impact the mirror $1/t$ . this rate does not depend on the acceleration ( relativity problems rarely do ) . instead it depends on the velocity of the mirror . as the mirror velocity increases , the distance traveled by the photons in the moving frame increases . thus the time between photons increases and the photon force decreases . of course in the unmoving rest frame the photon rate is unchanged , it is only in the moving frame that the photon rate changes . this effect is called time dilation . consequently , the non moving observer , on noting the force on the mirror , must conclude that the spring constant $k$ has changed due to the motion of the spring . for a discussion of this interesting effect , see : http://www.mathpages.com/home/kmath068/kmath068.htm
i will try to answer your questions while explaining as much basic quantum mechanics as possible , so that you will be able to fill in the blanks with only linear algebra . hilbert space- as a novice to qm i am very sad that in none of the books i have read i found the reasons for using hilbert space h at first place followed by a full geometrical explaination of this space and how we build this space out of r . it goes same for for its dual space hd . where can i get this ? every single author starts this topic by just bombard novices with bunch of rules for h , hd which i can not just trust and this is only pointless learning by heart . in qm , the state of a system is given by a set of complex amplitudes . if i have a coin that says 0 on one side and 1 on the other , and i flip it , classically i will have a coin in either the state 0 or the state 1 , with a probability of 1/2 for each outcome . in qm , instead of probabilities over different states , you have amplitudes . to get the probability you take the square of the amplitude . so , if i flip a quantum mechanical coin , i may end up in the state $$ \frac{1}{\sqrt{2}} |0\rangle + \frac{1}{\sqrt{2}}|1\rangle . $$ all this means is that the probability that i will observe state 0 is $$ \left ( \frac{1}{\sqrt{2}} \right ) ^{2}= \frac{1}{2} . $$ so , to describe the state of a quantum system , you need a set of observation outcomes , and an amplitude for each outcome . above , our outcomes were 0 and 1 , and our amplitudes were $1/\sqrt{2}$ . if we were dealing with regular probabilities , the space of states of a system with $n$ possible outcomes would be $\mathbb{r}^{n}$ . this is because each outcome has a probability , so you can write the entire state as a set of $n$ probabilities , which are just real numbers . in quantum mechanics it is the same , except instead of probabilities we have $n$ amplitudes . so the space of states is $\mathbb{c}^{n}$ . the reason physicists talk about hilbert spaces instead of just complex euclidean spaces is that in general your set of measurement outcomes could be infinitely big , instead of just $n$ . the hilbert space is just a generalization of the complex euclidean space , and you can often just think of complex euclidean spaces instead . dirac 's notation . . . because i do not understand hilbert space i do not understand what i am allowed to do with kets | ⟩ and bras ⟨ | and for example : why do we have to write an operator on the left side of kets |a^ψ⟩=a^|ψ⟩ but for bras it is vice versa ( we write it on the right ) ⟨a^ψ|=⟨ψ|a^ . why can we factor out a constant from kets |aψ⟩=a|ψ⟩ and we can only factor out complex conjugate from bras ⟨aψ|=a∗⟨ψ| . now that we understand that states are given by complex vectors , we can understand bra-ket notation . in my coin flipping example , imagine that i use the amplitudes $1/\sqrt{3}$ and $\sqrt{2/3}$ . i can represent the state of my coin as $$\left|\mbox{coin}\right&gt ; = \frac{1}{\sqrt{3}} |0\rangle + \sqrt{\frac{2}{3}}|1\rangle = \frac{1}{\sqrt{3}}\left ( \begin{matrix} 1 \\ 0 \end{matrix}\right ) + \sqrt{\frac{2}{3}}\left ( \begin{matrix} 0 \\ 1 \end{matrix}\right ) = \left ( \begin{matrix} 1/\sqrt{3} \\ \sqrt{2/3} \end{matrix}\right ) $$ there is a lot of notation being used in the expression above . as is often done , i wrote the name of the system inside a ket . this is to be read a " the state of the coin " and means nothing else mathematically , other than to say it is a vector in a hilbert space . this is actually what i have been doing when i wrote $|0\rangle$ and $|1\rangle$ . i have also decided that the outcome 0 should correspond to the vector $\left ( \begin{matrix} 1 \\ 0 \end{matrix}\right ) $ , and 1 should correspond to $\left ( \begin{matrix} 0 \\ 1 \end{matrix}\right ) $ . this was an arbitrary decision . i did it only because i know that my system 's state is given by a set of complex numbers , and i want to be able to write the entire state down as a simple vector in a hilbert space . so , kets are vectors in a hilbert space . i have decided to write them as column vectors , but this was also arbitrary . if kets are column vectors in a hilbert space , then bras are row vectors with the rule that for any $\left|\psi\right&gt ; $ , $$\left&lt ; \psi\right| = \left|\psi\right&gt ; ^{\dagger} . $$ here i used $\dagger$ to mean conjugate transpose . it is unfortunate that there are so many different notations for this operation , but you seem to understand what it means . okay , so states are vectors in a hilbert space . operators are matrices acting on the vectors . a matrix times a vector gives a vector , so this is a good way to model transformations . now the confusion about what is allowed and what is not allowed with bras and kets just comes down to linear algebra . hopefully you can now answer questions like why does $\alpha^{*}\left&lt ; \psi\right| = \left ( \alpha|\psi\rangle \right ) ^{\dagger}$ . this has turned into a long answer so i will leave it here for now .
attempts to find an average value of ac would directly provide you the answer zero . . . hence , rms values are used . they help to find the effective value of ac ( voltage or current ) . this rms is a mathematical quantity ( used in many math fields ) used to compare both alternating and direct currents ( or voltage ) . in other words ( as an example ) , the rms value of ac ( current ) is the direct current which when passed through a resistor for a given period of time would produce the same heat as that produced by alternating current when passed through the same resistor for the same time . practically , we use the rms value for all kinds of ac appliances . the same is applicable to alternating voltage also . we are taking the rms because ac is a variable quantity ( consecutive positives and negatives ) . hence , we require a mean value of their squares thereby taking the square root of sum of their squares . . . peak value is $i_0^2$ is the square of sum of different values . hence , taking an average value ( mean ) $i_0^2/2$ and then determining the square root $i_0/\sqrt{2}$ would give the rms . it is example time : ( i think you did not ask for the derivation of rms ) consider that both the bulbs are giving out equal-level of brightness . so , they are losing the same amount of heat ( regardless the fact of ac or dc ) . in order to relate both , we have nothing to use better than the rms value . the direct voltage for the bulb is 115 v while the alternating voltage is 170 v . both give the same power output . hence , $v_{rms}=v_{dc}=\frac{v_{ac}}{\sqrt{2}}=115 v$ ( but guys , actual rms is 120 v ) . as i can not find a good image , i used the same approximating 120 to 115 v . to further clarify your doubt regarding the peak value , it is simply similar to finding the distance between two points $ ( x_1 , y_1 ) $ and $ ( x_2 , y_2 ) $ in cartesian system represented as , ( sum of squares and then " root" ) $$d=\sqrt{ ( x_2-x_1 ) ^2+ ( y_2-y_1 ) ^2}$$
dear dan , first of all , you should not use the term " uncertainty principle " if you are talking about " light sources " and light may be explained by ordinary - classical ( non-quantum ) - electrodynamics where no uncertainty principle applies . this is just an exercise in the propagation of waves . second , when you flip the switch , there may be temporary variations of the intensity , but they are not necessary , either . for example , you may find a minimum such that the number of wave peaks on the two trajectories ( coming from the two slits ) differs by 13.5 - one arm is 13.5 wavelengths longer than the other one . it will mean that the destructive interference only occurs when the beams from both slits are synchronized , and there will always be a period lasting about 13 periods after each flip of the switch when only one beam is coming to the detector . that will indeed eliminate the destructive interference , and give you the " apostrophes " in your ascii art . the precise shape of the graph depends on the character of the switches , geometry of the experiment , and other things .
it is difficult to compare becuase of different distance and time scales . but iodine 131 fallout from nevada tests caused doses of 10mgray - 200mgray ( 1-20rad ) across the central and north eastern us , 10mgray is about 8 x-rays or 1 ct-scan . generally bomb fallout contains more of the short lived highly active isotopes which are distributed further by the effects of the bomb . reactor fires generally disperse fallout more slowly and so many isotopes have decayed before they reach the environment
you are talking about the inductive effects of the coil of wire . essentially a wrapped up coil of metal with electrons running through it creates a linear magnetic field since moving electrons through a wire creates a redial field and if you approximate the coil to have infinite loops the field becomes liner . but , this effect would be very , very small for the wires you are talking about since ( a ) the coils are not very densely packed and ( b ) not very much current is flowing through them here is the wiki on inductors : http://en.wikipedia.org/wiki/inductor the simple relationship between voltage ( $v$ ) , inductance ( $l$ ) , and current ( $i$ ) is : $$v ( t ) = l \frac{di ( t ) }{dt}$$ one last thing to consider , magnetic field drops off with distance fast ( so as you move away from the source it gets really weak ) . the plastic protective covering around your wires are a relatively similar thickness to the wires themselves and would buffer anything nearby to most the magnetic effects ( which would be weak to begin with )
a $w^-$ in one direction and a $w^+$ in the opposite direction are exactly the same thing . ( that is what it means to be an antiparticle . )
thermal conductivity has dimensions of $\mathrm{power / ( length * temperature ) }$ . power is the rate of heat flow , ( i.e. . ) energy flow in a given time . length represents the thickness of the material the heat is flowing through , and temperature is the difference in temperature through which the heat is flowing . in si units , it is commonly expressed as $\mathrm{watts / ( meter * kelvin ) }$ , and in us units , it is commonly given in $\mathrm{btu/hr/ ( feet\ *\ ^of ) }$ . it expresses the rate at which heat is conducted through a unit thickness of a particular medium . that rate will vary linearly based on the temperature difference across the material , so it is expressed as a value per degree of temperature difference , thus heat rate per unit thickness per degree of temperature difference .
nothing gets " pushed away " . instead , if it was left to itself everything would fly off in a straight line ( newton 's law , right ? ) . this is easy to see if you are , say , hovering over the playground in a helicopter watching children fly off the merry-go-'round . i did a sketch . the dark haired kid is holding on ; he goes in a circle . the blond it not ; he moves at a constant velocity . remember that acceleration in a change in velocity and that velocity means both speed and direction . so , the key question is why does it look like they are getting " pushed away " ? the answer is that when you see that happening you are getting pushed toward the center ( that is the centripetal force ) . you are accelerating , while they maintain a constant speed and direction , but they certainly end up a long way away from you .
your idea is realized within the formalism of lagrangian mechanics in terms of so-called constraints . it is based on the introduction of suitable generalized coordinates which capture the degrees of freedom of the system . you then introduce the effect of forces acting in your system ( e . g . forces keeping a point mass on a certain trajectory like a circle ) by constraint equations . these allow for an elegant solution of the problem in terms of the euler-lagrange equations . in case that the constraints are given by algebraic equations , one speaks of holonomic constraints . instead of giving a thorough mathematical treatment on the concept , i would rather refer to the rich literature on the subject . for a good introduction with nice examples i can recommend these lecture notes .
the notion of covariant derivative is equivalent to the notion of connection . more precisely , for every connection $\nabla$ and vector field $x$ , the operation $\nabla_x$ is a covariant derivative . connections $\nabla$ on the tangent bundle $tm$ of a manifold are usually induced by a metric , this is the so called levi-citiva connection . it is essentially determined by the requirement that the scalar product fulfills the product rule $$ d_x\langle v , w\rangle = \langle \nabla_x v , w\rangle + \langle v , \nabla_x w \rangle . $$ note that this does not yet determine the connection completely , one has to add the additional requirement that there is no torsion , $$ \nabla_x y - \nabla_y x = [ x , y ] . $$ this is a coordinate-free specification of a connection . note , however , that unlike the lie-derivative or the derivatives of differential forms , which are defined in terms of the manifold alone , connections represent additional data . many different connections are possible on a single manifold , whereas the other two notions of derivative are unique .
no , the friedmann equations assume the cosmological principle : the universe at large scales is homogeneous and isotropic . the metric that describes earth may be approximately isotropic ( the same in every direction ) , but it is not homogeneous ( the same in every place ) . a better approximation to the space-time around earth is actually the schwarzschild solution , the one that also describes static black holes and any other spherically symmetric mass .
at the critical point , the bulk gap is closed , and there is nothing to prevent the edge state from penetrating into the bulk . so the gapless mode simply merge into the bulk . the modes ( of opposite chirality ) from both edges will mix . both the chirality and majorana property will be " canceled out " by the mixing .
whether this is any faster is debatable , but you could do it this way : the trajectory of the shell is symmetric , so $m$ firing a shell at $o$ is the same as $o$ firing a shell at $m$ . so all you have to do is consider $o$ firing the mortar at 45° and ask what is the minimum velocity required to clear the wall . so , if $o$ fires the shell at 45° the equations of motion are : $$\begin{align} x and = t \frac{v}{\sqrt{2}} \\ y and = t \frac{v}{\sqrt{2}} - \frac{1}{2} g t^2 \end{align}$$ if we require that the trajectory passes through the point $ ( 25 , 10 ) $ then this gives us two simultaneous equations in $t$ and $v$: $$\begin{align} 25 and = t \frac{v}{\sqrt{2}} \\ 10 and = t \frac{v}{\sqrt{2}} - \frac{1}{2} g t^2 \end{align}$$ it is $v$ we are interested in , so we rearrange the first equation to get : $$ t = \frac{25\sqrt{2}}{v} $$ and substitute it in the second equation to get : $$ 10 = \frac{25\sqrt{2}}{v} \frac{v}{\sqrt{2}} - \frac{1}{2} g \left ( \frac{25\sqrt{2}}{v} \right ) ^2 $$ and a quick rearrangement gives : $$ v = \sqrt{\frac{g \space 25^2}{15}} = 20.2 m/s $$
there is a difference between temperature and energy . plasma is , as you said , very hot - but there is not very much of it . the density of plasma in the tube is very low . so when it does hit the walls of the tube it transfers very little energy . so the mass of the glass tube increases in temperature only very slightly . it is like a firework sparkler , the sparks are at 2000degc but they are very small , have very little mass and contain very little energy - so when one lands on you it transfer much less energy than a hot cup of coffee at 80deg c .
ok , i have found this : http://www.cv.nrao.edu/course/astr534/brightness.html i proves that it is not possible to build such optical system . the conservation of brightness also applies to any lossless optical system , a system of lenses and mirrors for example , that can change the direction of a ray . no passive optical system can increase the specific intensity or total intensity of radiation . if you look at the moon through a large telescope , the moon will appear bigger ( in angular size ) but not brighter . many people are disappointed when they see a large , nearby galaxy ( e . g . , andromeda ) through a telescope because it looks so dim ; they expected to see a brilliantly glowing disk of stars , as in the photograph below . the difference is not in the telescope ; it is in the detector—the photograph appears brighter only because the photograph has summed the light over a long exposure time . though the andromedia example is not necessarily correct . . . because it consists of many stars which have huge surface brightness . i think if we could have large enough aperture can could resolve sirius as a disk it would be more eye damaging sight than the sun . . .
analyzing the acceleration of the center of mass of the system might be the easiest way to go since we could avoid worrying about internal interactions . let 's use newton 's second law : $\sum f=n-mg=ma_\text{cm}$ , where $m$ is the total mass of the hourglass enclosure and sand , $n$ is what you read on the scale ( normal force ) , and $a_\text{cm}$ is the center of mass acceleration . i have written the forces such that upward is positive the center of mass of the enclosure+sand moves downward during process , but what matters is the acceleration . if the acceleration is upward , $n&gt ; mg$ . if it is downward , $n&lt ; mg$ . zero acceleration means $n=mg$ . thus , if we figure out the direction of the acceleration , we know how the scale reading compares to the gravitational force $mg$ . the sand that is still in the top and already in the bottom , as well as the enclosure , undergoes no acceleration . thus , the direction of $a_\text{cm}$ is the same as the direction of $a_\text{falling sand}$ . let 's just focus on a bit of sand as it begins to fall ( initial ) and then comes to rest at the bottom ( final ) . $v_\text{i , falling}=v_\text{f , falling}=0$ , so $a_\text{avg , falling}=0$ . thus , the ( average ) acceleration of the entire system is zero . the scale reads the weight of the system . the paragraph above assumed the steady state condition that the op sought . during this process , the center of mass apparently moves downward at constant velocity . but during the initial " flip " of the hour glass , as well as the final bit where the last grains are dropping , the acceleration must be non-zero to " start " and " stop " this center of mass movement .
first of all , this is a really amazing piece of technology , in particular because it has achieved something that optical engineers have dreamed of for a long time 1 , and done so with underlying techniques that we have had for quite a while . just to give you some impression of how cool this is , i am an optical engineer and the first time i heard about this i was certain that it was either a hoax , a massive exaggerated description of something less impressive ( like an unsharp filter in photoshop or something ) , or simply a piece of godawful technology journalism . however , once i found the doctoral dissertation of the guy who developed this technology dr . ren ng ) , i realized how it works , and really how clever it is . ( note : i will probably add on to this answer a couple times , because i do not have time to give a good and thorough summary right now . if you want a really thorough description , check out the thesis i linked above . it may be a little over your head if you do not have background in optics though , which is what my summary here should help with . ) hardware this technique depends on some very clever data processing techniques , and on the unusual type of camera ( ren ng calls it " plenoptic " which is not a term i have heard before ) that is used to capture the data . this camera has an array of very small lenses ( a " microlenslet array" ) at its image plane , where a normal camera would just have the image sensor . the image sensor is positioned slightly behind this . the microlens array alters the incoming light before it hits the sensor . if you were to look at this raw data as it is captured by the camera , it would look similar to the image you would expect from a normal camera , but it would be composed of thousands of little blobs rather than being a nice continuous image ( i am not talking about pixels , these dots are many pixels across ) . if you zoom in on this image , you would see that each dot is actually a very small , possibly blurry image of a portion of the scene being photographed . on its own , this image is ugly and not really good for anything , but because we know exactly how it was altered by the lenslet array , we actually have more information about the light that entered the camera 2 . with some clever data processing algorithms , we can retrieve this information and use it to determine the focus condition of each part of the scene being imaged . algorithms ( this section will be expand when i have more time ) in the most basic sense , the job of an imaging system is to produce an image where each point records the color and brightness of a corresponding point in the scene being photographed . a plenoptic camera also aims to determine , for each point in the image , how the light from the corresponding point in the scene was focused . this amounts to figuring out not only where that light hits your sensor ( which a normal camera measures ) but what path it took to get there . the data processing done after the image is captured is able to reconstruct this information because instead of having only one piece of information about each ray of light -- which pixel on our sensor it hit -- we now have a second piece of information -- which lenslet in the array did that ray pass through . once we have figured out the path of each bundle of light that we captured from the scene , we can calculate the image we would have gotten from a normal camera for any focus setting over a large range . 1: there are actually other way to achieve this , but the methods developed by ren ng are impressive and novel in that they are reliable and do not require insane amounts of computing power/time . this is what makes his technology marketable to consumers . 2: actually , we did not really get extra information , we just traded a little of one type of information for a little of another type . a normal camera would be able to produce one pixel in the output image for each pixel in its sensor , but this camera will produce an output image with about one pixel for each microlens in the array . the details of the algorithm may raise or lower that ratio a little , but the basic idea about trading one type of information for the other will always hold . this is , by the way , on reason that this technology did not happen sooner -- it was not until recently that we could produce high quality lenslet arrays with enough lenses to produce a good picture .
if you collide two protons in the lhc you get lots of particles coming out of the collision , and the total mass of these particles is far greater than the mass of the two protons . so the lhc demonstrates the conversion of energy to matter every day .
every ordinary star we are able to individually observe is a part of the milky way . well , except for stars in a small number of very nearby galaxies but even galaxies such as andromeda look like a " continuum " so we are not observing the stars individually although we see that the galaxy is not just a point . only if a star goes nova ( a lethal nuclear explosion of a white dwarf star ) of supernova ( a similar explosion but stronger ) , it may be observed outside the milky way . in all such cases we have experienced , one may always identify a galaxy at the same location that was known before the nova/supernova explosion . so the star going nova/supernova clearly belongs to that galaxy . please note that distant galaxies look like dots – pretty much visually indistinguishable from stars in the milky way . a star going nova has 50,000-100,000 times higher luminosity than the sun ; the number is even higher for a supernova . that is a sufficient increase of the luminosity for an exploding star in a distant galaxy to become " almost as bright " as the whole galaxy , well , not quite .
so . . . the delta function in time is defined as : $\delta ( t - t_0 ) = \int _0^\infty \delta ( t ) dt = \int_t^{t_0} \delta ( t ) dt \equiv 1$ which means that the delta function has the unit of $sec^{-1}$ hence i should divide by the time step .
i cannot tell you if 18kv is correct but yes - it must be a high voltage to create such a spark . addressing the current : it is not simply a u=ri behavior since you have a capacity in your circuit which means , that the current get less over time . this is what the circuit looks like : which means , that the current would behave like : the current behaves like : the current does not equal the charge on the plate but they are dependent on each other by the following equation : keep in mind that if the voltage gets too low for a spark on a certain distance the discharge process stops . i hoped that this answer helped you and contains the information you liked to know .
this comes from the microscopic origin of the model . for example , in the case of the hydrogen atom , the dipole operator is given by ( up to some signs ) $\hat d=e \hat z e$ where i have assumed that the electric field is in the direction $z$ , and $\hat z$ is the position operator of the electron ( of charge $-e$ ) . let 's now have a look at the effects of the parity operator $\hat \pi$ . we have $ [ \hat h , \hat \pi ] =0$ , meaning that the eigenstates such that $\hat\pi\ , |g/e\rangle=\pm|g/e\rangle$ and we also have $\hat \pi\ , \hat z\ , \hat \pi=-\hat z$ . it is thus easy to show that $\langle g/e|\ , \hat z\ , |e/g\rangle=0$ by symmetry , which answers the question . microscopically , one can show that the selection rule of the of the matrix elements of $\hat z$ between the eigenstates $|nlm\rangle$ of the hydrogen atom are such that $\langle nlm|\ , \hat z \ , |n\ , ' l\ , ' m\ , '\rangle\propto \delta_{m , m'}\delta_{l , l\ , '\pm1}$ .
this answer is specifically about guitars because i have guitar building and repair experience . the strings and the truss rod are under tension so the neck itself is mainly under compression . there is some tension on the back side of the neck due to neck relief ( forward bow of neck ) but not much . necks are made from wood from the trunk , a material ' built ' to handle the compressive stress from the weight of the tree . the safety factor ( failure load/service load ) of guitar necks are very large . a very basic calc . to demonstrate this : compressive yield strength parallel to grain for the most common guitar neck wood ( maple ) = 21.5 m pa approximate cross sectional area = 0.0007 m ^2 f = pa = ( 21.5 m pa ) * ( 0.0007 m^2 ) = 15 kilo newtons . sf = ( failure load/service load ) = ( 15 kn ) / ( 800 n ) = 18.75 basically , guitar necks are super strong and only fail due to impact ( see gibsons broken headstock syndrome ) or warp due to poorly seasoned wood or extreme humidity or temp . changes .
for geometrical optics we can introduce eikonal $\psi$ by relation $$ f = ae^{-ik_{\mu}r^{\mu} + i\varphi} = ae^{i\psi} . \qquad ( 1 ) $$ for small time interval and space lengths eikonal can be expanded in a form $$ \psi = \psi_{0} + \mathbf r \frac{\partial \psi}{\partial \mathbf r} + t \frac{\partial \psi}{\partial t} , $$ so , by compairing with left side $ ( 1 ) $ it can be directly show that $$ \mathbf k = \frac{\partial \psi}{\partial \mathbf r} , \quad \omega = -\frac{\partial \psi}{\partial t} . \qquad ( 2 ) $$ so by comparing $ ( 2 ) $ with hamilton-jacobi equations we have clear analogy : wave vector acts rule of classical momentum and frequency acts rule of hamiltonian in geometrical optics . so it is possible to introduce the analogy of principle of the least action for rays . for light it can be done by maupertuis principle : $$ \delta s = \delta \int \mathbf p d \mathbf l = 0 \to \delta \psi = \delta \int \mathbf k d \mathbf l = 0 . $$ for example , for optically isotropic and homogeneous space $\mathbf k = const * \mathbf n $ , and $$ \delta \int dl = 0 , $$ which leads to fermat 's least time principle .
no , it is not a boolean property . entanglement between two quantum systems ( could be particles , or anything else ) could be partial , and can be quantified using different measures . in the specific example of bell states , the two systems ( each of them with 2 states $|0\rangle$ and $|1\rangle$ ) are said to be maximally entangled with the entanglement entropy being 1 qubit .
the available methods depend on what you know about the cft ( or another quantum field theory ) . for example , there are nice classes of two-dimensional conformal field theories ( i mean " minimal models " etc . ) that are almost uniquely ( up to several integer-valued labels ) determined by the conformal symmetry . all the dynamics – correlators etc . – of these cfts are fully encoded by the opes which means that all this dynamics is fully encoded in the coefficients $c_{ijk}$ . these coefficients may be calculated as solutions to the constraints equivalent to the conformal symmetry . once you calculate them , you should view them as a major part of the definition of the cft – they are the most fundamental numbers defining the identity of the cft so it is futile to try to calculate them from something more fundamental . if you have more constructive cfts , the cfts may be essentially " free fermions " or " free bosons " . in such a case , the opes may be calculated by " wick contractions " of some kind . for more general ( but non-free ) cfts , it is helpful to notice that the operators $a_j ( w ) $ are in one-to-one correspondence to the states in the cft ( quantized on a circular space times infinite time ) and the structure coefficients $c_{ijk}$ may also be calculated from the action of the operator $a_i$ on the state corresponding to the equally named operator $|a_w\rangle$ .
i believe that the answer to this question involves multiple parts . i will try and hit all of them . since you mentioned the ray model , i will assume you are relatively familiar with geometric optics . first , we do see different images of the same object at times ! or rather , we see a blurry image rather than a sharp one . if you bring a pencil so close to your eyes that you cannot focus on it using your iris ( more on that later ) , then you will see a blurry image . this happens for exactly the reason you mention . rays from the same point of the object take different paths to your retina . if the path taken to your lens makes a large angle compared to the path that passes straight through the lens , then in general the rays will not recombine at a single point , but will have some spread . as the object moves closer to your eye , the angles increase , so the spread becomes larger and you see a blurry image . your iris is very important for creating sharp images . generally , the distance between your lens and your retina is fixed . the distance between the object and your lens is not fixed , but we would like to be able to resolve detail for some range of object distances , rather than just a single one . so , the only parameter your body can control is the focal length . by constricting and relaxing , the iris changes the curvature of your lens . the change in curvature leads to a change in focal length . so , whatever object you are attempting to focus on , your iris constricts so that the object is beyond the focal length of your lens . this ensures that the rays will converge toward the retina and produce an image . however , even with the object beyond the focal length you still get a blurry image if the rays make a large angle with the axis that is perpendicular to your eye/lens/retina ( as discussed before ) and this is one reason why you have a pupil . the pupil only allows rays that are approximately all parallel to each other to fall on your lens . it effectively acts as a collimator . so now , you have an object at or beyond the focal length and a set of approximately parallel rays that fall on your lens and are focused . this leads to a relatively sharp image on your retina ( assuming that the object is far enough away that the pupil can do its job ) . the final piece of the puzzle is your brain . even though your iris , lens , and pupil do what they can to create a sharp image for your retina , it is still imperfect . there are still a number of aberrations in the image that falls on your retina . your visual cortex and related support areas in the brain do all of the processing and reconstructing that leads to you perceiving a sharp image .
okay , so i gather from the link that $g^{ ( k ) }$ in your notation refers to the correlation between field values at $2k$ points , with $\varepsilon^+$ inserted at half of them and $\varepsilon^-$ inserted at the other half . this concept of an $n$-point correlation function is very similar to the $n$th moment of a random variable or statistical distribution . for simplicity , consider an example with 1 dimension : a field over 1 time dimension ( at a single point in space ) , described by a random variable . now , we can consider the probability distribution of this random variable and talk of it is moments . the mean value would be called the 1st moment and the variance would be related to the second moment ( it is in fact called the second central moment ) . similarly , you can generalize to higher order moments which help characerize the distribution . the moments help you characterize the distribution and also give an intuitive feel for the function . generalize this concept to random variables which are fields over many-dimensional spacetime . that is what your correlation functions are . btw , for a gaussian distribution ( non-interacting fields i.e. quadratic action ) , all odd moments vanish . ( that might be the motivation for $g^{ ( k ) }$ to be defined as the correlation between field values at $2k$ points . . . even though the actual physical theory you are considering will probably be interacting , else all correlation functions are fairly trivial ) . also , all even moments beyond the 2nd-moment are completely specified by the 1st and the 2nd moment . ref1 and ref2 if you had an interaction term in the hamiltonian/lagrangian involving 4 fields , then the 4-point correlation function would have 2 kinds of contributions : 2 sets of 2-point correlation functions between pairs of points among those 4 points a nontrivial contribution from the interaction term with one of it is field insertions at each of the 4 points . so you can see that higher order correlations functions give you very important ( an unique ) information in an interacting physical theory . update : the ( many ) answers to this se question might also shed some light on the discussion .
where you went wrong was $$-mgk \times \frac{1}{2}at^2=-mgk \times \frac{1}{2}\frac{\delta v}{t}t^2$$ . instead it should be $$-mgk \times \frac{1}{2}\frac{\delta v}{\delta t}t^2$$ anyways i would do it as $$f_{friction}s = mgk\frac{1}{2}at^2 = \frac{v-u}{2t}mgkt^2 = \frac{v-u}{2}mgkt = \frac{27.7ms^{-1}}{2} \times 540kg \times {10ms^{-2}} \times 0.6 \times t = ( 44874kgm^2s^{-3} ) t$$ so $$60kw \times t = ( 44874kgm^2s^{-3} ) t + 207000j$$ and solving gives $$t=13.7s$$
the anomalies in four dimensions are calculated from a triangular feynman diagram with a chiral ( left-right-asymmetric , when it comes to the couplings with the gauge bosons or gravitons ) fermion running in the loop and three gauge bosons ( and/or graviton [ s ] ) attached at the vertices . for the standard model , all the gauge anomalies cancel ( both leptons and quarks must be included , otherwise it would not work : it is somewhat nontrivial although the cancellation may be reduced to one simple observation in gut theories , among other fast methods to see why it works ) . they must also cancel in the supersymmetric models and in order to see what these anomalies are , we may look at the difference between the anomalies in the non-supersymmetric and supersymmetric models . the minimal supersymmetric standard model has almost the same spectrum of chiral spin-1/2 particles , the quarks and leptons : their anomalies cancel . their new superpartners are scalars which do not contribute to the anomaly . the new superpartners of gauge bosons are majorana fermions which are left-right-symmetric and contribute zero to the anomalies , too . the only new particles in the supersymmetric theory that may be running in the loop that contribute to the anomaly are higgsinos , the superpartners of the higgs boson ( the whole doublet ) . the anomaly ( for various combinations of gauge bosons ) from one higgsino , one new weyl fermion , is nonzero . it must be cancelled because gauge anomalies are inconsistencies ( preventing us from decoupling negative-probability time-like polarizations of gauge bosons ) . so the mssm deals with that by adding two opposite higgsinos whose charges are opposite to each other so all the anomalies cancel in between them . there is one more supersymmetric reason why we need two higgs doublets : the yukawa couplings must be holomorphic , arising from a superpotential $w=y\cdot h \bar q_l q_r$ , and when one distinguishes chiral superfields and antichiral superfields ( their complex conjugate ) , he finds out that only the up-type quarks ( or only the down-quark quarks ) could get masses from one higgs doublet ( the charges would not add up to zero if you added the opposite quarks ) . so the opposite , complex conjugate higgs doublet superfield ( whose higgsinos have the opposite handedness for the same sign of the supercharge and the weak isospin ) is needed to give masses to the remaining one-half of the quarks .
some other " basic " arguments in favor of susy also prefer tev-like superpartner masses although not necessarily as light as 1 tev . the dark matter composed of neutral superpartners , to agree with observations of the current amount of dark matter and the required amounts in the cosmological past needed to preserve some important cosmological processes , should also have mass at most several tev , say at most 10 tev . the gauge coupling unification – which may exist in nature because it is elegant but not it is not inevitable – in mssm works with a satisfactory precision but only if the running is dictated by the spectrum of mssm on " most of the logarithmic interval " between the electroweak scale and the gut scale . well , here the tolerance is clearly higher for higher masses . also , one may imagine that some corrections or modifications of the spectrum do the job despite very heavy superpartners . but the simplest mssm-like way to achieve gauge coupling unification , which still looks intriguing to most physicists , also has to assume that all the superpartners are much closer to the electroweak scale than e.g. to the gut scale .
the lamp is not a point source . the smaller the angular size of the source , the narrower is the penumbral shadow region .
you should use $u=q\epsilon_1$ . with the total number of particles $n$ being constant , we have : $$\frac{\partial s}{\partial q}=\epsilon_1 \frac{\partial s}{\partial e}=\frac{1}{t}\epsilon_1\tag{1}$$ as you said : $$\frac{\partial s}{\partial q}=k_b\ln ( n/q - 1 ) =k_b\ln ( n\epsilon_1/q\epsilon_1 - 1 ) =k_b\ln ( n\epsilon_1/u - 1 ) \tag{2}$$ $$\to \ , \ , \ , \ , \ , u ( t ) =n\epsilon_1\frac{e^{-\epsilon_1/ ( kt ) }}{1+e^{-\epsilon_1/ ( kt ) }}$$
basically , in atomic physics , you would have two electrons , each with an angular momentum $l_1$ and $l_2$ and spin $s_1$ and $s_2$ , and you want to couple all those to get the best approximation for the resulting spectrum . so you have two options : 1- you couple $l_1$ and $l_2$ to $l$ , and $s_1$ and $s_2$ to $s$ , and then you couple $l$ with $s$ to get $ls$ . 2- you couple $l_1$ and $s_1$ to $j_1$ , and $l_2$ and $s_2$ to $j_2$ , and then couple $j_1$ and $j_2$ to to get $jj$ . so you have two ways to couple those , and the choice depends on how far the electrons are from each other where the specific angular momentum coupling is more pronounced . so if the electrons are close to each other , then you use ls coupling . while if you have them far apart , you use jj coupling . i hope this helps .
the very claim that there are " four fources " is an approximation . we know that the electromagnetic and the weak force have to be unified to an electroweak theory . so counting the electroweak theory as one force , there are just three known elementary forces . the electroweak theory is based on the $su ( 2 ) \times u ( 1 ) $ group which has two factors , but these two factors are not in one-to-one correspondence with the electromagnetism and the weak force , respectively . the strong force with its $su ( 3 ) $ group is another seemingly independent factors , except that there is evidence that all three non-gravitational forces get unified into a grand unified force of a gut theory at high energies . string theory unifies the non-gravitational forces with gravity , too . every vacuum of string theory predicts gravity described by gr plus extra non-gravitational forces . the number of factors and their higgs-like breaking patterns are essentially random properties of the string vacua . according to the anthropic picture of the world , the number of low-energy forces is an accidental property of our world that could be different in different parts of the multiverse . according to non-anthropic reasoning , the precise selection of our vacuum - including the fact that it has 4 low-energy forces - could be derivable from some more unique theoretical principles . however , this research program remains a wishful thinking as of 2011 .
if you go to this link you will see that the lifetime of the pi0 is orders of magnitude shorter than of the charged pions . 8.4 ± 0.6 × 10^−17 seconds , a time characteristic of electromagnetic reactions . it decays to two photons , which can be measured in the laboratory . if it is produced with some energy in the laboratory system , its speed can be estimated by measuring the four momenta of the photons and equating the sum to the four momentum of the pi0 . its speed then can be found for that individual measurement . there is no general " speed " of the pi0 , as there is no general speed of any elementary particle , their four momenta being dependent of the interaction that produced them and very variable . to have a speed a fraction of the speed of light any pion or other elementary particle should have an energy given by the relativistic formulae . have a look here where they calculate the energy necessary for a velocity 1% of the velocity of light for various particles .
is mr . koks wrong or am i missing something conceptually ? you are simply misunderstanding him . here is the full context of article in question : these equations are valid in any consistent system of units such as seconds for time , metres for distance , metres per second for speeds and metres per second squared for accelerations . in these units c = 3 × 108 m/s ( approx ) . to do some example calculations it is easier to use units of years for time and light years for distance . then c = 1 lyr/yr and g = 1.03 lyr/yr2 . here are some typical answers for a = 1g . so in theory you can travel across the galaxy in just 12 years of your own time . if you want to arrive at your destination and stop then you will have to turn your rocket around half way and decelerate at 1g . in that case it will take nearly twice as long in terms of proper time t for the longer journeys ; the earth time t will be only a little longer , since in both cases the rocket is spending most of its time at a speed near that of light . ( we can still use the above equations to work this out , since although the acceleration is now negative , we can " run the film backwards " to reason that they still must apply . ) mr . koks is saying that the total proper time , for a journey that constantly accelerates half the distance and then constantly decelerates ( at the same rate ) the remaining half , will be nearly twice as long as a journey that accelerates at a constant rate the entire distance . once the rocket reaches ultra-relativistic speed , the proper time nearly stops . so , it is reasonable that the proper is nearly twice as long for a journey that decelerates to sub-relativistic speed and then to a stop at its destination instead of speeding by it at near light speed . also , it is reasonable that the total coordinate time does not change that much between the two scenarios for the reason give by the author . do you see why ?
you would have to use the fact that the momentum operator in position space is $\vec{p} = -i\hbar\vec{\nabla}$ and use the definition of the gradient operator in spherical coordinates : $$\vec{\nabla} = \hat{r}\frac{\partial}{\partial r} + \hat{\theta}\frac{1}{r}\frac{\partial}{\partial\theta} + \hat{\phi}\frac{1}{r\sin\theta}\frac{\partial}{\partial\phi}$$ so the radial component of momentum is $$p_r = -i\hbar\hat{r}\frac{\partial}{\partial r}$$ however : after a bit of investigation prompted by the comments , i found that in practice this is not used very much . it is more useful to have an operator $p_r&#39 ; $ that satisfies $$-\frac{\hbar^2}{2m}\nabla^2 r ( r ) = \frac{p_r&#39 ; ^2}{2m} r ( r ) $$ this lets you write the radial component of the time-independent schrödinger equation as $$\biggl ( \frac{p_r&#39 ; ^2}{2m} + v ( r ) \biggr ) r ( r ) = e r ( r ) $$ the action of the radial component of the laplacian in 3d is $$\nabla^2 r ( r ) = \frac{1}{r^2}\frac{\partial}{\partial r}\biggl ( r^2\frac{\partial r ( r ) }{\partial r}\biggr ) $$ and if you solve for the operator $p&#39 ; _r$ that satisfies the definition above , you wind up with $$p&#39 ; _r = -i\hbar\biggl ( \frac{\partial}{\partial r} + \frac{1}{r}\biggr ) $$ this is called the " radial momentum operator . " strictly speaking , it is different from the " radial component of the momentum operator , " which is , by definition , $p_r$ as i wrote it above , although i would not be surprised to find people mixing up the terminology relatively often .
here we will only consider the first half of the question ( v2 ) . the birkhoff 's theorem is e.g. proven ( at a physics level of rigor ) in ref . 1 and ref . 2 . imagine that we have managed to argue that the metric is of the form of eq . ( 5.38 ) in ref . 1 or eq . ( 7.13 ) in ref . 2: $$ds^2~=~-e^{2\alpha ( r , t ) }dt^2 + e^{2\beta ( r , t ) }dr^2 +r^2 d\omega^2 . \qquad\qquad ( a ) $$ it is a straightforward exercise to calculate the corresponding ricci tensor $r_{\mu\nu}$ , see eq . ( 5.41 ) in ref . 1 or eq . ( 7.16 ) in ref . 2 . the notation is here $x^0\equiv t$ , $x^1\equiv r$ , $x^2\equiv\theta$ , and $x^3\equiv\phi$ . assuming a vanishing cosmological constant $\lambda=0$ , the einstein 's equations in vacuum read $$r_{\mu\nu}~=~0~ . $$ the argument is now as follows . from $r_{tr}=0$ follows that $\beta$ is independent of $t$ . from $e^{2 ( \beta-\alpha ) } r_{tt}+r_{rr}=0$ follows that $\partial_r ( \alpha+\beta ) =0$ . in other words , the function $f ( t ) :=\alpha+\beta $ is independent of $r$ . define a new coordinate variable $t:=\int^t dt&#39 ; ~e^{f ( t&#39 ; ) }$ . then the metric $ ( a ) $ becomes $$ds^2~=~-e^{-2\beta}dt^2 + e^{2\beta}dr^2 +r^2 d\omega^2 . \qquad\qquad ( b ) $$ rename the new coordinate variable $t\to t$ . then eq . $ ( b ) $ corresponds to setting $\alpha=-\beta$ in eq . $ ( a ) $ . from $r_{\theta\theta}=0$ follows that $$1=e^{-2\beta} ( 1-2r\partial_r\beta ) \equiv\partial_r ( re^{-2\beta} ) , $$ so that $re^{-2\beta}=r-r$ for some real integration constant $r$ . in other words , we have derived the schwarzschild solution , $$e^{2\alpha}~=~e^{-2\beta}~=~1-\frac{r}{r} . $$ finally , if we switch back to the original $t$ coordinate variable , the metric $ ( a ) $ becomes $$ds^2~=~-\left ( 1-\frac{r}{r}\right ) e^{2f ( t ) }dt^2 + \left ( 1-\frac{r}{r}\right ) ^{-1}dr^2 +r^2 d\omega^2 . \qquad\qquad ( c ) $$ it is interesting that the metric $ ( c ) $ is the most general metric of the form $ ( a ) $ that satisfies einstein 's vacuum equations with $\lambda=0$ . the only freedom is the function $f=f ( t ) $ , which reflects the freedom to reparametrize the $t$ coordinate variable . references : sean carroll , spacetime and geometry : an introduction to general relativity , 2003 . sean carroll , lecture notes on general relativity , chapter 7 . the pdf file is available here .
trigonometric functions do not " preserve " units . the expression under a trigonometric function must be dimensionless and so is the value of a trigonometric function . thus , c 2 in your equations is in units of frequency : hz or 1/s . there is an error in one of the equations , perhaps a missing constant .
the usual approximation is to disregard liquid volume against vapor volume , and to consider the latter to be that of an ideal gas , so you get $\delta v = rt - 0 = rt$ and so $$ \frac{dp}{dt}=\frac{l}{rt^2} $$ if you simply want to check your results , there are also plenty of available resources on-line with boiling points for water as a function of pressure , such as this .
the wind does not impart a ' force ' on the object . the object does not ' feel ' the wind ( as long as it is not in contact with the ground ) . what actually happens is that the object moves concurrently with the wind in the same direction and with the same speed , in addition to the object 's own velocity . so you have the object 's velocity and the wind velocity ; add them up and you get a net velocity , which is the true direction of travel . from what is given , the angle that the velocity makes with the vertical is $$\theta=360^{\circ}-\text{wind bearing angle}$$ the horizontal and vertical components of velocity are $$\cos ( \theta ) =\frac{w_y}{w}$$ $$\sin ( \theta ) =\frac{w_x}{w}$$ where $w$ is the net wind velocity . you add the components of the wind speed to the object 's speed to get the net velocity $$v_{x}=w_x+o_x$$ $$v_y=w_y+o_y$$
there are a lot of methods to calculate free surface or multiphase flows , and most of them have some implementation of surface tension forces . a small list : volume of fluid method level set method marker and cell method moving mesh techniques
what you are talking about is called a combined cycle engine . they are commonplace in stationary power generation , i.e. utility-scale electricity generation . there has even been some talk of combined cycle engines in cars . as pointed out in the answer by dmckee , the reason this has not been widely applied in cars is that no one has demonstrated an economically competitive combined-cycle car . i promise you , if such a thing can pay for itself in gas savings then it will eventually be built and sold , unless some better technology makes it irrelevant . in general there are many reasonable ideas that are physically permissible but economically or technically difficult or nonviable . you are effectively suggesting to add a steam engine to a car , which is quite a difficult proposal . i would suggest that a hybrid gas-electric car is more economical than what you suggest , and even they have had a hard time catching on . in electric power generation it matters much less that the combined cycle engine has a larger sunk cost than a normal engine , is heavier , etc . , so the economic balance works out . bringing the question back to physics , no matter what you use for heat scavenging , your engine including all of its " subengines " cannot exceed the carnot efficiency corresponding to the largest temperature difference in the engine . adding additional heat engines will help to approach the carnot limit . in order to beat carnot , you can not use heat as an intermediate step between chemical energy ( fuel ) and mechanical work .
the electron-phonon scattering is a process that takes the electron across its fermi surface ( from an occupied state to an empty state , or vice versa ) by absorbing or emitting a phonon . compared to the electron energy in most solid state materials , the phonon energy is neglectable , such that the electron will ( almost ) not change its energy when scattering with a phonon . therefore the phonon can only scatter off the electrons on the fermi surface . however the greatest possible momentum transfer on the fermi surface is $2k_f$ ( assuming spherical fermi surface with radius $k_f$ ) . so all scattered phonon must have a momentum $q\leq 2k_f$ .
i would guess you mean self diffusion : see http://en.wikipedia.org/wiki/self-diffusion for details . suppose you take an aqueous solution of ( for example ) salt that is uniform so there are no concentration gradients . there is no net diffusion , but the sodium and chloride ions wander around due to random thermal motion , so if you watch a particular sodium atom it will " diffuse " around in a random walk motion .
from this google book hit , i think it is called a ketenyl radical . searching ketenyl radical seems to bring up hits with the same compound formula . incidentally , it was not that hard to find by searching hcco compound . ninth hit , i think . . .
i think your questions concerns somehow another question : what is the relation between the macroscopic observables ( like electrical current , temperature ) of system consisting of many particles and parameters of single particles forming this system . the answer on this question is given by statistical physics . first , it will be useful to think about an electron gas in a metal wire without any current . there is no current , but i dare you all particles are moving if the temperature is not equal zero . the electrical current results from averaging of velocities of all charge carriers and , therefore , equals zero . if we apply voltage all electrons gain an additional component to their chaotic velocities in the direction of the electric field . we have got electron drift , however it is not equal to microscopic current of each electron . is the velocity of the particle in the direction of the current ? it depends on the charge sign of charge carriers . if we have got electrons then their averaged speed is opposite to the direction of the electrical current .
k.y. tang is a geophysicist who is known for work on the allais effect , which is pathological science dating back to the 1950 's , when allais claimed anomalous effects on a foucault pendulum during an eclipse . a google scholar search shows no citations yet to tang et al . ' s february 2013 paper claiming to have measured the speed of gravity . as is often the case with pathological science , there seems to be a certain set of people who take the subject seriously and cite each other 's papers , while people outside their circle can not be bothered to debunk them . this particular subgroup includes kooks like van flandern , who has claimed , for example , that light propagates faster than $c$ . as discussed in the answers to this question , we have strong indirect confirmation from binary pulsars of gr 's prediction that gravity propagates at $c$ , whereas attempts at a direct measurement have been thwarted by the lack of any test theory that predicts any other speed for gravity . as with the previous bogus claim by kopeikin , tang et al . seem to have made no effort to seek the involvement of anyone competent in general relativity to help with analyzing and interpreting their data . a google scholar search shows a couple of papers , amador 2008 and duif 2004 , that reference tang 's previous work on the allais effect . amador , " review on possible gravitational anomalies , " 2008 , http://arxiv.org/abs/gr-qc/0604069 duif , " a review of conventional explanations of anomalous observations during solar eclipses , " 2004 , http://arxiv.org/abs/gr-qc/0408023
only with this , as the waves have different wavelengths , i guess there can not be any interference , we will only see the difraction pattern , the two functions of the form sin2 ( x ) /x2 , with the principal maximums separated a distance d . am i right here ? sort of . the diffraction pattern is visible " at infinity " , which is in fact your case #3 . i will explain there . 1 . - in the first one , the system is configured such that the slits are far away from the lens . here , we can approximate the wave that arrives as a planar wave , and therefore the lens will perform the fourier transform in the focal plane of the screen . the diffraction of the slits also performs the fourier transform , so this configuration should lead to having only two bars of light in the screen , centered in the focus . am i right ? sort of . your lens has a limited diameter , so if you place it far from the slits , it will capture only the central portion of the diffraction pattern , i.e. the top of the sinx/x function . in other words , you will loose the fringe pattern and reconstruct slits without fringes . 2 . - the slits are in the focal plane on the lens , such that the lens is in the middle of slits-screen . here , the same thing should happen , right ? as the light comes from the focal plane , the lens must do the fourier transform with no extra things , and we should get the two bars , again both of them in the same line ( center of the screen ) . am i right here ? this will be somewhat different because you do the image of the slits at infinity , i.e. a blurry image at close distances . depending on how close is the lens , you may only be taking the " no-fringe " portion of the diffraction pattern . 3 . - the last one , i can not see . . . the lens is just behind the slits so the distance between slits and lens is 0 . that is exactly the typical school case . the diffraction pattern is , before the lens , located at infinity , or in other words , the fringes are defined as angles and not position ( $\sin\theta/\theta$ ) . the role of the lens is to bring these fringes at a finite distance ( the focal length ) . you probably learned that a lens makes an image , initially located at infinity , located at the focal length . that is the same with the diffraction fringes . now , as the two slits are both close to the lens , you will not do an image of them . that means that you should not see separated slit images , but instead , you should see two superimposed diffraction patterns , centered at the same point .
the rule is : with a diagonal metrics , the character of the coordinate $x^i$ , is given by the sign of $g_{ii}$ setting $t= \frac{u + v}{2}$ and $z= \frac{u - v}{2}$ , your metrics becomes ( with the constraint $z &gt ; 0$ ) : $$ds^2= -dt^2 + dz^2+ ( 1-t-z ) ^2dx^2+ ( 1+t+z ) ^2dy^2$$ so , the character of $t$ as a time-like coordinate and $z$ as a space-like coordinate ( with the constraint $z &gt ; 0$ ) appears clearly . and $x$ and $y$ are space-like coordinates too . the variables $u = t + z$ and $v = t - z$ are called light-cone coordinates , you could say that they are light-like coordinates . they are very often used in general relativity and string theory . they are also called null coordinates , because for instance , if $dx=dy=0$ , then you have $ds^2 = - 2dudv$ , so each particle with $u=$constant or $v=$constant corresponds to $ds^2=0$ , that is a light-like interval ( a light ray ) . but you cannot say that one of the coordinates $u , v$ is a time-like coordinate , and the other a space-like coordinate . in schwarzchild metric , it is not correct to say that $r$ and $t$ could change their sign . the schwarzchild metric describes a gravitational field only for $r&gt ; r_s$ . it is a limited description ( which does not cover the entire manifold ) , and you have to use kruskal-szekeres coordinates to have a glbal view of the black hole . mathematically , " these coordinates have the advantage that they cover the entire spacetime manifold of the maximally extended schwarzschild solution and are well-behaved everywhere outside the physical singularity . "
the details of your analysis are not quite right - that is not what the electric field of a moving charge looks like , for example . this is probably because you have not learned all the rules of electromagnetism yet . still , the spirit of your question is hitting at an important point . charges do not conserve momentum and do not obey newton 's third law . you have to include the momentum of the electromagnetic field to see conservation laws hold . there is an accessible discussion in section 8.2 of griffiths " introduction to electrodynamics " if you would like a little more math .
remember that superposition holds for the electric and magnetic fields . that is , you can calculate them individually and then add their fields together to get the field at any point . for the moving charge , $q_1$ , the magnetic field is 0 in its frame but boosting to the co-moving frame , we have $$ \mathbf{b}_{q_1}=\gamma\frac{\boldsymbol{\beta}\times\mathbf{e}}{c}=\gamma q_1\frac{\boldsymbol{\beta}\times\hat{\mathbf{r}}}{4\pi\epsilon_0cr^2} $$ where $\gamma$ is the normal lorentz factor , and $\boldsymbol{\beta}=\mathbf{v}/c$ is the ( reduced ) velocity of the particle in the lab frame ( note that the above equation reduces to the biot-savart law for $\gamma\approx1$ ) . since , in the lab frame , the magnetic field of the stationary charge , $q_2$ , is 0 , then the total magnetic field is given by $\mathbf{b}=\mathbf{b}_{q_1}$ . in the case of the co-moving frame , as stated the magnetic field is zero for $q_1$ but now the charge $q_2$ is moving ( in the opposite direction ) and we get the similar magnetic field : $$ \mathbf{b}=\mathbf{b}_{q_2}=\gamma q_2\frac{-\boldsymbol{\beta}\times\hat{\mathbf{r}}}{4\pi\epsilon_0cr^2} $$ so , yes , there is going to be a magnetic field in both frames because you have a moving charge in both frames .
i think user689 's answer is basically correct , so you should regard this as just an extension/clarification of their answer . if you place a small volume of hot tea in contact with your tongue then the large thermal capacity of your tongue will cool the tea a lot and your tongue will heat up a little . this is essentially what happens when you sip tea . you pull in a certain volume of tea , this tea is spread out over the tongue and stays in contact with it for a second or so . in this time the tea cools and your tongue heats . however the heating of the tongue is relatively small because the heat from the tea is spread over a large area . if you suck the same volume of tea through a straw , then because the cross sectional area of the straw is small the velocity of the tea in the straw is high ( as user689 points out ) so the whole volume of the tea rapidly hits , and heats , a small area on the tongue . this causes much greater heating of that small area and scalds the tongue . exactly what the sensory effects of the scald are i will leave to our medical/biological colleagues . this also suggests a reason why drinking cool fluids through a straw is pleasant . the same effect means that when drunk through a straw a cold drink causes more intense local cooling than if it was drunk without a straw .
it seems you are correct the $p$ is the momentum of the center of mass ( com ) of your $qq$ ( sorry do not know how to add bars ) system in the lab frame . due to conversation of momentum the $qq$ system may only have momentum in the x direction since that is your initial conditions . with regards to the 2nd part of your question in the cmf ( center of mass frame ) of the $qq$ system the $qq$ pair will have equal and opposite momentums ; however , in the case where the e of the initial condition is just sufficient to produce the $qq$ pair they can have no additional kinetic energy , thus they will be at rest in their cmf . again due to conservation of momentum the $qq$ com must still be moving in the x direction in the lab frame .
i think there is some interesting physics to be had here . the rate of change of temperature depends on the rate of heat flow in from the electric heating element and the rate of heat flow out as heat is lost to the air . if we write the heat capacity of the hotplate as $c$ ( $c$ is the traditional symbol for heat capacity ) then : $$ \frac{dt}{dt} = c \left ( \frac{dh_{in}}{dt} - \frac{dh_{out}}{dt} \right ) $$ where $dh_{in}/dt$ is the rate of heat flow in and $dh_{out}/dt$ is the rate of heat loss . $dh_{in}/dt$ is simply the power being supplied to the hotplate . you can measure the current the hotplate draws from the mains and calculate the power that way , or you could simply use a power meter . the power in watts , call this $w$ , is simply the energy in joules per second , so it is exactly what you need for $dh_{in}/dt$ . the rate of heat loss , $dh_{out}/dt$ is harder because it depends on how the hotplate is cooled . if the cooling is dominated by convention ( it probably is ) then the cooling will obey newton 's law of cooling and the heat loss will be given by : $$ \frac{dh_{out}}{dt} = a \space ( t - t_0 ) $$ where $t_0$ is the ambient temperature and $a$ is some constant to be determined experimentally . put all this together and you will get : $$ \frac{dt}{dt} = c \left ( w - a \space ( t - t_0 ) \right ) $$ you will need to measure $c$ and $a$ experimentally . if you have a copy of excel to hand you can use its solver to fit values of $c$ and $a$ . alternatively , you can get $c$ from the initial rate of temperature rise . when $t \approx t_0$ the heat loss is small and : $$ \frac{dt}{dt} \approx cw $$ so if you know $w$ you can calculate c . you can calculate $a$ by heating the hotplate then turning the power off and letting it cool . as it cools the temperature variation is : $$ \frac{dt}{dt} = -c a \space ( t - t_0 ) $$ so if you know $c$ you can calculate $a$ .
an electromagnetic wave with a well-defined frequency and direction , i.e. $\vec k$ , only has two possible truly physical i.e. transverse polarizations , i.e. the linearly polarized waves in the $x$ and $y$ direction ( or the two circularly polarized ones ) . that implies that a truly physical counting of polarizations gives you 2 , more generally $d-2$ in $d$ spacetime dimensions . starting from the $a_\mu$ potential fields , one component is unphysical because it is pure gauge , $a_\mu\sim\partial_\mu\lambda$ , and one of them is forbidden due to gauss ' constraint $\rm div\ , \vec d=0$ etc . that already constrains the allowed initial state of the electromagnetic field . both of these killed polarizations are ultimately linked to the $u ( 1 ) $ gauge symmetry . if one is allowed to count off-shell and unphysical fields , there may be many more components than two . but it is always possible to deduce that there are two physical polarizations at the end . for example , when we view $\vec b , \vec e$ as basic fields , there are six components , a lot . but these fields only enter maxwell 's equations through first derivatives , and not second as expected for " normal " bosonic fields , so these fields are simultaneously the canonical momenta for themselves . this brings us to three polarizations but one of them is killed by the constraints , the maxwell 's equations that do not contain time derivatives . the hertz vector is just the most famous " non-standard " example how to write the electromagnetic field as a combination of derivatives of some other fields . one must understand that the room for mathematical redefinitions etc . is unlimited and it is a matter of pure maths . all these descriptions may describe the same physics . at the end , the only " truly invariant because measurable " number of " fields " that all these approaches must agree about is the number of linearly independent physical polarizations of a wave/photon with a given $\vec k$ . if you can analyze any mathematical formulation of electromagnetism or another field theory and derive that there are $d-2$ physical polarizations ( this usually boils down to the difference of the number of a priori fields minus the number of independent constraints and the number of parameters defining identifications i.e. gauge symmetries – but the independence is sometimes hard to see and requires you to make many steps of the counting ) , then you have proved everything that is " really forced to be true " . various formalisms may offer you other ways to count the number of off-shell fields ( with different answers ) and they may be useful ( because they satisfy certain conditions or enter some laws ) but to discuss them , one has to know what the laws where they enter actually are . a truly physical approach is only one that counts the physical polarizations . the gauge symmetry is just a redundancy , a mathematical trick to get the right theory with 2 physical polarizations out of a greater number of fields with certain extra constraints or identifications . the precise number of constraints or identifications may depend on the chosen mathematical formalism and it is not a physically meaningful question – it is a question of a subjectively preferred mathematical formalism because the physics is equivalent for all of them .
why is that the case ? why does all the heat go towards its kinetic energy per unit mass ? that is my one question basically there are two places for the energy to go in the output stream : thermal energy kinetic energy the problem through ( a ) is a direct energy balance problem . in this part , we are treating $t$ as independent . if $t$ is the average of the inlet temperatures , then the velocity will be zero because all energy is accounted for , and $\delta q$ will be zero . mentally , i see a thermal cycle with the $t_1$ feeds the boiler and $t_2$ feeds the condenser . that cycle outputs useful work . the more useful work it produces , the more the average temperature of the streams is lowered . this is similar to a thermal power plant . the boiler produces more heat than the condenser rejects . typical efficiency is 33% , so the condenser removes only 2/3rd of the boiler 's heat . the rest of the heat went to turning the turbine because heat is a form of energy and energy is the ability to do work . in your case , the stream might have been accelerated by a pump that feeds into a cavity that has a nozzle where the stream is accelerated . the more work the pump does , the more the temperature of the water has to decrease because this is a closed system . practically , either the temperature change would be miniscule or the velocity would be gigantic . the reason is that thermal vibrations are fast compared to speeds we are used to . you can look at it this way in terms of your problem too ! the average kinetic energy of the molecules is the same going in and going out - it is just that some of that kinetic energy going out is from the bulk motion of the stream so we do not count it as temperature . that temperature would have to be measured by a thermometer moving along with the stream .
note that there is a huge difference between standard particles ( elementary , atoms , or molecules ) and droplets . droplets are quite macroscopic and having a definite spherical shape , they would act as nontrivial optical medium ( think about rainbows caused by water droplets ) which could selectively prefer some directions . so fog is definitely not recommended . on the other hand , if you have some gas at room temperature the scattering should be effectively classical and in all directions . so my recommendation would probably to use some non-lethal ; ) gas .
i think it could be simply that the dirac operator is invariant under isometries , so if $\phi$ is an isometry and $\psi$ a solution to $$d\psi = 0 , $$ then $\phi^* \psi$ is also a solution , where $\phi^*$is pullback . then it would be similar to how harmonic functions $f$ on the sphere -- $\nabla^2 f = 0$ -- come in representations of the rotation group , the $y^l_m$ . in more detail if $\phi$ is a diffemorphism , that in coordinates takes the form $y^\mu = y^\mu ( x^\nu ) $ ( not a tensor expression ) , and $v^\mu$ is a vector field , then we can define a vector field $$ ( \phi_* v^\mu ) ( \phi ( p ) ) = \frac{\partial y^\mu}{\partial x^\nu} v^\nu ( p ) $$ called the pushforward of $v^\mu$ . naturally we can pushforward any tensor , in particular the metric . by definition $\phi$ is an isometry if $$ ( \phi_* g_{\mu\nu} ) ( \phi ( p ) ) = g_{\mu\nu} ( \phi ( p ) ) . $$ this means that if we have any tetrad ( also known as a vierbein or a frame ) , that is a set of vector fields $e_a^\mu$ such that $e_{a\mu} e^\mu_b = \eta_{ab}$ for some symmetric matrix $\eta_{ab}$ with signature $+---$ , it is pushed forward to another tetrad . i let $\eta_{ab}$ be general because in spinor problems it is more natural to use a null tetrad $$\eta_{ab} = \begin{pmatrix} 0 and 1 and 0 and 0 \\ 1 and 0 and 0 and 0 \\ 0 and 0 and 0 and -1 \\ 0 and 0 and -1 and 0\end{pmatrix} . $$ since $\eta_{ab}$ has zeros on the diagonal all the tetrad vectors are null . it is well known ( see for example spinors and space-time or the newman-penrose paper ) that to every null tetrad corresponds exactly two bases for two-spinors , called dyads , say $ ( o^a , \iota^a ) $ and $ ( -o^a , \iota^a ) $ . thus at least for isometries connected to the identity , the pushforward of tetrads lifts to a pushforward of dyads . ( however when the isometry group is not simply connected this might not be continuous globally , but i think it does not matter here , since we can consider isometries close to the identity , which will take us to lie algebra representations , and then we integrate them , and discard the representations that require passing to the simply connected cover . ) since we can pushforward dyads we can pushforward two-spinors ( by linearity ) , since we can pushforward two-spinors we can pushforward dirac spinors . $\newcommand{\dslash}{\ ! \not d}$ in particular for a dirac spinor $\psi$ , $\dslash\psi$ is of course also a dirac spinor , so $$\beta = \phi_* ( \dslash\psi ) = \phi_* ( \dslash \phi^* \phi_* \psi ) $$ makes sense , where $\phi^*$ as the inverse of $\phi_*$ so the second equality is just inserting the identity . now $\phi_* \dslash \phi^*$ defines a differential operator , it is the transformed dirac operator under the isometry $\phi$ . but since the dirac operator is defined by the metric and $\phi$ preserves the metric , this must be just the dirac operator again . ( you can probably make this argument more convincing . ) thus we have established that $$\beta = \dslash ( \phi_*\psi ) . $$ in particular if $\beta = 0$ , so that $\psi$ is a zero mode for the dirac operator , then $\tilde{\psi} = \phi_* \psi$ is also a solution . thus the isometry group ( or at least its lie algebra ) acts on zero modes .
however , there is no reason why $\phi_i$ should be a state of the system . how is that possible ? if you perform the measurement and find result $\lambda_i$ , with zero uncertainty in the measurement then indeed the state is now $\phi_i$ . you ask how this is possible . any state is possible , not just energy eigenstates . a few ways out occur to me : the state instantanously " evolves " to a new , valid state almost . the system does indeed evolve to a new state , namely $\phi_i$ , but that evolution is not instantaneous . it is very fast in some situations , but in others it can be quite slow . concretely , consider a particle in a 1-d box , and let ω be the momentum operator . then the state can be written as an " linear combination " ( in this case an integral combination ) but no state of definite momentum is a solution . if we imagine the walls of the box , the infinite potential barriers , to be finitely wide , then if 1 ) is the case , it looks like the measurement of the momentum could cause the particle to tunnel through an infinite barrier ( it would collapse to a state with uniform position probability , and evolve to a state that is nonzero on both sides of the barrier ) . the thing is , you can not build a system which measures the momentum to arbitrarily high precision in a particle-in-a-box . the boundary conditions of the system prevent that . the infinite wall potential is mathematically pathological ( the wave functions have discontinuous derivatives at the edge of the box ) . consider a less pathological system , such as a particle in a finite height box . in this system , it is possible in principle to measure the momentum to arbitrarily high accuracy . if you measure the momentum to be $p_0$ with an accuracy of $\sigma_p$ , then the resulting wave function in the $p$ basis will be a reasonably sharp wave packet : $$\propto \exp \left [ - ( p-p_0 ) ^2 / 2 \sigma_p^2 \right ] $$ in the position basis this is $$\propto \exp \left [ -x^2 / 2\sigma_x^2 \right ] $$ where $\sigma_x = 1/\sigma_p$ . as $\sigma_x$ is very large , the wave function in the $x$ basis is a very wide function . the crucial thing here is that you never ever measure anything to infinite accuracy , so the wave functions resulting from your measurement are not exact eigen-states of what you think is the measurement operator . this is not just " experimental dirtiness " . this is a fundamentally important aspect of qm which you should keep near your mental centre as you learn more .
ok the 5 equation is completely wrong . it should be t-fs=m1a . also acceleration of m1 = acceleration of m2 since rope is inextesible
spinless non-identical particles . ground state : $ ( 0,0 ) \implies \text{non-degenerate}$ first excited state : $ ( 0,1 ) \text{ and } ( 1,0 ) \implies \text{doubly degenerate}$ spinless identical particles . ground state : $ ( 0,0 ) \implies \text{non-degenerate}$ first excited state : $ ( 0,1 ) + ( 1,0 ) \implies \text{non degenerate}$
feynman in multiple writings suggested thinking about " exchanging particles " in terms of exchanging them as they move through time . that is , they can either move in two parallel paths as they move forward , or they can cross paths ( exchange roles ) . the antisymmetric cancellation applies to the latter , but not to the former . now if you think that through , it means that the parallel path remains strong even as the crossover paths cancel out , resulting in the two particles avoiding each other and maintaining unique paths ( wave functions ) . the net result is not full cancellation , but cancellation at the edges , where the particles would cross . ( feynman goes into a lot more detail about rotations , but frankly that part can get you sidetracked a bit ; it is the " anti-crossover " part that counts in terms of actual outcomes . ) another consequence of identical fermions cancelling each other out is that packing more fermions into a tight space forces their space-filling wavelengths to become shorter also . since in quantum mechanics the spatial wavelength of a particle defines its momentum , particles that are squeezed in this fashion also get very , very hot . a neutron star is a good example . pauli exclusion -- the " constriction of space because crossover cancels but parallel does not " -- allows neutrons to pack together very densely indeed . there are limits , however . when gravity gets too monumental , even pauli exclusion is unable to keep up with the pace , and the entire star collapses , very quickly . thus is born a stellar-sized black hole , or at least this is one example of how one can form .
the van de graaff generator itself has stored potential energy , even without the extra charged object . like a capacitor , there will be energy stored in the electric field . when you turn the generator off , this energy does not vanish by itself but either dissipates slowly through corona discharge , or quickly by arc discharge . the potential energy in moving a charged object close to the generator would go through the same pathway . by bringing the object close to the generator , you will alter the electric field , increasing the electric potential on the generator . more energy will then be dissipated in any arc or corona discharge .
i doubt that it is superheating , as i understand superheating is a rather violent phenomenon . the most likely explanation is dissolved gases ; gas solubility in water decreases with temperature . the kettle boiling process is very turbulent and so can release all the dissolved gases , resulting in pure hot water . the microwaved water is heated in a very gentle way , and so the gases do not yet have a chance to escape from the surface . . . not until they can start nucleating on your tea bag .
it is simple . for people with myopia ( nearsighted ) the corrective lenses are concave . in the other case , the corrective lenses are convex . so just by looking at the type of lens you can tell the difference . if the dioptres are small , look to the edges of the lens , there you will see the difference .
assuming that the center of the planet just collapses to a black hole , people on the surface will not notice any difference in gravity at all until the ground starts to fall out from under them ( the planet will no longer be stable , as there will be no way to support the ground under your feet , ultimately ) . this is due to a result known as birchoff 's theorem that says that one can treat any spherically symmetric mass distribution as if all of its mass were concentrated at it is center .
the up quark has a charge of $+2/3$ , the down has a charge of $-1/3$ . if you have a bound state of charged particles , the total charge is just the charge of the elementary constituents . the neutron consists of one up quark and two down quarks , so the total charge $q$ is : $$q = 2/3 + 2 \times ( -1/3 ) = 0$$
yes it can be seen , given that you do not get much noise and that your sensor is sensitive enough for it to be able to detect the signal . as you have not specified anything about the light not much can be concluded , more than that it depends on the sensor , the transmitter and the noise from everything else .
both " perfectly open " ( zero acoustic impedance ) and " perfectly closed " ( infinite acoustic impedance ) boundary conditions are only idealizations that never occur in practice . for the case of the human vocal tract , they are not even very good approximations . the " bottom end " of the resonating cavity is not , in fact , the lungs , but the vocal folds ( as georg pointed out ) . this end has some acoustic impedance that is neither extremely low nor extremely high . i am sure the impedance also changes somewhat with the pitch and volume of the phonation . the " top end " of the cavity is of course the mouth , and its impedance changes with the vowel sound you are pronouncing . for aptly-named " open " vowels such as " ah " , " oh " , and so on , the impedance is actually low enough that it might be a good approximation to say it is perfectly open . but for closed vowels ( "ee " , " oo " , . . . ) and especially for humming with closed lips , the acoustic impedance is much higher ( but still far from infinite ) .
maybe an example : a particle moving in 2 dimensions has a lagrangian $$l = \frac{\dot{x}^2 +\dot{y}^2}{2} $$ so $$p_x = \frac{\partial l}{\partial \dot{x}} = \dot{x}$$ $$p_y = \frac{\partial l}{\partial \dot{y}}=\dot{y}$$ suppose it is constrained to move on a circle $x^2+y^2=r^2$ now there is a constraint between the p 's which you can get from differentiating the constraining circle , namely $$x\dot{x}+y\dot{y}=0$$ this is a constraint , but not of the type you are talking about , since the lagrangian is still regular . to obtain a lagrangian which is singular rather than regular , we require c onstraints which result in the vanishing of the hessian matrix $\frac{\partial^2l}{\partial \dot{q}_i \partial \dot{q}_j}$ . this means that the legendre transform ( sometimes called the floer map ) from the tangent bundle to the cotangent bundle ( phase space ) $$\mathcal{fl} : tq \rightarrow t^{*}q$$ given by $$ ( q_i , \dot{q}_i ) \rightarrow ( q_i , p_i=\frac{\partial l}{\partial \dot{q}_i} ) $$ is not invertible . it is image is restricted by a bunch of constraint functions . ( caveat , assuming we are restricted to a neighbourhood where rank of hessian is constant ) . for example , for the following lagrangian $$l=\frac{1}{2} ( \dot{x}^2+\dot{y}^2 ) +\dot{x}\dot{y}+4x\dot{y}+2x^2+4xy$$ the hessian determinant is easily seen to vanish . the generalized momenta are $$p_x=\dot{x}+\dot{y}$$ $$p_y=\dot{x}+\dot{y}+4x$$ you can then eliminate $\dot{x}$ and $\dot{y}$ from these relations to find your constraint equation . ( edited to provide example appropriate to the op 's question )
suppose your robot walks vertically on two legs , and you want to mount a gyroscope in the center of the robot with a vertical spin axis . as a seat-of-the-pants engineer , i would ask how much rolling moment is needed to keep the robot from falling very far before it places a foot so it will stop falling . this depends on the robot 's mass , how high it is , how far apart the legs are , and how quickly they move . when you have decided on the maximum moment , consider how much precession you can tolerate . example . suppose the gyro consists of mass $m$ concentrated in a ring of radius $r$ spinning clockwise at angular velocity $w$ ( radians/sec ) when looked at from above . suppose you can pitch the vertical axis of the gyroscope forward or backward . if you move the vertical axis forward , that will have the effect of creating a gyroscopic reaction trying to roll the axis to the right ( and vice-versa ) . if $v$ is the angular velocity at which you pitch the axis forward , the roll moment will be : $m r^2 w v$ in newton-meters . the amount by which you pitch the axis forward depends on how long you need to apply the moment . btw , you can also apply a fore-and-aft moment by rolling the axis left or right . note : anti-rolling gyro . added in response to comment : ok , so you are concerned about pitch angle . the gyro axis could be in any direction except left-to-right . in order for the gyro to work , it needs to be free to precess . in some of the ship stabilizer applications , it simply has dampers ( springs and shock absorbers ) holding the axis in place , but giving it freedom to move . here 's how i think about it . suppose in front of your segway there was a stream of water from a fire hose being sprayed from left to right . you could , holding a flat plate in your hand , put it in the water stream in such a way as to deflect the water down or up at an angle . that would provide a corrective force that you could use to balance your segway . you can work out the physics of how much force you could get as a function of the deflection angle and the amount and speed of the water . now instead of a stream of water , you have a stream of metal . instead of going in a linear stream , it is going in a circle . instead of simply being deflected downward at an angle , it is axis of rotation is changing so as to deflect the direction of the metal downward on the side where it is moving left-to-right ( and upward on the return trip ) . so you need to get an estimate of how much corrective force you want to be able to get , and how much axis-movement you want to pay for to get it . i would experiment , first trying something like a bicycle wheel , maybe filled with water , maybe put heavy metal weights around the outside . spin it up by some means ( cord , electric drill , or direct motor ) and measure the effect . you can double the effect by doubling the mass or doubling the rotation rate . you can quadruple the effect by doubling the radius .
this question is very difficult to answer , and in the end , the answer is going to be more semantic than it is going to be physcial . the reason for this is that it is very difficult to pull apart what is done by " gravity " and what is done by the matter content of the universe . the safest answer to this is to say that during inflation , the matter content of the universe has a predominantly negative pressure , which causes objects to expand . during a truly inflationary period , the density of this substance does not decrease as the universe expands , so the rate of expansion is approximately exponential . there are various ways to create a scenario like this if you assume different properties for the quantum field theoretic vacuum or various types of matter coupled to gravity .
when no current is flowing , the system is in thermal equilibrium . the electrons do transfer kinetic energy to the atoms through collisions , but the atoms also transfer kinetic energy to the electrons , and these two processes happen at the same rate , so there is no net energy transfer and the system neither heats up nor cools down . this is just the same as any other case of thermal equilibrium : effectively , the electrons and the atoms are at the same temperature , and that is why there is no heat flow . however , when you switch the voltage on there is an electric current accelerating the electrons , which increases their kinetic energy . now they have , on average , more kinetic energy to give to the atoms than the atoms have to give to them . this means that there is a net transfer of energy from the electrons to the atoms . moving an electron in an electric field changes its potential energy , and this is where the energy for the heating ultimately comes from .
yes , in some cases , for some applications , depending on your definition of " beating the diffraction limit . " in the normal sense of imaging resolution , you can not beat the diffraction limit with a normal free-space optical system . but in lithography , what you need is not really the same as resolution in the imaging sense . because you are using the light to etch a material to a certain depth , all you need is a sufficient contrast between the bright part of your focal spot and the rest of it . if that contrast is sufficient , then your etched pattern may only become deep over a region smaller than the typical spot size estimate given by $2.44 \lambda \mathcal{f}/\#$ . to illustrate , think of the focal spot due to a perfect lens system with a circular aperture . it takes the form ( ignoring scale factors and such ) of : $$ \left ( j_1 ( r ) \over r \right ) ^2$$ where $j_1$ is the bessel function of the first kind . optics people call this a " jinc " function , due to similarity to the sinc ( ) function . it is squared to give intensity . an image of this spot looks like this : $$ \left ( j_1 ( 2\pi r ) \over r \right ) ^2 $$ an annular aperture can be considered the difference of two circular apertures of slightly different size , so by superposition the field at the focus of a system with an annular aperture will be the difference of two jink functions with different sizes . such a function looks like this : $$ \left ( \frac{j_1 ( 2\pi r ) }{r} - \frac{j_1 ( 2.1\pi r ) }{r}\right ) ^2 $$ you can see that the spot is considerably more spread out in general , which would be a problem for an imaging system ; but the central bright spot is significantly smaller . if the intensity is chosen properly , the outer rings will not affect the lithographic material , while the small central lobe will . this is even more effective if the material is chosen such that it does not absorb light except for rare two-photon events . in this case the intensity dependance of the absorption goes like the square of the intensity , making the contrast between the central spot and the lobes even better . i am not sure how common this is in industry yet .
as mark wrote , your reasoning is ( almost ) correct and your friend 's is not . here 's the sort of explanation i use when i am teaching this topic : in order to apply newton 's second law , you need to choose one object to apply it to . ignore everything else except the object you choose and the forces that act on it . in your case , the problem asks about a force exerted on the bottom mass , so you should write out newton 's second law for the bottom mass . there are exactly three quantities that go into the equation : mass of the object this is given in the problem . ( typically the mass is given . ) acceleration of the object this is also given in the problem . ( in most cases , it is either given or it is what you will be solving for . ) when you are determining acceleration , only consider how the object is changing its velocity . gravity is irrelevant , the value and direction of velocity are irrelevant , whether the object is slowing down or speeding up or curving is irrelevant . the problem tells you that the elevator is accelerating at $5\ \mathrm{m/s^2}$ upward , so the only thing you need to assume is that the mass is moving along with the elevator , and that means the acceleration of the mass is $a = 5\ \mathrm{m/s^2}$ ( assuming positive values are upward ) .netforce on the object this is usually the part of newton 's law that takes a bit of thought . you have to enumerate each of the forces acting on the object and include the correct term for each one . in this case , there are two forces acting on the object ( which is the lowest mass ) : the force exerted by the spring above , and gravity . the spring force goes up and gravity goes down , so you would write the net force as $f_s - f_g$ , or $f_s - mg$ once you plug in the value of the gravitational force . note that each of these terms represents a force on the object . $f_s$ is the force that the spring exerts on the mass ; $f_g$ is the force that the earth exerts on the mass . the problem asks for the force that the spring exerts on the mass , so you do not need to invoke newton 's third law in this case . once you have all these quantities , you can put them into the equation $$\sum f = ma$$ and solve for whatever you need to .
it depends on what do you mean when you say effective . you are absolutely right when you say that the only way for your oven to cool is by diffusing and radiating its heat to your house , regardless of whether its door is open or not . so the total heat energy transferred from your oven to your room is fixed . but when the door is open , the power , or energy per unit time , is larger . the word " efficiency " is usually used to denote the amount of energy produced divided by the amount invested , and is not appropriate in this context .
i hope i understand this question properly . the text appears to focus on time loops , or closed timelike curves ( ctcs ) , while the title of the question concerns the creation of a universe from nothing . the holographic principle of black holes tells us the field theoretic information of strings on the event horizon is completely equivalent to field theoretic information in the spacetime one dimension larger outside . this physics is observed on a frame stationary with respect to the black hole . the question naturally arises ; what physics is accessed by the observer falling through the event horizon on an inertial frame ? this question is important for the black hole small enough to exhibit fluctuations comparable to its scale . a sufficiently small quantum black hole will be composed of strings in a superposition of interior and exterior configurations or states . the motion of a string onto a black hole approximates the dynamics of a string in a rindler wedge . the rindler wedge is defined by the frame of an accelerated observer , which is equivalent to the frame of a stationary observer near a black hole event horizon . the observer witnesses the final emission of radiation by the string just above the event horizon , where upon the string becomes frozen eternally on the particle horizon . of course in the rindler wedge case the string proceeds onwards on its geodesic or string world sheet with no apparent change due to this observed state of affairs . this is approximated as well with the black hole , where the string passes through the event horizon unaffected so long as the radius of curvature is much smaller than the string length . this picture persists until the string approaches the center or singularity of the black hole . at this point the rindler wedge model departs from reality . the interior perspective or the physics of a string as measured by an observer falling with the string , is outside the domain of the holographic principle . once the string passes through the event horizon it evolves on a domain of causal support not included in the data set accessible to an observer on an accelerated frame stationary with respect to the black hole . the observer that falls through the event horizon observes the further evolution of the string beyond the frozen state the stationary observer finds as its final state . further , the string evolves into a different state as it approaches the singularity . there the string will begin to experience a rapidly growing weyl curvature . the stationary observer measures transverse modes of the string on the black hole horizon , while longitudinal coordinates are compressed to near the planck length . the observer falling in with the string will witness the string distended by the growing weyl curvature in the direction of motion . consequently , this observer witnesses the extension of longitudinal extension of the string . the frozen state of the string measured by the exterior observer is cancelled by hawking radiation which escapes later . the string is entangled with the black hole , and there is a superposition of configuration spaces for the string ; the exterior and interior configuration variables . this may put a new twist on the holographic result that events in spacetime do not have the a realism as understood classically . quantum mechanics removes a measure of reality with the existence of incompatible measurements of observables which exist in incommensurate commuting sets of operators . the invariant interval , or event , is left as something which has an ontology or “realism . ” however , this indicates that a realism to any event is not supportable on fundamental grounds . further , superposition of exterior and interior states of a black hole prevents a sharp distinction between pre-selected and post-selected states . this means that cosmic censorship , chronology protection are aspects of “classical reality , ” where underneath the ontology of ordered events or their invariant meaning is lost . the creation of the universe from nothing might involves something called the biverse . the de sitter spacetime is a solution of the scale factor $a ( t ) ~=~\sqrt{\frac{3}{\lambda}}cosh ( t\sqrt{\frac{\lambda}{3}} ) $ , which has this strange backwards part of the hyperboloid . hawking and others have considered the idea of a universe connected at this minimal scale factor as two universes which are time reversed . it is possible that this is an elementary model for how a “blob” of vacuum energy in a spacetime cosmology quantum tunnels across a potential barrier to form a nascent spacetime cosmology . the blog of vacuum energy is annihilated at one side of the potential by its “opposite” ( other half of the biverse ) , where the “opposite” is annihilated at the other side of the potential by the occurrence of the vacuum blob . this is similar to the tunneling of an electron across a barrier , where we can think of it as annihilated by a positron traveling backwards in time , which in turn is connected to an electron on the other side of the potential barrier . at the end of the day one might question what are the role of the hawking-penrose energy conditions , such as the averaged weak energy condition $t^{00}~\ge~0$ . it makes sense that the gravitational states are the hartle-hawking states which are degenerate and zero . so ultimately the physical states of the universe globally are zero , or in other words the net total of everything is zero .
according to quantum mechanics , the particles - whether it is the first particle or the second particle or any other particle in the universe - refuse to have any well-defined state or property prior to the measurement . the only " kind of " exception is the case - relevant for a maximally entangled scenario - in which we are interested in a property of the second particle - or any other particle - that is predicted to take a particular value with probability equal to 100 percent . of course , if the probability is 100 percent , then you can be sure what the measured property will be , and you may assume that this value of the property exists even before the measurement . however , for the very same particle that has some value of the quantity equal to something at 100 percent , there still inevitably exist other observables that are not known . ( just design a hermitian observable with random off-diagonal matrix elements . ) in the basis of eigenstates of those other properties , the probability amplitudes are generic , and some of the options have probabilities that differ from 0 percent as well as 100 percent . for those quantities - and it is a majority of observables - the usual prescriptions of quantum mechanics hold : the value is not determined prior to the measurement . it is not just unknown to the physicists : it is unknown to nature . a picture in which those observables are determined leads to wrong predictions and contradictions .
here we go : lets considere the lagrangian density of our free membrane . for convenience , we will use a cartesian set of coordinates , so that : $$\mathcal{l}=\frac{\rho}{2}\dot{h}-\frac{\kappa}{2}\left [ {\partial_x}^2 h+{\partial_y}^2 h\right ] ^2$$ apparently , this lagrangian depends on a quite large number of variables : $$\mathcal{l} ( x , y , t , h , \dot{h} , \partial^2_x h , \partial^2_y h ) $$ but still nothing to be afraid of ! : ) one can show ( see this ) that for this type of lagrangian density , euler-lagrange equation is : $$\sum_{\alpha\beta}\frac{\partial}{\partial\alpha}\left ( \frac{\partial\mathcal{l}}{\partial ( \partial_\alpha h ) }\right ) -\frac{\partial^2}{\partial\alpha\partial\beta}\left ( \frac{\partial\mathcal{l}}{\partial ( \partial^2_{\alpha\beta} h ) }\right ) =\frac{\partial\mathcal{l}}{\partial h}$$ where $ ( \alpha , \beta ) \in\{x , y , t\}$ computing this equation for our membrane lagrangian , it follows the differential equation : $$\rho\ddot{h}+\kappa\left ( \partial^4_x h + \partial^4_y h\right ) =0$$ then , simply by taking the ( space-time ) fourier transform $\tilde{h}$ of $h$ so that : $$h ( \vec{r} , t ) =\sum_\vec{q}\int\frac{d\omega}{2\pi} e^{i ( \vec{q}\cdot\vec{r}-\omega t ) }\tilde{h} ( \vec{q} , \omega ) $$ where $\vec{r}= ( x , y ) $ , and replacing it in the previous equation , you obtain the dispersion relation : $$\omega^2=\frac{\kappa q^4}{\rho}$$ for a quantum treatment , it feels like you will have to compute the associated hamiltonian $\mathcal{h}$ of $\mathcal{l}$ and then quantized it , using the correspondence $\{\cdot\}\rightarrow\frac{1}{i\hbar}\left [ \cdot\right ] $ between poisson brackets and commutators .
when we say that the spin of a silver atom ( in a magnetic field ) is $+1/2$ or $-1/2$ we mean its component in the direction of the magnetic field ( referred to as $s_z$ ) is $+1/2$ or $-1/2$ . the magnitude of the spin is the same in both cases , it is just the direction that is different . hund 's rule just tells you what the magnitude of the total spin is , and does not say anything about the direction that spin is pointing . for example with two unpaired electrons hund 's rule tells us the total spin will be $s = 1$ . however put that atom in a magnetic field and you can have $s_z = 1$ , $0$ or $-1$ . the two blobs in the stern-gerlach experiment correspond to the electrons with $s_z = +1/2$ and $-1/2$ , but all the electrons have the same total spin $s = 1/2$ . the two unpaired electron system with $s = 1$ would give us three blobs corresponding to $s_z = 1$ , $0$ and $-1$ .
whether the conformal symmetry is local or global depends on the theory ! more precisely , the symmetry that may be local is not really conformal symmetry but ${\rm diff}\times {\rm weyl}$ . for example , in all the cfts we use in the ads/cft correspondence , for example the famous ${\mathcal n}=4$ gauge theory in $d=4$ , the conformal symmetry is global – and , correspondingly , it is a physical symmetry with nonzero values of generators . this is related to the fact that the cft side of the holographic duality is a non-gravitational theory so it avoids all local symmetries related to spacetime geometry . the previous paragraph holds even if the dimension of the cft world volume is $d=2$ . in $d=2$ , it may happen that the global conformal symmetry is extended to the infinite-dimensional local symmetry where $\omega ( x ) $ depends on the location . however , such an enhancement looks " automatic " only classically . quantum mechanically , a nonzero central charge $c\neq 0$ prevents one from defining the general local conformal transformations . in all the cfts from ads/cft , we have $c\geq 0$ . such a nonzero $c$ leads to the " conformal anomaly " ( proportional to the world sheet ricci scalar and $c$ ) . on the contrary , the world sheet $d=2$ cft theories used to describe perturbative string theory always have a local diffeomorphism and local weyl symmetry . this is needed to decouple all the unphysical components of the world sheet metric tensor ; and a necessary condition is the incorporation of the conformal ( and other ) ghosts so that in the critical dimension , we have the necessary $c=0$ . we say that the world sheet cft is " coupled to gravity " as we add the world sheet metric tensor , the diff symmetry , and the weyl symmetry . the weyl symmetry is the symmetry under a general scaling of the world sheet metric by $\omega ( x ) $ that depends on the location on the world sheet . one may gauge-fix this local weyl symmetry along with the 2-dimensional diffeomorphism symmetry , e.g. by demanding the $\delta_{ij}$ form of the metric tensor . this gauge-fixing still preserves some residual symmetry , a subgroup of the originally infinite-dimensional " diff times weyl " symmetry . this residual symmetry is nothing else than the infinite-dimensional conformal symmetry generated by $l_n$ and $\tilde l_n$ . because its being infinite-dimensional , we may call it a local conformal symmetry but it is really just a residual symmetry from " diff times weyl " . the global $sl ( 2 , c ) \sim so ( 3,1 ) $ global subgroup is the mobius group generated by $l_{0 , \pm 1}$ and those with tildes , too . as far as i know , this local conformal symmetry is a special case of some $d=2$ theories . in higher dimensions , the weyl and diff are not enough to kill all the components of the metric tensor and the " partially killed " theories with a dynamical metric are still inconsistent as the usual naively quantized versions of general relativity . in all the cases above and others , it is true that the local symmetries – where the parameter $\omega ( x ) $ is allowed to depend on time and space coordinates ( if the latter exist ) – are gauge symmetries ( in the sense that the generators are obliged to annihilate physical states ) while the global symmetries are always " physical " in your sense of the charge 's being nonzero . these equivalences follow from some easy logical argument . when you have infinitely many generators of the ( space ) time-dependent symmetry transformations , it follows that all the quanta associated with these generators exactly decouple – have vanishing interactions – with the gauge-invariant degrees of freedom . so we always study the physical part of the theory only , and it is the theory composed of the gauge symmetry 's singlets . greetings to david .
from wikipedia : the number 5 is a relic of old notation in which $\gamma^0$ was called "$\gamma^4$" .
by definition 1 n = 1 kg m/s^2 to remember this , remember f = ma . the left hand side has units of force . the right hand side is kg * ( m/s^2 ) . dividing both sides by m^2 , we get 1 n/m^2 = 1 kg / ( m s^2 )
the first thing you need to get to grips with is that particles are waves . this can be shown with a simple experiment called the double slit experiment , which i will attempt to explain . imagine a water wave travelling across a tank . then imagine you place a wall in the middle of the tank , and place two thin slits in it . if you create a wave ( by dropping a stone etc ) on one side of the wall , it will travel through the two slits and interfere like this . the double slit experiment does the same thing , but for light . if you have a wall with two slits in it and shine a beam of light through the slits onto a flat screen behind , you can see a similar interference pattern on the screen . this shows that light acts as a wave . now imagine that rather than a beam of light you can create a steady stream of electrons . electrons are a small " fundamental " particle ( "fundamental " means they cannot be broken down into smaller components ) . if you point your electron stream at your two slits you will see a very similar interference pattern as before ! until this experiment was done it was believed that electrons were solid particles ( like billiard balls ) , but this showed that they also act as a wave ! since we have shown that particles can also show wave-like properties , can we show that waves can have particle-like properties ? it was shown by einstein and arthur compton that light can in fact be shown to be made up of particles , due to the fact that light must have momentum . this is known as wave-particle duality . as i said at the beginning , waves and particles are the same thing . there are some " waves " like electromagnetic waves which make particles move . these are only called " waves " because it is easier to model and calculate that way . it is possible to describe the interaction as 2 ( or more ) particles ( but it is considerably more difficult ) . i hope this answers your question .
yes . special relativity satisfies your question somehow . probably , there are many discussions in se based on the topic . but , there are several cons i would like to correct in your question . . . if movement occurs faster , time freezes . more the movement , more the time freezes . according to einstein - length , mass , time and space are interdependent variables . these motions depend on the speed of light $c$ in vacuum which is the only constant here and is what the second postulate says . ok , let 's take your " time " part . . . time does not freeze out anytime . it may freeze if you move at $c$ which is impossible ( practically and theoretically ) to achieve in vacuum . and , you say , " more the movement , more it freezes " . it is not a state of matter . and , no laws have ever proved that " time depends upon temperature " . but , time really slows down when you approach a good velocity comparable to $c$ . time dilation could be predicted indeed ( taking lorentz factor into account ) using $$t=\frac{t_0}{\sqrt{1-\frac{v^2}{c^2}}}$$ where $t_0$ is the time measured by an observer at its rest frame . $t$ is the apparent time measured which is always less than $t_0$ and it reduces with increasing velocity . ( freezes more - in your words ) how come then theory of relativity claims that " moving faster may transport you ahead in time " ? short answer - no , there is no such thing in relativity . but , its the postulates that made the physicists think about the plausibility of time travel . these guys thought it in a different way . if there is a possibility for objects to travel faster than light ( assuming hypothetical particles called tachyons to study the behaviour ) , maybe it would forward time into future ( like that ) . . . but , i think it is unsuccessful . . !
in fluids , conservation of mass and momentum are still applicable , only not that easily , because there are internal forces in a fluid , i.e. caused by viscosity . in liquids , conservation of mass is described by the continuity equation , while the conservation of momentum is described by the navier-stokes equations of course , you can incorporate the effect the solid has in these equation , but this easier said than done , and large depends on the properties of the solid ,
the notation $t_{n1}$ may come from the fact that the mean is also referred to as the " first moment " , this is what the number $1$ as a lower label might stand for . for reference , see the wikipedia article on moments in mathematics . regarding the derivative of the energy : you are supposed to formally take the derivative of some expression with respect to some variable , even if it is a discrete one .
the form of your book 's value $\frac{\hbar^2}{2ma^2}$ makes it clear that that is a fully classical ( non-relativistic ) limit ( $\frac{p^2}{2m}$ , right ? ) , while yours is a fully relativistic one ( after all $e , p \gg m_e$ ) . the argument about the nuclear confinement is simply that both the gravitational and elctromagnetic potentials on the electron due to the nucleus are many orders of magnitude smaller than 40 mev ( the weak nuclear force too , but you may not know how to compute this ) , and the electron is not affected by the strong nuclear force .
no , because all the off-rectilinear parts are destructively interfered away . huygen 's principle is often stated differently than how huygens stated it . he said that if you draw a sphere at every point of the wavefront , the future wavefront is the envelope of the spheres so drawn , and only one of the envelopes ( the one that keeps going forward ) . so from your sphere , you are supposed to imagine drawing a bunch of other spheres , and then looking at the outermost limit of where they reach . this is a larger sphere , of radius ct , and this is the new wavefront . the modern version of huygen 's principle ( i learned from this site ) is the fact that the green 's function of the wave equation in 3+1 dimension is a delta-function on the light cone . this is a way of saying that the full propagation of the wave is by superposing the circles from propagating each point on the previous front . the superposition though is cancelling away from the region where the nearby circles have a mutual tangent , and this is the envelope criterion .
you may just not bother to use a test function , here . this problem is so easy you can work it all just using the properties of the commutator . $$ [ xp_y , x ] =x [ p_y , x ] + [ x , x ] p_y$$ now $ [ p_y , x ] $ vanishes because of the fundamental commutation relation between $p_i$ and $x_i$ which is $$ [ p_i , x_j ] = -i\hbar \delta_{ij}$$ on the other hand $ [ x , x ] =0$ because anything commmutes with itself .
assuming 1 , you live somewhere that is colder outside than in 2 , the curtain has finite thermal resistance ( ie some insulating value ) 3 , the curtain is close enough to the window to reduce convection then yes . try measuring the air temperature on the window side of the curtain , it should be lower than the room .
i am happy to know that you are interested in space and the nature of time , your questions indicate that you dont have a very clear understanding of what is already understood about the subject , but show an interested student and potentially a good one . people talk about the expansion of the universe and black holes and all these things , but to talk about them correctly you need to learn how to talk about them . i would highly recommend einstein 's paper , " on the electrodynamics of moving bodies " . ( available online ) please look into it , if you have questions post them on this forum . saying that " is time the rate at which one moves through space " is not correct . one definition that can serves our purposes is that time is what is measured by clocks . to understand the behaviour of moving clocks in relation to stationary clocks , you have to understand this concept called the proper time . the concept of proper time is a generalization of the distance between 2 points you learn in high school . implication of the theory of relativity is that clocks measure proper time . definition of proper time is $$tp = ( c^2 dt^2-dx^2 ) ^{1/2}$$ where dt is the time elapsed and dx is the distance moved . ( it must be summed by dividing path into small segments but it does not matter if you move with constant velocity ) . now we know for something moving at the speed of light $$x=ct . $$ implies that $$tp = ( c^2t^2 - c^2 t^2 ) ^{1/2} = 0$$ which is why clocks moving at the speed of light dont show the movement of time . another way to understand this , is a clock moving at the speed of light shows the same time when you see it .
first , $\vec{r}^\prime$ is a vector that goes from the origin to the source of charge . if the source is a volumetric distribution , one must sum all contributions of charge , that is why one integrates over all the volume , say $\mathcal{v}$ ; the ( correct ) expression for the potential should be $$v ( \vec{r} ) = \frac{1}{4 \pi \epsilon _{0}} \int_\mathcal{v} \frac{\rho ( \vec{r}^\prime ) }{ℛ} d\mathcal{v}^\prime$$ so that all dependence of $v$ remains on $\vec{r}$ . then , $r^\prime$ is just the magnitude $|\vec{r}^\prime|$ , being the distance from the origin to the source of charge . second , usually , the series expansion of a function $f ( x ) $ about some point $x_0$ is useful because if you want to know the value of $f$ near $x_0$ , you may just take some few terms of the expansion ; it is as seeing the plot of $f$ with a magnifying glass . you should remember this from your first calculus courses , it is done a lot in physics . here the expansion about $\epsilon=0$ will be useful since $\epsilon\to0$ implies $r\to\infty$ ( just really big , if you will ) . the ( correct ) expression $$v ( \vec{r} ) = \frac{1}{4 \pi \epsilon _{0}} \sum ^{\infty}_{n=0}\frac{1}{r^{n+1}} \int ( r' ) ^n\ , p_{n} ( \cos \theta^\prime ) \ , \rho ( \vec{r}' ) \ , d\mathcal{v}'$$ is just another way of writing the series expansion in terms of $r$ , $r^\prime$ and $\theta^\prime$ , where $p_n$ are the legendre polynomials ( griffiths defines them there , ain't he ? ) . this expression is useful , as it means , explicitly , that $$v ( \vec{r} ) =\frac{1}{4\pi\epsilon_0}\left [ \frac{1}{r}\int\rho ( \vec{r}' ) \ , d\mathcal{v}'+\frac{1}{r^2}\int{r'}\cos\theta'\ , \rho ( \vec{r}' ) \ , d\mathcal{v}'+\frac{1}{r^3}\left ( \cdots\right ) +\ldots\right ] $$ so that if you want to evaluate the potential for points far from the source ( big $r$ ) , then you may just neglect higher order terms in $r$ and just take the $1/r$ ( monopole ) term ; and so on if you are considering a better approximation , you may take the $1/r^2$ ( dipole ) term , etc . . . that is the real usefulness of the series expansion ; in a lot of situations evaluating $v ( \vec{r} ) = \frac{1}{4 \pi \epsilon _{0}} \int \frac{\rho ( \vec{r}' ) }{ℛ} d\mathcal{v}'$ will get really ugly , and then , mostly , is when the multipole approximation will be useful .
there is a mistake in your diagram , in that you drew the x-axis for body one in the rest frame of body 3 as perpendicular in the euclidean sense to the t-axis of body one . the two lines are not euclidean perpendicular , but minkowski perpendicular . the t axis is correct for body 1 in the second diagram , but the x-axis for body 1 slopes up , not down . it slopes up by the same amount as the t-axis for body 1 slopes to the right . this looks awkward in a euclidean geometry diagram , but it is correct minkowski geometry . the direction of the slope is fixed by einstein 's argument about simultaneity at a distance , reproduced in this answer : einstein&#39 ; s postulates &lt ; ==&gt ; minkowski space . ( in layman&#39 ; s terms ) . this argument also fixes the relative non-simultaneity of e1 and e2 . when you use the correct notion of perpendicularity , you will find that t ( e2 ) is bigger than t ( e1 ) , not smaller . but they are indeed non-simultaneous , as you note .
the energy obviously grows with the height $z$ ( assuming that positive means up and negative means down ) so it is $$ e = \frac{p^2}{2m} + mgz $$ this sign error spread everywhere .
in theories with spontaneous symmetry breaking , the phase transition can usually be characterized by a local order parameter $\delta ( x ) $ , which is not invariant under the relevant symmetry group $g$ of the hamiltonian . the expectation value of this field has to be zero outside the ordered phase $\langle\delta ( x ) \rangle = 0$ , but non-zero in the phase $\langle\delta ( x ) \rangle \neq 0$ . this shows that there has been a spontaneous breaking of $g$ to a subgroup $h\subset g$ ( where $h$ is the subgroup that leaves $\delta ( x ) $ invariant ) . what local means in this context , is usually that $\delta ( x ) $ at point $x$ , can be constructed by looking at a small neighborhood around the point $x$ . here $\delta ( x ) $ can be dependent on $x$ and need not be homogeneous . this happens for example when you have topological defects , such as vortices or hedgehogs . one powerful feature of these landau-type phases , is that there will generically be gapless excitations in the system corresponding to fluctuations of $\delta ( x ) $ around its expectation value $\langle\delta ( x ) \rangle$ in the direction where the symmetry is not broken ( unless there is a higgs mechanism ) . these are called goldstone modes and their dynamics are described by a non-linear $\sigma$-model with target manifold $g/h$ . an example is the order parameter for s-wave superconductors $\langle\delta ( x ) \rangle = \langle c_{\uparrow} ( x ) c_{\downarrow} ( x ) \rangle$ , which breaks a $u ( 1 ) $ symmetry down to $\mathbb z_2$ . but there are no goldstone modes due to the higgs mechanism , the massive amplitude fluctuations are however there ( the " higgs boson" ) . [ edit : see edit2 for correction . ] a non-local order parameter does not depend on $x$ ( which is local ) , but on something non-local . for example , a non-local ( gauge-invariant ) object in gauge theories are the wilson loops $w_r [ \mathcal c ] = \text{tr}_r{\left ( \mathcal pe^{i\oint_{\mathcal c}a_\mu\text dx^\mu}\right ) } , $ where $\mathcal c$ is some closed curve . the wilson loop thus depends on the whole loop $\mathcal c$ ( and a representation $r$ of the gauge group ) and cannot be constructed locally . it can also contain global information if $\mathcal c$ is a non-trivial cycle ( non-contractible ) . it is true that topological order cannot be described by a local order parameter , as in superconductors or magnets , but conversely a system described by a non-local order parameter does not mean it has topological order ( i think ) . the above mentioned wilson loops ( and similar order parameters , such a the polyakov and ' t hooft loop ) , is actually a order parameter in gauge theories which probe the spontaneous breaking of a certain center-symmetry . this characterizes the deconfinement/confinement transition of quarks in qcd : in the deconfined phase $w_r [ \mathcal c ] $ satisfies a perimeter law and quarks interact with a massive/yukawa type potential $v ( r ) \sim \frac{e^{-mr}}r$ , while in the confined phase it satisfy an area law and the potential is linear $v ( r ) \sim \sigma r$ ( $\sigma$ is some string tension ) . there might be other examples of spontaneous symmetry breaking phases with non-local order parameter . [ edit : see edit2 . ] let me just make a few comments about topological order . in theories with with spontanous symmetry breaking , long-range correlations are very important . in topological order the systems are gapped by definition , and there is only short-range correlation . the main point is that in topological order , entanglement plays the important role not correlations . one can define the notion of long-range entanglement ( lre ) and short-range entanglement ( sre ) . given a state $\psi$ in the hilbert space , loosely speaking $\psi$ is sre if it can de deformed to a product state ( zero entanglement entropy ) by locally removing entanglement , if this is not possible then $\psi$ is lre . a system which has a ground state with lre is called topological order , otherwise its called the trivial phase . these phases have many characteristic features which are generally non-local/global in nature such as , anyonic excitations/non-zero entanglement entropy , low-energy tqft 's , and are characterized by so-called modular $s$ and $t$ matrices ( projective representations of the modular group $sl ( 2 , \mathbb z ) $ ) . note that , unlike popular belief , topological insulators and superconductors are sre and are not examples of topological order ! if one requires that the system must preserve some symmetry $g$ , then not all sre states can be deformed to the product state while respecting $g$ . this means that sre states can have non-trivial topological phases which are protected by the symmetry $g$ . these are called symmetry protected topological states ( spt ) . topological insulators/superconductors are a very small subset of spt states , corresponding to restricting to free fermionic systems . unlike systems with lre and thus intrinsic topological order , spt states are only protected as long as the symmetry is not broken . these systems typically have interesting boundary physics , such as gapless modes or gapped topological order on the boundary . characterizing them usually requires global quantities too and cannot be done by local order parameters . edit : this is a response to the question in the comment section . i am not sure whether there are any reference which discuss this point explicitly . but the point is that you can continuously deform/perturb the hamiltonian of a topological insulator ( while preserving the gap ) into the trivial insulator by breaking the symmetry along the way ( they are only protected if the symmetry is respected ) . this is equivalent to locally deforming the ground state into the product state , which is the definition of short range entanglement . you can find the statement in many papers and talks . see for example the first few slides here . or even better , see this ( slide with title " compare topological order and topological insulator " + the final slide ) . let me make another comment regarding the distinction between intrinsic topological order and topological superconductors , which at first seems puzzling and contrary to what i just said . as was shown by levin-wen and kitaev-preskill , the entanglement entropy of ground state for a gapped system in 2+1d has the form $s = \alpha a - \gamma + \mathcal o ( \tfrac 1a ) $ , where $a$ is the boundary area ( this is called the area law , not the same area law i mentioned in the case of confinement ) , $\alpha$ is a non-universal number and $\gamma$ is universal and called the topological entanglement entropy ( tee ) . what was shown in the above papers is that the tee is equal to $\gamma = \log\mathcal d$ , where $\mathcal d\geq 1$ is the total quantum dimension and is only strictly $\mathcal d&gt ; 1$ ( $\gamma\neq 0$ ) if the system supports anyonic excitations . modulo some subtleties , lre states always have $\gamma\neq 0$ , which in turn means that they have anyonic excitations . conversely for sre states $\gamma = 0$ and there are no anyons present . this seems to be at odds with the existence of ' majorana fermions ' ( non-abelian anyons ) in topological superconductors . the difference is that , in the case of topological order you have intrinsic finite-energy excitations which are anyonic and the anyons correspond to linear representations of the braid group . while in the case of topological superconductors , you only have non-abelian anyons if there is an extrinsic defect ( vortex , domain wall etc . ) which the zero-modes can bind to , and they correspond to projective representation of the braid group . the latter type anyons from extrinsic defects can also exist in topological order , but intrinsic finite-energy ones only exist in topological order . for more details , see the recent set of papers from barkeshli , jian and qi . edit2: please see my comments below for some corrections and subtleties . such as , it is in a sense not correct that superconductors are described by a local order parameter . it only appears local in a particular gauge . superconductors are actually examples of topological order , which is rather surprising .
the plane of the earth 's orbit is extremely stable . of course , the earth 's orbit is affected by the other planets , especially jupiter , but all the planets orbit in approximately the same plane , so the forces pulling the earth 's orbit out of its plane are small . we can see that the planes of the planets ' orbit are stable , because all the planets are in roughly the same plane after 4.5 billion years , and it is approximately the same plane as the sun 's rotation , so it was determined by the angular momentum of the cloud from which the entire solar system formed . a big change in the orbital plane of a planet could be caused by a close encounter with jupiter . this may have happened early on - indeed planets may have been ejected from the solar system altogether . will this change in orbital plane have any visual or physical effect for an observer on the earth ? let 's imagine that some slow and non-catastrophic process causes the earth 's orbital plane to rotate by 90º . we need to ask : what happens to the earth 's rotation axis ? if it stays the same ( and why would it change ? ) , then the effects would be dramatic , as the axis is now approximately in the plane of the orbit , pointing almost directly towards the sun twice a year . the whole of northern hemisphere would be in daylight for several months in the " summer " , then in darkness during the " winter " .
the effective gravity inside the iss is very close to zero , because the station is in free fall . the effective gravity is a combination of gravity and acceleration . if you are standing on the surface of the earth , you feel gravity ( 1g , 9.8 m/s 2 ) because you are not in free fall . your feet press down against the ground , and the ground presses up against your feet . inside the iss , there is a downward gravitational pull of about 0.89g , but the station itself is simultaneously accelerating downward at 0.89g -- because of the gravitational pull . everyone and everything inside the station experiences the same gravity and acceleration , and the sum is close to zero . imagine taking the iss and putting it a mile above the earth 's surface . it would experience about the same 1.0g gravity you have standing on the surface , but in addition the station would accelerate downward at 1.0g . again , you will have free fall inside the station , since everything inside it experiences the same gravity and acceleration ( at least until it hits the ground ) . the big difference , of course , is that the iss never hits the ground . its horizontal speed means that by the time it is fallen , say , 1 meter , the ground is 1 meter farther down , because the earth 's surface is curved . in effect , the station is perpetually falling , but never getting any closer to the ground . that is what an orbit is . ( as douglas adams said , the secret of flying is to throw yourself at the ground and miss . ) but it is not quite that simple . there is still a little bit of atmosphere even at the height at which the iss orbits , and that causes some drag . every now and then they have to re-boost the station , using rockets . during a re-boost , the station is not in free fall ; instead . the result is , in effect , a very small " gravitational " pull inside the station -- which you can see in this fascinating video .
first , it is important to properly understand the equations of rotational motion . rather than $f = ma , $ the operative equation of motion is $\tau = i \alpha$ , where $\tau$ is the torque , $i$ is the moment of inertia and $\alpha$ is the angular acceleration . this problem also requires you to know the definition of the radius of gyration in the form of $r_g \equiv \sqrt\frac{i}{m}$ . with a proper understanding of the definitions of $\tau$ and $\alpha$ , you then have all the information that you need to solve the problem . edit : the above answer referred solely to the rotational acceleration of the disk . the translational acceleration of the center of mass of the disk must be worked out separately using $f = ma$ .
you are very nearly there . you are correct to say that $power = fv$ , and that the velocity at the moment the train reaches the top of the slope is given by $v_{top} = 350/ ( 6 + 150gsin2 ) $ so the force at the top of the slope is $f_{top} = 1000 ( 6 + 150gsin2 ) $ . but the acceleration is the net force divided by the mass , and the net force is $f_{top}$ minus the 6kn frictional force i.e. $$ a = \frac{f_{top} - 6000}{m} = \frac {1000 ( 6 + 150gsin2 ) - 6000}{150000} = g sin2$$ which using $g = 9.81m/sec^2$ i get as 0.342 .
the most general relationship is $$c ( b ) = \frac{\int_0^b \frac{\mathrm{d}\sigma}{\mathrm{d}b}\mathrm{d}b}{\int_0^\infty \frac{\mathrm{d}\sigma}{\mathrm{d}b}\mathrm{d}b} = \frac{1}{\sigma_\text{inel}}\int_0^b \frac{\mathrm{d}\sigma}{\mathrm{d}b}\mathrm{d}b\tag{1}$$ ( source , one of many ) . in practice , we usually use the glauber model to describe heavy ion collisions , and this model predicts an impact parameter dependence of the differential cross section which can be ( very roughly ) approximated as $$\frac{\mathrm{d}\sigma}{\mathrm{d}b} \approx \begin{cases}2\pi b , and b \le b_\text{max} \\ 0 , and b &gt ; b_\text{max}\end{cases}$$ where $\pi b_\text{max}^2 = \sigma_\text{inel}$ . that reduces equation ( 1 ) to $$c ( b ) = \frac{\pi b^2}{\sigma_\text{inel}}$$ for $b &lt ; b_\text{max}$ . you do have to be careful because sometimes ( rarely ) a different definition is used , $c ( b ) = 1 - \pi b^2/\sigma_\text{inel}$ . just pay attention to whether large centrality values correspond to peripheral ( the former definition ) or central ( the latter ) collisions . in practice , this is all somewhat approximate anyway , because you can not definitively identify the centrality of a collision from the information collected by a detector . all you can do is estimate the centrality based on how many particles come out and how strongly they are scattered . if you get a lot of particles coming out roughly perpendicular to the beamline ( pseudorapidity $\eta\sim 0$ ) , then that means a lot of nucleons were involved in the collision , and thus it is characterized as central . if there are few particles coming out perpendicular to the beamline , then few nucleons were scattered , meaning the collision was peripheral .
one of the main ways black holes are noticed is by looking at a solar system where the star appears to move as though it were a binary star system ( e . i . two stars ) when only one is seen . in these situations , depending on the distances , the black hole " feeds " off the original star , and a stream of the stellar plasma is slowly pealed off the star into the black hole . this matter can sometimes form a very vivid accretion disk , that can be observed using telescopes ( see herbig–haro object ) . this process can take a very long time , on the order of millions of years . however , of course , a rough black hole could enter a star system head on and collide right with the sun and " suck it up , " which would happen rather quickly ( to an outside observer ) .
bohr realized that the weight of the device is made by the displacement of a scale in spacetime . the clock’s new position in the gravity field of the earth , or any other mass , will change the clock rate by gravitational time dilation as measured from some distant point the experimenter is located . the temporal metric term for a spherical gravity field is $1~-~2gm/rc^2$ , where a displacement by some $\delta r$ means the change in the metric term is $\simeq~ ( gm/c^2r^2 ) \delta r$ . hence the clock’s time intervals $t$ is measured to change by a factor $$ t~\rightarrow~t\sqrt{ ( 1~-~2gm/c^2 ) \delta r/r^2}~\simeq~t ( 1~-~gm\delta r/r^2c^2 ) , $$ so the clock appears to tick slower . this changes the time span the clock keeps the door on the box open to release a photon . assume that the uncertainty in the momentum is given by the $\delta p~\simeq~\hbar/\delta r~&lt ; ~tg\delta m$ , where $g~=~gm/r^2$ . similarly the uncertainty in time is found as $\delta t~=~ ( tg/c^2 ) \delta r$ . from this $\delta t~&gt ; ~\hbar/\delta mc^2$ is obtained and the heisenberg uncertainty relation $\delta t\delta e~&gt ; ~\hbar$ . this demands a fourier transformation between position and momentum , as well as time and energy . this argument by bohr is one of those things which i find myself re-reading . this argument by bohr is in my opinion on of these spectacular brilliant events in physics . this holds in some part to the quantum level with gravity , even if we do not fully understand quantum gravity . consider the clock in einstein’s box as a blackhole with mass $m$ . the quantum periodicity of this blackhole is given by some multiple of planck masses . for a blackhole of integer number $n$ of planck masses the time it takes a photon to travel across the event horizon is $t~\sim~gm/c^3$ $=~nt_p$ , which are considered as the time intervals of the clock . the uncertainty in time the door to the box remains open is $$ \delta t~\simeq~tg/c ( \delta r~-~gm/c^2 ) , $$ as measured by a distant observer . similary the change in the energy is given by $e_2/e_1~=$ $\sqrt{ ( 1~-~2m/r_1 ) / ( 1~-~2m/r_2 ) }$ , which gives an energy uncertainty of $$ \delta e~\simeq~ ( \hbar/t_1 ) g/c^2 ( \delta r~-~gm/c^2 ) ^{-1} . $$ consequently the heisenberg uncertainty principle still holds $\delta e\delta t~\simeq~\hbar$ . thus general relativity beyond the newtonian limit preserves the heisenberg uncertainty principle . it is interesting to note in the newtonian limit this leads to a spread of frequencies $\delta\omega~\simeq~\sqrt{c^5/g\hbar}$ , which is the planck frequency . the uncertainty in the $\delta e~\simeq~\hbar/\delta t$ does have a funny situation , where if the energy is $\delta e$ is larger than the planck mass there is the occurrence of an event horizon . the horizon has a radius $r~\simeq~2g\delta e/c^4$ , which is the uncertainty in the radial position $r~=~\delta r$ associated with the energy fluctuation . putting this together with the planckian uncertainty in the einstein box we then have $$ \delta r\delta t~\simeq~\frac{2g\hbar}{c^4}~=~{\ell}^2_{planck}/c . $$ so this argument can be pushed to understand the nature of noncommutative coordinates in quantum gravity .
the simple answer is that the chemical properties of oil and water result in them having smaller coefficients of friction than other surfaces like concrete . the frictional force exerted on a skidding body is given by $f=\mu_k n$ where $n=mg$ is the normal force of the body , and $\mu_k$ is the coefficient of kinetic friction . assuming the same object skids on both concrete and water , $n$ will be constant while $\mu_k$ will be smaller for water , thus resulting in a smaller frictional force ( allowing the object to skid further on water ) . now you may be wondering why $\mu_k$ is smaller for water than concrete . kinetic friction is primarily caused by chemical bonding between a surface and the object skidding on that surface . in classical mechanics however , it is not necessary to understand the chemical properties of the surfaces being studied . it is just accepted that some surfaces bond stronger than others , with $\mu_k$ being measured for different materials by experiment .
the problem is that the magnetic field around a bar magnet are not uniform , so different parts of your coil will experience different values of $b$ . if the magnetic is small enough , you can approximate it is field by a dipole , $\mathbf{b} ( {\mathbf{r}} ) =\frac{\mu_{0}}{4\pi}\left ( \frac{3\mathbf{r} ( \mathbf{m}\cdot\mathbf{r} ) }{r^{5}}-\frac{{\mathbf{m}}}{r^{3}}\right ) $ ( here $\mathbf{m}$ is a constant vector equal to the dipole moment ) . then , supposing your coil is very flat ( not really a solenoid ) , you can calculate what $\frac{\partial\mathbf{b}}{\partial t}$ is when $\mathbf{m} \cdot \mathbf{r} = 0$ , i.e. , at the moment the magnet passes through the coil like a basketball falling through a hoop . of course , $\mathbf{m}\cdot \dot{ \mathbf{r}} \neq 0$ . the peak emf should then be somewhat close to what you would predict from experiment . a lot of early experiments had to be really clever to isolate exactly what was going on . it really depends on what apparatuses are available to you .
i do not think this is possible . suppose that your unaided eye can focus on objects over some range of distanced from $d_1$ ( closest ) to $d_2$ ( furthest ) . putting a converging lens in front of your eye reduces both $d_1$ and $d_2$ . that is , it shifts the range of distances over which you can focus closer to you , not further . here 's the geometric-optics proof of this . varying the distance at which you can focus is equivalent to varying the effective focal length of your eye . if $d$ is the diameter of your eye , and $d$ is the distance of the object you are focusing on , then the focal length is given by $$ {1\over f}={1\over d}+{1\over d} . $$ when you put on your reading glasses , the effective $f$ is decreased , or to put it another way , the effective power of the lens , $1/f$ , is increased . ( to be specific , $1/f_{\rm new}=1/f_{\rm old}+1/f_{\rm lens}$ . ) so the left side of the equation becomes larger . $d$ does not change , so $1/d$ must become larger . so $d$ , the distance to which you focus , becomes less .
developments as of summer 2014 the hottest kid on the organic-pv block is perovskites : in february 2012 , hardin , snaith and mcgehee published an article in nature photonics announcing " the renaissance of dye-sensitized solar cells " . the inventors of one implementation , oxford photovoltaics ltd ( a spinoff of the university of oxford ) described their new technology in science , in november 2012 , and have a patent application pending for their dye-sensitized solar cell . for more , see recent papers by henry j . snaith 's team . folk have got excited because of the rapid increases in efficiency since 2009 , combined with the very low cost and abundance of the raw material : as of summer 2014 , the record was $17.9\pm0.8\%$ , held by the korea research institute of chemical technology ( image source ) confirmed organic pv records under standard test conditions progress in photovoltaics : research and applications publishes a biannual review of pv efficiency records , under the name solar cell efficiency tables . the 2014 june review gave a record efficiency under standard test conditions for an organic pv cell of $10.7\pm0.3\%$ . for an organic pv mini-module , the record is $9.1\pm0.3\%$ . ( nb the higher perovskite efficiencies discussed above had not passed audited standard tests in time for the latest biannual review ) cells have higher efficiencies than sub-modules , which tend to have higher efficiencies than modules . a ( sub- or mini- ) module is made up of several cells , and power is aggregated from all of the cells . the most efficient cell in a ( sub-/mini- ) module will , by definition , have an efficiency equal to or higher than all other cells in that ( sub-/mini- ) module ; and not all the module surface area is covered in cells ; therefore record cell efficiency will always be higher than ( sub-mini/ ) module efficiency . here 's the recent history of organic pv cell record efficiencies ( % , standard test conditions ) : 2012-10 $10.7\pm0.3$ 2011-10 $10.0\pm0.3$ 2010-11 $8.3\pm0.3$ 2006-12 $~5.15\pm0.3$ 2006-03 $~3.0\pm0.1$ overall pv record the record efficiency for any pv cell is $44.4\pm2.6\%$ , for a ingap/gaas/ingaas inverted metamorphic cell ( 2013-04 ) , measured at 302 suns , am 1.5 , cell temp $25^{\circ}c$ .
note that in the sum $$=\sum_{s_1=\pm 1} . . . \sum_{s_n=\pm 1}\langle s_2| t_{_{nn}}^{\dagger}|s_1\rangle\langle s_1| t_{_{nnn}}|s_3\rangle\langle s_3| t_{_{nn}}^{\dagger}| s_2\rangle\langle s_2| t_{_{nnn}}|s_4\rangle . . . \langle s_1| t_{_{nn}}^{\dagger}|s_n\rangle\langle s_n| t_{_{nnn}}|s_2\rangle$$ every pair $|s_i\rangle\langle s_i|$ occurs twice with some operator/matrix between them . so you cannot simply execute the sum over $s_i = \pm1$ for both occurrences separately . as pointed out in the paper you linked , one solution via transfer matrix is to group two neighbouring spins to one 4-state spin . then the problem can be reduced to nn interactions only .
it is happening because of the acceleration of the earth orbital speed around the sun ( earth is near the perihelion ) . between december 13 and december 31 the earth is speeding up and also it is normally rotating around its axis . these 2 movements ( constant rotation and increasing orbital speed ) add up to create the observed apparent movement of the sun on the earth sky . the sun rises later and also sets later every day . it is a bit tricky to visualize , so try it with a globe .
if you say that earth 's velocity around the sun is 67,000 mi/h , your reference point is the sun itself , which makes the aeroplane 's velocity 68,000 mi/h , not 1000 . using special relativity only , and ( a ) observing from the sun , a clock on the plane would seem to run slower than a clock on earth . a person ( b ) on earth would measure also measure an aeroplane 's clock to be slower , but by a different factor . time dilation , $ \delta t ' = \frac{\delta t}{\sqrt{1-\frac{v^2}{c^2}}} $ ( a ) observation from the sun : time dilation of aeroplane clock vs earth clock , ( i ) $\frac{\delta t}{\sqrt{1-\frac{68000^2}{c^2}}}$ and ( ii ) $\frac{\delta t}{\sqrt{1-\frac{67000^2}{c^2}}}$ ( b ) observation from the earth : time dilation of aeroplane clock vs earth clock , ( iii ) $\frac{\delta t}{\sqrt{1-\frac{1000^2}{c^2}}}$ these are all different values due to different velocities as measured from different observation locations . this is what relativity is all about . ( side note , this equations are correct iff $c$ is in mi/h . ) up to this point we have ignored general relativity , which takes into account time dilation due to gravitational acceleration ( clocks are measured to be faster in lower g ) . this is an opposite effect from the time dilation due to sr . as it turns out , aeroplanes are travelling way too slow to have their clocks observed to be slowed at all --in fact they measure to be faster than earth clocks , because the difference in gravity outcompetes the difference in velocity in this case . if you are wondering if there is a sweet spot where time dilation due to sr and gr cancel out , there is . you can find out more by searching for time dilation due to gravitation and motion . an important thing to note is that the effect of time dilation are observational effects , and are due different conditions at the observation point and the point being observed . when two objects have relative velocity , they both measure the other 's clock to be slower than their own but a third object with the same velocity as one of the original two will clearly not measure those two as equally slow .
no , you cannot say that $\hat{f}= ( x+a ) + ( x-a ) $ ( which would simplify to $2x$ ) , you specified its definition in the original problem : $$ \hat{f}\psi ( x ) =\psi ( x+a ) +\psi ( x-a ) $$ with words , you could say that $\hat{f}$ is a ( bi-directional ? ) translational operator . but symbolically , there is no other way to state what $\hat{f}$ means , except through how it operates on $\psi ( x ) $ ( or any general function of $x$ ) .
as mentioned in the wikipedia page $4 \times \frac{4 \pi r^3}{3}$ is the excluded volume per particle , so you have to sum over all the particles and divide by the number of particles . while summing up you divide by 2 , because a pair of particles only contribute once to the excluded volume .
