did he knew about the michelson-morley experiment ? he just knew the name of the experiment not any details . the experiment did not play any role in the formulation of str by albert einstein . the context is taken from the book : special theory of relativity by v . a . ; atanov , yuri ( trans . ) ugarov ( author ) art : was michelson 's experiment " decisive " for the creation οι the special theory οι relativity ? an article by r . shankland , published in 1963 , the following excerpt from his interview with einstein dating back to 1950: " when ι asked him how he had learned of the michelson morley experiment , he told me that he had become aware of it through writings of η . α . lοrentz , but only after 1905 had it come to his attention ! " otherwise " he said , " i would have mentioned it in my paper ! " indeed , einstein 's 1905 paper contains no mention of μichelson 's experiment or references to lorentz 's papers . " a letter written by albert einstein : "ιn my own development michelson 's result had not had a considerable influence . ι even do not remember if ι knew of it at all when i wrote my first paper on the subject ( 1905 ) . τhe explanation is that ι was , for general reasons , firmly convinced how this could be reconciled with our knowledge οf electro-dynamics . one can therefore understand why in my personal struggle michelson 's experiment played no role or at least no decisive role . . . "
any optical assembly , whether a telescope or a camera or whatever , will generally have a minimum distance at which it can focus . that said , the scale for distances is set by the focal lengths of the optics . as you point out , they are $360~\mathrm{mm}$ and either $6~\mathrm{mm}$ or $20~\mathrm{mm}$ . ( the $360~\mathrm{mm}$ is the really important one . ) distances long compared to these are all much the same to your telescope , so the building half a kilometer away and the stars thousands of light years away are all " at infinity " as far as it is concerned . the $360~\mathrm{mm}$ actually means that parallel rays " from infinity " need to travel $360~\mathrm{mm}$ beyond the lens to converge to a point . diverging rays from " closer than infinity " objects need to travel further . thus the minimum distance at which you can focus is set by how much you can extend the optical path , and the maximum distance you can focus is set by how short the path can be made . any telescope , whether for terrestrial or astronomical use , should be able to focus at infinity - that is after all where its targets of interest are . note though that i mentioned changing the optical path . in order to focus , assuming you do not have some multimillion dollar deformable optics on hand , you need to turn some knob somewhere to move something . while there are systems built that are fixed to focus at a single distance , i am guessing that is not what you have , since it seems designed for changing eyepieces ( doing so requires refocusing ) and personal use ( while contacts are fine , people who wear glasses are best served by removing them and changing the focus to correct for nearsightedness , since you generally need to get your eye close to the eyepiece ) .
just look at how people actually did it . ; ) measuring shadow length at the same time at different places , comparing horizons ( as also suggested in comments ) , . . . also have a look at this list , some of which are down to earth methods . ; ) by the way , there are easy ways to prove that the earth is rotating , like the foucault pendulum , as well .
quoting from my copy of the 2nd edition of jackson 's book on classical electrodynamics , section 1.2: assume that the force varies as $1/r^{2+\epsilon}$ and quote a value or limit for $\epsilon$ . [ . . . ] the original experiment with concentric spheres by cavendish in 1772 gave an upper limit on $\epsilon$ of $\left| \epsilon \right| \le 0.02$ . followed a bit later by williams , fakker , and hill [ . . . gave ] a limit of $\epsilon \le ( 2.7 \pm 3.1 ) \times 10^{-16}$ . that book was first published in 1975 , so presumably there has been some progress in the mean time .
no , op 's calculation is correct . in more detail , the paper states on page 323 ( apparently assuming that $\mu$ and $\nu$ are real numbers ) , that the result is $$ \mu^2 + 2 \nu^2 ~=~ 2\mu^2 -1~ . $$ the first expression is correct , and corresponds to op 's $3\mu^2-2$ . the second expression is wrong . in other words , the paper makes a mistake in the very last step while reducing with $\nu^2=\mu^2-1$ .
let 's start with your first expression sans some numerical constants : $$ i \equiv \iiint\mathrm{d}\vec{k}\ f ( \epsilon ( \vec{k} ) ) $$ where all we care about is that the function $f ( \epsilon ( \vec{k} ) ) $ is rotationally invariant : $$ f ( \epsilon ( \vec{k} ) ) = f ( \epsilon ( k ) ) $$ we can seperate the angular integrals , which give a factor of $4\pi$ , and we find $$ i = 4\pi \int_0^\infty \mathrm{d}k\ k^2 f ( \epsilon ( k ) ) $$ now we suppose that $\epsilon ( k ) $ is an invertable function . in fact , it is a simple quadratic , but we only need invertability and that it be sufficiently smooth . this means we can write $$ k = k ( \epsilon ) $$ we also assume that $\epsilon ( 0 ) =0$ and $\epsilon ( \infty ) =\infty$ , which is the only reasonable thing and also makes sure that the limits of integration stay trivial . make a change of variables $$ \mathrm{d}k = \mathrm{d}\epsilon \frac{\mathrm{d}k}{\mathrm{d}\epsilon} $$ to get $$ i = 4\pi \int_0^\infty \mathrm{d}\epsilon \frac{\mathrm{d}k}{\mathrm{d}\epsilon} k^2 ( \epsilon ) f ( \epsilon ) $$ matching to your next expression $$ i \propto \int_{-\infty}^\infty \mathrm{d}\epsilon\ g ( \epsilon ) f ( \epsilon ) $$ ( which we require to hold for all $f$ ) , we obtain $$ g\left ( \epsilon\right ) \propto\begin{cases} \frac{\mathrm{d}k}{\mathrm{d}\epsilon}k^{2} ( \epsilon ) and \epsilon\ge0\\ 0 and \epsilon&lt ; 0 \end{cases} $$ i leave you to work out the constant of proportionality ( the 4s and $\pi$s ) , but you should get a functional dependence $g ( \epsilon ) \propto \sqrt{\epsilon}$ . the only unusual thing is that they extend the range of integration to $\epsilon&lt ; 0$ for some reason , but you can do this if you set $g ( \epsilon ) =0$ for negative $\epsilon$ , since there are no states there .
ok , area $a$ is just $\pi d^2/4$ . the real question is : what is $c$ ? it depends on the shape of the orifice ( and reynolds number ) . there are some quick-and-dirty approximations here . it depends on the orifice geometry , like whether its edges are rounded or sharp .
the diffusion is determined from the broadening of the usual elastic scattering peaks . this is known as quasielastic scattering . there is no requirement for the neutron speed to be related to the diffusion speed . have a look at this book for an introduction to the subject .
not knowing your situation in detail it is hard to offer detailed advice , but here is one course of action that should be available to you and will give a sufficient energy calibration : use the co-60 and cs-137 source as you have been ( 3 points ) use only the 356 kev line from the ba-133 source ( 1 point ) use a banana ( suitably wrapped in shrink wrap and ziplock bags to avoid contaminating the detector ) as a k-40 source source ( 1 point ) that gives you five points which is sufficient for a full linear fit with uncertainties . now that i see the op 's data with it is very good signal to noise ratios ( i was using a rather aged and low activity barium source ) , i would modify this as several of the barium lines are usable , and there appears to be a k-40 background in the detector visible in the co-60 data which means that the banana is not actually needed .
knowing the reason why you think it is $2a$ rather than $4a$ could help with a more focussed answer , but here it goes anyway . first , note that in one period the block starts from $x=a$ and ends at the same position . second , the block does not first turn around at $x=0$ , but rather at $x=-a$ . now try breaking the motion up into two half-periods . that is , from $x=+a$ to $x=-a$ ( first half ) , and then from $x=-a$ to $x=+a$ ( second half ) . try drawing a picture if you are still having trouble here .
you are not " replacing a mathematical proof " . what the statements you are referring to mean is that in tensor notation , the proof is immediate , so that nothing needs to be written down . this is because if you have a tensor equation as above , in order to prove lorentz invariance , do a lorentz transformation and go to another set of coordinates $x^{\mu'}$ . then using the usual transformation laws we get that ${\partial_{\mu}} = \lambda^{\mu'}_{\mu}\partial_{\mu'}$ and $f_{\mu\nu} = \lambda^{\mu'}_{\mu}\lambda^{\nu'}_{\nu}f_{\mu'\nu'} $ , we can write the maxwell equation in terms of the new coordinates to become $\lambda^{\mu'}_{\mu}\lambda^{\nu'}_{\nu}\lambda^{\sigma'}_{\sigma} ( f_{\mu'\nu ' , \sigma'}+f_{\nu'\sigma ' , \mu'}+f_{\sigma'\mu ' , \nu'} ) =0 . $ however , this can only hold if the thing inside the brackets is zero itself . namely maxwell 's equation in the primed coordinate system also holds . more succintly , what a " tensor equation " means is that there was nothing special about the coordinate system in which the equations were derived . you could have equally well chosen another system and derived the same equations . thus invariance under coordinate change is immediate .
this is a very complex problem to solve so you will probably want to start with some simplifying assumptions such as n=2 to make it more tractable . you will be looking for a minimum energy solution where the energy is a combination of the gravitational potentials and the energy in the surface tension . depending on parameters there may be some meta stable solutions where energy is locally minimum but not globally . for example a state in which the fluids all form layers with horizontal separation boundaries ordered by density with the densest at the top would be at least metastable because any perturbation such as a distortion of the boundary would increase both types of energy . however , if one of the layers is sufficiently small in volume there may be a preference for the liquid in that layer to form a bubble between the layers above and below . this would depend on the surface tensions between the three layers . even with just two fluids there may be bubbles formed for either the top or bottom layer . working out the shape of the bubble to provide the minimum energy could be non-trivial . with more liquids the number of odd arrangements you need to look at is going to grow . you have to consider that a heavy fluid may prefer to form a bubble above a lighter one if the surface tensions make that a lower energy configuration . in all finding the optimal solution will be a mixture of working through large numbers discrete cases and then optimising the shape of the surface areas . you should perhaps be thinking in terms of how this can be done numerically on a computer rather than an analytical solution . you will want to think about whether the problem is np hard or not . some simplifying assumptions may help but i cant see any physical reason why anything like your possible relationship between surface tensions should hold .
i recently re-derived these equations with all the dimensionful constants in place . your last statement in the " edit " is correct : $t_{00} = \rho_{e}\ , c^{2} = \rho\ , c^{4}$ . it is easy to lose track of factors of $c$ in calculations like this ; the usual culprit is mixing up $t$ and $x^{0} = c\ , t$ , and $\partial_t$ and $\partial_0 = c^{-1}\ , \partial_{t}$ . for instance , $g_{tt} = c^{2}\ , g_{00}$ .
this is really a comment , since i do not think there is an answer to your question , but it got a bit long to put in as a comment . if you google for " why is technetium unstable " you will find the question has been asked many times in different forums , but i have never seen a satisfactory answer . the problem is that nuclear structure is much more complex than electronic structure and there are few simple rules . actually the question is not really " why is technetium unstable " , but rather " why is technetium less stable than molybdenum and ruthenium " , those being the major decay products . presumably given enough computer time you could calculate the energies of these three nuclei , though whether that would really answer the " why " question is debatable . response to comment : the two common ( relatively ) simple models of the nucleus are the liquid drop and the shell models . there is a reasonably basic description of the shell model here , and of the liquid drop model here ( there is no special significance to this site other than after much googling it seemed to give the best descriptions ) . however if you look at the sction of this web site on beta decay , at the end of paragraph 14.19.2 you will find the statement : because the theoretical stable line slopes towards the right in figure 14.49 , only one of the two odd-even isotopes next to technetium-98 should be unstable , and the same for the ones next to promethium-146 . however , the energy liberated in the decay of these odd-even nuclei is only a few hundred kev in each case , far below the level for which the von weizsäcker formula is anywhere meaningful . for technetium and promethium , neither neighboring isotope is stable . this is a quali­tative failure of the von weizsäcker model . but it is rare ; it happens only for these two out of the lowest 82 elements . so these models fail to explain why no isotopes of tc are stable , even though they generally work pretty well . this just shows how hard the problem is .
i am not sure what you mean by ' double contraction ' , but the ricci tensor in local coordinates is given by \begin{align} r_{\mu \nu} = r^\rho_{~~\mu \rho \nu} , \end{align} which is the same as $g^{\sigma \rho} r_{\sigma \mu \rho \nu}$ , exactly what you have written .
they are all the same phenomenon and they are basically just arbitrary distinctions . however they are useful ones . strong lensing normally means we see a clear image . we can then use the shape of the image to precisely calculate the mass distribution in whatever is doing the lensing . for strong lensing we need two things ( 1 ) the lens must be very massive to produce a big enough image to see , and ( 2 ) the alignment needs to be just right i.e. the object must be almost exactly behind the lens . we get weak lensing when the lens is massive , but there is not anything exactly behind the lens . in that case the lens produces small changes in the apparent distribution and appearance of objects around it . again we can use these changes to calculate the mass of the lens , though not as precisely as for strong lensing . microlensing is somewhat different . small objects like stars are very weak lenses and the image they produce is too small to be resolved . however the lensing does cause a measurable change in brightness . so if a star passes in front of some other object we may see the brightness rise then fall again , and it is this phenomenon that is referred to as microlensing .
1 ) irreversible processes are the ones , which by definition increase the entropy . and the increase , similarly to non-equilibrium cases , is added to the reversible $\mathrm{d} s$ . hence , for all irreversible processes : $\delta s &gt ; \dfrac{\delta q}{t}$ 2 ) for at least some non-equilibrium cases thermodynamic variables like $t$ actually may well be defined , but for example only locally , or separately for different chemical species etc . consider a chemical reaction , which goes quasi-statically , is not in equilibrium , hence produces entropy additional entropy and hence is irreversible .
not exactly . fusion of atoms in supernova nucleosynthesis is thought to be responsible for the various atoms that make up the periodic table . while there has not been one in our part of the galaxy for quite some time , plenty of supernova are occurring through out the universe right now . so , while you are made of old stuff , in terms of atoms , most of it probably is not as old as the big bang itself . “the nitrogen in our dna , the calcium in our teeth , the iron in our blood , the carbon in our apple pies were made in the interiors of collapsing stars . we are made of star stuff . ” -carl sagan
most of the astronomy images we find online have some color modification how close to the false color images would they be this is a common misconception , that the pictures you see of galaxies and nebulae are necessarily " false color " , " modified color " , or " photoshopped " . some of them , yes . but a lot of them are quite simply true color , but taken with a sensor ( cmos , ccd ) that does not suffer from the limitations of the human eye . e.g. look at this image of the horsehead nebula : all that color is real . it is there , in the photons reaching you . but your eye cannot see it . a cmos , however , can . this is not " false color " , although saturation was likely increased in post-processing , in addition to what the sensor can do . but the hues are probably real ( e . g . , the red you see in the image , or the blue , was present in the photons hitting the sensor - albeit at a lower saturation level ) . ( an astute observer may object that the eye and the cmos do not see the exact same hues , but let 's not go down that rathole now . ) " false color " means when the image shows green where the cmos ( or the human eye , if luminosity was higher ) would see red , or something like that . this is not always the case with images of nebulae and galaxies ; in fact , if the image was taken with visible light , chances are the hues are preserved . proper false color images are those taken in uv or ir , and then artificially converted to visible light . this is an example of it , the sun in ultraviolet : now , to answer your question : unfortunately , even from a near distance , most of these objects will not look much better . they are , after all , faint , rarefied clouds of dust and gas . they are just not bright enough for the human eye to see color . there are few exceptions . a notable one would be close binary systems where the components are stars of very different temperatures . kind of like albireo , but much closer . from a starship , looking at the two stars orbiting each other , you had see very clearly a striking color difference - perhaps a large , somewhat dim , deep red star , and a blinding , crisp dot of bluish white light , the smaller and more active companion . the views from the center of a globular cluster undergoing a compression phase should be pretty spectacular , too . night would never be dark on a planet in the middle of the cluster .
how can i prove thevenin 's and norton 's theorem ? here 's an outline - you can fill in the dots . measure the voltage ( with an ideal voltmeter ) between any two nodes of an arbitrary linear circuit . call this voltage the open circuit voltage $v_{oc}$ since there is zero current through an ideal voltmeter . then , place an ideal ammeter across the same two nodes and measure the current . call this current the short circuit current $i_{sc}$ since there is zero voltage across an ideal ammeter . now , you have the voltage when there is zero current and the current when there is zero voltage . so , you have two points on the current-voltage ( iv ) plot for these two nodes . as you know , it takes two points to uniquely identify a line in this plane and the equation for this line is $$v ( i ) = v_{oc} - \frac{v_{oc}}{i_{sc}}i$$ can you take it from here ? update 1 to respond to a comment . @alfredcentauri , how do we prove that the characteristic is a straight line ? if the circuit is linear , the superposition theorem holds and , thus , any circuit voltage or current can be expressed as a sum of terms , each term involving one source and equal to the circuit voltage or current due to that source alone , i.e. , the result obtained by zeroing all other sources . in the previous section , we measured the voltage across two terminals of a circuit and labelled that voltage $v_{oc}$ . by superposition , and assuming a dc circuit , if we connect a current test source across these terminals such that $i = i_s$ , the voltage across the terminals is given by $$v = v_{oc} - r_{eq}i_s$$ where $r_{eq}$ is the equivalent resistance between the terminals when all the circuit sources are zeroed ( with all circuit sources zeroed , there is just a resistor network between the terminals ) . thus , by superposition , we know that for a linear dc circuit , there is , between any two terminals , an equivalent circuit with identical terminal characteristics : a voltage source with voltage $v_{oc}$ in series with a resistor with resistance $r_{eq}$ and , since there is just one line between the two measured points in the i-v plane found in the previous section , it follows that $$r_{eq} = \frac{v_{oc}}{i_{sc}} $$ update 2: to verify the validity of my arguments above , i found a formal proof of thevenin 's theorem in one of my undergrad textbooks : " fundamentals of circuits , electronics , and signal analysis " by kendall l . su . i will excerpt and paraphrase this proof found in the appendix a . 1 on page 568 to simplify the proof , we shall assume that the network in question is excited by an independent current source [ $i_{sn}$ ] at its terminal pair ( terminals a and b in figure a . 1 ) . furthermore we shall assume that the network contains $n-1$ independent current sources and $m$ independent voltage sources , and a number of lti elements , including lti controlled sources . since the network is lti , the superposition property prevails . that is to say , $v_{ab}$ is a linear combination of all the strengths of the independent sources . this fact can be expressed analytically as $$v_{ab} = \sum_{j=1}^{n-1}z_{nj}i_{sj} + \sum_{k=1}^{m}h_{nk}e_{sk} + z_{nn}i_{sn} = v_{oc} + z_{nn}i_{sn}$$ $$v_{oc} = \sum_{j=1}^{n-1}z_{nj}i_{sj} + \sum_{k=1}^{m}h_{nk}e{sk}$$ $$z_{nn} = \frac{v_{ab}}{i_{sn}} , e_{sk}=0 , i_{sj}=0$$ . . . the circuit of figure a . 2 [ a voltage source with voltage $v_{oc}$ in series with $z_{nn}$ connected between terminals a and b where the source $i_{sn}$ is connected ] has exactly the relationship described by $$v_{ab} = v_{oc} + i_{sn}z_{nn}$$ the current source $i_{sn}$ cannot tell the difference between [ the actual circuit and the equivalent circuit ] . hence the two circuits are equivalent electrically .
note , that , on the third line , the $\beta$ indice of $\sigma^{\mu\nu}$ and the $\dot \beta$ indice of $\tilde \sigma^{\mu\nu}$ must be raised for indice coherence . same error for the following lines . the formula you want to demonstrate is certainly false . take $\beta = \nu = 0$ , and noting that $\sigma^{\alpha0}= -\frac{1}{2}\sigma^\alpha , \tilde \sigma^{\mu0}= \frac{1}{2}\sigma^\mu $ , if the formula was exact , it would imply ( with $g_{11}=g_{22}=g_{33}$ ) , and in short tensorial notation : $-\frac{1}{4} \vec \sigma \otimes \vec \sigma = 0\tag{1}$ which is obviously false .
potential energy is a property of the system , not any one object . thus there should only be one copy of the typical $1/r$ potential energy between two charges ( plus an analogous gravitational term if that can not be neglected ) . the easiest way to see this is to start from " infinite " separation . instead of pushing the two charges together , hold one fixed and move the other toward it . the moving charge must fight the standard coulomb force ( with a little help from gravity ) to get closer to the stationary one , so the potential energy obtained here is just the integral of this force over the distance traversed ( $d$ to $\infty$ ) . but what about the stationary object ? well , sure , we need to exert a force on it to keep it from being repelled by the approaching charge . but it is not moving , so the change in $\vec{f} \cdot \vec{x}$ energy vanishes . the fact that at some point in the future we will let both objects move does not change the potential energy , so you should get the same potential energy as if the problem were stated : a point mass $m_1$ with charge $q_1$ is fixed at the origin . another point mass $m_2$ with charge $q_2$ is brought in from infinity . what is the potential energy of the system ? it may also help to remember that "$2\infty = \infty$ . " moving objects from $x = -\infty$ and $x = \infty$ to the origin covers the same distance as moving one object from $x = \infty$ to the origin .
the radiation from an ultrarelativistic ( $v \approx c$ ) particle on a circular path is called synchotron radiation . the total power radiated from such a particle is $$p = \frac{e^2 a^2}{6\pi \epsilon_0 c}\gamma^4$$ where $a$ is the acceleration and $\gamma$ is the lorentz factor , $\gamma^2 = 1/ ( 1-v^2/c^2 ) $ .
no , it does not have to be numbered unless , like @dmckee comments , you have received specific instructions to do the numbering . i think your question can best be split up in 2 parts for the answer : writing a report and writing a research paper . writing a report for a report the main goal is typically to show exactly what you have done and how you have done it . quite often the report will be used by other students to continue your work . for this purpose it is convenient to have a numbered list for the methods , because it immediately attracts attention and makes it easy to follow the ' recipe ' . writing a research paper when you write a research paper your goal is in general to resolve a particular issue that exists in the scientific community . your focus will be on the research question and the conclusions you can draw from the experiments/simulations that you did . in this case a numbered list for the method will draw way too much attention to it , much more than it deserves . it is , after all , easy to spot because it disrupts the flow of the paper . in some journals it is not even allowed to use numbered lists if they are not inline ( i.e. . 1 ) . . . 2 ) . . . ) . so in conclusion , if you do not have specific instructions from someone ' higher up': use the numbered list if you want to focus on the method , use the paragraph if you want to focus on a different part of the report/paper
it seems that you have struck upon the answer yourself in the comments , but in the future i would recommend giving more context to your question . i have worked problems like this many times and the way they are to be approached depends significantly on the reason for solving the problem . is this for a physics class or a nuclear engineering one ? is this a classical mechanics class or a quantum mechanics one ? or perhaps a relativity one ? honestly the line " the palladium nuclei just after fission , when they are starting from rest . " does not even make sense . after the fission , the palladium nuclei are not at rest . . . is not that sort of the point ?
to recap : a classic ( not as in " classic versus quantum" ) picture is the one-dimensional model of $\alpha$ decay by gamow and gurney/condon . $q$ represents the energy of the particle within the well , which in this example will be the " disintegration energy " of the system , i.e. the energy the escaping particles are seen to have after a decay . $r&lt ; a$ represents the area within the nucleus , where the strong force is dominant . the attraction therein is represented by the negative potential $-v_0$ ( where $v_0&gt ; 0 \in \mathcal{r}$ ) . $a&lt ; r&lt ; b$ is the " classically forbidden region" ; if we think of macroscopic objects such as a ball rolling up a hill , we intuitively know that a ball with total energy ( potential+kinetic ) $q$ coming from e.g. the right will have depleted all its kinetical energy when it reaches the point $b$ on the hill , and will never be able to " jump " over the crest at $a$ . however , particles in the quantum mechanical region will " leak " some of their probability density through such barriers . in practice it means that in a fraction of the many attempts a particle bounces against such wall , it will actually penetrate and be measurably found on the other side . the potential at $r&gt ; a$ is modeled on the coulomb potential , which falls off as $1/r$ . the " coulomb barrier " is the hill that a particle ( from either direction ) faces due to this effect , that originates from the electromagnetic force . " what does $v_\mathrm{c}$ really represent ? " — this nomenclature most probably represents the height of the potential at $a$ in the picture above , which in the model is given by the electrostatic potential energy emanating from a point charge at $r=0$ . what it implies is different in classical ( cm ) and quantum mechanics ( qm ) . in cm , it is an actual " hard " potential barrier that needs to be overcome to pass the point in space . the ball will never roll over the hill no matter how many times we try unless we give it energy to overcome the potential at $a$ . consequentially , if the ball has such an energy coming up against the hill , it will always roll over ( if we go too far into the analogy we need to worry about friction and geometry and how that can be represented by a potential , so leave it at a theoretical stage for clarity ) . in qm , it enters as part of the schrödinger equation for the particle , from which we can decide the tunneling probability through the barrier . a higher ( and wider ) barrier means that the probability is ( drastically ) lowered and vice versa , but even if the particle has an energy above the barrier , there is always a non-zero chance that it will still not have passed it when we later measure the system . for the case of $\alpha$ decay , there will be many " attempts " of the particle inside the nucleus to escape , so the lifetime will simply put be decided by the ( inverse ) product of the frequency with which the particle presents itself at the barrier , and the probability that it will tunnel through said barrier each time . i have an order of magnitude calculation available for $^{238}\mathrm{u}$ , which puts the average probability of $\alpha$ tunneling at ${\sim}10^{-38}$ , with " attempts " happening at ${\sim}10^{21}\ , /\mathrm{s}$ , to yield a lifetime of ${\sim}10^9\ , \mathrm{y}$ ( from krane : introductory nuclear physics ) . so , to get your protons to fuse , the coulomb barrier can be approximated by the electrostatic potential between two point charges outside the range of the strong force ( ${\sim}\mathrm{fm}$ ) . if you put this into the schrödinger equation , you get a tunneling probability describing how many of the collisions that will have the chance of fusing . also included is the difference in binding energy per nucleon in the end product , and it is still a crude model to say the least ( the diproton is not bound ; also see later link on the proton-proton chain ) . to accelerate a proton to $1\ , \mathrm{mev}$ , you need to accelerate it through a field of electric potential $1\ , \mathrm{mv}$ , since a proton has a charge of $1\ , e$ , where $e$ is the elementary charge ( an example that shows the motivation for using $\mathrm{ev}$ as an energy unit to begin with ) . to be certain of tunneling happening , you will need many collision attempts , decided by the calculations above . how much it will cost you in electricity bills depends on how you generate the accelerating field . the answer to the core of your question is that there is not a " certain " energy where fusing will always take place in qm , but a continuous spectrum of probabilities depending on the potentials and energies involved . that is probably ( aha ) why the texts you read quickly turn to talking about energy distributions rather than distinct energies . if you set the dial on your machine high enough , the probability might eventually get close to $1$ . in practice , other reaction channels might have taken over before that , and there is more to be said on the proton-proton chain and fusing , but that is another question . for more details on the $\alpha$ decay model , one can start with e.g. modeling alpha halflife ( hyperphysics ) .
the most general lorentz transformation that is connected to the identity is given by the conjugation by $\exp ( -a ) $ where $$ a = \frac 12 \omega_{\mu\nu} \gamma^\mu \gamma^\nu $$ and $\omega_{\mu\nu}$ is an antisymmetric tensor containing $d ( d-1 ) /2$ parameters . the group of all such transformations is isomorphic to $spin ( d-1,1 ) $ . if $\omega$ only contains one component $0\mu$ , then it is a boost , and the nonzero numerical value of $\omega$ is the rapidity - the " hyperbolic angle " $\eta$ such that $v/c=\tanh\eta$ . if only one doubly spatial component of $\omega$ is nonzero , then this component $\omega_{\mu\nu} = -\omega_{\nu\mu}$ is obviously the angle itself . note that the spatial-spatial terms in $a$ are anti-hermitean , producing unitary transformations ; the mixed temporal-spatial terms in $a$ are hermitean and they do not product unitary transformations on the 4-component space of spinors ( but they become unitary if they are promoted to transformations of the full hilbert space of quantum field theory ) . in 4 dimensions , a general antisymmetric matrix $4\times 4$ contains 6 independent parameters and has eigenvalues $\pm i a , \pm i b$ , so in 3+1 dimensions , one can always represent a general lorentz transformation as a rotation around an axis in the 4-dimensional space followed by a boost in the complementary transverse 2-plane . this is the counterpart of the statement that any $su ( 2 ) $ rotations in 3 dimensions is a rotation around a particular axis by an angle . if you allowed $a$ to contain something else than $\gamma^{\mu\nu}$ matrices which generate the lorentz group , you could get other groups . only for a properly chosen subset of allowed values of $\omega$ , you would get a closed group from the resulting exponentials ( under multiplication ) . in particular , if you allowed $a$ to be an arbitrary complex combination of any products of gamma matrices , well , then you would allow $a$ to be any complex $4\times 4$ matrix , and its exponentials would produce the full group $gl ( 4 , c ) $ - surprising , carl ? ; - ) it is not a terribly useful groups in physics because actions are usually not invariant under this " full group " , are they ? also , there are not too many groups in between $spin ( 3,1 ) $ and $gl ( 4 , c ) $ - i guess that there is no proper group of $gl ( 4 , c ) $ that has a proper $spin ( 3,1 ) $ subgroup . obviously , there are many subgroups of $spin ( 3,1 ) $ - such as $spin ( 3 ) $ , $spin ( 1,1 ) \times spin ( 2 ) $ , and others .
i am assuming the units have to be s^-1 , as the damping constant is present in the exponential equation which plots damping of y=ae^kt ( which plots amplitude vs time ) . is that a correct assumption ? no , the damping ratio $\zeta$ is dimensionless : $$ [ c ] = \frac{ [ f ] }{\left [ \frac{dx}{dt}\right ] } = \frac{\mathrm{n}}{\mathrm{m}\cdot\mathrm{s}^{-1}} = \frac{\mathrm{kg}\cdot\mathrm{m}\cdot\mathrm{s}^{-2}}{\mathrm{m}\cdot\mathrm{s}^{-1}} = \mathrm{kg}\cdot\mathrm{s}^{-1}$$ $$ [ \zeta ] = \frac{ [ c ] }{\sqrt{ [ m ] [ k ] }} = \frac{\mathrm{kg}\cdot\mathrm{s}^{-1}}{\sqrt{\mathrm{kg}\cdot\mathrm{n}\cdot\mathrm{m}^{-1}}} = \frac{\mathrm{kg}\cdot\mathrm{s}^{-1}}{\sqrt{\mathrm{kg}^2\cdot\mathrm{s}^{-2}}} = \frac{\mathrm{kg}\cdot\mathrm{s}^{-1}}{\mathrm{kg}\cdot\mathrm{s}^{-1}} = 1$$ the solution of the damped harmonic oscillator differential equation ( when underdamped ) is $$x ( t ) = a e^{-\zeta \omega_0 t} \ \sin \left ( \sqrt{1-\zeta^2} \ \omega_0 t + \varphi \right ) $$ so the exponent is dimensionless ( as it must be ) : $$ [ \zeta \omega_0 t ] = 1\cdot\mathrm{s}^{-1}\cdot\mathrm{s} = 1$$ dimensionless and dimensionful parameters the differential equation for a damped harmonic oscillator is $$m\frac{d^2x}{dt^2} + c\frac{dx}{dt} + kx = 0$$ we can reduce the number of parameters to 2 just by dividing by $m$ $$\frac{d^2x}{dt^2} + \frac{c}{m}\frac{dx}{dt} + \frac{k}{m}x = 0$$ then we can transform the two remaining parameters to get a dimensionless one , controlling the shape of the solution , and a dimensionful one , setting the timescale . one way of doing that is to define $$\omega_0 = \sqrt{\frac{k}{m}}$$ $$\zeta = \frac{\frac{c}{m}}{\omega_0} = \frac{c\sqrt{m}}{m\sqrt{k}} = \frac{c}{\sqrt{k\ , m}}$$ so that the differential equation takes the form : $$\frac{d^2x}{dt^2} + \zeta\omega_0\frac{dx}{dt} + \omega_0^2x = 0$$ the reason to choose $\omega_0$ as the dimensionful parameter is physical : when the system is underdamped , $\omega_0$ is the angular frequency of oscillation . more information about this differential equation and its physical interpretation can be seen in wikipedia .
the heat equation is an example of a convection-diffusion equation . your problem is one-dimensional in space ( only $x$ ) , which simplifies it a bit . the term on the left hand side is the time-rate of change of the internal energy $u$ ( often a multiple of the temperature ) . the second term is a diffusion term , as , in time , it diffuses or " smooths " peaks . the last term is a convective term , i.e. your medium is moving at constant velocity $v_0$ to the right .
the sine function can be taylor expanded to yield , $$\sin x = x-\frac{1}{6}x^3 +\mathcal{o} ( x^5 ) $$ hence the argument of the sine function must be dimensionless , otherwise one would be adding quantities of differing dimension as evinced by the taylor series . therefore , $$ [ k ] =\frac{1}{ [ x ] }=\mathrm{meter}^{-1}$$ to ensure the product $kx$ has no dimensions . similarly , $\omega$ in your equation has dimensions of frequency , or inverse time to ensure $\omega t$ is dimensionless .
aluminium is paramagnetic , so external magnets will induce a magnetic field in it . however the magnetic susceptibility is only 2.2 $\times$ 10$^{-5}$ so i would be surprised if the induced magnetism would be high enough to be noticable . normally you can only measure paramagnetism in the lab with sensitive equipment . does the aluminium bar remain magnetic when you remove the magnets ? if not that would suggest it is induced magnetism , but if so there must be some ferromagnetic material present . either way a noticable magnetic field suggests your aluminium is not pure ( as anna suggests in a comment ) .
in a $d$-dimensional euclidean space ( with positive definite norm ) , one has $$ \vec{\nabla} \cdot \frac{\vec{r}}{r^d} ~=~{\rm vol} ( s^{d-1} ) ~\delta^d ( \vec{r} ) , $$ cf . the divergence theorem and arguments involving either test functions and integration by part , or $\epsilon$-regularization , similar to methods applied in this phys . se answer . here ${\rm vol} ( s^{d-1} ) $ is the surface area of the $ ( d-1 ) $-dimensional units sphere $s^{d-1}$ . by similar arguments one may show that the identity $$ \vec{\nabla} ( r^{2-d} ) ~=~ ( 2-d ) \frac{\vec{r}}{r^d} , \qquad d\neq 2 , $$ contains no distributional contributions in $d$-dimensional euclidean space . for the related questions in minkowski space , one suggestion is to introduce an $\epsilon$-regularization in the euclidean formulation , and then perform a wick rotation , and at the end of the calculation , let $\epsilon\to 0^+$ .
heat of vaporization is related to enthalpy change , while dew point is related to free energy change , i.e. enthalpy plus entropy . that is why they are very different concerning relative humidity . the enthalpy of a gas is more-or-less independent of pressure or partial pressure , because gas molecules do not really interact with each other . at insanely-high pressures there would be some effect on enthalpy of course , but the effect at everyday pressures is very low . pressure mainly affects a gas via entropy not enthalpy . the enthalpy of a liquid is somewhat dependent on total pressure : a high pressure will push the molecules closer together and therefore change their interaction energies . but obviously the enthalpy of the liquid does not depend on what the gas partial pressures are , it can only depend on the liquid 's own total internal pressure . so the answer is : heat of vaporization , being related to enthalpy not entropy , has essentially no dependence on relative humidity . ( given a constant total air pressure ) -- update -- oops , whenever i wrote " enthalpy " i should have said " enthalpy per molecule " or " enthalpy per mole " [ "molar enthalpy" ] . you can check for yourself that the enthalpy per molecule of an ideal gas is independent of pressure or partial pressure . for a real-world gas , it is approximately independent . the " per mole " quantities are what matter for dew point etc .
this case is discussed in the following article by paul hickson : eliminating the coriolis effect in liquid mirrors the large zenith telescope is a 6 meter diameter mercury mirror telescope . as part of the design process hickson did a theoretical exporation of how the coriolis effect would affect the mirror shape . according to hickson the coriolis effect is significant for the lzt . also , hickson 's calculations indicated the effect could be reduced below detection level by setting the telescope at a tilt of 13.1 arcsec . for another mercury mirror telescope , a 3 meter device in operation at the time the article was writtten ( 2001 ) , hickson calculated a required tilt of 12 arcsec . but " the effects were smaller than the atmospheric seeing and could not be adequately assessed " . for the lzt : " the larger mirror diameter and better image sampling should allow us to verify the technique using this telescope " . i do not know whether this was followed up . i gather from this that the accuracy of the paraboloid can only be assessed indirectly , from it is effects on the quality of the imaging . of course , it would be very difficult to obtain a value for the earth rotation rate in this way . i gather that even for the 6 meter mirror the effect is close to being swamped by atmospheric seeing . the error in the angle of tilt that yields the best imaging will be something like 5 or 10 arcsec .
suppose you have a constant angle $\theta$ slope in the original frame ( we suppose that the transitions from horizontal movements to the slope is quasi-instantaneous ) . call $x$ and $x'$ the horizontal displacements in the original and moving frame . call $t$ the total time for going to $z=h$ to $z=0$ . then you have $x'= x- v_0 t$ with $ x= h \cot \theta$ , the angle of the slope in the moving frame is given by : $$h \cot \theta ' = h \cot \theta - v_o t \tag{1}$$ the coordinates of the normal force ( in the original and moving frames ) are $ \vec n = mg ( - \sin \theta , \cos \theta ) $ . the unit displacement vector , in the moving frame , is $\vec n = ( - \cos \theta ' , - \sin \theta' ) $ the total supplementary work is : $$w_{supp} = \vec n . \vec n \quad \dfrac{h}{\sin \theta'} = mgh\ , ( \sin\theta \cot \theta ' - cos \theta ) \tag{2}$$ using $ ( 1 ) $ , we get : $$w_{supp} = -mg v_0 t \tag{3}$$ the work due to the gravity force is : $w = ( mg \sin \theta' ) \dfrac{h}{\sin \theta'} = mgh$ so , finally , in the moving frame , we have : $$ mgh - mgv_0t = \frac{1}{2} m ( \delta v ) ^2\tag{4}$$ however , $gt$ is nothing else than $\delta v$ . the work , in the lhs of the above equation , does not depend on the slope , so we may imagine a quasi-instantaneous $90$ degrees turn from horizontal to vertical , then a quasi-vertical slope , followed by a quasi-instantaneous $90$ degrees turn from vertical to horizontal . during the quasi-instantaneous turns , the modulus of the speed is conserved ( because energy is conserved and no work is done ) . so , considering a vertical movement , it is obvious that $\delta v= gt$ so finally you have , skipping the overall $m$ factor : $$ gh - v_0 \delta v = \frac{1}{2} ( \delta v ) ^2\tag{5}$$
no , nothing in physics depends on the validity of the axiom of choice because physics deals with the explanation of observable phenomena . infinite collections of sets – and they are the issue of the axiom of choice – are obviously not observable ( we only observe a finite number of objects ) , so experimental physics may say nothing about the validity of the axiom of choice . if it could say something , it would be very paradoxical because axiom of choice is about pure maths and moreover , maths may prove that both systems with ac or non-ac are equally consistent . theoretical physics is no different because it deals with various well-defined , " constructible " objects such as spaces of real or complex functions or functionals . for a physicist , just like for an open-minded evidence-based mathematician , the axiom of choice is a matter of personal preferences and " beliefs " . a physicist could say that any non-contractible object , like a particular selected " set of elements " postulated to exist by the axiom of choice , is " unphysical " . in mathematics , the axiom of choice may simplify some proofs but if i were deciding , i would choose a stronger framework in which the axiom of choice is invalid . a particular advantage of this choice is that one can not prove the existence of unmeasurable sets in the lebesgue theory of measure . consequently , one may add a very convenient and elegant extra axiom that all subsets of real numbers are measurable – an advantage that physicists are more likely to appreciate because they use measures often , even if they do not speak about them .
your voice , like any sound , is a combination of many frequencies . physically , your voice consists of pressure waves . if we plot the pressure as a function of time , we see that it goes up and down in a way that looks somewhat random . you can measure these pressure waves with a microphone , then visualize them with an oscilloscope . here 's a youtube video where they do this , starting 4:50 into the video . you may be able to do this at home using the microphone on your computer and some software like audacity . the data collected by your microphone is a time series . the pressure is a function of time . if you sang a pure note ( or a reasonable approximation thereof ) , like you hear from an electronic tuner , the pressure would just be a sine wave . you could imagine a more complicated sound that was two sine waves on top of each other . this could produce beats . as you add more and more frequencies , more and more complicated sounds become possible . it is a remarkable result that in fact any sound can be represented as a sum of infinitely-many sines waves of different periods added up on top of each other . this is fourier 's theorem . a human voice thus consists of many sine waves combined simultaneously . presumably , each individual voice has some special patterns to the way these frequencies are combined , assisting us in recognizing voices . however , speaker recognition is probably based on other information as well . i do not know too much about it , but you can check out the wikipedia article . we frequently try to isolate the different frequencies in a sound . this is done electronically through electronic filters . a crude example is " turning up the bass " - amplifying the low-frequency components of a sound . of course , a professional music studio has far more sophisticated control of the various frequencies . this control can also be mimicked digitally through music sequencers . on a cruder level , you could simply talk directly into an open piano . the string in the piano will be excited by your voice . the strings each have a specific frequency , so the strings that are excited the most tell you that their particular frequency is present the most in your voice . your ear accomplishes a similar task . the cochlea has many small hairs , similar to piano strings , which are tuned to different frequencies . when they vibrate , they mechanically trigger an ion channel to open , beginning an action potential that is eventually interpreted as sound by your brain . so , in essence , you are distinguishing the various frequencies in people 's voices already .
i agree that this might be better for chemistry stackexchange . however i will give you an short answer after doing a little searching . you should look at henry 's law which states that that the concentration , $c$ , of a gas in a liquid at a specific temperature is proportional to the partial pressure , $p$ , of that gas in the atmosphere above the liquid , $$ p=k_hc . $$ so the concentration will be the higher if $k_h$ is smaller . wikipedia also gives a table for a few common gasses of these $k_h$ constants at room temperature $ ( 298.15 k ) $ . for $co_2$ it is equal to $29.41\frac{atm}{mol/l}$ . however the constant of $o_2$ is equal to $769.23\frac{atm}{mol/l}$ , so you would need roughly 26 times higher partial pressure of $o_2$ to get the same concentration . and for nitrogen it is even roughly 56 times higher $ ( k_{h , n_2}=1639.34\frac{atm}{mol/l} ) $ . i do not know how many other gasses also have a relatively low henry 's law constant , $k_h$ . but i believe , if i did the conversion correctly , that ammonia has an even lower value of roughly ( the literature values deviate quite a bit ) $0.02\frac{atm}{mol/l}$ . edit : apparently ozonated water is being used medically . ozone has a henry 's law constant of about $100.30\frac{atm}{mol/l}$ , so it will escape the water eventually and therefore it has to be made before consumption . but i was not able to find a source that ozonated water is fizzy , so i do not know why it does for $co_2$ , it probably has something to do with the fact that it also forms carbonic acid .
here is a cute little trick i have often found pretty handy : just keep squaring your matrices until they are diagonal ! in this case you are going to have to make use of the standard identities of pauli matrices $$\left\{ \sigma_{i} , \sigma_{j}\right\} =2\delta_{ij}$$ you also need to make use of the fact that the different “species” of pauli matrices , $\sigma$ and $\tau$ , won’t “see” each other . in other words , when you’re working through the algebra , pauli matrices of different species can pass through each other as if they were scalars . anyways , the given hamiltonian is $$h = \eta_{k}\tau_{z}+b\sigma_{x}+\alpha k\sigma_{y}\tau_{z}+\delta\tau_{x}$$ as i mentioned above , we first square it : $$h^2 = \eta_{k}^{2}\tau_{z}^{2}+b^{2}\sigma_{x}^{2}+\left ( \alpha k\right ) ^{2}\sigma_{y}^{2}\tau_{z}^{2}+\delta^{2}\tau_{x}^{2}+2b\eta_{k}\tau_{z}\sigma_{x}+2\alpha k\eta_{k}\sigma_{y}\tau_{z}^{2}+2\delta b\sigma_{x}\tau_{x}+\delta\eta_{k}\left\{ \tau_{z} , \tau_{x}\right\} +\alpha kb\left\{ \sigma_{x} , \sigma_{y}\right\} \tau_{z}+\alpha k\delta\sigma_{y}\left\{ \tau_{z} , \tau_{x}\right\}$$ now , using the anticommutator identity for either species of pauli matrices , the above expression simplifies to $$h^2 = \eta_{k}^{2}+b^{2}+\left ( \alpha k\right ) ^{2}+\delta^{2}+2b\eta_{k}\tau_{z}\sigma_{x}+2\alpha k\eta_{k}\sigma_{y}+2\delta b\sigma_{x}\tau_{x}$$ for reasons that will become obvious shortly , we rearrange the above expression in the following way and square it $$\left ( h^{2}-\eta_{k}^{2}-b^{2}-\left ( \alpha k\right ) ^{2}-\delta^{2}\right ) ^{2}=\left ( 2b\eta_{k}\tau_{z}\sigma_{x}+2\alpha k\eta_{k}\sigma_{y}+2\delta b\sigma_{x}\tau_{x}\right ) ^{2}$$ expanding out that further we get $$\left ( h^{2}-\eta_{k}^{2}-b^{2}-\left ( \alpha k\right ) ^{2}-\delta^{2}\right ) ^{2} = 4b^{2}\eta_{k}^{2}\tau_{z}^{2}\sigma_{x}^{2}+4\left ( \alpha k\right ) ^{2}\eta_{k}^{2}\sigma_{y}^{2}+4\delta^{2}b^{2}\sigma_{x}^{2}\tau_{x}^{2}+4\alpha kb\eta_{k}^{2}\tau_{z}\left\{ \sigma_{x} , \sigma_{y}\right\} +4\delta b^{2}\eta_{k}\sigma_{x}^{2}\left\{ \tau_{x} , \tau_{z}\right\} +4\alpha k\eta_{k}\delta b\left\{ \sigma_{x} , \sigma_{y}\right\} \tau_{x}$$ once again , using the anticommutators identities we get $$\left ( h^{2}-\eta_{k}^{2}-b^{2}-\left ( \alpha k\right ) ^{2}-\delta^{2}\right ) ^{2}=4b^{2}\eta_{k}^{2}+4\left ( \alpha k\right ) ^{2}\eta_{k}^{2}+4\delta^{2}b^{2}$$ note that the above expression contains only diagonal matrices ; we have effectively diagonalized the hamiltonian . this can be made more explicit by writing $$h^{2}=\left [ \eta_{k}^{2}+b^{2}+\left ( \alpha k\right ) ^{2}+\delta^{2}\pm2\sqrt{b^{2}\eta_{k}^{2}+\left ( \alpha k\right ) ^{2}\eta_{k}^{2}+\delta^{2}b^{2}}\right ] \mathbb{i}_{4\times4}$$ now , from the above expression , it is not hard to figure out that $$e_{k}^{2}=\eta_{k}^{2}+b^{2}+\left ( \alpha k\right ) ^{2}+\delta^{2}\pm2\sqrt{b^{2}\eta_{k}^{2}+\left ( \alpha k\right ) ^{2}\eta_{k}^{2}+\delta^{2}b^{2}}$$ forgive me if you’re wondering : “with all this algebra , how is that a trick ? ” well , this was a tough example . but this trick is pretty general whenever your hamiltonian consists of matrices ( or their tensor products ) which satisfy the clifford algebra . i’m sure you can pick a much simpler example where this trick will really be a trick . for example , you can check the hamiltonian in equation ( 51 ) of : martin leijnse and karsten flensberg . “ [ introduction to topological superconductivity and majorana fermions . ] [ 1 ] ” semiconductor science and technology 27 , no . 12 ( 2012 ) : 124003 . ( [ arxiv ] [ 2 ] ) where you can simply compute the eigenvalues ( equation ( 52 ) ) in your head using this trick .
from math and the power rule : $\dfrac{d ( x^2 ) }{dx} = 2x$ and we assume that l is a function of time : $\vec{l} = \vec{l ( t ) }$ . to refresh you on the chain rule : if x were a function of time , then $\dfrac{d ( f ( x ) ) }{dt} = \dfrac{d ( f ( x ) ) }{dx} * \dfrac{dx}{dt}$ . back to math : $\dfrac{d ( x^2 ) }{dx}$ is actually $2x\dfrac{dx}{dx}$ if you apply said chain rule . $\dfrac{dx}{dx}$ is commonly held to be equal to $1$ , so it is left out . as such : $\dfrac{d ( \vec{l}^2 ) }{dt} = 2\vec{l}\dfrac{d\vec{l}}{dt}$ . this is all assuming that we are operating element-wise on your vector $\vec{l}$ . that means it is the same as a normal ( scalar ) equation , but there is one scalar equation for each dimension of your vector . additionally , the notation $\vec{l}^2$ for a vector should be avoided . use dot product or cross product . this equation should be written as : $$2\vec{l}\cdot\dfrac{d\vec{l}}{dt} = \dfrac{d ( \vec{l}\cdot\vec{l} ) }{dt}$$ this equation is not true if $l^2$ were to be interpreted as a cross product ( $\vec{l}\times \vec{l} = 0$ ) of a vector with itself . as for the difference between single and double bars , both mean magnitude , but for vectors we like to use double bars to remove any similarity to the absolute value of a number : $$|-3| = 3$$ $$||\vec{\langle 3 , 4\rangle}|| = 5$$
the symbol $k_\textrm{b}$ pretty much invariably denotes boltzmann 's constant . apart from that , your question is asking about the statistical mechanics fact that for a canonical ensemble ( i.e. . a physical system in contact with a heat bath at some temperature $t$ ) the probability for the system to have energy $e$ is equal to $$p = \frac{1}{z}e^{-e/k_\textrm{b}t}$$ where $z$ is a normalization factor known as a partition function .
the square brackets mean antisymmetrization . that is : $$ x_{ [ a_1a_2\dots a_n ] } = \frac{1}{n ! }\sum_{p\in s ( n ) } \text{sign} ( p ) x_{a_{p ( 1 ) }a_{p ( 2 ) }\dots a_{p ( n}} $$ where $s ( n ) $ is the set of permutations of $n$ elements , and $\text{sign} ( p ) $ is the sign of the permutation $p$ , that is , $\text{sign} ( p ) =-1$ if you need an odd number of element exchanges , and $\text{sign} ( p ) =+1$ if you need an even number of element exchanges . in particular , $r_{ [ abc ] }{}^d = \frac{1}{6}\left ( r_{abc}{}^d+r_{bca}{}^d+r_{cab}{}^d-r_{bac}{}^d-r_{acb}{}^d-r_{cba}{}^d\right ) $ $\nabla_{ [ a}r_{bc ] d}{}^{e} = \frac{1}{6}\left ( \nabla_{a}r_{bcd}{}^{e}+\nabla_{b}r_{cad}{}^{e}+\nabla_{c}r_{abd}{}^{e}-\nabla_{b}r_{ac d}{}^{e}-\nabla_{a}r_{cbd}{}^{e}-\nabla_{c}r_{bad}{}^{e}\right ) $ you can " move " indices up and down using the metric tensor . that is , $$r_{abcd} = g_{de}r_{abc}{}^e , \quad r_{abc}{}^d = g^{de}r_{abce} . $$ the square brackets just affects the indices ; the $r$ is inside because the antisymmetrization affects indices both from $\nabla$ and from $r$ .
the assumption is that the electric flux lines are going to go through the conductor parallel to the conductor . under those conditions , the electric field within the conductor is going to have a constant magnitude , and point parallel to the conductor . i assume you are familiar with $$e=-\nabla \phi\ \ , $$ where $\phi$ is the electric potential . we then integrate along a line through the conductor $$v=\int_{0}^{\ell} \nabla \phi \ dx = \int_{0}^{\ell} – e \ dx = - e \ell\ \ , $$ which is the same as $e=v/\ell$ if you consider a positive $e$ to mean a vector pointing in the $-x$ direction instead of the $+x$ direction .
that in mind , does this mean the light can be bent around or does the light loose energy when its waves rotate ? this answer expands on the previous one by user3814483 . you must be familiar with the fact that light is composed of alternating electric and magnetic fields propagating in space whether there is a medium or not . the electromagnetic waves that compose electromagnetic radiation can be imagined as a self-propagating transverse oscillating wave of electric and magnetic fields . this diagram shows a plane linearly polarized emr wave propagating from left to right . the electric field is in a vertical plane and the magnetic field in a horizontal plane . the electric and magnetic fields in emr waves are always in phase and at 90 degrees to each other . the direction of the electric field is taken as the polarization direction , and the wave is polarized if it is uniformly so throughout its extent . the faraday effect does not affect the energy of the beam of light , just the direction in the vertical to the motion plane , of the electric and magnetic fields , so it does not change the direction in which the energy propagates , which is given by the cross product of electric and magnetic fields . so in this case the light does not bend around or lose energy . polarization of light is measurable , and the effect changes it . light can bend in diffraction when entering a medium , and in a suitable medium , it can bend as the medium bends , as happens in the optical fiber cables widely used for phone lines , and not only , ( but that is another story ) . no energy is lost in transparent media usually , otherwise they would not be transparent .
the question is clear enough to answer . the question does involve mass defect . the best place to start is with ordinary problems . we can consider a nonrelativistic problem of a particle in a potential well , $v ( r ) ~=~-r^n$ for some power of $n$ . for mathematical reasons $n~=~\pm 2$ have nice properties with closed form solutions . this is input into a schrodinger equation $$ i\hbar\frac{\partial}{\partial t}\psi ( r , t ) ~=~-\frac{\hbar^2\nabla^2}{2m}\psi ( r , t ) ~+~v\psi ( r , t ) $$ so if we consider a stationary phase $\psi ( r , t ) ~=~e^{-i\omega t}\psi ( r ) $ and the frequency determines energy $e~=~\hbar\omega$ $$ \hbar\omega~=~\frac{p^2}{2m}~+~v $$ where the potential energy is negative . if the potential energy were “turned off” so the particles are free then energy is larger . the energy of system is then lower , and if this energy $e~=~mc^2$ is some appreciable fraction of the mass-energy of the system $e’~=~mc^2$ , say $m/m~~\simeq~ . 01$ to $ . 1$ the system is not highly relativistic but the mass equivalence is measurable . for atomic physics the energy levels of electrons are on the order of electron volts , while the mass of electrons are $ . 51$mev . so the mass defect is pretty small . for nuclear physics of nucleons and mesons the energy levels are on the order of $10mev$ while the collection of nucleons has mass-energy in multiples of $1gev$ . the energy levels are determined by $\pi^0 , ~\pi^\pm$ mesons , which are the intermediary gauge bosons between the nucleons ${p , ~n}$ . this in fact forms a doublet which has energy level splitting due to the electric charge difference . the above model may be made more exact if the mesons are considered to be similar to a photon , with a gauge potential $$ {\vec a} ( k ) ~=~{\vec n} ( a_ke^{-ikr}~+~a^\dagger_ke^{ikr} ) $$ which interacts with a dipole formed from the nucleon doublet ${\vec{\cal p}}~=~p{vec\sigma}$ in an interaction hamiltonian $$ h_{int}~=~-{\vec{\cal p}}\cdot{\vec a} ( k ) . $$ we are only considering interactions with one momentum or wave number . now expand that out and keep terms $a ( k ) \sigma^+$ and $a^\dagger\sigma^-$ , which is the rotating wave approximation in atomic interactions with photons . this makes the above schrodinger equation and potential more exact . this interaction hamiltonian will then reproduce the mass-defect . this may be further improved of course . the nucleon doublet is su ( 2 ) , and the meson potential may be extended from this naïve u ( 1 ) approximation to su ( 2 ) as well . the momentum operator may be made covariant with respect of the gauge potential and the theory refined further . in fact the yang-mills theory was derived to understand this isopin theory of nuclear physics as understood in the 1950s .
it is all in what you want to describe mathematically . you can have an n dimensional space and yes , you could " visualize " the analogue of two dimensions going into three . these are euclidean spaces , i.e. the metric is ds* 2=dx *2+dy* 2+dz *2+ . . . . up to n terms . time is the fourth dimension in current physics because we are attempting to describe and predict motions and interactions of matter and light in a mathematical manner , and the equations are such that they simplify when time is assumed to be the fourth dimension in what is called a pseudo euclidean space . , and in our case dt**2 has a negative sign . it is what the physics comes out with that makes time the fourth dimension .
it is a standard terminology – and set of insights – not only in string theory but in quantum field theories or anything that can be approximated by ( other ) quantum field theories at . . . low energies . such a low-energy action becomes very accurate for the calculation of interaction of particles ( quanta of the fields ) of low energies , in this case $e\ll m_{\rm string}$ . equivalently , the frequencies of the quanta must be much smaller than the characteristic frequency of string theory . the previous sentence may also be applied in the classical theory : the low-energy effective action becomes accurate for calculations of interactions of waves whose frequency is much lower than the stringy frequency or , equivalently , whose wavelength is much longer than the string scale , $\lambda\gg l_{\rm string}$ . low-energy effective actions may completely neglect particles whose mass is ( equal to or ) higher than the characteristic energy scale , in this case $m_{\rm string}$ , because such heavy particles can not be produced by the scattering of low-energy particles at all – so they may be consistently removed from the spectrum in this approximation . the scattering of the light and massless particles that are kept may be approximately calculated from the low-energy effective action and this approximation only creates errors that are proportional to positive powers of $ ( e/m_{\rm string} ) $ so these errors may be ignored for $e\ll m_{\rm string}$ . you may imagine that there are corrections in the action proportional to $\alpha'$ or its higher powers that would make the effective action more accurate at higher energies but become negligible for low-energy processes . there are lots of insights – conceptual ones as well as calculations – surrounding similar approximations and they are a part of the " renormalization group " pioneered mainly by ken wilson in the 1970s . in particular , by " low-energy effective actions " , we usually mean the wilsonian effective actions . but they are pretty much interchangeable concepts to the 1pi ( one-particle-irreducible ) effective actions , up to a different treatment of massless particles . it is impossible to teach everything about the renormalization group and effective theories in a single stack exchange answer . this is a topic for numerous chapters of quantum field theory textbooks – and for whole graduate courses . so i just conclude with a sentence relevant for your stringy example : string theory may be approximated by quantum field theories for all processes in which only particles much lighter than the string mass are participating and in which they have energies much smaller than the string scale , too . if that is the case , predictions of string theory for the amplitudes are equal to the predictions of a quantum field theory , the low-energy effective field theory , up to corrections proportional to powers $ ( e/e_{\rm string} ) $ .
strictly speaking , tension is not the same as force , although it is sometimes described as the magnitude of the ' pulling force ' experienced by an element ( such as a rope ) . the important thing to remember when resolving forces in classical mechanics and to understand tension is to apply newton 's three laws of motion . they are : 1st law : an object with no external force will not change velocity 2nd law : force = mass x acceleration 3rd law : every applied force ( action ) has an equal and opposite force ( reaction ) . so for the one dimensional cases you have given , think of the ' tension ' of the rope as the magnitude of any pulling force it would be experiencing , bearing in mind that this tension is not actually a force ( it has no direction ) , whereas the force whose magnitude it has , would be appear to be pulling the rope in opposite directions ( as per newton 's 3rd law ) . $t\leftarrow\rightarrow t$ so , back to your questions : 1 - when you pull on a rope tied to an immovable object , applying a force $f$ , it reacts with force $-f$ ( newton 's 3rd law ) and the ' tension ' in the rope is the magnitude of this force $f$ . $f\leftarrow\rightarrow f$ 2 - if you pull on a rope which is tied a mass $m$ ( initially at rest and free to move ) it will accelerate towards you ( newton 's second law ) . if you keep keep pulling the rope , keeping it taut by applying a constant force $f$ for a time $t$ and then remove the force thereby slackening the rope ( no tension ) , the final velocity of the mass will be $v=at$ ( neglecting friction ) . you can determine the force applied by $f=mv/t$ . 3 - if you apply a force of $x$ newtons pulling a rope tied to a mass $m$ which i am holding , the tension on the rope is $x$ as long as the mass is not moving . if i increase my pulling force to $y$ , the resultant force , $f=y-x$ will pull you along with the mass , towards me . note that we subtract the forces because they are acting in opposite directions . the resultant force $f$ will accelerate both you and the mass towards me at a rate $a=f/ ( m+m ) $ , where $m$ is your mass ( assuming the mass of the rope is negligible ) . the tension on the rope will be equal to the magnitude of resultant force on the rope , which is $t =\lvert x-ma\rvert = \lvert x-m\times \frac{f}{m+m} \rvert= \lvert x-\frac{ ( y-x ) m}{m+m}\rvert$ . note that if your mass , $m$ is negligible , the tension of the rope becomes $x$ , whereas if the mass of the body $m$ is negligible , the tension of the rope becomes $y$ . if your mass is equal to the mass of the body $ ( m=m ) $ then the tension on the rope is $ ( y-x ) /2 = f/2$ . if i apply a pushing force $y$ directly to the body of mass $m$ , while you pull on the rope tied to it by applying a force $x$ , the resultant force on the mass will be $f=x+y$ ( in your direction ) . the two forces are added not subtracted ( since they are applied in the same direction towards you ) . the body will therefore accelerate in your direction ( newton 's second law ) under the total force $a=f/m$ and the tension on the rope will be equal to the magnitude of the resultant force , $ ( f-y ) =x$ . note in this instance , your mass is irrelevant , because the rope does not transmit my pushing force $y$ to you ( a rope does not work under compression ! ) . 4 - if two bodies of mass $m$ are tied together with a rope and are moving in opposite directions at a speed $v$ , they will each have momentum with magnitude $mv$ but in opposite directions . since neither mass is experiencing a force , they will continue to move at at constant velocities in opposite directions ( newton 's 1st law ) , until the rope between them becomes taut . at that point , they will quickly decelerate and travel back towards each other . the rate of deceleration and subsequent speed at which they will travel towards each other will depend upon the ' elasticity ' of the rope as well as the amount of ' friction ' in the rope . in the case of an ' inextensible ' rope with no friction , the rope will have a non-zero ' impulse ' tension only at the instant it is taut . the two bodies will then move towards each other with the same velocity as they were previously moving away from each other ( due to conservation of momentum ) . until you realise that tension is not the same as force , you may experience a little tension yourself as you grapple with the concept ! as an aside , you may come across some textbooks on engineering mechanics or materials which describe tension as a type of pressure or stress ( force per unit area ) as in ' tensile stress ' applied to a truss member . if we define the area as a vector whose magnitude is the cross sectional area of the material under stress and whose direction is normal ( perpendicular ) to the cross sectional area , then the resulting force is the product of stress and area . in the most general sense , since the tension may have a different effect in different directions ( anisotropic ) , the resulting force is not necessarily in the same direction as the area . in a three-dimensional euclidean space , the tension is a tensor of rank 2 . this is a linear transformation ( mapping ) with $3^{2}$ co-ordinates , something like a ( 3x3 ) matrix , which when ' multiplied ' by the " area vector " produces the resultant " force vector " ( not necessarily in the same direction ) . however , since your examples are all dealing with forces in 1 dimension only , we can treat tension as a scalar ( that is , a tensor of rank 0 ) whose magnitude is that of the force exerted by the rope under tension .
presuming that there are not nonlocal constraints , a differential operator that is polynomial in differential operators is local , it does not have to be quadratic . my understanding is that irrational or transcendental functions of differential operators are generally nonlocal ( though that is perhaps a question for math . se ) . a given space of solutions implies a particular nonlocal choice of boundary conditions , unless the equations are on a compact manifold ( which , however , is itself a nonlocal structure ) . there is always an element of nonlocality when we discuss solutions in contrast to equations . [ for the anti -locality of the operator $ ( -\nabla^2+m^2 ) ^\lambda$ for odd dimension and non-integer $\lambda$ , one can see i.e. segal , r.w. goodman , j . math . mech . 14 ( 1965 ) 629 ( for a review of this paper , see here ) . ] edit : sorry , i should have gone straight to hegerfeldt 's theorem . schrodinger 's equation is enough like the heat equation to be nonlocal in hegerfeldt 's sense . there are two theorems , from 1974 in prd and from 1994 in prl , but in arxiv:quant-ph/9809030 we have , of course with references to the originals , theorem 1 . consider a free relativistic particle of positive or zero mass and arbitrary spin . assume that at time $t=0$ the particle is localized with probability 1 in a bounded region v . then there is a nonzero probability of finding the particle arbitrarily far away at any later time . theorem 2 . let the operator $h$ be self-adjoint and bounded from below . let $\mathcal{o}$ be any operator satisfying $$0\le \mathcal{o} \le \mathrm{const . }$$ let $\psi_0$ be any vector and define $$\psi_t \equiv \mathrm{e}^{-\mathrm{i}ht}\psi_0 . $$ then one of the following two alternatives holds . ( i ) $\left&lt ; \psi_t , \mathcal{o}\psi_t\right&gt ; \not=0$ for almost all $t$ ( and the set of such t 's is dense and open ) ( ii ) $\left&lt ; \psi_t , \mathcal{o}\psi_t\right&gt ; \equiv 0$ for all $t$ . exactly how to understand hegerfeldt 's theorem is another question . it seems almost as if it is not mentioned because it is so inconvenient ( the second theorem , in particular , has a rather simple statement with rather general conditions ) , but a lot depends on how we define local and nonlocal . i usually take hegerfeldt 's theorem to be a non-relativistic cognate of the reeh-schlieder theorem in axiomatic qft , although that is perhaps heterodox , where microcausality is close to the only definition of local . microcausality is one of the axioms that leads to the reeh-schlieder theorem , so , no nonlocality .
the force caused by this disbalance ( 1 gram at 20 000 g ) is the weight of 20 kilograms , assuming coefficient of friction of 0.5 you need at least 40kg to stop it from wobbling around . somewhat more depending to the exact geometry ( the centrifuge would act as lever ) . it really sounds very unsafe . high speed centrifuges need to be shielded so that the fragments of failed rotor could not kill you .
i will make this an answer , even though it is more of a drawn out comment . as i mentioned as a comment , computing the potential energy is trivial . if you want speed , you will probably want to look at fast methods for long-range interactions . the link takes you to state-of-the-art libraries and methods , but any introductory book on computational statistical mechanics ( or molecular dynamics ) will explain ewald sums . this is to say that i do not think you will be able to analytically calculate the stuff you want , but these methods should cope well with thousands of particles ( or maybe hundreds if you want real-time performance ) . the entropy is tricky for several reasons . first of all , it is difficult to compute in general from simulations . also , i do not know if you even can in general compute absolute entropies . so what we are left is the relative entropy between two states . what are the two states ? herein lies the next problem : entropy is an equilibrium quantity . so , say you want to compare the entropies between t = 1 and t = 2 . this is to say that you want systems that are equilibriated with the charges that would occur at t = 1 , and t = 2 . to this effect you can do , for example , thermodynamic integration . this is an expensive computation .
your book is incorrect . since $p$ is a length , $pv$ and $l$ cannot be equal by dimensional analysis alone . the specific angular momentum , however , does equal $pv$ , though it is very misleading to use the letter $l$ for it . a body 's specific angular momentum is its angular momentum divided by its mass , i.e. its angular momentum per unit mass . it captures the interesting kinematics of angular momentum ( i.e. . how bodies move in space through time ) but it is quite useless when it comes to dynamics ( i.e. . how bodies interact ) .
first thing , for a rotating ball , $i=\frac{2}{5}mr^2$ . you also need to be clear on what $\omega$ you are talking about . the kinetic energy of a rotating ball is $\frac12 i_{cm}\omega_{cm}^2 + \frac12 mv_{cm}^2$ . here , $v_{cm}=v$ . but , $\omega_{cm}=v_{cm}\times \frac{r}{r}$ . since $r&lt ; &lt ; r$ , we can take the net kinetic energy to be just $\frac12 mv^2$ ; the $\frac12 i_{cm}\omega_{cm}^2$ term becomes too small to matter . the main thing is is that you need to remember that the formula " kinetic energy=rotational energy + translational energy " works only when you consider all rotations about center of mass . you cannot just keep tacking on terms for each motion you see . even though the ball is revolving around the center of the loop , we still classify this as translational motion . if you do not do this , you can easily get confused while building the expression for ke . basically , for a ball of center of mass moment of inertia $i$ , mass $m$ , radius $r$ , rotating about itself with $\omega_cm$ , revolving in a circle of radius $r$ with $\omega'$ , the energy is not $\frac12 i\omega^2+ \frac12 ( i+mr^2 ) \omega'^2+\frac12 mv^2$ , it is $\frac12 i\omega^2+ \frac12 mv^2=\frac12 i\omega^2+ \frac m ( \omega'r ) ^2$ .
in practice , the apparatus measuring the spin should be localized somewhere in space ( it cannot fill the whole universe ! ) and this fact implies that you always make a measurement of position ( actually very rough in general ) , even if you are measuring the spin . suppose that $\omega \subset r^3$ is the bounded region in $r^3$ where the apparatus is localized . the simplest ( naive ) mathematical model of the apparatus i could imagine is the following . the yes-no observable associated with the apparatus measuring , say , if the spin is directed along z+ , has the form of the orthogonal projector : $$p_{\omega} \otimes p_{z+}$$ here $p_{z^+} = |z+\rangle \langle z+|$ is the obvious projector in $c^2$ along the states with spin $z+$-directed , whereas $p_\omega$ is the operator ( orthogonal projector in $l^2 ( r^3 ) $ ) $$ ( p_\omega \psi ) ( x ) = \chi_\omega ( x ) \psi ( x ) \: . $$ this observable admits two values ( its eigenvalues ) $0=$ no and $1=$yes . yes means that the particle is found in $\omega$ and the spin is found to be directed along $z+$ . no means that the the particle is not found in $\omega$ or the spin is not along $z+$ . there is another elementary yes-no observable associated with the spin detected along the direction $-z$ , with analogous meaning . it is the orthogonal projector : $$p_{\omega} \otimes p_{z-}\: . $$ the observable associated with the spin along $z$ -- referring to this experiment -- is not the standard operator $s_z = \sigma_z/2$ ( i am assuming $\hbar =1$ ) . it is instead constructed , via spectral decomposition , taking the above elementary observables ( projectors ) into account and combining them with the corresponding values of the spin ( which turn out to be the eigenvalues of the overall observable ) . $$\gamma_{z , \omega} = \frac{1}{2}p_{\omega} \otimes p_{z+} - \frac{1}{2} p_{\omega} \otimes p_{z-} = p_\omega \otimes s_z\: . $$ you see that it includes a rough measurement of the position : it just checks if the position of the particle is in $\omega$ . to measure the spin of the particle the supports of the components $\phi_i$ must have a non-negligible intersection with $\omega$ . in general the measurement procedure of the spin , for instance along $z+$ , even affects the surviving component $\phi_{+1/2}$ . you see that , only if the support of $\phi_{+1/2}$ is completely included in $\omega$ , the wavefunction is not affected by the measurement of the spin , otherwise , after the procedure ( supposing to have found spin $+1/2$ ) , the state , up to a normalization constant , is described by : $$p_\omega \phi_{+1/2} \otimes |z+\rangle \: . $$ it is questionable if we have defined an observable $\gamma_{z , \omega}$ in that way . the point is that $\gamma_{z , \omega}$ admits a third eigenvalue , $0$ , associated with the projector $p_{r^3-\omega} \otimes i$ . actually , there is no real measurement in the region $r^3-\omega$ , since we are not assuming that there are detectors therein . we are simply using the argument : " if the particle is not found in $\omega$ it must be found outside it " . for several reasons i am always a bit suspicious to this sort of formal arguments . a more physically safe interpretation could be that we are performing a conditioned measurement of the spin . $p_\omega$ is nothing but a filter : only the particles which pass through it are measured .
" total energy of the earth " is somewhat of an odd concept , but there is no reason we can not really entertain it . it brings up some genuinely difficult questions . the right way to approach this is to define the system correctly and then identify forms of energy content and flows . things to " count " in the earth 's energy : heat content nuclear energy rotational energy gravitational energy as i look at this list , i believe that all of them are steadily decreasing . nuclear decays in the earth 's core continue over time , and this converts nuclear energy into heat content . i have heard that nuclear decay comprises a large fraction of the geothermal energy conducting through the crust , so it follows that the nuclear energy is declining at a similar rate as the heat content . the rotational energy is constantly being transferred to the moon slowly , and this is similar to the maximum theoretical tidal energy that could be extracted . heat energy , of course , is lost by blackbody radiation to space . global warming " blankets " our planet a little more , so it would initially decrease this . however , the heat content of the oceans and biosphere ( which have the capability to absorb this energy ) are small compared to the total earth . there has always been a deficit between earth 's radiated energy and the sun 's incoming energy which is from the nuclear and thermal energy of the planet . earth has always been on-net losing energy to space by radiation . an increased greenhouse gas could theoretically change this , and cause the earth to keep more of its heat . however , it is small compared to earth 's natural flows . perhaps after some past super-volcano the earth temporarily gained energy . however , that is certainly not the case today . i will point out a slight fallacy in the question : many years ego earth was hot and over time has lost energy and has got colder . if normal heat content was the only store of energy , this would be a logical connection . losing energy would mean lowering temperature . however , other stores of energy are present , notably rotational , nuclear , and gravitational . the conversion of these into thermal energy is strange . rotational turns into tidal heating , and then contributes to the radiation deficit . nonetheless , if the earth is taken as a whole , only the large center should matter significantly . that has almost certainly gotten cooler over time , in addition to releasing stored nuclear energy . if nuclear heat production was large enough , or if the mantle was insulating enough , it could have increased temperature because the heat from nuclear decay was not dispelled to the surface fast enough . venus , for instance , may have had cycles where the planet stored up extra energy , and then an event where the mantle went through a massive volcanic shift .
firstly , note that they postulate those commutation relations in the beginning of section 3.5 in order to show that they are wrong , which they demonstrate in the ensuing pages . the ultimate point is to show that one needs to impose anti-commutation relations on fermionic fields . in fact , the correct relations are postulated in equation 3.96 ; \begin{align} \{\psi_a ( \mathbf x ) , \psi_b^\dagger ( \mathbf y ) \} and = \delta^{ ( 3 ) } ( \mathbf x - \mathbf y ) \delta_{ab} \end{align} you could then ask , are these equivalent to the anti-commutation relations of the mode operators that they write in ( 3.97 ) ? namely , \begin{align} \{a^r_\mathbf p , {a^s_\mathbf q}^\dagger\} = \{b^r_\mathbf p , {b^s_\mathbf q}^\dagger\} = ( 2\pi ) ^3\delta^{ ( 3 ) } ( \mathbf p - \mathbf q ) \delta^{rs} \end{align} and the answer is yes . to show that the second set implies the first , write the fields in their integral mode expansions , compute the anti-commutator of these integral expressions , and apply the anti-commutators between modes . to show that the first set implies the second , invert the integral expressions for the fields in terms of the modes to obtain integral expressions for the modes in terms of the fields , and do the analogous thing . main point . the commutators/anti-commutators between fields are equivalent to the commutators/anti-commutators between modes .
the tl ; dr version : even if we could form a synthetic event horizon , it would not help us learn about the black hole interior . the long version : the phenomena you describe where light essentially orbits a black hole is called the " photon sphere " and it does not happen at the event horizon . the radius of a black hole , $r$ is where the event horizon is and the photon sphere where photons can orbit ( unstable orbits ) is $\frac{3}{2} r$ . you do not actually have to form an event horizon to form the photon sphere although i am pretty sure the density needed to form the photon sphere is greater than the quark degeneracy pressure and so you had still get a collapse into a black hole . unfortunately there are not any magic tricks and everything we know about general relativity and quantum mechanics says that even if we were infinitely advanced technologically we would not be able to learn anything about what happens beyond the event horizon . other than the possibility of a firewall at the horizon , there is nothing special or interesting about either the photon sphere or horizon . the only way we are going to learn about what is inside of a black hole is with a good theory of quantum gravity . no amount of making one to run experiments will help us .
the ability to " avoid the singularity " is generally regarded as a special property of the very special , stationary , exact solutions we know for black hole space-times . it has to do with the analytic continuation of the solution of einstein 's equations beyond the region that one can predict based on causal principles . ( basically , if you only know that there is a black hole , and you only know what goes on outside of the black hole , you cannot predict what happens inside the black hole ; if you actually see the black hole form , on the other hand , you may have a chance at making this prediction . ) so one should not take that possibility too seriously . ( i for one will not bet my life on it and jump into a charged black hole . ) in fact , one interpretation of the strong cosmic censorship conjecture is precisely that for generic black holes , the singularity is unavoidable once you entered the event horizon . the bound on the charge-to-mass ratio is , at the present day , more of an imprecise conjecture than a stated fact . there are several problems with that statement : in a dynamical space-time , mass-energy can radiate . so the definition of the " mass " of a black hole is already problematic . ( this is also related to the fact that mass in general relativity cannot be defined locally ; though there are a lot of work put into definitions of quasilocal mass . ) similarly , the charge of a black hole , in a general dynamical space-time , is not well-defined . what we do know is that we have a three parameter family of exact , stationary solutions to einstein 's equation depending on $m , a , q$ . because these solutions are all stationary , the mass and charge are well-defined . because these solutions are all axisymmetric , angular momentum is well-defined . and within this family we know that were the charge $q$ to exceed the mass $m$ , the formula that gives the expression of the metric tensor still makes sense as a solution to einstein 's equations , but the formula will lead to solutions with no event horizons . so we conjecture that this is a general fact , despite not knowing how to define the mass and the charge of a generic black hole . now , there are some cases where this conjecture is known to be true for dynamical solutions . for example , in spherical symmetry , if we also assume that we have , in addition to the electromagnetic field , some other " good " uncharged matter fields ( this makes the electromagnetic field non-dynamical , but the gravitational field is still dynamical ) , then we can prove such a statement using the hawking mass of the black hole . these types of statements are related to penrose-type inequalities , most of which are conjectural and only a few have been proven to hold generally . ( remark : there is however evidence that one cannot start with a subextremal black hole and then ``supercharge'' it . ) the horizon area of charged black-holes are smaller than that of uncharged ones . ( again , because of the difficulty in definition , interpret the above in terms of the known stationary black holes . ) your last statement in the third paragraph is incorrect . the answer to your general question about the interplay and the mechanism is : " no one really knows " . the problem is that general relativity , unlike classical newtonian mechanics coupled to electro-magnetism , or even special relativistic mechanics coupled to electro-magnetism , is a highly nonlinear theory . in classical electrodynamics , the linearity of the system allows you to pin-point the contributors to the dynamics : you can say that the total force acting on this particle is a sum of the gravitational force plus the lorentz force plus this-and-that . in gr , because of the non-linear feedback , it is in general impossible to disentangle the sum of the parts from the whole . ( while the equivalence principle tells you that locally in inertial frames stuff behave as it were linear , the sort of questions you were asking necessarily involve long range effects of gravity and electromagnetism . ) to summarise : there is still too much we do not know , even with regards to the basic definitions , in general relativity to be able to answer your questions . we do not have a completely satisfactory definition of black holes that can be used locally ( as oppose to teleologically ) , and we do not know what the local definitions of mass or charge should be . we certainly do not have a general description of how black holes should behave under the influence of electric charge . what we do have is a rather limited zoo of examples . this dearth of data points means that a lot of different conjectures can be made to fit those data points it is hard enough to know even which of those conjectures are right , nevermind to try and understand the principles behind such behaviour .
given that the universe is expanding because space itself is expanding , is that expansion occuring in all places and on all scales ? oversimplifying a little , the answer is that expansion can occur on any scale , but it does not occur for tightly bound systems . see this question : can the hubble constant be measured locally ? if that photon is unchanging , would not that mean that its apparent wavelength is decreasing relative to the expanding space it is travelling through ? you have this backwards . if you like , you can interpret cosmological redshifts as expansions of the space occupied by electromagnetic wave-packets . in this description , cosmological expansion does not decrease the wavelength , it increases it .
if the metal is magnetic , like iron , chromium and some others , the atoms will be orient themselves to the magnetic filed , resulting solid material which will be magnetic . the opposite phenomena explains why magnets loses their magnetism when heated ( not nescessarily to their melting point ) as well ; the atoms gets to higher energy states and will realign themselves randomly . if the molten metal is not magnetic , the material will of course not be magnetic after the magnetic field is removed , but i guess there are properties that can be affected anyway , maybe the resulting material more easily could form crystals when it becames solid , and therefore could be slightly stronger in some directions , but also more brittle . maybe .
you are missing the concept of critical mass . in a small amount of uranium , like a single pellet , some fraction of the atoms will decay spontaneously every second . when enough of this is put in proximity , then the emissions of some of the decaying atoms kick other atoms just right so that they fission too . as a result , more atoms fission than you would predict from just the probability of a single atom doing so . the main difference between a small pile of uranium and a large one is that in a large one you get a chain reaction . power plant reactors rely on this chain reaction mechanism to get the large amounts of output power . the control rods control how much the emissions of fissioning atoms can hit other atoms , thereby controlling the overall reaction rate . in reality , this description is over simplified . there can also be moderators envolved that sortof convert some of the fission results into stuff that can kick other atoms to fission when without the moderator they would not . however , that is a aside to this question .
large inflatable balls such as soccerballs , footballs and basketballs have an internal rubber bladder which needs to be inserted by hand into the carcass of the ball and inflated to the desired pressure to suit the user ( eg : basketballs can be inflated to the produce the desired bounce height for the individual user ) . also , transporting deflated balls from the factory to distributors who package and pass onto retailers saves shipping volume and hence cost . small balls such as tennis balls can be pressurized in the factory , but will lose pressure within a few onto or so after being opened ( they come in a pressurized can ) . also , the higher curvature of small balls allows them to ' spring back ' more easily when they bounce , even when they are not pressurized , although unpressurized balls do not bounce as high as inflated balls .
the index of refraction of a material can be less than 1 at high frequency , this is called " anomalous dispersion " and it happens as you cross an energy level of certain materials . it means that the phase velocity of light of a certain frequency is higher than c . if the index of refraction is constant , as it is for long wavelengths , n has to be bigger than 1 to avoid superlumimal communication . the principle of energy conservation in a static environment forbids a frequency shift for a photon , since this would add energy or take away energy , and nothing in the medium is changing with the right frequency to do that . but light entering a moving medium shifts frequency . photons can combine to make one of double the frequency in a strong light beam in a nonlinear medium , and this corresponds to making higher harmonics of the classical field .
the acceleration of planet number $n$ except for the planet $0$ will go like $-1/n^3$ because the shift of planet $0$ from zero to $\epsilon$ is equivalent to adding a " dipole " ( a pair of positive and negative mass , relatively shifted ) at the location $0$ relatively to the balanced ( but unstable ) uniform chain and this dipole acts with inverse cube , instead of the inverse law . we see that indeed the planets $+1$ and $-1$ are most affected and fastest to get some acceleration . however , planet $-1$ will move to the left , away from a potential collision . nevertheless , planet $-2$ is trying to escape from planet $-1$ , although by a smaller speed , but that will be enough to guarantee that the $0-1$ collision will be the first one . other collisions will follow . you may numerically simulate it – the problem is not integrable even for small $\epsilon$ , i think , simply because you are interested in the moments when the distance $\epsilon$ grew to a large number $o ( 1 ) $ , anyway .
one possibility is that your approach is hugely sensitive to measurement uncertainty : integrating noisy signals can be a huge problem . you might think about ways to average the measurements over time , so that they are ( hopefully ! ) more stable . another possibility is that your instrument does not output the data in quite the format that you think . have you verified that your inputs are sensible ?
to answer this question , we will first compute the values of $\lambda$ for which $\rho ( \lambda ) $ is ppt and separately compute the values for which it is entangled . let $t$ be the transpose map , such that the partial transpose map may be written as $ ( \mathbb{i}\otimes t ) $ , where $\mathbb{i}$ is the identity on $\mathbb{c}^d$ . one can show that the partial transpose maps the standard maximally entangled state into the swap operator $$ ( \mathbb{i}\otimes t ) |\psi\rangle\langle\psi|=\frac{1}{d}w , $$ where $w=\sum_{i , j}|i\rangle\langle j|\otimes|j\rangle\langle i|$ . for reference , you can take a look at john watrous ' excellent lecture notes . the swap operator has states with eigenvalue $-1$ , let 's call one of them $|w\rangle$ . we then have \begin{align} \langle w| ( \mathbb{i}\otimes t ) \rho ( \lambda ) |w\rangle and =\lambda\langle w|\frac{\mathbb{i}}{d^2}|w\rangle+ ( 1-\lambda ) \langle w|w|w\rangle\\ and =\frac{\lambda}{d^2}-\frac{ ( 1-\lambda ) }{d} . \end{align} we want this expression to be positive , which gives us the condition $$\lambda\geq\frac{1}{1+d} . $$ on the other hand , we can calculate the maximum overlap $\langle\psi|\rho_s|\psi\rangle$ that a separable state $\rho_s$ can have with $|\psi\rangle$ , such that if the overlap of $\rho ( \lambda ) $ is greater than this maximum , we know that $\rho ( \lambda ) $ is entangled . it can be shown ( see for example this review ) that in our case this maximum is precisely $\frac{1}{d}$ . therefore , $\rho ( \lambda ) $ is entangled whenever \begin{align} \langle\psi|\rho ( \lambda ) |\psi\rangle and \geq\frac{1}{d}\\ \rightarrow\frac{\lambda}{d^2}+ ( 1-\lambda ) and \geq\frac{1}{d} , \end{align} which gives the condition $$\lambda\geq\frac{d^2-d}{d^2-1} . $$ however , you can quickly check that both conditions cannot be met simultaneously , so there is no value of $\lambda$ for which $\rho ( \lambda ) $ is entangled and ppt .
two photon absorption is the nonlinear mechanism most sensitive to band gap , and it depends on the band gap being twice $\hbar \omega$ or smaller . it can be estimated using standard second order perturbation theory . see for example http://aristotle.sri.com/srini/73-jap.pdf
i am going to assume that the setup of the problem is such that the expansion of the gas is a free expansion , namely it expands into a larger container without doing work on anything because otherwise , i am skeptical that there is enough information to determine the final state . in this case , using the fact that the work done by the gas is zero ( $w=0$ ) , the first law tells us that $$ \delta u = q-w = q = 0 $$ where the last equality comes from the fact that the process is adiabatic ( $q=0$ ) . since the change in internal energy is zero , this means that $\delta t = 0$ because the internal energy of an ideal gas only depends on temperature and the number of particles in the sample . now that you know the temperature remains constant and that the pressure is given , you can use the ideal gas law to determine the final volume of the gas , and you therefore know the entire final state . if we were not given enough information to determine the work done by the gas ( like how i assumed a free expansion so that $w=0$ ) , then i do not immediately see how to proceed or if it would be possible to proceed . i will let you attempt to do the rest ; let me know if you had like more detail and/or guidance ! cheers !
well , you end up with integrals but those are very , very easy to solve for the harmonic oscillator ! since your problem is already formulated in terms of the raising and lowering operators $a_+$ and $a_-$ . recall that $$a_+ | n \rangle = \sqrt{n+1} | n+1 \rangle$$ $$a_- | n \rangle = \sqrt{n} | n - 1 \rangle$$ $$ a_+ a_- | n \rangle = n | n \rangle$$ where $|n\rangle$ is short-hand for $|\psi_n^0 \rangle$ . these relations make it almost trivial to compute matrix elements involving eigenstates of the harmonic oscillator and those operators . just as an example , let 's prove that all eigenstates have zero expectation value for $x$: we know $x$ is proportional to $a_+ + a_-$ . inserting that into the matrix element gives us $$\langle n | x | n \rangle \propto \langle n | a_+ | n \rangle + \langle n | a_- | n \rangle = \sqrt{n+1} \langle n | n+1\rangle + \sqrt{n} \langle n | n- 1 \rangle = 0$$ because eigenstates are orthogonal . edit : continuing with the derivation where you left off , you see that you get a non-zero contribution only if $m = n+1$ or $m = n-1$ . so in the infinite sum over all states $m \not= n$ , only two terms will contribute , making it possible to easily carry out that sum : just add those two non-zero terms .
the 100 w light bulb dissipates more energy per second ( 1 watt = 1 joule per second ) than the 20 w light bulb , and consequently the light emanating from the 100 w bulb carries more energy than the light emanating from the 20 w bulb . in the picture of light as an electromagnetic wave , the energy carried by the light is proportional to the square of the wave 's amplitude . the technical term for this energy is " poynting flux " . ( in fact we usually take the time-average over one period of oscillation as the definition of the energy in the wave . ) in this model , the photo-receptors in your eye are oscillators . what is oscillating ? electric charge . charges are accelerated in response to the electric field of the light : the greater the electric field ( or amplitude ) , the greater the amplitude of the oscillation , and the greater the electric currents in your eye ( and the greater the brightness ) . in the picture of light as a particle ( a photon ) , each particle carries with it an amount of energy proportional to its frequency : $e=h\nu$ , where $h$ is planck 's constant , and $\nu$ is the frequency of light . the energy flux is then the energy per photon multiplied by the flux of photons ( # of photons per unit area per second ) . so the 100 w bulb emits more photons per second than the 20 w bulb . in this model , the photoreceptors in your eye undergo chemical reactions as a result of absorbing photons . the more photons absorbed per second , the brighter the light appears .
neutron star properties indeed are determined by quantum effects , without a distinct location for individual neutrons . this special form of matter is referred as neutron-degenerate matter . and in a certain sense neutron star can be treated like a giant nucleus . neutrons in it could be seen as filling consecutive states according to pauli principle , one neutron for each state up to a certain maximal energy level . but the number of neutrons is so large that speaking of individual levels , shells etc . ( which is appropriate for the systems of few particles ) is pointless . however there are some differences : neutron star is not in a ground state like a stable nucleus ( and not in a distinct excited state like say nuclear isomer ) but is rather a thermal system , with temperature and thermodynamical properties with fermi-dirac statistics . additionally neutron star has outer regions ( actually usually having more volume than volume of mostly neutron matter ) which have a lot of ' life ' other than just neutrons : electrons , various atomic nuclei , ions , magnetic fields . . . the force holding a neutron star is gravity . . . indeed the neutron star is the result of balance between the gravity which tries to pull the matter together and the pressure of degeneracy opposing it . and because the gravity force acting on a single nucleon is so much weaker we end up with such a large object .
electricity is the flow of electrical charge - generally electrically charged particles called electrons in a wire . it can not flow through air , except in the form of electrically charged particles of air - as in a spark or lightning stroke . magnetic fields can travel in air , so you can send electricity by using it to make a magnetic field and then using the magnetic field at the other end to make electricity . this is how a transformer works - but it only works efficiently if the two sets of wire making the magnetic field are very close . you can use it for sending small amounts of electricity a short distance where a wire ( or connector ) would be difficult , such as charging an electric toothbrush - but it is not efficent for large amounts or a long distance .
start with your $\hat{h} = \hbar \omega \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) $ . i will omit hat notation from this point . the commutator then reads as \begin{equation} \left [ h , a \right ] = \hbar \omega \left [ \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) a - a \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) \right ] = \hbar \omega \left ( a^\dagger a a - a a^\dagger a \right ) , \end{equation} which is nothing but \begin{equation} \left [ h , a \right ] = \hbar \omega ( a^\dagger a - a a^\dagger ) a = \hbar \omega \left [ a^\dagger , a \right ] a , \end{equation} but we know that \begin{equation} \left [ a^\dagger , a \right ] = -1 , \end{equation} therefore \begin{equation} \left [ h , a \right ] = -\hbar \omega a , \end{equation} qed . proof of the second relation is done in the same way .
show that if $|\nu\rangle$ is an eigenvector of $n$ with eigenvalue $\nu$ , and if $a|\nu\rangle\neq 0$ , then $a|\nu\rangle$ is an eigenvector of $n$ with eigenvalue $\nu-1$ . convince yourself that the spectrum of the number operator is non-negative . assume , by way of contradiction , that there is some non-integer eigenvalue $\nu^*&gt ; 0$ and let $m$ denote the smallest integer larger than $\nu^*$ . use property 1 repeatedly ( $m$-times ) to show that $a^m|\nu^*\rangle$ is an eigenvector of $n$ with eigenvalue $\nu^*-m &lt ; 0$ . this is a contradiction qed .
to name an simple example , a 1d simple gravity pendulum with lagrangian $$l ( \theta , \dot{\theta} ) = \frac{m}{2}\ell^2 \dot{\theta}^2 + mg\ell\cos ( \theta ) $$ has one degree of freedom ( d . o.f. ) , $\theta$ , although its solution $\theta=\theta ( t ) $ has two integration constants . here , $\theta$ is the angle of the pendulum ; $\dot{\theta}$ is the ( angular ) velocity ; and $p_{\theta}:=\frac{\partial l}{\partial\dot{\theta}}=m\ell^2\dot{\theta}$ is the ( angular ) momentum . furthermore , the configuration space with coordinates $ ( \theta , \dot{\theta} ) $ and the phase space with coordinates $ ( \theta , p_{\theta} ) $ are both two dimensional spaces . in other words , it takes two coordinates to fully describe the instantaneous state of the pendulum at a given instant $t$ . so , to answer the main question : no , the corresponding velocity ( or momentum in the hamiltonian formulation ) is not counted as a separate d . o.f. . references : landau and lifshitz , mechanics : see e.g. first page of chapter 1 or first page of chapter 2 ; h . goldstein , classical mechanics : see e.g. page 13 or first page of chapter 8 in both 2nd and 3rd edition ; j.v. jose and e.j. saletan , classical dynamics : a contemporary approach : see p . 18 ; or wikipedia , either here or here .
the drag of a fluid acting on an object inside is the flow of momentum through the boundary of the object . the momentum conservation law is the entire content of the navier stokes equation , which can be written in integral form : $$ {\partial\over \partial t} \int_r \rho v^i = - \int_{\partial r} \rho v^i v\cdot \hat{n} + \int_{\partial r} ( p \hat{n} + \nu ( \rho ) \nabla v^i ) \cdot \hat{n} $$ where $\hat{n}$ is the normal to the boundary of $r$ , $p$ is the pressure , $\nu$ is the viscosity ( as a function of the density $\rho$ ) , and v is the velocity . the left hand side says that you are looking at the flow of total i-component of momentum out of region r . the first term on the right is the physical amount of momentum flowing out of the boundary of r by the flow of the fluid . the last term is the flow of momentum through the boundary of r due to forces at the edge . using the divergence theorem , you learn that $$ \int_r {\partial\over\partial t} ( \rho v^i ) + \partial_j ( \rho v^i v^j ) - \partial_i p - \nabla\cdot ( \nu \nabla v^i ) d^dx = 0$$ and you conclude that the ns equations are satisfied . $$ {\partial\over\partial t} ( \rho v^i ) + \partial_j ( \rho v^i v^j ) - \partial_i p -\nabla \cdot ( \nu \nabla v^i ) $$ if you expand this out , and use the continuity equation , you will recover the more standard forms , but this is the form in which it is most transparently a continuity equation for the momentum flow . so you see that the flow of the i-component of momentum into any region r due to the fluid , which is the i-th component of the force exerted by the fluid on whatever is inside r , is given by the boundary integral $$ f^i_r = - \int_{\partial r} \rho v^i v\cdot \hat{n} + \int_{\partial r} ( p\hat{n} + \nu \nabla v^i ) \cdot \hat{n}$$ for the case where you have a solid object that the fluid cannot penetrate , the velocity is perpendicular to the object 's surface , and the first term is zero ( obviously-- the first term describes the momentum carried along with the fluid , and this is not entering r ) . so the drag is the integral of two terms across the surface , the pressure across the object , which tells you how much the object is pushing to get the water to go around , and the gradient of the velocity , which describes how the viscosity pulls the object . for a moving object , this works at one instant to tell you how much momentum is entering or leaving the object , which is the instantaneous drag force .
what you want are fermi geodesic coordinates . from some initial point $p$ on a timelike geodesic with four-velocity $u$ , take the proper time $\tau$ as the time coordinate and select three orthonormal vectors $\{{\mathbf{e}}_\hat{\alpha}\}$ that serve as a basis for the orthogonal complement of the geodesic 's tangent vector at $p$ . parallel-transport the spatial vectors along the geodesic by solving the equation $$\nabla_u \mathbf{e}_{\hat\alpha} = 0\text{ , }$$ where the metric-compatibility of the levi-civita connection ensures that the four basis vectors stay orthonormal along the points of the geodesic . at every $\tau$ , take the $3$-manifold of spacelike geodesics going orthogonally to the given timelike geodesic . on this manifold , we can construct the usual riemann normal coordinates using our orthonormal spatial vectors : basically , pick a unit vector $\alpha{\mathbf{e}}_\hat{1} + \beta{\mathbf{e}}_\hat{2} +\gamma{\mathbf{e}}_\hat{3}$ , send out a spatial geodesic there , and then label each point on it with coordinates $ ( \tau , s\alpha , s\beta , s\gamma ) $ , where $s$ is distance along the spatial geodesic . in the geodesic case , not only does the metric minkowski form on the points of the geodesic , but the christoffel symbols also vanish there . in general , we can also use fermi-walker transport to consider an accelerated observer that is also rotating . this is described , e.g. , in mtw §13.6 . this makes the the christoffel symbols have $\gamma^{\hat{0}}_{\hat{j}\hat{0}} = \gamma^{\hat{j}}_{\hat{0}\hat{0}} = a^\hat{j}$ , the components of the acceleration four-vector , as well a term corresponding to the rotation of the observer in $\gamma^{\hat{j}}_{\hat{k}\hat{0}}$ . things you might also be interested in : tetrads and gullstrand-painlevé coordinates for the schwarzschild spacetime that correspond to the frame field of lemaître observers freely-falling from rest at infinity .
the fringe pattern of a double slit is entirely classical - it even works with ocean waves . why it still works with only one photon , and why it does not work if you look at the photon is quantum .
there are many sources of info about ads geometries on the net . i think the simplest introduction to these models is : http://www-thphys.physics.ox.ac.uk/people/maximegabella/rs.pdf this covers the randall-sundrum model , you can also check the original paper : http://arxiv.org/abs/hep-ph/9905221 the texts above address ads geometry in extra dimensional models . the paper below addresses extra dimensions in general and also talks about the ads extra dimensions : http://arxiv.org/abs/hep-ph/0404096 is this what you had in mind ? there is a lot more material to be found on the arxiv if you search it from spires ( www.inspirehep.net ) .
the line you wrote 105,000 $\frac{kg}{m*s^2}$ * 330 k * 287 $\frac{m^2}{s^2*k}$ has to read in fact $\frac{105,000 \frac{kg}{m*s^2} }{ 330 k * 287 \frac{m^2}{s^2*k}} = 1.11 \frac{kg}{m^3}$ . this comes from the gas law $p=\rho \ r \ t $ where $p$ is the air pressure and $\rho$ is the air density . solving for $\rho$ you get $\rho =\frac{p}{r t} $ from which the numerical solution follows .
the first bullet is correct , the outer shell does not contribute . this easily follows from gauss ' law . for this you use the fact that the electric field must be radial and any cylinder inside the cylindrical shell does not enclose the charge density $-\lambda$ . you might think that close to the negatively charged shell there is an additional electric field pointing in the same direction ( towards the shell ) , but this contribution is cancelled by the electric field created by the rest of the shell . the second bullet does not assign $r_b$ as $r_0$ and $r_a$ as $r$ in equation $ ( 1 ) $ . rather , it assigns $r_a$ as $r$ to calculate $v_a$ and $r_b$ as $r$ to calculate $v_b$ , which yields $v_a=\frac{\lambda}{2\pi\epsilon_0}\ln\frac{r_0}{r_a}$ and $v_b=\frac{\lambda}{2\pi\epsilon_0}\ln\frac{r_0}{r_b}$ . then $v_{ab}\equiv v_a-v_b=\frac{\lambda}{2\pi\epsilon_0}\left ( \ln\frac{r_0}{r_a}-\ln\frac{r_0}{r_b}\right ) =\frac{\lambda}{2\pi\epsilon_0}\left ( \ln ( \frac{r_0}{r_a}/\frac{r_0}{r_b} ) \right ) =\frac{\lambda}{2\pi\epsilon_0}\ln\frac{r_b}{r_a}$ .
when the electrostatic force was originally being studied , force , mass , distance and time were all fairly well understood , but the electrostatic force and electric charge were new and exotic . in the cgs system , the charge was defined in relation to the resulting electrostatic force ( it is called a franklin ( fr ) an " electrostatic unit " ( esu or ) sometimes a statcoulomb ( statc ) ) . in that system , we express the force on one charged particle by another as $f_e=\frac{q_1 q_2}{r^2}$ where the unit of charge is the esu , the unit of force is the dyne and the unit of distance is the centimeter . in the mks system ( now called si ) , we would write $f_e = k_e\frac{q_1 q_2}{r^2}$ where the unit of charge is the coulomb , the unit of force is the newton , and the unit of distance the meter . it would seem that if things are equivalent , then $k_e$ is indeed just a conversion factor , but things are definitely not equivalent . a little history is probably useful at this point . before 1873 , when the cgs system was first standardized , it finally made a clear distinction between mass and force . before that , it was common to express both in terms of the same unit , such as the pound . so if you think of it , people still say things like " i weigh 72 kg " rather than " i weigh 705 n here on the surface of earth " and they also say $1 \mathrm { kg} = 2.2\mathrm{ lb}$ confusing mass and weight ( the cgs imperial unit of mass is actually the slug ) . this is important , because there is a direct analogy to the issue of units of charge and to your question about the units of $k_e$ . the franklin is defined as " that charge which exerts on an equal charge at a distance of one centimeter in vacuo a force of one dyne . " the value of $k_e$ is assumed to be 1 and is dimensionless in the cgs system . in cgs , the unit of charge , therefore , already implictly has this value of $k_e$ built in . however in the si units , they started with amperes and derived coulombs from that and time ( $c=it$ ) . the resulting units of $k_e$ are a result of that choice . so although the physical phenomenon is the same , it is the choice of units that either gives $k_e$ dimension or not . see this paper for perhaps a little more detail on how this works in practice .
it depends very much on the hamiltonian and the external potential . there is a huge class of possible situations and of possible behaviours , and many of those do lend themselves quite often to two-level approximations . the most common , i think , is when you have a weak perturbation which oscillates at the right frequency to couple only two levels and leave out the others . in this scheme , you have a hamiltonian with a discrete spectrum ( e . g . an atom ) , and you start off in the ground state . then , if your perturbation is weak , so only single-photon transitions can occur , your perturbation is tuned exactly to the energy difference between the ground and some excited state ( not necessarily the first ) , and most importantly its bandwidth is smaller than the distance to any neighbouring states , so it has essentially no power at those frequency components , you can essentially ignore all other levels and simply treat your system as having two levels . this scheme , or variations to include more levels with suitably-tuned additional lasers , is essentially everywhere in quantum optics and any quantum information processing with matter systems . another possible scheme is to have a perturbation that can be strong enough to appreciably shift the energy levels of your initial hamiltonian , but that varies slowly enough . in the adiabatic limit of infinitely slow variations , you will stay in the " dressed " ground state , which is the ground state of the total hamiltonian at each particular instant . as you slowly increase the rate of change of the perturbation , you begin to get landau-zener transitions to the first excited state at the points where the two levels are closest . if the second excited state is far away enough , then you can increase the nonadiabaticity strongly enough that you get significant transfer of population between the ground and first excited states , while still ignoring all other levels . further than that there are , of course , more situations where you can reduce the dimensionality of interest of your problem to two or only a few states , but they get more and more specific . in general , when one is faced a difficult quantum mechanical problem , a large part of the solution process is finding the subspaces where most of the population is , and coming up with schemes to reduce the dimensionality to something more manageable ; once you do that you are essentially in a position to easily solve whatever 's left to do .
you define the density auto-correlation function as $$s_{\rho\rho} = \langle \delta \rho ( \mathbf{x}_1 ) \delta \rho ( \mathbf{x}_2 ) \rangle$$ where $\delta \rho ( \mathbf{x} ) = \rho ( \mathbf{x} ) - \langle \rho ( \mathbf{x} ) \rangle$ is deviation from the local mean value . the fourier transform of $s_{\rho\rho}$ is related to the structure-factor $$s ( \mathbf{q} ) = \langle \rho \rangle^2 ( 2\pi ) ^d \delta ( \mathbf{q} ) + \frac{1}{v}\int d^d x_1 d^d x_2 e^{-i \mathbf{q} \cdot ( \mathbf{x}_1-\mathbf{x}_2 ) } s_{\rho\rho}$$ where $\langle \rho \rangle$ is the average density of the whole system , i.e. , $v \langle \rho \rangle = \int d^d \mathbf{x} \ , \rho ( \mathbf{x} ) $ . the structure factor $s ( \mathbf{q} ) $ is related to the pair-correlation function $g ( \mathbf{x} ) $ via $$s ( \mathbf{q} ) = \langle \rho \rangle \big [ 1 + \langle \rho \rangle \int d^d \mathbf{x} \ , g ( \mathbf{x} ) e^{-i \mathbf{q} \cdot \mathbf{x}} \big ] $$ if the system is isotropic , then $g ( \mathbf{x} ) = g ( |\mathbf{x}| ) $ is called the radial-distribution function . most of these relations are already in the wikipedia page linked in the question .
is there any computer program to calculate the dose of the whole decay chain to get a picture of the artificial radiation and supports logging . i do not want to look up all the individual numbers and calculate it manually . yes , there is . mcnp will do dose calculations , among many many other things . it is a stochastic code , meaning it does random flights and interactions of radiation , which would be beta particles in your case . those are a good bit more tricky to model than gamma rays because they bounce around a lot more and also have continuous interactions as a charged particle . radioactive decay is also handled with the mcnp code , but not directly . the national labs use the monteburns code for this purpose and it is linked with origen2 , although i should specify these can change depending on the version number . you would likely get mcnp5 or mcnp6 if you ordered it now . in order to use these codes , you may submit a request to rsicc , although ideally you want to have a us university affiliation , you can also obtain it working in industry although probably at a higher price . you also would really need someone on your team with a few years on experience working with these codes . the cost of accurately obtaining the dose rate numbers will certainly affect that approach you take , as the acquisition of data , even from models , has a very real cost associated with it . it is likely that your lab will not have the budget and you will instead consult a textbook and try to combine rough estimates for the various sources you have . there are a variety of tools available for managing radiological dose to workers in a lab . in the us we would expect that a lab working with any significant sources would have an alara policy , which dictates minimizing exposure through the fundamentals of time , distance , and shielding . in a nutshell , alara reflects the precautionary assumption that there is no " safe " dose so any unnecessary or frivolous dose is unacceptable . modeling , direct radiation detection , and tld devices are common tools for radiation safety , but these are not necessary for all labs and any combination of these measures may be employed based on the needs of the specific lab . many sources are low enough hazard that none of these will be employed . however , an understanding of radiation and the types of sources you are working with should come before any of these options . if you are the primary person in your lab responsible for a source and you do not know the hazard level or the appropriate precautions that should be taken for it , then that would be a serious problem and you should consult the management in your organization or the nuclear regulating body in the area in which you reside .
here are some wordy , math-free answers . a phonon is the minimal amount of energy which can be stored in an lattice vibration in a given mode sounds good . i.e. that when a crystal vibration interacts with matter it does so by the creation/destruction of whole phonons at a time , which may also get absorbed at more or less precise locations , e.g. the energy of a single phonon is absorbed by a localized electron . a phonon is a periodic motion of the atoms in a solid , so i would argue that it is always interacting with matter since it is matter in motion . localization of phonons is a tricky business . the textbook derivations for phonons result in vibrations ( waves ) that extend through the whole material . however , they are usually treated as localized . you can make any function by adding up waves of different wavelengths ( the waves form a basis ) , so you can build up localized phonon " packets " from lattice vibrations of different frequencies . unlike photons , phonons have non-linear dispersion relations -- meaning that waves of different frequencies travel at different speeds ( unlike light where all frequencies travel at the same speed , at least in a vacuum ) , so the packets will eventually fall apart if left alone . however , they can stick together long enough that they can be thought of as particles . if the frequencies in the packet are of a narrow range , you can think of the packet as having a frequency equal to the average frequency of its constituent waves . this localization makes sense if electrons are likewise localized . if an electrons scatters with a phonon , and that electron is localized , that means the electron is only really interacting with nearby atoms . so , any lattice vibration the electron creates should be initially localized to that region too . i should add that a major form of phonon scattering is with other phonons . it turns out that you can not have two-phonon processes ( two phonons colliding and create two other phonons ) ; you can only have three-phonon processes and higher ( e . g . two phonons merge to create a third ) . you do not have to think of these processes as being localized in space . finally i would like to understand how phonon exchange can effectively establish an attractive force between electrons the atoms in a lattice are charged , so they can pull on nearby electrons . if several atoms are pulling on one electron , then the atoms are effectively pulling on each other and are brought closer together . if the electron is moving , it can leave a wake of atoms that are closer together ( a phonon ) . atoms being closer together means more positive change in an area , and that in turn can draw in another electron -- effectively attracting the electrons together . see http://hyperphysics.phy-astr.gsu.edu/hbase/solids/coop.html
topological order is a new kind of order in zero-temperature phase of quantum spins , bonsons , and/or electrons . the new order corresponds to pattern of long-range quantum entanglement . topological order is beyond the landau symmetry-breaking description . it cannot be described by local order parameters and long range correlations . however , topological orders can be described/defined by a new set of quantum numbers , such as ground state degeneracy , non-abelian geometric phases of degenerate ground states , quasiparticle fractional statistics , edge states , topological entanglement entropy , etc . fractional quantum hall states and quantum string liquids are examples of topologically ordered phases . the low energy effective theory of topological phases happen to be topological quantum field theory . in nature , topological quantum field theory always appears as the low energy effective theory of topological phase of quantum spins , bonsons , and/or electrons , etc . by definition topological phase is always a quantum phase of quantum spins , bonsons , electrons , etc . ie topological phase is always a quantum state of matter .
when the electrons pair up this opens an energy gap between the energy of the cooper pairs and the energy of the lowest quasiparticle excitation . there is a nice article discussing this effect here ( nb it is a pdf ) . the gap means that you cannot scatter a cooper pair by an arbitrarily small energy . if the energy is less than the gap energy it will not scatter because there are no available energy states for it to scatter into . that is why the electrons in a superconductor do not scatter off impurities , defects , etc . if you apply enough energy , e.g. a very high voltage , the collisions are energetic enough to scatter the cooper pairs and the superconductivity breaks down .
let 's start with the metric $$\mathrm{d}s^2 = -\left ( 1- \frac{2 g m}{r} \right ) \mathrm{d}t^2 + \left ( 1- \frac{2 g m}{r} \right ) ^{-1} \mathrm{d}r^2 + r^2 \mathrm{d}\omega^2 . $$ " absorbing " a metric coefficient really means defining a new coordinate so that in terms of the new coordinates that coefficient disappears ( or becomes one ) . then you probably give the new coordinate the same name as the old one , but that is a seperate step . so let 's try to define a new time coordinate $\tau ( t , r ) $ such that $$ \mathrm{d}\tau = \frac{\partial \tau}{\partial t} \mathrm{d}t + \frac{\partial \tau}{\partial r} \mathrm{d}r= \sqrt{1- \frac{2 g m}{r}} \mathrm{d}t . $$ if this works then the metric will be $\mathrm{d}s^2 = -\mathrm{d}\tau^2 + \cdots$ , which is what i think you are after . from the previous equation we have $\partial \tau/\partial r = 0$ , so $\tau ( t , r ) =\tau ( t ) $ , but then we need $\partial \tau/\partial t$ to be a function of $r$ , which is clearly impossible if $\tau$ itself is not a function of $r$ . so there is no $\tau$ coordinate we could introduce with the desired property ( this is actually a consequence of the nontrivial curvature ) . mathematically $\sqrt{1- \frac{2 g m}{r}} \mathrm{d}t$ is not an exact differential . you can get around this if you are willing to introduce cross terms $\mathrm{d}\tau \mathrm{d}r$ etc . it is different for the $r$ coordinate . introduce $\rho ( t , r ) $ such that : $$ \mathrm{d}\rho = \frac{\partial \rho}{\partial t} \mathrm{d}t + \frac{\partial \rho}{\partial r} \mathrm{d}r= \left ( 1- \frac{2 g m}{r}\right ) ^{-1/2} \mathrm{d}r . $$ the first term gives $\rho ( t , r ) = \rho ( r ) $ and the second gives $$\frac{\mathrm{d}\rho}{\mathrm{d} r} = \left ( 1- \frac{2 g m}{r}\right ) ^{-1/2} , $$ which integrates to ( for $r &gt ; 2 g m$ ) $$ \rho = \text{const}+r \sqrt{\frac{r-2 g m}{r}}+g m \left ( \log \left ( r \sqrt{\frac{r-2 g m}{r}}-g m+r\right ) \right ) , $$ which i invite you to try and invert for $r ( \rho ) $ . : ) in principle it can be inverted and you get the metric $$\mathrm{d}s^2 = -\left ( 1- \frac{2 g m}{r ( \rho ) } \right ) \mathrm{d}t^2 + \mathrm{d}\rho^2 + r ( \rho ) ^2 \mathrm{d}\omega^2 . $$ this has absorbed the coefficient in front of the $\mathrm{d}r$ at the expense of making the rest of the metric more complicated .
i think that wolfram is arguing that the study of cellular automata and perhaps similar computational systems could serve as an organizational principle , providing a coherent framework to look at different problem ( just like the more familiar frameworks provided by physics and chemistry ) . this explains the title of his new book , a new kind of science ( i.e. . the study of the above-mentioned structures ) . on the other hand , tegmark argues that our universe is one big mathematical structure . this may be difficult to wrap your head around , but it would mean that we are just mathematical structures that are complex enough to be self-aware and do everything we do . i assume this would not have any observational consequences ( as we cannot proof that something cannot be described by mathematics , exactly because we need mathematics to prove anything ) and is therefore purely speculative . as you can see , wolfram is calling for a new framework to conceptualize and study problems , while tegmark is positing a theory of the universe . in my opinion , these are two completely different things . disclaimer : i have not read the book by wolfram , nor was i previously familiar with tegmark 's proposal .
is $\int u u_x dy = \frac{1}{2}\int \partial_x ( u^2 ) dy = \frac{1}{2}\partial_x\int u^2 dy$ an identity ? yes , the order of differentiation in the $x$-direction and integration in the $y$-direction can be exchanged under some mild technical assumptions . where does $vu_y= -v_y u$ come from ? nowhere , it is not true . is there an identity to use in the final step ? yes , you need the given information that the boundary terms at $y=\pm\infty$ vanish , so that you can integrate by part , e.g. , $$\int ( v u_y + v_y u ) dy = \int \partial_y ( v u ) dy = \left [ v u\right ] ^{y=\infty}_{y=-\infty}=0 , $$ and $$\int u_{yy} dy = \int \partial_y ( u_y ) dy = \left [ u_y\right ] ^{y=\infty}_{y=-\infty}=0 . $$ piecing together the various parts yields $$ \partial_x\int u^2 dy = 2 \int uu_x dy= \int u ( u_x -v_y ) dy = \int ( u u_x + v u_y ) dy=\nu\int u_{yy} dy = 0 . $$ the second question supposes that the streamfunction is self-similar and takes the form : $\psi = x^a f ( \eta ) $ , $\eta = yx^b$ . which is fine , sub in for $u^2 = \psi_y^2$ and equate the power of the factor of $x$ that comes out to be zero . however , in the solution , it comes out as $2a=−b$ and i do not see how that works . you are almost there . just be careful with the algebra . $$\psi = x^af ( yx^b ) , $$ so $$ u = \psi_y = x^{a+b}f^{\prime} ( yx^b ) , $$ and therefore $$ \underbrace{\int u^2 dy}_{\mathrm{independent~of~} x} = x^{2a+b} \int f&#39 ; ( yx^b ) ^2 d ( yx^b ) = x^{2a+b} \underbrace{\int f^{\prime} ( \eta ) ^2 d\eta}_{\mathrm{independent~of~} x} . $$
the answer is no , as lubos motl has already pointed out . here i would like to make a couple of general remarks . 1 ) on one hand , the notion of superalgebras is a huge topic , which includes , e.g. , associative superalgebras and lie superalgebras . important examples of lie superalgebras are super-poincaré algebras . 2 ) on the other hand , a clifford algebra $\mathrm{cl} ( v , g ) $ over a vector space $v$ satisfies $$vw+wv=2g ( v , w ) {\bf 1} , \qquad v , w\in v . $$ in physics applications , the vector space $v$ is often a vector space spanned by a basis of gamma matrices , $$\gamma^{\mu}\gamma^{\nu}+\gamma^{\nu}\gamma^{\mu}=2g^{\mu\nu}{\bf 1} . $$ in more mathematical applications , the vector space $v$ sometimes comes with an odd grading , so that the anticommutator is a supercommutator $$ [ v , w ] =2g ( v , w ) {\bf 1} , $$ which can be viewed as a super heisenberg algebra with odd grading , and which is hence an example of a superalgebra . more generally , the vector space $v$ could be a super vector space $v=v_0\oplus v_1$ with both an even and an odd sector , leading to a notion of super clifford algebras .
there are two aspects . one is sort of trivial and comprehensible ; the other is a bit technical . the trivial reason is that $\tilde t \bar{\tilde t}$ has two " accents " on top of each other and the symbol therefore occupies too much vertical space which is undesirable because we may get overlapping characters and/or non-uniform spacing between lines . the asterisk in $\tilde t \tilde t^*$ is horizontally shifted so the vertical space is saved . however , there also exists a more technical reason . the bar $\bar t$ is not " just " the complex conjugation or a symbol of antiparticles . this bar is a symbol for the dirac conjugate spinor , $\bar t = t^\dagger \gamma_0$ . and indeed , the top quark is described by the dirac spinor $t$ and $\bar t$ is the most natural form of its complex ( plus other operations ) conjugation which makes the construction of lorentz-covariant express more intuitive . on the other hand , the top squark $\tilde t$ is not a dirac spinor ; it is a scalar field . it is more usual to denote complex conjugate scalar fields by the asterisk .
geant is a framework---which means that you use it to build applications that simulate the detector and physics you are interested in . the simulation can include all of physics and the complete detector including electronics and trigger ( i.e. . you can write your simulation so that it output a data file that looks just like the one you are going to get from the experiment 1 ) . 2 the various parts of geant are validated by being able to correctly predict the outcomes of experiments . particular models are tuned on well known physics early in the analysis of the data . this allows you to get simulated optical properties , detector gains and so on correctly matched to the actual instrument . geant is also heavily documented . read the introduction and the first two chapters of the user 's guide for application developers , which will give you the basics . after that you can delve into the hairy details in the physics and software references . there is much , much too much to cover in a stack exchange answer . ( i mean literally . . . . if i tried i would end up overrunning the 32k characters per post limit . ) it helps to know that geant4 derives from geant3 and earlier efforts . this thing has a history that goes back for decades and has been tested in thousands of experiments large and small . the use in the higgs search goes something like this we have a theory--the standard model--which tells us what coupling to expect for the particle we hope to detect we write ( and test ) a geant physics module implementing those physics . maybe more than one . we may need to write a new event generator or tweak an existing one in parallel to this effort . you construct a geant simulation of your detector . you include a simulation of the electronics , trigger and so on . 3 you simulate a lot of data from the desired channel and from possible interfering channels ( including detector noise and backgrounds ) . you are going to use a cluster or a grid for this , because it is a big problem you combine this simulated data . you run your analysis on the simulated data . 4 you extract from these results an " expected " signal . actually , you did all of the above at lower precision several times during the design and funding phase and used those result to determine how much data you would have to collect , what kinds of instrumentation densities you needed , what data rate you had to be able to support and so on ad nauseum . once you have got the data , you start by showing that : you can detect lots of well known physics in your detector ( to validate the detector and find unexpected problems ) 5 that your model correctly represents the detector response to that well known physics ( to let you debug and tune your model ) then you may need to re-run some of the " expected " processing . only then can you try to compare data to expectation . 6 1 indeed the data format is often thrashed out and debugged from the mc before the experiment is even built . 2 for big , complicated experiments like those at the lhc geant is usually paired with one or more external event generators . in the neutrino experiments i am currently working on that means genie and cry . not sure what the collider guys are using right now . 3 for speed reasons we often simulate the electronics and trigger outside of geant proper , but this decision is made on a case by case basis . 4 indeed the analyzer is often programmed and debugged from the mc output before there is real data . 5 this is also where most of the actual repetition of results in the particle physics world comes from . you will not get funding to repeat bigexper 's measurement of the wingding sum rule , but if your proposed nextgen spectrometer can do that as well as your spiffy new physics ( tm ) it helps your case with the funding agencies . 6 many of these steps will be done by more than one person/group in the collaboration to provide copious cross-checks and protection against embarrassing mistakes . ( see also , opera 's little issue last year . . . )
great question lucas . the velocity of an object in orbit around a massive body can be expressed roughly as $v ( r ) \sim \sqrt{ \frac{gm}{r}}$ the closer you are to the mass ( e . g . the black-hole ) , the bigger v ( r ) becomes . it turns out , for a black-hole like the one at the galactic center , with stars about 100 au away . . . . they travel at about 500 km/s---fast ! now , the effects of general relativity are only significant when you are near the event horizon . in this case , even though the stars are relatively ' close ' ( about $10^{15}$ cm ) , they are still almost about 1000 times further away than the event-horizon ! and so the effects of general relativity ( e . g . time dilation ) are very very small ( in this case , currently unobservable at all ) .
the earliest stars did not have planets primarily due to a lack of metals . metals in this sense is an element ( with some extra properties that are not relevant in this context ) heavier than helium . the very article that you linked to references this . this leads to the following : stars without metals tend to not last very long . metals in a star act to slow down the reaction speed of the fusion . without metals , the stars quickly get to a state where they will explode . short time scales do not allow for enough time to form planets . metals seem to be the initial building block of planets . this wikipedia article discusses the current leading theories for rocky and gas planets . basically , they both start with a rock forming that is big enough , leading to a chain effect which ends up to be a planet . rocks can not form from hydrogen and helium , making planet formation difficult .
malicious counter example the desired object is a sphere of radius $r$ and mass $m$ with uniform density $\rho = \frac{m}{v} = \frac{3}{4} \frac{m}{\pi r^3}$ and moment of inertia $i = \frac{2}{5} m r^2 = \frac{8}{15} \rho \pi r^5$ . now , we design a false object , also spherically symmetric but consisting of three regions of differing density $$ \rho_f ( r ) = \left\{ \begin{array}{l l} 2\rho\ , and r \in [ 0 , r_1 ) \\ \frac{1}{2}\rho\ , and r \in [ r_1 , r_2 ) \\ 2\rho\ , and r \in [ r_2 , r ) \\ \end{array} \right . $$ we have two constraints ( total mass and total moment of inertia ) and two unknowns ( $r_1$ and $r_2$ ) , so we can find a solution which perfectly mimics our desired object .
you have the right idea , but the question asks for situations where there is a force and no work . centripetal force does do no work but there is a force , so i is true . in iii you are exactly right , but it says there is a force and no work , which falls under the question . i think you have misunderstood the question . ii is false because a force in the opposite direction does negative work on the object . negative is not 0 . the answer is c .
the linear terms it seems you can handle . as piece of general advice , the meaning of these terms are always clearly if integrate over the momentum coordinates of each of the fields , using delta functions to preserve the value . so the non-linear term would be $$\sum_{q_1 , q_2 , q_3} g ( q_1 , q_2 , q_3 ) \psi ( q_1 ) ^*\psi ( q_2 ) \psi ( q_3 ) \delta ( -q_1+q_2+q_3 -k ) $$ maybe you can also see this way that the structure is determined by momentum conservation/translation invariance . now when i integrate this by $\int\ ! dk\ , e^{ikr}$ , the $k$ integral is resolved trivially and i am left with fourier transforms over the $q$s . since fourier transforms take multiplication to convolution , you can calculate that we get $$\int dr_{123}\ , \tilde{g} ( r-r_1 , r-r_2 , r-r_3 ) \tilde{\psi}^*\ ! ( r_1 ) \tilde{\psi} ( r_2 ) \tilde{\psi} ( r_3 ) $$ which is more or less the most general third order nonlinear term you can write . in your case you can also reduce this further by using the real space transforms of $x$ , either by plugging in directly to the first equation i wrote , or by calculating $\tilde{g}$ and plugging into the second .
they talk about contact voltages because it affects “stop voltage” as measured by their instruments , but it doesn’t affect $\delta u_{\rm stop} / \delta\nu$ since aforementioned contact voltages are assumed independent of illumination conditions . because a photovoltaic cell made of a homogeneous piece of material won’t work . a piece of conductor will not produce any voltage , whereas you will be unable to extract the current from a dielectric . no , it doesn’t . you can also read an interesting discussion at ambiguity on the notion of potential in electrical circuits ? . btw , where does wikipedia tell you about the contact voltage ? https://en.wikipedia.org/wiki/volta_potential , or… ? a posting that vaguely refers to “books and wikipedia” without any specificity demonstrates a shortage in internet communication skills , especially given that the people refers to several different things as to contact voltages . i don’t known ( i’m not an english speaker , usually ) .
in most cases , it does not really make sense to talk about a lowered effective mass caused by sitting in a gravitational potential well , since the equivalence principle says that locally the spacetime looks flat , and hence it looks like the gravitational field vanishes . however , in certain special cases , there is a sensible notion of energy that is different from your rest mass . this is when you have a timelike killing vector , which means there is a preferred time coordinate under whose flow the metric is invariant . the existence of time translation symmetry leads to a conserved energy . if $\xi^a$ is the killing vector representing the time flow , and $p^a$ is the 4-momentum of an object , the conserved energy is $$e = -g_{ab}\xi^a p^b$$ while the rest mass is $$m = ( g_{ab}p^a p^b ) ^{1/2} . $$ the schwarzschild spacetime is a classic example of this . the metric is $$ds^2 = -\left ( 1-\frac{2gm}{r}\right ) dt^2+\frac{dr^2}{\left ( 1-\frac{2gm}{r}\right ) }+r^2d\omega^2$$ since the metric is independent of time , it has a killing vector $\xi^\alpha = ( 1,0,0,0 ) $ . let 's first consider the 4-momentum of an observer initially at rest at some radius $r_0$ . initially at rest here means they start of with $p^a \propto \xi^a$ , and the normalization of $p^a$ tells us that it is $$p^\alpha = m\left ( \left ( 1-\frac{2gm}{r_0}\right ) ^{-1/2} , 0,0,0\right ) , $$ then the energy for this particle is $$e = -g_{0\alpha}p^\alpha=m\left ( 1-\frac{2gm}{r_0}\right ) ^{1/2}$$ as long as we are outside the event horizon $r=2gm$ ( which is the only place where the energy really makes sense ) , this shows that the killing energy $e$ is less than the rest mass . and if you are far away from $r=2gm$ , you can expand to first order in $gm/r$ to get $$e\approx m - \frac{gmm}{r_0}$$ which is the rest mass minus the newtonian gravitational potential energy . so in this sense the potential energy of the particle is negative , since it is killing energy is less than its rest mass energy . finally , for a particle falling in from at rest at infinity , we use the fact that killing energy is conserved along all points along the geodesic . at infinity , the metric is asymptotically minkowski , and being initially at rest means $p^a\propto\xi^a$ , hence \begin{align} p^\alpha and = m ( 1,0,0,0 ) \\ e and = m \end{align} since $e$ is conserved , we see that in this case it is always equal to the rest mass energy . you can sort of see this as a cancellation between kinetic energy and potential energy : to get the kinetic energy you need to specify who your observer is , so lets say it is the observers at constant radius . their 4-velocity is $$u^\alpha=\left ( \left ( 1-\frac{2gm}{r}\right ) ^{-1/2} , 0,0,0\right ) $$ and they would define the total energy ( rest mass plus kinetic , but not including potential energy ) as $$t = -g_{ab}u^a p^b = \frac{m}{1-\frac{2gm}{r}} \approx m + \frac{gmm}{r} . $$ to derive this , we used the fact that $e=m$ is conserved , which means that $p^t = \dfrac{m}{\left ( 1-\frac{2gm}{r}\right ) ^{1/2}}$ . if we continue to assign the potential energy $v = \frac{gmm}{r}$ , then we get $$e=t-v = m + k-v = m \implies k=v $$ so in some sense the relations you postulated hold when there is a well-defined " effective mass " i.e. killing energy , but in a general spacetime with no timelike killing vector , you will not be able to make a sensible definition of such a thing .
first of all , earthquakes are not necessarily transverse waves . both transverse and longitudinal waves are there in seismic waves . these waves depend upon both modulus of elasticity and density of medium . longitudinal p-waves ( primary ) have properties similar to that of sound and due to their compressive and rarefactive wave motion , they reach us faster than transverse s-waves ( secondary ) . these are the before-socks as what we call . it is the effect of s-waves which cause the shear fracture of the rocks ( due to high amplitudes ) . they result of rapid sideways movement of faults thereby causing rocks to shake randomly around their hypo-center . they typically travel up to 60% of the velocity of p-waves . they necessarily require the shear-modulus of the medium . but , they are the most destructive type of all . all the answers for your questions are yes . ' cause they have already been a done-deal . . ! refer wiki for a more detailed description regarding the topic . . .
part 1: conceptual/physical intuition since there is an electrostatic attraction between the 2 particles , then when they are apart they are at a higher potential energy then when they are together . here 's an analogy : physically , this situation is like having a ball at the top of a hill overlooking a valley or well . the ball will roll down the hill and that potential energy is converted into kinetic energy . when the ball reaches the bottom of the valley it will start climbing back out of the well and turn that kinetic energy back into potential , so if the ball starts at rest it only gets back to being as high as it started . however in the real world there is friction that will steal some of this kinetic energy and so the ball will roll back and forth , but eventually come to rest at the bottom of the hill . for the electron an proton you will see something similar . the 2 particles will accelerate towards each other , pass/scatter off each other ( and then repeat ) and will slowly lose energy to " friction " i.e. to radiation . part 2: specific questions 1 ) do they collide and bounce off ? ( conserving momentum ) 2 ) does the electron get through the proton , i.e. between its quarks ? the collision between the two particles is perfectly elastic . in addition the energies ( ~13ev ) are so small relative to the strong force holding together the proton that quarks are not involved in any way , and the scattering is described by rutherford scattering . 3 ) do both charges give off brehmsstrahlung radiation while moving towards each other ? the 2 particles will radiate and lose their kinetic energies . the term brehmsstrahlung is generally reserved for much higher particle energies ( > kev ) , and much larger accelerations . suppose i can control the two particles , and i bring them very close to each other ( but they are not moving so quickly as before , so they have almost no momentum ) . then i let them go : 1 ) would an atom be spontaneously formed ? you can immediately describe the 2 particles by their center of mass description ( an atom ) plus their individual attributes ( i.e. . what the particles are doing within the atom ) . assuming the 2 particles start off at rest , then they are in a bound state already because they can not escape each other ( go off to infinite separation ) due to lack of energy . however the atom will not be in it is ground state until it has decayed into the lowest level via spontaneous emission of radiation . 2 ) if anything else happens : what kind of assumptions do we make before solving the tise for an hydrogen atom ? does the fact that the electron is bound enter in it ? this is to say : is quantum mechanics ( thus solving the schrödinger equation ) the answer to all my questions here ? the tise of the atom itself will give you energy levels etc , but you will not get spontaneous emission into the ground state unless you put it in by hand ( and it would not be time independent anymore ) or also quantize the em vacuum ( which is how you derive se ) . so trying to solve it would be like solving the ball moving on the hill while ignoring friction , it will just oscillate at constant energy forever .
what you are suggesting is that there is a quantization law for magnetic fields , so that if the field is too small , it is actually exactly zero . this is not true in quantum electrodynamics , the field shrinks to zero as a power law , and the influence of the magnet is felt arbitrarily far out . but the experiment which you need to measure the field becomes larger and larger . given a particle of charge e , you can take it in a loop around the region with a magnetic field , to see interference fringes which depend on the area of the loop according to the phase law $$\delta \phi = \oint a dl = \int b da $$ to get an equal phase change , you need to make a bigger loop when you are far enough out . the question of the measurability of tiny fields is not purely academic . the phase method of detecting fields is extremely sensitive to tiny fields when you use a superconducting loop and measure magnetic fields by the phase the current gets around the loop , this is a squid . the squid can measure fields which would be too small to measure other ways , and it can be adjusted to measure fields that are infinitesimal with regards to less sensitive detectors . the philosophical position that all real quantities must be eventually discrete is not particularly useful , because the real quantities can arise from large-system-limits , and so be arbitrarily fine-grained as you get to larger and larger system sizes . for example , if the temperature is arbitrarily small , is it indistinguishable from exactly zero temperature ? this depends on the size of the system . if you make the system bigger , you can see the difference from zero temperature even finer . the limit of large number of photon exchanges , in a feynman particle view of the field , is the large number limit that makes a tiny field make sense . if the field is too tiny , you will only get a finite number of photons affecting your device , and the effects vanish . but if you make the device bigger , you become sensitive to more photons . these type of large n quantities can philosophically be real valued without any contradiction , because the grain-size for the real-number quantity is physical , and determined by the parameters of the system and measuring device .
it loses some properties , but at a , for most practical usage cases , negligible rate . i remember the rate being in the order of 1% every decade or so .
a spherically symmetric charged body behaves as a point charge . by this i mean that the electric potential outside the sphere is the same as for a point charge at the centre of the sphere . this is a consequance of gauss 's law . so suppose we have a sphere of radius $r$ and charge $q$ , the potential at a distance $r &gt ; r$ from the centre of the sphere is just : $$ v = -\frac{1}{4\pi\epsilon_0}\frac{q}{r} $$ and the voltage at the surface of there sphere is therefore : $$ v_{surface} = -\frac{1}{4\pi\epsilon_0}\frac{q}{r} \tag{1} $$ now consider your two spheres of radii $r_1$ and $r_2$ , and charges $q_1$ and $q_2$ . when the spheres are touched together they will have the same potential because current can flow between them until the potential difference is zero . that means : $$ v_1 = v_2 $$ and using equation ( 1 ) for the surface voltage we find : $$ -\frac{1}{4\pi\epsilon_0}\frac{q_1}{r_1} = -\frac{1}{4\pi\epsilon_0}\frac{q_2}{r_2} $$ or more simply : $$ \frac{q_1}{r_1} = \frac{q_2}{r_2} $$ and this answers your question because if we rearrange this to get $q_1$ we get : $$ q_1 = q_2\frac{r_1}{r_2} $$ and since we know $r_1 &gt ; r_2$ it follows that $q_1 &gt ; q_2$ . the ratio of $q/v$ is called the capacitance , or in this case it would be more precise to call it the self capacitance , and the capacitance of a conducting sphere is : $$ c = \frac{q}{v} = 4\pi\epsilon_0 r $$ another way of answering your question is to point out that the capacitance of a large sphere is greater than the capacitance of a small sphere , so when the voltages are equal the large sphere will contain a greater charge .
this is a tricky question and in my opinion some elder physicists are deliberately try to confuse students by using euphemisms when characterizing phase transitions . roughly speaking ( there might be counter examples , please comment . i am interested of finding all of them ) : first order phase transition : finite correlation length scales as e.g. $k^\alpha , \alpha = 2$ ( short or finite range interaction ) in fourier space ( 1d ) second order phase transition : infinite correlation length scales as e.g. $k^\alpha , \alpha = 1$ ( long or infite range interaction ) in fourier space ( 1d ) scale invariance ( i think soc goes here . . . ) note that there is a continuous transition with exponent $\alpha$ that escapes my mind . also $\alpha$ is dependent of dimension and probably something else ( see e.g. anne tanguy et al . from individual to collective pinning : effect of long range interactions , pre 1998 ) . also daniel fisher has a nice paper , collective transport in random media : from superconductors to earthquakes . also i just stumbled upon : http://www.tcm.phy.cam.ac.uk/~bds10/phase/introduction.pdf which has a nice overview . in general the purpose of these correlation lengths , roughness exponents and orders of phase transitions is just to find universality classes . the goal is to group the phenomena together and say " look , all these systems have properties of x , y and z . this simple model has the same properties . so by explaining the simple mode , i explain all these systems . " simple characterization of phase transitions can be found at : http://link.aps.org/doi/10.1103/revmodphys.76.663
the subscript is simply the charge of the given representation under the $u ( 1 ) $ , the hypercharge in this case . all irreducible unitary representations of $u ( 1 ) $ are one-dimensional and they just map the $e^{i\alpha}$ element of $u ( 1 ) $ ( a $1\times 1$ matrix ) to $e^{iq\alpha}$ , its power , where the exponent contains the factor of the charge $q$ . one needs to study the representations of lie groups to be able to produce decompositions such as yours . take e.g. lie algebra in particle physics by howard georgi , a book by a gut pioneer and my ex-colleague at harvard . there are also much more formally , mathematically oriented books , of course . in many of them , getting to the decompositions needed in gut model building could be hard . but if you actually understand how the representations , tensor products , charges , generators , irreducible representations etc . are defined , it is a matter of mathematical reasoning you may do yourself to derive similar decompositions . take this particular one . the $su ( 5 ) $ adjoint representation may be thought of as a hermitian matrix $m$ and the action of the group element $g\in su ( 5 ) $ is simply $$ m \mapsto g m g^{-1} $$ note that there are two copies of $g$ and $g^{-1}=g^\dagger$ because it is unitary . the hermitian $5\times 5$ matrix has 25 independent real entries . a general matrix would have 25 complex entries but those below the main diagonal are given by those above the main diagonal . and the diagonal entries are real so they contain " one-half of the real parameters " . to summarize , exactly one-half of the parameters survives the hermiticity condition . the actual adjoint representation of $su ( 5 ) $ as opposed to $u ( 5 ) $ is just 24-dimensional . the lie group element would have a unit determinant ( a priori a complex number with absolute value equal to one ) . however , $m$ is from the lie algebra so the determinant condition gets translated to ${\rm tr} ( m ) =0$ . the matrix is traceless . now , embed the standard model group to this $su ( 5 ) $ . the $su ( 3 ) $ and $su ( 2 ) $ elements are embedded simply by a block-diagonal matrix with a $3\times 3$ block in the left upper corner representing the $su ( 3 ) $ information and another $2\times 2$ block in the right lower corner representing the $su ( 2 ) $ element . this embedding makes it clear that we are splitting the vectors with 5 components to 3+2 components . similarly , the 24 or 25 entries of the square matrix is split into the squares and rectangles , $3\times 3$ , $2\times 3$ and in the next thick row $3\times 2$ and $2\times 2$ . you are supposed to draw a $5\times 5$ square and divide it into these four squares or rectangles . the 25 of $u ( 5 ) $ would simply decompose to these four representations but they would not quite be irreducible . the trace of the $3\times 3$ square block in the left upper corner and the trace of the $2\times 2$ block in the opposite corner may be separated . in the 24-dimensional representation of $su ( 5 ) $ , one of these two traces is eliminated so only one is left . it is the $ ( 1,1 ) $ representation in your list . otherwise the remaining four representations exactly correspond to the squares and rectangles . the $3\times 3$ square gives you the $ ( 8,1 ) $: it is the same square as the original for $su ( 5 ) $ but smaller . it obviously only transforms under the $su ( 3 ) $ transformations that mix the first three rows and first three columns . it is the adjoint of the $su ( 3 ) $ and $8=3^2-1$ much like $24=5^2-1$ . similarly $ ( 1,3 ) $ comes from the adjoint of $su ( 2 ) $ only affected by the $su ( 2 ) $ transformations and $3=2^2-1$ . that is the right lower square . then you have the off-block-diagonal elements which have size $3\times 2$ so they obviously have to transform as $ ( 3,2 ) $ , the tensor product of the fundamental representations of the $su ( 3 ) $ and $su ( 2 ) $ groups . there are two such representations – above the diagonal and below the diagonal . they are complex conjugate to each other . because the representations $3$ and $2$ of the smaller groups are real , the only influence of the complex conjugation is the opposite sign of the $u ( 1 ) $ charge . finally , i must discuss the subscripts . the rectangle off-diagonal representation i just mentioned has a nonzero charge $y$ , the other one has the same value with the opposite sign . all other representations obviously have to have a vanishing charge under $u ( 1 ) $ because those three terms , $ ( 8,1 ) $ , $ ( 1,3 ) $ , and $ ( 1,1 ) $ are nothing else than the adjoint representation of the standard model group and all of this group 's generators commute with the $u ( 1 ) $ , the hypercharge . so the only number left to explain is the $5/6$ charge for the rectangular representation , and $-5/6$ for the complex conjugate one on the opposite side of the square . the normalization of this charge is a convention . you may rescale the hypercharge to suit your conventions . however , the ratios of the hypercharges are physical . and the hypercharge has to be a multiple of ${\rm diag} ( +2 , +2 , +2 , -3 , -3 ) $ because it has to commute with all the matrices of $su ( 3 ) $ and $su ( 2 ) $ in the blocks , so in the blocks , it must be a multiple of the unit matrix , and the hypercharge must be traceless to be a generator of $su ( 5 ) $ , as i mentioned , and $2\times 3 - 3\times 2$ really cancels . the normalization of this $u ( 1 ) $ hypercharge generator in physics is chosen in such a way that it agrees with the conventions adopted in electroweak physics . the hypercharge $y$ is defined as the average electric charge in an $su ( 2 ) $ electroweak multiplet so that $q=y+t_3$ . sometimes , the definition $q=y/2+t_3$ is used . in gut theories , the off-diagonal blocks of the adjoint become hugely massive particles because of some gut symmetry breaking . these $ ( 3,2 ) $ states cause proton decay so they better be very heavy . we can not compare them with any known particles . however , you may take a $5$ of $su ( 5 ) $ , the fundamental representation , which should produce the quark fields which are electroweak singlet : note that this $5$ only has components that transform either under $su ( 3 ) $ or $su ( 2 ) $ . so it is a right-handed up quark , for example . its hypercharge is the average of the mutliplet but because it is the anti-down-quark only , actually , you get $y=1/3$ . similarly , the remaining two components are an electroweak doublet , like the electron and neutrino , whose $y= ( ( -1 ) +0 ) /2=-1/2$ . the states $ ( 3,2 ) $ in the decomposed $su ( 5 ) $ adjoint come from $5\otimes \bar 5$ and the off-diagonal pieces arise from $3\otimes \bar 2$ so the $y$ of the first should be added to $-y$ of the second factor and you get $y=-1/3-1/2=-5/6$ . note that the convention $q=y+t_3$ without the factor of $1/2$ was used . getting the signs and factors of $2$ right may be messy but i hope it is essentially right .
it goes out forever , but the total energy it imparts is finite . the reason is that when things fall off as the square of the distance , the sum is finite . for example : $$ \sum_n {1\over n^2} = {1\over 1} + {1\over 4} + {1\over 9} + {1\over 16} + {1\over 25} + . . . = {\pi^2\over 6} $$ this sum has a finite limit . likewise the total energy you gain from moving a positive charge away from another positive charge from position r to infinity is the finite quantity $$\int_r^{\infty} {qq\over r^2} dr = {qq\over r}$$ so there is no infinity . in two dimensions ( or in one ) , the electric field falls off only like ${1\over r}$ so the potential energy is infinite , and objects thrown apart get infinite speed in the analogous two-dimensional situation .
the basic setup is correct , conservation of energy might be the quickest way to go . $$ ( m_1 -m_2 ) g h = \frac 1 2 i \omega^2 + \frac 1 2 ( m_1+m_2 ) v^2 , i=\frac{mr^2}{2} , \omega = v/r$$ gives me one of your options as the result . the two $m$ in your formula seem to refer to different quantities .
in a liquid like water , the pressure acts in an isotropic way . that being said , imagine a slice of water in the middle tube ; what are the forces acting on this slide ? the force exerted by the pressure on the left side , and the one on the right side . the one on the left depends and the height of the water column in the left pipe . the one on the right depends on the height in the right pipe . if you want equilibrium , both have to be equal . therefore the heights have to be equal . about the pressure : pressure has dimension of force divided by surface , in common units : $n/m^2$ . the column of water on the left pipe exert a force , due to its weight ( gravity ) that is $g \rho s h$ , where $\rho$ is the volumic mass , s is the cross section of the pipe and h is the height . but the pressure is $g \rho h$ , thus independent of the cross section of the pipe . this force ( gravitational ) acts downward , but the fluid make it acts in an isotropic way , thus being directed from left to right on the slice of water ( see above ) . a good schematic explanation is available in hyperphysics . edit : altough the diameter of the left pipe is bigger , the force exerted on the " slice " of water is not higher because the pressure on a given infinitesimal volume depends only on the height of the column of water above it . imagine two simple straight vertical pipes filled with water and with equal height , one with a large diameter and the other with a smaller . it is true that the force on the bottom of the big one is higher , but the pressure will be the same , because the force acts on a larger surface .
hint : change order of integration $$ \int_0^1 dx \int_x^1 \frac{dz}{z} p_{e\leftarrow e} ( z ) \left ( f_{e} ( \frac{x}{z} , q ) - f_\bar{e} ( \frac{x}{z} , q ) \right ) $$ $$ =\int_0^1 \frac{dz}{z} \int_0^z dx \ p_{e\leftarrow e} ( z ) \left ( f_{e} ( \frac{x}{z} , q ) - f_\bar{e} ( \frac{x}{z} , q ) \right ) $$ $$ \stackrel{x=zx&#39 ; }=\int_0^1 dz \ p_{e\leftarrow e} ( z ) \int_0^1 dx&#39 ; \left ( f_{e} ( x&#39 ; , q ) - f_\bar{e} ( x&#39 ; , q ) \right ) = 0 , $$ because $$\int_0^1 dz \ p_{e\leftarrow e} ( z ) = 0 , $$ cf . formula $$ \int_0^1 dz \frac{1+z^2}{ ( 1-z ) _+} = -\frac{3}{2} , $$ mentioned between ( 17.106 ) and ( 17.107 ) in peskin and schroeder .
i cannot claim to speak for " the community " ( whoever they might be ) , but so far i have only heard positive replies from knowledgeable people . of course , people will need to read the paper in close detail , there will be discussions in seminars etc . so it'll take at least a couple of months before there will be a serious consensus . let me give a few brief comments on the proof . first of all , much of the machinery feeds off the a-theorem proof by komargodski and schwimmer ( http://arxiv.org/abs/arxiv:1107.3987 ) and its refinements ( "lpr " http://arxiv.org/abs/arxiv:1204.5221 ) . the idea is that you can turn on a " dilaton " background and then probe the theory using these dilatons . before people studied 4-pt " on-shell " ( in a technical sense ) amplitudes of these dilatons , in the new paper they look at 3-pt functions that are off-shell . these dilaton amplitudes are connected to ( essential fourier transforms of ) matrix elements of the trace $t$ of the stress tensor . if $t = 0$ then the theory is conformal invariant . the new idea is to note that $t$ has integer scaling dimension ( $\delta = 4$ ) which means that there will be logs $$e \ln \frac{-p^2}{\mu^2}$$ in these amplitudes , and in a unitary theory $e \geq 0 . $ if $e = 0$ you can show ( using unitarity ) that $t = 0$ identically so you would be done . there is a final step ( which i have not internalized yet ) where they say that in a scale-invariant theory this $e &gt ; 0$ anomaly is not allowed because unitary gives bounds on operator dimensions , which in turn control the small-distance or large-momentum behaviour of the amplitude .
intuitively , if the volume integral of a function is 0 over any arbitrary volume , the function itself must be 0 at all points in space . more concretely , consider a function for which $\int_v \ , f \ , \mathrm{d}x = 0$ for any volume $v$ . then , $\int_{v+dv} \ , f \ , \mathrm{d}x = 0$ for any infinitesimal addition to v . $$\int_{v+dv} \ , f \ , \mathrm{d}x - \int_v \ , f \ , \mathrm{d}x = \int_{dv} \ , f \ , \mathrm{d}x = f ( \text{at dv} ) = 0$$ in your case , $f = \nabla \cdot b$ , so $\nabla \cdot b = 0$ . ( note : i was a bit lazy with my notation above , so it is not a formal proof . however , it should still provide the intuitive answer to your question . )
let 's start with the evaporation of water ( or sublimation , in this case ) . carbon dioxide exists as a gas at normal temperature and pressure . if it is compressed and cooled , you make dry ice . when dry ice heats up , the solid becomes a gas directly ( any liquid is from water condensing on the dry ice ) . this process is called sublimation . water ( or ice ) can also sublimate at temperatures below freezing . because of the dry air in the freezer , ice cubes will sublimate and will disappear . also , any water vapor in the air can freeze out on cold surfaces . this is what happens in older fridges as the moist air from the room enters the freezer when the door is open ( ice will build up ) . in frost free fridges , a number of them go through a heating cycle to remove ice from the cooling surfaces . the heating cycle is not long enough to warm the contents of the freezer . water ( or ice ) becomes water vapor faster when it is warmer then the air .
in $s'$ , there is a flux of material across any surface of constant $x'$ as the material has the velocity of the $s$ frame in $s'$ . the material flux is the product of the material density in s ' and $v_{sx}$ . the invariance of charge means that the current density goes as the material flux$^1$ . $j'_x = \rho ' v_{sx} = \gamma \rho v_{sx}$ [ 1 ] extended answer to address ops comment . let $n$ be the number density , the number of particles per unit volume , in the frame of reference $s$ in which all of the particles are at rest . let $q$ be the electric charge , in $s$ , carried by each particle . in $s$ , the charge density is $\rho = qn$ . now , we know that $n ' = \gamma n$ due to length contraction . however , only if q is lorentz invariant will $\rho ' = qn ' = \gamma q n = \gamma \rho$ in $s'$ , there is a number flux across surfaces of constant $x'$: $n'_x = n'v_{sx} = \gamma n v_{sx}$ . but , since each particle carries charge , there is a charge flux , a current density $j_x = \rho ' v_{sx} = \gamma \rho v_{sx}$ . note that if $q$ were not lorentz invariant , e.g. , $q ' = \gamma q$ , then the charge and current density would not be components of a four-vector but rather a four-tensor since we would get a factor of $\gamma^2$ under a lorentz transformation .
dimensional analysis is arguably the most important technique in any physicist 's bag of tricks . it is used regularly throughout physics . variations of the principle are also used in mathematics , biology and probably in other fields as well . sticking to physics , here is an example from classical mechanics : derive kepler 's third law . the independent variables for a planet are its mass $m$ , its distance $r$ from the sun , its period $t$ , and the force $f$ acting on it . all other variables , like the velocity or acceleration , are functions of those . the only dimensionless number we can construct from the variables is $$ \frac{f t^2}{m r} \ , , $$ so this number must be the same for any planet in our solar system ( assuming that there is some functional relation between the variables ) . we also have newton 's law of gravitation , $$ f = \frac{g m m_s}{r^2} \ , . $$ together , we see that $$ \frac{t^2}{r^3} = const \ , . $$ most fields outside the sort-of fluid mechanics described above are easy enough to calculate without dimensional analysis . i encourage you to pick up polchinski 's book on string theory and work through the first few chapters . then review that statement .
yes , you have taken the analogy too far : electrons do not actually move through the wire in the way that fluids flow through a pipe . hence , there is no reason why an analog of bernoulli 's principle should apply .
after a really brief cursory review of the literature , i think that a dalitz decay is a meson decay that involves two leptons in the final state , plus a photon . a double dalitz decay has four leptons in the final state : see this paper and this paper for examples of the usage . the dalitz decay is when a virtual photon from 2 photon decay of $\pi_0$ internally converts to a real lepton pair before it gets too far , and analogous thing for other meson or higgs processes ( two electrons from an internal photon conversion , plus a neutral object ) . i guess that the usage comes from the kinematic decay product phase space is described by a dalitz plot , hence the name . i do not think it is anything deep .
i do not think the particle-anti-particle picture is a very good one to grasp what is going on . essentially , it is a consequence of zero-point energy . in classical physics , the lowest energy state of a system , it is ground state , is zero . in quantum mechanics , its a non-zero ( but very small ) value . the easiest way to see how this zero point energy arises is through an elementary problem is quantum mechanics , the quantum harmonic oscillator . the classical harmonic oscillator is a system in which there is a restorative force proportional to the displacement . for example , a spring - the further you pull the end of a spring , the more force the spring resists your pull . modeling this system in classical physics is very easy . things are a bit different in quantum mechanics - the state of a particle is specified by its wavefunction , which encodes the probabilities of finding the particle in certain positions . another property of quantum systems is that their energies come in discrete energy levels . if you are interested in how it is worked out , you can see here . you can derive the following result for the energy levels of the particle $$e=\hbar \omega ( n+\frac {1}{2} ) $$ since n specifies the energy level , setting n to zero will give us the ground state . however , we can see this is not zero - so the lowest possible state of a quantum system still contains some energy . in a practical example , liquid helium does not freeze under atmospheric pressure at any temperature because of its zero-point energy . one very important thing to note is the following - zero-point energy does not violate the conservation of energy . a common explanation is that the uncertainty principle allows particles to violate it ' if they are quick enough ! ' . this simply is not true . from the wiki page on conservation of energy : in quantum mechanics , energy of a quantum system is described by a self-adjoint ( hermite ) operator called hamiltonian , which acts on the hilbert space ( or a space of wave functions ) of the system . if the hamiltonian is a time independent operator , emergence probability of the measurement result does not change in time over the evolution of the system . thus the expectation value of energy is also time independent . the local energy conservation in quantum field theory is ensured by the quantum noether 's theorem for energy-momentum tensor operator . note that due to the lack of the ( universal ) time operator in quantum theory , the uncertainty relations for time and energy are not fundamental in contrast to the position momentum uncertainty principle , and merely holds in specific cases ( see uncertainty principle ) . energy at each fixed time can be precisely measured in principle without any problem caused by the time energy uncertainty relations . thus the conservation of energy in time is a well defined concept even in quantum mechanics . now , on to your question - in quantum field theory , all particle are modeled as excitations of fields . that is , every particle has an associated field . for the particles that carry forces , these are the familiar force fields - such as the electromagnetic field . fields take a value everywhere in space . now , in classical mechanics , this value would be zero in most places . however , as we saw above , the ground state of a quantum field is non-zero . so , even in empty space ( or ' free space' ) these fields have a a very small value . so , empty space has vacuum energy .
if the neutron decayed to a two body state ( any two body state ) the energy spectrum of the products in the neutrons rest frame would be single valued ( this is required by the conservation of energy and momentum ) . it is not . instead the electron energy spectrum is a continuum that runs from that roughly the two-body limit down to as near zero as our instruments can measure . to grab an image from the wikipedia : so , a third particle is required . that third particle is known to be uncharged ( because our detectors are sensitive to charged particles and do not see it ) . it is also known to be of very low mass because the end-point of the electron energy spectrum is almost exactly what you would expect from the two body decay . the lifetime of the neutron suggests that the interaction that is responsible for it is decay is very weak ( and going on a little further in history it obeys the principle of weak universality suggesting that it is the same interaction responsible for the decay of strange hadrons ) . the sum of these requirements constrain the properties of the third particle quite a lot , and much observation since then has shown quite conclusively that neutrinos exist .
no . momentum is conserved . since momentum is mass times the velocity of the center of mass , if the momentum is zero , the center of mass can not move . alternately , if the center of mass is already moving , it will keep moving indefinitely in a straight line when there are no external forces . however , in curved spacetime the above may not hold . see http://dspace.mit.edu/handle/1721.1/6706
the lagrangian for ordinary massless e and m is ( in natural units ) \begin{equation}\mathcal{l}=-\frac{1}{4}f^{\alpha\beta}f_{\alpha\beta}\end{equation} which gives a canonical stress-energy tensor of \begin{equation}t^{\mu\nu}=f^{\mu\alpha}f^{\nu}_{\alpha}-\frac{1}{4}\eta^{\mu\nu}f^{\alpha\beta}f_{\alpha\beta}\end{equation} now if we were give the photon a mass , the lagrangian would be \begin{equation}\mathcal{l}=-\frac{1}{4}f^{\alpha\beta}f_{\alpha\beta}+\frac{1}{2}m^{2}a_{\alpha}a^{\alpha}\end{equation} and the resulting stress-energy tensor would be \begin{equation}t^{\mu\nu}=f^{\mu\alpha}f^{\nu}_{\alpha}-\frac{1}{4}\eta^{\mu\nu}f^{\alpha\beta}f_{\alpha\beta}+\frac{1}{2}\eta^{\mu\nu}m^{2}a_{\alpha}a^{\alpha}\end{equation} as you can check , this stress-energy tensor is no longer traceless , which is solely due to the inclusion of a massive photon .
use the euler-lagrange equations ( motivated in a previous post of mine ) . these are $$ \partial_x l - \frac{\mathrm{d}}{\mathrm{d}t} \left ( \partial_{v_x} l \right ) = 0 $$ we therefore need $$ \partial_x l = -q \partial_x \phi + q \left ( \partial_x a_x v_x + \partial_x a_y v_y + \partial_x a_z v_z \right ) $$ $$ \partial_{v_x} l = m v_x + q a_x $$ $$ \frac{\mathrm{d}}{\mathrm{d}t} \left ( \partial_{v_x} l \right ) = m \dot v_x + q \dot a_x $$ putting this all together into the first equation and adding $\dot v_x m$ gives the equation for which you are looking . note especially that each component of $a$ is a function of all three coordinates : $a_i = a_i ( x , y , z ) $ ; the same goes for $\phi = \phi ( x , y , z ) $ ( where usually $a_0$ is identified with $\phi$ in relativistic electrodynamics ) .
a vector in a ( polynimial ) $gl ( 3 ) $ representation is highest weight if it is annihilated by the raising root operators $a_jk = b_j^{\dagger}b_k$ , $k&gt ; j$ . in our case , the relevant operators are $a_{12}$ , $a_{23}$ , and $a_{13}$ . we do not need to check the third case , because $a_{13} = [ a_{12} , a_{23} ] $ is given by the commutator of the two other operators . now , the check is fairly easy : $a_{12} b_1^n | 0 \rangle = ( b_1^{\dagger} ) ^{n+1} b_2| 0 \rangle = 0$ $a_{23 }b_1^n | 0 \rangle = ( b_2^{\dagger} ) ( b_1^{\dagger} ) ^{n} b_3| 0 \rangle = 0$ of course the vectors $b_1^n | 0 \rangle$ for different $n$ will belong to distinct representations . also , please notice that these vectors do not generate all the $gl ( 3 ) $ irreducible representations , because the weights of these highest weight vectors will be $ ( n_1 , n_2 , n_3 ) = ( n , 0 , 0 ) $ where , $n_i$ is the eigenvalue of the $i$-th number operator $n_i = b_i^{\dagger}b_i$
calling it a built-in voltage is something of a misnomer . people usually think of " voltage " as " what you measure with a voltmeter " . so " voltage " is normally synonymous with " electrochemical potential of electrons " ( in stat mech terminology ) and with " difference in fermi level " ( in semiconductor terminology ) . under this definition , the built-in " voltage " is not actually a voltage . then what is it ? it is what chemists call " galvani potential " , and some physicists call " electrostatic potential " . it is the line-integral of electric field . ( maybe you should call it " built-in potential " , not " built-in voltage " . ) voltage / fermi level measures the total " happiness " of electrons , the sum of all influences on the electron . the electric field ( galvani potential ) is just one of those many influences . other influences include diffusion ( entropy ) , the kinetic energy of the electron 's wave function , etc . etc . but it is the sum of all influences that determines how the electron moves . that is why it is the voltage , not the galvani potential , that determines the most important things like current flow and energy dissipation . so to summarize : the " voltage " across a p-n junction is zero , when the word " voltage " is defined in the most common and sensible and intuitive way . after all , the junction is in equilibrium ; an electron is equally happy to be on either side . for more details see my other answer : fermi level alignment and electrochemical potential between two metals going around a loop , both the voltage differences and the galvani potential differences sum to zero . but only the former is really important . for the galvani potential differences , most of them are unobservable , like the volta potential at the junction when you solder an aluminum wire to a copper wire . it is possible to figure out the galvani potential differences everywhere in a p-n junction circuit , including at the wire contacts , at the voltmeter , and so on . if you do figure them out , and add them all up , you will get zero ! but since none of those parameters matter for the circuit behavior , people rarely think about them or try to figure them out .
you need the units because though $x$ kelvin is the same as $x$ celcius it is not the same as $x$ fahrenheit . you can treat $\delta t$ as a temperature . a temperature scale has a fixed zero point ( absolute zero for the kelvin scale and the freezing point of water for the centigrade scale ) and an interval defining 1 degree . to make $\delta t$ a temperature you are just specifying that the fixed zero point is $t_1$ , that is $\delta t = 0$ when the temperature is $t_1$ . this may seem pedantic , but you would no doubt claim your height is six feet ( or whatever it is ) . i am sure you would not respond to a query by saying instead that the difference in height between the top of your head and the bottom of your feet is six feet .
it seems to me that " symmetric fission " refers to any fission process where the end products are symmetric about some point . specifically , where the end products are symmetric in their atomic mass . this website explains it quite well , but for completeness i am quoting the relevant paragraph in the article below . it is thought to be helpful for a better understanding to take the atomic numbers into consideration . the atomic numbers of the above-mentioned elements are : ru = 44 , rh = 45 , pd = 46 , ag = 47 , cd = 48 , in = 49 and sn = 50 , respectively . by noticing that the atomic number 46 of palladium is just half that of 92 of uranium , it is supposed that one uranium atom splits into two palladium isotopes . when rhodium ( atomic number 45 ) is produced with some probability ( cross section ) , silver ( atomic number 47 ) is the counter fragment . in the same way , ruthenium ( atomic number 44 ) and cadmium ( atomic number 48 ) are the pairing fragments . thus , the nuclear fission observed by nishina and kimura is highly symmetric .
gyrochronology is semi-empirical in the sense that there is some justification for the temporal dependence that you mention . there is a line of argument for the $t^{1/2}$ dependence and it can be found on pp7-8 of this pedagogical review by jerome bouvier . http://arxiv.org/pdf/1307.2891v1.pdf the basic idea is of a spherically symmetric , ionised wind that corotates with the star , held by its radial magnetic field , but which decouples from the magnetic field at some distance away from the star carrying away angular momentum . further assuming a linear dynamo model , such that the magnetic field scales linearly with rotation rate , yields an angular momentum loss rate that is proportional to rotation rate cubed . equating this to $i d\omega/dt$ ( assuming a constant moment of inertia ) leads to $\omega \propto t ^{-1/2}$ ( or $p \propto t ^{1/2}$ ) . some limitations of this model ( and gyrochronology in general ) are reviewed here ( by me ! ) . http://arxiv.org/abs/1404.7156 for instance on the pre-main-sequence you can not assume a constant $i$ ; there are different ideas for how magnetic field scales with rotation rate ; different ideas about magnetic topologies and different ideas for how the wind decouples from the magnetic field at large distances . all of these things mean it would be a surprise if rotation period was exactly proportional to the square root of time . the initial conditions also play a role at young ages - although the angular momentum losses lead to a convergence of rotation periods , this takes time , and is the dominant source of uncertainty ( along with differential rotation ) , even at older ages . so , the approach taken is to assume that rotation period can be represented by the product of a time-dependent function ( usually taken to be $t^{n}$ ) and another function representing a mass-dependence . observationally , the situation is that $n$ is found to be 0.52-0.57 by comparing the rotation periods of young sun-like stars and the sun itself . ( e . g . mamajek and hillenbrand 2008 , apj , 687 , 1264 ) . but this relationship is poorly calibrated at lower masses and also for stars older than the sun .
at tree level , the conditions for the electroweak-symmetry breaking vacuum of the mssm can be found in any of the standard review articles and are : $$ \sin ( 2\beta ) = \frac{2b_\mu}{2|\mu|^2 + m_{h_u}^2 + m_{h_d}^2} $$ $$ \frac{1}{2} m_z^2 = -|\mu|^2 - \frac{m_{h_u}^2 \tan^2\beta - m_{h_d}^2}{\tan^2\beta - 1} $$ now , the right logical way to think about this is that a top-down theory determines the values of $\mu$ and the soft-breaking parameters $m_{h_u}^2$ , $m_{h_d}^2$ , and $b_\mu$ . these are really the inputs . on the other hand , we only want to consider the slice of parameter space on which ewsb is realized as in our world , with a particular value of $m_z$ . one can choose other useful coordinates on this slice , like $\tan \beta$ , as software inputs to choose only those points on this slice of parameter space , rather than specifying the high-scale input , which in general will fail to realize correct ewsb .
suppose you have an object which is a perfect absorber/emitter of electromagnetic radiation , a.k.a. a " black body " . suppose we try to compute the electromagnetic power radiated by this object using statistical mechanics and classical electromagnetic theory . we would find that the object emits electromagnetic radiation at all wave lengths , and the wave length dependence of the emitted power would go like $$p_{\text{classical}} ( \lambda ) \ , d\lambda \propto \lambda^n$$ where $\lambda$ is wave length and $n&lt ; 0$ . the problem here is that integrating at $\lambda \rightarrow 0$ diverges and you get infinite power , which is obviously wrong . in one of your comments you said : my question is why should a heated metal , according to the electromagnetic wave theory , emit only a single frequency of light regardless of how hot it is note that this is neither true nor the real issue . the electromagnetic wave theory in classical physics predicts power at all wave lengths ( all frequencies ) . the problem is that there is too much power at low wave lengths ( high frequencies ) . this is not due to the wave nature . if you redo the calculation assuming that you still have waves , but that each mode of the electromagnetic field can only have discrete quantities of energy in it , you get a $p ( \lambda ) $ which contains finite power , and more importantly , is reproduced in experiment ! this " quantum " theory still has waves , but the energy in each wave comes in discrete chunks . to recap , the thing that makes the classical electromagnetic theory fail is that it assumes that each mode of the electromagnetic field can have any level of energy in it . this leads to an infinite radiation power for a black body . in quantum theory , each mode 's energy comes in discrete ( not continuous ) values , and this leads to a correct prediction for the wavelength-dependent radiated power . the actual form of the radiated power predicted in quantum theory is the planck law .
the normal force does decrease with angle . this does not mean that the coefficient of friction changes : we can , depending on the angle $\theta$ of the slope , split the gravitational force $f_g = mg$ acting upon a thing with mass $m$ resting on the slope into the normal force $f_n = mg \cos ( \theta ) $ and the force pointing down the slope , $f_s = mg\sin ( \theta ) $ . now , the coefficient of friction is a property of materials , and does not change with the angle - but it is the case that the friction force will decrease since it is $f_k = \mu_kf_n$ . the " greater propensity " of things to slide down steeper inclined slopes is due to the friction force decreasing , and due to the force pointing down the slope increasing with increasing angle .
for organic matter , such as bread and human skin , cutting is a straightforward process because cells/tissues/proteins/etc can be broken apart with relatively little energy . this is because organic matter is much more flexible and the molecules bind through weak intermolecular interactions such as hydrogen bonding and van der waals forces . for inorganic matter , however , it is much more complicated . i collaborate with a group who perform nanoindentation experiments on ceramics which involves forcing a nanoscopic tip into a material - essentially equivalent to cutting it with a knife . i have probed them on what is actually happening at the atomic level during these experiments and what ' hardness ' means in this context , but they simply do not know . much of the insight that we do have actually comes from computer simulations . for instance , here is an image taken from a molecular dynamics study where they cut copper ( blue ) with different shaped blades ( red ) : in each case the blade penetrates the right side of the block and is dragged to the left . you can see the atoms amorphise in the immediate vicinity due to the high pressure and then deform around the blade . this is a basic answer to your question . but there are some more complicated mechanisms at play . for a material to deform it must be able to generate dislocations that can then propagate through the material . here is a much larger-scale ( $10^7$ atoms ) molecular dynamics simulation of a blade being dragged ( to the left ) along the surface of copper . the blue regions show the dislocations : that blue ring that travels through the bulk along [ 10-1 ] is a dislocation loop . if these dislocations encounter a grain boundary then it takes more energy to move them which makes the material harder . for this reason , many materials ( such as metals , which are soft ) are intentionally manufactured to be grainy . there can also be some rather exotic mechanisms involved . here is an image from a recent nature paper in which a nano-tip is forced into calcite ( a very hard but brittle material ) : what is really interesting about it is that , initially , crystal twins form ( visible in stage 1 ) in order to dissipate the energy . this involves layers of the crystal changing their orientation to accommodate the strain . in short : it is complicated but very interesting !
you have the answer in the edit - the dwarf stars orbits will be unstable . gravitational systems with more than two bodies are inherently chaotic ( n-body problem ) . even the solar system is unstable over the long term . the heavier the bodies and the closer they are , the more gravitational energy can be exchanged . you will find stars being ejected all over the place . there is a " very far apart " limit : for example multiple star systems like a trinary system where two stars are very close and the third one is far out orbiting the central pair like a planet , or two pairs widely separated so that the other pair is approximately a point source . the record so far is 5 stars . to do that with a dozen stars would take a lot more space than the orbit of mercury . if you tried , you would find singles and pairs being ejected at random and maybe even get some spectacular dwarf collisions .
$k=\frac1{4\pi\epsilon_0}=98.9\times 10^9 \:\mathrm{n m^2 c^{-2}}$ this is coulomb 's constant , part of coulomb 's law : $$\vec f=\frac{kq_1q_2\hat{r}}{r^2}=\frac{q_1q_2\hat{r}}{4\pi\epsilon_0r^2}$$ ( $\epsilon_0$ is the permittivity of free space , and is another constant ) in a dielectric medium , you replace $\epsilon_0$ with $\epsilon_r\epsilon_0$ ( the permittivity in that medium ) in this equation , since the induced charges decrease the force . in case of a vacuum ( and to an approximation , air ) , $\epsilon_r=1$
there are two different senses of the word " mass " that will be useful to disentangle here . first , we have rest mass , which is the mass of a particle as seen in a frame where the particle is at rest ( not moving ) . we might write this mass as $m_0$ . this never changes ; it is just a property of the particle . so when you ask " does the proton lose mass faster than it gains ? " , you might be asking if it loses rest mass faster than it gains . no , because the rest mass is constant . second , we have the mass-energy of the particle , which is the time component of the four-dimensional mass-energy-momentum vector . the mass-energy just depends on the velocity of the particle relative to the observer . in particular , if the particle 's speed is $v$ , then the mass-energy is given by \begin{equation} e = \frac{m_0\ , c^2}{\sqrt{1-v^2/c^2}}~ . \end{equation} in particular , when $v=0$ , we have $e = m_0\ , c^2$ , which you will recognize . and the faster the particle is going , the closer $v$ is to $c$ , and the larger $e$ becomes . note that the energy only depends on a couple of constants , and the speed $v$ . it does not matter where the energy is coming from , or where it might be going ; it just depends on the speed at each instant . so presumably you are asking if the particle loses mass-energy faster than it gains . and the answer is : if it move faster , it is gaining mass-energy , no matter what the reason for it moving faster . you say it is gaining speed , so it is gaining mass-energy . period . you do not have to even think about the radiation , because that does not come into it directly . it may be losing energy due to radiation , but the electric field you have provided is evidently providing more power than that . if you bring in bound states ( like an atomic nucleus or an electron around a nucleus ) , you can think of a third type of " mass " , and that is the binding energy , which adds a negative amount to the rest mass of the bound state ( but not to the constituent particles ) . for example , if $m_{0 , p}$ is the rest mass of a proton , $m_{0 , n}$ is the rest mass of a neutron , and $m_{0 , d}$ is the rest mass of their bound state ( a deuteron ) , then we know that \begin{equation} m_{0 , d} &lt ; m_{0 , p} + m_{0 , n}~ . \end{equation} the " missing " mass is energy given off in the fusion of the proton and the neutron into the deuteron , which we count as binding energy , which adds a negative amount to the rest mass of the deuteron . but the rest masses of the two constituents are still exactly what they were before they came together . you can not really say that the rest mass of the deuteron has changed because it did not exist previously . so no rest mass is lost , though some mass-energy that the proton and neutron had before fusion was given off in the form of radiation . i will emphasize the point that , even in nuclear reactions , you can not really say that rest mass is lost , because you are talking about a combined particle that never existed before .
forces do not always induce motion . instead , they can be counteracted by other forces . in this case , we can clamp the wires into a form . any force created by the current is counteracted by the form , so the wires do not move . steady current , static wires , constant force . since there is no motion in this case , there is no work done . you are correct that if we allow the charges to move arbitrarily ( rather than forcing them to remain on a wire that is fixed in place ) , then the situation would be much more complex .
fluids are complicated systems described by non-linear differential equations that can not be reasonably treated in a full generality ( certainly not analytically ) . just consider the kinds of waves that propagate in the sea -- deep or shallow water , solitons , tsunami and many others ( this is not to say that these are sound waves ; but as an illustration of complicated wave behavior it should suffice ) . so , to proceed one often employs some approximation . by far the most popular one ( with many applications ) rests on the linearization of the problem around an equilibrium solution where one replaces complicated non-linear equations with second-order wave equation that describes propagation of the perturbation in the system . now , this places some consistency conditions on how big those perturbation can be so that higher-order effects can be ignored . depending on the precise form of the equations , this might require that the temperature or some other parameter stays constant ( otherwise the induced heat transport effects might destroy the linearization , for example ) roughly , isothermal processes are slow ( so that there is enough time for the transfer of the heat with the environment which keeps the temperature constant ) whereas in the adiabatic case the wave propagates so fast that the environment can not catch up with it and so no heat is exchanged ( but the temperature can change ) . i think for the most familiar types of materials where the speed of sound is quite big one uses the adiabatic approximation ( certainly for the propagation of sound in the air ) . i guess for less standard materials ( as encountered in astrophysics ) you might need isothermal approximation too but it is hard to say more than this without knowing what system you have in mind precisely .
the correct option is really option 3 . most of the time when a physicist says a theory is renormalizable , they mean that the theory is a relevant deformation of some conformal field theory . this is a non-perturbative definition . it contains the physically meaningful content that the other more technical definitions about counterterms in perturbation theory are attempting to capture . indeed , it implies them . ( however , the reverse implication is not always true . for example , perturbative qed is renormalizable in the sense of option 2 , but there is no underlying non-perturbative qed , so one can not even ask about option 3 . ) it is good practice to always try to think about the non-perturbative meaning of the physical formalism you are studying . perturbation theory is a sometimes useful tool for computations , but it can obscure the physics in a cloud of virtual technicalities . so what does it mean for a theory to be a relevant deformation of a cft ? it means that there is a cft whose observables are essentially the same as the observables in your qft , and that you can compute any correlation function in the qft as $\langle \mathcal{o} \mathcal{o}' . . . \rangle_{qft} = \langle \mathcal{o} \mathcal{o}' . . . e^{\sum_i g_i \int\mathcal{o}_i} \rangle_{cft}$ where the $\mathcal{o}_i$ are relevant operators in the cft and $g_i$ are ( dimensionful ) coupling constants . knowing that your qft is near a cft in this sense is what allows you to study the behavior of the qft 's expectation values under changes of scale , which is the heart of the renormalization group analysis . edit : first , an easy example : the free scalar field theory is a conformal field theory . this theory is basically described by $\langle \mathcal{o} \rangle_{cft} = \int \mathcal{o} ( \phi ) e^{i\int |d\phi|^2} \mathcal{d}\phi$ . in this theory , in dim > = 3 , the operator $\phi^2 ( x ) $ is relevant , so we can deform with this term and get a non-conformal field theory . the expectation value in this qft is then described by $ \langle \mathcal{o} \rangle_{qft} = \langle \mathcal{o} e^{i m^2 \int \phi^2 ( x ) dx}\rangle_{cft} = \int \mathcal{o} ( \phi ) e^{i\int [ |d\phi ( x ) |^2 + m^2\phi^2 ( x ) ] dx} \mathcal{d}\phi . $ so , not surprisingly , the theory we get by deforming the free scalar cft with a mass term is the massive free scalar . second , a subtlety that i should point out . the first equality above is not exact in most situations . the problem is the deformations we want may not be integrable with respect to the cft 's path integral measure , thanks to uv singularities . this is dealt with by regularizing . so , in most qfts , what we get is a family of approximations $\langle \mathcal{o} \mathcal{o}' . . . \rangle_{qft} \simeq \langle \mathcal{o} \mathcal{o}' . . . e^{\sum_i g_i ( \lambda ) \int\mathcal{o}_i ( \lambda ) } \rangle_{cft}$ where the relevant operators and the coupling constants depend on a cutoff scale $\lambda$ and the errors vanish as $\lambda \to \infty$ .
creating a vacuum above carbonated drinks causes the co2 to outgas faster--simply because there is no co2 above the drink to diffuse back into the liquid . in physical terms this means there is no vapor pressure of co2 above the liquid , so net movement of co2 is from the drink to the space above it . if you leave a closed carbonated drink bottle long enough , the partial pressure of co2 in the drink and in the space above the drink are the same--rate of gas that escapes the drink is equal to the rate that dissolves back into it--equilibrium . note that it is not solely about gas pressure but partial pressure of the gas you are interested in . pressurising your champagne bottle with pure air to above atmospheric and sealing it wont extend the bubblyness . you need to pressurise with co2 gas . for soft drinks you need 2 bar co2 ( in a typical coke bottle left alone for awhile , the space above the drink is almost pure co2 and is at 2bar ) . your question about pet bottles is a good one . as mentioned , without a co2 pump you cant extend the life of your sodas . squeezing a half full bottle and sealing it removes the volume of available space above the drink , so when co2 inevitably escapes the liquid to form equilibrium vapor pressure above it , less co2 is required for this to happen--if the bottle doesnt expand back . the problem is that the bottle has structurual integrity and is designed to spring back . this creates low pressure in the bottle , causing co2 to outgas faster than if you had not done all this in the first place . the upside is that equilibrium is the same--you lose the same amount of co2 from the drink as if you had not done this . to really save gas , when you squeeze the bottle and seal it , you have to find a way to keep the bottle from springing back . or just store unfinished soda in bottles where there is very little space for co2 to outgas into .
calculate $x^2 +\frac{v^2}{\omega ^2}$: $x^2 + \frac{v^2}{\omega ^2}=a^2 ( cos^2 ( \omega t ) + sin^2 ( \omega t ) ) $ $x^2+ \frac{v^2}{\omega ^2}=a^2$ $v^2=\omega ^2 ( -x^2 + a^2 ) $ which gives us $v=\pm \omega \sqrt{-x^2+a^2}$
i do not know why they should violate thermodynamics either , but they do not exist because they are static . they cannot be created at any finite time - they must have existed since the beginning of time and will exist forever . the physically realistic schwarzschild solution is created from collaps and does not have the second asymptotic region .
the capacitance is the ratio of charge on the plates over the voltage applied . $$c = \frac{q}{v} \leftrightarrow q = c \cdot v$$ the calculation you show determines the capacitance from measured voltage and charge on the plates . you basically know the result you want and determine the size of the capacitor you need . a larger capacitor , with a larger capacity , will hold a bigger charge at the same voltage . doubling the area will double the capacitance ( in case of a plate capacitor ) , so for 4 farads of capacity you get $$q = c \cdot v = 4 f \cdot 5 v = 20 c$$ the pysics works as follows : the voltage is a driving force , pushing electrons through the wires an onto the plates of the capacitor ( or sucking them off on the positive pole ) , until the mutual repulsion of the electrons leads to a balance of foces . if you have a larger plate , the charge can distribute over a larger area , there is less " pileup " and therefore a smaller " pushback force " . this is why , with larger plates , you get a bigger charge into your capacitor with the same voltage .
i do not understand question 1: where does he equate a speed to a position ? as far as question 2 is concerned , it is basically what davephd said , but maybe i can extend it a bit more saying something about the conservation of linear momentum : along the x-direction , there is no external force ( because gravity points downwards only , assuming a flat surface ) so the linear momentum of the projectile is conserved . since $p_x = mv_x$ , $v_x$ is constant .
what you calculated there is the electric flux generated by a proton , measured at a distance that is greater the size of the proton : you have used e as the charge , which means that your surface d s encloses the whole of the proton ( at least within classical electrodynamics , where we can assume that the proton has some kind of spherical/definite shape ) . the number of field lines is not something that has much of a physical meaning . basically because there could be an infinite number of field lines through a given patch of surface d s . you would need to specify the distance between these field lines , so that you can divide a length ( say 10 m ) through the separation between each field line ( say 1 m ) therefore resulting in 10 field lines . this , however , assumed that the field lines are equally spaced . check this
some instruments would require modifications in order to facilitate playing them . large instruments would need to be strapped down , and something like a double bass or drums would probably require its player to be strapped into a harness in order to prevent them from pushing themselves off as they played and floating away from the instrument . i can imagine that it would take some time to develop a technique for playing the drums without the assistance of gravity , though i suspect it would be possible . other instruments would require modification in order to be played at all . pianos , for example , rely on gravity to return the keys and hammers to their original position after releasing a key , so a piano would not work without modification . ( you would probably have to introduce springs to replace the action of gravity , and it would probably be hard to replicate the traditional feel of the keys ' movement . ) it is also possible that a grand piano would need to be re-tuned , because its bass strings are long and heavy enough that the lack of gravity might make a ( barely ) audible difference to their frequency . ( but i would expect a piano to need tuning after being launched into space anyway . ) it is surprisingly hard to think of instruments that fundamentally rely on gravity , and which therefore would not work at all . there are a few though - they include the glass harp and rainstick that you mentioned , as well as the waterphone . speaking of water-based instruments , a hydraulophone would probably work but would make a terrible mess . however , most small , hand-held string , brass and woodwind instruments would work just the same as on earth , just like the guitar does . the air pressure on a spacecraft might be different from on earth 's atmospheric pressure . you might expect brass and woodwind instruments to have tuning issues in such a situation , but in fact they would not . this is because the frequency is determined by the speed of sound . the air is a good approximation to an ideal gas , and ( perhaps surprisingly ) for ideal gases the speed of sound does not depend on pressure but only on temperature . however , if the air 's composition is different ( e . g . pure oxygen instead of an $\mathrm{o_2}$-$\mathrm{n_2}$ mixture ) then the change in density might cause tuning issues for these instruments , for the same reason that helium makes your voice go squeaky . this issue will not affect the tuning of stringed instruments , though it might affect their tone slightly by changing the resonant frequencies of their body . of course , the really interesting question is whether there are any instruments that could only be played in microgravity . i can not think of any ideas off the top of my head , but it is an interesting thing to think about .
the flux tube contains a certain energy per unit distance . therefore , pulling quarks further apart costs energy ; pulling them infinitely far apart costs infinitely much energy ( because the flux tube must then be infinitely long ) . btw , you talk about the " true " vacuum in your question . do you mean the true physical vacuum or the original , naive , perturbative one ? it is the second one that exist ( kind of ) within the proton , but i do not know if you can call that one " true " .
suppose you are using a waterwheel to do some form of work ( e . g . grind corn ) . you need a head of water to make the wheel move , and you could use either 1kg of water at a height of a million metres or you could use a million kg of water at a height of one metre . in both cases the water would do the same amount of work as it flowed through your wheel . the pressure of the water ( i.e. . the height ) is analogous to the voltage , so the water at a height of a million metres ( ok , ok , that would be above earth 's atmosphere but it is just an analogy :- ) has a voltage a million times greater than the water at a height of one metre . the current is analogous to the water flow rate . if all the water flows through the wheel in the same time then obviously the million kg of water at 1 metre has a flow rate a million times as great as the one kg of water at a million metres . although in both cases the total energy of the water is the same , they would behave very differently in practice . your water wheel probably has an ideal flow rate ( i.e. . current ) at which it is most efficient . so in practice the two cases almost certainly would not grind the same amount of corn . you had choose whichever was best suited to your purpose . the same is true of electricity . for example , resistive losses scale with current , so for transmission across the country you want a high voltage and low current . in the home a high voltage would lead to lots of fried customers , so you want a low voltage and correspondingly higher current .
i sincerely doubt that a computer could use 400+w under normal circumstances . that is the typical power rating of the transformer powering the computer . so , at peak the computer could use that much power . this peak consumption in practice means something like : cpu fully loaded gpu fully loaded hard disk at maximum throughput all the usb and firewire ports supporting powered devices wi-fi and bluetooth turned on and trasmitting also , ibm compatible motherboards and power transformers are made to support different hardware configurations . examples : some motherboards support multiple cpus all support many hard drives etc . . . you can think of it this way as well . servers normally have two psus ( for redundancy ) . both are plugged in at the same time and both could theoretically support the server by themselves . so if the maximum peak wattage of the server is 500w , you would have 2 500w psus plugged in at the same time . however , the consumption of the server will not exceed 500w .
verifying this in its entirety is tedious but good practice , so here 's the skeleton of what you need to do without giving it all away : recall the fundamental structure relation \begin{align} \{\gamma^\mu , \gamma^\nu\} = 2g^{\mu\nu} \end{align} where , as usual , there is an identity matrix implicit on the right hand side . the expressions you really want to compare are the expression on the first line which reads \begin{align} -ig_{\nu\rho} ( -ie\gamma^\nu ) i ( k_\alpha\gamma^\alpha + m ) \gamma^u i ( k_\beta \gamma^\beta+m ) ( -ie\gamma^\rho ) \end{align} and the expression on the second line which reads \begin{align} 2ie^2 ( k_\alpha\gamma^\alpha\gamma^\mu\gamma^\beta k'_\beta -2m ( k+k' ) ^\mu+ m^2\gamma^\mu ) \end{align} it is useful to match the stuff in each line that does not depend on $k$ and $k'$ first , and then match the stuff that does depend on $k$ and $k'$ . for example , the term on the first line that does not have $k$ and $k'$ in it is \begin{align} -ie^2g_{\nu\rho}\gamma^\nu\gamma^\mu\gamma^\rho m^2 \end{align} while the stuff on the second line that does not have $k$ and $k'$ in it is \begin{align} 2ie^2m^2\gamma^\mu \end{align} these things are the same since \begin{align} g_{\nu\rho}\gamma^\nu\gamma^\mu\gamma^\rho and = g_{\nu\rho}\gamma^\nu ( \{\gamma^\mu , \gamma^\rho\} - \gamma^\rho\gamma^\mu ) \\ and = g_{\nu\rho}\gamma^\nu ( 2g^{\mu\rho}-\gamma^\rho\gamma^\mu ) \\ and = 2\gamma^\mu-g_{\nu\rho}\gamma^\nu\gamma^\rho\gamma^\mu \\ and = 2\gamma^\mu-\frac{1}{2} ( g_{\nu\rho}\gamma^\nu\gamma^\rho + g_{\rho\nu}\gamma^\rho\gamma^\nu ) \gamma^\mu \\ and = 2\gamma^\mu - \frac{1}{2}g_{\nu\rho}\{\gamma^\nu , \gamma^\rho\}\gamma^\mu \\ and = 2\gamma^\mu - \frac{1}{2}g_{\nu\rho} ( 2g^{\nu\rho} ) \gamma^\mu \\ and = 2\gamma^\mu-4\gamma^\mu \\ and = -2\gamma^\mu \end{align} do a similar ( but more tedious ) thing for the stuff that depends on $k$ and $k'$ .
they do not make any claim in the paper about interpretations of quantum theory , either for the copenhagen interpretation or against many-worlds interpretations . nor does the phys . rev . lett . 101 , 20403 ( 2008 ) that they cite as their principal theoretical source . the vienna group 's stated intention here , as , i think , in a number of papers over the last few years , has been to try to rule out contextual classical particle models for experiments . this is to me something of a straw man , but they have been hacking away at it . i was going to ask that you expand your question to say why you think this experiment supports the copenhagen interpretation over other interpretations , because i do not see that to be the case , but i finally saw the off the cuff remark to that effect from zeilinger at the very end of the new scientist article [ you should have cited this ] . that is definitely not enough to rule out many-worlds interpretations without a much more substantial argument . there is not , so far , enough of an argument to have loopholes in it .
you probably know the equation for the drag , but just for record it is : $$ f_{drag} = \frac{1}{2} \rho \space c_d a \space v^2 $$ and rearranging this gives : $$ \frac{2f_{drag}}{\rho \space c_d \space v^2} = a $$ and you are given $c_d$ ( 1.4 ) and $v$ ( 3 m/s ) . i would guess you are meant to take the density of earth 's atmosphere at sea level , and a quick google gives this as 1.2754 kg/m$^3$ ( is the density of the martian atmosphere really 2/3 that of earth ? i thought it was more like 1% ) . the force is just the weight of the probe ( 40kg ) multiplied by the acceleration due to gravity at the surface of mars ( $g_{mars} = 3.75\ m/s^2$ ) . so you have everything you need to calculate $a$ . it is not obvious to me why you need the speed of the probe without the parachute . . .
when first coming into contact with the water , it is conduction . the skin feels the water colder than air because water is a better conductor of heat than air . so the skin cools faster in water than in air . for longer intervals convection will enhance the effect bringing cooler water next to the skin and removing the water already heated by the skin . the difference will persist plotting water temperatures ( equal with air temperature ) up to the temperature the skin raises the water when in contact with it . after that , the water is felt as warm .
the line integral would only work for finding a potential difference , which i presume is what you need . since the line integral needs a path ( to perform the integration ) , hence , it would need a general expression for electric field ( in your case , finding the potential requires integrating from $\infty$ to $a$ , so you had need the expression for all points between $\infty$ and $a$ ) . so , i would suggest calculating the potential at both points and then finding the difference . please give more details about how you calculated the field at a .
it depends on what you mean by " propagation of energy . " usually when we think of energy propagating , there is an non-constant distribution of energy in space , and so you can follow features of that distribution to see which way the energy is moving . for example , in an electromagnetic wave , the energy density as a function of position takes a sinusoidal form , $u\sim\sin^2 kx$ , and you can follow the peaks and troughs in this distribution over time to see the movement of the wave . but if you really think about it , energy is not a vector , so it does not have an inherent direction , and so in general there may not be any propagation to speak of . without some feature in the energy distribution to follow , the whole idea of propagation becomes essentially inapplicable . in that case , all you can say about energy is how the amount of it in some particular region changes with time . that quantity is related to the divergence of the poynting vector , $$\frac{\partial u}{\partial t} = -\nabla\cdot\vec{s}$$ ( in empty space ) , so as far as the energy distribution is concerned , it is only the divergence of $\vec{s}$ that matters , not its value . from wikipedia : the poynting vector is usually interpreted as an energy flux , but this is only strictly correct for electromagnetic radiation . the more general case is described by poynting 's theorem above , where it occurs as a divergence , which means that it can only describe the change of energy density in space , rather than the flow . however , the poynting vector does have a different interpretation as the momentum density of the em field . momentum is a vector , so it does have a direction , and you can meaningfully talk about a " momentum propagation " ( loosely speaking ) . when you calculate that $\vec{s} \neq 0$ , what you are finding is that the em field of this configuration does have a nonzero momentum density . however , the momentum normally has no effect because it never gets transferred to anything . if you put something that interacts with the em field in its way , though , the object will experience radiation pressure .
dear onkar , they are the same thing . the ( massless or at least light ) scalar fields are what parameterizes the moduli spaces - in any theory - and the metric on the moduli space ( which is a mathematical concept that does not " a priori " exist in physics ) is defined from the ( ultimate low energy ) kinetic terms of these scalar fields . in a supersymmetric theory , these kinetic terms $$\frac{1}{2}g_{ij} ( \phi_a ) \partial_\mu \phi^i \partial^\mu \phi^j$$ are determined from the kähler potential , $g_{i\bar j}\sim\partial_i \partial_{\bar j} k$ because of the basic supersymmetric calculus . there is a lot of nontrivial maths here but the particular statement you are quoting is a tautology .
i will undertake a hand waving answer and maybe since the question will come to the top somebody knowledgeable will give a full answer . virtual particles are the province of quantum mechanics and led to the development of quantum field theory with creation and annihilation operators . this led to the realization that the vacuum , in the qft description is not " empty " but is like a sea where virtual particles continuously create and annihilate with no loss of energy . the vacuum is a " ground state " . interestingly enough this field theoretic description with creation and annihilation operators does not correspond one to one with particle physics . back in 1963 i sat through a field theoretical course for nuclear physics where creation and annihilation operators acted on nuclear levels . but i digress . now the curvature of space is a unique proposal of general relativity . general relativity has not been quantized in an irrefutable manner . string theorists believe that they have managed to do that , but i leave it to them to describe what a sea of virtual particles in a string universe is like . trying to naively say : suppose gravity is quantized in the classic qft manner and gravitons exist in the vacuum sea too , the answer would be : the higher the curvature the more the distribution of the particle antiparticle sea would be weighted statistically towards heavier pairs , due to energy considerations with respect to flat space .
if you connect a battery with a voltage $v$ between two points that already have a potential difference of $v$ , then no current will flow out of the battery . of course if you remove the other source of the potential difference , then the battery will start to push out the current needed to keep that voltage drop across the load . when you say that the the cell is in parallel with the potentiometer you make a mistake : the cell is in parallel with the part of potentiometer and with the dc source ( which is in series with the remaining part of the potentiometer ) .
take its differential form : $$\mathrm{d} \rho = \frac{1}{4/3 \pi r^3} \mathrm{d}m - \frac{m}{4\pi r^4}\mathrm{d}r$$ the greatest variation in $\rho$ will be achieved when all the terms add positively $$\delta \rho =\frac{1}{4/3 \pi r^3} \delta m + \frac{m}{4\pi r^4}\delta r$$ factor of $-3$ appears as matter of integration . which you can recast into $$\frac{\delta \rho}{\rho} = \frac{\delta m }{m} + 3\cdot\frac{\delta r}{r}$$ this will be accurate if the uncertainties are small enough compared to the value . otherwise , you can use the more general formula . for a function $f ( \{x_i\} ) $ , where $x_i$ is a quantity you measured with an uncertainty $\delta x_i$ , then $$\delta f = \sqrt{\sum \limits_{i=1}^{n} \left ( \frac{\partial f}{\partial x_i}\right ) ^2 ( \delta x_i ) ^2}$$
this is one of those questions that is more subtle than it seems . in gr the velocity of light is only locally equal to $c$ , and we ( approximately ) schwarzschild observers do see the speed of light change as light moves to or away from a black hole ( or any gravity well ) . famously , the speed that radially moving light travels falls to zero at the event horizon . so the answer to your first question is that yes gravity does slow the light reaching us from the sun . to be more precise about this , we can measure the schwarzschild radius $r$ by measuring the circumference of a circular orbit round the sun and dividing by 2$\pi$ . we can also measure the circumference of the sun and calculate its radius , and from these values calculate the distance from our position to the sun 's surface . if we do this we will find the average speed of light over this distance is less than $c$ . however suppose we measured the distance to the sun 's surface with a ( long ) tape measure . we had get a value bigger than the one calculated in the paragraph above , and if we use this distance to calculate the speed of the light from the sun we had get an average speed of $c$ . so i suppose the only accurate answer to your question is : it depends . re your other question , assuming the spacetime around the sun is described by the schwarzschild metric , the time dilation at the surface of the sun is given by : $$ \text{time dilation factor} = \frac{1}{\sqrt{1 - r_s/r}} $$ where $r_s$ is the radius of a black hole with the mass of the sun and $r$ is the radius of the sun . the former is about 3,000m and the latter about 700,000,000m so i calculate the time dilation factor to be around 1.000002 and this is too small to measure directly . however you can interpret gravitational lensing to be due to changes in the speed of light , and since we can measure the gravitational lensing due to the sun you can argue we have measured its effect on the speed of light . this is not really true as what gravitational lensing measures is the spacetime curvature . however the change in the speed of light ( measured by a schwarzschild observer ) is an aspect of this .
the primary reason is that surface tension arises from attractive interactions between the molecules of a liquid a . in the bulk there are other molecules of liquid a all around so the interactions are balanced . at the interface however , there is a lack of molecules a on one side which results in a net force that pulls the surface molecules in the direction of the bulk . since this is a pulling action of the liquid itself arising from the attractive forces ( not a pushing action of the other liquid ) it is termed a tension .
firtree is correct - i will just try to flesh out his answer a bit . ( 1 ) your last question first - charge ( or current ) at a point is like mass at a point . for finite masses , if you want to see how much is contained in an infinitely small volume ( i.e. . , at a point ) , the answer is zero . so instead , people consider the mass density which can have non-zero values at a point . you probably understand the relationship between mass and ( mass ) density quite well . similarly for a finite current , the amount of current at a point ( i.e. . , in an infinitely small volume ) is zero . the current density is the limit of the amount of current in a small volume around a point as the volume goes to zero - just like mass density , but with current instead . so , just as one speaks of mass density at a point and not mass at a point ( for extended bodies ) , one speaks of charge density at a point and not charge at a point or current density at a point and not current at a point ( we are ignoring point particles for now - they do fit into this formalism , but you need dirac delta functions ) . ( 2 ) now , in analogy with mass flow , your picture of flow of charge is correct . mass density times velocity gives a mass current density . $\vec{j}_{m} ( t , \vec{x} ) :=\rho_{m} ( t , \vec{x} ) * \vec{v} ( t , \vec{x} ) $ . if you have a mass current density $\vec{j}_{m}$ and want to know the mass flow $\dot{m}$ through some area a , then you take \begin{equation} \dot{m} = \int \vec{j}_{m} \cdot d\vec{a} \end{equation} similarly , charge density times velocity gives a charge current density . $\vec{j}_{q} ( t , \vec{x} ) :=\rho_{q} ( t , \vec{x} ) * \vec{v} ( t , \vec{x} ) $ . if you have a charge current density $\vec{j}_{q}$ and you want to know the flow of charge $\dot{q}$ through some area a , then you take \begin{equation} \dot{q} = \int \vec{j}_{q} \cdot d\vec{a} \end{equation} so , the picture in your head is quite close - just picture charge or a fluid of charged particles flowing .
in a sense this could be an interpretation of gravitational waves . in this case , anything with energy or mass or momentum ( relating by the stress-energy tensor in gr ) could bend space-time producing gravitational waves . hopefully i am understanding you correctly .
there are a few things that keep saturn 's rings roughly the way they are . first , saturn 's d ring actually is " raining " down on saturn currently . but , the phenomenon of shepherd moons prevents the vast majority of material from leaving the other rings : " the gravity of shepherd moons serves to maintain a sharply defined edge to the ring ; material that drifts closer to the shepherd moon 's orbit is either deflected back into the body of the ring , ejected from the system , or accreted onto the moon itself . " ( quote from wikipedia ) besides this , the majority of the particles within the ring system have almost no motion towards or away from saturn ; no motion towards the planet prevents them from being lost . second , saturn 's rings cannot clump into " full-fledged " moons , but they can clump into moonlets up to several hundred meters to a few kilometers across . at last count , i think there were over 200 that had been found , and they also come out of numerical simulations . beyond these larger moonlets , quasi-stable clumps and clusters of ring particles form with great frequency the farther you get from saturn . these clusters of particles are constantly changing size , trading material , etc . , and so there is no time for them to become solid and cohesive . this gets into the idea of the roche limit and hill spheres . the basic idea of the roche limit is that the closer you are to a massive object , the more tidal forces are going to tear you apart ( or prevent you from forming to begin with ) . hill spheres are related , where the idea is at what point you are gravitationally bound to one object or another . if you are within saturn 's hill sphere versus a moon 's hill sphere , you are going to be pulled to saturn . with both concepts , you will need to have a moon forming farther away from saturn than its rings are now to actually be stable . you can see the effects of these by looking at n-body dynamical simulations of the rings . this was my research for a year and a half , and it culminated in over a hundred simulations , many of which i made movies of , and then i posted them on one of my personal websites . if you go to it , scroll down and take a look at one of the c ring simulations , b ring simulations , and a ring simulations ( warning - the movies are a bit big ) . you should choose ones with a large &tau ; value and &rho ; of 0.85 because those will show clumping better . what you will see is that , in the c ring , almost no clumping occurs . go farther from saturn into the b ring and you will see a spider web start to happen of strands of clumps of particles . then if you go to the farther away a ring the strands are fragmented more into clusters . ( note on the movies : the " l " value next to each one is how large the simulation cell is on a side , in meters . so you are just looking at a very small region of the ring . it is set so that the center of the cell does not move , so you had imagine that whole thing orbiting around saturn . )
magnetic fields do no work since $$ \left ( \frac{d{\bf{r}}}{dt} \times {\bf{b}} \right ) \bullet d{\bf{r}} = 0 , $$ so they cannot change a charged particle 's speed ( they can , however , change the direction of its velocity ) .
the article that describes this research does discuss their favored model for explaining these features . the article starts off by talking about earth analogues , which are always helpful . in this case , it talks about brines rising in antarctic ice shelves ( brines being just what they are in cooking -- salty water , and salt lowers the freezing point of water by up to and over 10°c ) . the model the authors suggest starts with a thermal plume that heads from the liquid interior up through the ice and melts some of the overlying ice . the melting causes the surface to collapse a bit and fracture ( volume of water is &lt ; volume of ice ) , and this confines the water to that region . the thermal plume was transient/temporary and sinks back down , and the water at the base of the " lake " re-freezes . the article then talks a lot about the morphology ( what it looks like ) in images and theory , showing that the two intersect . as the lens of water lies below the surface , the surface fractures and calves , like glaciers calving on earth . material fills in with an impurity-rich matrix and freezes . when the lake underneath refreezes , since the volume of the ice is greater than water , it creates a positive convex topographic relief ( jumbled because of the previously calved blocks ) as opposed to the concave one when there was liquid below . from what i can tell , the article is not actually suggesting that all these areas presently contain liquid water lakes underneath them . they do suggest one particular area , thera macula , to be presently active , however . their argument for this is based on its morphology : " the large concentric fracture system encircling thera macula resembles those of collapsing ice cauldrons , and , given the absence of a continuous moat , suggests that subsurface melt and ice disaggregation is forming thera macula , rather than the collapse of a dome . " they suggest , " today , a melt lens of 20,000–60,000 km 3 of liquid water probably lies below thera macula ; this equates to at least the estimated combined volume of the great lakes . " the timescale for this amount of liquid to re-freeze is 100,000-1,000,000 years . the type of terrain that this creates ( "chaos terrain" ) is spread throughout the moon , so if their model is correct , then it has had many intra-ice lakes throughout its history , and likely throughout its recent history . the authors conclude with , " our analyses suggest that ice–water dynamics are active today on europa , sustaining large liquid lakes perched in the shallow subsurface . "
different boundary conditions represent different models of cooling . the first one states that you have a constant temperature at the boundary . this can be considered as a model of an ideal cooler in a good contact having infinitely large thermal conductivity the second one states that we have a constant heat flux at the boundary . if the flux is equal zero , the boundary conditions describe the ideal heat insulator with the heat diffusion . robin boundary conditions are the mathematical formulation of the newton 's law of cooling where the heat transfer coefficient $\alpha$ is utilized . the heat transfer coefficient is determined by details of the interface structure ( sharpness , geometry ) between two media . this law describes quite well the boundary between metals and gas and is good for the convective heat transfer . http://www.ugrad.math.ubc.ca/coursedoc/math100/notes/diffeqs/cool.html the last one reflects the stefan-boltzman law and is good for describing the heat transfer due to radiation in vacuum
will $\rho$ always behave well ? no , not at all . this is physics , not mathematics -- if something is wrong with solutions , the explanation is simple : too bad for equations , probably approximations behind them lack some process or the mathematical description starts to be invalid . in your case , it is obvious that a cloud with no angular momentum will be collapsing into one point , yet this is not a problem -- it is just " physical " that there will be some repulsion that would start to be significant in high density regions invalidating this simplified model .
if you look at it from the quantum mechanical point of view you might invoke the energy-time uncertainty relation , which for a foton might become : $\delta\omega\delta t \geq \frac{1}{2}$ . which says that for very short times the uncertainty on the frequency can become very large .
yes . in mathematics , the symbol $f&#39 ; $ means $$ f&#39 ; ( x ) = \frac{{\rm d}f}{{\rm d}x} = \lim_{\epsilon\to 0} \frac{f ( x+\epsilon ) -f ( x ) }{\epsilon} $$ this notation using ${\rm d}$ was introduced by leibniz ; the notation with the prime was introduced by lagrange . you also ask whether there is some problem with noncommutativity . there is absolutely nothing noncommutative in the integrals you write down . they are integrals with ordinary functions and their derivatives . the only objects that do not commute in quantum mechanics are observables i.e. operators such as $\hat x$ and $\hat p$ . however , the derivative $\psi&#39 ; $ is not really an operator . more precisely , it is proportional to the operator $\hat p$ acting on the wave function $\psi$ but $\psi$ is not an operator so you can not really move it to the left side from the derivative . if we have things like $v&#39 ; ( x ) $ in quantum mechanics , the derivative of the potential energy , then the potential energy $v ( x ) $ may be interpreted as an operator . then $v&#39 ; ( x ) $ may be rewritten as $$ v&#39 ; ( x ) = [ \frac{\rm d}{{\rm d}x} , v ( x ) ] = \frac{i}{\hbar} [ \hat p , \hat v ( x ) ] $$ however , when we choose the explicit integrals over $x$ etc . , they always mean the operations with ordinary commuting functions as they do in the calculus . one only has noncommuting objects when we write the actions of the operators more abstractly , in a way that does not depend on the representation of the wave function .
in the einstein convention , pairs of equal indices to be summed over may appear at the same tensor . for example , the formula ${a_k}^k=tr~a$ is perfectly legitimate . but your formula looks strange , as one usually sums over a lower index and an upper index , whereas you sum over lower indices only , which does not make sense in differential geometry unless your metric is flat and euclidean ( and then higher order tensors are very unlikely to occur ) .
i will point to a reference , it says , three things must happen for you to see a rainbow 's colors . first , the sun must be shining . second , the sun must be behind you , and third , there must be water drops in the air in front of you . sunlight shines into the water drops , which act as tiny prisms that bend or " refract " the light and separate it into colors . actually , the rays of light bend twice . as they enter the drops , the rays of light bend , then reflect off the back of the drops . then they bend again , this time while exiting the drops . that is when the light appears before our eyes . each drop reflects only one color of light , so there must be many water drops to make a full rainbow . you will see the brightest rainbows when the water drops are large , usually right after a rain shower . the rainbow is circular because when a raindrop bends light , the light exits the raindrop at an angle 40 to 42 degrees away from the angle it entered the raindrop . and regarding full circular rainbow , but if the sun is very low in the sky , either just before sunset or just after sunrise , we can see a half circle . the higher the sun is in the sky , the less we see of the rainbow . the only way to see the full circle of a rainbow in the sky is to be above the raindrops and have the sun behind you . you would have to look down on the drops from an airplane .
the problem with my analysis before was that $t_p/2$ is not the relevant coupling constant . to read off the coupling constant , we must put the action into a slightly different form , in which the kinetic and interaction terms are separate and apparent . to do that , we assume that everything is ( real ) analytic , we work locally and pick coordinates $x^\mu$ on space-time ( the codomain manifold ) such that $g ( 0 ) =\eta _{\mu \nu}$ in these coordinates . then , $g ( x ) =\eta _{\mu \nu}+\varepsilon f^{ ( 1 ) }_{\mu \nu \rho}x^\rho +\cdots$ , where we are taking $f^{ ( 1 ) }$ to be dimensionless , so that $ [ \varepsilon ] =l^{-1}=m$ . plugging this into the action , we find $$ s_{\text{p}}=\int d\sigma ^{1+p}\ , \left [ -\frac{t_p}{2}\partial _\alpha x^\mu \partial ^\alpha x_\mu -\varepsilon \frac{t_p}{2}f^{ ( 1 ) }_{\mu \nu \rho} ( \partial _\alpha x^\mu ) ( \partial ^\alpha x^\nu ) x^\rho +\cdots \right ] . $$ to get the kinetic term in the usual form so that we can read-off the appropriate coupling constant , we define $y:=\sqrt{t_p}x$ and write the action in terms of $y$: $$ s_{\text{p}}=\int d\sigma ^{1+p}\ , \left [ -\frac{1}{2}\partial _\alpha y^\mu \partial ^\alpha y_\mu -\frac{\varepsilon}{2\sqrt{t_p}}f^{ ( 1 ) }_{\mu \nu \rho} ( \partial _\alpha y^\mu ) ( \partial ^\alpha y^\nu ) y^\rho +\cdots \right ] . $$ from this , we see that the coupling constant of the lowest-order interaction is $$ \frac{\varepsilon}{2\sqrt{t_p}}f^{ ( 1 ) }_{\mu \nu \rho} , $$ which has mass dimension $$ 1-1/2 ( 1+p ) =1/2-1/2p . $$ the theory will thus be superficially renormalizable iff $1/2-1/2p\geq 0$ , that is , iff $p\leq 1$ . this is essentially user10001 's answer given in the comments above , with further details added .
as explained by iwo bialynicki-birula in the paper quoted , the maxwell equations are relativistic equations for a single photon , fully analogous to the dirac equations for a single electron . by restricting to the positive energy solutions , one gets in both cases an irreducible unitary representation of the full poincare group , and hence the space of modes of a photon or electron in quantum electrodynamics . classical fields are expectation values of quantum fields ; but the classically relevant states are the coherent states . indeed , for a photon , one can associate to each mode a coherent state , and in this state , the expectation value of the e/m field results in the value of the field given by the mode . for more details , see my lectures http://www.mat.univie.ac.at/~neum/ms/lightslides.pdf http://www.mat.univie.ac.at/~neum/ms/optslides.pdf and chapter b2: photons and electrons of my theoretical physics faq .
fermionic generators of course do not act just geometrically on a bosonic space ; all differential operators acting on bosonic coordinates are bosonic . at most , you could consider a superspace extension of $ads_5 \times s^5$ but superspaces are not too useful if there are too many supercharges ( they have too many components ) . so it is a kind of misguided approach to ask about the action of the supercharges on the spacetime only ; one should learn what is the action of the supergroup at the hilbert space – the whole actual theory – and it is pretty straightforward if you define the $n=4$ gauge theory . witten – when he mentions that the group acting on the ads-space times the sphere is the supergroup – really wants to say that $psu ( 2,2|4 ) $ is the ( or " a" ) maximal supergroup of symmetries that a theory defined on $ads_5\times s^5$ may have . but he surely does not mean that all the generators - the fermionic ones in particular - may be defined as differential operators acting on the 10 bosonic coordinates of this spacetime only . for the terminology of superalgebras ( and the same supergroups ) , look at page 58 of this kac 's review ( pdf ) : http://projecteuclid.org/dpubs?service=uiversion=1.0verb=displayhandle=euclid.cmp/1103900590 finally , the extra $p$ in $psu$ means that one eliminates a block-diagonal " hypercharge-like " generator from $su$ . it is similar to the embedding of $su ( 3 ) \times su ( 2 ) \times u ( 1 ) $ in $su ( 5 ) $ in grand unification ; in the superalgebra case , one can consistently eliminate the $u ( 1 ) $ here , at least if the number of bosonic entries and fermionic entries ( dimensions of the fundamental representation ) are equal . and it is equal , both are 4 for $psu ( 2,2|4 ) $ . if you check a se question about an $su ( 5 ) $ decomposition here introduction to physical content from adjoint representations the equal dimensions allow us to set the " hypercharges " of the off-block-diagonal entries ( all the fermionic generators ) which were $\pm 5/6$ above to zero and eliminate the " hypercharge " $u ( 1 ) $ which was just shown to become a center ( generator commuting with all others ) completely . without the $p$ which stands for " projective " , the bosonic subgroup of $su ( 2,2|4 ) $ would really be $su ( 2,2 ) \times su ( 4 ) \times u ( 1 ) $ with the extra last factor that actually gets eliminated in $psu$ .
i walk through a physical argument similar to what trimok has shown in my blog article ramanujan and the casimir effect . you basically assume that each available classical e-m mode is filled to exactly the level of one-half of a " quantum " , and put a small box inside one twice as big . the gist of the argument goes like this ( quoting from my article ) : we simplify things a little by putting everything in a one-dimensional box . so the energy modes are 1,2,3 , etc . we can choose our units so that the pressure is numerically equal to the energy . now make the box twice small . the energy modes are 2,4,6 . . . etc . but the pressure is energy per unit volume ( length , since it is one-dimensional ) . . . so the pressures are ( get this : ) 4,8,12 . . . what is the difference in pressure between the big box and the small box ? 1 + 2 + 3 . . . . - ( 4 + 8 + 12 . . . ) which is . . . 1 - 2 + 3 - 4 + 5 . . . . = 0.25 ! to get the actual casimir effect , you then put the big box in yet a bigger box . you get a pressure difference equal to one-quarter of what you got in the first set of boxes . continuing this process gives you a chain of boxes from which you can calculate the pressure relative to infinity .
some key reading if you want to understand this stuff is chapters two to five of the iers technical note 36 , the iers conventions ( 2010 ) . it is not just the j2000/fk5 frame ( aka the eme2000 frame ) that is associated with some epoch date . every earth-centered inertial frame has some epoch date . there are two fundamental reasons why this must be the case : the earth 's rotation axis is not constant . solar system astronomers are regularly improving their best estimate of what constitutes an inertial frame . note that the j2000/fk5 frame is now twice passé . the current best estimate of what constitutes an inertial frame is the international celestial reference frame 2 ( icrf2 ) . it is predecessor , the icrf , represented a vast improvement over the j2000/fk5 frame . the icrf2 is even better than the icrf . the icrf was supposed to be co-aligned with the j2000/fk5 frame on j2000.0 ( 12 noon terrestrial time on 2000 january 1 ) . it turned out that this was not the case ; there is a slight bias between the frames at the epoch . the j2000/fk5 frame also turns out to be rotating a tiny bit , about 3 milliarcseconds/year . unless you are doing milliarcsecond astronomy , you can ignore that bias and rotation . for most applications , j2000/fk5=icrf=icrf2 . the first item , that the earth 's rotation axis is not constant , is important . the earth 's rotation axis precesses with a period of about 26,000 years . accounting for the change in the precession between the epoch time and the time of interest ( e . g . , today ) yields a transformation from the epoch frame ( e . g . j2000 ) to the mean of date frame . in addition to this long-term precession , the earth 's axis also displays some shorter term variations in where it points . these short-term variations ( from ~5.5 days to 18.6 years ) are collectively called nutation . accounting for the earth 's nutation on top of the precession yields the transformation to the true of date frame . finally , the earth rotates at about one revolution per sidereal day about this precessed and nutated axis . applying this rotation of the true of date frame yields the earth-centered , earth-fixed frame . a somewhat widely-used name for the result of this process is the earth rnp ( rotation , nutation , and precession ) matrix . well , almost . precession and nutation are semi-analytical models , as is the concept of one revolution per sidereal day . there are some things those models just can not capture . that one revolution per sidereal day is incorrect for two reasons . one is that the earth 's rotation rate is very gradually slowing down . another is that when you look at the rotation rate very closely , the earth sometimes rotates faster than nominal , other times slower . there are two key parameters that describe this , dut1=ut1−utc and δt=tt-ut1 . if you care about this detail i suggest you use the latter as it is continuous . dut1 has discontinuities at the leap seconds . this is a correction that you add when you compute the rotation part of the rnp matrix . there are some things that the semi-analytical precession and nutation model just do not cover ( yet ) . the chandler wobble , for example . these are collectively called " polar motion " , and can only be observed ( and predicted to some extent ) . polar motion needs to be applied after computing the rnp matrix . the full result is sometimes called the prnp ( polar motion , rotation , nutation , and precession ) matrix . these fine scale variations in the earth 's orientation , along with dut1 and δt , are called the " earth orientation parameters " . these are published on a regular basis as " iers bulletins a and b " . i will say more about this below . you do not want to code this on your own . you can get code to do these calculations from a number of places . the best sites are : international astronomical union ( iau ) standards of fundamental astronomy ( sofa ) the sofa code is the " official " version of all of the concepts described above . you can get both fortran and " ctran " ( fortran converted to ugly c ) versions of the sofa code from the sofa website . also be sure to check out the cookbooks , particular the cookbook on " sofa tools for earth attitude " . naval observatory vector astrometry software ( novas ) the us naval observatory is responsible for iers bulletins a and b . they have their own software , distinct from the sofa code . the novas software is available in fortran , c , and python . the difference in terms of results is negligible , in the microarcseconds . that is the kind of error one would expect from using double precision and performing the same computations a bit differently . there are a number of others ( e . g . , jpl spice , gsfc gmat , orekit ) , but i suggest you go to the source , and that would be either the iau or the us naval observatory . i mentioned iers bulletins a and b couple of times above . the international earth orientation and reference systems service ( iers ) is the worldwide organization responsible for defining things such as the icrf and for determining how the earth is oriented . ( yes , the acronym does not match . it used to before they changed the name but not the acronym . ) as far as those technical bulletins are concerned , they just contain numbers ( and a tiny bit of text ) . these numbers are time-tabulated values for the earth orientation parameters . these bulletins are updated monthly . a couple of final points : i put the link to iers technical note 36 at the top of this answer . read it . be very careful of time . there are a number of time scales involved in this modeling . a few of them that you will run into are : tai - international atomic time . time according to an earthbound physicist who uses an atomic clock at sea level . tt - terrestrial time . time according to an earthbound astronomer . physicists and astronomers disagree by 32.184 seconds . ut1 - universal time . conceptually , what a sundial says the time is , but smoothed to eliminate things such as the equation of time . utc - coordinated universal time . that is what the clock on your computer shows if you are using network time protocol . next time we have a leap second ( probably the end of next year ) , you will be able to see a minute with 61 seconds in it . utc ticks at the same rate as tai and tt but occasionally has leap seconds so as to stay within a second of ut1 . tcb - barycentric coordinate time . a general relativistic timescale that on average ticks faster than clocks on the surface of the earth . tdb - barycentric dynamical time . a general relativistic timescale that on average ticks at the same rate as clocks on the surface of the earth . gast - greenwich apparent sidereal time . if you run into this time scale you are looking at an out-of-date concept for calculating earth rotation . use the newer earth rotation angle concept , which relies on ut1 . update i did not answer the title of the question , how to determine satellite position in j2000 from latitude , longitude and distance from earth ? this is the easy part . the only tricky aspect is that latitude is almost always geodetic latitude rather than geocentric latitude . i will assume that latitude $\phi$ , longitude $\lambda$ , and altitude $h$ are in the wgs84 reference system . see [ department of defense ( 2000 ) , " world geodetic system 1984: its definition and relationships with local geodetic systems " nima tr8350.2 ] ( http://earth-info.nga.mil/gandg/publications/tr8350.2/wgs84fin.pdf ) equations 4-14 and 4-15 in the above reference describe the transformation from latitude $\phi$ , longitude $\lambda$ , and altitude $h$ are in the wgs84 reference system to cartesian earth-centered , earth-fixed ( ecef ) coordinates . the equations below use two key parameters that describe the shape of the earth ( see tables 3.1 and 3.3 of the above reference ) : $$ \begin{aligned} a and = 6378137\ \text{m} and and \text{earth equatorial radius} \\ e^2 and = 6.69437999014\times10^{-3} and and \text{square of earth eccentricity} \end{aligned} $$ first you need to compute the " radius of curvature in the prime vertical " ( equation 4-15 in the reference ) : $$n = \frac a {\sqrt{1-e^2\sin^2\phi}}$$ then simply compute the ecef coordinates via equations 4-14: $$ \begin{aligned} x and = ( n+h ) \cos\phi \cos\lambda \\ y and = ( n+h ) \cos\phi \sin\lambda \\ z and = ( ( 1-e^2 ) n+h ) \sin\phi \end{aligned} $$
let be $$\frac{2a}{q}v ( \theta , \varphi ) =f ( \theta , \varphi ) =2\sin\theta\cos\varphi+\cos^2\theta . \tag 1$$ the laplace spherical harmonics form a complete set of orthonormal functions and thus form an orthonormal basis of the hilbert space of square-integrable functions . on the unit sphere , any square-integrable function can thus be expanded as a linear combination of these : $$ f ( \theta , \varphi ) =\sum_{\ell=0}^\infty \sum_{m=-\ell}^\ell f_\ell^m \ , y_\ell^m ( \theta , \varphi ) \tag 2 $$ where $y_\ell^m ( \theta , \varphi ) $ are the laplace spherical harmonics defined as $$ y_\ell^m ( \theta , \varphi ) = \sqrt{{ ( 2\ell+1 ) \over 4\pi}{ ( \ell-m ) ! \over ( \ell+m ) ! }} \ , p_\ell^m ( \cos{\theta} ) \operatorname{e}^{i m \varphi } =n_{\ell}^m p_\ell^m ( \cos{\theta} ) \operatorname{e}^{i m \varphi }\tag 3 $$ and where $n_{\ell}^m$ denotes the normalization constant $ n_{\ell}^m \equiv \sqrt{{ ( 2\ell+1 ) \over 4\pi}{ ( \ell-m ) ! \over ( \ell+m ) ! }} , $ and $p_\ell^n ( \cos\theta ) $ are the associated legendre polynomials . the laplace spherical harmonics are orthonormal $$ \int_{\theta=0}^\pi\int_{\varphi=0}^{2\pi}y_\ell^m \ , y_{\ell'}^{m'*} \ , d\omega=\delta_{\ell\ell'}\ , \delta_{mm'} , $$ where $δ_{ij}$ is the kronecker delta and $\operatorname{d}\omega = \sin\theta \operatorname{d}\varphi\operatorname{d}\theta$ . the expansion coefficients are the analogs of fourier coefficients , and can be obtained by multiplying the above equation by the complex conjugate of a spherical harmonic , integrating over the solid angle $ω$ , and utilizing the orthogonality relationships . this is justified rigorously by basic hilbert space theory . for the case of orthonormalized harmonics , this gives : $$ f_\ell^m=\int_{\omega} f ( \theta , \varphi ) \ , y_\ell^{m*} ( \theta , \varphi ) \operatorname{d}\omega = \int_0^{2\pi}\operatorname{d}\varphi\int_0^\pi \operatorname{d}\theta\ , \sin\theta f ( \theta , \varphi ) y_\ell^{m*} ( \theta , \varphi ) . \tag 4 $$ where $ y_\ell^{m*} ( \theta , \varphi ) = ( -1 ) ^m y_\ell^{-m} ( \theta , \varphi ) $ . the evaluation of the expansion $f_\ell^m$ may be very long in this way . . . we can use some tricks in your case for $f ( \theta , \varphi ) =2\sin\theta\cos\varphi+\cos^2\theta$ observing that $$ \sin\theta=-p_1^1 ( \cos\theta ) $$ and $$ y_{1}^{-1} ( \theta , \varphi ) - y_{1}^{1} ( \theta , \varphi ) = {1\over 2}\sqrt{3\over 2\pi}\cdot e^{-i\varphi}\cdot\sin\theta-{-1\over 2}\sqrt{3\over 2\pi}\cdot e^{i\varphi}\cdot\sin\theta =\sqrt{3\over 2\pi} \sin\theta\cos\phi $$ so that $$ 2\sin\theta\cos\varphi=2\sqrt{\frac{2\pi}{3}}\left ( y_1^{-1} ( \theta , \varphi ) -y_1^{1} ( \theta , \varphi ) \right ) =-2p_1^1 ( \cos\theta ) \cos\varphi\tag 5 $$ and observing that $$ \cos^2\theta=\frac{1}{3}p_0^0+\frac{2}{3}p_2^0 $$ and using the relation $y_\ell^0 ( \theta , \varphi ) =\sqrt{\frac{2\ell+1}{4\pi}}p_\ell^0 ( \cos\theta ) $ where $p_\ell^0 ( \cos\theta ) $ are the ordinary legendre 's polynomials $p_\ell ( \cos\theta ) $ , we obtain $$ \cos^2\theta=\frac{1}{3}p_0^0 ( \cos\theta ) +\frac{2}{3}p_2^0 ( \cos\theta ) =2\sqrt{\pi}y_0^0 ( \theta , \varphi ) +\frac{4}{3}\sqrt{\frac{\pi}{5}}y_2^0 ( \theta , \varphi ) . \tag 6 $$ finally , putting together ( 5 ) and ( 6 ) in ( 1 ) we obtain $$ f ( \theta , \varphi ) =2\sqrt{\frac{2\pi}{3}}y_1^{-1} ( \theta , \varphi ) -2\sqrt{\frac{2\pi}{3}}y_1^{1} ( \theta , \varphi ) +2\sqrt{\pi}y_0^0 ( \theta , \varphi ) +\frac{4}{3}\sqrt{\frac{\pi}{5}}y_2^0 ( \theta , \varphi ) \tag 7 $$ so that , comparing ( 7 ) and ( 2 ) , the coefficients $f_\ell^m$ are $$ f_1^{-1}=2\sqrt{\frac{2\pi}{3}}\qquad f_1^{1}=-2\sqrt{\frac{2\pi}{3}}\qquad f_0^{0}=2\sqrt{\pi}\qquad f_2^{0}=\frac{4}{3}\sqrt{\frac{\pi}{5}}\tag 8 $$ so you have $$\small v ( \theta , \varphi ) =\frac{q}{a}\left [ \sqrt{\frac{2\pi}{3}}y_1^{-1} ( \theta , \varphi ) -\sqrt{\frac{2\pi}{3}}y_1^{1} ( \theta , \varphi ) +\sqrt{\pi}y_0^0 ( \theta , \varphi ) +\frac{2}{3}\sqrt{\frac{\pi}{5}}y_2^0 ( \theta , \varphi ) \right ] $$ the general solution to the laplace equation outside the sphere is $$ \phi ( r , \theta , \varphi ) =\sum_{\ell=0}^\infty \sum_{m=-\ell}^\ell \frac{b_\ell^m}{r^{\ell+1}} \ , y_\ell^m ( \theta , \varphi ) \tag 9 $$ with $b_\ell^m=\frac{q}{a}f_\ell^m$ , that is $$\small \phi ( r , \theta , \varphi ) =\frac{q}{a}\left [ \frac{1}{r^2}\left ( \sqrt{\frac{2\pi}{3}}y_1^{-1} ( \theta , \varphi ) -\sqrt{\frac{2\pi}{3}}y_1^{1} ( \theta , \varphi ) \right ) +\frac{\sqrt{\pi}}{r}y_0^0 ( \theta , \varphi ) +\frac{1}{r^3}\left ( \frac{2}{3}\sqrt{\frac{\pi}{5}}\right ) y_2^0 ( \theta , \varphi ) \right ] $$
this is a quantum partition function , not a statistical mechanical partition function . he is just talking about an idealized self-interacting field . if you have a scalar with cubic self interactions , you write the lagrangian as $$ \partial_\mu \phi \partial^\mu \phi - \lambda \phi^3 $$ if you fourier transform the field variables , this is $$ \int_k k^2 |\phi_k|^2 + \int_{k_1 , dk_2 , dk_3} \delta ( k_1+k_2+k_3 ) \phi_{k_1}\phi_{k_2}\phi_{k_3} $$ which , if you think of k as a lattice , can be abstrated to the form banks writes down . the remaining s_eff term is from renormalization , which changes the low energy theory according to the contributions to the low-energy effective action from high-energy degrees of freedom you are neglecting . this is heuristic , because a real renormalizable model requires a $\phi^4$ term too .
the following diagram shows the prism with the incoming and outgoing light rays . if you follow the incident light ray in , it gets bent by an angle $\theta_1 = i- r_1$ . if you follow the light ray where it leaves the glass , it gets bent again by an angle $\theta_2 = e - r_2$ , so the total deviation is : $$ \begin{align} d and = \theta_1 + \theta_2 \\ and = i + e - ( r_1 + r_2 ) \end{align} $$ for the next step look at the triangle formed by the top of the prism and the light ray , and note that the internal angles must add up to 180° . so : $$ a + ( 90 - r_1 ) + ( 90 - r_2 ) = 180 $$ and a quick rearrangement gives : $$ a = r_1 + r_2 $$ now substitute for $r_1 + r_2$ in our first equation and we get : $$ d = i + e - a $$ or : $$ d + a = i + e $$
let 's say your target is a film $10^{-2}$ mm thick . nuclei are about $10^{-14}$ m in diamater at most . this means that the alignment of the beam with the target would have to be $10^{-9}$ radians , which is not possible with realistic beam optics . even if your beam optics were that perfect , the perfect parallelism would be destroyed by scattering once the beam entered the target .
$$\newcommand{\holonomy}{ [ \mathcal{h\mathbb{o} \ell} ] }$$ this answer is an expansion of qmechanic 's comment/ . * ** * ** * ** * ** * ** * * holonomy holonomy can be imagined as the integral , or global version , of the riemann curvature tensor . the riemann curvature tensor , indeed is $$r_{\mu\nu\rho}^\sigma=\mbox{d}\holonomy$$ where $\mathcal{\holonomy}$ is the holonomy . holonomy groups now , this holonomy is the group action of the holonomy group of the manifold . so , in other words , the holonomy of the identity of the holonomy group ( not doing any sort of a transport ) does not do anything to a point on the manifold , and that holonomies are hand - wavily , sort - of " associative " ( use this statement with caution ! ) , i.e. , instead of writerighteing $\phi ( g , x ) $ or something , if we choose to write something like , say , $g\dagger x$ , then : $$g\dagger\left ( h\dagger x\right ) =\left ( gh\right ) \dagger x$$ oh , and the first statpement becomes , : $$e\dagger x =x $$ now , this is not as trivial as it looks . $e$ is the identity of the holonomy group , not of the manifold ! . so , where does $g ( 2 ) $ come in ? now , where in the world does $g ( 2 ) $ come from ? $g ( 2 ) $ is a holonomy group of $\bf{\mathbf{\it{7}}}$-dimensional manifolds , called $g ( 2 ) $ manifolds . this means that it is possible to use this as a compactification manifold for m-theory . m-theory has a supersymmetry of $\mathcal n=8$ . but , if we want a supersymmetry of $\mathcal{n}=1$ ( accessible at lower energies ) , n the compactificaqtion manifoldk must get rid of $\frac78$ of the supersymmetry , i.e. retain only $\frac18$ . it so happens to be that $g ( 2 ) $ manifolds do indeed satisfy this criteriaon .
there are a few standard textbooks on neutron star . for interior structure and nuclear physics side two books by glendenning are good . http://www-nsdth.lbl.gov/~nkg/description.html for more general relativity side shapiro and teukolsky has been a standard texk book for many years . http://www.amazon.com/black-holes-white-dwarfs-neutron/dp/0471873160 finally , if you seek for real rigor , a new book by friedman and stergioulas is must . http://www.amazon.com/rotating-relativistic-stergioulas-cambridge-monographs/dp/0521872545 there are several review papers including two in living review . http://relativity.livingreviews.org/articles/lrr-2003-3/ http://relativity.livingreviews.org/articles/lrr-2007-1/ several by lattimer and prakash are also good starting point . for example , http://arxiv.org/abs/astro-ph/0612440
since you say you are a programmer , i see where criterion #1 comes from . but telescopes are not computers , you can not upgrade the cpu today , the ram tomorrow , and so on . a scope is defined largely by its aperture ( the diameter of the objective lens or mirror ) . that puts a major cap on pretty much everything else , performance-wise . aperture is like an old boarding school taskmaster who says " you are allowed up to here , no more " , and anything else you may do can only place you lower than that ideal level of performance . scopes optimized for visual and scopes optimized for photo are different animals . they are interchangeable to some extent , but after some point their respective traits start acting up . usually , people start with a small price-efficient visual scope ( like a small dobsonian ) , then migrate to astrophoto after some learning is done . but if you are intent on doing ap directly , fine . the instrument that is typically used for ap is some kind of catadioptric , like a schmidt-cassegrain ( sct ) , on a tracking mount . it does not have to be an sct , it could be a refractor , a ritchey-chretien , it could be a newtonian , or what have you . but an sct is typically short , stubby , and rather not unwieldy for its aperture , which are good attributes if you put it on a tracking mount . the mount does not necessarily have to be a go-to mount . i would argue that go-to is a waste of money if you are smart enough that you can use a star map ( either paper , or software ) . but it absolutely needs to track the motion of the sky on one axis , because you are going to take a lot of long-exposure photos . some examples : http://www.telescope.com/telescopes/astrophotography-telescopes/pc/1/19.uts?currentindex=0pagesize=34defaultpagesize=20mode=viewallcategoryid=1subcategoryid=19type=thumbnail2level as you can see , ap scopes and mounts are expensive . you could get away with a mediocre scope , and still take good pictures , but a bad mount is a deal breaker . a lot of beginners are like " okay , that stuff is way too expensive " and simply purchase a small cassegrain on a cheap go-to . it is definitely less expensive , but the performance changes accordingly . some examples : http://www.telescope.com/telescopes/cassegrain-telescopes/pc/1/14.uts?currentindex=0pagesize=51defaultpagesize=20mode=viewallcategoryid=1subcategoryid=14type=thumbnail2level finally , if you decide to take a while and school yourself in purely visual astronomy before purchasing a killer ap rig , you could start with a bang-for-the-buck visual scope such as a classic or intelliscope dobsonian - as much aperture per coin spent as possible : http://www.telescope.com/telescopes/dobsonian-telescopes/pc/1/12.uts ( i am in the us , but the general principles should apply ; maybe less so the particular examples above . ) edit : here 's a bare bones rig for ap : http://www.garyseronik.com/?q=node/52 if you have a digital camera already , it will cost you almost nothing , and you could assemble it in one week-end .
faraday 's cage is known to block static and non-static electric fields . the mechanism of blocking depends on whether the electric field is static or non-static ( em field ) . i suppose you question is about how the cage works in non-electrostatic case . in em case ( time changing field ) , two scenarios could happen . the first is electric discharge where the current flows from a distant electrode to the cage . the second is an em wave with high power propagating toward the cage generating its current locally within the conductor . i will explain how the cage works for both cases . with respect to the first case , it can be mathematically described by charge continuity equation ( equation 3 in this link ) . this equation basically relates the current flowing through a conductor to the charge accumulating in it . what happens in the first scenario is that the external current ( being moving charges ) coming from the electrode accumulates at the point where it ( the spark or the streamer ) hit the cage . because the cage is a conductor the charge continuity equation tells us that the local accumulation of charge where the spark hit the cage will cause current to flow within the conductor to remove that accumulation . the characteristic time required to remove the accumulation is called the relaxation time . it can be derived from charge continuity equation . for the derivation have a look at pages 57-59 of this document . i think that is taken from a book called elements of electromagnetics chapter 5 . if the conductor is made from a material with infinite conductivity , the relaxation time is zero . that means the current will keep flowing though the cage without any problem and that the electric field in the conductor is always zero . in other words , the electrostatic point of view holds even for non-electrostatic case if the conductivity is infinite . that is a direct consequence from charge continuity equation . for non infinite conductivity cases , the electric field within the conductor will survive within the conductor with a time scale related directly to relaxation time of that conductor . i hope it is clear now with respect to first case . the second case is related to em waves where they generate their currents locally within the conductor , that is where the skin effect comes into play . an em wave penetrates into a conductor the skin effect occurs . in general , em waves when they penetrate a conductor they are attenuated until their fields become almost zero . a characteristic depth of penetration is called skin depth . the skin depth is the distance it takes an em wave to be attenuated to certain value . this skin depth depends on many factors such as conductivity and frequency , the following figure taken from wikipedia shows the skin depth of different materials for different frequencies : for the cage to protect from em waves , it is thickness has to be larger than multiples of skin depth at the particular frequency of interest . so briefly with respect to the second scenario , the skin depth becomes relevant when we speak about shielding from electromagnetic waves rather than discharge current . the first and the second scenarios can be put together in frequency spectrum , the first scenario describes why the cage protects current in low frequencies while the second scenario describes why it protects from both current and radiation at high frequencies . i think the cage in the picture shows scenario 1 . you can clearly see the distant electrodes and the point at which the spark hits the cage i hope that answered your question
work done by tension on both the blocks can be regarded as 0 . this can be said by the virtual work method . the virtual work method : consider that block 1 ( mass $2kg$ ) displaces by a certain $d\vec{s_1}$ . infinitesimal work done on the block 1 by tension will be given by $$dw_1 = \vec t . d\vec s_1=tds_1\cos\theta_1$$ similarly , for block 2 we can say that $$dw_2=\vec t . d\vec s_2=tds_2\cos\theta_2$$ using string constraint , we can say that displacement of each block along the string is zero ( because the string is inextensible ) . so we get $$ds_1\cos\theta_1+ds_2\cos\theta_2=0$$ notice that i have used the same $\theta$ for each block as in tension because the direction along the string is the direction along tension vector .network done by tension thus becomes $$dw_t=t ( ds_1\cos\theta_1+ds_2\cos\theta_2 ) $$ $$\therefore dw_t=0$$ $$w_t=0$$ the solution to the actual problem : if we apply $w=\delta k$ on the system of the two blocks from initial position to the final position where block 1 is at the bottom of the cicrular arc , we get $$m_1g\delta h_1+m_2g\delta h_2=\frac 12 m_1 v_1^2+\frac 12 m_2 v_2^2$$ i do not include work done by tension on the system because i proved it to be 0 . we now need to find a relation between $v_1$ and $v_2$ . we can do this by applying string constraint . $$v_1\cos\theta=v_2$$ where $\cos\theta=\frac 35$ ( by geometry ) . $$\therefore \frac 35 v_1 = v_2$$ substituting $v_2$ in terms of $v_1$ in the above equation , we can find $v_1$ . then applying $w=\delta k$ on block 1 only we get $$w_t + w_{gravity} = \frac 12 m_1v_1^2$$ substitute $w_{gravity}$ and $v_1$ in this equation and find $w_t$ .
this is the solution for $n=2$ , $d=2$: you are trying to compute the average entropy of $$ \rho ( u_1 , u_2 ) = u_1|0\rangle\langle0|u_1^\dagger + u_2|0\rangle\langle0|u_2^\dagger $$ with $u_1$ and $u_2$ distributed according to the haar measure . this is equal to the entropy of $$ \rho' ( u ) = |0\rangle\langle0| + u|0\rangle\langle0|u^\dagger $$ where $u=u_1^\dagger u_2$ , which is of course still distributed according to the haar measure . in turn , this is $$ \tilde\rho ( |\psi\rangle ) = |0\rangle\langle0| + |\psi\rangle\langle\psi| $$ with $|\psi\rangle$ a haar-random qubit state . we know that a haar-random qubit state can be parametrized as $$ |\psi\rangle = \cos\tfrac\theta2 \ , |0\rangle + e^{i\phi}\sin\tfrac\theta2\ , |1\rangle\ , $$ where one needs to integrate in spherical coordinates ( i.e. . , with a measure $\tfrac1{4\pi}\sin\theta\ , \mathrm{d}\theta\ , \mathrm{d}\phi$ , $\theta\in [ 0 , \pi ] $ , $\phi\in [ 0,2\pi ] $ ) . we now have $$\tilde\rho ( |\psi\rangle ) = \frac12 \left ( \begin{matrix}1+\cos^2\tfrac\theta2 and e^{i\phi}\sin\tfrac\theta2\ , \cos\tfrac\theta2\\ e^{-i\phi}\sin\tfrac\theta2\ , \cos\tfrac\theta2 and \sin^2\tfrac\theta2\end{matrix}\right ) \ . $$ the entropy of this state is independent of $\phi$ ( so we set $\phi=0$ ) , and then we can easily check ( e . g . using mathematica ) that the von neumann entropy ( defined with the logarithm base $2$ ) is $$ h ( \tilde\rho ) = \tfrac12\left [ -\cos^2\tfrac\theta4\log_2\cos^2\tfrac\theta4 + \sin^2\tfrac\theta4\log_2\sin^2\tfrac\theta4\right ] \ . $$ the haar-average can then again be evaluated with mathematica and turns out to be $$ \frac{1}{4\pi}\int_0^{2\pi}\mathrm{d}\phi \int_0^\pi\sin\theta\ , \mathrm{d}\theta\ , h ( \tilde\rho ) = \frac13+\frac{1}{6\ln 2} \approx 0.5738 \ . $$
the given formula ( in that website ) is correct . assume we have a cylinder with permanent ( axial ) magnetization $\mathbf{m}$ . it will cause a surface current density $\mathbf{k}$: $$\cases{\bf{k}=\bf m \times \hat n \\ \mathbf m=m_0\hat z}\to \mathbf{k}=m_0\hat \phi$$ so it is like a finite length solenoid . to find the field on it is axis ( $z$ , point $p$ in the below picture ) , from biot–savart law we will arrive at : $$db=db_z=\frac{\mu_0k\mathrm{d}z}{2}\frac{r^2}{\xi^2}$$ $$\cases{dz=\frac{rd\theta}{\sin^2\theta}\\ \frac{r}{z}=\tan \theta \\ \xi=\frac{r}{\sin \theta}}\to db=-\frac{\mu_0 k}{2}\sin \theta d\theta$$ $$b=\int_{\theta_1}^{\theta_2}db=\frac{\mu_0 k}{2} ( \cos \theta_1-\cos \theta_2 ) $$ so $$\boxed {\mathbf{b}=\frac{\mu_0 m_0}{2} ( \cos \theta_1-\cos \theta_2 ) \hat z}$$
i found a solution to my problem using the vertical displacement : $$\begin{equation}\tag{2} h_d = d_1 tan ( \theta ) + \frac{d_2 tan ( \theta ) }{n} + d_3 tan ( \theta ) \end{equation}$$ solved for $\theta$ this becomes $$\begin{equation}\tag{3} \theta = \arctan \left ( \frac{h_d}{d_1 + d_3 + \frac{d_2}{n}} \right ) \end{equation}$$
consider non-relativistic quantum mechanics with a finite range potential . the spectrum of the hamiltonian has a discrete sector ( possibly empty ) , corresponding to bound states , and a continuous sector , corresponding to scattering states . the asymptotic wave function of the scattering states can be characterized by phase shifts . the small momentum limit of the phase shifts determines the scattering lengths ( and scattering volumes etc . ) . by the magic of analyticity there are some relations between binding energies and phase shifts . one example is levinson 's theorem ( http://ajp.aapt.org/resource/1/ajpias/v32/i10/p787_s1 ) . another important example has to do with shallow bound states . if there is a shallow bound state with energy $e=-e_b$ , then the s-wave scattering length a satisfies $e_b=1/ ( ma^2 ) $ . this is explained in most text books on qm ( i was perusing weinberg 's book earlier this year , and he has a whole section devoted to shallow bound states . ) with regard to your questions : 1 ) for shallow bound states , $e_b=1/ ( ma^2 ) $ . in general , no direct relation except for ``global'' statements like levinson 's theorem . 2 ) the scatttering length is defined through the low-momentum limit of the scattering phase shift ; phase shifts determine asymptotic behavior of scattering states . 3 ) we call negative $a$ attractive and positive $a$ repulsive because the asymptotic wave functions are pulled in or pushed out , respectively . also , the simplest mechanism for small negative/positive $a$ is a weak repulsive/attractive potential . if there is a shallow bound state then $a$ is positive . this means that the scattering wave is pushed out ( ``repulsive'' ) even though the underlying potential is obviously attractive . this means that at low energy one cannot distinguish scattering from weakly repulsive potentials and strongly attractive ones with a shallow bound state .
x-rays in order to make metal radioactive one have to turn it into another element or isotope . this can be performed only with high-energy particles ( including photons ) . x-rays can be produces if an electron enters metal with very high speed in two ways : deceleration radiation ( bremsstrahlung ) an atom absorbs part of the electron 's kinetic energy , moves to one of the excited states and moves back to ground state emitting a high-energy photon in any case the energy of the incident radiation ( or particles ) must be comparable to the energy of x-ray or gamma photons . this is far from cellular phone frequency range for sure . shields and cages solid metallic shield reflects electromagnetic waves back . the material of the shield is important since it should have good conductivity . most of metals works well . as far as i know , copper and gold are the best especially for high frequencies ( microwaves ) . if the frequency is quite low there is no need for solid shield . the effective area that reflects the wave is proportional to $\frac{\lambda^2}{4\pi}$ , where $\lambda$ is the wavelength . it can be much larger than the antenna size . so metallic lattice works well if the distance between the wires is lower than $\lambda$ . gsm phones uses frequencies about 1 ghz which corresponds to $\lambda\approx$ 30 cm . for low frequencies the diffraction effects are important . if the size of the shield is comparable to $\lambda$ the radiation can just bypass the obstacle as it happens with sound . in this case metallic box is the best solution . it can be a cage with appropriate cell size . there should be no big holes like doors and windows . grounding grounding removes charges from the outside surface of faraday cage , but if you are inside there is no way to determine whether it is grounded or not . this is more concerned with safety . shape of the antenna this is important if you need something more interesting than just screening . using the effect of bragg diffraction , it is possible to build a shield that reflects one frequency and does not affect others ( this will work only for some directions ) . the shape of the shield also allows to control polarization of the radiation . edit 1 . answering the questions in the comments is the ' energy ' of incident radiation proportional to the frequency of transmitted emf waves , or what people commonly also call ' radiated power ' measured in watts/meter-sq ( as well ) ? there are two energy characteristics for emws : intensity - the amount of energy incident on unit area per unit time ( measured in w/m$^2$ ) . it describes total power of the radiation . energy of quantum - the energy of single photon ( measured in joules ) . it is equal to product of planck 's constant and frequency : $h\nu$ or $\hbar\omega$ . this value is very important in quantum mechanics since quantum system can absorb only integer number of photons . if one photon is not enough to change system state then even 1000 photons will just pass through with no effect . if emf radiation that is several hundred/thousand times in excess of international norms , could cause the x-ray generation this is possible if you have a system that can collect energy of low-frequency radiation and turns it to something else . for example , if emw induces plasma discharge between some metallic details and electrons collect enough energy before collision there can be x-rays ( i am not sure such situation is possible ) . edit 2 . answering the questions in the comments would the lattice structure/size computation be good enough if i base it on the highest frequency ( thus get the smallest lattice size needed ) ? also , does it matter if the material ( s . a . common steel mesh ) has the cross-over joints fused or insulated from each other ? if lattice period is smaller than $\lambda$ then it should work as a good screen . since you need computations and optimization it is better to ask someone who specializes in electrodynamics and antenna theory . may be it is better to ask this as a separate question . edit 3 . answering the questions in the comments is it possible to make practical application of bragg diffraction to cause destructive interference of the emf wave , when the waves are for large no . of different carrier waves , and clustered around 4-5 group of central frequencies ? this can be done with multiple bragg mirrors one for each frequency . afaik it is done for infrared radiation . apart from bragg diffraction are there other ways in which the emf can be reduced / nullified in a small region ( say within a radius of 5-6 meters ) , where the emf energy is captured using an antenna , converted to electrical energy , and converted to heat/light single antenna affects emf only within $\lambda$ distance . 5 meters is too much for 1 ghz which corresponds to $\lambda\approx$ 30 cm .
look , at your units . when something is $10^{-3}$ out , it is defiantly worth checking your units . if you are working in si units the density of water is $10^3 [ kgm^{-3} ] $ not $1 [ kgm^{-3} ] $ . imagine 1 metre cubed of water . it is very heavy .
it may be a good idea to add the appropriate units in your calculations . doing so will help you to localize the mistake . at the end , your results are only wrong because of a multiplicative factor of $\sqrt{1,000}\sim 31.7$ that must be added to your result to obtain the right one . it is actually not hard to see where this wrong factor comes from . http://en.wikipedia.org/wiki/atomic_mass_unit in your denominators , you used a value for the mass of the nuclei that is based on the ratio of the type $32$ divided by avogadro 's constant ( number of particles per mole ) . however , in this way , you obtain the value that assumes the natural conversion factor $1\ , {\rm g/mole}$: avogadro 's constant was originally defined as the number of molecules in one gram-molecule . however , you want to get the masses in kilograms – and the proton mass is about $1.66\times 10^{-27}\ , {\rm kg}$ , to proceed in the si units . so effectively , the right easiest fix of your formulae is either to substitute the explicit masses in kilograms or to replace your avogadro 's constant by $6.023\times 10^{26}/{\rm mole}$ whose numerical value is the number of molecules in one kilogram-molecule ( note the kilo ) or , equivalently , keep avogadro 's and replace $32$ by $0.032$ etc . then you get the right results within some tiny error margins ( the masses 32 and 28 amu are not quite accurate : proton and neutron masses differ and there are additional corrections from electrons and from nuclear binding energies ) .
$$ \frac{1}{\sqrt{2}} \begin{vmatrix} \chi_{\mathbf{p}_1} ( \mathbf{x}_1 ) and \chi_{\mathbf{p}_2} ( \mathbf{x}_1 ) \\ \chi_{\mathbf{p}_1} ( \mathbf{x}_2 ) and \chi_{\mathbf{p}_2} ( \mathbf{x}_2 ) \end{vmatrix} $$ corresponds to $a_{\mathbf{p}_1 s_1}^{\dagger} a_{\mathbf{p}_2 s_2}^{\dagger} |0\rangle$ . here $s$ is the spin for the fermions . in the slater determinant $\mathbf{x}$ includes both spatial and spin parts .
batteries produce a charge difference across the terminals as a result of a chemical reaction . a chemical gets changed into another one . even in a rechargeable battery a chemical change takes place that is reversed . an electrolytic capacitor uses chemistry to create a thin layer with an electric field across it . the thinner layer than a " regular " capacitor allows more charge to be stored in a smaller space . but the fundamental difference is that there is no chemical change when a capacitor is charged
to close the loop , andrew , the answer to your newest question is : the best and most famous reference about the electrodynamics of moving bodies is einstein , albert ( 1905-06-30 ) . " zur elektrodynamik bewegter körper " . annalen der physik 17: 891–921 . see also a digitized version at wikilivres:zur elektrodynamik bewegter körper . the english translation , " on the electrodynamics of moving bodies " , is here : http://www.fourmilab.ch/etexts/einstein/specrel/www/ the content of this paper became known as the special theory of relativity . i am just partly joking because for uniformly moving media , the lorentz boost to the rest frame is still the most natural way to proceed .
there is really no such thing as a semi-classical particle in the sense you are thinking of it . everything follows quantum laws , even large , massive objects . however , when you have a large number of particles and/or a large number of interactions with the environment , the different parts of the system are not in a coherent superposition . so you can take a statistical average of sorts , and the quantum laws simplify to classical laws . perhaps this is better explained with an example : consider trying to send a car through a double-slit apparatus . in principle , you could do it , if you managed to create a coherent superposition of ( all the car 's atoms going through the left slit ) + ( all the car 's atoms going through the right slit ) + ( all the car 's atoms crash into the wall together ) and maintain that superposition throughout the experiment . but in practice , that is not going to happen . the car is going to interact with its surroundings through radiation , vibrations , etc . and also different parts of the car interact among themselves , and that serves to collapse the superposition of left+right long before the car ever makes it to the slits .
nb : i feel like this is a pretty half-assed job , and i apologize for that but having opened my mouth in the comments i guess i have to write something to back it up . we start with fermi 's golden rule for all transitions . the probability of the transition is $$ p_{i\to f} = \frac{2\pi}{\hbar} \left|m_{i , f}\right|^2 \rho $$ where $\rho$ is the density of final states which is proportional to $p^2$ for massive particles . to find the rate 1 for all possible final states we sum over these probabilities incoherently . when the mass difference between the initial and final states is much less than the $w$ mass the matrix element depends only weakly ( hah ! ) on the particular state and the sum is well approximated by a sum only over the density of states : $$p_\text{decay} \approx \frac{2\pi}{\hbar} \left|m_{}\right|^2 \int_\text{all outcomes} \rho . $$ this sum is collectively called the phase space available to the decay . in these cases the matrix element is also quite small for the reason that dr bdo discusses . the phase space computation can be quite complicated as it must be taken over all unconstrained momenta of the products . for decays to two body states it turns out to be easy , there is no freedom in the final states except the $4\pi$ angular distribution in the decay frame ( their are eight degrees of freedom in two 4-vectors , but 2 masses and the conservation of four momentum account for all of them except the azimuthal and polar angles of one of the particles ) . the decays that you have asked about are to three body states . that gives us twelve degrees of freedom less three constraints from masses , four from conservation of 4-momentum which leaves five . three of these are the euler angles describing the orientation of the decay ( and a factor of $8\pi^2$ to $\rho$ ) , so our sum is over two non-trival momenta . the integral looks something like $$ \begin{array}\\ \rho \propto \int p_1^2 \mathrm{d}p_1 \int p_2^2 \mathrm{d}p_2 \int \mathrm{d} ( \cos\theta ) and \delta ( m_0 - e_1 - e_2-e_3 ) \\ and \delta ( e_1^2 - m_1^2 - p_1^2 ) \\ and \delta ( e_2^2 - m_2^2 - p_2^2 ) \\ and \delta ( e_2^2 - m_2^2 - p_2^2 ) \\ and \delta ( \vec{p}_1 + \vec{p}_2 + \vec{p}_3 ) \end{array} $$ which is easier to compute in monte carlo than by hand . ( btw--the reason for introducing the seemingly redundant integral over the angle $\theta$ between the momenta of particles 1 and 2 will become evident in a little while ) . for beta decays the remnant nucleus is very heavy compared to the released energy , which simplifies the above in one limit . in the case of muon decay , it is not unreasonable to treat all the products as ultra-relativistic , and the above reduces to $$ \begin{array}\\ \rho \propto \int p_1^2 \mathrm{d}p_1 \int p_2^2 \mathrm{d}p_2 \int \mathrm{d} ( \cos\theta ) and \delta ( m_0 - e_1 - e_2 - e_3 ) \\ and \delta ( e_1 - p_1 ) \\ and \delta ( e_2 - p_2 ) \\ and \delta ( e_3 - p_3 ) \\ and \delta ( \vec{p}_1 + \vec{p}_2 + \vec{p}_3 ) \\ = \int p_1^2 \mathrm{d}p_1 \int p_2^2 \mathrm{d}p_2 \int \mathrm{d} ( \cos\theta ) and \delta ( m_0 - p_1 - p_2 - p_3 ) \\ and \delta ( \vec{p}_1 + \vec{p}_2 + \vec{p}_3 ) \\ = \int p_1^2 \mathrm{d}p_1 \int p_2^2 \mathrm{d}p_2 \int \mathrm{d} ( \cos\theta ) and \delta ( m_0 - p_1 - p_2 - \left|\vec{p}_1 + \vec{p}_2\right| ) \\ = \int p_1^2 \mathrm{d}p_1 \int p_2^2 \mathrm{d}p_2 \int \mathrm{d} ( \cos\theta ) and \delta\left ( m_0 - p_1 - p_2 - \sqrt{p_1^2 + p_2^2 - p_1p_2\cos\theta} \right ) \end{array} $$ the integral over the angle will evaluate to one in some regions and zero in others and as such is equivalent to correctly assigning the limits of the other two integrals , so writing $\delta m = m_0 - m_1 - m_2 - m_3$ we get $$ \begin{array} \rho and \propto \int_0^{\delta m/2} p_1^2 \mathrm{d}p_1 \int_0^{\delta m-p_1} p_2^2 \mathrm{d}p_2 \\ and \propto \int_0^{\delta m/2} p_1^2 \mathrm{d}p_1 \left [ \frac{p_2^3}{3}\right ] _{p_2=0}^{\delta m-p_1} \\ and \propto \int_0^{\delta m/2} p_1^2 \mathrm{d}p_1 \frac{ ( \delta m - p_1 ) ^3}{3} \end{array} $$ which i am not going to bother finishing but shows that that phase space can vary as a high power of the mass difference ( up to the sixth power in this case ) . 1 the lifetime of the state is inversely proportional to the probability
you are not doing this wrong . as you know energy of each photon is $e = hf = 2.27ev$ so they can not produce any photoelectrons on a metal with work function greater than that .
the idea is correct ( it is called the hubbard-stratonovich transformation ) , but i can not say more without the details of the action . it is discussed in any good textbook on quantum field theory for condensed matter . concerning you questions ( if you are okay with the physicist 's approach to grassman numbers ) : the product of two grassmann numbers commutes with any c-number ( `bosonic ' number ) or grassmann number , so it can be considered for most purposes as a c-number . by the way , that is why the action of fermions is also a c-number , as you would expect . therefore , 1- you can safely shift $\phi$ by $\bar\psi\psi$ , that is just a change of variables . 2- when you shift $\phi$ , you do that at $\bar\psi\psi$ constant ( because you do the integral over $\phi$ first ) , so there is no problem with the integration either . i am sure mathematicians would find plenty of subtleties ( well , they do not even agree that functional integrals exist . . . ) , but as a physicist , you are ready to go !
1 ) this varies by textbook . a common format you will see is h=6.62606957 ( 29 ) ×10−34 ( from wikipedia : planck constant ) . the digits in parentheses indicate they are uncertain . hence , you had expect that h is known to at least 0.00000001/6.6260957 ( pretty well known . ) other references will explicitly state what the error bars are , or may simply cite the sources . you had expect the error bars to be in the references in the latter case . 2 ) this is a fine way to determine your accuracy . i am presuming , of course , that your error bars are much larger than the uncertainty in the boiling point of water . do not be so sure that is the case ! water boiling point changes with air pressure , humidity , saline content . . .
no . here is a counter-example . i will consider a $h_4 \otimes h_4$ space , where $h_4$ is four dimensional . i will express the counter-example basis in terms of the standard separable basis states $|0\rangle , \ldots , |3\rangle$ . to form the basis , first take all states $|kj\rangle$ , where $k$ and $j$ are such that $k \neq j$ or $k \in \{0,1\}$ or $j \in \{0,1\}$ . so basically all the usual basis states except $|22\rangle$ and $|33\rangle$ . now we have 14 of the 16 states in our basis . the final two are $2^{-1/2} ( |22\rangle + |33\rangle ) $ and $2^{-1/2} ( |22\rangle - |33\rangle ) $ . the last two are entangled , the rest are not .
there is an interesting paper on the subject , written at a lower level than some , on my website here : http://brannenworks.com/plavchan_feynmancheckerboard.pdf it notes that there are issues with the particle in 3 dimensions apparently being superluminal with a speed of at least $\sqrt{3}\ ; c$ .
the energy conservation law is compatible with every single observation we have made inside the milky way in science , or outside science , so the empirical evidence in favor of it is overwhelming , diverse , and universal . theoretically , the case is also clear . emmy noether demonstrated that conservation laws are linked to symmetries . the validity of the energy conservation law is equivalent to the time-translational symmetry of the laws of physics : the same phenomena occur if one starts with the same initial conditions but just a bit later . this is true for the laws of mechanics , field theory , electromagnetism , nuclear interactions , classical physics , quantum mechanics , thermodynamics and statistical physics , special relativity . the energy conservation law is valid in all these situations and respected by all the major theories describing these subfields of physics . motors , those produced by faraday , tesla , or anyone else , as well as all other engines and objects in the universe preserve the energy , too . and there exists no equivalence or analogy between the energy conservation law and the existence of tesla 's or faraday 's motor . in particular , there has never existed any solid evidence – empirical or theoretical – that electricity or magnetism could not do work . so any analogy between these totally different questions is an example of something technically referred to as demagogy . to create legitimate doubts about the validity of such an important and well-established law , one would need an observation or an arguments that actually discusses the technical properties of energy ( and one would probably have to construct a viable theory disagreeing with the energy conservation law that is compatible with the observations ) – rather than demagogic comparisons to completely different questions that the speaker desires to be answered by the same answer no although there does not exist a glimpse of a rational reason why the answers should be the same . to see violations of the energy conservation law , one has to go to cosmology . however , due to the slow evolution of the universe today , one needs to wait approximately for 10 billion years for the total energy of a system to change by an amount comparable to 100% . in the early stages of the cosmological evolution of our universe , the total energy was not conserved – this is particularly important for cosmic inflation that created the energy of the whole cosmos out of " almost nothing " . but this non-conservation depended on the background spacetime 's heavy violation of the time-translational symmetry .
if you look at this problem in 2d you have the following parameters at some instant which describe your trajectory ( position and velocity ) around a celestial body with gravitational parameter $\mu$: radius $r$ , radial velocity $\dot{r}$ and angular velocity $\omega$ . there are also a few others , but these do not really matter in this problem , due to symmetry . you can calculate the radius of your periapsis by using conservation of specific angular momentum and orbital energy . $$ \epsilon=\frac{\omega^2r^2+\dot{r}^2}{2}-\frac{\mu}{r}=\frac{\omega_p^2r_p^2}{2}-\frac{\mu}{r_p} $$ $$ h=\omega r^2=\omega_pr_p^2 $$ from here you can derive an expression for the radius of the periapsis $r_p$: $$ \omega_p=\frac{h}{r_p^2} $$ $$ \epsilon=\frac{\left ( \frac{h}{r_p^2}\right ) ^2r_p^2}{2}-\frac{\mu}{r_p}=\frac{h^2}{2r_p^2}-\frac{\mu}{r_p} $$ $$ \epsilon r_p^2=\frac{h^2}{2}-\mu r_p\rightarrow \epsilon r_p^2+\mu r_p-\frac{h^2}{2}=0 $$ $$ r_p=\frac{-\mu\pm\sqrt{\mu^2+2\epsilon h^2}}{2\epsilon} $$ these two solutions correspond with apoapsis and periapsis ( the to point in the orbit at which the radial velocity is zero ) . and you might suspect that the ' plus ' solution might correspond with apoapsis and ' minus ' with periapsis . however the opposite is true , since for an elliptical orbit $\epsilon$ is negative ( it will also hold for trajectories with higher eccentricity ) , so : $$ r_p=\frac{\sqrt{\mu^2+2\epsilon h^2}-\mu}{2\epsilon}=\frac{\sqrt{\mu^2+\left ( \omega^2r^2+\dot{r}^2-\frac{2\mu}{r}\right ) \omega^2r^4}-\mu}{\omega^2r^2+\dot{r}^2-\frac{2\mu}{r}} $$ to find what would be the best angle to burn to lower your periapsis the most you could use a fixed amount of $\delta v$ and see which angle $\phi$ would lower the periapsis the most ( $\frac{\delta}{\delta\phi}\delta r_p=0$ ) : $$ \delta r_p=\frac{\sqrt{\mu^2+\left ( \omega^2r^2+\dot{r}^2-\frac{2\mu}{r}\right ) \omega^2r^4}-\mu}{\omega^2r^2+\dot{r}^2-\frac{2\mu}{r}} - \frac{\sqrt{\mu^2+\left ( ( \omega r+\delta v\sin{\phi} ) ^2+ ( \dot{r}+\delta v\cos{\phi} ) ^2-\frac{2\mu}{r}\right ) ( \omega r+\delta v\sin{\phi} ) ^2r^2}-\mu}{ ( \omega r+\delta v\sin{\phi} ) ^2+ ( \dot{r}+\delta v\cos{\phi} ) ^2-\frac{2\mu}{r}} $$ i will leave deriving this equation to you . but i suspect that lowering the specific angular momentum will have the greatest impact , so burning in the radial direction , especially when are far away . edit : to answer your second question in your ( first ) comment . adding the radial part to $\omega^2r^2$ can be done using the fact that the radial velocity component ca be expressed as $v_\theta=\omega r$ , so : $$ v_\theta+\delta{v_\theta}=\omega r+\delta{v}\sin{\phi}\rightarrow \left ( v_\theta+\delta{v_\theta}\right ) ^2=\left ( \omega r+\delta{v}\sin{\phi}\right ) ^2 $$ however your equation can be simplified to this as well : $$ \left ( \omega+\frac{\delta{v}\sin{\phi}}{r}\right ) ^2 r^2=\left ( \left ( \omega+\frac{\delta{v}\sin{\phi}}{r}\right ) r\right ) ^2=\left ( \omega+\frac{\delta{v}\sin{\phi}}{r}\right ) ^2 r^2 $$
the general two-particle state will look like $\displaystyle \int dp_1 dp_2 \psi ( p_1 , p_2 ) a^\dagger_{p_1} a^\dagger_{p_2}| 0\rangle $ here $\psi ( p_1 , p_2 ) $ is the momentum-space wavefunction . since the creation operators commute , only the symmetric part matters , so we may as well take $\psi ( p_1 , p_2 ) =\psi ( p_2 , p_1 ) $ ( there would be a minus sign if they were fermions ) . if you would like the state to be normalizable , it should be square integrable . the position-space wavefunction $\psi ( x_1 , x_2 ) $ , is the fourier transform . you can then choose this to be supported when $x_1$ and $x_2$ are close to the positions at which you would like to localize the particles ( or vice-versa , because of the symmetry : the particles are indistinguishable ) , for example by gaussians . the phase can then carry information on the momenta of the particles ( as is hopefully familiar from 1-particle gaussians ) , as well as on how the two are entangled . this really does not depend much on the details of the sort of particle you are talking about , except the wavefunction will be symmetric or antisymmetric depending on whether you have bosons and fermions , and particles other than scalars will carry spin degrees of freedom so the wavefunction becomes a matrix .
think of those not as a capacity to absorb/emit but as simply absorption/emission . imagine you put an cold metal cube next to a hot identic cube . the hot one will emit a lot of heat but receive very little amout of heat from the cold one . therefore αλ&lt ; ϵλ , and vice-versa for the other cube . after a while , they will reach the same temperature , and at that moment , the radiation form cube a will be equal as cube b 's . at that moment αλ will equal ϵλ since the " output " radiation and " input " radiation are equal . you are right when saying it does not take into accout the other object , but this equation becomes true only when both objects reach thermal equilibrium so it is not necessary to explicitly refer to the other one . it is like if the surface was a mirror , and you were asking with this equation when does the reflect equals the original ? you only need the original to answer this question .
assuming that the proton is heavier than the neutron , by more than the mass of the electron ( plus the mass of a neutrino , plus the ionization energy of hydrogen ) , this is easy to answer , it would just make hydrogen unstable to decay to a neutron an an electron positron pair , so that a mostly hydrogen universe will decay into neutrons and electron-positron pairs , which wil annihilate into photons . so i will assume that the difference between neutron mass and proton mass is less than the mass of the electron , so that both the proton and the neutron are stable . the most drastic effect of this is on big-bang nucleosynthesis , where two new stable species can be created , neutrons and tritium , and he-3 would be unstable to inverse beta-decay into tritium . so you would produce hydrogen , deuterium , tritium , helium , lithium , and neutrons . the initial conditions are mostly neutrons , not mostly protons , because the mass is inverted , and so you would get a lot of he-4 , very little h-1 , and most of the universe 's mass would consist of stable neutrons and alpha-particles . these neutrons might collide to form neutron clusters , which would then beta-decay to protons once the binding energy was greater than the neutron proton mass difference . there would not be stars , but there might be gravitationally bound neutron clusters . neutrons are neutral , and find it hard to dissipate energy , but the time scales are long , so they might be able to eventually settle down into neutron-star-like objects .
the short answer is that if you have coefficients for all the terms , you have two independent exact fake scale invariance for the field $\phi$ and $m$ which just rescales the fields and the coefficients appropriately to keep the hamiltonian exactly the same . this is not a real invariance of the action , since it changes the parameters of the action , it is best thought of as choosing the dimensional scale of the two fields . you usually do this by fixing the terms " l " and " k " , but you get a different scaling if you fix the " t " and " l " terms , which is physical in different limits . i should point out that this model is exactly solvable , there are no real interactions in this model , so the renormalization group analysis is just dimensional analysis in disguise . there are two rotated q-modes mixing $m ( q ) $ and $\phi ( q ) $ which are completely free .
from what you describe there are two different length scales associated with this problem . the one ascociated with flow through the porous medium ( the ' bed packed with some objects' ) , and the second ascociated with the flow around the ' bed ' . i will assume you are talking about the flow through the porous ' bed ' . normally the characteristic dimension or length scale for internal flows is taken to be the hydraulic diameter . this is defined to be four times the cross-sectional area ( of the fluid ) , divided by the wetted perimeter . however , for such things as ' pebble beds ' etc . the reynolds number is defined differently . for flow of fluid through a bed of approximately spherical particles of diameter d in contact , if the voidage ( fraction of the bed not filled with particles ) is ε and the superficial velocity v ( that is , the fluid velocity through the bed as if the spheres/objects were not present ) then a reynolds number can be defined as : $$re = \frac{\rho v d}{\mu ( 1 - \epsilon ) }$$ laminar conditions apply up to re = 10 , fully turbulent from 2000 ( wikipedia ) . there are more advanced formulas for this , and they work in a variety of regimes ; from not-so-packed beds , to very packed-beds , also with a variaty of pebble/object shapes . many experiments have been done on convective and radiative heat transfer in pebble bed nuclear reactors and other such heat exchangers . i am sure you should be able to find some journal papers on this stuff along with the standard correlations you need for your particular flow . for the convective heat transfer coefficient for this flow however , you should be using the nusselt number which is a measure of the ratio of convective to conductive heat transfer a solid-fluid boundary . i hope this helps .
for particles in a beam , making a superposition of spin is easy ; you just split the beam and recombine it after making appropriate modifications to the split beams . in creating a superposition , this works even if the beam has only a single particle in it . a particle in a box is a tougher situation as it is difficult to split the states . the problem is that the usual methods of measuring a particle 's position or momentum ( in a box ) are destructive . for example , after you measure the position of an electron in a potential well you destroy all information about its momentum and its previous positions . i will take a whack at it . . . to get what you are asking for , you would need to have a measurement that ( a ) is non destructive , and ( b ) allows you to make some modification to the electron ( s ) . suppose that the electron begins in its lowest energy state . you want a measurement that will split that energy state into two different states and then you make a modification to one of the states . suppose that you begin with an electron with spin-up and lowest energy in your potential well . one thing you could do is to apply a magnetic field in a direction other than up or down . this will split the electron into appropriate superpositions of spin along that new direction . if you make the new magnetic field in some horizontal direction u ( so you are perpendicular to spin-up ) , you will have a superposition : $ ( |+u\rangle+|-u\rangle ) /\sqrt{2}$ . those two superpositions have two different energies . when an electron absorbs a single photon , its spin flips . so you can convert a $|-u\rangle$ electron to the $|+u\rangle$ state by arranging for it to absorb a photon oriented in the +u direction . the $|+u\rangle$ electron can not absorb such a photon because of conservation of angular momentum . therefore , you have just added energy to one half of your superposition . now turn off the magnetic field . you have created a state that is a superposition of two energy states . ( maybe more , depending how copacetic the excited energy state with the magnetic field on are with the excited energy states with no magnetic field . ) in fact , the magnetic field really was not necessary . i put it in there to make you think of the electron in terms of the +-u basis for its spin . all you really have to do is to arrange for the photon to have spin in the +u direction . by the way , a photon with spin in the +u direction is " circularly polarized " . also , the turning on and off of the magnetic field needs to be done slowly . to arrange for particular relative phases in the superposition between the + and - spins you can use the technique of " quantum phase " , or " berry-pancharatnam phase " . this is the phase acquired when a system is slowly sent through a sequence of states ( but which also applies to sudden state changes with the same topology ) . one can induce a phase by a slow change to the spin axis . the phase one obtains is equal to half the spherical area cut out by the spin axis in the bloch sphere ( i.e. . the set of possible directions for a spin axis ) . to get a relative phase you had want to " park " one of the spin states by , say , arranging for it to have an energy that prevents its spin state from being modified , and then send the other state through a sequence .
1 ) some of the assumptions of the gross-pitaevskii equation ( gpe ) are : all atoms are in the same condensate wave function , the condensate is at $t=0$ , collisions between atoms are sufficiently low energy that the interactions can be well described by the $s$-wave scattering length , so that the interaction can be written $g\delta ( \mathbf{x}_i-\mathbf{x}_j ) $ . generalized gpes can also be solved , allowing for thermal and quantum depletion ( some atoms not in the condensate ) and allowing for other forms of interaction , such as dipolar . 2 ) the interaction term , $g|\psi ( \mathbf{x} ) |^2$ , is in addition to the external potential $v_\mathrm{ext} ( \mathbf{x} ) $ , the effective potential is the sum of both : $v_\mathrm{ext} ( \mathbf{x} ) +g|\psi ( \mathbf{x} ) |^2$ . the condensate density is $n_0 ( \mathbf{x} ) =|\psi ( \mathbf{x} ) |^2$ , so the interaction term is $gn_0 ( \mathbf{x} ) $ which is the potential due to interaction with the condensate itself . more detail in response to the op 's comment : the interaction potential between two atoms can usually be written as $v ( \mathbf{r}_{ij} ) $ where $\mathbf{r}_{ij} =\mathbf{x}_i-\mathbf{x}_j$ . for neutral atoms without a significant magnetic dipole moment , the dominant interaction is van der waals so $v ( \mathbf{r}_{ij} ) \propto r_{ij}^{-6}$ . when considering the scattering between two atoms , we can do a partial wave expansion ( matching incoming and outgoing wave functions and expanding in terms of legendre polynomials , e.g. " quantum mechanics " , ch . 17 , landau and lifshitz ) . for slow particles with van der waals interaction , the $s$-wave term is dominant and the interaction can be simplified to $v ( \mathbf{r}_{ij} ) = g \delta ( \mathbf{r}_{ij} ) $ where $g=4\pi\hbar^2 a_s/m$ and $a_s$ is the $s$-wave scattering length . to get a feel for the scattering length , in the the $s$-wave approximation , the cross section is $\sigma=4\pi a_s^2$ , so $a_s$ is a length scale for the interaction . the interaction potential in the gpe can be written $$\int d\mathbf{x'} v ( \mathbf{x}'-\mathbf{x} ) |\psi ( \mathbf{x'} ) |^2$$ when $v ( \mathbf{x}'-\mathbf{x} ) =g\delta ( \mathbf{x}'-\mathbf{x} ) $ , this simplifies to $$\int d\mathbf{x'} g\delta ( \mathbf{x}'-\mathbf{x} ) |\psi ( \mathbf{x'} ) |^2 = g|\psi ( \mathbf{x} ) |^2$$ 3 ) the external potential $v_\mathrm{ext} ( \mathbf{x} ) $ is generally due to applied optical or magnetic fields , and is often approximately a harmonic oscillator . the oscillator strength may be very strong in some directions creating quasi one or two dimensional confinement . a particle in a box is not possible yet ( the atoms would interact with the " walls" ) , but the external potential may be locally approximately uniform near the center of the trap . lattice potentials are also common , where ( in addition to harmonic confinement ) the atoms are trapped in a standing wave created by counterpropogating lasers resulting in a periodic potential . many other shapes are possible , such as toroids . a good reference is the book " bose-einstein condensation in dilute gases " by pethick and smith . this slightly dated review is also good ( free arxiv version here ) : section iii is relevant to your question 2 .
the pauli exclusion principle is not a repulsive force . it applies to fermions . it says that two electrons cannot occupy an energy state in a potential well with exactly the same quantum numbers . they have to differ by at least one quantum number . it is the pauli exclusion principle that organizes the electron shells filling them sequentially from low to higher energy levels in atoms , otherwise they would all pile up at the lowest energy level . also the periodic table of elements filling the baryons in the strong potential well . it makes matter as we know it . yet if you compress them really strongly , the electromagnetic interaction will no longer be the main force pushing them apart to balance the force that pushes them towards each other . instead , you get a a repulsive force as a consequence of the pauli exclusion principle . the above is a misunderstanding . it is not a force , since at the particle level forces have carriers that are exchanged between particles so that momentum and energy change . in your " compression " description there is a continuum and not a quantized state so the pep does not apply . when one scatters an electron on an electron one can get very close until the exchange particle ( the photon in this case ) transfers enough energy in the center of mass system to start creating other elementary particles . the process is accurately described by quantum electrodynamics .
analyzing one moving clock from the perspective of one stationary person will be inadequate to derive special relativity from . with just that set-up , you are not actually using the key fact that the speed of light is the same for all observers – all you are actually using is just the fact that the speed of light is finite . with just taking into account that the speed of light is finite , all you will arrive at is the non-relativistic doppler effect , which is different from time dilation .
no , from experimentation point of view ( check emilio 's comments ) . yes , from the " practical"-theoretical point of view . no , from the rigorous theoretical point of view . ( edited paragraph ) . let 's assume a planet with mass $m_p$ , and a object in which will make a slingshot , of mass $m$ . planet has speed $v_p$ . mass $m$ has speed $v$ . after slingshot , mass $m$ has speed $2v_p + v$ . linear momentum must conserve . so , initially $p_i$ and after slingshot $p_f$ . $$ p_i = mv + m_p v_p = m ( v + 2v_p ) + m_p v_p ' = p_f $$ where $v_p'$ is the final velocity of the planet . we can isolate it : $$ v_p ' = \frac{mv + m_p v_p - m ( v + 2v_p ) }{m_p} = \frac{m_p v_p - 2mv_p}{m_p} = v_p - \frac{2m}{m_p}v_p $$ therefore , the variation of planet speed $\delta v_p = v_p ' - v_p$ is : $$ \delta v_p = -\frac{2m}{m_p}v_p $$ now we can throw up $n$ times a mass $m$ object to peform slingshot , which means , $n$ slingshots . if $m_p &gt ; &gt ; m$ ( which is of course true since you will not slingshot a planet in another planet ) it is valid the approximation such that $\delta v_p \approx dv_p$ and then we integrate over $n$ slingshots . $$ \frac{dv_p}{v_p} = -\frac{2m}{m_p}dn \quad\longrightarrow\quad \int \frac{dv_p}{v_p}dn = -\int \frac{2m}{m_p}dn $$ $$ \ln v_p = -\frac{2m}{m_p}n + c $$ we can find out the integral constant such that be in function of $v_0$ , where $v_0$ is the initial speed of the planet . then we get a function $v_p ( n ) $ , which means , velocity of the planet is dependent from the amount $n$ of slingshots performed . we end up with : $$ v_p ( n ) = v_0 \exp\left ( -\frac{2m}{m_p}n\right ) $$ where $n$ is the number of slingshots . so , you can see each slingshot the planet speed drops exponentially , and therefore , rigorously never reaches zero . but , it will be close enough to zero after a lot slingshots . a nice observation , from here we can notice that after a slingshot , the planet speed is independent from the initial speed of mass $m$ object . only depends on mass $m$ of the object .
it matters what is on fire . if it is a flammable material that floats , like oil or gasoline , some fraction of it will remain on or return to the surface and you may have flames on the water . ( this is basically the only thing that i remember from watching black beauty as a kid . ) if it is a flammable material that sinks , the water will probably extinguish the flames , but the residual heat may boil the water around the fuel and send up clouds of steam . if it is a flammable material that reacts with water , like metallic sodium or lithium , it may burn more violently when it hits the water .
for the reasons that you already mentioned in the question , it would not be possible to modify stereoscopic depth without adding artifacts . so instead i would argue if the depth is actually increased . it probably is not . we have gotten used to watching monoscopic ( regular ) photographs with both our eyes . theoretically the perceived scale of the scenes on those photographs should be huge . but we have gotten used to it . so , the depth effect of an stereoscopic image that is captured in the range of monoscopic ( 0 camera distance ) up to a regular human eye-distance will be perceived natural . a small amount of depth is already obvious to the brain . if you would increase the camera distance beyond human eye distance , then your brain would get an unusual stimulus hence it might conclude that the scene is a miniature . also note that the viewing angle ( field of view ) of a photograph or mobile screen is also significantly smaller than the captured angle . not that it would compensate for reduced depth , but just another example of our tolerance in accepting the illusion of a photograph . if you are shooting images for a more immersive viewing experience ( eg . 3d cinema ) , then the tolerances are probably much smaller .
the term " focusing " means something else than the op suggests . it means that the different light rays coming that a single , specific point $p$ of the object emits to different directions re-converge back and reach the same place of the retina . if the object , and/or its point $p$ , is infinitely ( very ) far , then the light rays coming from $p$ are ( nearly ) parallel near the eye . but even if they are divergent , not parallel , the eye is able to refocus them so that they reach the same pixel of the retina . so the single convex lens do not magnify anything . at most , they do exactly what the lens in the eyes do and what is needed for focusing – to redirect the nearly parallel light rays so that they intersect against less than an inch from the eye 's surface , on the retina , again . the diagram posted above is misleading because it suggests that the two parallel rays that are supposed to converge to the same point of the retina come from different ends of the objects we observe , $p_1$ and $p_2$ . but that is not the case at all . if the diagram is fixed so that it makes sense , we see just one point $p$ and both ( nearly parallel ) light rays originate from the same $p$ . if we want to consider two points $p_1$ , $p_2$ of the object , as the bottom part of the picture clearly wants , they must create two distinct dots $q_1 , q_2$ on the retina , i.e. two different intersections of pairs of light rays ! so again , a single convex lens does not magnify anything . it just does what the eye has to do to focus , anyway : to make the light rays converge . to calculate whether an arrangement of lens ( and yes , at least two lens or lens+mirrors are needed ) are able to magnify , one has to consider the size of the image on the retina , i.e. different intersections of the light rays on the retina , separated from each other . to approximate the eye by a single dot is not good enough to calculate the magnification !
the classical equations of motion are not affected by changing the lagrangian $$l \qquad \longrightarrow \qquad l&#39 ; = l+ \frac{df}{dt}$$ by a total time derivative . put $f= -q_1 q_2$ . then $$l&#39 ; = -\frac{1}{2} ( q_1^2 + q_2^2 ) . $$ this lagrangian $l&#39 ; $ does not contain time derivatives , and thus there are no dynamics . the classical equations of motion are $$ q_1=0 \qquad \mathrm{and}\qquad q_2=0 , $$ in conflict with what is said in the original question formulation ( v1 ) .
jerry schirmer 's answer applies in an infinite space . if you put the system in a box there is no problem : the normalized wavefunction is $\mathrm{e}^{ikx}/\sqrt{v}$ . this is the usual theoretical device to make everything nice and well behaved . then we take the limit $v\rightarrow\infty$ at the end of the day and , if we have done our job correctly , all of the $v$ dependence should drop out of the final answer . it is not too unreasonable either , since we do not actually know that the universe is infinite . infinite for all practical purposes might as well just be a big box . edit in response to comments : assume we operate in a box of volume $v$ ( in any number of dimensions - interpret as length , area or volume as appropriate ) . we want to find the normalisation factor $n$ for the wavefunction $\psi = n \mathrm{e}^{i\vec{k}\cdot\vec{x}}$ . so we calculate the norm of the wavefunction , which must be equal to one : $$ \begin{array}{lcl} 1 and = and \int\mathrm{d}x\ \psi^\star \psi \\ and = and \int\mathrm{d}x\ n^\star n \\ and = and \left| n \right|^2 v \end{array}$$ so $n$ has a magnitude of $1/\sqrt{v}$ and an arbitrary phase which can be chosen to make it real and positive .
just compare the resolution of the two : prism depending on n , there is no good material n> 1.7 ( besides diamond ) depending on base length if you use a equilateral triangle have to use more than one to overcome this prism absorb light , you have got scattering ( stray light ) too now a grating : optimize it for your wavelength choose lines per milimeter resolution depending on the number of lines that are illuminated compact device just transmission gratings have got absorption , you can do your measurement in reflection with a blazed grating design your blazed grating to get the most light in e.g. 2nd order quantitatively prism : $\frac{\lambda}{\delta \lambda} = t \frac{dn}{d\lambda}$ grating : $\frac{\lambda}{\delta \lambda} = \frac{zd}{g}=zn$ where t is your base length , z . . . order of spectrum , g . . . grating constant , d . . . entrance beam diameter , n . . . number of illuminated lines so just use a grating , nowadays they can be fabricated in excellent quality . on my university learning the pros of a diffraction grating is part of the 1st year laboratory exercises .
we are talking just newtonian gravity here . you should probably know that the orbit in this case is just a conic section . depending on the initial angular momentum and energy you can compute the closest point to the star on the orbit . the object will fall into star if and only if this point is inside the star . now , supposing you already determined that the object will fall into the star , you can solve for the point of crossing star 's surface ( this is just geometry , intersection of the conic section and the circle ) and then solving for the time of arrival into that point . all of this is simplified by noting that you only need to know the $r$-coordinate which is simply the radius of the star . so this all boils down to finding a solution to your equation . you can do that just by taking a square root , separating variables and computing the integral . that will give you dependence of $t = t ( r ) $ . so then just plug in the radius of the star and you are done . this means that the problem is reduced to finding integrals . try some software for that ( because the integral does not look easy ) like wolfram mathematica integrator . if you have any problems with this i suggest you ask what to do next at math . se to get better answers .
in the case of a real , scalar field , we start from the action $$s=\int d^4x\ \eta^{\mu\nu}\partial_\mu\phi\partial_\nu\phi+m^2\phi^2$$ the equation of motion is the klein-gordon equation $$ ( -\eta^{\mu\nu}\partial_\mu\partial^\nu+m^2 ) \phi=\ddot{\phi}-\nabla^2 \phi+m^2\phi=0$$ now , we introduce the fourier modes $\phi_k$: $$\phi=\int \frac{d^3k}{ ( 2\pi ) ^3}e^{i\vec{k}\cdot\vec{x}}\phi_k $$ in terms of the fourier modes , the equation of motion becomes $$\ddot{\phi}_k +\omega^2\phi_k=0 , \hspace{2cm}\omega\equiv \sqrt{k^2+m^2}$$ as you see , this is just an independent harmonic oscillator equation for each value of $k$ . this is what it means to say that the different fourier modes are independent . when we introduce an interaction term proportional to $\phi^n$ where $n\geq 3$ ( e . g . the canonical $\lambda \phi^4$ ) , we will have terms proportional to $\phi^{n-1}$ ( $4\lambda\phi^3$ in my example ) in the equation of motion . when going to fourier space , these powers of $\phi$ all have ' their own label ' , so we obtain a term something like $\phi_{k_1}\phi_{k_2}\dots\phi_{k_{n-1}}$ . as you see , the equation of motion no longer depends only on a single fourier mode ! the fourier modes are now coupled .
consider an expression of the form $$\int_x^c f ( x , y ) \mathrm{d}y$$ where $c$ is a constant . you can think of this as a mapping from real numbers to real numbers : you pick any real number $x$ , plug it in , and calculate the value of the integral . that is exactly what a single-variable function is . so you can label this function $i_c$ , define it as $$i_c ( x ) = \int_x^c f ( x , y ) \mathrm{d}y$$ and then it should make sense that you can integrate it like any other function : $$\int_a^b i_c ( x ) \mathrm{d}x = \int_a^b\int_x^c f ( x , y ) \mathrm{d}y\ , \mathrm{d}x$$ so that tells you that integration over a variable that appears in the limit of an inner integral is a perfectly reasonable thing to do , and conceptually , there is nothing complicated about it . actually coming up with a symbolic expression for the double integral is another matter , of course . in this case , the major step in going from equations ( 4 ) and ( 5 ) is integrating over $\varphi$ , so let 's do that : for the term on the left side , $$\int_{\color{red}\varphi}^{\varphi_w}\epsilon_o\frac{\mathrm{d}}{\mathrm{d}\varphi}\biggl ( \frac{e^2}{2}\biggr ) \mathrm{d}\varphi = \epsilon_o\biggl ( \frac{e_w^2}{2} - \frac{\color{red}{e}^2}{2}\biggr ) \tag{a1}$$ where $e_w = e ( \varphi_w ) $ and $\color{red}{e} = e ( \color{red}{\varphi} ) $ and for the first term on the right , $$\int_{\color{red}\varphi}^{\varphi_w}\sqrt{\frac{m_e}{2e}}\frac{j_{eo}}{\sqrt{\varphi}}\mathrm{d}\varphi = \sqrt{\frac{m_e}{2e}}j_{eo}\bigl ( 2\sqrt{\varphi_w} - 2\sqrt{\color{red}{\varphi}}\bigr ) \tag{a2}$$ the reason i am using some red variables , by the way , is that when a variable of integration appears as one of the limits , you should consider it to be a separate variable inside and outside the integral . so think of $\color{red}{\varphi}$ and $\varphi$ as different variables . if you prefer , you could give the variable a different label instead of using a different color ( so you could call it , say , $\varphi_b$ , instead of $\color{red}{\varphi}$ ) . anyway , that was easy . the tricky term is $$\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\int_\varphi^{\varphi_w}\frac{\varphi'/\varphi_i - 1}{\sqrt{\varphi ' - \varphi}}\frac{\mathrm{d}\varphi'}{e'}\mathrm{d}\varphi\tag{a3}$$ in this one you can not do the integral over $\varphi'$ because you do not know $e ' = e ( \varphi' ) $ and you do not have enough information about it to express it as something you can integrate . what they have done in the paper is exchange the order of integration . this is a procedure you can use on any multiple integral where the limit of the inner integral depends on an outer variable of integration . for example , in a double integral of the form $$\int_a^b \int_x^b f ( x , y ) \mathrm{d}y\ , \mathrm{d}x$$ the region of integration is $$\begin{align} a and \leq x \leq b and x and \leq y \leq b \end{align}$$ which is the blue shaded region in this picture : but you can express the same region as $$\begin{align} a and \leq y \leq b and a and \leq x \leq y \end{align}$$ as shown by this picture : which gives you this identity $$\int_a^b \int_x^b f ( x , y ) \mathrm{d}y\ , \mathrm{d}x = \int_a^b \int_a^y f ( x , y ) \mathrm{d}x\ , \mathrm{d}y$$ using this procedure on equation ( a3 ) from above turns it into $$\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\int_{\color{red}{\varphi}}^{\varphi'}\frac{\varphi'/\varphi_i - 1}{\sqrt{\varphi ' - \varphi}}\frac{1}{e'}\mathrm{d}\varphi\ , \mathrm{d}\varphi'$$ which is a fairly simple function of $\varphi$ , integrable as $\int 1/\sqrt{\varphi'-\varphi}\mathrm{d}\varphi = -2\sqrt{\varphi'-\varphi}$ . the full result after performing the inner integral over $\varphi$ is $$\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\biggl ( \frac{\varphi'}{\varphi_i} - 1\biggr ) \bigl ( -2\underbrace{\sqrt{\varphi ' - \varphi'}}_{0} + 2\sqrt{\varphi ' - \color{red}{\varphi}}\bigr ) \frac{\mathrm{d}\varphi'}{e'}\tag{a4}$$ putting together all the terms , ( a1 ) , ( a2 ) , and ( a4 ) , we get $$\begin{multline}\epsilon_o\biggl ( \frac{e_w^2}{2} - \frac{\color{red}{e}^2}{2}\biggr ) = 2\sqrt{\frac{m_e}{2e}}j_{eo}\bigl ( \sqrt{\varphi_w} - \sqrt{\color{red}{\varphi}}\bigr ) \\ - 2\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\biggl ( \frac{\varphi'}{\varphi_i} - 1\biggr ) \sqrt{\varphi ' - \color{red}{\varphi}}\frac{\mathrm{d}\varphi'}{e'}\end{multline}$$ then divide both sides by $2\sqrt{\frac{m_e}{2e}}j_{eo}$ and you get equation ( 5 ) , except for one factor of 2 on the left side which i can not seem to account for ( maybe it is lost somewhere in my calculations ) .
start with differential form of poisson 's ratio : $$\frac{\text{d} x}{x}=- \nu \frac{\text{d} l}{l}$$ $$\int_{x_0}^{x_0+\delta x} \frac{\text{d} x}{x}=- \nu \int_{l_0}^{l_0+\delta l} \frac{\text{d} l}{l}$$ $$\ln \frac{x_0+\delta x}{x_0}=- \nu \ln \frac{l_0+\delta l}{l_0}$$ $$1 + \frac{\delta x}{x_0}=\left ( 1+\dfrac{\delta l}{l_0}\right ) ^{-\nu}$$
it does not sound exactly like a chord , but this type of technique was widely used in the 8-bit and 16-bit computer game eras , when the number of sound channels available was limited . it has a very distinctive retro video game sound , but the ear is able to identify the chord that it is supposed to be . here is a youtube video explaining how to achieve the effect using synthesis software - your situation is different , but you can probably adapt some of the advice . you can experiment with the speed of modulation , but the guy in the video sets it to around 30 hz , which sounds good . actually , i think you should be able to achieve the effect of a chord through a different method . to play a note at a frequency of $f_1$ at the same time as a note with frequency $f_2$ , you just need to time the sparks so that there are sparks at times $0 , \frac{1}{f_1} , \frac{2}{f_1} , \frac{3}{f_1} , \dots$ and also at times $\frac{1}{f_2} , \frac{2}{f_2} , \frac{3}{f_2} , \dots$ . this will sound to the ear exactly like the two notes being played at once , because that is essentially what it is .
if m1 has a positive velocity and m2 has a negative velocity , you get (m1.velocity - m2.velocity) = (positive - negative) = positive . on the other hand if the signs are switched , you will get a negative result because the particles are moving away from each other .
a slight variant on your fine answer . . . a reference is ramo et al , fields and waves in communication electronics , chapter 12 . first , reciprocity : $z_{21}=z_{12}$ tells you that ( assuming a conjugate-matched load ) : $$ g_{dt} a_{er} = g_{dr} a_{et}$$ for both transmitting ( subscript t ) and receiving ( r ) antennas , $g_d$ is the antenna directional gain . $a_{er}$ is the effective area of the receiving antenna , defined as the ratio of useful power removed from the receiving antenna $w_r$ to average power density $p_{av}$ in the incoming radiation . thus the ratio $g_d/a_e$ is the same for both transmitting and receiving antennas . for large aperture antennas , it can be shown that the maximum possible gain satisfies : $$ \frac{ ( g_d ) _{max}}{a_e} = \frac{4 \pi}{\lambda^2} $$ for other geometries , $a_e$ is defined to give the same result . for example , for a hertzian dipole , with a maximum directivity of 1.5: $$ ( a_e ) _{max} = \frac{\lambda^2}{4 \pi} ( g_d ) _{max} = \frac{3}{8 \pi} \lambda^2 $$ anyway , for the problem at hand , as you deduced , the useful power removed from the receiving antenna is : $$ w_r = p_{av} a_{er} \text{ , with the power density } p_{av} = \frac{e_b^2}{2 z_o} , z_o=377 \text{ ohms} $$ ( here , electric field and voltage are sinusoids measured as peak values . ) with a conjugate-matched load with real part $r_l$ , equating load power dissipated with power delivered gives for the receiving antenna 's thevenin equivalent source voltage $v_a$: $$\frac{ ( v_a/2 ) ^2}{2 r_l} = \frac{e_b^2}{2 z_o} a_{er} $$ $$ v_a = 2 \sqrt{a_{er}} \sqrt{\frac{r_l}{z_o}} \ , e_b $$ substituting for $a_{er}$ from the reciprocity relation , the maximum voltage $v_{a , max}$ is : $$ v_{a , max} = \sqrt{\frac{ ( g_{dr} ) _{max}}{\pi }} \sqrt{\frac{r_l}{z_o}} \ , \ , \lambda e_b $$ i am cautious about the $\cos \psi$ factor because beam patterns differ for different antennas .
outside the light-cone , distances are spacelike and not timelike . this means that you can always find a frame of reference such that your " spacially displaced past " is in the future for some other inertial frame of reference . by definition , a spacelike distance is neither in the past nor the future . by itself , this would not create any causality violations . since wormholes ( as shown in other answers ) could set up a scenario where you do violate causality , i think you will find you cannot use wormholes to travel outside of your light-cone .
i give the answer with the general method even though a much straightforward way would be to guess the result because it is simple . the unit of $h$ is the inverse of time denoted by $ [ \mathrm t^{-1} ] $ . the dimension of a temperature is denoted by $ [ \theta ] $ . to find the numerical value of $t$ in kelvin , one should find a combination of $c : [ \mathrm{lt^{-1}} ] $ , $\hbar= [ \mathrm{ml^2t^{-1}} ] $ , $\mathcal g : [ \mathrm{m^{-1}l^3t^{-2}} ] $ and $k_{\mathrm b}: [ \mathrm{ml^2t^{-2}\theta^{-1}} ] $ ( $ [ \mathrm l ] $ and $ [ \mathrm m ] $ represent length and mass respectively ) such that $$ c^x \ ; \hbar^y \ ; \mathcal g ^z\ ; k_{\mathrm b}^u\times h$$ has the dimension of a temperature ( $x$ , $y$ , $z$ and $u$ are unknown ) . this gives the system $$\left\{\begin{array}{rcc} y-z+u and =0 and \quad [ \mathrm m ] \\ x+2y+3z+2u and =0 and \quad [ \mathrm l ] \\ -x-y-2z-2u and =1 and \quad [ \mathrm t ] \\ -u and =1 and \quad [ \theta ] \end{array}\right . $$ the solution is $$\left\{\begin{array}{cl} x and =0\\ y and =1\\ z and =0\\ u and =-1 \end{array}\right . $$ we obtain thus $$t=\frac{\hbar}{2\pi k_{\mathrm b}}h . $$ the value of $h$ is $h=67.8\ , \mathrm{km . s^{-1} . mpc^{-1}}=2.194\times 10^{-18}\ , \mathrm{m . s^{-1}}$ . we find a temperature of $t=2.67\times10^{-30}\ , \mathrm k$ .
after stating the solution , i will try to give some physical insights to the best of my knowledge and some more references . the dimension of the required state space is given by the verlinde formula , having the following form for a general compact semisimple lie group $g$ on a riemann surface with genus $g$ corresponding to the level $k$: $$ \mathrm{dim} v_{g , k} = ( c ( k+h ) ^r ) ^{g-1} \sum_{\lambda \in \lambda_k}\prod_{\alpha \in \delta} ( 1-e^{i\frac{\alpha . ( \lambda+\rho ) }{k+h}} ) ^{ ( 1-g ) }$$ ( please see blau and thompson equation 1.2 . ) . here , $c$ is the order of the center , $h$ is dual coxeter invariant , $\rho$ is half the sum of the positive roots , and $r$ is the rank of $g$ . $g$ is the genus , $\delta$ is the set of roots and $\lambda_k$ is the set of integrable highest weights of the kac-moody algebra $g_k$ . for the torus ( $g=1$ ) , this formula simplifies to : $$ \mathrm{dim} v_{\mathrm{torus} , k} = \# \lambda_k $$ i.e. , the dimension is equal to the number of integrable highest weights of the kac-moody algebra $g_k$ . the integrable highest weights of a level-$k$ kac-moody algebra are given by the following constraints : $$ \lambda - \mathrm{dominant} , 0 \leq \sum_{i=1}^r \frac{2 \lambda . \alpha^{ ( i ) }}{ \alpha^{ ( i ) } . \alpha^{ ( i ) }}\leq k$$ where $ \alpha^{ ( i ) }$ are the simple roots , please see , for example , the following review by fuchs on kac-moody algebras . ( my favorite reference for the representation theory of kac-moody algebras is the goddard and olive review which seems not available on line ) for example for $su ( 3 ) _k$ whose dominant weights are $2$-tuples of nonnegative numbers $ ( n_1 , n_2 ) $ , the above condition reduces to : $$\mathrm{dim} v^{su ( 3 ) }_{\mathrm{torus} , k} = \# ( n_1\geq 0 , n_2\geq 0 , 0\leq n_1 + n_2 \leq k ) = \frac{ ( k+1 ) ( k+2 ) }{2}$$ to perform the computations for the more general cases , one can use the seminal review by slansky . the verlinde formula was discovered before the chern-simons theory came into the world . originally it is the dimension of the space of conformal blocks for the wzw model . this formula has been derived in a large variety of ways , please , see footnote 26 in the fuchs review . it is still an active research topic , please see for example a new derivation in this recent article by gukov . the chern-simons theory may be the most sophisticated example in which the dirac quantization postulates can be carried out in spirit . ( more precisely their generalization in geometric quantization ) . i mean starting from a phase space and utilizing a specified set of rules to associate a hilbert space to it . in the case of the chern-simons theory , the phase space is the set of solutions of the classical equations of motion . the classical equations of motion require the field strength to vanish , in other words the connection to be flat . this phase space ( the moduli space of flat connections ) is finite dimensional , it has a kähler structure and it can geometrically quantized as a kähler manifold , just like the case of the harmonic oscillator . thus the problem can be reduced in principle to a problem in quantum mechanics . the case of the torus is the easiest because everything can be carried out explicitly in the abelian and the non-abelian case , please see the following explicit construction by bos and nair , ( a more concise treatment appears in dunne 's review ) . in the case of the torus , the moduli space of flat connections in the abelian case is also a torus and in the non-abelian case it is : $$\mathcal{m} = \frac{t \times t}{w}$$ where $t$ is the maximal torus of $g$ . basically , a fock quantization can be carried away , but there is a further restriction on the admissible wave functions coming from the invariance requirement under the large gauge transformations ( please see for example , the dunne 's review ) . the invariant wave functions are called non-abelian theta functions and they are just in a one to one correspondence with the kac-moody algebra integrable highest weights . ( in the abelian case , the wave functions are the jacobi theta functions ) . in the higher genus case , although the quantization program leading to the verlinde formula can be carried out in principle , few explicit results are known , please see the following article by lisa jeffrey ( and also the following lecture notes ) . the dimension of these moduli spaces is known . in addition . witten in an ingenious work computed their symplectic volumes and their cohomology ring in some cases . witten 's idea is that as in the case of a simple spin , the dimension of the hilbert space in the semiclassical limit ( $k \rightarrow \infty$ ) becomes proportional to the volume and the leading exponent of $k$ is the complex dimension of the moduli space ( please observe for example , that in the case of $su ( 3 ) $ on the torus , the leading exponent is $2$ which is the rank of $su ( 3 ) $ which is the dimension of the maximal torus $t$ ) .
no . when you hit the wall , the bicycle rotates around the front axis . the angular momentum l that you create for an arbitrary number of mass particles is $$l=\sigma_i ( r_i \times m_iv_i ) . $$ if you split location r=r+r_i and v=v+v_i with r and v being center of mass location and velocity , respectively , and r_i and v_i deviation from it , then it can be shown that l does not change when the center of mass does not change . so , the wood block on wheels should work ( in theory ) .
firstly , you can not just assert that $v=ex$ . absolute potential is not defined here , since there is an infinite increase of potential energy when going from $x=-\infty$ to $x=+\infty$ . we can only use absolute potential when the assertion that absolute potential is $0$ at infinity holds . however , this was not really your issue here , it would have worked regardless . you basically forgot a negative sign while calculating v : $$\delta v=\color{red}{-}\int\vec e\cdot d\vec l$$ so , we use : $$\delta ke+\delta pe=0$$ and , $$\delta pe= ( -e ) \delta v= ( -e ) ( -\int\vec e\cdot d\vec l ) =ee\delta x$$ $$\frac12m0^2-\frac12mv_0^2+ee\delta x=0$$ $$\implies eex=\frac12mv_0^2\implies x=\frac{mv_0^2}{2e}$$ which solves the problem .
first make sure all your screws are tight , and that there is not any shaking because of slack in any areas where things connect to each other . another thing you can do is buy vibration dampening pads to put your tripod on . finally , you can add counter weights and pendulum weights to the tripod to give it more mass to withstand the wind and touches .
i would like to expand a bit on the answer to the second question , but for completeness i will do both . as said in brightblades ' answer , the expansion of space is not limit by the speed of light . so objects can be moving at moderate speeds , but because the space between them and us expands faster than light , it is emissions will never reach us . for this reason , there are already regions of space that are beyond our horizon . whether or not this will always be depends on what " big thing " your cosmology ends with . if it is a " crunch " , then everything comes back together in a reverse " bang " at the end of time . if it is just a " freeze " , then the acceleration expands and possibly even accelerates forever . abraham loeb wrote a curious paper ( available on the arxiv ) about how to reach cosmological conclusions in the absence of nearby galaxies . about 100 billion years from now , all the galaxies in our local group will be beyond the horizon of the milky way ( or rather milkomeda , after the milky way collides with andromeda ) , and the cmb will be at a wavelength longer than the observable universe . but you will still be able to reach conclusions about cosmology by using hypervelocity stars being ejected from the galaxy . the point is that you can still get accurate results about the global structure of the universe using local results . as for our current model , we have a great deal of evidence that the cosmos is structured according to the concordance model . you can always say " it might turn out to be wrong " , but it can not turn out to be that wrong because of said evidence . it is like gr as a generalization of newtonian gravity : yes , newton was " incorrect " but his theory was also quite accurate , to the extent that we still use it for , say , n-body simulations of star clusters .
light++ it is not open source but you can try to contact the author , werner benger . a few years ago we have access to the source code of ' light++' . not anymore : ( light++ raytracer ! ( general relativistic raytracing ) simulation of a black hole by raytracing the black earth about the simulation of galactic close encounters , or a n-body general simulation , under the constraints of gr i found nothing . edit add " i found nothing " can be read like this " there is not a single software package " because , afaik , no one knows how to apply gr in the computation of planetary and galaxy dynamics ( small scale with matter ) . the zeldovich approximation is used in the linearization of gr ( with caveats ) : and has been successfully applied to describe the large scale clustering in the distribution of galaxy clusters . . . however , within the zeldovich prescription , after a pancake forms in correspondence of crossing of particle orbits , such particles continue travelling along straight lines , . . i think that your aim is hopeless because gr is around since 1917 and no one succeeded . interesting questions , imo : how close to the reality are the simulations that are performed with newtonian codes . what kind of problems we may expect if we are gonna try to do a simulation code . edit add end
beam_mass mass of the beam particles in gev target_mass mass of the target particles threshold minimum energy for the reaction to occur final_state_multiplicity number of particles in the final state plab ( gev/c ) momentum of the beam particle w.r.t. lab reference coordinate ? yes lab_min , plab_max minimum and maximum of plab . it is a bit of a problem , as one should change the standard chi2 formula , which only takes care of y errors . typically , the fits presented neglect this . sy_er+ ( pct ) , sy_er- ( pct ) : positive and negative systematic errors in % reference flag " elioff 62 pr 128 , 869" , author , year , journal ( pr=physical review ) , volume , page number
carbon 14 has a mass of 14 , not 12 .
your formula seems slightly odd to me as in qm one usually deals with operators and if for the planetary case we have the relation : $$ e = \frac 1 2 m v^2 + u_{eff} = \frac 1 2 m v^2 + \frac{l^2}{2mr^2} + \frac{gmm}{r} $$ then in quantum mechanics the hamiltonian for the hydrogen like atom ( only one electron ) would look like : $$ \hat h = \frac{\hat p^2}{2m} - \frac 1 {4\pi\varepsilon_0} \frac{ze^2}{r} = - \frac{\hbar^2}{2m} \nabla^2 - \frac{ze^2}{4\pi\varepsilon_0r} $$ where then we use the hamiltonian to solve the schroedinger equation : $$ \hat h \left| \psi \right&gt ; = e \left| \psi \right&gt ; $$ which is just a simple eigenvalue eigenfunction problem . now the thing is that we can recast the formula to a somewhat more understandable expression : $$ \hat h = - \frac{\hbar^2}{2m} \frac 1 {r^2} \frac{\partial}{\partial r}\left ( r^2 \frac{\partial}{\partial r}\right ) + \frac{\hat l^2}{2mr^2} - \frac{ze^2}{4\pi\varepsilon_0r} $$ where the operator $\hat l$ contains all the angular bits of the laplacian in spherical coordinates . then , you can solve the schroedinger equation to find the energy values . as you see , your intuition was correct , that there is quite a lot of similarity between the two problems , but the actual formalism is quite different . note : $\hbar$ is a constant , so your premise that it is the angular momentum of the electron is not quite right , as it is just a prefactor . the total angular momentum is given by the formula of : $$ l^2 = \hbar^2 \ell ( \ell + 1 ) $$ where $\ell$ is a non-negative integer value describing the angular momentum state of the system . i hope that this helps you to get on the right track . edit : i was not entirely correct with the first version of the additional note .
to calibrate our expectations , consider the largest nuclear weapon ever detonated , the tsar bomba . it is yield was at most about $58$ megatons tnt equivalent , or about $2.43\times10^{24}$ erg . now , let 's consider a smallish star , something like gliese 581 , which is reasonably nearby , small and faint , and has a planetary system ( of some sort : the number of planets is debated ) . it has a luminosity of $0.013$ times solar , which is roughly $5\times10^{31}$ erg . s$^{-1}$ . in other words , the luminosity of gliese 581b is about 20 million tsar bombas per second . this says nothing , however , about in what waveband the emission occurs , but i think the energetic argument is quite strong . . . ( i.e. . maybe if the nuclear bomb peaks in gamma-rays you could separate it from the starlight , but i do not know about our detection capabilities or the gamma-ray emission from the star ) . but , what about bigger things ? like an asteroid similar to the one that killed the dinosaurs ? it is yield was 100 teratons of tnt equivalent , or about one-twelfth of gliese 581 's per second luminosity . which might sound hopeful , but i suspect it took many seconds for that energy to come out , in which case it would still be washed about by the starlight . it turns out stars are quite bright in absolute terms !
you can derive the relativistic doppler shift from the lorentz transformations . let 's start in the frame of the moving rocket , and let 's take two events corresponding to nodes in the emitted wave ( i.e. . 1/$f$ ) . then in the rocket 's frame the two events are ( 0 , 0 ) and ( $\tau$ , 0 ) , where $\tau$ is the period of the radiated wave . to see what the period of the radiation is in our frame we just have to use the lorentz transformations to transform these two spacetime points into our frame . for simplicity we will take our rest frame and the frame of the rocket to coincide at $t = 0$ . this is convenient because then the first event is just ( 0 , 0 ) in both frames . now the lorentz transformations tell us : $$ t ' = \gamma \left ( t - \frac{vx}{c^2} \right ) $$ $$ x ' = \gamma \left ( x - vt \right ) $$ if we are tranforming from the rocket 's frame to ours , and the rocket is moving at velocity $v$ wrt us , then we have to put the velocity in as $-v$ , and we are transforming the point ( $\tau$ , 0 ) . putting these in the lorentz transformations we find that the point ( $\tau$ , 0 ) in the rocket 's frame transforms to the point ( $\gamma \tau$ , $\gamma v \tau$ ) in our frame . the last step is to note that if we are sitting at the origin in our frame the light from the event at ( $\gamma \tau$ , $\gamma v \tau$ ) takes a time $\gamma v \tau/c$ to reach us . so the time we see the second event is $\gamma \tau + \gamma v \tau/c$ and this is equal to the period of the radiation , $\tau'$ in our frame : $$ \tau ' = \gamma t + \gamma v t/c $$ we just need to rearrange this to get the usual formula . noting that $f'$ = 1/$\tau'$ and $f$ = 1/$\tau$ we take the reciprocal of both sides to get : $$ f ' = f \frac{1}{\gamma ( 1 + v/c ) } $$ to simplify this note that : $$\begin{align} \frac{1}{\gamma} and = \sqrt{1 - \frac{v^2}{c^2}} \\ and = \sqrt{ ( 1 - \frac{v}{c} ) ( 1 + \frac{v}{c} ) } \end{align}$$ and substituting this back in our expression for $f'$ we get : $$\begin{align} f ' and = f \frac{\sqrt{ ( 1 - v/c ) ( 1 + v/c ) }}{1 + v/c} \\ and = f \frac{\sqrt{ ( 1 - v/c ) }}{\sqrt{1 + v/c}} \\ and = f \sqrt{\frac{c - v}{c + v}} \end{align}$$ and presto it is proved !
there are lots of questions here that i will try to answer , hopefully i will get to them all . . . creature comforts it is hard to " just fly higher " when you consider passenger planes . supersonic military aircraft like the sr-71 do fly ridiculously high . it is service ceiling is 85,000 feet ! but , it has the advantage that it does not need to keep anybody but the pilot comfortable . the issue deals with pressurization . as you increase altitude , the aircraft must also be able to withstand a larger pressure differential if the cabin will be kept at a comfortable pressure . most very high altitude military aircraft do not pressurize the cabin ; rather , the pilot wears a pressure suit . imagine if you had to suit up for a flight to visit relatives ! it is not that we can not build a plane that can withstand the pressure difference , but doing so would require very heavy or very expensive materials . the former makes it much harder to fly while the latter makes it not very commercially viable . increased drag there is a reason going past the speed of sound was called " breaking the sound barrier . " there is a magic number called the drag divergence mach number ( mach number is the fraction of the speed of sound at which you are traveling ) . beyond this number , the drag increases tremendously until you are supersonic , at which point it decreases quite rapidly ( but is still higher than subsonic ) . therein lies one of the biggest problems . you need very powerful engines to break the barrier , but then they do not need to be very powerful on the other side of it . so it is inefficient from a weight/cost standpoint because the engines are so over-engineered at cruise conditions ( note : this does not imply the engines are inefficient on their own ) . increased heat there is no denying that it will get hot . it is storied that the sr-71 would get so hot and the metal would expand so much , that when it was fully fueled on the runway , the fuel would leak out of the gaps in the skin . the plane would have to take off , fly supersonic to heat the skin enough to close the gaps in the metal , then be refueled mid-air again because it used it all up . then it would go about it is mission . at the mach numbers for a commercial aircraft , the heating would not be as extreme . but it would require some careful engineering , which makes it more expensive . so why can not it just fly higher ? ignoring international law for a moment , there is several reasons why flying higher just is not as viable : cabin pressure issues emergency procedures : let 's assume for a moment we could pressurize the cabin . in the event it loses pressure , what do we do ? the normal procedure would be to dive down to a safe altitude , that takes considerably longer from 60,000 feet than 30,000 feet . drag is proportional to density , but so is lift . this means to fly higher , an aircraft needs bigger wings . but bigger wings mean more drag , so it gets into a vicious cycle . there is a sweet-spot that can be optimized for an ideal balance , but that means that " just go higher " may not be a good option . ceilings and speeds this one does not have to do entirely with legal issues , but that is part of it . a service ceiling is defined as the maximum altitude at which the aircraft can operate and maintain a specified rate of climb . this is entirely imposed by the aircraft design ( laws may require a minimum ceiling , but not a maximum . . . although they may restrict a plane from flying at the maximum ) . likewise , an absolute ceiling is the altitude at which the aircraft can maintain level flight at maximum thrust . naturally , as the plane burns fuel and becomes lighter , it needs less lift to stay at the same altitude . but the lift force is based solely on the geometry and speed , so actual lift will exceed what is needed and the plane will climb . as it climbs , the air density drops and so does lift . this means as the plane flies , it is absolute ceiling actually increases . now for the speeds . . . commercial aircraft fly as close as they can to the drag divergence mach number because it is the most economic point to fly . the plane goes as fast as it can go without the drag coefficient increasing tremendously . this is usually around mach 0.8 . but they can , and often do , go faster than that . it is not unusual for an airplane that is delayed taking off to land on time or even early . this happens because they can still go faster than they normally operate ( not significantly of course , perhaps mach 0.83-0.85 ) . it may cost some more fuel because the drag coefficient is likely increasing as it approaches mach 1 , but a delayed plane is more expensive for the airline than the extra fuel used ( maybe not in direct dollars , but in pr , reputation , etc . )
let 's imagine a swimmer in a swimming pool and approximate the earth as an inertial frame . the swimmer can certainly accelerate relative to the earth frame ( in the direction parallel to the earth 's surface ) ; we see this happen all the time in real life . it follows from newton 's second law , as you point out , that the net external force on the swimmer is nonzero . the only object that can possibly exert a horizontal force on the swimmer is the water in the pool ( the swimmer is making physical contact with nothing else , and the force due to gravity cannot affect his/her horizontal acceleration ) . it follows that the net horizontal force of the water on the swimmer is nonzero . if you had a blob of water floating around in outer space , and a swimmer inside , and if the blob of water and swimmer began stationary relative to an inertial frame , then the swimmer attempting to swim would cause some of the water to move backward , and this would propel the swimmer forward relative to the inertial frame . however , in this case the total external force on the blob+swimmer system would be zero , so the center of mass of the water+blob would remain stationary , but even in this case , the swimmer could accelerate relative to the inertial frame . essentially , the swimmer is doing the same thing that a torpedo would do ; he/she expels water backward , and this propels him/her forward .
heat pump should transfer heat from outside into the house . it should not generate heat ( ideally ) it should only force the heat to move . one joule of work executed by the heat pump can transfer several joules of heat - for example 4 joules ( it is the reason why the heat pump is efficient " source " of thermal energy ) . this ratio is called coefficient of performance or cop . the bigger is the difference between input and output temperature -> the more work must the heat pump do to transfer the heat . therefore the cop is decreasing . if the cop did not decrease with increasing temperature difference , it would be possible to construct a perpetual motion machine of the second kind . if work required to compress the gas was included into the computation the cop would be visible . and in order to achieve good cop the much lower output temperature would be needed . edit : computation of $cop$ with example . we will start from ideal heat engine modeled by carnot cycle . carnot cycle has efficiency $$\eta = \frac{t_h - t_c}{t_h}$$ let 's assume $t_h=310k$ , $t_c=270k$ and assume $100j$ of heat $q$ will be delivered from hot reservoir to the heat engine . as a result $$\eta \times 100j = \frac{310 - 270}{310} \times 100j = 12.9j$$ of work $w$ will be done by heat engine and $87.1j$ of heat will end up in cold reservoir . what happens when we reverse the process ? ( carnot cycle is reversible ) in the reversed process $87.1j$ of heat will be taken from cold reservoir , $12.9j$ of work will be delivered to the engine ( now the heat pump ) and $100j$ of heat will end up in the hot reservoir . the cop is $$cop = \frac{q}{w} = \frac{100j}{12.9j} = 7.75$$ and in general $$cop_{ideal} = \frac{q}{w} = \frac{q}{\eta \times q} = \frac{1}{\eta} = \frac{t_h}{t_h - t_c}$$
any physics equations you write down could be wrong , so you need to verify them experimentally , or you are just doing math . i can imagine a universe in which the gauss 's law does not work for moving charges ; and i have to test to see if we live in such a universe . in that sense , there is no non-experimental way to verify it . on the other hand , if you mean to ask if there is a mathematical way to prove it from the experimentally verified equations of electromagnetism , then sure ! but you need to be able to calculate the electric field of a moving charge . the derivation is a bit complicated , and i will leave it to you to find your favorite version of it , but the answer for a single charge moving at constant velocity is \begin{equation} \vec{e} = \frac{k\ , q\ , \vec{r}}{r^3}\ , \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}} \end{equation} it is enough to show this for just one charge because the integral is linear , so you can just add up contributions from different charges . also , we can take the integral over a sphere centered on this charge , since the divergence theorem tells us that moving the surface of integration will not change anything ( as long as we keep the charge inside ) . so , we do \begin{align} \oint \vec{e} \cdot d\vec{a} and = \oint \frac{k\ , q\ , \vec{r} \cdot \hat{r}}{r^3}\ , \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}}\ , r^2\ , \sin \theta\ , d\theta\ , d\phi \\ and = k\ , q\ , \int_0^{2\pi} \int_0^\pi \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}}\ , \sin \theta\ , d\theta\ , d\phi\\ and = 2\pi\ , k\ , q\ , \int_0^\pi \frac{1-v^2}{ ( 1-v^2\ , \sin^2\theta ) ^{3/2}}\ , \sin \theta\ , d\theta \\ and = 4\pi\ , k\ , q~ , \end{align} which is just the usual gauss 's law . this , of course , is only for constant velocities , as peter kravchuk pointed out below . to see it more generally , it would probably be easier to go over to the differential form of gauss 's law , which is one of maxwell 's equations . then , you can note that maxwell 's equations are relativistically covariant .
here is a rough explanation of what they do . you start with an integral over a complex variable $k=k+iz$ along a path $\mathcal c_0$ ( in the complex plan ) that follow the real axis ( that is a path characterized by $z=0$ and $k$ between $-\infty$ and $+\infty$ ) . calling $i$ the integral without the prefactors , we thus start with $$ i = \int_{\mathcal c_0} dk \frac{ke^{ik\delta x}}{\sqrt{k^2+m^2}} . $$ now the integrand is analytic everywhere but on the cuts , so you can change the contour the way you want as long as it does not go through the cut , and the integral still converges . the usual way to do this kind of integrals is to change the contour such that you just have to integrate around poles or along the cuts . but if you want to do that , you need that the contour has a contribution from $z$ at $\pm \infty$ , depending if you go along the cut in the upper/lower half plan . now you see the role of $\delta x$: if $\delta x&gt ; 0$ , $e^{ik\delta x}$ converges/diverges if $z\to\pm\infty$ , so you can not integrate along the lower cut ( the integral is divergent ) , but you can for the upper cut . that is how you get the contour in your graph , let 's call it $\mathcal c_1$ . to do the integral , you now have to parametrize $\mathcal c_1$ by two integrals : first the part along the left of the cut : $k=-\epsilon$ and $z$ going from $\infty$ to $m$ ; and second the part along the right of the cut : $k=\epsilon$ and $z$ going from $m$ to $\infty$ . $\epsilon$ is an infinitesimal number that you have to send to zero at some point . with all that , you should be able to recover the result you are looking for .
you should show your work , but my guess is that you have to notice the change of variables : $$\frac{d\chi}{dx}=\frac{d\chi}{d\xi}\frac{d\xi}{dx}$$ you need to do this a second time ( using the derivate of a product . see if you can continue from there .
the velocity after a time t 1 of accelerating is the starting velocity of the deceleration phase . thus $$a t_1 = b t_2$$ ( not worrying about the sign here . i suppose you could ) further you have $$\frac12at_1^2+\frac12bt_2^2=s$$ now you have two equations with two unknowns . solving : rearrange first equation $$ \frac{a}{b} = \frac{t_2}{t_1}\\ t_2=\frac ab t_1\\ t_2^2=\left ( \frac ab\right ) ^2 t_1^2 $$ now substitute into second equation : $$ \frac12at_1^2+\frac12\frac{a^2}{b}t_1^2=s\\ $$ substitute values : $$9t_1^2 + 27t_1^2=14400\\ t_1^2=\frac{14400}{36}\\ t_1=20\\ t_2=3t_1\\ t_1+t_2=80$$
it is a steady state . if there were a pressure gradient , there would be net force on the gas ( ignoring gravity ) . there is no net force here because the air is not accelerating . thus the pressure is constant . the number density varies across the box inversely to the temperature so the ideal gas law holds .
i am not an expert on the history , but wiki has a page on the history of entropy that may be helpful ( i could not tell you if it is accurate ) . note that the early work in thermodynamics was almost entirely experimental , with people working on the efficiency of heat engines and so on . so it was in that context that entropy emerged as a useful quantity , without any special " microscopic " interpretation attached to it . in any case the interpretation of entropy as " disorder " makes no sense before statistical mechanics , and it is innacurate and sometimes misleading even with statistical mechanics .
great and important question ; i hope this response is illuminating and encourages you and others to explore lie groups , lie algebras , and their representations . when you want to rotate a vector $\mathbf v$ in three dimensions , then you act on that vector with a rotation matrix $r$ to obtain a rotated vector $\mathbf v'$ related to the original vector by $$ \mathbf v ' = r\mathbf v $$ it is a mathematical fact that every rotation ( special orthogonal transformation ) $r$ is a rotation by some angle $\theta$ about some axis defined by a unit vector $\mathbf n$ , and that each such rotation can be written as the matrix exponential of a particular linear combination of certain 3-by-3 matrices $j_i$ called rotation generators . explicitly $$ r ( \theta , \mathbf n ) = e^{-i\theta n_i j_i} , \qquad ( j_i ) _{jk} = i\epsilon_{ijk} $$ so we can write the rotation of a vector as $$ \mathbf v ' = e^{-i\theta n_i j_i} \mathbf v $$ it turns out , that we can define the rotation of spinors in an analogous way . instead of the rotation generators $j_i$ which are 3-by-3 matrices , we choose the pauli matrices which are 2-by-2 , and for a given spinor $\xi$ , we define a rotated spinor by $$ \chi ' = e^{-i\frac{\theta}{2} n_i\sigma_i}\chi $$ notice how this is basically the same as rotating a vector in 3 dimensions , it is just that we have represented rotations acting on the vector space of spinors in a different way . the math behind all of this is called representation theory . in particular , when we are talking about spin , the representation theory is related to that of the lie groups $\mathrm{so} ( 3 ) $ and $\mathrm{su} ( 2 ) $ and their so-called lie algebras and their representations .
sure , it is no problem to do this . the thing that has to change is that $i$ should index over all possible configurations of the $n$ elements , and the energy in the boltzmann distribution has to be the total energy of the system . so if $m=10$ , $n=3$ and $d=2$ then , for example , $$ p ( [ 1,0,0,1,0,0,1,0,0,0 ] ) = \frac{1}{z}e^{-\frac{\epsilon_1 + \epsilon_4 + \epsilon_7}{kt}} , $$ but $$ p ( [ 1,0,1,0,0,0,1,0,0,0 ] ) = 0 $$ because it is not allowed by the constraint . to calculate the normalising factor ( or " partition function" ) $z$ , you have to sum over all allowed configurations of the system . it is not immediately obvious ( to me ) how to do that analytically in this case , but you are the mathematician so i am sure you can find an elegant way . incidentally , you should be able to see that if there are no interactions between the $m$ positions then this reduces to the formula you originally quoted .
feeling silly now . just equating the component of velocities along the wall : $$1/2 sin\theta=cos\theta$$ we get $$\tan\theta =2$$ so , $$e=1/4$$
i see four reasons to make this assumption : the first one is simply that we do not know really how to deal with the non linear problem in general cases and therefore we linearize but i agree that it is not a good justification although this is most of the time the hidden reason why people use it the poisson-boltzmann equation is mostly used in aqueaous solutions and therefore you have a factor 80 ( owing to the dielectric constant of bulk water ) that appears in all your potentials . . . this more or less ensures that a monovalent ion does not generate a so high potential energy with other monovalent ions . if you have free ions in solution , it means that the thermal energy was enough to begin with to unbind them from the groups there were bound to it can be rigorously shown that at large distances from an ion at any valency , the generated potential decays exponentially with the distance ( or rather like a yukawa potential ) because of the screening owing to the mobile charges in solution ( this result holds in the non linear poisson-boltzmann case but also beyond the poisson-boltzmann theory ) . incindentally , a yukawa potential is what you can expect from the field generated by an isolated ion in solution within the linearized poisson-boltzmann theory . also , if you are a bit familiar with liquid theory , a particular closure of the orstein-zernike equation leads to an infinite summation over tree-like diagrams that leads to a total correlation function between two point ions in solutions that decays as a yukawa as well which tells you more or less that although you linearize , you still encapsulate important many body effects because of the last point , there is a huge litterature on charge renormalization of charged particles in solution as a function of their valency , size and salt concentration . in the context of the poisson-boltzmann theory , charge renormalization refers to a mapping from the non linear theory to the linearized one via the use of effective charge paramaters
the question op is proposing is linked to the question of the mass formulas . here , what really matters is if the mass of the u quark is indeed very near zero and if one has some compelling theoretical reason to believe this . the strong cp problem could not be of much help here as pointed out in the dine 's review . the reason is quite simple : if one should have a $\theta$ term into qcd lagrangian , the neutron would have a measurable electric dipole . from experiments we know that is not the case and a lower bound is fixed . but the electric dipole of the neutron does not depend only from the mass of the quark u and so , having $m_u\approx 0$ is a sufficient condition but not necessarily the right one . from a theoretical stand point , from qcd sum rules a lower bound for the masses of u and d quarks can be estimated . the main reference is s . narison , qcd as a theory of hadrons ( cambridge university press , 2007 ) . i report here the estimation given in this book for the sake of completeness ( chapter 53 in the book ) : $$ ( m_u+m_d ) ( 2\ gev ) &gt ; 7\ mev . $$ this grants a small but yet finite mass and whatever mass formula should satisfy this bound . of course , this is consistent with $m_u\approx 0$ . but a more recent review ( see here ) gives $m_u\approx 3\ mev$ that is not so small but it is on the strong interaction scale . smallness of $m_u$ and $m_d$ masses makes chiral symmetry a very good yet approximate symmetry .
note that you begin with a superpotential $w$ which gives you a generalization of the creation/anihilation operator as $a^{+} = -\frac{d}{dx} + w ( x ) $ and $a^{-}= +\frac{d}{dx} + w ( x ) $ . you will get 2 hamiltonians $h_- = a^+ a^-$ and $h_+ = a^- a^+$ . so you obtain 2 different potentials $v_{+}$ and $v_{-}$ for superpartners $v_{-} ( x ) = w ( x ) ^2 - \frac{dw}{dx}$ and $v_{+} ( x ) = w ( x ) ^2 + \frac{dw}{dx}$ . then , suppose that $\psi^-$ is a solution of $a^-\psi^- ( x ) = 0$ with $\psi^-$ normalizable , you will find that $h_- \psi_- = a^+a^-\psi^- = 0$ , so $\psi^-$ is an eigenstate of $h_- $ , with eigenvalue 0 . you have $\psi^- ( x ) \sim e^{-\int^x_{x_0} w ( x ) dx}$ ( from the definition of $a^-$ ) with the same reasoning , you will find that a $\psi^+$ solution of $a^+\psi^+ ( x ) = 0$ will give you $\psi^+ ( x ) \sim e^{+\int^x_{x_0} w ( x ) dx}$ which would be an eigenstate of $h_+ $ , with eigenvalue 0 . but you have a problem , you can see that you cannot have , at the same time $\psi^+$ and $\psi^-$ normalizable , because $\psi^+ ( x ) \sim \frac{1}{\large \psi^- ( x ) }$ . so , one of the functions $\psi^+$ or $\psi^-$ has to vanish . so , you have , at most , one ground state with zero energy . [ edit ] the practical link with supersymmetry is as follows . we define a 2 -dimensional space , with $\psi = ( \psi_- , \psi_+ ) $ . one of the $\psi$ states is a bosonic state , the other is a fermionic state . we define the supersymmetric generators : $q^- =\left [ \begin{array}{cccc} 0 and 0 \\ a^- and 0 \end{array} \right ] $ $q^+ =\left [ \begin{array}{cccc} 0 and a^+ \\ 0 and 0 \end{array} \right ] $ the hamiltonian is $h = q^-q^+ + q^+q^-$ $h =\left [ \begin{array}{cccc} a^+a^- and 0 \\ 0 and a^-a^+ \end{array} \right ] = \left [ \begin{array}{cccc} h_- and 0 \\ 0 and h_+ \end{array} \right ] $ it can be easily seen that $ ( q^- ) ^2= ( q^+ ) ^2 = 0$ and $ [ h , q^- ] = [ h , q^+ ] = 0$ unbroken supersymmetry corresponds to a ground state $|0&gt ; $ such as $&lt ; 0|h|0&gt ; = 0$ , and this imply $q^+|0&gt ; = q^-|0&gt ; = 0$ in this case , one of the functions $\psi^+$ or $\psi^-$ is normalizable , and the other vanishes . for instance , suppose $\psi^-$ is normalizable , then it means that $a^- \psi^- =0$ the case of ( spontaneously ) broken supersymmetry , is when $&lt ; 0|h|0&gt ; \neq 0$ , which implies $q^+|0&gt ; \neq 0$ or $q^-|0&gt ; \neq 0$ . in this case , neither of the ground states $\psi^+$ nor $\psi^-$ are normalizable .
before going into the details , let me tell you that this type of actions describe the deepest connection between geometry and physics and generalizations of these types of theories are still under active research even today . the lagrangian describes $n=1$ supersymmetric quantum mechanics on a riemannian manifold the bosonic part of this lagrangian is the kinetic term of a particle moving on a riemannian manifold $\mathcal{m}$ having a metric $g$ . as very well known the trajectories of the particles are the geodesics of the manifold . the functions $\phi^i$ are just the coordinates on the manifold . the fermionic parts of the lagrangian make the lagrangian invariant under the ( n=1 supersymmetry ) transformations : $$\delta \phi^i =\epsilon \bar{\psi}^i$$ $$\delta \psi^i = -i \gamma^0 \dot{\phi}^i \epsilon - \gamma^l_{jk} \bar{\epsilon} \psi^j \psi^k$$ please see the following article by luis alvarez-gaume ' , the supersymmetry operator can be written in terms of the canonical momenta : $$ \pi_i = g_{ij} \phi^j$$ as : $$q= i \pi_i \bar{\psi}^i - \gamma^0 \gamma_{ijk} \bar{\psi}^i \psi^j \psi^k $$ it can be easily checked that this operator generates the correct supersymmetry transformation given the canonical poisson brackets : $$\{\phi^i , \pi_j\} = \delta^i_j$$ $$\{\psi^i , \psi^*_j\} = g_{ij} ( \phi ) $$ the inclusion of the fermionic coordinates in the lagrangian gives spin to the particle moving on the riemannian manifold . this fact was discovered by berezin and marinov in 1975 , please see their original article ( they consider the case of flat space time ) . most importantly , when the theory is quantized , then if we add to the canonical quantization rules $$\pi_i \rightarrow i \frac{\partial}{\partial \phi^i}$$ canonical quantization rules for the fermionic coordinates $$\psi^i \rightarrow \gamma^i$$ i.e. , quantize the grassmann algebra to dirac matrices or clifford algebra ( please do not get confused with the gamma matrices in the classical action which must be treated as numerical coefficients ) , then the supersymmetry operator becomes the dirac operator ( in curved space ) . this is the reason why this action describes a spinning particle . also , the square of the supersymmetry operator is the dirac hamiltonian : $$ qq^{\dagger}+q^{\dagger}q = h = \pi_i \dot{\phi}^i + i g_{ij} \bar{\psi}^i \dot{\psi}^j - l$$ the four fermion term expresses the fact that in curved space , the dirac hamiltonian differs from the scalar hamiltonian . in differential geometry , this dirac hamiltonian is the laplacian on forms . one of the important applications of these types of actions is that they are used to provide quantum mechanical proofs of the various index theorems . now , it is not difficult to think the following generalizations . if supersymmetric quantum mechanics in $0+1$ dimensions describes a spinning particle , then a supersymmetric sigma model in $1+1$ dimension will describe a spinning string . in fact , witten used this observation to compute the index of the dirac operator on a loop space . one can think of the particles as probes for studying the geometry and topology of the spaces they are confined to move on . a classical particle can be used to study the geodesics . upon quantization , more information can be obtained for example the energies which constitute the spectrum of the laplacian can give topological and geometrical information . when the particle is given a spin , then even more topological information can be deduced due to these index theorems , for example spinning particles can see holes and handles in the manifold . specifically , the model under consideration can be used to prove the atiyah-singer theorem for the dirac operator index ( the difference between the number of zero modes of $q$ and $q^\dagger$ ) on a riemannian manifold and evaluate the result by means of the partition function path integral . please see the following article by friedan and windey $$ ind ( q ) = \int \mathcal{d} \phi \mathcal{d} \psi e^{i \int_{pbc} l dt}$$ ( pbc denotes periodic boundary conditions ) the zero modes of the dirac hamiltonian are just the harmonic forms which generate the de-rham complex of the manifold . finally , if we replace the particle by a string still we can probe much more topological and geometric information about the manifold .
this is not a bad question . i would appreciate it if you expanded some to give context and explain what you are thinking about . short answer : no , there is not . it is not clear that m-theory has a description in the lagrangian framework . longer answer : the nature of spacetime in m-theory is radically different from the nature of spacetime in classical and quantum field theory . exactly how different is still under investigation . but it is not clear that spacetime is infinitely divisible in this theory . if short distances do not exist , then it is not clear we should be using the lagrangian framework to describe fundamentla physics , since these implicitly associate degrees of freedom to all distance scales . that said : lagrangians do play a role in the physics of m-theory . they are used to describe worldvolume qfts , which are effective qfts which describe how strings and branes see the classical spacetime around them : the matrix model of d0-branes . the d3-branes of ads/cft are described by n=4 gauge theories . the basis of perturbative string theory is a nonlinear sigma model . a stack of 5-branes in m-theory is described by a ( 2,0 ) theory .
there is no " vectorlike " gauge theory in the standard model , and this is a consequence of naturalness . this means that all particles in the standard model are naturally massless , and the mass only comes from higgs mechanism . this is one of the great features of the standard model that is easy to break in any modification or extension . the teminology " vectorlike " comes from the 1950s , when people did not like 2-component spinors and thought that the world is fundamentally parity invariant . a " vectorlike " gauge field couples to a 4-spinor according to $\gamma^\mu a_\mu$ , while a " pseudovectorlike gauge field " couples to a 4-spinor according to $\gamma^5\gamma^\mu a_\mu$ . both are parity invariant , but in the first case , a is a vector ( meaning it changes sign under reflection ) , and in the second case it is a pseudovector . but the gauge fields in nature are neither vectors or pseudovectors , they are parity-violating . they couple as " v-a " meaning $ ( 1-\gamma_5 ) \gamma^\mu a_\mu$ , which is a projection operator to one two component part of the 4 component dirac spinor . this means that 4-component language is a little obfuscatory for this ( although 4 component spinor notation is still useful , becuase feynman trace identities are easier than fierz identities , and the 4-component notation most easily generalizes to higher dimensions ) . the point is that there is no parity , and the gauge fields are neither " vectors " or " pseudovectors " , they are parity violating vector fields which do not have a definite transformation under parity , because nature is chiral . so i would drop the " vectorlike " terminology , and use the term " naturally mass allowing " . a vectorlike gauge theory is " naturally mass allowing " because you can make the fermion massive . this means that the left and right partners have the same charges , and this can be considered an accident . the correct question is " why are all gauge theories in nature mass forbidding ? " this is true of all the fields on the standard model--- none of the right handed and left handed fields in the standard model can pair up to form a mass , because they are different su ( 2 ) multiplets and have different u ( 1 ) charge . why are they all unpartnered and charged ? there is a simple reason for this--- any field which can get partnered will have an arbitrary mass term in the lagrangian , and this term , without fine tuning , will end up generically being of order the planck mass . so the only fermions we see at low energies are those which are forbidden to have a mass , and therefore are chiral fermions without a partner to make a mass term with . further , all the fermions we see at low energies need to have a gauge charge , because without a charge of some sort , the fermion can get a majorana mass even without a partner , just by mixing with it is antiparticle . this is only forbidden if the particle is gauge charged in some way , so that the antiparticle has the opposite charge and the majorana mixing is forbidden . so all the fermions are chiral fermions with no partner to make a mass , so none of the low energy theories are vector-like . the simplest right way to formulate gauge theories in a parity violating universe is in terms of 2-component spinors , each with an independent coupling to a collection of gauge fields . this procedure can lead to an inconsistency , if there is an anomaly in one of the gauged symmetries , so there are global constraints on the type of chiral fermions and the representations they can be in . if none of the fermions have a partner , then the theory is natural , meaning " naturally massless " and the fermions can only get a mass from a higgs mechanism . the naturalness arguments say that the higgs mechnism must be the source of mass of all fermions in nature . but if the higgs is a fundamental scalar , the higgs itself can have a mass , and the naturalness argument fails for the higgs itself . so there is the question of why the higgs has an unnaturally light mass . this is the hierarchy problem .
john , that is my blog and antenna design . fractal designs are used to fit a larger antenna in a smaller space . the self similar pattern has been found to reduce loss of gain when needing to compress the size of an antenna . the wikipedia page on fractal antennas might be a useful read for you . http://en.wikipedia.org/wiki/fractal_antenna
the planckian distribution is a thermal distribution . if you change the energy distribution of the photons , then they are not at equilibrium with the environment . so that will not work . you have to change the temperature of the system , and that will change the mean number of photons .
your question " is the predictability of the future to whatever extent is possible ( based on the present and the past ) equivalent to the principle of causality ? " has the trivial answer ''no'' as the qualification ''to whatever extent is possible'' turns your assumption into a tautology . the tautology makes your statement false , as your question asks whether the universally true statement is equivalent to causality . an answer " true " would make any theory causal , thus making the concept meaningless . why is your assumption a tautology ? no matter which theory one considers , the future is always predictable to precisely the extent this is possible ( based on whatever knowledge one has ) . in particular , this is the case even in a classical relativistic theory with tachyons or in theories where antimatter moves from the future to the past . however , in orthodox quantum mechanics and quantum field theory , causality is related to prepareability , not to predictability . on the quantum field theory level ( from which all higher levels derive ) , causality means that arbitrary observable operators $a$ and $b$ constructed from the fields of the qft at points in supports $x_a$ and $x_b$ in space-time commute whenever $x_a$ and $x_b$ are causally independent , i.e. , if ( x_a-x_b is spacelike for arbitrary $x_a\in x_a$ and . $x_b\in x_b$ . loosely speaking , this is equivalent to the requirement that that , at least in principle , arbitrary observables can be independently prepared in causally independent regions . arguments from representation theory ( almost completely presented in volume 1 of the qft books by weinberg ) then imply that all observable fields must realize causal unitary representations of the poincare group , i.e. , representations in which the spectrum of the momentum 4-vector is timelike or lightlike . this excludes tachyon states . while the latter may occur as unobservable unrenormalized fields in qfts with broken symmetry , the observable fields are causal even in this case .
there is an expression for the lorentz force on a charge in a magnetic field . this expression is based on experimental facts , and the order in the vector product of the charge velocity and the magnetic field is fixed .
most higher derivative theories —and in particular lee-wick 's model— do not have ghost excitations but they are unstable ( hamiltonian unbounded from bellow ) . yes , almost everyone says the opposite but all them are unfortunately wrong . they do not quantize the theory properly . whenever a degree of freedom has negative energy at the classical level , it must have negative energy at the quantum level as well . people try to fix the problem of negative energy exchanging the frequencies between creation and annihilation operators at the price of introducing ghosts . but this is not the right way of quantizing the theory because the classical limit is totally wrong . one may canonically quantize the harmonic oscillator with an additional higher derivative term to convince himself of what i am emphatically claiming . anyway , either one quantizes the theory correctly getting an unstable theory or one quantizes the theory incorrectly getting a theory with negative norm states , the quantum theory does not make any sense . it is not a quantum theory . is there any general mathematical theorem by which we can show that a nth order derivative theory can be quantized into n different kind of particles ? the classical and the quantum theory must have the same number of degrees of freedom ( dof ) . the classical theory has half dof of the number of initial conditions must be given to determine a solution . in a normal field theory —let 's say klein-gordon— one must specify initial value of the field and momentum ( or velocity ) for every field . thus one has one dof for a real field , two for a complex field , four for two complex fields . when one adds higher time derivatives , one requires more initial conditions to know a solution ( initial accelerations , initial fourth derivatives , etc ) . when there are constrains the counting of degrees of freedoms is a little bit more subtle . for instance , electrodynamics is a second-order theory and the electromagnetic four-potential $a_{\mu}$ has four components so one could naively think that the theory has four degrees of freedom , but this is not true because there are gauge redundancies ( there are first class constraints ) that make different ( gauge related ) $a_{\mu}$ correspond to the physical situation . so that the theory has only two dof corresponding to the two polarizations of electromagnetic waves . regarding the quantum theory , the number of degrees of freedom corresponds to the number of particles ( each physical polarization counts as a particle ) .
in short , i think the answers are : 1 ) yes , the approximation $ \langle s_i s_j \rangle \approx \langle s_i \rangle \langle s_j \rangle $ gives you the correct behavior for a spin system with homogenous spin values , but 2 ) there is more to mean field theory than this level of calculation the approximation $$ \langle s_i s_j \rangle \approx \langle s_i \rangle \langle s_j \rangle $$ provides an approximation for determining what the mean spin value is within the ising model , but is insufficient to actually calculate $ \langle s_i s_j \rangle$ , as you have noted . the result of this approximation is a free energy in terms of the mean field $\langle s \rangle = m$ . to get the two-point correlation function , we have to determine the energetic cost for having non-uniform $m ( {\bf r} ) $ . one natural way of doing this is to use the landau expansion , i.e. we write ( see , e.g. chaikin and lubensky chapter 4 ) $$ f = \int d^d x f + \int d^d x \frac{c}{2} |\nabla m |^2 $$ where $f ( x ) = \frac{1}{2} r m^2 + u m^4 + \cdots $ . the free energy from the first term is something that you can get from making the approximation $\langle s_i s_j \rangle \approx \langle s_i \rangle \langle s_j \rangle$ . however , this does not get you the value $c$ , which is essentially a phenomenological term ( you can relate it to the effective line tension between domains in the ising model ) . whenever you see a description of the correlation function in mft , some term like this has been included . there is also an equivalent mft scheme in field theory where the mft can be derived by a saddle-point approximation ( see kardar 's statistical physics of fields , for instance ) . however , i do not remember offhand how to get from an ising model to the appropriate field theory . . . i think this is done with a hubbard-stratonovich transformation , but i do not remember the details .
actually , in the first situation ( elastic impact ) , the block a will stand still after the impact and the block b will continue at 5m/s . you have to make the summary of momentum ( quantité de mouvement ) and kinetic energy to solve those problems . for the spring deflection , the maximum deflection will occur at speed 0 , 0 kinetic energy , as all the energy will be store in the spring . you need to calculate the kinetic energy of the system then calculate how much deflection you need to achieve that amount of energy . the energy stored in a spring is define e= . 5*k*x hope it helps .
why do you assume that in the case of torques , the torques must cancel out , in my opinion the best way to deal with this problem is your method 1 , but you can solve by taking torques as follows : when you are balancing along y-axis ( calculating x ) $6a × g × 4 + 29a × g × x = 35a × g × 2.5$ here a is mass per unit area , 4 is the x coordinate of centre of mass of small cut off block , x is the coordinate of centre of mass of left over block and 2.5 is the coordinate of original block we have equate the combined torque of cut off and left off pieces with that of original piece , you can do similar operation for y coordinate $6a × g × 5.5 + 29a × g × y = 35a × g × 3.5$ these operations give the same results as those obtained by method 1 , so there is no error .
for resistors $r_1 , r_2 , \dots , r_n$ in parallel , the equivalent resistance $r_e$ is given by $$ \frac{1}{r_e} = \frac{1}{r_1} + \frac{1}{r_2} + \cdots + \frac{1}{r_n} $$ if two resistors with equal resistance $r_1 = r_2 = r$ are in parallel , then this gives $$ r_e^{ ( 2 ) } = \frac{r_1r_2}{r_1+r_2} = \frac{r^2}{2r} = \frac{r}{2} $$ if three identical resistors $r_1 = r_2 = r_3 = r$ are in parallel , then the equivalent resistance is $$ r_e^{ ( 3 ) } = \frac{r_1r_2r_3}{r_1r_2 + r_2r_3 + r_3r_1} = \frac{r^3}{3r^2} = \frac{r}{3} $$ in fact , for $n$ identical resistors one has $$ \frac{1}{r_3^{ ( n ) }} = \frac{n}{r} $$ so that $$ r_e^{ ( n ) } = \frac{r}{n} $$ and therefore the resistance decreases with the addition of each successive resistor in parallel .
the x-ray diffraction pattern is the fourier transform of whatever is doing the diffracting . if you had an infinite plane of atoms then the spots ( rings in a powder pattern ) would be infinitely sharp because the fourier transform of an infinite wave is a delta function . however a real crystal is the product of an infinite plane with an envelope function , where the envelope is the size of the crystal , so the spot is the convolution of a delta function with the fourier transform of the envelope function . in a powder pattern we have many crystals of differing sizes , so the average fourier transform of the crystal size ends up looking something like a gaussian and the spots have a roughly gaussian profile . re your formula , suppose the average crystal ends up looking like a sphere , i.e. a disk in profile , then it is fourier transform is going to be an airy disk ( but blurred out by the variation in particle size ) . the half angle subtended by the airy disk , $\beta$ , is ( for small angles ) : $$ \beta \propto \frac{\lambda}{d} $$ which is the scherrer equation for small $\theta$ ( i would have to go away and look at the derivation of the scherrer equation to remember why there is a factor of $\cos\theta$ , but for small $\theta$ this factor is approximately $1$ anyway ) . response to comment : the incoming x-rays are scattered by the atoms in the crystal , so each atom acts as an x-ray source . we get the diffraction pattern by summing up the x-rays emitted ( i.e. . scattered ) by all the atoms in the crystal . if you start with one atom then the scattered x-rays will be just be a spherical wave . add a second atom and now the pattern will be the same as the young 's slits experiment . as you add more and more atoms the pattern will tend towards the pattern we expect from a large crystal , however to get an infinitely sharp spot would require an infinite number of atoms . when we have a finite number of atoms the spot will have a finite width . it is a bit like a fourier synthesis ( which is where we came in ) . each atom adds a term to the fourier sum , but to get a perfect transform of the lattice requires contributions from an infinite number of atoms . with a finite number of scatterers the diffraction pattern will only be an approximation to the ft of the lattice .
a dispersion relation tells you the conditions under which a certain solution holds , which means that in order to get a dispersion relation you need to assume a solution of some general form . this is a system of harmonic oscillators coupled over a long range , so it is natural to assume a plane wave solution . using your notation , try $x_n= e^{i ( kna - \omega t ) }$ where $\omega$ is the frequency of phonon oscillation . you want to specify which $\omega$ satisfy the equations of motion . it should only take a couple of steps to arrive at the dispersion relation from this . i hope this helps .
in a word , no . the drift velocity is always very small in common circuits ( ~$10^{-4}cm/s$ ) . in this scenario , when the plates are connected , the electrons still travel slow but all of the electrons along the wire start moving almost at the same time , so even though they move slowly , the electric charge on the plates will vanish quickly . the electrons on the negative plate move into the wire and the positive plate is filled with electrons that were previously in the wire right next to it . this is the difference between the speed of current and the speed of the electrons . current travels very fast ( it is like the speed of sound in the free electron sea ) . electrons drift very slow .
i do not know of any research to find out if skin sunburns faster when wet , though someone did a comparable experiment to find out if plants can be burnt by sunlight focussed through drops of water after the plants have been watered . you need to be clear what is being measured here . the total amount of sunlight hitting you , and a plant , is unaffected by whether you are wet or not . the question is whether water droplets can focus the sunlight onto intense patches causing small local burns . the answer is that under most circumstances water droplets do not cause burning because unless the contact angle is very high they do not focus the sunlight onto the skin . burning ( of the plants ) could happen if the droplets were held above the leaf surface by hairs , or when the water droplets were replaced by glass spheres ( with an effective contact angle of 180º . my observation of water droplets on my own skin is that the contact angles are less than 90º , so from the plant experiments these droplets would not cause local burning . the answer to your question is ( probably ) that wet skin does not burn faster . i would agree with will that the cooling effect of water on the skin may make you unaware that you are being burnt , and this may lead to the common belief that wet skin accelerates burning .
yes , the resistance to the magnetic field is called reluctance . as the magnet moves through a copper coil ( consider circular ) , the change in magnetic field induces current in the coil . due to the current in the coil , another magnetic field is produced in the opposite direction to the magnet moving through the copper coil . also , the magnetic strength of both the field are same . but , the directions are exact opposite .
matt strassler goes into detail with lhc data here : http://profmattstrassler.com/articles-and-posts/largehadroncolliderfaq/whats-a-proton-anyway/checking-whats-inside-a-proton/
here is an explanation by bo danforth : shown in the picture above is a segment of a wurlitzer jukebox bubble tube . in the tube , the bubbles rise as expected , but as they approach the top , odd things begin to occur . instead of remaining the same size as one might think , or increasing slightly from a minute reduction in pressure , they instead decrease in size , in some cases disappearing entirely . the reason for this bewildering sight is that the bubbles do not actually contain air , but are pockets of heated vapor . the bubble tube is a sealed system where the air has been completely removed by a pump , creating a partial vacuum , which causes the liquid to fill the remaining space with vapor . the system is at a pressure where it will change states near room temperature . when this sealed tube is placed next to a heat source at the bottom ( the light bulb ) the liquid near it reaches a boiling point , according to the equation of pv=nrt . as the bubbles of vapor float upwards along the exposed glass and away from the heat source , they slowly shrink as the gas cools down , condensing back into a liquid state .
consider a solid rod made of a glassy substance , and model it as a set of atoms in random locations , held together by randomly oriented harmonic springs ( enough so that the graph is rigid ) . the hamiltonian of this system is in principle diagonalizable , and since all the springs are harmonic , the potential energy is quadratic in all the atomic coordinates , so the whole system is equivalent to a set of uncoupled harmonic oscillators , which are of course the normal modes . the lowest frequency mode will be rod flexing back and forth with two nodes , the second will be the second overtone with three nodes , and so on , but importantly this set of normal modes goes all the way up to modes with a number of nodes on the order of the number of atoms in the rod . none of these normal modes has a precisely defined wavevector , because of the lack of periodicity in the glass , but they do have approximate wavevectors , and of course they have precise frequencies corresponding to the eigenvalues of the hamiltonian . one weird thing about this that seems completely different from the usual treatment of phonons in crystals is that all these modes are standing waves - they do not propagate in a particular direction , and their approximate wavevectors are only defined up to sign . this is actually the case for a finite-size crystal as well - no propagating waves can actually be eigenstates , and instead the boundary conditions at the ends of the crystal cause them to mix into standing waves that are the exact physical eigenstates . the only reason we introduce periodic boundary conditions and talk about the propagating waves in crystals is that it is so much more convenient . of course , if you create a wave packet at one end of the material , either crystal or glass , you can always express that in whatever basis of eigenstates you want , and as they evolve the packet will end up moving through the crystal and spreading out according to some dispersion relation . i do not know how you would actually calculate that dispersion relation for a glass ( other than brute-force computation ) , but it is possible in principle . the same considerations also apply to quasicrystals , but with the interesting addition that there are now diffusively propagating modes called phasons with long relaxation times . 2 phonons in a gas is a really weird thing to think about because in an ideal gas , the particles are assumed to be non-interacting , and they have to be in a thermal distribution of single-particle quantum states for it to be a gas ( rather than a bose-einstein condensate or something ) . if the gas particles are delocalized , and do not interact with each other , then what the heck is a phonon ? yet sound waves in a gas obviously exist , so the question remains whether they are quantized or not . i can not answer this part of the question .
start with $$ m{dv\over dt}=b_1 ( v_1-v ) -b_2v . $$ move everything involving $v$ to one side of the equation , and everything involving $t$ ( in this case , just $dt$ ) to the other side . integrate both sides . one side will be just $\int dt$ , or $t+c$ . the other side will be some function of $v$ . algebraically solve the result to get $v$ in terms of $t$ .
the schwarzschild solution for a neutral black hole is $$ c^2 {d \tau}^{2} = \left ( 1 - \frac{r_s}{r} \right ) c^2 dt^2 - \left ( 1-\frac{r_s}{r}\right ) ^{-1} dr^2 - r^2 \left ( d\theta^2 + \sin^2\theta \ , d\varphi^2\right ) $$ you see that the terms $dt^2$ and $dr^2$ are multiplied by $ ( 1-r_s/r ) $ or its inverse where $r$ is the radial coordinate and $r_s$ is a constant , the schwarzschild radius . an important subtlety of this $ ( 1-r_s/r ) $ is that it becomes negative for $r_s\lt r$ ; this is true for any black hole beneath its event horizon . that is why in the black hole interior , the changes of the coordinate $r$ are actually timelike and the changes of the coordinate $t$ are spacelike ; the role of the space and time are interchanged in the interior relatively to the exterior ! when we say that the outgoing/infalling particles from the pair have positive/negative energy , we are talking about the energy that generates translations of the $t$ coordinate . however , the $t$ coordinate is really spacelike in the black hole interior so the $t$-component of the energy-momentum vector is interpreted as a spatial component of the energy-momentum vector by observers inside . it means that from the internal observers ' viewpoint who are capable of observing the infalling particle ( and not all of them can ! ) , it is just an ordinary particle with some value of the momentum $p_t$ , which is a spatial component and may unsurprisingly be both positive and negative . there is certainly no " new kind of matter " occuring in general relativity right beneath the horizon . all the local physics obeys the same laws as it does outside the black hole . this general assertion is a special example of the fact that the event horizon is a coordinate singularity , not an actual singularity . when one choose more appropriate , minkowski-like coordinates for the region of the spacetime near the horizon , it looks almost flat – as seen from the fact that the riemann curvature tensor has very small values , at least if the black hole is large enough . so an observer crossing the event horizon of a large enough black hole does not feel anything special at all . his life continues for some time before he approaches the singularity and this is where the curvature becomes intense and where he is inevitably killed by extreme phenomena . but life may continue fine near the horizon and even beneath it . note that the energy is the only conserved component of the energy-momentum vector on the schwarzschild background – because this solution is time-translationally invariant but surely not space-translationally invariant . due to the intense curvature caused by black hole , one must be careful and not interpret individual components of vectors " directly physically " . we saw an example that what looks like a temporal component of a vector , namely energy , from the viewpoint of the observer at infinity , can really be a competely different , spatial , component from the viewpoint of coordinates appropriate for a different observer , one who is inside .
i think a good , and classic , reference for your case is the following , introductory functional analysis with applications the very last chapter of kreyszig deals with quantum mechanics . and , once you have learned how to " translate " the language of functional analysis into that of quantum mechanics , you can go to more advanced texts in specific topics .
yes , $ ( h^\dagger h ) ^2$ is invariant under $su ( 2 ) \times u ( 1 ) $ because even without the second power , $h^\dagger h$ is invariant under it . by that , we mean $$\sum_{i=1}^2 h_i^* h_i$$ of course that it is invariant under $u ( 1 ) $ because the $u ( 1 ) $ charges of $h^\dagger$ and $h$ are opposite in sign and add up to zero . note that the transformation of a charge-$q$ field is $f\to f\exp ( iq\lambda ) $ . the invariance under $su ( 2 ) $ is also self-evident because $\sum_i z^*_i z_i$ is exactly the bilinear ( with one asterisk ) invariant that defines the unitary groups . no , the invariance of $h^\dagger h$ or its square does not mean that the hypercharge of $h$ itself is zero or isospin is zero . moreover , the op seems to confuse the spin and the isospin .
i am guessing this is related to the archaic paris inch , which is $27.069$mm , i.e. $10^8 \times$ the conversion factor . reference : scientific papers vol 2 1881-1887 , john william strutt .
this should be a comment since i am not a string theorist but its too big . when luboš ( luboš correct me if i am wrong ) speaks of the " shape " in his comment : they are spatial dimensions - new temporal dimensions always lead to at least some problems if not inconsistencies - but otherwise they are the same kind of dimensions as the known ones , just with a different shape . to a creature much smaller than their shapes ' size , they are exactly the same as the dimensions we know . the theory implies that the total number of spacetime dimensions is 10 or 11 . we do not have " intuition " for higher-dimensional shapes because the extra dimensions are much smaller than the known ones but otherwise they'er intuitively exactly the same as the known spatial dimensions he means that the higher dimensions are " compactified " . a simple example of a compact space is a circle , or a cartesian product of circles ( a torus ) or a high dimensional sphere . the crucial idea is that they are topologically compact , meaning roughly that they are finite and closed *i . e . * they have no boundary just like the torus or sphere have no boundary . so luboš 's little creatures would return to their beginning point if they walked far enough in the same direction . as i understand it , one of the proposed " shapes " for the compactified dimensions is the calabi-yau manifold . wholly for gazing on beauty 's sake , its worth also looking these here at the wolfram demonstrations site . be aware that you are looking at a projection , hence the seeming " edges " are not the manifold 's boundary . like the torus and the sphere , these manifolds would let a little creature return to their beginning point eventually by travelling in a constant direction and nowhere would they come across a barrier or boundary . actually , it is not out of the question that the three spatial dimensions of our wonted experience are like this too , just that we are talking awfully big distances ( 10s to 100s of billions of light years ) for us to come back to our beginning points if we blasted off into space and kept going in the same direction . as i understand this , this idea is seeming less and less likely since our universe globally is observed to be very flat indeed . see an interesting discussion at mathoverflow on what the fundamental group of the universe might be like update : see also this answer clarifying some of my description of compactified dimensions . if they are big enough ( as for our everyday three spatial dimensions , if they are compactified too ) even though a constant direction vector can be integrated to a closed loop through space , the fact that the universe is expanding means that one cannot traverse this loop in a finite time .
zeta-function regularization can be thought of as analytic regularization with a special choice of the subtraction scheme . like any other regularization , there are going to be possible ambiguities that unless treated consistently across a calculation will make the results of a naive/minimal subtraction result incorrect . however , these ambiguities should always be able to be accounted for by a finite counter-terms . 1 so the art of regularization is in setting up a consistent subtraction scheme -- either by using a consistent minimal subtraction , using renormalization conditions , an r-operation , or by consistently using some implicit subtraction like zeta-function regularization . however , as you noted in your question , the latter option is not always so easy . the zeta-function regularization approach to one-loop qft calculations comes from the observation that $\partial_t h^{-t}|_{t\to0} = -\log ( h ) $ . then 2 $$\begin{align} \log\det ( h ) and = \mathrm{tr}\log h := -\zeta&#39 ; _h ( 0 ) \ , , \\ \zeta_h ( s ) and = \mathrm{tr} h^{-s} = \frac1{\gamma ( s ) }\int_0^\infty\mathrm{d}t\ , t^{s-1}\mathrm{tr} ( \exp ( -h t ) ) \end{align}$$ the heat kernel is $k ( x , x&#39 ; |t ) = \exp ( -h t ) \delta ( x , x&#39 ; ) $ and taking its trace involves setting $x&#39 ; \to x$ and integrating over all spacetime $x$ ( and taking the trace over any group or flavour indices ) . often the spacetime integral is not performed as you want the answer as an effective action . $\zeta_h$ is called the zeta function of $h$ since $$ \zeta_h ( s ) = \mathrm{tr} h^{-s} = \sum_{n} \lambda_n^{-s} \ , , $$ where the $\lambda_n$ are the eigenvalues of $h$ . $\zeta_h ( s ) $ is basically just the analytically regularized one-loop integral and you could just as easily expand it in powers of $s$ to extract the divergent part ( $s^{-1}$ ) , the finite part ( $s^0$ ) and the terms that vanish as $s\to0$ . zeta-function regularization just returns the $s^0$ part and throws away the rest - as you noted in your question , this is not always guaranteed to work in all cases and all order of operations . there is no reason to think that the implicit subtraction scheme of zeta-function regularization is better than any other subtraction scheme for analytic regularization . normally the coincidence limit $x&#39 ; \to x$ is done before the propertime integral , since it makes things simpler . also , the heat kernel is often calculated via momentum space and then it is possible to leave the momentum integral until after the propertime integral - this means you never have a position space expression for the heat kernel , but it can also make calculations simpler . if different results are obtained by different operation orders , then there is some sort of conditional convergence that is not fixed by your regularization scheme . the result of a zeta-function regularized calculation or any other renormalized qft calculation should not be taken as correct unless it satisfies a sensible choice of physically motivated renormalization conditions ( and ward identities etc . . . ) . any other subtraction scheme is merely a convenient inbetween result . finally , as noted in stackexchange-url the trick of writing $\log h$ as the derivative of some $h^{-n}$ is not unique . this non-uniqueness can be used to parameterize the ambiguity of zeta-regularization so that different methods can be compared and renormalization conditions more easily enforced . the inadequateness of the naive " zeta-function " regularization of heat kernels becomes clear in higher-loop calculations . 1 . that said , i have done calculations where the ambiguity arises in a finite ( higher-mass dimension ) term that is not present in the classical action nor amenable to correction by any renormalizable counter-term . this ambiguity , being in a finite term , comes from conditional convergence in the one-loop integrals in either their unregularized or regularized forms . in such cases , i am not sure how to deal with the ambiguities . . . 2 . all equality signs are to be taken with a pinch of salt . they depend on the regularization and renormalization scheme etc . . .
as anna pointed out in her comment , it really depends on the system . in general , there is not much you can say except that the minimum of kinetic energy corresponds to the minimum speed and the maximum of kinetic energy corresponds to the maximum speed . there are many systems in which the minimum speed ( and thus the minimum kinetic energy ) is zero , and in those systems , finding the times at which the kinetic energy is a minimum tells you when the object stops . mathematically , if you actually take the derivative of kinetic energy with respect to time , you get $$\frac{\mathrm{d}k}{\mathrm{d}t} = \frac{\mathrm{d}}{\mathrm{d}t}\biggl [ \frac{1}{2}mv^2\biggr ] = m\vec{v}\cdot\vec{a}$$ ( using the nonrelativistic expression for $k$ ) . there are three ways this can be equal to zero : $v = 0$: the object is not moving . this corresponds to a minimum of kinetic energy . $a = 0$: the object is not accelerating ( and by $f = ma$ is also experiencing no net force ) . this corresponds to either constant velocity motion , or a maximum of kinetic energy , in a simple harmonic oscillator for example . $\vec{v}\perp\vec{a}$: the object is experiencing pure centripetal acceleration , which means it is moving with a momentarily constant radius of curvature . this can be either a minimum or a maximum of kinetic energy , in an orbit for example . using the relativistic expression , you get $$\frac{\mathrm{d}k}{\mathrm{d}t} = \frac{\mathrm{d}}{\mathrm{d}t}\bigl [ ( \gamma - 1 ) mc^2\bigr ] = \frac{m\vec{v}\cdot\vec{a}}{\bigl ( 1-\frac{v^2}{c^2}\bigr ) ^{\frac{3}{2}}}$$ from which the same conclusions follow .
it is not clear what you mean by " the r-charge . " if you have a u ( 1 ) r-symmetry , you can make linear combinations of its charges and those of a non-r u ( 1 ) symmetry and get a new r-symmetry . so " values greater than 1/2" are not , generically , special . at a superconformal fixed point , there is a special u ( 1 ) r-symmetry that is part of the superconformal algebra . in this case , r-charges are related to operator dimensions and so are constrained by unitarity bounds . maybe you have this in mind . the literature on $a$-maximization might be the sort of thing you are looking for .
you need to be careful what you mean by a force in general relativity . the usual definition of a force is that you get a non-zero reading on an accelerometer you are holding , but this can lead to some surprising conclusions . to illustrate this suppose you are falling towards some massive body ( with no atmosphere to complicate the issue ) . does the gravity of the massive body create a force ? the answer has to be no , because you are in free fall so you are weightless and any accelerometer you were carrying would read zero . suppose know we give you a harness and tie you to some support fixed wrt the massive body . now you feel a force , and your accelerometer reads non-zero . but this force is due to the fact you have been restricted ( by the harness ) from following the geodesic you would otherwise follow . it is the harness that exerts the force on you ( and you on it ) because it is pulling you away from geodesic motion and therefore imparting a non-zero four acceleration . the force is not due to gravity , it is due to the harness and without the harness there will be no force . incidentally , although it is peripheral to this issue there is a nice calculation of the four acceleration and force in twistor59 's aswer to what is the weight equation through general relativity ? . let 's go back to expanding spacetime . hopefully you will now see why we say that the expansion of spacetime does not create a force . suppose we place you and me at some distance apart in an frw universe and constant comoving position and we wait to see what happens . we will each have an accelerometer so we can tell if we are accelerating . if we now wait the expansion of spacetime will increase the proper distance between us - that is , we will move apart . however because both of us are at constant comoving position we are moving along a geodesic and experience no acceleration . our accelerometers will read zero , which means we feel no force . in this sense the expansion of spacetime does not produce a force . this is exactly analogous to the claim i started out with , that the gravity of a massive body does not create a force either . now suppose we tie ourselves together with a rope . once we have done this we cannot remain at constant comoving position and this means we must be accelerating . our accelerometers would now register a non-zero acceleration towards each other and we had feel a force . any objects we drop will fall away from us . this is exactly analogous to using a harness to support yourself against the gravity of a massive body . it is the rope between us that generates a force not the expansion of spacetime . by now you are probably thinking that this is all a bit of a swindle and i have just redefined what is meant by force to make it zero . well , yes , but this is key to understanding general relativity . we do not often talk about force , but four acceleration is precisely defined in gr and can be calculated as described in the question i linked . when a general relativist talks about force they implicitly mean four acceleration . this does mean they are using the term in a different way to the general public - hence the confusion .
what force particle mediates electric fields and magnetic fields ? photons , as you have suggested . 1 ) would not that mean that a charged particle ( e . g . an electron or even a polarized h2o molecule ) would constantly be losing energy from sending out photons ? you must describe this process in a quantum field theory . virtual photons emitted by charged particles are reabsorbed in a time consistent with uncertainty principle . hence over some finite amount of time , energy is conserved . 2 ) would not that mean that an electric field is inseparable from a magnetic field , as photons have both - and that one can not have one without the other ? you can already show this in classical electromagnetism - see maxwell 's equations . 3 ) would it be possible , then , to determine the wavelength of magnetic-field-mediating photons ? if so , what is the wavelength - is it random or constant ? the wavelength of a photon is related to it is energy , which is again related to the uncertainty principle . the longer the time borrowed from the vacuum , the lower the energy of the photon , so it has a longer wavelength . hence the wavelength of virtual photons at large distances from the em source is much longer than at short distances . 4 ) how can a photon ( which has momentum ) from one electrically charged particle to an oppositely charged particle cause these particles to be pulled toward each other - or how can a magnetic field cause an electrically charged moving particle to experience a force perpendicular to the source of the magnetic field if a particle with a non-zero mass moving between the two is the mediator of that force ? this become less intuitive depending on your background . richard feynman introduced a trick which offers a way to imagine the process . imagine the photon is emitted between opposite-charge particles in the future and travels ' backward in time'- therefore its momentum minus minus what it really is . this is explained in good detail here . if " virtual photons " are involved , please explain why they work differently from regular photons unlike ' real ' photons ( which have transverse polarisation ) , virtual photons have both transverse and longitudinal polarisations . the energy momentum four vector of the virtual photons , and generally all virtual particles , is not necessarily 0: virtual particles are off mass shell . this means that virtual photons may have non-zero mass - which means that they also have a longitudinal polarisation state . it is important to consider the extra polarisation in your calculations .
before thinking about circuits , let 's think about two conducting spheres of charge that i connect by a wire . before i connect the wire , sphere 1 is at voltage $v_1$ and sphere 2 at voltage $v_2$ , let 's say $v_1&gt ; v_2$ . i find it useful , in terms of thinking about what is going on , to notice that if the spheres are the same size then saying the spheres have different potentials is equivalent to saying that the 2 spheres have different charges residing on their surfaces ( you can justify this by noting that the capacitance of a sphere is determined by its radius ) . now let 's connect the spheres . what will happen ? well , a current will flow in the wire . this will take positive charge off of sphere 1 and deposit it on sphere 2 [ strictly speaking if you want electrons to be charge carriers , then negative charge is flowing from 2 to 1 ; but in terms of thinking about what is going on it is easier to imagine , and mathematically equivalent to say , that positive charges are going from 1 to 2 ] . this in turn changes the voltages on the two spheres ; $v_1$ decreases and $v_2$ increases . the process stops when $v_1=v_2$ . again , if the spheres are the same size this condition is equivalent to the charges on both spheres being equal . ok , now imagine a battery hooked to a resistor and a switch , the simplest circuit imaginable . before we close the switch , terminal 1 is at $v_1$ and terminal 2 is at $v_2$ . at this point , it makes perfect sense to think of each terminal of the battery as being a sphere of charge . then we close the switch , this is like connecting our spheres with a wire . based on our silly model of a battery , you would the voltage between the two terminals of the battery ( ie , $v_1-v_2$ ) to decrease until eventually it reached equilibrium with $v_1=v_2$ . clearly , a battery does not behave like two spheres of charge after the circuit is closed . the whole point of a battery is that it maintains the potential difference between its two terminals . after we close the switch , a little bit of positive charge flows from terminal 1 to terminal 2 by going through the circuit . naively this means that terminal 1 has less positive charge and terminal 2 has more positive charge , so terminal 1 's voltage decreases while terminal 2 's voltage increases . inside the battery , some process takes place to to take the excess positive charge on terminal 2 and put it back onto terminal 1 . whatever this process is , it cannot be electrostatic , because positive charges following the electric field can only ever move from terminal 1 to terminal 2 [ positive charges move from high voltage to low voltage , if the only force is electrostatic ] . the details of what the battery does to maintain the potential difference varies depending of the kind of battery . a conceptually simply example of a battery is a van de graaff generator . in a van de graaff generator , you have a conveyer belt that literally carries the excess positive charge on terminal 2 and deposits it back on terminal 1 , undoing the naive ' equilization process . ' most useful batteries rely on some chemical process to maintain the potential difference . for example , one can use oxidation reactions to do this . the details involve some chemistry ( there is a wikipedia summary at http://en.wikipedia.org/wiki/electrochemical_cell ) , but essentially you put each terminal in a bath of ions , and the chemical energy of the reactions at each terminal [ balancing oxidation and reduction ] forces ionized atoms to carry electrons from terminal 2 to terminal 1 .
all correct . your values for $ \dot{x}_p$ and $ \dot{x}_c$ are negative because you defined $ \dot{x}_r$ to be positive ( both boats are moving against the river ) . no problem there , this is completely up to you , but to satisfy the requirement of the question you are required to state that motion of the river has been designated by you as positive direction ( equivalent to stating that the boat movement direction are designated negative direction ) . also by defining $ \dot{x}_r$=+3 , this implies a reference being made to the river bank ( the origin ) . your subsequent equations involving this definition of $ \dot{x}_r$ therefore also produce velocities with reference to the river bank . so the values -27 , -31 , -11 are all velocities relative to the river bank . you did not obtain the velocity of the stone relative to the police boat . this is $ \dot{x}_s- \dot{x}_p$ , using values you already obtained .
in order to understand asymptotic freedom , you need to be aware of the concept of renormalization . since you want a qualitative description , just think of renormalization a modification of the coupling strengths and masses of particles at high energies . this is roughly like pushing a ball through the water ; the harder you push , the more the water sticks around it and the harder it is to move . this can be modeled with newton 's 2nd law $f=ma$ by replacing the mass with a slightly larger mass $m+\delta m$ , and this $\delta m$ depends on the velocity of the ball in the water . ( that discussion can be found in section 3.2 of connes and marcolli , " noncommutative geometry , quantum fields and motives" ) once you have the concept of renormalization , asymptotic freedom is a property the strong force has as you scale the coupling constant to high energy . rather then the coupling getting stronger , it gets weaker . this has major consequences for confinement - that is , bound quarks . at low energies , quarks in bound states are forever bound - it becomes harder and harder to pull them apart the further apart you pull them . at high enough energies ( say , colliding two protons at 7 tev like the lhc ) the quark coupling gets small and quarks are essentially free and unbound . it should be easy to see how this would change the cross section . as a sidenote , only the strong force is asymptotically free . the e/m and weak force become stronger as the energy gets higher . in addition , it is important to realize that we cannot solve problems involving the strong force at low energies ( if you could , the clay mathematics institute would give you $1 million ! ) . once they are at high energies , the strong coupling is weak so qcd acts quite a bit like qed .
the sound barrier theory is complete nuts . however , lightning does not travel at the speed of light . in fact , when the voltage between the cloud and the earth reaches a critical level , many small low-intensity lightning bolts propagate more or less randomly downwards , ionizing the air along the way , forming a channel of conductive ionized air . once one of those bolts reaches the ground , the actual bolt is formed from the bottom up into the sky . this can be observed in this video i found on youtube . so what does cause the thunder . when the lightning strikes , the air along its path rapidly heats up and expands . this pressure wave is perceived as sound .
from the relativistic covariance of the dirac equation ( see section 2.1.3 in the qft book of itzykson and zuber for a derivation . i also more or less follow their notation . ) , you know how a dirac spinor transforms . one has $$\psi' ( x' ) =s ( \lambda ) \ \psi ( x ) $$ under the lorentz transformation $$x'^\mu= {\lambda^\mu}_\nu\ x^\nu= {\exp ( \lambda ) ^\mu}_\nu\ x^\nu= ( i + {\lambda^\mu}_\nu+\cdots ) \ x^\nu\ . $$ explicitly , one has $s ( \lambda ) =\exp\left ( \tfrac{1}8 [ \gamma_\mu , \gamma_\nu ] \ \lambda^{\mu\nu}\right ) $ . to show reducibility , all you need is to find a basis for the gamma matrices ( as well as dirac spinors ) such that $ [ \gamma_\mu , \gamma_\nu ] $ is block diagonal with two $2\times 2$ blocks . once this is shown , it proves the reducibility of dirac spinors under lorentz transformations since $s ( \lambda ) $ is also block diagonal . such a basis is called the chiral basis . it is also important to note that a mass term in the dirac term mixes the weyl spinors in the dirac equation but that is not an issue for reducibility . while this derivation does not directly use representation theory of the lorentz group , it does use the lorentz covariance of the dirac equation . i do not know if this is what you wanted . ( i am not interested in your bounty -- please do not award me anything . )
the weight of liquid impact will be minimal ( since with a screw cap the force will be redirected along the screw threading ) - it is the lubricating effect of the liquid that actually helps . you start unscrewing , the liquid gets into the threading and lubricates it .
first of all , i am not an expert on magnetism , so this is more of an additional question than answer ( cannot add pictures to comments , so thats why its here ) . in the case of ferrous materials they generate an magnetic field inside material ( ok ? ) . opposite signs attract each other ( right ? ) . the position of the spring happens to be the local minimum of potential energy by symmetry principle ( or you can actually calculate this ) . all the other phenomena are just corrections to above phenomena ( ? ) . if all above are summed together , the spring is just oscillating around a local potential energy minimum , because of the magnetic field , not because of the spring properties . this is also why the coin oscillates the same way . anyway could you comment on this , i would like to know where i went wrong ( if anywhere ) .
your confusion lies within your perception of natural length in the young 's modulus formula . when we say strain=$\delta l/l$ , the $l$ refers to the natural length of the rod at a given temperature . so , if the rod is not clamped , and we increase the temperature , there is no deviation from natural length at that temperature ( as we can define natural length of a rod at a temperature by calling it " the length of the rod at that temperature in the absence of any other influences" ) , so strain is zero . stress is obviously zero . if the rod is clamped , its length stays $l_0$ , but it is natural length becomes $l_0 ( 1+\alpha\delta t ) $ , so the $\delta l$ comes from the fact that its natural length has changed but its length is constant . summing up , in your young 's modulus formula , use strain=$\delta l_t/l_t$ , where $\delta l_t$ is $|l_0-l_t|$ , $l_t=l_0 ( 1+\alpha\delta t ) $ , and $l_0$ is length at a reference temperature . use this to solve the problem now .
starting in the fifties , there was a lot of work ( see rdd-8 , v.c. 1 . g ) trying to build a pure fusion weapon for mainly two reasons : they promised to be cleaner than conventional thermonuclear devices ( important for peaceful uses and some of the not-so-peaceful ones ) and they would not need relatively scarce fissionable materials . as you can use staging to scale to essentially unlimited yields , the problem was reduced to making the smallest possible fusion explosion ( early steps toward inertial fusion energy , p . 1-2 ) . eventually this program transformed into inertial confinement fusion research . the required energy to implode this " secondary " was originally estimated in approximately 1 mj , but this result assumed an ideal driver/primary . after the failure of over-optimistic attempts to get ignition with smaller drivers , a test program called halite/centurion was carried out to induce ignition of icf capsules using radiation from nuclear devices . this program was successful " putting to rest fundamental questions about the basic feasibility of achieving high gain " ( progress toward ignition and burn propagation in inertial confinement fusion ) . the exact results of this test program are still classified , but it seems that " some dozens mj of driver energies " ( edward teller lectures , p . 6 ) were required to reach ignition with x-rays from fission primaries . it sounds reasonable to assume that a more controllable driver can reach ignition with a smaller amount of energy and nif is trying to reach ignition using only 1.8 mj of driver energy .
$\hat{p}$ is hermitian and hermitian operators $o$ satisfy , by definition , $$\hat{o} = \hat{o}^\dagger$$ adjoint is not synonym for complex . $\hat{p} = -i\hbar \nabla \rightarrow +i\hbar \nabla^\dagger \rightarrow -i\hbar \nabla =\hat{p}^\dagger$ , but $\hat{p} \neq \hat{p}^*$ .
if the energy difference between two sites separated by $\mathbf{r}$ , then the effective electric field $\mathbf{e}$ between those two sites is given by \begin{equation} \mathbf{e}\cdot\mathbf{r}=\frac{de}{e} , \end{equation} where $e$ is the electron charge .
dear hde , the laser beam obviously has energy and momentum so the laser transmitter gets recoiled due to the conservation of energy and momentum . see also : http://en.wikipedia.org/wiki/poynting_vector the property we call mass is expressed by : $m ~=~ \sqrt{\frac{e^2}{c^4} - \frac{p^2}{c^2}}$ which is zero for photons even though the energy $e$ and momentum $p$ are not zero . the mass m is zero if energy relates to momentum as $e^2=p^2c^2$ . this shows you that photons can not be at rest because in that case both $p$ and $e$ are zero . regards , hans
there are some very interesting subtleties here . let 's analyze the situation very carefully . let 's choose our system to consist of the block , spring , and earth . by choosing the earth and block to be in our system , we will have a change in gravitational potential energy . in the beginning , the ( massless ) spring hangs vertically with a block of mass $m$ attached at the bottom . we could calculate how much the spring is stretched by equating the gravitational and spring forces ( $kx_1=mg$ ) but we will not need this . now , during the pulling process you describe , it is important to note that you are doing positive work on the system , which means that the energy in the system increases . it is tempting to say that the change in energy is zero , but this is not the case for the system we have chosen . let 's use the work-energy theorem to answer your question of where the gravitational potential energy " goes . " $$\underbrace{w_\text{net , external}}_\text{positive}=\delta e_\text{tot}=\underbrace{\delta u_\text{grav}}_\text{negative}+\underbrace{\delta u_\text{elastic}}_\text{positive}$$ yes , the gravitational potential energy decreases . where does it go ? well , the only other term that could ( mathematically ) compensate for this decrease in gravitational potential energy is the increase in elastic potential energy . but be careful with wording here . the spring is not storing gravitational potential energy ; rather , gravitational potential energy was converted to elastic potential energy . as a side note , since the left-hand side of the equation above is positive , the absolute value of $\delta u_\text{elastic}$ is greater than that of $\delta u_\text{grav}$ . so , not only was the gravitational potential energy converted to elastic potential energy , the positive work you did on the system also adds to the increase in elastic potential energy .
update 2: it might also be useful add the relation to usual galilean physics . this will probably only make sense after reading update 1 . recall that one can parametrize boosts in ( 1+1 ) by $ [ \cosh ( \eta ) , \sinh ( \eta ) ] $ . as it so happens this equals to $ [ \gamma , \gamma {v \over c} ] $ . the classical physics correspond to the asymptotic case $c \to \infty$ ( i.e. . no limit on the speed of light ) . so this reduces to $\gamma \to 1$ , ${v \over c} \to 0$ and that means $\eta \to 0$ . so in galilean case all boosts degenerate to trivial transformation and this is why time and space separate and addition of velocities starts to work . update 1: having seen kennytm 's strange answer i decided to add some notes to this answer . first , there exists a concept of rapidity . this is a natural variable for parametrization of boosts . first consider a circle . why circle ? because it has to do with rotations and rotations are very similar to boosts . circle is an object given by an equation $x^2 + y^2 = 1$ . you can decide that you will parametrize ( part of ) it by coordinates $ [ x , \sqrt{1 - x^2} ] $ . now what if you want to rotate the circle by some angle ? can you figure out how will the parametrization change ? maybe you can but i assure you this is not pretty . but there is a nicer way . let 's try parametrizing the circle by an angle $\phi$ so that it would become set of points $ [ \cos ( \phi ) , \sin ( \phi ) ] $ and now the rotation by angle $\psi$ corresponds to parametrization $ [ \cos ( \phi + \psi ) , \sin ( \phi + \psi ) ] $ . so our parameter is additive ! you will not find any better parametrization of the circle than this . okay , so we understand circles a little better now . but as already said , rotation in two space dimensions is almost the same thing as boost in ( 1+1 ) space-time dimensions . in the same way that rotations ( around origin ) preserve circles , boosts preserve hyperbolas . so instead of working with $\sin$ and $\cos$ you will work with hyperbolic functions $\sinh$ and $\cosh$ and instead of $\phi$ you will obtain rapidity $\eta$ . now , this only works this nicely in ( 1+1 ) -dimensions . in ( 3+1 ) you will have many more interesting effects ( similarly to like rotations are strange beasts in 3 dimensions as opposed in 2 ) . but it is still true that like the general 3-dimensional rotations are nicely parametrized by an axis and rotation angle , the boosts are nicely parametrized by the direction of boost and rapidity . so if you perform two rotations about the same axis it is the same as rotation by a sum of the angles and if you perform two boosts in one direction it is the same as doing a boost with added rapidities . i will assume you are talking about galilean principle of relativity whereby the velocities transform by pure addition . this concept breaks down when the speeds one is dealing with are too large . speed of light is an extreme case of such speed . then one has to use special relativity and instead consider four-vectors transforming by lorentz transformation . now , this transformations preserve the minkowski length of four-vectors ( in the same way that rotations preserve length of usual vectors ) . the point is that velocity of light corresponds to zero minkowski length and so light moves at the speed of light in every inertial frame . this is the famous einstein 's postulate .
a good analogy for the difference between the two can be given in terms of two other examples of anomalies , that are possibly more familiar . consider a field theory with a global symmetry , take $u ( 1 ) $ for simplicity . at the classical level , the equations of motion lead to the existence of a conserved current ( noether 's theorem ) . at the quantum level , the conservation of the current is valid as an operator equation , namely it is valid in correlators at separated points . the two effects , related but very different in nature , that are referred to as anomalies , are : 1 ) there can exist contact terms in correlators ( i.e. . terms that are non-zero only when two or more of the operators in the correlator are evaluated at the same point ) that do not respect the operator equation . in 4d field theory this typically happens in correlators of three current operators . this is what sometimes is referred to as an ' t hooft anomaly . it does not represent a breaking of the symmetry , because the conservation of the current operator is still valid at separated points , and one still gets a conserved charge . however , it leads to interesting constraints ( the coefficients of such contact terms must match between the uv and ir , if the symmetry is not broken along the rg flow ) . 2 ) there can be quantum effects ( you can think about them as loop corrections , assuming we are in a perturbative setting ) that violate the operator equation even at separated points . in this case the symmetry is broken , much like if you add a term in the lagrangian that does not respect the symmetry . there is no conserved charge any more . the relation between 1 ) and 2 ) can be explained in a slightly refined example . take the global symmetry to be $u ( 1 ) ^2$ . than you could have an anomaly of type 1 ) in a correlator involving one current of the first $u ( 1 ) $ , and two currents of the second $u ( 1 ) $ . now suppose modifying the theory by gauging the second $u ( 1 ) $ , i.e. coupling the current of the second $u ( 1 ) $ to dynamical gauge fields . in the new gauged theory , the first $u ( 1 ) $ is broken by an anomaly of type 2 ) . the divergence of its current is now non-zero , and given by the pontryagin density of the gauge fields of the second $u ( 1 ) $ . the first example of trace-anomaly that you discuss is the analogue of 1 ) , while the second is the analogue of 2 ) , when instead of a global $u ( 1 ) $ we consider the dilatation symmetry . the first example do not represent a violation of the symmetry , it is just the statement that certain contact terms in the correlators with multiple insertions of the energy-momentum tensor are not compatible with the traceless-ness condition . the second example instead is a genuine violation of the symmetry . the analogy with the $u ( 1 ) $ symmetry does not go through when we try to relate 1 ) with 2 ) , because the equivalent of " coupling the current to gauge field " would be introducing dynamical gravity , which brings us away from the domain of quantum field theory . this analogy becomes very concrete in supersymmetric theories . there , the energy-momentum tensor belongs to the same multiplet of the current associated to the so-called r-symmetry . supersymmetry relates the ' t hooft anomaly of this current to the first kind of trace-anomaly that you discuss ( i.e. . they have the same coefficient ) . moreover , when dilatation symmetry is broken by a gauge coupling via the trace anomaly of second type that you discuss , then the current has an anomaly of type 2 ) . again , the trace anomaly and the current anomaly have the same coefficient by supersymmetry .
there are several points of evidence that the oort cloud exists , though it is indeed still a hypothesis and lacks direct observation . the first is indirectly observational , as proposed by ernst öpik back in 1932 as the source of long-period comets . this was revised by jan oort in 1950 . all you need to determine an orbit is three observations of the object , separated in time . the greater the separation in time and the more observations , the more certainty we have in its orbit . comets with periods longer than pluto 's must , by definition , have come from beyond pluto . pluto 's orbit basically loosely defines the extent of the kuiper belt ( 30-50 au ) . so there needs to be a source for these bound objects , and interstellar ones do not cut it because if they are interstellar , then they should not be on bound orbits . the second is theoretical : solar system formation models predict that the formation of the giant planets would have scattered small icy objects into the outer solar system . while some would be given enough energy to completely escape the solar system , others would be scattered out to the hypothetical oort cloud . third , we have seen kuiper belts around other star systems , and it is likely that the oort cloud is a continuation of the kuiper belt , so this may be evidence for oort clouds as well . so if we need a source for long-period comets and the orbits work out to this cloud beyond the kuiper belt , dynamical models predict that the bodies would exist there , and we see similar dynamical structures around other stars , then that is fairly compelling evidence it exists . but , you are correct that , at present , it is not technologically possible to view comets that are members of the oort cloud that are still in the oort cloud . viewing a chunk of ice 1/4 of the way to the nearest star is simply not possible . . . yet .
let 's try the hard way without feynman 's argument . just snell 's law . we can choose a frame $ ( x , z ) $ where $z$ is along $oo'$ and $o$ is at $ ( 0,0 ) $ . we suppose the curve equation is written $z=f ( x ) $ where $ ( x , z ) $ is the location of point $p$ . the vector normal to the curve at $p$ can be written $\vec n = ( 1 , -1/f' ( x ) ) $ . now , knowing snell 's law , you can write $\sin\theta =n\sin\theta'$ , and rewrite it with vector products as $$\frac{\vec{op}\times\vec n}{op\ ; \ ; n}=-n\frac{\vec{o'p}\times\vec n}{o'p\ ; \ ; n}$$ where $n$ can be eliminated on the denominator . expressing all terms as a function of $x$ and $z=f ( x ) $ , you can obtain a differential equation on $f ( x ) $ . $$ [ x+f ( x ) f' ( x ) ] \sqrt{ ( x'-x ) ^2+ ( z'-f ( x ) ) ^2}+n [ ( x'-x ) + ( z'-f ( x ) ) f' ( x ) ] \sqrt{x^2+f ( x ) ^2}=0$$ that seems really difficult to solve unless there is some clever calculus to do . in fact , feynman argument saying that the light should take the same time to travel that distance , whatever the trajetory , is very clever . it should not be absolutely necessary to use that , but it simplifies the problem : $op+no'p=\mathrm{cste}=c$ . this translates into $$\sqrt{x^2+z^2}+n\sqrt{ ( x'-x ) ^2+ ( z'-z ) ^2}=c$$ which you can rewrite as a fourth order equation .
your definitions are in fact those for proper , orthochronous lorentz transformation , not for general lorentz transformations , that is why you are having trouble telling the difference ! ( if it makes you feel any better , yesterday a collegue and i were trying to debug his test setup and two hours of complex testing passed before we two geniusses realised we had not switched the power to a key bit of kit on ! ) a general lorentz transformation is defined by criterion 1 ) alone - it is simply any linear transformation that preserves the quadratic form $t^2 - x^2 - y^2 - z^2$ . the proper , orthochronous transformations are those that belong to the identity connected component $so^+ ( 1 , \ , 3 ) $ of the full lorentz group $o ( 1 , \ , 3 ) $ . that is , the proper , orthochronous transformations are those that can be reached from the $4\times4$ identity matrix by following a continuous path through the lorentz group . equivalently , they are the matrices that are on paths through the lorentz group defined by the differential equation : $$\begin{array}{lcl}\mathrm{d}_s l and = and ( a_x ( s ) \ , j_x + a_2 ( s ) \ , j_y+a_z ( s ) \ , j_z + b_x ( s ) \ , k_x + b_y ( s ) \ , k_y+b_z ( s ) \ , k_z ) \ , l\\l ( 0 ) and = and \mathrm{id}\end{array}\tag{1}$$ where $\mathrm{id}$ is the $4\times 4$ identity , $a_j ( s ) , \ , b ( s ) $ are continuous functions of the parameter $s$ and the $j_j , \ , k_j$ are six matrices $4\times 4$ that span the lie algebra of the lorentz group , i.e. the real vector space of all possible " tangents to the identity " , i.e. all possible values of $\mathrm{d}_s l|_{s=0}$ . one possible set is : $$\begin{array}{lcllcllcl}j_x and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and -1\\0 and 0 and 1 and 0\end{array}\right ) and j_y and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 0\\0 and 0 and 0 and 1\\0 and 0 and 0 and 0\\0 and -1 and 0 and 0\end{array}\right ) and j_z and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 0\\0 and 0 and -1 and 0\\0 and 1 and 0 and 0\\0 and 0 and 0 and 0\end{array}\right ) \\k_x and = and \left ( \begin{array}{cccc}0 and 10 and 0 and 0\\1 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\end{array}\right ) and k_y and = and \left ( \begin{array}{cccc}0 and 0 and 1 and 0\\0 and 0 and 0 and 0\\1 and 0 and 0 and 0\\0 and 0 and 0 and 0\end{array}\right ) and k_z and = and \left ( \begin{array}{cccc}0 and 0 and 0 and 1\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\1 and 0 and 0 and 0\end{array}\right ) \end{array}\tag{2}$$ ( see how the $j_j$ are skew-hermitian , thus have pure imaginary eigenvalues , so that $\exp ( a_j\ , j_j ) $ has stuff like $\sin , \ , \cos$ of an angle and is a rotation matrix , whereas the $k_j$ are hermitian , with purely real eigenvalues , so that $\exp ( b_j\ , k_j ) $ has stuff like $\sinh , \ , \cosh$ of a rapidity and is a pure boost matrix ) . an intuitive description : imagine you are sitting at the console of your spaceship 's " hyperdrive": it has two track balls each with their own levers marked " spin " and " boost " and a set of accelerometers - linear and rotational . your spaceship is initially moving inertially . you roll the trackballs around to set the axis of rotation and direction of boost respectively . when you pull on the levers , the spin lever accelerates the angular speed about the rotation axis , the boost lever accelerates the linear velocity in the boost direction . otherwise put , the " rotate " trackball and its lever set the superposition weights $a_j ( s ) $ of the $j_j$ in ( 1 ) when we use the definitions in ( 2 ) and the " boost " trackball and its lever set the weights $b_j$ of the $k_k$ . you go through a control sequence , ending so that your accelerometers read nought , so that now a set of $x , \ , y , \ , z$ axes attached to your spaceship is moving inertially relative to the beginning frame . the proper , orthochronous transformations are precisely every transformation between the beginning frame and an inertial frame that you can reach with your controls . however , there are other transformations possible that preserve the quadratic form $t^2 - x^2 - y^2 - z^2$ that do not fulfill your criteria 2 . and 3 . but they follow only a " simple " pattern that makes them " not much different " from the identity connected component . a discrete subgroup of the full lorentz group is $\{\mathrm{id} , \ , p , \ , t , \ , p\ , t\}$ with $$p=\text{"parity flipper"} = \mathrm{diag} [ 1 , \ , -1 , \ , -1 , \ , -1 ] ; \\t=\text{''time flipper''} = \mathrm{diag} [ -1 , \ , 1 , \ , 1 , \ , 1 ] $$ with the exception of $\mathrm{id}$ , none of these can be reached from the identity by paths fulfilling ( 1 ) . they belong to different connected components from the identity component $so^+ ( 1 , \ , 3 ) $ . indeed , the identity connected component is a normal subgroup of the full lorentz group $so ( 1 , \ , 3 ) $ and the quotient $o ( 1 , \ , 3 ) / so^+ ( 1 , \ , 3 ) $ is the little group $\{\mathrm{id} , \ , p , \ , t , \ , p\ , t\}$ . so any full lorentz transformation can be represented as a proper orthochronous transformation followed by one of $p , \ , t$ or $p\ , t$ . there are four separate connected components to the full lorentz group . ( an aside : $\{\mathrm{id} , \ , p , \ , t , \ , p\ , t\}$ is the klein " fourgroup": the only possible group of four elements aside from $\mathbb{z}_4$ ) . to sniff out a non-proper or non-orthochronous transformation , you do one of two things : compute the matrix 's determinant . if it is -1 , then you know it has to include one of $p$ or $t$ , so it is not proper or not orthochronous . you can further differentiate the $p$ and $t$ cosets by looking at the $l_0^0$ component of the transformation : the $t$ coset has $l_0^0&lt ; 0$ , since such a transformation swaps the roles of the " future " and " past " ( actually reflects minkowsky vector space in the $t=0$ plane ) . if the determinant is $+1$ , then it may belong to the $p\ , t$ coset of $o ( 1 , \ , 3 ) $ . as in point 1 , the $t$ coset and the $p\ , t$ coset can be recognised as transformations with $l_0^0&lt ; 0$
first of all , the question you are asking is very important and you may master it completely . dimensionful constants are those that have units - like $c , \hbar , g$ , or even $k_{\rm boltzmann}$ or $\epsilon_0$ in si . the units - such as meter ; kilogram ; second ; ampere ; kelvin - have been chosen partially arbitrarily . they are results of random cultural accidents in the history of mankind . a second was original chosen as 1/86,400 of a solar day , one meter as 1/40,000,000 of the average meridian , one kilogram as the mass of 1/1,000 cubic meters ( liter ) of water or later the mass of a randomly chosen prototype , one ampere so that $4\pi \epsilon_0 c^2$ is a simple power of 10 in si units , one kelvin as 1/100 of the difference between the melting and boiling points of water . clearly , the circumference of the earth , the solar day , a platinum prototype brick in a french castle , or phase transitions of water are not among the most " fundamental " features of the universe . there are lots of other ways how the units could be chosen . someone could choose 1.75 meters - an average man 's height - to be his unit of length ( some weird people in the history have even used their feet to measure distances ) and he could still call it " one meter " . it would be his meter . in those units , the numerical values of the speed of light would be different . exactly the products or ratios of powers of fundamental constants that are dimensionless are those that do not have any units , by definition , which means that they are independent of all the random cultural choices of the units . so all civilizations in the universe - despite the absence of any interactions between them in the past - will agree about the numerical value of the proton-electron mass ratio - which is about $6\pi^5=1836.15$ ( the formula is just a teaser i noticed when i was 10 ! ) - and about the fine-structure constant , $\alpha\sim 1/137.036$ , and so on . in the standard model of particle physics , there are about 19 such dimensionless parameters that " really " determine the character of physics ; all other constants such as $\hbar , c , g , k_{\rm boltzmann} , \epsilon_0$ depend on the choice of units , and the number of independent units ( meter , kilogram , second , ampere , kelvin ) is actually exactly large enough that all those constants , $\hbar , c , g , k_{\rm boltzmann} , \epsilon_0$ , may be set equal to one which simplifies all fundamental equations in physics where these fundamental constants appear frequently . by changing the value of $c$ , one only changes social conventions ( what the units mean ) , not the laws of physics . the units where all these constants are numerically equal to 1 are called the planck units or natural units , and max planck understood that this was the most natural choice already 100 years ago . $c=1$ is being set in any " mature " analysis that involves special relativity ; $\hbar=1$ is used everywhere in " adult " quantum mechanics ; $g=1$ or $8\pi g=1$ is sometimes used in the research of gravity ; $k_{\rm boltzmann}=1$ is used whenever thermal phenomena are studied microscopically , at a professional level ; $4\pi\epsilon_0$ is just an annoying factor that may be set to one ( and in gaussian 19th century units , such things are actually set to one , with a different treatment of the $4\pi$ factor ) ; instead of one mole in chemistry , physicists ( researchers in a more fundamental discipline ) simply count the molecules or atoms and they know that a mole is just a package of $6.022\times 10^{23}$ atoms or molecules . the 19 ( or 20 ? ) actual dimensionless parameters of the standard model may be classified as the three fine-structure constants $g_1 , g_2 , g_3$ of the $u ( 1 ) \times su ( 2 ) \times su ( 3 ) $ gauge group ; higgs vev divided by the planck mass ( the only thing that brings a mass scale , and this mass scale only distinguishes different theories once we also take gravity into account ) ; the yukawa couplings with the higgs that determine the quarks and fermion masses and their mixing . one should also consider the strong cp-angle of qcd and a few others . once you choose a modified standard model that appreciates that the neutrinos are massive and oscillate , 19 is lifted to about 30 . new physics of course inflates the number . susy described by soft susy breaking has about 105 parameters in the minimal model . the original 19 parameters of the standard model may be expressed in terms of more " fundamental " parameters . for example , $\alpha$ of electromagnetism is not terribly fundamental in high-energy physics because electromagnetism and weak interactions get unified at higher energies , so it is more natural to calculate $\alpha$ from $g_1 , g_2$ of the $u ( 1 ) \times su ( 2 ) $ gauge group . also , these couplings $g_1 , g_2$ and $g_3$ run - depend on the energy scale approximately logarithmically . the values such as $1/137$ for the fine-structure constant are the low-energy values , but the high-energy values are actually more fundamental because the fundamental laws of physics are those that describe very short-distance physics while long-distance ( low-energy ) physics is derived from that . i mentioned that the number of dimensionless parameters increases if you add new physics such as susy with soft breaking . however , more complete , unifying theories - such as grand unified theories and especially string theory - also imply various relations between the previously independent constants , so they reduce the number of independent dimensionless parameters of the universe . grand unified theories basically set $g_1=g_2=g_3$ ( with the right factor of $\sqrt{3/5}$ added to $g_1$ ) at their characteristic " gut " energy scale ; they may also relate certain yukawa couplings . string theory is perfectionist in this job . in principle , all dimensionless continuous constants may be calculated from any stabilized string vacuum - so all continuous uncertainty may be removed by string theory ; one may actually prove that it is the case . there is nothing to continuously adjust in string theory . however , string theory comes with a large discrete class of stabilized vacua - which is at most countable and possibly finite but large . still , if there are $10^{500}$ stabilized semi-realistic stringy vacua , there are only 500 digits to adjust ( and then you may predict everything with any accuracy , in principle ) - while the standard model with its 19 continuous parameters has 19 times infinity of digits to adjust according to experiments .
first impressions based on a quick read of the preprint : i am out of my depth on this ! i could not tell you if their derivation is correct , but assuming that it is : they do not treat real qcd . they study su ( 2 ) ym without quarks . the authors claim they can do real qcd and get the same result , but this is not demonstrated in the paper ( they defer this to a later publication ) . they derive the gauge invariant infrared finite effective action of this theory at one loop . this is surely an impressive achievement , and an important milestone if it is true , but is probably still far from what a mathematician would accept as a " proof . " with the above caveats they show that monopole-antimonopole condensation is responsible for confinement , and that the tachyons appearing in previous calculations are unphysical . edit : user1504 mentions the millenium prize , which involves pure ym ( with an arbitrary gauge group though ) . this paper definitely does not satisfy the prize conditions : it uses the regular not entirely rigorous ( i.e. . , not axiomatically formulated ) definition of yang-mills theory used by physicists , and it does not prove a mass gap . you need a calculation to all orders to do that to a mathematician 's satisfaction .
carroll and ostlie , and shu are both excellent introductory texts which have good discussions of star formation . the former is a little more quantitative , the latter qualitative . also the online notes of mark krumholz are fantastic if you have some background in physics . the wikipedia page is also not bad for concepts . star formation the most basic treatment of star formation is generally ' jeans collapse ' ( or ' instability' ) . you start with a large extent of gas , and estimate at what mass and radius ( the " jean 's mass " and " jean 's radius" ) the ' cloud ' will collapse and start forming stars . the initial material has to have a non-zero temperature , because 1 ) reaching 0 kelvin is impossible , 2 ) the thermal motion and pressure is what keeps the gas from having already collapsed . as you suggest , any initial density perturbations ( which there are always plenty of ) can be seeds for initial collapse . if you imagine a perfectly uniform distribution of cold matter , any increase in density is unstable - and will trigger gravitational collapse . collapse is generally thought to be ' hierarchical ' --- a large cloud or ' clump ' ( 1,000s - 10,000s of solar masses ) will start to collapse , then smaller ' cores ' ( 100 's of solar masses ) will collapse within it , then finally protostars within cores . if gas is hot , it will not collapse because the thermal pressure resists gravity . thus star formation requires cooling ( the details of which are a very active area of research ) . to form stars , gas needs to reach about 10 kelvin ( very cold ! ) . at such low temperatures , hydrogen is neutral ( ions exist at higher temperatures ) , and eventually combined into ' molecular hydrogen ' ( $h_2$ ) . dust ( molecules heavier than $h_2$ in astro-parlance ) helps cooling , and thus is associated with enhanced star formation . dust and molecular gas are the main things you see in active star formation regions , like the carina nebula
in general , quantum numbers are labels of irreducible representations of the relevant symmetry group , not primarily eigenvalues of an otherwise simply defined operator . but for every label that has a meaningful numerical value in every irreducible representation , one can define a hermitian operator having it as an eigenvalue , simply by defining it as the sum of the projections to the irreducible subspaces multiplied by the label of this representation . it is not clear whether such an operator has any practical use . this also holds for the spin . however , one can define the spin in a representation independent way , though not via eigenvalues . the spin of an irreducible positive energy representation of the poincare group is $s= ( n-1 ) /2$ , where $s$ is the smallest integer such that the representation occurs as part of the foldy representation in $l^2 ( r^3 , c^n ) $ with inner product defined by $~~~\langle \phi|\psi \rangle:= \displaystyle \int \frac{dp}{\sqrt{p^2+m^2}} \phi ( p ) ^*\psi ( p ) $ . the poincare algebra is generated by $p_0 , p , j , k$ and acts on this space as follows . $p$ is multiplication by $p$ , $~~~p_0 := \sqrt{m^2+p^2}$ , $~~~j := q \times p + s$ , $~~~k := \frac{1}{2} ( p_0 q + q p_0 ) + \displaystyle\frac{p \times s}{m+p_0}$ , with the position operator $q := i \hbar \partial_p$ and the spin vector $s$ in a unitary irreducible representation of $so ( 3 ) $ on the vector space $c^n$ of complex vectors of length $n$ , with the same commutation relations as the angular momentum vector . the poincare algebra is generated by $p_0 , p , j , k$ and acts on this space irreducibly if $m&gt ; 0$ ( thus givning the spin $s$ representation ) , while it is reducible for $m=0$ . indeed , in the massless case , the helicity $~~~\lambda := \displaystyle\frac{p\cdot s}{p_0}$ , is central in the universal envelope of the lie algebra , and the possible eigenvalues of the helicity are $s , s-1 , . . . , -s$ , where $s= ( n-1 ) /2$ . therefore , the eigenspaces of the helicity operator carry by restriction unitary representations of the poincare algebra ( of spin $s , s-1 , . . . , 0$ ) , which are easily seen to be irreducible . the foldy representation also exhibits the massless limit of the massive representations . edit : in the massless limit , the formerly irreducible representation becomes reducible . in a gauge theory , the form of the interaction ( multiplication by a conserved current ) ensures that only the irreducible representation with the highest helicity couples to the other degrees of freedom , so that the lower helicity parts have no influence on the dynamics , are therefore unobservable , and are therefore ignored .
these are chunks of rock that existed as part of the crust of mars but were ejected into interplanetary space by a very powerful impact and then eventually impacted the earth . it was not until we had sent probes to mars and began to understand the composition of martian minerals and atmosphere that we started realizing some of the meteorites we had already found were from mars . the clincher has been the analysis of trapped gasses within meteorites . nearly 100 martian meteorites are known to have been found . interestingly , most martian meteorites fall into only 3 mineralogical categories ( shergottites , nakhlites , and chassignites , or snc meteorites ) and had been identified as being unusual as meteorites even before they were confirmed to have originated on mars . similarly , there are likely some number of earth meteorites on mars .
if the uniformity were somehow perfectly smooth , all we would have is a very tenuous nebula of mostly hydrogen , with some helium and a little bit of lithium . without the irregularities to start off star formation or any of those activities , that is all we had have . i am not about to run a back of the envelope calculation , but i would think that the overall density would not be much greater than current interstellar space . going further back , we would possibly be in a state of having no baryonic matter at all . while delving into areas we do not really know that much about ( but we are exploring ) , one school of thought says that because of the irregularities , somehow regular matter won out over anti-matter . as to the massive black hole idea , it would most likely not be the case because of cosmic inflation . the space would still be expanding so that a collapse would not be feasible in that epoch of the universe . and the question is pretty nonsensical in that nothing is perfect . an irregularity is pretty much a certainty given we are dealing with particles that have mass .
ok . the formula $i=i_1+i_2+\sqrt{i_1 i_2}\cos ( \delta\phi ) $ is correct , but only if we consider that the two beams always have the same phase and $\delta\phi$ is the angle between their unit polarisation vectors . however , this formula normally reads a little bit different . normally $\delta\phi$ denotes the phase shift of your two electromagnetic waves ( which light is ) and still it is a special case , in which the polarisation is parallel ( like you said , you get the biggest contribution then ) . let 's now look at the general case : let our two beams be $\vec e_1 ( \vec r , t ) =e_1 ( \vec r ) \hat e_1 e^{-i ( \omega t-\phi_1 ) }$ and $\vec e_2 ( \vec r , t ) =e_2 ( \vec r ) \hat e_2 e^{-i ( \omega t-\phi_2 ) }$ , where $e_i ( \vec r ) $ is the amplitude , $\hat e_i$ the unit polarisation vector and $\phi_i$ the phase shift respectively . then we got for our intensities ( without interference ) ( omitting $\frac{1}{2}c\epsilon_0\epsilon$ ) : $i_i ( \vec r ) =\langle|\vec e_i ( \vec r , t ) |^2\rangle$ . now with interference : $$\vec e ( \vec r , t ) =\vec e_1 ( \vec r , t ) +\vec e_2 ( \vec r , t ) = [ e_1 ( \vec r ) \hat e_1e^{i\phi_1}+e_2 ( \vec r ) \hat e_2 e^{i\phi_2} ] e^{-i\omega t} \ , . $$ thus $$\begin{array}{}i ( \vec r , t ) and =\langle|\vec e ( \vec r , t ) |^2\rangle\\ and = [ \langle|\vec e_1|^2\rangle+\langle|\vec e_2|^2\rangle+2\langle\operatorname{re} [ ( \vec e_1\cdot\vec e_2 ) ] \rangle ] \\ and =i_1 ( \vec r ) +i_2 ( \vec r ) +2\langle\operatorname{re} [ e_1 ( \vec r ) e_2 ( \vec r ) ( \hat e_1\cdot\hat e_2 ) e^{i ( \phi_1+\phi_2 ) } ] \rangle\\ and =i_1 ( \vec r ) +i_2 ( \vec r ) +2\sqrt{i_1 ( \vec r ) i_2 ( \vec r ) }\cos ( \phi_1-\phi_2 ) ( \hat e_1\cdot\hat e_2 ) \ , . \end{array}$$ now we just call $\delta\phi=\phi_1-\phi_2$ . so as you thought , calling the angle between $\hat e_1$ and $\hat e_2$ $\theta$ , we have with $ ( \hat e_1\cdot\hat e_2 ) =|\hat e_1||\hat e_2|\cos ( \theta ) =\cos ( \theta ) $ a cosine dependency . now to the different cases . let 's say $\hat e_1=\left ( \begin{array}{c} 1 \\ 0 \end{array}\right ) $ . if we have parallel polarisation , then $\hat e_2=\left ( \begin{array}{c} 1 \\ 0 \end{array}\right ) $ . this gives us $ ( \hat e_1\cdot\hat e_2 ) =1$ , so we get the maximum interference . if we have orthogonal polarisation , $\hat e_2=\left ( \begin{array}{c} 0 \\ 1 \end{array}\right ) $ . then $ ( \hat e_1\cdot\hat e_2 ) =0$ , so we have no interence after all . now to the case that their angle is $45°$: naively , we would then assume that $\hat e_2=\left ( \begin{array}{c} 1 \\ 1 \end{array}\right ) $ . however this is no unit vector . so , we have to make it one by dividing by its magnitude , which is $\sqrt2$ . so then we get , $ ( \hat e_1\cdot\hat e_2 ) =\frac{1}{\sqrt2}$ , which is the same as $\cos ( 45° ) $ .
first , there are too much errors in the context of your question . the last term in the expression $1$ is certainly false , because $h_{\alpha\beta\alpha'\beta'}$ is antisymmetric in the transformation $\alpha \to \beta , \alpha ' \to \beta'$ , while the last term is symmetric for this same transformation . moreover , the terms $h_{ ( \alpha\beta ) }$ are certainly zero because this a multiplication of $\epsilon^{\alpha\beta}$ , antisymmetric ( in $\alpha , \beta$ ) , and $h_{ ( \alpha \beta ) \dot {\alpha }\dot {\beta }}$ , a symmetric quantity ( in $\alpha , \beta$ ) . secondly , it is certainly false that : $\varepsilon^{\dot {\alpha } \dot {\beta } } ( \sigma^{\mu} ) _{\alpha \dot {\alpha}} ( \sigma^{\nu} ) _{\beta \dot {\beta}} = \pm ( \sigma^{\mu}\tilde {\sigma }^{\nu } ) _{\alpha \beta } , $ with your notations , $\sigma^\mu$ has indices $ ( \sigma^{\mu} ) _{\alpha \dot {\alpha}}$ , and $\tilde {\sigma }^{\nu }$ has indices $\tilde {\sigma }^{\nu }_{\beta \dot {\beta}}$ ( the standard notation $\tilde {\sigma }^{\nu }_{\dot \beta {\beta}}$ is preferable ) , anyway you cannot have a matrix $ ( \sigma^{\mu}\tilde {\sigma }^{\nu } ) $ with indices $ ( \sigma^{\mu}\tilde {\sigma }^{\nu } ) _{\alpha \beta }$ . even if you define a matrix $\tilde { ( \sigma }^{\nu } ) ^{\beta \dot {\beta}}$ or $\tilde { ( \sigma }^{\nu } ) ^{\dot \beta {\beta}}$ , this does not work , you are unable to find 2 lower indices $_{\alpha\beta}$
sure , feynman was concerned with a physical system doing the actual computation , and all physical evolution is unitary , and hence the hamiltonians must be unitary . a slight but trivial generalization would be to allow for evolution with completely positive maps , but such evolution can always be mimicked by unitary evolution with some auxiliary degrees of freedom followed by tracing out of those extra degrees of freedom . allowing for non-unitary evolutions would allow for very unphysical situations ( signalling , . . . ) and would also allow quantum computers to solve np-complete problems ( which is certainly not expected to be possible )
if the plates are disconnected , the charge has nowhere to go . rather U will have to change . what happens is the charged capacitor does work on the dielectric ( pulling it in ) , resulting in a change in the energy stored in the capacitor .
there is no " better " or worse here . it is just that " work " in physics is defined differently than in chemistry . in chemistry , all quantities follow this sign convention : they are positive if their effect is on the system . so , basically , $du$ is ( infinitesimal ) energy imparted to the system by the surroundings $\delta q$ is the heat passed to the system from the surroundings $\delta w$ is the work done on the system by the surroundings in physics , the sign convention of $w$ is the opposite $du$ is energy imparted to the system by the surroundings $\delta q$ is the heat passed to the system from the surroundings $\delta w$ is the work done by the system on the surroundings which means that $du_c = \delta q_c + \delta w_c$ ( $c$ means chemistry ) becomes $d u_p = \delta q_p - \delta w_p$ try to keep these conventions separate in your mind . do not use the physics flt for a chemistry problem and vice versa , many times problems specify values of $w , q , u$ and expect you to know the sign convention . note that these are the iupac/iupap conventions . some books ( as @dmckee mentions , feynman 's lectures is one of them ) use different conventions . in such cases , just make note of the convention and remember that the flt is just a statement of conservation of energy . here 's the menemonic i used to remember it . it is not a great one , but it works : chemists are interested in supplying hear/energy/pressure to a reaction to make it occur . thus , action done by the surroundings on the system is " good " or " positive . physicists are more interested in supplying heat/energy to a system and making it do work . so , supplying heat/energy is " good " , and getting work out is " good " .
energy is conserved so it can not be created or destroyed . all we can do is change energy from one form to another . in your example we are changing the potential energy of the mass $m$ into kinetic energy . the increase in kinetic energy must be equal to the decrease otherwise energy would not have been conserved . by an external force i assume you mean some third party outside the system . to give a slightly ridiculous example this could be me standing well away from the earth and the mass and poking the mass with a long pole to accelerate it . in this case the energy of the earth + mass would not be conserved , but also my energy would not be conserved . however the energy of the earth , the mass and me would be conserved . the distinction between internal and external forces is a bit artificial because all systems are closed and all forces are internal if you look on a big enough scale .
general remarks . in general , you cannot " derive " a representation of a given group $g$ on the objects you are considering , but there are some really standard definitions of certain group representations which are given special names like " scalar , " " vector , " and so on . however , given the representation of a lie group $g$ , this induces a representation of its lie algebra $\mathfrak g$ , and determining an explicit formula for this lie algebra representation is precisely what we do when we find the so-called " infinitesimal generators " of the corresponding group representation . an example . $\mathrm{so} ( 2 ) $ let $c^\infty ( \mathbb r^2 ) $ denote the vector space of smooth functions on the plane $\mathbb r^2$ . the scalar representation $\rho$ of $\mathrm{so} ( 2 ) $ acting on $c^\infty ( \mathbb r^2 ) $ is defined as \begin{align} ( \rho_0 ( r ) \phi ) ( \mathbf x ) = \phi ( r^{-1}\mathbf x ) . \end{align} for each $\phi\in c^\infty ( \mathbb r^2 ) $ and for each $r\in\mathrm{so} ( 2 ) $ . what the heck is going on here ? well , notice that this can also be written as follows : \begin{align} ( \rho_0 ( r ) \phi ) ( r\mathbf x ) = \phi ( \mathbf x ) \end{align} so this definition encapsulates the intuitive idea that the transformed field $\rho ( r ) \phi$ evaluated at the transformed point $r\mathbf x$ agrees with the untransformed field $\phi$ evaluated at the untransformed point $\mathbf x$ . in physics , it is common to see " primed " notations for the transformed field and transformed point ; \begin{align} \rho_0 ( r ) \phi = \phi ' , \qquad r\mathbf x = \mathbf x ' \end{align} in which case the definition of the scalar representation can be written as \begin{align} \phi' ( \mathbf x' ) = \phi ( \mathbf x ) \end{align} this probably looks familiar . so basically the " invariance " that is happening is that the value of the field does not change provided the transformed field is evaluated at the transformed point . infinitesimal generators . to find the infinitesimal generators of a given representation , we are really just trying to find a certain representation of the lie algebra of the group . this lie group representation $\rho$ naturally induces a lie algebra representation $\bar \rho$ as follows : \begin{align} \bar \rho ( x ) = \frac{d}{dt}\rho ( e^{tx} ) \big|_{t=0} \end{align} so , for the $\mathrm{so} ( 2 ) $ example , we know that the lie algebra $\mathfrak{so} ( 2 ) $ is generated by the single element \begin{align} j = \begin{pmatrix} 0 and -1 \\ 1 and 0 \\ \end{pmatrix} , \end{align} and we can determine how this element is represented in representation induced by the scalar representation defined above as follows : \begin{align} ( \bar\rho_0 ( j ) \phi ) ( \mathbf x ) and = \frac{d}{dt}\phi ( e^{-tj}\mathbf x ) \big|_{t=0} \\ and = \frac{d}{dt}\phi ( x-ty , y+tx ) \big|_{t=0} \\ and = -y\partial_x\phi ( x , y ) + x\partial_y\phi ( x , y ) \\ and = ( -y\partial_x + x\partial_y ) \phi ( \mathbf x ) \end{align} in other words , in the scalar representation , the generator of rotations on the plane is represented by a differential operator ; \begin{align} \bar\phi_0 ( j ) = -y\partial_x + x\partial_y . \end{align} this same procedure can be extended to find infinitesimal generators of other representations as well , like the vector representation $\rho_1$ of $\mathrm{so} ( 2 ) $ which is defined to act on vector fields $\mathbf v$ on the plane as follows : \begin{align} ( \rho_1 ( r ) \mathbf v ) ( \mathbf x ) = r\mathbf v ( r^{-1}\mathbf x ) \end{align} by the way , you might find the following links interesting and/or helpful as well : tensor operators representations of lie algebras in physics differential realizations of certain algebras generators of poincare groups idea of covering group unitary spacetime translation operator rigorous underpinnings of infinitesimals in physics
i hope you know that intensity $ ( i ) $ of light at any point on the screen due to interference in the young 's double slit experiment can be given as $$a^2=i=a_1^2+a_2^2+2a_1a_2\cos{\phi}$$ where $a_1 , a_2$ are the amplitudes of the light waves with constant phase difference of $\phi$ , $a$ is the amplitude of the resultant displaement at the point on the screen . for simplicity , we can assume that intensity of light to be equal to square of the amplitude as given above . thus , $$i_{max}=a_1^2+a_2^2+2a_1a_2 ( 1 ) = ( a_1+a_2 ) ^2$$ $$i_{min}=a_1^2+a_2^2+2a_1a_2 ( -1 ) = ( a_1-a_2 ) ^2$$ therefore , $\frac{i_max}{i_min}=\frac{ ( a_1+a_2 ) ^2}{ ( a_1-a_2 ) ^2}=\frac{25}{9}$ thus , $a_1+a_2=5 , a_1-a_2=3$ $a_1+ ( a_1-3 ) =5=2a_1-3$ thus , $a_1=8/2=4 , a_2=1$ the intensity of light due to a slit ( source of light ) is directly proportional to width of the slit . therefore , if $w_1$ and $w_2$ are widths of the tow slits $s_1$ and $s_2$ ; $i_1$ and $i_2$ are intensities of light due to the respective slits on the screen , then $$\frac{w_1}{w_2}=\frac{i_1}{i_2}=\frac{a_1^2}{a_2^2}=\frac{4^2}{1^2}=16$$
in special relativity , you have to choose as frame of reference which is an inertial frame . in this inertial frame , you may consider the movement of any object , whatever this movement is ( accelerated or not ) . let the coordinates of the moving object , relatively to an inertial frame $f$ , be $x$ and $t$ . we can consider an other initial frame $f'$ , which coordinates of the moving object , relatively to $f'$ , are $x'$ and $t'$ the heart of special relativity is that exists an invariant which is $c^2 dt^2 - \vec dx^2 = ds^2$ . this means that : $c^2 dt^2 - \vec {dx}^2 =ds^2= ds'^2 = c^2 dt'^2 - \vec {dx'}^2$ . all inertial frames , when looking at the moving object , agree on the same value $ds^2$ now , at some instant $t_0$ , you may always consider a inertial frame $f' ( t_0 ) $ which has , at this instant , the same speed as the moving object , relatively to $f$ . of course , you will have a different inertial frame $f' ( t ) $ for each instant . however , the key point is , that the instantaneous speed of the moving object relatively to $f' ( t ) $ is zero , that is , you have $dx ' =0$ , so you may write : $ds^2 = c^2 dt^2 - \vec {dx}^2 = ds' ( t ) ^2=c^2dt'^2$ the time $t'$ defined in this manner is called the proper time of the moving object , and is noted $\tau$ ( $c^2 dt^2 - \vec {dx}^2 = c^2d\tau^2$ ) . it represents the time elapsed for a clock moving with the moving object . with your problem , note that if you take the parametrization : $$\left\{ \begin{array}{l l} ct= b ~sh ( \frac{c \tau}{b} ) \tag{1}\\ x= b ~ch ( \frac{c \tau}{b} ) \end{array}\right . $$ you will find , with a little algebra , that , first , $x ( t ) = \sqrt{ ( b^2 ) + ( ( ct ) ^2 ) }$ , and secondly , that $c^2 dt^2 - \vec {dx}^2 = c^2d\tau^2$ ( we suppose here $dy=dz=0$ ) . so , $\tau$ is the proper time , that you are looking for , and you may find a expression of $\tau$ relatively to $t$ , by inversing the first equation of the parametrization $ ( 1 ) $ : $$ \tau = \frac{b}{c}~argsh ( \frac{c t}{b} ) \tag{2}$$
that is the leidenfrost effect . if the surface is hot enough , a layer of vapor exists between the hot surface and the droplet , insulating the droplet from the full heat . the droplet levitates above the hot surface .
i think there would not be a hard collision of the object with the ground . it would pass through a column of air of comparable mass , so i suspect the energy of the object would be consumed before it hits the ground . this would be a sort of massive cosmic ray event . the interaction energy of atoms would be comparable to tevatron energy ( 1/10 of that or so ) and what would reach the ground would be a huge pulse of secondary particles that generate an enormous thermal-mechanical shock wave . at 10^3kg the energy would be $e~\simeq~\gamma mc^2$ or about $10^{19}j$ of energy . one ton of explosives is 4.18 mega joules of energy , so the energy released would be about the equivalent of $2\times 10^7$ megatons of explosives . clearly you would not want to be anywhere near this . however , i suspect the damage done to the earth’s surface would not be due to the body actually impacting the surface , but from this huge shock front . so one would have to search out what is known about that sort of mechanics .
this is a pretty vague question , but i take it that you are groping for some " physical significance " . the clearest one is that the logarithm is the inverse of the exponential function $x\mapsto e^x$ which itself arises whenever the rate of quantity 's variation is equal to or proportional to that quantity , a fairly common statement describing physical processes . for example : rates of chemical reactions , radio active decays , attenuation of light or other em radiation through mediums all follow such laws . given this " physical definition " it follows then that the inverse function is simply that given by $x\mapsto \int_1^x \frac{\mathrm{d}z}{z}$ and then this definition is broadened into the punctured complex plane $\mathbb{c}\sim \{0\}$ by analytic continuation . moreover the functions $\exp$ and $\log$ defined in this way have particularly simple taylor series ( the former is universally convergent , the latter convergent in an open unit radius circle about $z=1$ ) that make their definitions relatively easy to broaden to objects other than numbers such as matrices , operators and so forth . the idea of a rate of a quantity 's variation being proportional to that quantity is further generalized in operator equations and , in particular , in the theory of lie groups , where $\exp$ and its inverse $\log$ play central roles in mapping neighbourhoods of the group 's identity to and from the " lie algebra " , i.e. the space of the linear transformations that play the role of generalized " rates of change " - these can now be complex numbers , quaternions or in general square matrices ( for the lie algebra they can always be thought of as square matrices - ado 's theorem - but this is not always so for the lie group ) . again , it is the natural base $e$ logarithm that falls from the definitions by dint of its taylor expansion around the identity . the theory of lie groups , with its fundamental reliance on $\exp$ and $\log$ , plays many important roles in physics and the sciences in general . in an even more generalized setting , the schrödinger equation is also a generalized " rate of change proportional to the quantity " equation , as are the descriptions of flows and the exponential map defining geodesics in differential geometry . lastly , since you ask about thermodynamics and the formula graven on boltzmann 's headstone , the logarithm is the grounding of the natural encoding of the idea that numbers of possibilities ( volumes of phase spaces ) multiply , whereas intuitively the corresponding " entropies " , as extensive protperties of thermodynamic systems should add . whilst it should be clear that the logarithm 's base does not matter for this definition ( indeed information theorists choose base 2 logarithms to write informational entropies in bi nary digi ts or bits ) , one could argue that the natural base $e$ logarithm that is the " prototypical " isomorphism ( which is what boltzmann 's intuitive idea is all about ) between the group of reals and addition and the group of strictly positive reals and multiplication that arises from the lie theoretical idea of mapping the lie group $ ( \mathbb{r}^+\sim\{0\} , \ , \times ) $ onto its lie algebra $ ( \mathbb{r} , \ , + ) $ what is the probability of all this happening ? it is precisely equal to unity : for the above ideas are how we define the natural logarithm ( i.e. . as the ones defined above as opposed to logarithms with another base or even indeed other functions altogether ) .
is it possible to focus the sun in such way ? yes , as others have pointed out , all of the ideas in your sketch are already used in existing designs - perhaps excepting the shutter ( which actually performs no useful purpose so far as i can see ) . as chris white commented - " this exact design ( with the shutter permanently open ) is a schmidt-cassegrain telescope , probably the most popular high-end consumer scope these days . " is it possible to increase the power of the beam by making it bounce between the mirrors no , focusing a beam of light , or reflecting it , does not increase the power . the amount of power is the amount of light energy entering the system per second . that is limited by the diameter of the entry pupil . energy is conserved .
in an oversimplified picture , imagine that a cloud of neutrons collapses gravitationally into a neutron star . you have lost a lot of gravitational potential energy , so by conservation of energy , the neutron star is now very hot . it radiates energy away and cools down . because there has been a net loss of energy by radiation , the total mass-energy of the system has been decreased . what is a little confusing about the presentation is that the author is using gravitational redshift as a measure of gravitational potential . they are equal , but to me it would be more conceptually transparent just to refer to gravitational potential .
it seems that it is an electron microscope image . it does not use photons , but electrons . so the image has been coloured digitaly .
there is a branch in physics which studies fire , it is called combustion . in a nutshell , there is a state when matter is burning which is basically a plasma . in plasma , the matter ionizes , i.e. bonds are broken , electrons are all flying around etc . the system as a whole is still neutral , but it is composed of ions and other charged particles which normally are bound in molecules . that is not the only thing that happens when materials burn . there are all kinds of intermediate states , when it is not quite plasma yet . to those who can read russian or love google translate , here 's the syllabus of the phd comprehensive exam on chemical physics of combustion and explosion , it has the references to textbooks and other literature at the end .
i ) op essentially asked ( v1 ) : if two lagrangian densities ${\cal l}$ and $\tilde{\cal l}$ have the same eqs . of motions , must they necessarily differ by a total divergence ? answer : no , one e.g. can always multiply a lagrangian density ${\cal l}$ with a constant factor $\tilde{\cal l}=\lambda {\cal l}$ different from one $\lambda\neq 1$ without altering the el equations , but the difference $$\tilde{\cal l}-{\cal l}= ( \lambda-1 ) {\cal l}$$ is not a total divergence if ${\cal l}$ is not . ii ) op essentially asked ( v4 ) : if el equations are trivially satisfied for all field configurations , is the lagrangian density ${\cal l}$ necessarily a total divergence ? answer : yes , modulo topological obstructions in field configuration space . this follows from an algebraic poincare lemma of the so-called bi-variational complex , see e.g. ref . 1 . we should mention that an elementary follow-your-nose-type proof exists for lagrangians of the form $l ( q^i , \dot{q}^j , t ) $ without higher-order derivatives , see e.g. ref . 2 . we stress that the proof-technique of ref . 2 does not work in the presence of higher-order derivatives or in the case of field theory . references : g . barnich , f . brandt and m . henneaux , local brst cohomology in gauge theories , phys . rep . 338 ( 2000 ) 439 , arxiv:hep-th/0002245 . j.v. jose and e.j. saletan , classical dynamics : a contemporary approach , 1998 ; section 2.2.2 , p . 67 .
think about it this way : when you measure some observable $o$ , you get some measured value . if you measure many identically prepared systems , you may measure different values corresponding to the operator $o$ . in the limit where you do this infinitely many times , you are able to recover exactly which values are possible , and with which probability . basic quantum theory now tells you that the system ( before you measured anything ) was in a superposition of eigenstates corresponding to the ( eigen ) values for $o$ which you ended up measuring . this means that the system is in the state $$|\psi\rangle =\sum_{i=1}^\infty c_i |o_i\rangle$$ i.e. it can be written as a weighted sum of eigenkets $|o_i\rangle$ of $o$ . this basically just expresses that one can always measure the observable $o$ , and get some value . in some cases , the sum should be replaced by an integral , or it can be a sum over finitely many eigenstates , but the idea is the same .
as was stated in the comments , your question can not be answered precisely . here 's my reactions , i hope it could help you : from a biological ( evolution ) point of view , a creature can be considered as " good " or " bad " only in a given environment . it means that there can not be a " best creature " , because you will always find environments in which the " best " becomes " weak " . from a physical point of view , you also need to precise the task that you want to evaluate . strength and agility is very vague . in my opinion , the human body can be outclassed by other species in any precise physical task . so maybe your question was more about the impression that the human body is the best design when you consider all the activities of human beings ? or a selection of these activities , like in olympic games ? in this case , it is not a surprise because : that is how evolution works ( the design was selected based on the activities necessary to survive , so we are not bad for these activities in our environment ) and because human beings prefer doing things that their body enables them to do ( that seems stupid i know , but would you add ' flying ' or ' digging ' without tools in the olympic games ? )
as you described , we substitute $y=0$ and $x=r$ into the trajectory equation : $$0=h+r\tan{\theta}-r^2\frac{g}{2u^2}\sec^2\theta . \tag{1}$$ then , differentiating with respect to $\theta$ and setting $\frac{dr}{d\theta}=0$: $$0=r_{max}\sec^2\theta-r_{max}^2\frac{g}{2u^2}2\sec^2\theta\tan\theta , $$ which simplifies to $$r_{max}=\frac{u^2}{g}\cot\theta . \tag{2}$$ solving $ ( 1 ) $ and $ ( 2 ) $ will yield the desired expressions for $\theta$ and $r_{max}$ .
no physical experiment can disprove the existance of god . let 's get that out of the way so we can concentrate on the interesting stuff . if you consider some area of intergalactic space far from anything , then this is a pretty good definition of zero energy because there is nothing there . quantum mechanics complicates this a bit , but for now lets ignore that and just take our vacuum as zero energy . if we have some test mass , e.g. a baseball , and let it fall towards a planet then as it falls it picks up speed and therefore it has kinetic energy . but we believe in the conservation of energy , and we started with zero energy . if our baseball acquires ( positive ) kinetic energy as it falls into a gravity well then there must be an equal negative energy that balances it out , so the total energy stays at zero . this is why we say the energy of the gravity well is negative . now go back to our patch of vacuum . suppose we want to create a baseball from nothing . this costs energy because even a stationary baseball has energy $e = mc^2$ ( from einstein 's famous formula where $m$ is the mass of the baseball ) . but suppose at the same time we create the baseball we create a gravity well with a matching negative energy $-mc^2$ . that means the total energy is still zero so we have created something from nothing , but without violating any physical laws . creating a baseball from nothing may seem an unreasonable thing to do , but quantum mechanics allows this sort of thing . well , do not take my example too literally since you have probably noticed that baseballs do not pop into being every day . the point is that when you start thinking about the creation of the whole universe you can make the sums work . the positive energy of the mass in universe can be balanced out by the negative gravitational energy that all that mass creates . but i think a health warning is in order here . all these ideas are speculative since we have no firm theory to describe how the universe started , just lots of interesting ideas .
briefly : because the moon 's orbit " wobbles " up and down , so it is not always in the plane of the earth 's orbit around the sun . there is a 2d plane you can form from the ellipse of the earth 's orbit and the sun . this plane is known as the ecliptic . the moon 's orbit is not exactly in the ecliptic at all times ; see this ( slightly overcomplicated ) picture from wikipedia : so the moon has got its own orbital plane , separate from the ecliptic . this orbital plane " wobbles " around - there are two points of the lunar orbital plane which intercept the ecliptic , known as the " nodes , " and these nodes rotate around the earth periodically . the moon will only pass right in front of the sun and cause an eclipse when one of the two nodes is along the line of sight to the sun and right in the ecliptic plane ( hence the name " ecliptic" ) .
a simple model that explains the frequency dependency of the resistivity of metals reasonably well is the drude model ( http://en.wikipedia.org/wiki/drude_model ) . there we have frequency dependency because the electrons in a plasma are not moving arbitrarily fast , which is consistent with xurtio 's explanation . the cutoff frequencies are usually in the optical domain . for dielectrics similar models exist , which are often a sum of lorentzian resonances . these have their origin in resonant absorption which is a quantum physical effect . the imaginary part of the permittivity is related to the conductivity . this can be seen as follows : amperes law is $\nabla \times \mathbf{h} = \mathbf{j} +i \omega \epsilon_r \epsilon_0 \mathbf e$ and insert ohms law in differential form $\mathbf{j} = \sigma \mathbf{e}$ then you get $\nabla \times \mathbf{h} = i \omega ( \epsilon_r \epsilon_0 -i \sigma/\omega ) \mathbf e$ which is just of the same form of as original form of amperes law but without the explicit $\mathbf{j}$ term . in conclusion ohms law can be integrated in free space maxwells equations ( without the source terms ) when the relative permittivity $\epsilon_r$ is taken as a complex value ( $\widetilde\epsilon_r = \epsilon_r - i \sigma/ ( \omega \epsilon_0 ) $ ) , where an imaginary part is added related to the conductivity . this essentially models the effect of moving charges under the influence of an oscillating field ( light ) . so the relation between polarization ( $\mathbf d = \widetilde{\epsilon}_r \epsilon_0 \mathbf e = \mathbf p + \epsilon_0 \mathbf e$ ) and conductivity $\sigma$ is given as $\mathbf{p} = \epsilon_0 ( \epsilon_r - i \sigma/\omega - 1 ) \mathbf e$ . since the real part of the permittivity is frequency dependent , so is the conductivity . this is because of the kramers-kronig relations which follow from a causality relation .
in theory , yes , this could be done . pretty much exactly as much gravitational energy is lost by the water coming down as is gained by the water going up , so you could then supply the water while hardly using any energy at all . ( just enough to offset the heat generated by friction in the pipes . ) one way in which it can be done in theory is simply to connect two gear pumps with a solid axel . water from the down pipe will force the axel to turn , which then drives the up pump . water can be made to flow by applying just a little bit of extra torque to the axel . however , in practice i do not think this would be done . i imagine there would be a lot of practical issues involved in passing waste water through a pump - it would at least have to be filtered first - and as energynumbers points out in a comment , the energy needed to pump water up 20 floors is pretty small in comparision to ( for example ) heating the apartments .
i finally found some prior art . this object has been introduced as the " husimi matrix " by harriman " some properties of the husimi function " harriman , john e . , the journal of chemical physics , 88 , 6399-6408 ( 1988 ) , http://dx.doi.org/10.1063/1.454477 and briefly referred to by morrison and parr " approximate density matrices and husimi functions using the maximum entropy formulation with constraints " morrison , robert c . and parr , robert g . , international journal of quantum chemistry , 39: 823–837 http://dx.doi.org/10.1002/qua.560390607 the treatment was fairly basic . from what i can tell , harriman primarily introduced the husimi matrix to highlight an analogy with density matrices ( since you can use it as the kernel of an integral operator ) . morrison and parr use it for something related to calculating a density matrix as a maximum entropy husimi function , but i do not really understand . i do not believe anyone has explored the relationship to decoherence .
there are two phenomena present diffusion , which happens due to inhomogeneity in concentration . particles " want to " go from areas of higher concentration to the lower ones . one can write this in the form of diffusion current $$j_{diff} ( x ) = - d \nabla \rho ( x ) $$ where $\rho ( x ) $ is the concentration . this expression is known as fick 's law but it is actually just the standard linear response to inhomogeneities . drift , which is the terminal velocity particles attain due to presence of some force . e.g. the drift one can observe for balls falling in viscous liquid . one can write $$j_{drift} = \rho ( x ) v ( x ) = \rho ( x ) b f ( x ) = -b \rho ( x ) \nabla u ( x ) $$ from the requirement of equilibrium we have that $j_{diff} + j_{drift} = 0$ and from boltzmann statistics we can obtain the concentration $\rho ( x ) \sim \exp ( -{u ( x ) \over k_b t} ) $ . putting it all together we get $$0 = - d \nabla \rho ( x ) - b \rho ( x ) \nabla u ( x ) = - \nabla u ( x ) \rho ( x ) ( -{d \over k_b t} + b ) $$ and we can see the required relation in the last term .
i do not know a good answer to your first question ( i would be interested in a good text for that myself ) , but i can answer the second . it is easier to explain if we temporarily imagine $\phi$ represents the concentration of some dye made up of little particles suspended in the fluid . the convective term ( aka advective term ) is transport of $\phi$ due to the fact that the fluid is moving : a single " particle " of $\phi$ will tend move around according to the velocity of the fluid around it . the diffusive term , on the other hand , represents the fact that the dye tends to spread out , regardless of the motion of the fluid , because each particle is undergoing brownian motion . so if you were moving along at the same velocity as the fluid you would see a small spot of dye tend to become more and more blurred over time . for quantities like energy and momentum the diffusion happens for a slightly different reason ( transfer of the quantity between fluid molecules when they collide ) but the principle is the same . the property is transported along with the fluid 's bulk velocity ( convective term ) but also tends to spread out and become blurred of its own accord ( diffusive term ) .
though total potential energy of the system of solid earth + oceans + moon + sun would remain approximately constant the energy of one of these can increase at the expense of the other three . thats how the tidal energy comes up . tidal friction does contribute to the reduction of the total gravitational potential energy of the entire system . it also causes reduction in the rotation speed of the earth .
if two particles are close to each other , there is more space for the rest of the particles to move . this gives rise to an effective entropic attraction between the particles because when looking at two particles for different separations while " tracing out " over the degrees of freedom of the rest of the system , the entropy of the rest is higher when the two tagged particles you are looking at are close to each other . in fact at high density , you should also observe oscillations in the g ( r ) and not a single peack . the width of the bumps in these oscillations is related to the particle size .
your question mentions ' fusing ' cracks . i think you may be asking whether there are actual examples of what is sometimes referred to as " reversible crack growth . " this is often the ideal case discussed when students are learning crack mechanics theory . the closest approximation to reversible crack growth demonstrated in the laboratory that i am aware of are experiments performed using cleavage cracks in mica . the cracks must be made under a vacuum to avoid contamination of the surfaces . when the applied stress is removed the crack can ' heal . ' it is never perfectly reversible , because of reasons that can include surface contamination and plastic deformation near the crack tip . another strain mechanism that might be approximated ( under simple conditions ) is possibly stress induced twinning . if the applied stresses are removed and the twinned crystal is heated , the twins might reverse . ' removing ' a dislocation from a crystal lattice means moving the dislocation to a point where it is ' annihilated ' by reaction with other defects or a surface . a dislocation may move to a free surface , and out of the lattice . a dislocation loop may shrink down to no radius . two dislocation segments of exactly opposite burgers vector may move towards each other cancel out . dislocation motion means plastic strain . it would be rare that all the strains would cancel out exactly , so in most real cases annealing probably result in at least a little residual strain .
imho , the current use of the word helicities happens only when one is looking at some representation of $su ( 2 ) $ . 1 ) now , a first point of view is to try to go back to representations of $ \otimes^n su ( 2 ) $ , when working with representations of $so ( d-2 ) $ . in the best case , you will have different kind of " helicities " . suppose we work with $d=6$ , so spin-$1$ massless particles are in the fundamental representation of $so ( 4 ) $ , which i write $4$ . in term of $su ( 2 ) \otimes su ( 2 ) $ representations , this gives : $4 \to ( 2,2 ) $ [ here i write the number of states in the representations ] so , multiplying photon representations gives $4 \times 4 \to ( 2,2 ) \times ( 2,2 ) = ( 3,3 ) + ( 1,3 ) + ( 3,1 ) + ( 1,1 ) $ $ ( 3,3 ) $ is the graviton traceless symmetric representation that we are looking for , with $9 = \dfrac{6 ( 6-3 ) }{2}$ so here photons have " helicities " $ ( \pm 1 , \pm 1 ) $ , while gravitons have " helicities " $ ( 0 \pm 1 , 0 \pm 1 ) $ gravitons states could be written from photons states , for instance : $ ( +1 , +1 ) = ( +1 , +1 ) ( +1 , +1 ) $ $ ( -1 , -1 ) = ( -1 , -1 ) ( -1 , -1 ) $ $ ( +1 , -1 ) = ( +1 , -1 ) ( +1 , -1 ) $ $ ( -1 , +1 ) = ( -1 , +1 ) ( -1 , +1 ) $ $ ( +1,0 ) = \frac{1}{\sqrt 2} [ ( +1 , +1 ) ( +1 , -1 ) + ( +1 , -1 ) ( +1 , +1 ) ] $ $ ( 0,0 ) = \frac{1}{ 2} [ ( +1 , -1 ) ( +1 , -1 ) + ( +1 , -1 ) ( -1 , +1 ) + ( -1 , +1 ) ( +1 , -1 ) + ( -1 , +1 ) ( -1 , +1 ) ] $ and so on . 2 ) a second point of view is to work directly with the representations of $so ( d-2 ) $ let us use this ( french ) lie group on-line tool ( université de poitiers ) . choose $d3 ( so ( 6 ) ) $ , " tensor product decomposition " ( then " proceed" ) . let 's type $ ( 1,0,0 ) \times ( 1,0,0 ) $ , ( then " start" ) , and you get $ ( 2,0,0 ) + ( 0,1,1 ) + ( 0,0,0 ) $ . here we are working with dynkin indices . so $ ( 2,0,0 ) $ is the graviton symmetric traceless representation , and it is also the highest weight state of the representation . you may get the other states of the representation by substracting with the simple roots you may directly from the cartan matrix of $d3= so ( 6 ) = su ( 4 ) $ ( they are the lines of the cartan matrix ) until you get no positive number . here the simple roots are $ ( 2 , -1,0 ) , ( -1,2 , -1 ) , ( 0 , -1,2 ) $ . so , for instance , substracting the first root , you get the state $ ( 2,0,0 ) - ( 2 , -1,0 ) = ( 0,1,0 ) $ , and so on . so each state for the gravitons ( or the photons ) could be represented by $3$ integers , so it is an alternative way to classify the states into a given representation .
this really does not belong here , but i will answer anyway , because i do not like to see people being ripped off by the kind of cheap toy telescope that you seem to be referring to ( at least that is what showed up when i entered your description into google ) . the sad reality is , that you can not see much of interest beyond the moon , and even that is , imho more enjoyable with cheap binoculars . i bought one of these toys a month ago for \$10 at a thrift store to get a first hand experience of the " quality " . to be honest with you , i was disappointed even for \$10 . the only useful eyepiece that came with the " instrument " was the 20mm . given the focal length of the primary mirror of approx . 600-800mm , the effective magnifications is approx . 30-40 . that is it . even the 12mm eyepiece was , in my opinion useless and i could not properly focus the 9mm and shorter ones , at all . i never tried the 2x focal length extender . . . it would be a waste of time . so forget about 625 times magnification . . . that is more than beyond the useful range of even the best instrument with 114mm aperture . anything above 100 times magnification is complete nonsense , not to mention that you could not find what you are looking for without a precision mount to begin with . now , since i also own other optical instruments ( for professionals ) , i matched a low quality 25mm wide angle microscope eyepiece to the instrument ( that is approx . a $25 investment , if you do not have one of these laying around ) . this made the telescope both more enjoyable and more useful . the moon appears in reasonable clarity now and i actually like pulling my frankenstein out of the garage to look at it for a couple of minutes occasionally . as for a real telescope eyepiece , they will not fit , because the plastic eyepieces on this toy are approx . 1" in diameter , while the smaller amateur eyepieces are 1.25" , if i am not mistaken . at the very least you would need an adapter . mars is not resolved , except as a tiny disk , and the coma is horrible , even after adjusting the primary mirror , which was screwed in so tightly , that one could see it bending visibly . i loosened the mirror mount but did not take the time to adjust the optical path perfectly , since the primary mirror has visible coating problems and scratches around the rim , so i figure that it is nowhere near the right shape , anyway . saturn shows the slightest hint of a girdle , again barely distinguishable from the instrument 's optical errors . i did not have a chance to look at venus , yet , but i do not expect to see much more than a hint of its phase , either . jupiter 's moons should be visible , but they can be seen with binoculars and it is easy enough to take a good photo of them with a 150mm or so zoom lens on a low end dslr , so you do not really need a telescope for that . the other day i looked at the andromeda galaxy . even though i know how to find it with naked eyes , it took me about half an hour to get it into the view of the instrument , since the finder scope is too dim to see it in there . you are basically tapping around blindly across the sky . . . until you are lucky to hit what you are looking for , assuming that it has distinguishable features . well , andromeda looks like a mere wisp , even in my wide angle frankenstein eyepiece ( i would not expect to see it , at all , with the eyepieces that came with the scope ) . all one can see is the galactic core , there is no hint of the spiral galaxy structure . for comparison , i can clearly see the spiral galaxy in digital camera images using a simple tripod , a 50mm lens and 10-30 seconds of exposure time . a very motivated amateur astronomer might be willing to look for things that are less luminous than andromeda , but i think that andromeda will be the one and only deep space object i will ever see in my toy , it is just too hard . so where does that leave you ? if you listen to me and everybody else who ever wrote a basically indistinguishable critique of " toy catalog telescopes " , it will leave you about \$79-99 richer . if you want a telescope badly , the cheapest ones worth any money are probably 8" dobsonian " light buckets " for approx . \$400 . they are just what the name implies : an 8" primary in the lowest grade of mechanical mount that makes them useable for magnifications in the range of 30-100 , or so . they are bright , have a wide field of view , and you can learn how to navigate the sky with them . if you want to trade the size of the primary against the mount , you can go with a smaller aperture and a higher quality mount . meade sella " better " versions of the aforementioned toy telescopes for around \$200 , but i would not spend that money , either , but save , at least \$400 for the next level up from there . even better , if you can find an amateur astronomy club , you may be able to look trough instruments in the \$10k price range that are owned by your new club friends . that will give you a much better idea of what can and can not be done with visual observing , even for a lot of money . if you can stand the disappointment , that andromeda does not look nearly as magnificent as trough the focal plane array of the hubble space telescope , then amateur astronomy may be for you ! good luck !
you have 2 kinds of transformation to help you to find the ope $ ( 2.4.14 ) $ , dilatations $ ( 2.4.13 ) $ and translations . for each of these transformations , we have to identify infinitesimal transformations quantities $v ( z ) $ defined by : $z ' = z+\epsilon v ( z ) $ and the infinitesimal modification of the fields $\delta a ( z , \bar z ) $ . the current being given by $j ( z ) = i v ( z ) t ( z ) $ $ ( 2.4.5 ) $ , we are going to use ward identities $ ( 2.3.11 ) $ : $$res_{z \rightarrow z_0} ( j ( z ) a ( z_0 , \bar z_0 ) ) + \bar res_{\bar z \rightarrow \bar z_0} ( \bar j ( z ) a ( z_0 , \bar z_0 ) ) = \frac{1}{i \epsilon} \delta a ( z_0 , \bar z_0 ) $$ we suppose an ope of the form : $t ( z ) a ( 0,0 ) \sim \cdots + \frac{a}{z^2}a ( 0,0 ) + \frac{b}{z} \partial a ( 0,0 ) +\cdots$ , where $a$ and $b$ are to be determined . dilatations the infinitesimal transformation corresponding to $z'= \zeta z$ , is , using $\zeta = 1 + \epsilon$ , $z ' = z +\epsilon z$ , so here $v ( z ) = z$ ; and $\bar v ( z ) = \bar z$ the transformation of fields is $a ( z ' , \bar z' ) = \zeta^{-h}\bar \zeta^{- \tilde h} a ( z , \bar z ) $ , this corresponds to a infinitesimal transformation $\delta a ( z , \bar z ) = - \epsilon h~ a ( z , \bar z ) - \bar \epsilon \tilde h~ a ( z , \bar z ) $ so , appying ward identity , and only keeping the holomorphic part , we see that : $$res_{z \rightarrow 0} ( i ~z ~t ( z ) ~a ( 0,0 ) ) = \frac{1}{i ~\epsilon} ( - \epsilon h a ( 0,0 ) ) $$ this means that $t ( z ) ~a ( 0,0 ) $ has a component $\frac {h}{z^2}a ( 0,0 ) $ , in order to have a pole with the correct residue . translations here $v ( z ) = v$ = constant ; and $\delta a = - \epsilon ( v\partial a + \bar v \bar \partial a ) $ . so , applying the ward identity , keeping the holomorphic part , we get : $$res_{z \rightarrow 0} ( i ~v ~t ( z ) ~a ( 0,0 ) ) = \frac{1}{i ~\epsilon} ( - \epsilon v \partial a ( 0,0 ) ) $$ this means that $t ( z ) ~a ( 0,0 ) $ has a component $\frac {1}{z}a ( 0,0 ) $ , in order to have a pole with the correct residue . so , finally : $$t ( z ) a ( 0,0 ) \sim \cdots + \frac{h}{z^2}a ( 0,0 ) + \frac{1}{z} \partial a ( 0,0 ) +\cdots$$ of course , an equivalent demonstration is valid for the anti-homorphic part .
first , change your thinking away from weight to pressure . pressure is weight per unit area . the easiest way to do that is to assume a certain cross-section throughout , say 100 square centimeters , or a square tube 10 centimeters on a side . that is a nice size , because 10 centimeters of that tube enclose 1 liter , or 1 kilogram of water . so if you have a vertical tube that size , and you fill it with 10 cm of water , the pressure at the bottom is the weight of the water divided by the area of the bottom - 1 000 grams divided by 100 square centimeters is 10 grams resting on each square centimeter . if you fill it to twice that height , then the pressure at the bottom is twice that , and so on . so , to rephrase your question , if the 50kg stone is sitting at the bottom of one tube , what pressure is it exerting on the bottom of the tube ? 50 000 grams over 100 sq cm , or 500 grams per square centimeter . so what height of water would it take on the other side to equal that ? since 10 cm gave you 10 grams/cm^2 , 500 cm ( 5 meters ) would give you 500 gm/cm^2 . did i get it right ? you check my math . then you can replace any of that excess water by a stone , as long as that stone weighs the same amount as the water you are getting rid of , and fits in the tube . now , what happens if you use a more narrow tube ? what if you use a wider one ? what if the two tubes are different sizes ?
obviously , the coil will move . when we are moving the bar magnet toward the coil a current will flow through the coil in a direction to reduce the rate of change of flux through it . now you can think the coil as a tiny magnetic dipole of dipole moment m =i a ( or a small bar magnet for your convenience ) . and this will move in the presence of other magnet if it is freely hanging . you may find this wikipedia article useful .
adding the word " modern " to the title of the question completely changes it . in modern computers you need semiconductors , and the whole theory of solid state physics ( band structures , doping , etc ) is based on a foundation of quantum mechanics - since electrons in semiconducting solids behave in a manner that is more wave-like than particle-like , with each electron occupying its own distinct state . making a semiconductor work well requires in depth understanding of these things .
comment to the question ( v2 ) : p and s is using the notation of a ' same-spacetime ' functional derivative . to illustrate this notation , let us for simplicity stay within first variations , and leave it to the reader to generalize to higher-order variations . i ) first of all , functional/variational derivatives should not be confused with partial derivatives . in practice , from an operational point of view ( if we are not worried about mathematical details about existence and boundary terms ) , all we need to know is the following rules : the formula $$\tag{a} \frac{\delta \phi^{\beta} ( y ) }{\delta\phi^{\alpha} ( x ) } ~=~\delta^{\beta}_{\alpha}~\delta^n ( x-y ) , $$ where $n$ is the spacetime dimension . appropriate generalizations of elementary rules in calculus , such as , e.g. , the chain rule , integration by parts , commutativity of derivatives , and the dirac delta distribution . for instance , by these rules 1 and 2 , we have that $$ \frac{\delta}{\delta\phi^{\beta} ( y ) } \frac{\partial}{\partial x^{\mu_1}}\ldots \frac{\partial}{\partial x^{\mu_r}}\phi^{\alpha} ( x ) ~=~ \frac{\partial}{\partial x^{\mu_1}}\ldots \frac{\partial}{\partial x^{\mu_r}} \frac{\delta}{\delta\phi^{\beta} ( y ) }\phi^{\alpha} ( x ) $$ $$\tag{b}~\stackrel{ ( a ) }{=}~\delta_{\beta}^{\alpha}~\frac{\partial}{\partial x^{\mu_1}}\ldots \frac{\partial}{\partial x^{\mu_r}}\delta^n ( x-y ) . $$ similarly , by rules 1 and 2 , we can deduce that the action $$\tag{c}s~=~\int d^nx ~{\cal l} ( x ) , \qquad {\cal l} ( x ) \equiv {\cal l} ( \phi ( x ) , \partial \phi ( x ) , \ldots , x ) , $$ has the euler-lagrange expression as its functional derivative $$ \tag{d}\frac{\delta s}{\delta\phi^{\alpha} ( x ) }~=~ \frac{\partial{\cal l} ( x ) }{\partial\phi^{\alpha} ( x ) } - d_{\mu} \left ( \frac{\partial{\cal l} ( x ) }{\partial\partial_{\mu}\phi^{\alpha} ( x ) } \right ) +\ldots . $$ the ellipsis $\ldots$ in eqs . ( c ) and ( d ) denotes possible contributions from higher-order spacetime derivatives . ii ) from formula ( a ) it becomes clear that it does not makes sense to consider the functional derivative $\frac{\delta {\cal l} ( x ) }{\delta\phi^{\alpha} ( x ) }$ wrt . the same spacetime argument $x$ , because that would lead to infinities , cf . $\delta^n ( 0 ) =\infty$ . nevertheless , it is tempting to introduce the notation of a ' same-spacetime ' functional derivative $$\tag{e}\frac{\delta {\cal l} ( x ) }{\delta\phi^{\alpha} ( x ) }~:=~ \frac{\partial{\cal l} ( x ) }{\partial\phi^{\alpha} ( x ) } - d_{\mu} \left ( \frac{\partial{\cal l} ( x ) }{\partial\partial_{\mu}\phi^{\alpha} ( x ) } \right ) +\ldots . $$ we stress that eq . ( e ) is only a notational definition . it becomes meaningless if we try to interpret the lhs . of eq . ( e ) using the above rules 1 and 2 . iii ) similarly , p and s talk about second-order ' same-spacetime ' functional derivative $$\tag{f}\frac{\delta^2 {\cal l} ( x ) }{\delta\phi^{\alpha} ( x ) \delta\phi^{\beta} ( x ) } . $$ we recommend to first work out the ordinary second-order functional derivative $$\tag{g}\frac{\delta^2 s}{\delta\phi^{\alpha} ( x ) \delta\phi^{\beta} ( y ) }$$ using rules 1 and 2 . then it should be fairly straightforward to translate ( g ) into the ' same-spacetime ' functional derivative language ( f ) , if needed . [ in particluar , eq . ( g ) contains a $\delta^n ( x-y ) $ while eq . ( f ) does not . ] iv ) finally we should mention that in field theory one often suppresses the spacetime indices $x , y , \ldots$ , by using dewitt 's condensed notation .
when one says that an elementary particle is point-like , one is referring to the fact that theoretically , there is no limit to how small a region a detector can localize a particle to . for the sake of argument , let 's imagine two wrong things ( a ) that such an ideal detector is possible and ( b ) complications arising from planck scale physics do not change anything conceptually . even if you allow for that , your worry that information is being stored in a zero volume region is still unfounded . it would be a legitimate worry in pre-relativistic-qft physics . but we know particles are not pellets that move around carrying information . they can disappear and be spontaneously created out of the vacuum . what this is hinting at ( though some might prefer a different picture ) is that particles are not fundamental - fields are . the quantum fields for various particles are defined everywhere in space . once you specify what kind of structure the quantum field is ( a scalar , vector , spinor , etc . ) and what its other properties are ( say , the symmetry group under which it has local gauge invariance ) , you have specified what spin , charge , mass etc . its particle excitations will carry . since the field is defined everywhere in space , there is plenty of room for all that information . so in a certain sense , the information that a detector detects is encoded everywhere in space ( because the field is everywhere ) - and the specific structure of the detector just picks out the right information you asked for . finally , two point-particles ( say an electron and a muon ) have different properties because they are excitations of two different fields defined everywhere in space - and the detector you build specifically for the electron will pick out the " signal " from the electron .
any photon ( pure ) state may be described by a q-bit formalism : $$|photon\rangle = \alpha |0\rangle + \beta|1\rangle$$ where $|0\rangle$ and $|1\rangle$ represent the two possible polarizations of the photon . so , any photon " is " a q-bit . you do not have to " create " q-bits . just prepare photons is some state . an entangled state of $2$ photons may be described by a $2$-photons ( $2$-qbit ) state , for instance : $$|entangled\rangle = \frac{1}{\sqrt{2}} ( |0\rangle|0\rangle + |1\rangle|1\rangle ) $$
a caesium clock generates a microwave signal that it tunes to match the absorption peak in the caesium spectrum . a counter counts every oscillation of the generated microwave , and every 9,192,631,770 counts = 1 second . that is how the clock counts the seconds and keeps time . but if the peak in the caesium spectrum is broad it is difficult to tune the microwave generator to exactly the peak maximum . that means there is a potential error in the microwave frequency and hence in the measured time . this will make the clock run slow or fast . so the narrowness of the peak is very important to keeping accurate time . when i last looked the limitation on the peak width was doppler broadening . this happens because some of the caesium atoms are moving towards the microwave generator and some caesium atoms are moving away . this motion shifts the position of the absorption peak , and the end result is that the peak broadens . you can design the clock to reduce the speed the caesium atoms move , and in principle you can eliminate doppler broadening . however the uncertainty principle sets a lower bound for the peak width . for an electronic transition like the one in caesium the uncertainty principle tells us that : $$ \delta e \delta \tau &gt ; = \frac{\hbar}{2} $$ where $\delta \tau$ is the lifetime of the excited state i.e. the average time taken for the excited state to emit a photon and relax to the lower energy state . this $\delta \tau$ is a characteristic of the caesium atom and is effectively a constant beyond our control . this matters because the energy $e$ is related to frequency by $e = h\nu$ , so we end up with an uncertainty in the frequency given by : $$ h\delta \nu &gt ; = \frac{\hbar}{2} \frac{1}{\delta \tau} $$ this tells us we will never be able to tune the microwave frequency to better than $\delta \nu$ , so there is a fundamental source of error that we cannot eliminate .
however , i had difficulty understanding that answer and would like to understand how to do it this way . that is to say , i would really like to know what property or identity that i am missing before i can use use the bianchi identities to show that it is manifestly zero . the other proof uses the first bianchi identity . that is where the starting assumption $r^a{}_{bcd}\xi^d = \xi^a{}_{ ; bc}$ comes from . if you want to use the second bianchi identity , it is $$ ( \nabla_\xi r ) ( x , y ) + ( \nabla_x r ) ( y , \xi ) + ( \nabla_y r ) ( \xi , x ) = 0\text{ , }$$ and therefore applying it and the leibniz rule produces : $$\begin{align} \underbrace{\nabla_\xi [ r ( x , y ) ] +\nabla_x [ r ( y , \xi ) ] +\nabla_y [ r ( \xi , x ) ] }_\mathrm{foo} = \underbrace{r ( \mathcal{l}_\xi x , y ) + r ( \mathcal{l}_xy , \xi ) + r ( \mathcal{l}_y\xi , x ) }_\mathrm{bar}\text{ , } \end{align}$$ where it was assumed that the torsion vanishes , so that $\mathcal{l}_ab = \nabla_ab-\nabla_ba$ . additionally , $$\begin{eqnarray*} ( \mathcal{l}_\xi r ) ( x , y ) and = and \mathcal{l}_\xi [ r ( x , y ) ] - r ( \mathcal{l}_\xi x , y ) - r ( x , \mathcal{l}_\xi y ) \\ and = and \mathcal{l}_\xi [ r ( x , y ) ] - r ( \mathcal{l}_\xi x , y ) - r ( \mathcal{l}_y\xi , x ) \\ and = and \underbrace{\mathcal{l}_\xi [ r ( x , y ) ] - [ \mathrm{foo} ] + r ( \mathcal{l}_xy , \xi ) }_\mathrm{qux}\text{ . } \end{eqnarray*}$$ so the objective is to show that the right-hand side , $\mathrm{qux}$ , is identically zero whenever $\xi$ is a killing vector field . let 's write $s^a{}_b = [ r ( x , y ) ] ^a{}_b = r^a{}_{bcd}x^cy^d$ , and just crank it out : $$\begin{eqnarray*} \mathcal{l}_\xi s^a{}_b and = and \nabla_\xi s^a{}_b - s^e{}_b\xi^a{}_{ ; e} + s^a{}_e\xi^e{}_{ ; b}\\ and = and \nabla_\xi s^a{}_b + x^cy^d ( r^a{}_{ecd}\xi^e{}_{ ; b} - r^e{}_{bcd}\xi^a{}_{ ; e} ) \\ and = and \nabla_\xi s^a{}_b + x^cy^d ( \nabla_c\nabla_d-\nabla_d\nabla_c ) \xi^a{}_{ ; b}\text{ , } \end{eqnarray*}$$ where the last step is actually valid for arbitrary $z^a{}_b$ , not just $\xi^a{}_{ ; b}$ . the first term of this cancels with the first term of $\mathrm{foo}$ . so far we have not used the fact that $\xi$ is a killing vector field . let 's do so now by considering the other two terms of $\mathrm{foo}$: $$\nabla_x [ r ( y , \xi ) ] ^a{}_b - \nabla_y [ r ( x , \xi ) ] ^a{}_b = \nabla_x\nabla_y\xi^a{}_{ ; b} - \nabla_y\nabla_x\xi^a{}_{ ; b}\text{ , }$$ where the starting identity $r^a{}_{bcd}\xi^d = \xi^a{}_{ ; bc}$ was used . the same identity also gives : $$r ( \mathcal{l}_xy , \xi ) ^a{}_{b} = \nabla_{ [ x , y ] }\xi^a{}_{ ; b}\text{ . }$$ therefore , we have shown that for any vector fields $x , y$ , $$\begin{eqnarray*} x^cy^d ( \mathcal{l}_\xi r^a{}_{bcd} ) and = and \left [ x^cy^d ( \nabla_c\nabla_d-\nabla_d\nabla_c ) - ( \nabla_x\nabla_y-\nabla_y\nabla_x ) + \nabla_{ [ x , y ] }\right ] \xi^a{}_{ ; b}\\ and = and 0\text{ . }\end{eqnarray*}$$ ( if you have trouble with the last step , check christoph 's answer to the other question and modify appropriately . ) thus $\mathcal{l}_\xi r^a{}_{bcd} = 0$ , qed .
so this was the answer given in the key :
the answer provided by ja72 pointed me to the correct direction but did not answer directly my questions . the answers are : the author states rr is the rotation velocity matrix between earth fixed reference frame and body fixed reference frame . i assume the " between " means a rotation from body coordinates to earth coordinates . rt is the translation velocity matrix between earth fixed reference frame and body fixed reference frame . i also assume the " between " means a rotation from body coordinates to earth coordinates . one is right the other not . according to equation $ ( 1 ) $ $r_t$ is a transformation matrix from body frame to earth fixed frame . $r_r$ on the other hand is a rotation matrix from body fixed frame to earth fixed frame so my assumption on $r_r$ was wrong . one question is how do i calculate the value of the angular acceleration described by the partial derivative of $\dot{\phi}$ and $\dot{\theta}$ the answer to this question is through the gradient of the vector , which in matrix from , is similar to ( but not ) the jacobian of the matrix . this can be obtained analytically taking the derivatives of $r_r$ . the other question is in which referential is ζ˙ and why did the author make a rotation and " derotation " on kt ? according to $ ( 1 ) $ the referential of $\zeta$ is earth fixed . the " derotation " is due to symbolic manipulation in $ ( 15 ) $ . regarding the state space formulation i still do not follow how it works out , but from $ ( 9 ) $ i was able to get angular and linear acceleration : $$\begin{cases} \ddot{\eta_{i}} and =\left ( -i_{t}r_{r}\right ) ^{-1}\left ( -t_{pb}+i_{t}\left ( \frac{\partial r_{r}}{\partial\phi}\dot{\phi}+\frac{\partial r_{r}}{\partial\theta}\dot{\theta}+\frac{\partial r_{r}}{\partial\psi}\dot{\psi}\right ) \dot{\eta}+k_{r\_aero}r_{r}\dot{\eta}+\left ( r_{r}\dot{\eta}\right ) \times\left ( i_{t}r_{r}\dot{\eta}\right ) \right ) \\ \ddot{v}_{i} and =\left ( -mr_{t}^{t}\right ) ^{-1}\left ( k_{t}r_{t}^{t}\dot{v}_{i}+mr_{t}^{t}g-f_{pb}\right ) \end{cases}$$ so far this formulation seems to be giving good results but i have not verified against another opinion , and i may be missing something .
the big problem with controlled fusion is that the equations governing the plasma are highly non-linear . so each time the physicist increase the size of the tokamak , new effects are discovered . so i guess that the answer is no-one really knows the correct scaling laws ! this contrasts a lot with fission reactors , where the relevant equations are essentially linear ( neutron diffusion ) . it was then possible to ' easily ' scale up enrico fermi 's first nuclear reactor chicago_pile-1 which had a power of just 0.5w in 1942 to the design of the b reactor in 1944 , which had a power of 250 megawatt . that is essentially a factor 500 millions between the first and the second nuclear reactor ! edited to add i have just found this wikipedia page about dimensionless parameters in tokamaks which is quantitative . it essentially says that constructing a 1:3 model of a power producing tokamak having the same turbulence transport processes is essentially infeasible , because it would need a too high magnetic field . then , there is a discussion i do not fully understand in order to try to guess the properties of the large machine . . . in short : the turbulence in the plasma make the use of scaling laws difficult .
let me give it a shot : if i interpret this correctly , $\mathbf{f}$ will be the operator for the full spin of the coupled system , $\mathbf{s}$ will be the operator of the electron spin ( usually , one would consider $\mathbf{j}$ , the spin containing also spin-orbit coupling , but we are on the s-shell , hence no angular momentum ) and $\mathbf{i}$ will be the nuclear spin . then it should hold that $\mathbf{f}=\mathbf{s}+\mathbf{i}$ , right ? first , let 's have a look at the hyperfine structure hamiltonian $\mathbf{h}_{hf}$ . by construction of $\mathbf{f}$ , the eigenstates of $\mathbf{h}_{hf}$ will be eigenstates of $f^2$ and $f_z$ . this is just the same as for angular momentum and electron spin ( and we construct $\mathbf{f}$ to have this property - this lets us label the eigenstates by the quantum number corresponding to $\mathbf{f}$ ) . hence the hamiltonian must be diagonal in the $|f^2 , m_f\rangle$-basis . one can also see that $f^2$ commutes with $i^2$ and $s^2$ ( and so does $f_z$ ) , since $\mathbf{f}=\mathbf{i}+\mathbf{s}$ . now we have a look at $\mathbf{h}_b$ , the interaction hamiltonian with a constant magnetic field . we can see that ( up to some prefactor ) $\mathbf{h}_b=s_z$ . hence the eigenstates of $\mathbf{h}_b$ must be eigenstates of $s_z$ and thus also of $s^2$ and , since the two operators are independent ( they relate to two different types of spins , hence the operators should better commute ) also to $i^2$ and $i_z$ , if you want . the crucial problem is that $s_z$ and $f^2$ do not commute . why ? well : $\mathbf{f}=\mathbf{i}+\mathbf{s}$ hence $f^2=s^2+i^2+2\mathbf{s}\cdot \mathbf{i}$ . now $s_z$ and $\mathbf{s}$ do not commute , because $s_z$ does not commute with e.g. $s_x$ , which is part of $\mathbf{s}$ . since $f^2$ commutes with $\mathbf{h}_{hf}$ and $s_z$ commutes with $\mathbf{h}_b$ , but not with $f^2$ , we have that $\mathbf{h}_{hf}$ does not commute with $\mathbf{h}_b$ . this means that $\mathbf{h}_b$ and $\mathbf{h}_{hf}$ cannot be diagonal in the same basis , hence you need to have off-diagonal elements . in order to see how the matrix representing $\mathbf{h}_b$ looks like in the $|f^2 , m_f\rangle$-basis , you can express the $|m_i , m_s\rangle$-basis ( in which $\mathbf{h}_b$ is diagonal ) in terms of the other basis . this is exactly what equations ( 4.21 ) do . these are obtained by ordinary addition of angular momenta . from there , you can construct the unitary transforming the basis $|m_i , m_s\rangle$ into $|f^2 , m_f\rangle$ and $\mathbf{h}_b$ will be the diagonal matrix in the basis $|m_i , m_s\rangle$ conjugated with this unitary . edit : i am not quite sure whether i understand correctly what your problem is , but let me elaborate : we want to find the hamiltonian $\mathbf{h}_b$ in the $|m_im_s\rangle$ basis . in this basis , it is diagonal , because $\mathbf{h}_b$ is essentially $s_z$ ( hence commutes with $s_z$ ) and it must also commute with $i_z$ since $s_z$ and $i_z$ are independent . if we order the basis according to $|\frac{1}{2} , \frac{1}{2}\rangle , |-\frac{1}{2} , -\frac{1}{2}\rangle , |\frac{1}{2} , -\frac{1}{2}\rangle , |-\frac{1}{2} , \frac{1}{2}\rangle$ , then , we can just read off the hamiltonian : the first and fourth vector are eigenvectors to eigenvalue $\mu b$ , the others of $-\mu b$ ( by definition of $s_z$ , since the second component in $|m_im_s\rangle=| ( si ) i_z , s_z\rangle$ tells us the eigenvalue of $s_z$ that the basis vector corresponds to ) , i.e. $$ \mathbf{h}_b=\begin{pmatrix}{} \mu b and 0 and 0 and 0 \\ 0 and -\mu b and 0 and 0 \\ 0 and 0 and -\mu b and 0 \\ 0 and 0 and 0 and \mu b\end{pmatrix}$$ now , as i said , you just have to change the basis . the matrix transforming the above basis into the new basis is given by eqn . ( 4.21a-d ) : $$u:=\begin{pmatrix}{} 1 and 0 and 0 and 0 \\ 0 and 1 and 0 and 0 \\ 0 and 0 and \frac{1}{2} and \frac{1}{2} \\ 0 and 0 and -\frac{1}{2} and \frac{1}{2} \end{pmatrix}$$ where the ordering of the $|fm_f\rangle$-basis is as for $\mathbf{h}$ in your text . now calculate $u\mathbf{h}_b u^{\dagger}$ and that should give you the part of $\mathbf{h}$ coming from $\mathbf{h}_b$ in the $|f , m_f\rangle$-basis and this will be exactly what is written in your book . edit 2: i sort of suspected this , so here is some more linear algebra for the problem . i will use dirac notion since i suspect you are more familiar with this : now suppose you have given two bases $|e_i\rangle$ and $|f_i\rangle$ and suppose they are orthonormal bases . what we want is a matrix $u$ that transforms one basis into the other ( i will call it $u$ , since it'll be a unitary - if the bases are not orthonormal , it'll only be an invertible matrix ) . so we want a matrix such that $$ |f_i\rangle:=u|e_i\rangle \qquad \forall i$$ how to construct this matrix ? well , given an equation for $|f_i\rangle$ in terms of the $|e_i\rangle$ will give you the i-th row of the matrix . you can also see the matrix elements in dirac notation : $$ \langle e_j|u|e_i\rangle=\langle e_j|f_i\rangle $$ in your case , $|e_i\rangle=|m_im_s\rangle$ and $|f_i\rangle=|f^2 , m_f\rangle$ . hence equation ( 4.21a ) will give you the first row of the matrix ( the ordering of the basis vectors $|m_im_s\rangle$ as i proposed above ) , ( 4.21c ) the second ( notice the basis ordering in the matrix $\mathbf{h}$ ! ) ( 4.21b ) the second and ( 4.21d ) the last row of the matrix . using the equation for the matrix elements above , you should be able to check that with not too much trouble . you can also easily check that $u$ is indeed a unitary ( i.e. . $uu^{\dagger}=u^{\dagger}u=\mathbb{1}$ . then we can calculate the matrix elements : $$ \langle e_i |\mathbf{h}|e_j\rangle=\langle e_i|u^{\dagger}u\mathbf{h}u^{\dagger}u|e_j\rangle=\langle f_i| u\mathbf{h}u^{\dagger}|f_j\rangle $$ , which tells you how the matrix looks like in the other basis .
it actually gets a bit complicated , since several effects are involved : evaporating water does require heat , which comes primarily from the hot stones . so throwing water on the stones does cool them down . ( this is where the claim one occasionally hears , that " throwing water on the stones makes the sauna colder " , comes from . technically it is true , if one considers the total heat content of the sauna as a whole . but since most of that heat is in the stones , and since you do not sit on the stones , that is pretty much completely irrelevant to how hot the part of the sauna that you do sit in gets , or feels . ) on the other hand , throwing water on the stones also significantly increases the heat transfer rate from the stones to the air : the evaporation produces a lot of hot steam , which will rise and mix with the ambient air in the sauna . so it is possible for the air temperature in the sauna to increase , even as the stones are cooled down . also , the introduction of steam obviously increases the humidity of the air , which will increase the rate of water precipitation on skin , and/or decrease the rate of sweat evaporation . ( the relative importance of these two effects will depend on the baseline humidity of the air , which can vary quite a lot . my gut feeling , based on experience , is that in all but the driest of saunas condensation probably dominates , simply because human skin is so much cooler than the air . ) in either case , the effect will be to transfer more heat to the skin , and thus to make the air feel hotter . finally , as the hot steam rises off the stones , it will push hot air around the sauna in front of it . while this increase in air movement is slight and transient , it probably does have a noticeable effect : as the hot air flows past the people in the sauna , it will act to disperse the layer of cooler air that forms over the skin , and thus increases heat transfer to the skin . ( if you do not believe me , try blowing some air over your skin in a sufficiently hot sauna . it burns . ) the upshot is that throwing water on the stones increases heat transfer , both from the stones to the air and from the air to your skin . as long as the stones stay hot enough to supply that heat , the net effect will be that you feel hotter . however , if you throw too much water on the stones , it is possible to " kill the stones " by cooling them close to or even below the boiling point of water . at that point , throwing more water is useless , and all you can do is add more firewood or turn up the thermostat and wait for the stones to heat up again . or , if you manage to do this in a smoke sauna , go wash yourself and get dressed up because the sauna is over for the night .
there is no unambiguous correct answer to this question because it is not well posed in terms of logical positivism : what is the difference between the two processes ? there is no way to tell which happens if you do not muck up the intermediate state with a measurement . if you mean this in terms of some quantum field theory with given fields and interactions and asymptotic states , then you can ask how the processes appear in a feynman description . the scattering process in qed is always two-step , the absorption and emission are separate space-time points . but the emission can precede the absorption both in coordinate time and in proper time along the electron 's world line , so you should include " emitted and then absorbed " to the list of possibilities . light does not have to be resonant in order to scatter off an atom . the amount of scattering/emission-reabsorption is smaller away from resonance . a light wave is also a long coherent field , and this field can acquire a phase push from the emission-reabsorption , leading the phase-velocity to be bigger than the speed of light . the issue of " how come the phases add up coherently " is adressed by two things : there is a large scale difference between the atoms and the light wavelength . each atom scatters the light independently and randomly into a spherical waves , which add up coherently in the original direction only to alter the phase velocity by a constant amount . there is no scattering from the bulk of a perfect crystal , for long wavelengths , because there is still a discrete translation invariance which means momentum is conserved up to big jumps , and the big jumps give waves with the wrong frequency for long enough wavelengths . but there are discrete momentum additions which are allowed for a short wavelength x-ray in an atomic crystal , and if the photon momentum comes out different but at the same frequency due to the coherence in a different direction , that is called diffraction . if you want scattering in a crystal , you need to scatter off defects which have a good amount of random variation in a box the size of one wavelength . similarly , if you scatter of a fluid , you need fluctuations in density to be meaningful in one wavelength . this is easier for blue light than for red light , so transparent fluids scatter blue .
there is two important differences between air and water : air is compressible , and the densities are about a factor of ~1000 apart - 1 kg/m³ vs 1 t/m³ ! for most concerns where you use propellers , compression plays no role because the pressure diferences are very low . the densities , however play a large role . the thrust can be described as $f = \dot m * \delta v$ , with $\dot m$ beeing the mass flow - kg and /s or such - and $\delta v$ the difference in velocity a volumeelement of fluid is accelerated . so , to achieve a similiar thrust , the same propeller would have to move 1000 times more air than water by volume . hence the often larger and faster spinning propellers for planes . on the other hand , in a heavier medium each wing of the propeller is subject to stronger torque ( all else beeing equal ) : $$q = \rho v_{a}^2 d^3 f_q ( \frac{nd}{v_a} ) $$ ( source ) $\rho$ is density , $v_a$ rate of advance ( how much the propeller moves forward per revolution ) , d is diameter and n number of revolutions . without going into the math it can probably be shown that in a heavier medium the propeller will experience somewhat more torque for the same thrust - i am to lazy to try now . the propeller will be built with more robust ( and possibly heavier ) material than would be the case id it is an only air propeller . that said , i believe that a propeller for both media is entirely possible , though challenging . however , a propeller for a both media will need a drivetrain that can accomodate speed roughly a factor 1000 apart ( that is not trivial ) . one other reason why do not see a propeller for both media is that there is no vehicle that could make use of one .
i second the suggestion of @chrisgerig in the comment above about reading the wiki article . this is the relevant paragraph : if the system consists of more than one particle , the particles may be moving relative to each other in the center of momentum frame , and they will generally interact through one or more of the fundamental forces . the kinetic energy of the particles and the potential energy of the force fields increase the total energy above the sum of the particle rest masses , and contribute to the invariant mass of the system . the sum of the particle kinetic energies as calculated by an observer is smallest in the center of momentum frame ( or rest frame if the system is bound ) . what they call a particle is not an elementary particle , i.e. one which is point like and whose invariant mass is constant on all frames . once there is a system of particles , even two photons , their invariant mass is variable .
it seems that these clouds are in fact reasonably well understood . roger smith , who seems to be something of an expert on these clouds , has several papers discussing the physics of morning glory clouds , many of which are linked from his " list of publications " page and can be viewed for free . this paper seems to be particularly good . i am not a meteorologist , either , but from what i have read they appear to be solitary waves ( a . k.a. , solitons ) which are travelling in a sort of atmospheric waveguide . i am sorry i can not give you a better answer than that ; please do look at the linked references . cheers !
in 3d there are 7 lattice systems which are classes of lattices having the same point group . one of them is the class of cubic lattices . this class contains three different bravais lattices which are distinguished by their translation group .
the reason you are hearing the train farther away is more consequence of the the geometry of different spaces than anything else . it starts with an inversion layer of cold air clinging close to the ground . just as glass bends light by making light move more slowly through it , an inversion layer of cold air bends sound because sound moves more slowly through cold air ( the molecules move more slowly is why ) . so , this inversion layer behaves like the audio equivalent of a large sheet of glass covering the ground and guiding the sound away from the air above it . this is the same principle that allows a fully transparent optical fiber to capture light moving through it and transmit that light for many kilometers with very few losses . ( @hwlau already noticed the inversion in the second of his three possible answers . ) on the ground side of the inversion layer , a fresh fall of snow further helps confine and preserve sound because it looks smooth to the long wavelengths of sound . so , even though snow thoroughly jumbles up the much shorter wavelengths of light , which is why it looks white , it looks very different and much more mirror-like to sound . put those two together -- diffraction on the top and reflection on the bottom -- and you have an example of two-dimensional sound dispersion . by way of contrast , sound dispersion from a train on a summer day that lacks any inversion layer and has sound-absorbing grass on the ground is an example of largely three-dimensional sound dispersion . so , why is the dimensionality of the sound dispersion important ? because sound ( or any other radiation ) disperses at a rate that is dependent on the number of dimensions of the space into which into which it is dispersing . if $l_n$ is the perceived loudness of the sound , $s$ is the distance to the sound source , and $n$ is the number of space dimensions into which sound is dispersing , the general equation for how loud the train will sound is : $l_n = 1/s^{n-1}$ notice that the lower the number of dimensions , the more slowly the sound disperses . ( i discussed this same issue from a slightly different perspective a few months ago in my answer to this question about why objects look smaller when they are farther away . ) this equation explains why optical fibers can transmit light many kilometers without loss any significant of intensity ( "loudness" ) . the dispersion space $n$ for optical fibers is $n=1$ , so $l_1 = \frac{1}{s^{1-1}} = \frac{1}{s^0} = 1$ . that is , there is no diminution of intensity . the audio equivalent would be a long tube , like the ones they used to use as intercoms in old houses ( and still use in some playgrounds ) . now for your inversion layer case , $n=2$ and : $l_2 = 1/s^{n-1} = 1/s$ but because your perception of train distance was tuned to $n=3$ space , you expected the sounds of the train to diminish at the much faster rate of : $l_3 = 1/s^{n-1} = 1/s^2$ the analysis of how much farther away the train really is turns out to be trickier than it might seem . that is because the model i just described assumes that the energy of a 3d sound source can be compressed into a mathematically precise 2d plane . the physical world just does not work that way , since the sound energy in a 3d volume cannot be forced into a true 2d plane without creating infinitely high energy densities in the plane . why ? well , pretty much for the reason that you cannot compress a 3d volume of air into an infinitely thin 2d plane without creating infinite mass densities . crossing dimensionalities is often done a bit casually in physics , but one need to be careful with it . so , in this case , instead of assuming a simple 2d plane , what you have to do is model the problem by using a " pancake " that more realistically represents the thickness of the inversion layer confining the sound . that allows sound intensities that " look " 3d in the immediate vicinity of the train , but then fade off more according to the dimensionality diffusion rules as distances increase to many times the thickness of the inversion layer . so , everything from this point is obviously guesswork about what happened in your case , but a nice ballpark height for your inversion layer might be 10 meters . approximating again , that 10 meters also becomes the " unit of equality " for distance from the train at which the sound of the train is perceived as the same in both cases . this approximation should work reasonably well for any more-or-less point source of sound coming from the train , in particular its whistle . so , call this unit of distance $s_u$ for hearing a similar loudness for the whistle $s_u = s_w = 10 m = 0.01$ km . alas , it gets messier . the sound of the train itself is anything but a point source , since you may be able to hear wheel-on-rail sounds for very long lengths , such as a kilometer for a long train . that also messes up the model and adds even more complexity in the form of orientation and sound delays . so , i am going to wrap all of that complexity up into a single huge approximation and say that for a long train , the sound of the all the train wheels on all the track sounds " about the same " for anyone within a kilometer of the train as it passes by , inversion layer or not . so , the length unit for assessing how train track noises changes over distance becomes $s_u = s_t = 1$ km . the equation now has to be altered slightly so that these " sounds the same " units $s_u$ are factored in : actual : $l_2 = s_u/s^{n-1} = s_u/s$ perceived : $l_3 = s_u/s^{n-1} = s_u/s^2$ solving for the $s$ distances in terms of loudness : actual : $s_2 = s_u/l$ perceived : $s_3 = \sqrt{s_u/l}$ the error factor $e$ for how far off your distance estimate was then is : $e = actual/perceived = s_2/s_3 = \frac{s_u/l}{\sqrt{s_u/l}} = \sqrt{s_u/l}$ for the train whistle , $s_u = s_w = 0.01 km$ . with $l$ in km : $e_w = \sqrt{s_u/l} = \sqrt{0.01/l} = 0.1/\sqrt{l}$ for the track noise from the entire train , $s_u = s_t = 1 km$ . with $l$ in km : $e_t = \sqrt{s_u/l} = 1/\sqrt{l}$ so , finally , a couple of very rough estimate examples are possible . assume the train is actually about $l = 16$ km away . in that case , the whistle sounds like it is $e_wl$ km away , or : $e_wl = 16e_w = 16 ( 0.1/\sqrt{16} ) = 0.4$ km away . in the same case , the train track sound will appear to be $e_tl$ km away , or : $e_tl = 16e_t = 16/\sqrt{16} = 4$ km away . so , not only are sounds moving through a winter inversion layer highly deceptive for estimating distances , they can be deceptive in different ways at the same time ! a point source such as the train whistle may well sound like it is even closer than the train as a whole -- and both perceptions will sound way , way closer than the actual distance .
the term ' equation of motion ' is somewhat subjective as it depends on the context , but for any given context there is usually one single equation , or set of equations , which can be described as an equation of motion . these are typically differential equations in time , usually of second order , and for simple objects in newtonian mechanics they do not involve other partial derivatives . in that context , equations of motion are usually expressions of newton 's 2 nd law of motion . most importantly , the defining characteristic of an equation of motion is that its solutions , once appropriate initial conditions are fixed , must completely determine the evolution of the system after the initial time . beyond this , and without having more information about your problem , it is impossible to say anything else .
what would be the potential of gnd here ? gnd is the reference node which , by definition , measures 0v . to see this , consider that every one of the node voltages in the circuit are referenced to the gnd node . in other words , if you wish to physically measure the voltage at a node in the circuit , you connect the black lead of your voltmeter to the gnd node and your red lead to the node your wish to find the voltage of . clearly , if you put the red lead on the gnd node , you have connected the red and black leads together and thus , you will read 0v there . so , at the node that is labelled $-10v$ , the voltmeter placed between that node and gnd will read $-10v$ . if you move the red lead to node b , you will measure $-9.3v$ since , given the voltage source between the two nodes , node b must be $0.7v$ more positive than the $-10v$ node . but there is no current flowing through r1 there must be a current through $r_1$ since there is $-9.3v$ volts across it . the current is ' up ' through the resistor , ' up ' through the $0.7v$ source and through the $-10v$ source ( not shown ) back to gnd . i think your confusion lies with the $-10v$ node - it appears to be disconnected but , in fact , it is assumed that there is a $-10v$ voltage source connected there .
a sun or a star is not possible to exist on this scale ; to be as massive as a core of a planet , it is just not massive enough . but you did not mention the size of it . so if we put that aside , first of all there is no such thing as no gravity . where there is mass there is gravity , and that gravity has to be strong enough to hold gas ( atmosphere ) . and the rocks will have to sink into the core since they are the denser objects . if however we compared this to an existing example , where the sun is in the center of the solar system and holding planets ( floating chunks of rocks ) , there is still vacuum in between . because at the distances these planets are from the sun , the sun 's gravity is not strong enough to hold gas . where the sun 's gravity is strong enough there is gas , and that ends as far as the outer atmosphere of the sun itself . which mainly does not extend to the planets . therefore it is not possible .
the $b$ in these equations refers to the magnetic fields each individual wire creates . these fields are proportional to the individual wires ' own currents . in other words , the force of wire i on wire ii is $f_{i \to ii} =i_{ii} \ell b_{i} \sin \alpha$ . what is $b_{i}$ , how does it relate to $i_i$ , and how does it differ from $b_{ii}$ ?
there exists a basic misunderstanding in this question concerning mass and energy . the way special relativity works there can be massless particles , of which the photon is a prime example . even though the individual photon is massles , two photons have an invariant mass , the measure of the sum of their four vectors . proof is the two gamma decay of the pi0 . in the big bang model , after all the universe " exploded " from a point , there was only energy which particle physics posits was carried by elementary particles with zero masses since the symmetries of the standard model were not broken in the first moments . once one has elementary particles , even with zero mass each , the ensemble will have an invariant mass which will be the measure of the sum of the four vectors of the particles in the ensemble .
show me a distribution of remote mass that would provide the behavior we see for both jupiter in orbit around the sun the many moons in orbit around jupiter which both appear to be $1/r^2$ forces . now try to generalize to support all the moons and planets in the solar system . you can not do it because the system is highly over-constrained .
the real space propagator for the massive dirac fermion in $3+1$ dimensions is calculated in r . feynman 's book quantum electrodynamics ( lecture 17 , page 84 in the edition linked to ) . the result is very much as indicated in the question : first solve the wave equation , then differentiate the solution with the dirac operator again . in particular , feynman calculates the propagator of the klein-gordon equation in real space : $$ i_+ ( t , x ) = ∫ \frac{d^4p}{ ( 2π ) ^4} \frac{\exp [ -i ( p\cdot x ) ] }{p^2 - m^2 + i\varepsilon} = - ( 4π ) ^{-1} \delta ( s^2 ) + \frac{m}{8πs}h_1^{ ( 2 ) } ( ms ) $$ here , $s = + ( t^2-x^2 ) ^{1/2}$ for $t&gt ; |x|$ and $s = -i ( x^2-t^2 ) ^{1/2}$ for $t &lt ; |x|$ . moreover , $\delta ( s^2 ) $ is a delta function and $h^{ ( 2 ) }_1 ( ms ) $ is a hankel function . then , you have to differentiate the delta function , indeed . however , note that to be physically meaningful , the propagator $g ( x_2 , t_2 ; x_1 , t_1 ) $ for the dirac equation should only take into account the positive energy states for the retarded time frame $t_2 - t_1 &gt ; 0$ , while the advanced portion $t_2 - t_1 &lt ; 0$ should only take into account the negative energy eigenstates ( holes ) .
i have had a discussion with my father today , about the fuel usage of a vehicle at the same rpm , but a different gear . the fuel injector system does not always inject a constant amount of fuel into the engine . if you are driving down a hill and remove your foot from the gas pedal ( a little ) the engine will stay the same rpm but with less fuel passing thou the system . and if you are driving up a hill you need to press down the gas pedal to maintain the same rpm and speed , more or less you need to push more fuel into the engine . this is why you will not have a constant fuel consumption at a given rpm . then when you remove some load , let 's say you are on the top of the hill , you will have a small surplus of " energy released " inside the engine that will push the rpm higher until you remove your foot from the gas pedal and reducing the amount of fuel sent into the engine . he is also claims that the force to maintain the speed will be the same across different gears with the same rpm . the faster you travel the more wind resistance you will have , therefore you need more power to overcome this resistance . and to get more power out from a engine you need to send in more fuel ( or use the amount have in there in a more efficient way ) . but why do we need to switch gears up and down then , why can not we just inject more fuel into the engine ? this is where we go into chemistry since you can only make gas burn within a specified rate between fuel and air , and the volume inside the engine has a fix size and therefore has a maximum volume of gas that you can burn per rotation . send in more fuel than this and the engine will drop in efficiency . and when you then switch down a gear you will increase the rpm at this speed and that way increase the amount of fuel you can burn in the engine within this timeframe and you will in most of the cases more power out from the engine . but at the end of the day there is a lot of other constraints in a combustion engine that will limit how it works beside this simplified version i just describe here .
okay we have the center of mass coordinate $r_{cm} = ( r_e + r_p ) /2$ , and the reduced mass coordinate $r = r_e - r_p$ . so given the wavefunction $\psi ( r_{cm} , r ) $ what you are asking is just a change of basis from $|r_{cm} , r\rangle$ to $|r_e , r_p\rangle$ . so you just need to consider $$\langle r_e , r_p|r_{cm} , r\rangle = \delta\left ( r_{cm}- ( r_e+r_p ) /2\right ) \ \delta\left ( r- ( r_e-r_p ) \right ) $$ to get more at what appears to be confusing you , let 's focus on how positronium can be polarized . the spherical harmonics will have a defined even or odd parity to inversion , and so yes the square of the wavefunction ( probability density ) is invariant to inversion . but this does not mean all superpositions of these harmonics will have this property . consider for instance just s + p_z orbital . on one side the wavefunction amplitude will add constructively , while on the other it will add deconstructively . the density is no longer symmetric to inversion . this is the standard chemistry description of orbitals , so probably a good way to get follow up reading is searching for hybrid orbitals . here 's a link i found with some images for you : http://www.uwosh.edu/faculty_staff/gutow/orbitals/n/what_are_hybrid_orbitals.shtml
we can prove it in perturbative string theory but it is probably valid beyond it . in perturbative string theory , any ( continuous ) global symmetry has to be associated with a conserved charge which , because of the locality of the physics on the world sheet , implies the existence of a world sheet current $j$ or $\bar j$ or both ( left movers vs right movers ) whose left/right dimension is $ ( 1,0 ) $ or $ ( 0,1 ) $ or both ( because its integral has to be a conformally invariant charge ) . typically , such a symmetry might be something like the isometry of the target ( spacetime ) manifold or something on equal footing with it . it follows that one may also construct operators $j\exp ( ik\cdot x ) \bar\partial x^\mu$ or $\bar j \exp ( ik\cdot x ) \partial x^\mu$ or both with a null vector $k$ which have the dimension $ ( 1,1 ) $ , transform as spacetime vectors , and therefore belong to the spectrum of vertex operators of physical states which moreover transform as spacetime vectors , i.e. they are the gauge bosons ( the $x$ only go over the large spacetime coordinates ) . one would need to prove that the multiplication of the operators does not spoil their tensor character but it usually holds . consequently , any would-be global symmetry may automatically be shown to be a gauge symmetry as well . the argument above only holds for the gauge symmetries that transform things nontrivially in the bulk of the world sheet . but even symmetries acting on the boundary degrees of freedom , i.e. the chan-paton factors , obey the same requirement because one may also construct ( open string ) vertex operators for the corresponding group that transform properly . we do not have a universal non-perturbative definition of string theory but it is likely that the conclusion holds non-perturbatively , too . in some moral sense , it holds for discrete symmetries as well even though discrete symmetries do not allow gauge bosons .
from the point of view of people standing on the earth , no effect whatsoever . that is because the so-called " relativistic mass " is an effect of different frames of reference . ( it is also pretty trivial from the pov of someone at rest with respect to the sun , surpressed by factors of order $\frac{30000\text{ m/s}}{300000000\text{ m/s}} = 10^{-4}$ . ) for many purposes working scientists have almost entirely stopped using the phrases " relativistic mass " and " rest mass " , finding the former concept to be of little practical use .
here i basically do what joshphysics has already mentioned , just in a little more detail , and in a bit more intuitive basis ( which makes effectively no difference ) . so definitely not a slick way . i use the basis $|m_1\rangle\otimes|m_2\rangle$ , where $m_i=\pm1/2$ . keeping only the sign we denote the basis as $\{|++\rangle , |+-\rangle , |-+\rangle , |--\rangle\}$ . now for $s_z=s_{1z}+s_{2z}$ you can check that $$ s_z|++\rangle=\left ( \frac{\hbar}{2}+\frac{\hbar}{2}\right ) |++\rangle $$ and similarly $$ s_z|--\rangle=\left ( -\frac{\hbar}{2}-\frac{\hbar}{2}\right ) |--\rangle $$ so that you and up with the matrix $$ s_z\rightarrow\hbar\left [ \begin{array}{cccc}1 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and -1\end{array}\right ] $$ and $$ s_z^2\rightarrow\hbar^2\left [ \begin{array}{cccc}1 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\0 and 0 and 0 and 1\end{array}\right ] $$ for $s_x$ and $s_y$ you use raising and lowering operators as josh suggested and obtain the matrix elements by acting on each state in the basis and seeing what has a non-zero dot product with the result , e.g. $$ \begin{align} s_y|+-\rangle=\frac{1}{2i}\left ( s_+-s_-\right ) |+-\rangle and =\\\frac{1}{2i}\left ( s_{1+}+s_{2+}-s_{1-}-s_{2-}\right ) |+-\rangle and =\frac{1}{2i}\left ( \hbar|++\rangle-\hbar|--\rangle\right ) \end{align} $$ thus $\langle++|s_y|+-\rangle=\frac{\hbar}{2i}$ and $\langle--|s_y|+-\rangle=-\frac{\hbar}{2i}$ . once you do this you can get $$ s_x\rightarrow\hbar\left [ \begin{array}{cccc}0 and 1 and 1 and 0\\1 and 0 and 0 and 1\\1 and 0 and 0 and 1\\0 and 1 and 1 and 0\end{array}\right ] $$ and $$ s_y\rightarrow\hbar\left [ \begin{array}{cccc}0 and 1 and 1 and 0\\-1 and 0 and 0 and 1\\-1 and 0 and 0 and 1\\0 and -1 and -1 and 0\end{array}\right ] $$ finally the hamiltonian looks like $$ h=\left [ \begin{array}{cccc}\hbar^2b and 0 and 0 and \hbar^2a\\0 and 0 and 0 and 0\\0 and 0 and 0 and 0\\\hbar^2a and 0 and 0 and \hbar^2b\end{array}\right ] $$ therefore $|+-\rangle$ and $|-+\rangle$ are eigenvectors with eigenvalue $0$ , and you only need diagonalize $2\times2$ matrix to find that the eigenvalue $\hbar^2\left ( b+a\right ) $ corresponds to the eigenvector $\left ( |++\rangle+|--\rangle\right ) /\sqrt{2}$ , and $\hbar^2\left ( b-a\right ) $ corresponds to the eigenvector $\left ( |++\rangle-|--\rangle\right ) /\sqrt{2}$ .
i do not have the book here right now , so i am not sure what he is referring to exactly by that comment . but by modifying the energy-momentum tensor , you will change the central charge and actually get a whole new cft . so i would not call that a symmetry of the free scalar theory . however the theory does have a lot more symmetry , let me focus on one chiral sector only . as you know already , the special feature of two-dimensional cft 's is that the spin-two conserved current ( em-tensor ) $\bar{\partial}t ( z ) =0$ , automatically implies that there is an infinite number of conserved currents $\bar{\partial} ( z^n\ , t ( z ) ) = 0$ in the theory . this is essentially the reason why the conformal algebra , extends to the infinite-dimensional virasoro algebra i two-dimensions . lets use the notation $w_2\equiv t$ , where the index refers to the spin . free-field theories , however , contain an infinite tower of higher-spin conserved currents $w_s ( z ) $ where $s= 2 , 3 , \dots$ is the spin . similar to the $w_2$ case , for any conserved current $w_s$ , there is infinite number of associated conserved currents $\bar{\partial} ( z^n\ , w_s ( z ) ) =0$ . thereby each current $w_s$ extends the virasoro algebra with infinite number of new generators , and there is by itself an infinite number of currents $w_s$ . so this is a vast extension of the virasoro algebra . for example in the case of free complex scalar theory the currents are given by [ a ] $w_s ( z ) = b ( s ) \sum_{k=1}^{s-1} ( -1 ) ^k\ , a^s_k\ , :\partial^k\phi\ , \partial^{s-k}\bar{\phi}: ( z ) $ , where $b ( s ) $ and $a^s_k$ are constants . see in particular equations ( 2.11 ) and ( 2.18a ) - ( 2.18e ) in [ a ] . here $w_2 ( z ) $ is the usual energy-momentum tensor leading to $c=2$ and the virasoro algebra . all the generators combined give rise to the so-called $w_\infty^{prs}$-algebra , which contain the virasoro algebra as a " small " subalgebra . similar things can be done for other free-field theories , i myself used a similar construction for the free ghost system in a recent paper . higher-spin extensions of the virasoro algebra are usually called $\mathcal w$-algebras and they do not lead to conventional lie algebras , but certain types of non-linear algebras . see [ b ] for a review . the free field theories realize the rare type of $\mathcal w$-algebras which are usual ( linear ) lie algebras . in higher dimensions free-field theories also have an infinite tower of higher-spin conserved currents and thereby an infinite dimensional symmetry algebra . but each conserved current only lead to a finite number of generators in the algebra , unlike the the two-dimensional case . maybe polchinski is referring to this vast number of higher-spin symmetries of the free-field theories ? [ a ] : bakas and kritsis - bosonic realization of a universal $\mathcal w$-algebra and $z_{\infty}$ parafermions [ nucl . phys . b 343 , 185 ( 1990 ) ] [ b ] bouwknegt and schoutens - $\mathcal w$ symmetry in conformal field theory [ phys . rept . 223 ( 1993 ) 183-276 ]
starting with your given equation , we add $p^2 c^2$ to both sides to get $$ e^2=m^2 c^4 + p^2 c^2$$ now using the definition of relativistic momentum $p=\gamma m v$ we substitute that in above to get $$e^2 = m^2 c^4 + ( \gamma m v ) ^2 c^2=m^2 c^4 +\gamma^2 m^2 v^2 c^2$$ now , factoring out a common $m^2 c^4$ from both terms on the rhs in anticipation of the answer we get $$e^2=m^2 c^4 ( 1+\frac{v^2}{c^2}\gamma^2 ) $$ now using the definition of $\gamma$ as $$\gamma=\frac{1}{\sqrt{1-\frac{v^2}{c^2}}}$$ and substituting this in for $\gamma$ we get $$e^2=m^2 c^4 \left ( 1+\frac{\frac{v^2}{c^2}}{1-\frac{v^2}{c^2}}\right ) $$ and making a common denominator for the item in parenthesis we get $$e^2=m^2 c^4 \left ( \frac{1}{1-\frac{v^2}{c^2}} \right ) =m^2 c^4 \gamma^2$$ taking the square root of both sides gives $$e=\pm \gamma mc^2$$ hope this helps .
how do you know something is electrically charged ? well , it interacts with other charges . classically , this is described by the maxwell equations , i.e. by fields . one special case of such an electromagnetical field , the plane wave , is what we call light ( amongst other phenomena , radio transmission etc . ) . that is pretty much it ! perhaps it is actually easier understand from a more " modern " point of view : in qft , the electromagnetical fields are quantised to particles called photons . in " slow " processes like electrostatic repulsion of balloons , those photons have low frequency , and thus tiny energy $e=\hslash\cdot \omega$ , where $\hslash$ is the minute ( by classical measures ) planck quantum . therefore , we recognise the interactions readily as smooth fields . at higher frequencies , such as light ( $\approx 10^{18}\:\mathrm{hz}$ ) , the energy for each quantum is much higher , so when an atom emits light it is much more naturally single photons we are dealing with . but in principle , both are the same thing .
in your first reference , page $58$ , equation $ ( 3.55 ) $ , there is a personal definition by the author of what it calls " spinor adjoint of a matrix": $\overline m \stackrel {def}{\equiv} \gamma_0 m^\dagger \gamma_0$ with this definition , as you noticed , you have obviously $\overline {\gamma^\mu}= \gamma^\mu$ the above definition of " spinor adjoint of a matrix " is compatible with the definition of the adjoint $ ( 3.54 ) $: $\overline \psi = \psi ^\dagger \gamma_0$ in the following way : $\overline {m\psi} = ( m\psi ) ^\dagger \gamma_0 = \psi^\dagger m^\dagger \gamma_0 = ( \psi^\dagger \gamma_0 ) ( \gamma_0 m^\dagger \gamma_0 ) = \overline {\psi}~ \overline {m}$
an object is not necessarily heated to a plasma when it falls into a black hole . with quasars matter spirals down towards the event horizon so both it is speed and density increases , but this does not heat it directly . it is because matter interacts with the other matter around the event horizon that you get collisions and heating and the spectacular x-ray emission . by contrast the black hole at the centre of our galaxy is thought to be fairly quiet because it is already gobbled up all the matter in it is vicinity . if you jumped into it you had probably make it through the event horizon unharmed and it would only be near the singularity that tidal forces squished you . you need to bear in mind that once matter has passed the event horizon it is fall to the singularity is very quick , so there is not very much matter within the event horizon that has not already fallen into the singularity . what happens to the matter at the singularity no-one knows . response to comment : my point is that an acretion disk is not a feature of all black holes . accretion disks only form where a black hole is actively swallowing matter . also accretion disks will form around any heavy object . arguably saturn 's rings are a form of accretion disk with the matter in them eventually falling into saturn . my other point is that for matter falling into a black hole nothing special happens when it crosses the event horizon . if you were falling into a black hole then you would not be able to detect when you had crossed the event horizon . so if you had been heated up by friction in an accretion disk before you hit the event horizon you had be in pretty much the same state after you had crossed it . the state you are in , whether it is plasma or not , is dependant on how much you got heated up as you fell through the accretion disk ( if an accretion disk is present ) and is not anything specifically related to the presence of an event horizon .
the residual resistivity can be due to any kind of imperfection which destroys the complete periodicity of the lattice ; most famous kinds are structural defects ( provided that they happen in a disordered manner ) as you correctly mention and impurities ( again they should be distribited in a disordered manner ) . the way alloys are made , their lattice structure is certainly disordered since people do not make them by manipulating the lattice to make it perfect , and this disorder introduces a mean-free-path of order of the microscopic inhomogenities ( the spatial fluctuation of the density of the different metals composing the alloy ) of the alloy to the electric transport . this disorder has a small temperature dependence ( at least at low temperatures ) compared to the contribution from phonons and i guess it usually remains the main source of resistivity . now as to why the trend is more linear in the case of alloys , your graphs are not on the same scale to be used for the comparison and i was not able to find a good collection of graphs either , so let 's wait for a better collection of graphs before the final conclusion ; maybe that is not always the case .
when the finite speed of light – the delay of the rays from the source – is taken into account , one encounters many optical effects aside from " how the world is in relativity effects " such as lorentz contraction . flat lines look like arcs or cicles , one may see " behind himself " , and there is of course the doppler shift of the frequencies depending on the relative speed . also , streetcars going from left to right are rotated around a vertical axis , and so on . you may download real time relativity as a great " relativistic 3d game " . for a link and other comments on relativistic optical effects , see http://motls.blogspot.com/2009/02/relativistic-optical-effects.html?m=1
a ( somewhat idealized plot ) is shown on page 6 of this paper . in a three body decay , the energy peaks close to the maximum due to phase space . the maximum is $53$ mev and the peak looks to be around $45-48$ mev
the standard book is introduction to solid state physics ( 8th ed . 2005 , isbn 0-471-41526-x ) by charles kittel . your question should be answered in chapter 13 . all that is said there should in principle be applicable to semiconductors . but since their bandgap is lower than in dielectrics you might have a problem measuring this ( they might just be too conductive an the effect is lost ) . on the other hand in your prototypical gaas crystals there is a rather strong effect in the ( 111 ) direction due to the stacking of ga and as layers and their asymmetric response to stress . . . note though that this has nothing to do with electron/hole pairs . this is strictly a polarisation phenomenon .
in 1958 dac was developed in nist by weir , lippincott , van valkenburg , and bunting . this dac article shows an image of a hand palm sized dac in the nist museum . the diamond anvil cell ( dac ) is a refined device , based on the bridgman cell . the latter was developed in the beginning of the first half of the 20th century . your question aims on the updated dac technique . what is the technical distinctive feature ? the bridgman cell has a quatro-plate geometry for applying force . this first developed technique has the feature to allow rays or wires between a pair of pressure plates and is available in a wide pressure range . polished diamond flat-tops allow beam transmission through the sample . this is depicted in the wikipedia article . however beam intensity is extenuated due too high fresnel losses . refractive index $n_{diamond}\approx 2.4$ is high . spectroscopic measurement may be influenced by natural lattice defects in diamond lattice . the refined method , called dac , allows higher pressure . to archive this the bridgman anvil made of tungsten-carbon alloy was replaced by a single-crystal diamond . the problem of breaking anvils was solved and a new pressure range was available . fun fact diamonds used by nist often were destroyed due to high pressure . the diamonds were former property of smugglers . government agents confiscated the gem diamonds . they were given for scientific purpose to the researchers .
since this question is still open and therefore not definitely answerable at present , i save the valuable discussion of the topic in the comments as an answer such that it does not get lost : this is just an accident of 10 dimensions--- there is too much supersymmetry to have a full susy superspace . it is a very good question , but research level , if you answer it fully , everyone will breathe easier . – ron maimon apr 12 at 2:11 3 up voted there are superspace formulations for 4d n=4 theories , the problem is that they only are on-shell formulations . the n=4 multiplet contains the n=2 hypermultiplet and there is a no-go theorem saying that there is no off-shell formulation with a finite number of auxiliary fields . thus you get projective and harmonic superspaces with infinite numbers of auxiliary fields . this works for n=2 , but in n=4 all constraints to reduce the unconstrained superfields down to the physical multiplets force the fields on-shell . – simon apr 12 at 3:47 3 up voted and as @ron says , the reason why it is so difficult to construct such formulations is an open research-level question . if the reason was known , then we had either have a workable n=4 superspace formulation or a no-go theorem by now . . . – simon apr 12 at 3:48 3 up voted dear dilaton , good question . ron and simon have already answered to some extent and i will only offer a different extent , extending ron 's comment in particular . if you want to make n=1 susy in 4d i.e. 4 real supercharges manifest , you need 4 superspace fermionic coordinates . with 16 supercharges , you would probably need at least 16 fermionic coordinates in the superspace but then the fields would have 216=256 components which is pretty high give that you only need 16 on-shell components only . most of the component fields would have to be auxiliary , linked to deritives of others , etc . hard – luboš motl apr 12 at 5:37 4 up voted there is also an interesting twistor-like transform for the 10d super yang-mills by a guy called witten – luboš motl apr 12 at 5:39 thanks guys for these valuable comments and the cool links therein . i would " like " and appreciate them as " partial " answers ( since as you say there is no full answer on this yet ) too . . . :- ) . – dilaton apr 12 at 8:45 @lubošmot l must admit that twistors are one of my black holes of ignorance ( i did not get it from roger penrose 's " road to reality" ) :-/ . . . so i am checking from time to time if i can find a nice " pedagogical " introduction to this on trf ; - ) – dilaton apr 12 at 8:50 1 up voted @dilaton : this is the problem with research level stuff , i can not answer because i do not feel confident enough in my biases about what the answer could or should be to put them in writing , and i would mention a bunch of things that i tried and did not work to answer this , and are not interesting , and i think everyone else is hesitant to answer too for similar reasons . maybe you could copy the comment thread into an answer box , and accept your own answer . – ron maimon 2 hours ago 1 up voted i mean , if you want a little more on this--- there is the question of whether superspace is fundamental in the first place--- it is just a trick for writing multiplets in a way that takes the susy off shell naturally , but the physical reasoning has always eluded me . i tried nicolai maps as an alternative , but it never worked , and it always is tantalizingly close to working , and i tried learning harmonic superspace for on-shell n=4 , but although it is correct , its so annoyingly complicated to work with ! and the s-matrix is simple , so there is a better language out there , i do not know what . – ron maimon 1 hour ago thanks @ron maimon , that is a good idea to save the discussion into an answer . i`m somehow intrigued too by the question if superspace itself could have some physical meaning . . . – dilaton 2 mins ago even though the question is still open , it could probably nevertheless be worthwhile to know what you tried and why it did not work . i mean similar to some kind of " null results " who can be interesting too . . . ? – dilaton
if you consider a standard differential operator $b$ working on functions defined in $\mathbb r^n$ , like $\partial/\partial x_i$ or a polynomial of partial derivatives , and pick out a sufficiently smooth function $f$ vanishing in a neighbourhood $\omega$ , you see that also $bf$ vanishes therein . this is the relevant notion of locality for operators . in the rhs of the equation you wrote down an operator shows up which does not fulfil locality in the sense i said . that equation is , in fact , the equation satisfied by the positive energy solutions of klein-gordon equation . the operator in the rhs cannot be defined by formal taylor expansion ( it works only formally ) , but one has to use spectral theory . in the considered case it is equivalent to translate that equation in fourier transform . non locality arises here due to a known property of the operator $a:= \sqrt{-\delta + ai}$ and , more generally , for $ ( \delta + ai ) ^\nu$ with $\nu \not \in \mathbb z$ . this property is called anti locality ( i.e. . segal , r.w. goodman , j . math . mech . 14 ( 1965 ) 629 ) and is related to the famous reeh and schlieder property in qft . anti locality means that if both $f$ and $af$ vanish in a bounded region $\omega \subset \mathbb r^3$ then $f$ is everywhere zero . if $f$ has support included in a bounded open set $\omega$ , then , remarkably and very differently from what happens for standard differential operators , $af$ does not identically vanish outside $\omega$ otherwise $f$ would be the everywhere zero function .
while this is not necessarily going to answer your request , i think it might be interesting none the less : phases of n=2 theories in two dimensions in a string theory context : the basic idea is to study a glsm in 2d which exhibits the interesting property to lead to calabi-yau compactification in one phase and orbifold compactification in the other . the hope and current research is to better understand calabi-yau compactification by taking a look at the orbifold phase and perhaps find a suitable way to give rise to the standard model in string theory .
1 ) yes . you are right with the perception of white and black . regarding other colors , it depends on the energy of photon which excites your cone and rods . 2 ) the brightness in physical ( cosmological to be specific ) terms , is differentiated into luminosity and flux . both are quite comparable . luminosity is the amount of energy emitted per second ( $j/s$ or simply , watts ) from the source whereas flux is the power received per unit area $ ( wm^{-2} ) $ . both are related simply by using the surface area of a sphere of radius $r$ , which is probably the distance from the source and the relation is $l=f ( 4\pi r^2 ) $ . this flux tells you how much energy is emitted as a function of time which explicitly shows the amount of photons impinging on your retina . when colors combine to produce different colors , is there any photon combining that exists or is it because your eyes see mixtures of photons and not photons themselves ? for now , there is a difference . photons do not combine . instead ( as i have previously mentioned ) , different photons excite your photo-receptors at different or same time periods . based on the excitation , the color is observed by you . on the other hand ( if those were assumed to have wave character ) , they can constructively or destructively interfere to produce additive or subtractive color . now , to other question . to get light blue , is not it a mixture of mostly blue photons with white light ( photons of all frequencies ) to produce a blueish white or a light blue ? this does produce light blue . but , the flux factor produces greyish blue and not whitish blue . now as white light contains all the frequencies , you will perceive all in the same way but with several blue-ly energized photons along with it . imagine this to be the surface area of a white circular object with some blue spots in it . at a comparable distance , you can see it as whitish blue ( your favorite ) . . . if you are still confused of brightness , wiki has a nice quote on it . . . " brightness " was formerly used as a synonym for the photometric term " luminance " and ( incorrectly ) for the radiometric term " radiance " . in rgb color space , brightness can be thought of as the arithmetic mean of r , g and b color coordinates ( although some components make the light seem brighter than others , which again , may be compensated by some display systems automatically )
this might not be feasible for your setup , but you could try rapidly rotating your light source , which would doppler broaden your spectral lines . you are correct in that the speed would need to be a substantial with respect to the speed of light . as an example , if your frequency is 500nm let 's say . if you had like it to spead out on the order of a single nanometer , you would need to solve : $$ f = f_{0} \left ( 1 + \frac{v}{c} \right ) $$ for v , the velocity , given that $f_0 = 500$nm , and $f = 501$nm . if you did so the velocity would have to be 600,000 m/s . alternatively , you could try heating your source up so that the atoms/molecules on average have higher velocities which will broaden your spectral lines ( since some will be traveling away from you and some will be traveling toward you ) . assuming that the thermal energy of an atom at temperature t with mass m is equally divided among each degree of freedom it has ( see : equipartition theorem ) , the root mean square velocity relates to the temperature in the following way : $$ v_{rms} = \sqrt{\frac{3rt}{m}} $$ where m is the mass of a mole of ideal gas particles . the question is then , in order to see a spreading of 1nm , you would need some of the particles to reach the aforementioned velocity , and at what temperature is this reached for the particular gas you are interested in using ? i personally think the easiest thing to do is to do this numerically , in fact there are plenty of python packages which could assist you in convolving your spectral lines with a known smoothing kernal . you could use a gaussian kernel ( effectively a point-spread-function ) . if you are hell bent on it being a physical spreading of your emission lines i think you have your work cut out for you ( unless i am forgetting something here ) .
the particle data book , published by the particle data group is probably the nearest to what you are asking for . the actual book is a massive tome costing a fortune . i am not sure how much of and in what form it is online , but some intrepid googling should find you most of what you want . as fqq points out the book is available from phys . rev . d vol . 86 issue 1
mathematically spoken , since you want your wave functions to be square integrable , your wave functions must be in $l^2$ or some subspace thereof . however , you will not find a function in this space that has a support on a countable set of points , since the lebesgue integral cannot see countable sets ( measure 0 ) , hence there cannot be a function ( i.e. . no wave function ) with support in a single point ( incidentally , the delta function is not a " function " in a way for that reason ) . this tells us that a wavefunction for a particle that is fully localized cannot be defined in the usual setting of square lebesgue-integrable functions , which is not too tragic , because we do not really think it makes physical sense anyway .
the no-cloning theorem states that it is not possible to have a quantum state $|\psi&gt ; $ evolve into two separable ( non-entangled ) copies described by the tensor product state $|\psi&gt ; |\psi&gt ; $ . the proof boils down to the simple observation that when expressing $|\psi&gt ; $ in some basis ${|0&gt ; , |1&gt ; , |2&gt ; , . . . }$: $$|\psi&gt ; = \alpha_0 |0&gt ; + \alpha_1 |1&gt ; + \alpha_2 |2&gt ; + . . . $$ the cloning operation would be a unitary evolution of the form : $$u ( \alpha_0 |0&gt ; + \alpha_1 |1&gt ; + . . . ) = \alpha_0^2 |0&gt ; |0&gt ; + \alpha_0 \alpha_1 |0&gt ; |1&gt ; + \alpha_1 \alpha_0 |1&gt ; |0&gt ; + \alpha_1^2 |1&gt ; |1&gt ; . . . $$ this leads to a contradiction , as the unitary operator $u ( . . ) $ is linear and can never create amplitudes like $\alpha_0^2$ and $\alpha_0\alpha_1$ that are quadratic functions of the $\alpha_i$ . so the linearity the author is referring to is the linearity of the unitary evolution . in quantum physics evolution is described by unitary operators which transform incoming states into outgoing states that are a linear combination of the ingoing states .
cygnus x-1 is a binary system , and the radius you cite ( taken presumably from wikipedia article ) is the radius of the binary system not the radius of the black hole . the wikipedia article is highly misleading in this respect . at least , i think it is the radius of the binary system . 20-22 r☉ is about 0.1 au which i think is about the suggested spacing between the two stars .
the main thing to note is that the definition of the wavenumber you cite above is dependent on the underlying function satisfying the standard wave equation , because any function that satisfies $$\eta^{ab}\partial_{a}\partial_{b}\phi ( x ) = 0$$ will have its fourier transform , $\phi ( k ) = \int d^{4}x e^{ik^{a}x_{a}}\phi ( x ) $ satisfy $$k^{a}k_{a}\phi ( k ) = 0$$ but this is no longer true , because for the case of curved spacetime , the wave equation is $$g^{ab}\partial_{a}\partial_{b}\phi ( x ) - g^{ab}\gamma_{ab}{}^{c}\partial_{c}\phi ( x ) = 0$$ and this will require some modification to the fourier transform to work . more physically , you have effects like gravitational lensing that cause light to interact with the gravitational field , so you do not get simple straight-line propogation of monochromatic modes . note , however , that it is always possible to locally transform to a coordinate system where $\gamma_{ab}{}^{c} =0$ and $g_{ab} = \eta_{ab}$ , and there , you will be able to have a well defined wavenumber and frequency which satisfies $\omega^{2} = k^{i}k_{i}$ . this just will not work outside of your local neighborhood .
the external field causes the magnetic dipole moments $\mathbf m$ of the atoms in the material to align with the applied field $\mathbf b$ . if one now imagines summing up the fields due to all of the tiny little dipole moments that are now aligned with the external field , then one will find that the net effect is that the field inside is augmented .
nai think what you are essentially asking is that , " can we violate or circumvent the second law of thermodyanmics ? " the answer is no , based on all the physics we know so far and observations we have made so far . when you freeze the melted ice back to ice you are creating irreversible changes elsewhere in the universe ( e . g . , outside your refrigerator ) . that said , there is a deeper conversation on whether the second law fundamental or only a measure of our ignorance about the complex system . a conversation that is not philosophically well resolved . you can search for work by ilya prigogine , or about the arrow of time . but setting philosophical questions aside , in the real-known world we cannot violate or circumvent the second law of thermodynamics .
using the method of images , you can calculate the force between the ring of charge and the sphere . assume the sphere is on the z axis with it is center on the point $z$ , a radius of $r_s$ and the ring 's radius is $r_r$ with a charge density $\lambda$ . so $z$ denotes the center of the sphere . to calculate the force , you can replace the sphere with a charged ring ( method of images ) with a charge density $\lambda'$ placed at a distance $d$ below the sphere 's center and a point charge $q'$ at the center of the sphere to make the sphere electrically neutral : $$\lambda'=-\lambda\frac{ \sqrt{r_r^2+z^2}}{r_s}$$ $$d=\frac{zr_s^2}{z^2+r_r^2}$$ $$q'=q\frac{r_s}{\sqrt{z^2+r_r^2}}$$ now the problem reduces to finding the force between the main ring and the induced ring and point charge . the image below shows the choose of parameters : i assume that $r_{\text{ring}}&gt ; r_{\text{sphere}}$ . the equations for the case of $r_{\text{ring}}&lt ; r_{\text{sphere}}$ are the same , except that the motion is restricted to $z&gt ; \sqrt{r_s^2-r_r^2}$ . the field of the main ring differs for points with $r&gt ; r_r$ and $r&lt ; r_r$: ( $q$ is the ring 's total charge ) $$\phi ( r , \theta ) =\frac{q}{4\pi\epsilon_0}\sum_{l=0}^\infty \mathrm{p}_{2l} ( \cos \theta ) \cases{\frac{r_r^{2l}}{r^{2l+1}}\mathrm{p}_{2l} ( 0 ) \ , \ , \ , \ , \ , \ , \ , \ , \ , \ , \text{$r&gt ; r_r$}\\\frac{r^{2l}}{r_r^{2l+1}}\mathrm{p}_{2l} ( 0 ) \ , \ , \ , \ , \ , \ , \ , \ , \ , \ , \text{$r&lt ; r_r$}}$$ and $$\mathbf{e}=-\frac{\partial \phi}{\partial r}\hat r-\frac{1}{r}\frac{\partial \phi}{\partial \theta}\hat \theta$$ because of the rotational symmetry of the induced charges , on which we want to find the force , the force will be in the $z$ direction : ( i omitted the lengthy calculations ) $$\mathbf{f}_{\text{total}}=\hat z \frac{q^2}{4\pi \epsilon_0} \left [ \frac{z}{ ( r_r^2+z^2 ) ^2}-\frac{1}{r_r^2\sqrt{z^2+r_r^2}}\cases{{\times f_{\text{out}} ( r , \theta ) }\\{\times f_{\text{in}} ( r , \theta ) }} \right ] $$ where $f_{\text{out}}$ is used when $r&gt ; r_r$ and $f_{\text{in}}$ is used when $r&lt ; r_r$ . these are dimensionless functions that appear when differentiating the above potential : $$f_{\text{out}}=\sum_{l=0}^\infty\mathrm{p}_{2l} ( 0 ) \left ( \frac{r_r}{r}\right ) ^{2l+2}\left ( ( 2l+1 ) \mathrm{p}_{2l} ( \cos \theta ) \cos\theta +\mathrm {p}^1_{2l} ( \cos \theta ) \sin\theta \right ) $$ $$f_{\text{in}}=\sum_{l=0}^\infty-\mathrm{p}_{2l} ( 0 ) \left ( \frac{r}{r_r}\right ) ^{2l-1}\left ( 2l\mathrm{p}_{2l} ( \cos\theta ) \cos \theta-\mathrm{p}^1_{2l} ( \cos \theta ) \sin \theta \right ) $$ and $r$ and $\theta$ are functions of $z$: $$r=\left| z-\frac{r_s^2}{\sqrt{z^2+r_r^2}} \right|$$ $$\cos \theta=\frac{z-\frac{zr_s^2}{z^2+r_r^2}}{r}$$ $$\sin \theta=\frac{\frac{r_rr_s^2}{z^2+r_r^2}}{r}$$ $\mathrm {p}^1_{2l} ( \cos \theta ) $ s are associated legendre functions , and appear when differentiating $\mathrm {p}_{2l} ( \cos \theta ) $ w.r.t. $\theta$ . the reason of summing over even terms ( indexes are $2l$ instead of $l$ ) is that $\mathrm {p}_{l} ( 0 ) $ is nonzero for even terms , as is equal to : $$\mathrm {p}_{l} ( 0 ) = \frac 1 {2^l} \sum_{k=0}^l {l\choose k}^2 ( -1 ) ^{l-k}$$ it seems impossible to say anything intuitively , but the above equations can be used to simulate the system ( with a rather good accuracy , since the higher order terms in the above series go as $r^{2l-1}$ instead of $r^l$ ) .
parallax is linearly proportional to separation , so to get meaningful depth perception to even one star , your eyes would have to be ( present eye separation ) * ( distance to proxima centauri ) / ( longest distance at which we naturally have meaningful depth perception ) . having been to meteor crater , i can tell you that the last quantity is definitely under a half mile , i.e. distance from rim to center , but for a conservative estimate we will call it that . our formula , then , is sep=3 inches * 4.2 l-yr / 2640 feet = . 0004 l-yr = 25 au . that is past uranus , almost to neptune .
there are really two questions here . one question is , " what does detailed balance mean for equilibrium systems ? " and the second question is , " what does detailed balance mean in the context of selecting a move for a metropolis monte carlo algorithm like the one i linked to ? " i will start with the first one . imagine we have an ensemble of particles that can be in one of three internal states , $a$ , $b$ , and $c$ , ( say a spin that can have $z$ component $-1$ , $0$ , or $1$ ) and suppose that this system in thermal equilibrium . then as the system evolves , the internal state of each particle may change with time . however , since the system is equilibrated , the fraction of particles in each state must be constant . thus if one particle , say , goes from state $a$ to state $b$ , then there must be one particle going from state $b$ to state $a$ . thus detailed balance says that for any two states $i$ and $j$ , the transition rate for $i \to j$ must equal the rate for $j \to i$ . this is the story of detailed balance for systems in thermodynamic equilibrium . the above paragraph , however , is not the explanation for what you were reading about in the ising model article . the part of the article you were reading about had to do with simulating ising systems . one method of simulation is a metropolis monte carlo . if you want to simulate a system with many states , like an ising system , what you can do is start the system out in a random state $a$ , then iterate as follows . you pick another state $b$ so that you have the candidate move $a \to b$ . you then accept this move with a probability which gets small if $b$ has a much higher energy than $a$ . if this is done right , then the probability of being at a given state in a given iteration is just the boltzmann probability , so this method gives you botlzmann statistics . now there are two things that have to be done right . first , you must pick the candidate moves fairly , and second you must pick the acceptance probability of the candidate move correctly . picking the acceptance probability is easy , if the energy of the new state is lower , automatically accept the move . if the energy is higher , accept the move with probability $\exp ( -\delta e / t ) $ , which gets small when $\delta e$ is big . picking the candidate moves fairly is when detailed balance comes in . suppose you always decided the candidate move would always be to the same state . clearly this will destroy boltzmann statistics because your system will always be in the given state regardless of its energy or any other energies . clearly there must be a better way to pick moves . the correct way to choose candidate moves is for them to satisfy the detailed balance condition : $p ( a \to b ) = p ( b \to a ) $ , where $p ( a \to b ) $ is the probability picking the candidate move to $b$ if you are in state $a$ . this is what was meant by detailed balance in the article you linked to . note it does not refer to the actual moves which are accepted by the algorithm , just the moves that are proposed .
no . it does not necessarily mean that the acceleration of $a$ is greater than the acceleration of $b$ . here 's an explicit counterexample : object $a$ is moving at $10\ , \mathrm{m/s}$ with constant velocity while object $b$ is moving at $5\ , \mathrm{m/s}$ with an acceleration of $1\ , \mathrm m/\mathrm s^2$ . in this case , the acceleration of $a$ is zero , so $b$ 's acceleration is greater , but it is velocity is lower . note that the initial conditions of the motions of the two objects are irrelevant ; we are talking about instantaneous velocities and accelerations , and given any two objects , one can completely independently pick their velocities and accelerations .
consider the following scenario : i am on a train moving away from you . i throw a ball to you . the speed of the ball as measured by you when you catch it , is less than the speed of the ball as measured by me when i threw it . where did the energy go ? this situation is precisely the same as the doppler shift situation you describe . in both cases , there is no problem with energy conservation , because the energies in question are measured in two different reference frames . energy conservation says that , in any given reference frame , the amount of energy does not change . it says nothing about how the energy in one frame is related to the energy in another frame .
in canonical quantization , a quantum field is a linear combination of so-called " creation and annihilation operators " . that means that the field $\phi$ creates and destroys particles of type $\phi$ . the state $|0\rangle$ is the vacuum : the state with no particles . if $\phi$ is a quantum field that creates and destroys particles , it must be that $\langle 0 | \phi | 0 \rangle=0$ , because the state with particles created/destroyed , $\phi|0\rangle$ must be orthogonal to the empty state $|0\rangle$ . we have no choice , then , but to write $\phi=v+\eta$ , such that $\langle \eta \rangle=0$ . th new field $\eta$ now has a good particle interpretation , whereas the original field $\phi$ did not , because $\langle\phi\rangle = v$ .
to show that this measure is lorentz invariant you first need to explicitly write your integral as an integral over mass shell in 4d k-space . this could be done by inserting dirac delta function $\delta [ k^\mu k_\mu-m^2 ] $ and integrating over the whole 4d space . then you could apply the following transformations : \begin{align} \theta ( k_0 ) \cdot\delta [ k^\mu k_\mu-m^2 ] and = \theta ( k_0 ) \cdot\delta [ k_0^2-|\mathbf{k}|^2-m^2 ] \\ and =\theta ( k_0 ) \cdot\delta\left [ ( k_0-\sqrt{|\mathbf{k}|^2+m^2} ) ( k_0+\sqrt{|\mathbf{k}|^2+m^2} ) \right ] \\ and =\frac{\delta\left [ k_0-\sqrt{|\mathbf{k}|^2+m^2}\right ] }{2\ , k_0} , \end{align} where heaviside function $\theta ( k_0 ) $ is used to select only future part of the mass shell .
i ) the proofs of both the first ( algebraic ) bianchi identity and the second ( differential ) bianchi identity crucially use that the connection $\nabla$ is torsionfree , so they are not entirely consequences of the jacobi identity . proofs of the bianchi identities are e.g. given in ref . 1 . ii ) the second bianchi identity may be formulated not only for a tangent bundle connection but also for vector bundle connections . iii ) the lie bracket in the pertinent jacobi identities is the commutator bracket $ [ a , b ] :=a\circ b -b\circ a$ . the jacobi identity follows because operator composition "$\circ$" is associative . iv ) in the context of yang-mills theory and em , the second bianchi identity follows because the gauge potential $a_{\mu}$ and the field strength $f_{\mu\nu}$ may be viewed as ( part of ) a covariant derivative and corresponding curvature tensor , respectively . references : m . nakahara , geometry , topology and physics , section 7.4 .
the central charge term as an example of a quantum anomaly ; a symmetry that is modified in the quantized version of a classical theory . the central charge is , in fact , often referred to as the conformal anomaly . as di-francesco et . al . put it at the start of section 5.4.2: the appearance of the central charge $c$ , also known as the conformal anomaly , is related to the " soft " breaking of conformal symmetry by the introduction of a macroscopic scale into the system . they then go on to show that if , for example , you consider a generic conformal field theory on $\mathbb c$ , and if you map the theory onto a cylinder of circumference $l$ with coordinate $w$ , then \begin{align} \langle t_\mathrm{cylinder} ( w ) \rangle = -\frac{c\pi^2}{6l^2} \end{align} they also , in appendix $5a$ , go on to show that when a conformal field theory is defined on a curved two-manifold , then the central charge is related to the so-called trace anomaly ; \begin{align} \langle t^\mu_{\phantom\mu\mu} ( x ) \rangle = \frac{c}{24\pi} r ( x ) \end{align} where $r$ is the ricci scalar . the central charge can be seen to arise naturally in radial quantization in the operator formalism of cft : see di-francesco et . al chapter 6 . anomalies arizing from quantization are not restricted to conformal symmetry . see , for example , the chiral anomaly or the gauge anomaly .
the difference has to do with the units in which $\vec{b}$ is measured in . in si units faraday 's law reads as , $$ \nabla \times \vec{e} = - \frac{\partial \vec{b}}{\partial t} $$ in gaussian and heaviside-lorentz units it reads as , $$ \nabla \times \vec{e} = - \frac{1}{c}\frac{\partial \vec{b}}{\partial t} $$ basically this amounts to redifining $\vec{b}$ . $$ c\vec{b}_\text{ ( si ) } \equiv \vec{b}_\text{ ( gaussian ) }$$
yes , your understanding is correct . someone who is conscious and/or usefully using the result of the measurement is not necessary for the experiment to be modified ; it is the particles used to detect the slit or other information that modify the experiment . they interact with the photon in the slits etc . ( although it is hardly other photons because photon-photon interactions are virtually unmeasurably weak ) . quantum mechanics implies that the experiment is modified regardless of any consciousness that is really not a part of physics . however , the existence of a conscious observer who learns the value of an observable is a sufficient condition for the experiment to be altered . without an alteration of the experiment , one could never be " aware " of the slit through which the photon went , for example . so the conscious realization of an outcome is a possible proof – one of the possible proofs – that some particles or objects measuring the physical system ( double slit experiment , for example ) have modified it . that is an " indirect proof " in some sense – one may always say that the actual " constructive " proof involved the actual particles/apparatuses that were used to measure the " which slit " information or any other observable .
this problem with $n$ point charges on a sphere is a famous problem in electrostatics known as the thomson problem . for large $n$ , it is in general an open problem still under active research . references : wikipedia . org mathworld . wolfram . com mathpages . com
the mmf due to a current is determined by the current through the surface bounded by the closed path along which the magnetic field is integrated . a closed path within a cross-section of a conductor with , say , a uniform current density , will have a non-zero mmf associated with it and thus , a non-zero magnetic field exists within the conductor .
the pairs of lines are the same phase and at the same voltage - they are really just a single thick wire split into two thinner ones . it is easier to install two smaller wires to double the current capacity than a single thicker wire . it is easier to handle the lighter cable and you can stock just a single gauge of wire and handling equipment . it also provides some redundancy if one wire fails . there is an effect with ac electricity that the current mostly flows near the surface of the conductor . a number of thinner wires have more area-near-the-surface and so a larger effective cross section area than a single thick one . at the 50/60hz frequencies used by ac transmission this only affects wires more than a cm thick .
permittivity $\varepsilon$ is what characterizes the amount of polarization $\mathbf{p}$ which occurs when an external electric field $\mathbf{e}$ is applied to a certain dielectric medium . the relation of the three quantities is given by $$\mathbf{p}=\varepsilon\mathbf{e} , $$ where permittivity can also be a ( rank-two ) tensor : this is the case in an anisotropic material . but what does it mean for a medium to be polarized ? it means that there are electric dipoles , that units of both negative and positive charge exist . but this already gives us an answer to the original question : there are no opposite charges in gravitation , there is only one kind , namely mass , which can only be positive . therefore there are no dipoles and no concept of polarizability . thus , there is also no permittivity in gravitation .
the lorentz transformation you used is a lorentz transformation for a fixed ( constant ) $v$ , the relative velocity between two frames , so by the construction $dv=0$ . frames with non-constant velocities are not " inertial " and they do not belong to special relativity . you may still measure the proper time along an accelerating world line but the coordinate system in which the accelerating observer would always have $x=0$ is not inertial , so it is not good for a simple formulation of the laws of physics in special relativity . the proper time of an accelerating observer is simply obtained by dividing his world line to infinitesimal pieces and summing ( integrating ) their proper time . it can not be computed without an integral which is complicated in general . because you have not considered any integrals of complicated functions , you have not been ( correctly ) calculating the proper time of an accelerating observer , so considering $dv\neq 0$ could not have served any useful purpose . moreover , the very idea to differentiate the lorentz transformations is a bit redundant . without a loss of generality , you could have additively shift the coordinates so that $0=x=t=x'=t'$ around the point you are interested in . when you do so , $dx , dt , dx ' , dt'$ are really nothing else than $x , t , x ' , t'$ that are just assumed to be infinitely small . the time dilation is then simply derived by setting e.g. $x=0$ which reduces the first equation to $t'=\gamma t$ . it is really the same thing as $dt'=\gamma \cdot dt$ when $t , t'$ are infinitesimal . the $\gamma$ factor is on the proper side of the equation because $x=0$ means that the unprimed coordinates are those for which $x=0$ is the world line of the moving object . that means that $t$ is the time measured in the rest frame of the moving object and indeed , we have $\tau_{\rm proper}=t=t'/\gamma$ where $t'$ is the time measured from some other frame . ( of course , the same calculation may be done passively and/or with the opposite interpretation of primed and unprimed systems – but one is not allowed to mix these two approaches inconsistently . )
to remain on the merry-go-round , the person must be accelerated towards the center . how is the force applied to the person to provide that acceleration ? the static friction provides a way for the floor of the merry-go-round to force the person along the circular path . if the floor were to suddenly become frictionless and the person was not otherwise attached , the person would continue moving along a line tangent to the merry-go-round .
a quantum of em radiation has energy $e=hc/\lambda$ . for a number state , the energy is $e=nhc/\lambda$ . to find the power this corresponds to , imagine that all of those excitations are being generated in a laser in a time $\tau$ . the rate of energy production ( the power ) is then $p=nhc/\lambda\tau$ . the average occupation number of a coherent state is $\left&lt ; n\right&gt ; = |\alpha|^2$ . this gives us $$p=\frac{|\alpha|^2hc}{\lambda\tau}$$ now take as an example a laser pointer operating at 534 nm , having a power of 1 mw . take $\tau$ = 1 s in order to get a proper s.i. unit ( joules per second ) . if i set my calculator on that i get $\alpha\approx 5\times10^7$ . ( check my work on that . )
it is true quite generally ( for seperable hilbert spaces at least ) . the original proof is in " proof of the strong subadditivity of quantum mechanichal entropy " , by lieb and ruskai ( j . math . phys . 14 , 1938–1941 ( 1973 ) ) . the main idea is to prove the finite-dimensional case , and then extend it by taking a limit of the inequality on finite-dimensional subspaces ( which is why seperability is needed ) .
no . of course , to argue if a definition applies , we must first agree on a definition . wikipedia gives this one : a crystal or crystalline solid is a solid material whose constituent atoms , molecules , or ions are arranged in an orderly repeating pattern extending in all three spatial dimensions . humans are certainly solid-ish , and our constituent molecules are arranged in a somewhat orderly pattern in all three dimensions . however , i think we fail the ' repeating ' portion of this definition . if you want to use a more broad definition of a crystal , link to it . finally , supposing the answer was ' yes ' . what are the practical predictions which follow from this assertion ? we certainly do not diffract x-rays into a regular grid , for example .
but why is not sr taught with an imaginary time coordinate as standard ? from " gravitation " , page 51 , via google books . i will type up a paraphrase later .
the earth goes around the sun kind of like a ball on a string goes in a circle when you swing it around . instead of the string holding the earth , the sun 's gravity holds it . as the earth goes around the sun , it also spins . this makes day and night . you can see this with a flashlight on the ball when you spin it with your hand . part 2: since gravity is a little flexible , in a sense , the earth 's orbit around the sun is not a perfect circle . sometimes we are closer to the sun , and sometimes we are farther away . the earth 's spin is not directly lined up with the sun . the earth is tilted . so sometimes , the sun hits our part of the earth more directly . when this happens , we get more heat and that makes summer . that is why in summer the sun is higher in the sky at noon .
the factor of two is coming from the place you identified . think about throwing out that factor of two , so you are considering only the bottom hemisphere . when you make your gaussian shell and have it enclose charge in the bottom hemisphere only , the charge is no longer uniformly distributed inside your gaussian shell . thus , the electric field created by the charge you are considering is not the same at all parts of the shell , so you can not find the magnitude of the electric field in the way you described . that only works when the charge distribution has some sort of symmetry you are exploiting . you had have to do a difficult integral instead . however , if you do not throw out that factor of two , you are simply finding the electric field inside the shell . suppose you carry out the rest of your calculation . then you have found the net force in the z-direction in the north half of the sphere . however , the north half cannot exert any net force on itself , so this entire net force must be the same as the net force from the southern hemisphere . so you are including all the charge when you make your gaussian surface because you need to find the true electric field in the shell . the true electric field , when integrated , gives you the net force , which by basic mechanics arguments must be due to the southern hemisphere .
no , this is talking about correlations between s random particles . the s-particle distribution function is a 2*d*s ( so 6s in 3 dimensional space ) dimensional pdf that statistically describes s particles . for s=1 , this is just the normal density in phase space . for s=2 , this might show , for example , that more often than not two particles are traveling away from each other ( maybe they just collided ) . a good source for this is ch . 2 in " the statistical physics of particles " by mehran kardar .
the fraction of baryonic matter to dark matter is not deduced only from galactic dynamics . it is also derived from big bang nucleosynthesis and from the higher multipole acoustic peaks in the cmb spectrum . i would say that the element abundance is a far more important indicator of the fraction between baryonic and dark matter . big-bang nucleosynthesis theoretical overview of cosmic microwave background anisotropy : 1.2 . results
the resistivity of any material is related to the mobility of the charge carriers within it by : $$ \rho = \frac{1}{ne\mu} $$ where $\mu$ is the mobility , $e$ is the electronic charge and $n$ is the number density of charge carriers . i have deliberately used the term charge carriers rather than electrons because in semiconductors like diodes the carriers can be holes as well as electrons . obviously the electron charge $e$ is a constant , so changes in the resistance arise either from changes in the mobility or the carrier density . in a metal neither of these change with applied voltage , so in a metal the resistance is independant of voltage . by contrast in a reversed biased diode the carrier density is not constant . as you increase the voltage you increase the energy of the electrons flowing across the depletion layer . at some point the energy gets high enough to excite bound electrons into the conduction band and we get an avalanche beakdown . the carrier density rises rapidly and hence the resistivity falls rapidly , so in a diode the resistance is strongly dependant on voltage . this argument applies to any material . if you see a resistance that varies with voltage , temperature or whatever else , this will be due to changes in the carrier mobility and/or density .
photoelastic constant i have typically seen this as part of the optical property list of glass or other optical materials . it predicts the birefringence . it is also called the stress-optic constant ( defined in mueller , the theory of photoelasticity , 1938 ) : $$ b=\frac{n_p - n_n}{p} $$ where $p$ is the pressure , $n_n$ is the index of refraction normal to the direction of pressure ( with a ${\bf{k}}$-vector normal to the pressure direction ) , and $n_p$ is the index of refraction parallel to the pressure direction . this equation is for a non-crystalline material , you get two constants and two equations for a uniaxial crystal for example . photoelastic coefficient i found a few papers where this was used to mean photoelastic constant . i also found a definition for photoelastic coefficient in properties of group-iv , iii-v and ii-vi semiconductors by adachi : $$ \alpha_{pe} = \frac{\delta\epsilon_{ij}}{x} $$ for a uniaxial crystal where "$\delta\epsilon_{ij}$ is the change in the dielectric constant parallel and perpendicular to the direction of stress $x$ . " in this definition they are using the dielectric constant ( aka the permittivity ) instead of the index of refraction . this means that the units will be different between the two definitions , but they are basically measuring the same thing in the optical regime ( where $\mu \approx 1$ , $\mu$ is the permeability ) . a lot of materials ( other than glasses ) papers seem to use this definition . acousto-optic coefficient the only reference that i could find to the above term was in a conference paper for cleo/pacific rim 2001 : sound field measurement through the acousto-optic effect of air by using laser doppler velocimeter by nakamura . this paper is so terse that i cannot figure out what he is actually calling the acousto-optic coefficient . i have never heard this term before . i found another definition from a thesis from moscow state university here : morozov thesis $$ \gamma = \frac{\partial n}{\partial p} $$ where $n = n ( p ) $ is the pressure dependent index of refraction and $p$ is the pressure , which is the differential case of the stress-optic coefficient .
this is the lcao approximation i.e. approximating the wavefunction of a hydrogen molecule as the sum/different of the two atomic orbitals . if $\phi_a$ is the wavefunction of one hydrogen atom and $\phi_b$ the wavefunction of the other then we guess that the wavefunction of the hydrogen molecule can be approximated as : $$ \psi_{h_2} = c_1\phi_a + c_2\phi_b $$ where $c_1$ and $c_2$ are constants to be determined . from symmetry we know that the ground state must have $c_1$ and $c_2$ equal in magnitude , so the only possibilities are $c_1 = c_2$ and $c_1 = -c_2$ . that is why we can write the two molecular wavefunctions as : $$ \psi_+ = c\phi_a + c\phi_b = c \left ( \phi_a + \phi_b \right ) $$ and : $$ \psi_- = c\phi_a - c\phi_b = c \left ( \phi_a - \phi_b \right ) $$ where i have dropped the subscript $1$ because it is not needed . the molecular orbitals $\psi_+$ and $\psi_-$ look like : to actually calculate the value of $c$ you use the fact that $\psi$ must be normalised so : $$ c = \frac{1}{\sqrt{2 ( 1 + s ) }} $$ where $s$ is the overlap integral $\langle\phi_a|\phi_b\rangle$ .
your process will be reversible only if it is a ) quasi-static and b ) non-dissipative . it will be quasi-static if it is carried out infinitely slowly in such a manner that the pressure on either sides of the piston varies only infinitesimally . it will be non-dissipative if the piston is frictionless and there is no viscous heating of the gas as it expands .
the word comes from the ancient greek meaning " pertaining to man ; " ' man ' here means human . the etymologyonline dictionary is helpful . the anthropic principle is so-named because it is fundamentally based on the fact of a human observer .
the velocity of the orbiting space junk is a vector , with both a radial and a tangential component . $$\vec{v}_f = \dot{r}_f\hat{r} + r_f\dot{\theta}_f\hat{\theta}$$ ( my $r_f$ is your $r$ ) the equation for conservation of angular momentum involves only the tangential component of velocity , because it comes from the cross product of the radius vector and the velocity . $$\vec{l}_f = m\vec{r}_f\times\vec{v}_f = mr_f\hat{r}\times\bigl ( \dot{r}_f\hat{r} + r_f\dot{\theta}_f\hat{\theta}\bigr ) = mr_f ( r_f\dot{\theta}_f ) \hat{z} = mr_fv_f\hat{z}$$ but the equation for energy conservation involves both components . $$e_f = \frac{1}{2}mv_f^2 - \frac{gmm}{r_f} = \frac{1}{2}m\dot{r}_f^2 + \frac{1}{2}mr_f^2\dot{\theta}_f^2 - \frac{gmm}{r_f}$$ the combination $r_f\dot{\theta}_f$ corresponds to your $v_f$ , but the equation as you have written it in the question is missing the $\frac{1}{2}m\dot{r}_f^2$ term , which corresponds to the energy of motion toward or away from the moon . at apapsis and periapsis ( furthest and closest points of the orbit ) , $\dot{r}_f = 0$ momentarily , so you can ignore this term , as is done in the problem you are asking about . but for other points on the orbit , that term is nonzero , which means you have an extra variable . that prevents you from finding a unique solution without specifying which point in the orbit you are at .
having given it some more thought , there is an unambiguous philosophical difference , with practical implications . the two-slit experiment provides a good example of this . in a classical universe , any particular photon that hits the screen either went through slit a or slit b . even if we did not bother to measure this , one or the other still happened , and we can meaningfully define p ( a ) and p ( b ) . in a quantum universe , if we did not bother to measure which slit a photon went through , then it is not true that it went through one slit or the other . you might say it went through both , though even that is not entirely true ; all we can really say is that it " went though the slits " . ( asking which slit a photon went through in the two-slit experiment is like asking what the photon 's religion is . it simply is not a meaningful question . ) that means that p ( a ) and p ( b ) just do not exist . here 's where one of the practical implications comes in : if you do not understand qm properly [ i am lying a bit here ; i will come back to it ] then you can still calculate a probability that the particle went through slit a and a probability that it went through slit b . and then when you try to apply the usual mathematics to those probabilities , it does not work , and then you start saying that quantum probability does not follow the same rules as classical probability . ( actually what you are really doing is calculating what the probabilities for those events would have been if you had chosen to measure them . since you did not , they are meaningless , and the mathematics does not apply . ) so : the philosophical difference is that when studying quantum systems , unlike classical systems , the probability that something would have happened if you had measured it is not in general meaningful unless you actually did ; the practical implication is that you have to keep track of what you did or did not measure in order to avoid doing an invalid calculation . ( in classical systems most syntactically valid questions are meaningful ; it took me some time to come up with the counter-example given above . in quantum mechanics most questions are not meaningful and you have to know what you are doing to find the ones that are . ) note that keeping track of whether you have measured something or not is not an abstract exercise restricted to cases where you are trying to apply probability theory . it has a direct and concrete impact on the experiment : in the case of the two-slit experiment , if you measure which slit each photon went through , the interference pattern disappears . ( trickier still : if you measure which slit each photon went through , and then properly erase the results of that measurement before looking at the film , the interference pattern comes back again . ) ps : it may be unfair to say that calculating a " would-have " probability means that you do not understand qm properly . it may simply mean that you are consciously choosing to use a different interpretation of it , and prefer to modify or generalize your conception of probability as necessary . v . moretti 's answer goes into some detail about how you might go about doing this . however , while this sort of thing is interesting , it does not appear to me to be of any obvious use . ( it is not clear that it gives any insight into the disappearance and reappearance of the interference pattern as described above , for example . ) addendum : that has become clearer following the discussion in the comments . it seems that it is thought that the alternative formulation may have advantages when dealing with more complicated scenarios ( qft on curved spacetime was mentioned as one example ) . that is entirely plausible , and i certainly do not mean to imply that the work lacks value ; however , it is still not clear to me that it is pedagogically useful as an alternative to the conventional approach when learning basic qm . pps : depending on interpretation , there may be other philosophical differences related to the nature or origin of randomness . bayesian statistics is broad enough , i believe , that these differences are not of any great importance , and even from a frequentist viewpoint i do not think they have any practical implications .
half life variations have been suspected for decades , and almost all ( maybe all…but i do not pretend to have a comprehensive knowledge ) have been shown to be caused by limits in experimental design . this latest set from this ( now expanded ) group ( they did another paper on this topic a couple of years ago http://arxiv.org/ps_cache/arxiv/pdf/0808/0808.3283v1.pdf ) is interesting . any periodic variation in decay rates is of great importance regardless of whether it correlates to some astronomical phenomenon . i guess that is a good setup for a “i’m not really going to answer your question , but” type answer . the 2008 paper was followed by a set of measurements from the cassini probe ( http://arxiv.org/ps_cache/arxiv/pdf/0809/0809.4248v1.pdf ) . the distance between that probe and the sun was such that any sun effect should have been magnified . none was seen . this may disprove the solar effect , or it could say something about the isotopes used . the cassini isotopes were different than the purdue group’s isotopes . the isotopes they used in the paper you cite were interesting : mn 56: 2.6 hr , si 32: 101 yr , ra 226: 1,600 yr . from a quick scan of the plots it looks like the longer the half-life the greater the adherence to the annual periodicity . i also notice that the bulk of the data was accumulated before 1990 , and the data after 1996 does not show the same adherence to the annual periodicity . i’m trying not to discount these decay-rate observations , and there are good reasons to not discard them out-of-hand . most observations of constant decay rates have been made with rapidly decaying isotopes over a short time frame . the experiments are just much easier to set up . very long term experiments are relatively rare , and could illuminate previously unobserved phenomena . if the periodicity is on the order of a year it should only take a couple of years to confirm the data using available methodologies .
assuming non-relativistic velocities , the power radiated by a charge accelerating at constant acceleration $a$ is given by the larmor formula : $$p = \frac{e^2 a^2}{6\pi \epsilon_0 c^3} $$ to do the calculation properly is surprisingly complicated , but it is easy show that the effect of the radiation on the electrons fall is negligible . if the electron falls a distance $h$ then the time it takes is given by : $$ h = \frac{1}{2}gt^2 $$ so : $$ t = \sqrt{\frac{2h}{g}} $$ if we assume the electron is accelerating at a constant rate of $g$ , the total energy radiated is just power times time or : $$ e_{rad} = \frac{e^2 g^2}{6\pi \epsilon_0 c^3} \sqrt{\frac{2h}{g}} $$ in your question $h$ is 1000m , so : $$ e_{rad} = 7.83 \times 10^{-51}j $$ the potential energy change is , as you say , just $mgh$: $$ e_{pot} = m_e g h = 8.94 \times 10^{-27} j $$ so the ratio of the radiated energy to the potential energy is about $10^{-24}$ , and therefore the effect of the radiation on the electron 's fall is entirely negligible . response to comment : the power radiated from the electron produces a force that opposes the acceleration due to gravity . assume we can ignore the deviations from accelerating at a constant rate $g$ , then in a small time $dt$ the energy radiated is $pdt$ . the energy is force times distance ( $dx$ ) so to get the force we divide by the distance : $$ f = p\frac{dt}{dx} = \frac{p}{v} = \frac{p}{\sqrt{2gh}} $$ using $v^2 = 2as$ . the acceleration produced by this force is just $f/m_e$ , so the net acceleration on the electron is : $$ a_{net} = g - \frac{p}{m_e \sqrt{2gh}} $$ so the electron does accelerate slightly more slowly than $g$ , but the difference between the acceleration and $g$ is inversely proportional to distance fallen so it gets increasingly negligible the further the electron falls . you have probably spotted that the above equation says the force should be infinite at the moment you release the particle . that is because as you approach the moment of release it is no longer safe to make the approximation that you can ignore the change in the acceleration due to radiation .
can we conclude from having same physics in all rest frames ( that are non accelerating ) [ . . . ] same time rates for particle decays , life span . . no . quite the contrary : from " having the same physics " for all participants ( beginning with the same physics being used for determining which participants are " at rest " to each other , and which are not ) we have , for instance , the means of comparing life spans of instable particles to each other , from trial to trial , in the first place ; without having to resort to any " foregone conclusions " or expectations , for instance about any life spans being equal , between any particular trials .
when a particle is deflected by gravity the gravitational field will also be modified by the particle . to form a conservation law for momentum you need to take into account the momentum in the gravitational field as well as the particle . this can be done e.g. using pseudo-tensor methods . this works but remember that momentum is a relative concept . even in newtonian dynamics it depends on the velocity of your reference frame . in general relativity it also depends on the frame but a much wider class of frames is valid . this means that momentum conservation depends on the choice of co-ordinates . locally you can pick an inertial reference frame but over extended regions there is no inertial frame . a momentum in one location cannot be simply added to a momentum vector in another location . nevertheless , momentum conservation laws over extended regions do work correctly in general relativity . for an extended description of the formalism and why it works see my article at http://vixra.org/abs/1305.0034 edit : in the comments below mwt p457 has been cited to support the idea that energy and momentum are only conserved in specific cases . i am adding this to directly refute what has been said there . mwt begin by saying that there is no such thing as energy or momentum for a closed universe because " to weigh something one needs a platform on which to stand to do the weighing " this is pure wheeler rhetoric of the type for which he is greatly admired , but in this case it is simply misleading . weight is a newtonian term with no useful counterpart in general relativity except in the specific case of an isolated system in an asymptotically flat spacetime . for other situations such as the closed cosmology energy and momentum conservation take a different but equally valid form . they go on to say that in a closed universe total energy or momentum or charge is " trivially zero " they justify that it is zero because you can use gauss 's divergence theorem to write the charge , energy or momenta as a boundary integral . for a closed universe the boundary disappears making the result zero . this is of course correct , but they give no justification for calling this answer trivial . energy and momenta are initially defined as a sum of volume integral contributions from each physical field including electromagnetic fields , fermionic fields , gravitational field etc . it is in this sense that we understand that conserved quantities can move and can transform from one form to another but the total remains constant . it is a property of gauge fields that when the dynamical field equations are used the conserved quantity is the divergence of a flux from just the gauge field so that it can be integrated over a volume and be calculated as a boundary surface integral . this gives charge/energy/momenta a holographic nature where they can be considered either as a volume integral over contributions from different fields of a surface integral over the gauge field flux . the important thing to understand is that to go from the volume to the surface form the field equations must be used . this means that the total charge/energy/momenta in a closed universe is zero but that this is not in any sense a trivial result . if you calculate total energy as a volume integral for a configuration of fields that do not satisfy the equations of motion the answer will not necessarily be zero . stating that it is zero is therefore making a non-trivial assertion about the dynamics . this is what conservation laws are all about . mwt go on to explain why it would make no sense to have an energy-momentum 4-vector globally . the invalid assumption they are making is that energy and momenta need to form a 4-vector . a 4-vector is a representation of the poincare group and is the natural form that energy-momentum takes in special relativity where poincare invariance is the global spacetime symmetry . in general relativity the global spacetime symmetry is diffeomorphism invariance so the correct expectation is that all quantities should take the form of a representation of the diffeomorphism group for the manifold . this is what happens . if you demand an energy-momentum 4-vector then of course you will only get an answer locally , and also for an asymptotically flat spacetime where poincare symmetry is valid at spatial infinity , but demanding such a 4-vector is simply the wrong thing to do in general relativity . in the fully general case of any spacetime we can apply noether 's theorem using invariance under diffeomorphism generated by any contravariant transport vector field ( observe that it is invariance of the equations that is required , not invariance of the solutions . some people like to confuse the two ) the result is a conserved current with a linear dependence on the transport vector field . this is the correct form for a representation of the diffeomorphism group . i refer to my cited paper for the mathematical details . this current gives conservation laws for energy , and momenta including generalizations of angular momenta as well as linear momenta depending on the transport vector field chosen . if it transports space-like hypersurfaces in a timelike direction it will give an energy conservation law and if it transports in spacelike directions it gives momenta conservation laws . these energy and momenta do not normally form 4-vectors but they can be integrated to give non-trivially conserved quantities . the global form a conservation law must take is that the total energy and momenta in a volume must change at a rate which is the negative of the flux of the quantity over the boundary , and this is what you get with the currents derived from noether 's theorem . it may be that other people will want to add comments here that dispute the validity of energy conservation in other ways . i refer once again to my new article at http://vixra.org/abs/1305.0034 where i refute all the objections that i have heard . triviality is dealt with in item ( 6 ) and 4-momentum is dealt with in item ( 8 ) . unless someone comes up with a novel objection i will just refer to the numbered objections in this paper in future . remember , there are no authorities in science and any expert may be shown to be wrong either by reasoning or by experiment .
seems good to me . you are right integrating only from 0 to $a$ because $\psi$ is zero in the region of infinite potential . the solution would be $\psi ( p ) =\frac{1}{\sqrt{a\pi h}}\int_o^ae^{-ipx/\hbar}\sin ( \pi x/a ) dx=\frac{\sqrt{a\pi \hbar^3 }}{\pi^2 \hbar^2 - p^2 a^2 } ( e^{-ipa/ \hbar }+1 ) $ as for the other question , that is what one typically does
long shot , since lack of context , but here is my attempt . any event can be described with 4 coordinates [x,y,z,t] , where [x,y,z] point in some coordinate system and [t] - synchronized clock at event point . wep is same thing as universality of free fall . the universality of free fall , states that all bodies fall with the same acceleration in a gravitational field , independently of their mass and composition , theory and experiment in gravitational physics , by clifford m . will that means trajectory depends only on its position and velocity . if we need to describe position there is no privileged coordinate system , that will give to us some additional information about event , each coordinate system differ only by some transformation . in same time we need to combine geometrical properties with physical , position and time , this can be done by use of tensor , which combine spatial position of event [x,y,z] and time . this technique also used in analysis of electromagnetic fields .
the conservation of energy means that the potential energy liberated in falling must be present as heat . but there are precisely three ways for the water to lose that heat conduction/convection : by contact with air radiatively : by emitting predominantly infrared light . evaporation : by losing vapor into the air as the water is falling . from experience sweating , it should be apparent that in a fast air-flow environment , evaporative cooling is much more important than the other two effects . but this list is exhaustive--- there is no other place you can put the energy liberated by the fall .
you are right . the field will indeed extend outside the electric conductor as well . however , since there are very few electrons affected outside the conductor , the field strength will quickly drop outside the conductor . however , inside the conductor , where the field does move many additional electrons , those movements will contribute to the field strength . the field strength is not directly related to its speed , though .
newton 's first law of motion for a point particle states that a particle at rest will stay at rest and a particle in motion will stay in motion unless acted on by an unbalanced force . in other words , if the net force on the particle is zero , then the velocity of the particle will stay constant . newton 's first law of motion for a system of particles states that if the net external force on a system is zero , then the velocity of the center of mass of the system will remain constant . it says nothing about the velocity of each of the particles . so if the center of mass of the rigid body is initially at rest , and there is no net external force , then the center of mass will continue to be at rest . but that does not mean that the individual parts of the rigid body will remain at rest . there is two ways to think about this . one way is to apply newton 's first law to each part of the rigid body : if two forces act on the rigid body , but they act on two different places , then one part of the rigid body will only experience one of the forces , so it can move . the other way to think about this is to use the angular version of newton 's first law : if the net external torque on a rigid body is zero , then the angular velocity of the rigid body is constant . since there is an external torque in our example , there is no requirement that the angular velocity must be constant , so the rigid body can rotate even though it was not rotating before .
in order to talk about diffusion you need at least two particle species ( something has to diffuse relative to something else ) . then the continuity equation refers to the total mass density of the fluid $$ \rho = \int d\gamma\ , m_1f_1 + \int d\gamma\ , m_2 f_2 $$ and the fluid velocity is defined by $$ \rho\vec{u} = \int d\gamma\ , m_1v_1f_1 + \int d\gamma\ , m_2v_2 f_2\ , . $$ mass conservation in the collision term ensures that $\partial_t\rho+\vec{\nabla} ( \rho \vec{u} ) =0$ and no dissipative fluxes appear . now we can define a concentration , e.g. $$ c = \int d\gamma\ , f_1 / \int d\gamma\ , f_2\ , . $$ the quantity $\rho c$ satisfies a more complicated continuity equation , with a diffusive current $\vec{\jmath}$ on the right hand side . this current satisfies ( to leading order in gradients ) fick 's law , $$ \vec{\jmath} = -\rho d\vec{\nabla} c + \ldots $$ where the $\ldots$ are thermal diffusion terms , and higher order gradients .
your pump needs to provide enough pressure to push the water all the way up to the deposit . that would amount to $\rho g h$ . if your inlet in the deposit is above water level , $h$ is measured to the inlet . if it is underwater , then it is measured to the water level . that pressure is enough to keep the water from backflowing , but not to push any more water up to the deposit . to do so , you will need additional pressure , that will go into kinetic energy of the water moving through the pipe , $\frac{1}{2}\rho v^2$ , and into overcoming friction losses in the pipe , which can be calculated with the darcy-weisbach equation . the velocity of the water in the pipe can be figured out from the pipe diameter and the mass flow being sent through it .
the expression $$ k_b \frac{\omega}{\bar{\omega}} $$ equals $$ k_b\frac{1}{\bar{\omega}}\frac{d\bar{\omega}}{de} $$ which equals $$ \frac{ds}{de} . $$ in thermodynamics , where $s$ is the clausius entropy , this is equal to $1/t$ where $t$ is the kelvin temperature . in statistical physics , this expression can be taken as a definition of $1/t$ of a system from the microcanonical ensemble $e , a$ .
using this from my book ( physics for scientists and engineers with modern physics 9th ed ) : if more than one force acts on a system and the system can be modeled as a particle , the total work done on the system is just the work done by the net force . if we express the net force in the x direction as o fx , the total work , or net work , done as the particle moves from xi to xf is σw = the integral of σf from xi to xf a proof that d ) the speed of the particle must be unchanged is true : if σw = the integral of σf from xi to xf = 0 , then δx = 0 or σf = 0 ( this alone is proof that e ) there must be no displacement is false ) . in the case that δx = 0 , the particle is not displaced and the speed is unchanged . if σf = ma0+ma1+ . . . + man = m ( σa ) = 0 and m ≠ 0 , then σa = 0 and the speed is again unchanged . therefore , if σw = 0 , then it is necessarily true that the speed remains unchanged .
to quote a comment scott aaronson made on his blog : can you perform an arbitrarily long computation with minimal effort , by leaving your computer on earth , boarding a spaceship that accelerates to close to the speed of light , then turning around and returning to earth , where you find civilization collapsed , your friends long dead , and the sun going cold , but your important computation finished ? here , as i like to point out in talks , the crucial problem is the energy needed to accelerate to relativistic speed . indeed , if you want to get a superpolynomial speedup by the above means , it’s not hard to show that you need to accelerate your spaceship to faster than c-1/p ( n ) for any polynomial p . but that , in turn , requires a superpolynomial expenditure of energy ( assuming , of course , that you have nonzero mass ! ) . so , as long as we make the reasonable ( and separately justifiable ) assumption that in t seconds you can only collect poly ( t ) joules of energy , the extended church-turing thesis once again seems safe . to summarize : yes , physically possible . no , probably not more useful than regular computation . black hole computers are often lumped in with the rest of relativistic computers , but i am not so clear about what is the problem with them .
take for example $\mathcal{n}=2$ supersymmetry . the algebra , including a central charge $z$ , is given by $$\{q_\alpha^a , q^\dagger_{\dot{\alpha}b}\}=2\sigma^\mu_{\alpha\dot{\alpha}}p_\mu \delta^a_b$$ $$\{q_\alpha^a , q_\beta^b\}=2^{3/2}\epsilon_{\alpha\beta}\epsilon^{ab}z$$ $$\{q^\dagger_{\dot{\alpha} a} , q^\dagger_{\dot{\beta} b}\}=2^{3/2}\epsilon_{\dot{\alpha}\dot{\beta}}\epsilon_{ab}z . $$ we can now define $$a_\alpha=\frac12\left ( q_\alpha^1+\epsilon_{\alpha\beta}\left ( q_\beta^2\right ) ^\dagger\right ) $$ and $$b_\alpha=\frac12\left ( q_\alpha^1-\epsilon_{\alpha\beta}\left ( q_\beta^2\right ) ^\dagger\right ) , $$ which reduces the algebra to $$\{a_\alpha , a_\beta^\dagger\}=\delta_{\alpha\beta} ( m+\sqrt{2}z ) $$ and $$\{b_\alpha , b_\beta^\dagger\}=\delta_{\alpha\beta} ( m-\sqrt{2}z ) . $$ a bps-state satisfies $m=\sqrt{2}z$ , hence the second part of the algebra reduces to $$\{b_\alpha , b_\beta^\dagger\}=0 . $$ this tells us that the operators in the second half of the algebra , which now vanishes , only generate states of zero norm . this is how you should understand the statement you were asking about .
there are two equivalent descriptions$^1$ of the reduced two-body problem with a central potential $v ( r ) $: in an inertial frame with no fictitious forces : here $\frac{1}{2}\mu r^{2}\dot{\theta}^{2}$ is the angular part of the kinetic energy . in a rotating frame following the reduced particle with fictitious forces and only 1d radial kinematics : here $-\frac{1}{2}\mu r^{2}\dot{\theta}^{2}$ is the centrifugal potential ( notice the minus sign ! ) . each description leads to the same lagrangian $$l~=~t-u~=~l=\frac{1}{2}\mu ( \dot{r}^{2}+r^{2}\dot{\theta}^{2} ) -v ( r ) . $$ $^1$ the reduced particle is confined to an orbit plane , so the problem is effectively just two-dimensional described by two coordinates $r$ and $\theta$ . in both descriptions , the center-of-mass serve as the origin of the reference frame .
assuming your pipette has a volume of 15ml , the error in measuring the volume of your sample is 0.02 in 15 i.e. 0.133% . the weight of 15cc of ethanol is 0.789 $\times$ 15 = 11.835g . the error in measuring the weight is 0.002 in 11.835 i.e. 0.025% . hedge physicists like me would immediately note that the error in the weight is a lot lower than the volume , so we can ignore it and just take the error in the density to be 0.13% . however , for the sake of the exercise let 's do this thoroughly . the density is given by : $$ \rho = \frac{m}{v} $$ so the percentage error in the density , $\sigma_\rho$ is given by : $$ \sigma_\rho = \sqrt{\sigma_m^2 + \sigma_v^2} $$ where $\sigma_m$ and $\sigma_v$ are the percentage errors in the weight and volume respectively . this gives : $$\sigma_\rho = 0.136\%$$ so the absolute error in the density of 0.789 would be 0.136% of 0.789 or 0.0011 . there is one last step to do : to distinguish the two fluids we need to measure their densities ( with an error of 0.0011 ) then subtract the measurements . because we are subtracting two measured densities , each with an error of 0.0011 , the error in the result is given by an expression similar to the one above : $$ \sigma_{diff} = \sqrt{0.0011^2 + 0.0011^2} = 0.0016 $$ note that this time we are using absolute errors not percentage errors . when you are multiplying or dividing you combine the percentage errors and when you are adding and subtracting you combine the absoluite errors . anyhow , the difference in the densities is 0.789 - 0.785 = 0.004 , so our result would be 0.004 plus or minus 0.0011 . assuming the errors quoted are the 1$\sigma$ errors , the error in the result is 4$\sigma$ , so we had be 98% confident we could tell the difference .
tl ; dr they shift but only if you have non perfect system , phase difference is compensated in a perfect system . first how to get no shift . imagine it is an ideal transformer . you apply the induction law once and get the $b ( t ) $ . $$u ( t ) =\int_\ell e ( t ) \mathrm{d}\ell=-\frac{\mathrm{d}b ( t ) }{\mathrm{d}t} $$ now the $b ( t ) $ is shifted in relation to $u ( t ) $ because of the derivation , but when we do the calculation of $\hat u$ we shift it back : $$\hat u ( t ) =-\frac{\mathrm{d}b ( t ) }{\mathrm{d}t} $$ so two shifts in opposite directions give zero phase difference . perfect . ( you should now think : but the second conductor influences the first . . . in some way , so i think this may be wrong ! that is what i was thinking , then i got over it . explanation : you would use the same formula over and over again ( ping-pong ) always getting net zero shift . ) if you really want to have a shift , you can look at it as a non-ideal transformer ( if you like add capacitances ) . $$\hat u ( t ) =\hat l\frac{\mathrm{d}\hat i ( t ) }{\mathrm{d}t}+ m\frac{\mathrm{d} i ( t ) }{\mathrm{d}t}+r\hat i$$ $$u ( t ) =l\frac{\mathrm{d} i ( t ) }{\mathrm{d}t}+ m\frac{\mathrm{d}\hat i ( t ) }{\mathrm{d}t}+ri$$ for a constant frequency $\omega$ you can write : $$\hat u ( t ) =j\omega \hat l \hat i+ j\omega m i+r\hat i$$ $$u ( t ) =j\omega l i+j\omega m\hat i+ri$$ if you know how to solve simple circuits these equations should not be a problem for you . the question remains , what are the values of $\hat l , m$ and $l$ ? the derivation of these formulas is something i can not remember , but you will find it for sure somewhere . the resistances can be calculated easier . what would happen if the resistances were the same ? if you add the capacitances you will have a characteristic impedance rating . even more fun stuff to think about !
i did not find the equation and the argument you quoted in that paper . but , yes , it is the brouwer degree , deg$ ( \hat{\phi} ) $ , which equals the monopole number $$ n\equiv\frac{1}{4\pi}\int_{\mathbb{r}^3} \mathrm{tr} ( f_a\wedge d_a ( \phi ) ) =\frac{1}{4\pi}\int_{\mathbb{r}^3} d ( \mathrm{tr} ( \phi ) f_a ) =\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr} ( \phi f_a ) $$ $$ =\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr} ( \hat{\phi} f_a ) $$ where the one has used bianchi identity , stokes ' theorem , to obtain the first two equalities , and jaffe and taubes show in their book that one can replace $\phi$ by $\hat{\phi}$ . now this coincides with the brower degree , for which there is an explicit formula : $$n=\mathrm{deg} ( \hat{\phi} ) =-\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr ( \hat{\phi} d\hat{\phi}\wedge d\hat{\phi}} ) \in \mathbb{z}=\pi_2 ( s^2 ) = [ s^2 , s^2 ] $$ ( what you wrote . ) this is physically understood as an infinite wall potential , separating the monopole sectors corresponding to different integers . now , to actually answer your question , you can compute this integral for the t'hooft-polyakov monopole solution , for which $$ \hat{\phi}= ( \sin ( \theta ) \cos\phi , \sin\theta \sin\phi , \cos\theta ) _i\cdot \sigma^i , $$ and you will find $$n=-\frac{1}{4\pi}\int_{s^2_\infty} \mathrm{tr ( \hat{\phi} d\hat{\phi}\wedge d\hat{\phi}} ) =+\frac{1}{4\pi}\int_{ [ 0,2\pi ] } \int_{ [ 0 , \pi ] }\sin\theta d\theta\wedge d\phi=1 . $$
there is much more matter in the interstellar medium than in the visible stars . our best estimate of the total matter/energy content of the universe is shown in this image from nasa : now the dark energy is not really matter - it acts like an anti-gravitational vacuum energy which is responsible for the accelerating expansion of the universe . the dark matter is probably some kind of matter that does not have any strong nuclear or electromagnetic interactions with our ordinary ( baryonic ) matter ( atoms ) . it may possibly only have gravitational interactions , but all the current searches for dark matter assume that it also has a weak nuclear interaction with ordinary matter . so that leaves $4.6\%$ of the universe that is made from atoms . it is believed that only about $\frac{1}{2}\%$ of the mass of the universe is in the form of the atoms that are inside stars ( and planets ) . so for every atom in a star , there are 9 atoms in the interstellar medium . some of the atoms in the interstellar medium would be gas clouds inside of galaxies and some of it would be the diffuse interstellar gas between galaxies and even clusters of galaxies . the amount of mass/energy in black holes is a more difficult question ( and about which i know much less ) . some of the atoms have already collapsed into black holes such as the super-massive black holes in the center of most galaxies and the black holes that may result when stars go supernova . the super-massive black holes in the centers of galaxies are still only a very small fraction of the total mass of the galaxy so all of these " known " black holes are probably only a small fraction of the $\frac{1}{2}\%$ of the atoms that are in the form of stars . however , there could be some primordial black holes left over from the big bang . these primordial black holes could constitute part of the dark matter of the universe . most standard cosmological theories do not predict any ( or many ) primordial black holes and there have been some searches for them with gravitational lensing . in this area , i know much less , but i believe these primordial black holes cannot be a big fraction of the dark matter , but i do not know what the upper limit is precisely .
this should be a comment as i am not knowledgeable about the lhc refrigeration , but it is too long for a comment and i can hazard a pretty good guess . sheer heat capacity likely accounts for a great deal of this time . assuming the total amount of kit that needs to be cooled is , say $2\times10^4$ tonnes , and if it has roughly a $1{\rm kj k^{-1} kg^{-1}}$ heat capacity , that means we have to extract $20{\rm gj}$ for every degree k that we cool . once we get down below $100{\rm k}$ were going to see some serious multipliers happenning . an ideal heat pump needs work input $w = q_{lhc} \left ( \frac{t_{out}}{t_{lhc}} - 1\right ) $ to pump out heat $q_{lhc}$ from the kit at temperature $t_{lhc}$ and dump it to the environment at $t_{out}$: this is the reversible heat pump . so if we are drawing this heat out and dumping it at $300k$ , say , the energy needed to get from $300k$ to $5k$ we get as a rough estimate ( assuming heat capacities stay constant , which they will not , but there will not be any phase changes of most of the kit ) : $$w_{total} = \sigma \int\limits_{t_{lhc}}^{t_{out}} \left ( \frac{t_{out}}{t} - 1\right ) \ , {\rm d}t$$ where $\sigma$ is the $20{\rm gj k^{-1}}$ total heat capacity i estimated above . plugging in the numbers $t_{lhc} = 5k$ ( not everything will need to be cooled all the way down to $1.9k$ ) and $t_{out} = 300k$ we get : $$w_{total} = \sigma \left ( t_{out}\left ( \log\left ( \frac{t_{out}}{t_{lhc}}\right ) -1\right ) +t_{lhc}\right ) = 933 \sigma \approx 20{\rm tj}$$ this is the total output of a $5{\rm gw}$ power station for over an hour , roughly the energy released by the first of the only two nuclear weapons brought to bear in combat . $5gw$ electricity generation is the electricity consumption of two million australians , and we are extremely greedy electricity users by world standards , so i do not know how many normal people this would represent . the lhc site quotes a peak power consumption of $180mw$ and about $30mw$ is used for cryogenics . $30{\rm tj}$ at $30mw$ is about ten days . another factor is stresses in the kit induced by too swift cooling or warming . i am slightly familiar with the design of some of the magnetic beamsteering hardware , and much of this kit is toleranced to within tens of microns . one can not brook even the tiniest of any irreversible , plastic deformations of the kit and still have it work properly . so it is likely that heat transfer could not go much faster than this even if the refrigeration capacity were there . there will also be economic considerations too . even if one can cool faster than with $30mw$ refrigeration and still meet technical / engineering constraints , refrigeration capacity is expensive , particularly if you are only using this full capacity for cooldown during maintenance . the rest of the time the refrigeration needs are much less , so you have an economic tradeoff between capital spent on capacity that is unused most of the time and the cost of project delays arising from downtime . i am absolutely sure exactly this calculation has been done , as it ones like it are done for all soundly managed engineering projects .
if you ask whether there is a phase difference of 90° between the electric field and the magnetic field , the answer is yes . the electric field and the magnetic field oscillate in quadrature . you can see that from the conservation of the total electromagnetic energy during one oscillation cycle , or as you mentioned it , to conserve the energy of a " single photon " during a cycle . plug quickly $$e = e_0 cos ( \omega t ) $$ and $$b = b_0 sin ( \omega t ) $$ in the instantaneous electromagnetic energy density ) , remembering that $$cos ( \omega t ) ^2 + sin ( \omega t ) ^2 = 1 $$ and $$ b_0 = e_0/c $$ in the present case of transverse electromagnetic waves propagating in free-space .
of course it is normal . most of the variation in your table is controlled by the leap years . note that in 75% of the years , the number of hours drops by $-0.08\pm 0.02$ hours relatively to the previous year ; it is really because the day is $365.24219-365 = 0.24219$ solar days closer to the summer than the day of the same date in the previous year . the exceptions are the first years after the leap years such as 2009 ; in those years , the number of hours moves in the opposite i.e. positive direction , by $+0.25\pm 0.03$ or so relatively to the previous year , in order to approximately compensate the drop in the previous three years . it is because the leap day such as february 29th , 2008 is in between . compare the februaries 22nd in 2008 and 2009 , the most extreme ones in your table . in 2009 , the day had 5.85 hours of light but in 2008 , it was just 5.60 hours . a significant difference . why is it so ? it is because the day in 2009 is more summer-like than the day in 2008 , by almost one day . try to express february 22nd as a day in march . in 2008 , march 0th was february 29th so february 22nd was march -7th . however , in 2009 , march 0th was february 28th so february 22nd was march -6th . the number $-6$ is larger than the number $-7$ so it is later in the season , closer to the summer by 3/4 of a solar day or so ( if i compute the most appropriate weighted compromise between the schemes to describe the day as a day in february or day in march ) , which is why there was more light on february 22nd , 2009 . to treat the variations above properly , you need to understand that years divisible by 100 are not leap years unless they are multiples of 400 as well . for a discussion of leap years and the length of the years in solar days , see e.g. http://motls.blogspot.com/2012/01/happy-new-leap-year-2012.html this 4-100-400 rule makes the average year 's length in solar days to be 365.2425 , close to the actual figure close to 365.24219 ( one day of deviation accumulates in 3,000+ years or so with our rules ) but the number 365.24219 is somewhat variable anyway so it does not make sense to define the leap years more accurately than the 4-100-400 rule does . if you deal with the leap years correctly , it will still leave some variations . there are interannual irregularities in the earth 's motion due to the gravity and mass of the moon and due to the attraction from ( "randomly located" ) jupiter and other planets as well as very slow corrections taking thousands of years due to precession of the earth 's axis and changing eccentricity of the " approximately elliptic " orbit of the earth ( some of these variations of eccentricity were already counted with jupiter etc . ) . the solar system is very close to a set of regularly moving planets but it is not quite accurate due t othe internal dynamics of the gyroscopes , deviations of the positions of the planets due to their moons , and due to the gravitational influence of other bodies different from the sun . incidentally , wolfram alpha seems to yield significantly shorter days in svalbard than you but the logic does not change . in wolfram alpha , the day is between 4.9 and 5.15 hours or so : http://www.wolframalpha.com/input/?i=length+of+day+february+22nd%2c+2008+in+svalbard http://www.wolframalpha.com/input/?i=length+of+day+february+22nd%2c+2009+in+svalbard but maybe the difference is due to the large size of svalbard and different latitudes picked by you and wolfram alpha . aside from the impact on the calendar and leap years , irregularities and non-integralities in the motion of the earth also influence the fact that one solar day is not exactly 86,400 seconds as most of our clocks assume . with a more accurate definition of one second we have today – linked to atomic clocks – the typical solar day is usually a bit longer . in order to keep the highest-sun moment near 12:00:00 , people got used to insert leap seconds , see http://motls.blogspot.com/2012/01/leap-seconds-may-be-abolished-in-2015.html since 1972 , 25 leap seconds have been inserted . this literally means that at the end of june or december , some most accurate of atomic clocks show an excessive , otherwise non-existent time 23:59:60 for a second . some officials are negotiating possible cancellation of this policy . this would have advantages as well as disadvantages .
suggestion to the question ( v3 ) : generalize the question to a $1/r^s$ potential law in $n$ spatial dimensions ! then according to henry cohn 's mathoverflow answer here , the charges rush to the boundary iff $s\leq n-2$ . so in op 's example $ ( s=2 , n=3 ) $ , the charges do not rush to the boundary , in contrast to the real world $ ( s=1 , n=3 ) $ .
the light from most sources spreads out in all directions so there is a straight line from any point on the surface to the source and a photon heading out from the source in that direction would hit that point . if you have a source which only emits light in one direction , like a laser , then only a small part of the surface is illuminated and you only see a spot of light
nothing will change with maxwell 's equations due to the discovery of the higgs bosons . maxwell 's equations describe the continuum of electromagnetic theory and know nothing about particle physics .
victor stenger 's stuff is vague unquantitative musings with no significant value , about back and forth in time particle paths and such things . it is not important to this question , and it is not important for anything at all . if quantum mechanics is deterministic underneath , and the number of variables is reasonably bounded , for example a number of bits less than the area of the cosmological horizon in planck units , about $10^{138}$ , then the the exponentially growing search ability of a quantum computer will crap out . the point at which factoring necessarily fails can be easily estimated from the number of independent multiplications performed in shor 's algorithm . to do $10^n$ multiplications in superposition at once , you need $10^n$ classical states ( it is actually nlogn in the exponent , but same difference ) , so when n is larger than around 130 , so around 130 digit numbers , you exceed this bound . since the entire universe can not be expending all it is effort on your dinky quantum computer , the actual bound will be closer to 100 digits , on the same the order of the numbers used for cryptography today . to be on the absolute safe side , perhaps nature realizes something about the multiplications and reduces the operations accordingly , you should have n to some power less than 1 in the exponent , because of redundancies , and you should have n to some power less than 1 in the exponent . this power cannot be as small as 1/2 , because the best classical algorithm scales exponentially in a higher exponent than this , and nature is not as clever as mathematicians trying hard to do factoring . so you get a firm bound at around 10,000 digits , and a quantum computer factoring larger numbers than this is definite evidence that quantum mechanics is exponentially large . if you do not say there is a limit to the amount of classical computation in the deterministic model , if you allow exponentially large deterministic descriptions , you can mimic quantum mechanics exactly using bohmian mechanics . you make a lattice description of some field theory , you make a wavefunction for the fields , and you have definite field values that wander around according to the bohmian quantum force law . this type of thing can not be ruled out experimentally , but the description is not unique , it is different depending on which fields you choose to make bohmian , and in which time-slicing you define the quantum-force dynamics , so it is certainly not the right description . bohmian mechanics might as well be quantum mechanics , which is why bohm 's theory is more often called an interpretation of quantum mechanics rather than a new theory . there is no point in asking whether it is deterministic underneath if shor 's algorithm works , the determinism in this case would be the universe simulating quantum mechanics and fooling us into thinking it is correct . in this case , you might as well believe quantum mechanics is exact with no substructure . so the only options , modulo philosophy , are that quantum mechanics fails to factor 10,000 digit numbers , or that it succeeds and is exact . this a wonderful experimental distillation of the hidden-variable question , and for those that have full confidence that quantum computers will work , this definitively excludes hidden variable theories . this is shor 's answer to this question : why do people categorically dismiss some simple quantum models ? .
consider derivative at $t=0$ ; denote $\psi ( 0 ) $ as $\psi_{0}$ $ ( \partial /\partial t ) t\psi ( t ) \big|_{t=0}=\displaystyle lim_{h\rightarrow0} ( ( t\psi_{0} ) ( h ) -t\psi_{0} ) /h$ since $t\psi$ evolves according to $i ( \partial /\partial ( -t ) ) t\psi ( t ) =ht\psi ( t ) $ so $ ( t\psi_{0} ) ( h ) = exp ( ihh ) t\psi_{0}$ . hence we have : $\displaystyle lim_{h\rightarrow0} ( ( t\psi_{0} ) ( h ) -t\psi_{0} ) /h$ $=\displaystyle lim_{h\rightarrow0} ( exp ( ihh ) ( t\psi_{0} ) -t\psi_{0} ) /h$ $=\displaystyle lim_{h\rightarrow0} ( texp ( -ihh ) \psi_{0}-t\psi_{0} ) /h$ $\:\:\ ; \ ; \ ; $ ( since $t$ and $h$ commute , and $t$ is antilinear ) $=\displaystyle lim_{h\rightarrow0}t ( exp ( -ihh ) \psi_{0}-\psi_{0} ) /h$ $=t ( \partial /\partial t ) \psi ( t ) \big|_{t=0}$ notation : $ ( t\psi_{0} ) ( h ) $ means we first act $\psi_{0}$ by $t$ and then time evolve the resulting state by an amount of time $h$ . another argument : following argument seems more relevant here than above one :- we have a one parameter family of states $\psi ( t ) $ which satisfy $i ( \partial /\partial ( t ) ) \psi ( t ) =h\psi ( t ) $ for definiteness suppose $t\in [ 0,1 ] $ , and suppose we partition this interval into $n$ equal parts ( where $n$ is some large number ) as {$0=t_0&lt ; t_1&lt ; . . . . &lt ; t_{n-1}&lt ; t_n=1$} . denote $\psi ( t_j ) $ as $\psi_j$ for $j=0 , . . . , n$ , and let $1/n=\delta$ ( length of one small interval ) . then above differential equation can be written as a set of $n$ linear equations in terms of states $\psi_j$ 's as : $i ( \psi_1-\psi_0 ) /\delta=h\psi_0$ $i ( \psi_2-\psi_1 ) /\delta=h\psi_1$ . . . . $i ( \psi_j-\psi_{j-1} ) /\delta=h\psi_{ ( j-1 ) }$ . . . . $i ( \psi_n-\psi_{ ( n-1 ) } ) /\delta=h\psi_{ ( n-1 ) }$ now in zee 's book the one parameter family of vectors $t\psi ( t ) $ is required to satisfy the differential equation $-i ( \partial /\partial t ) t\psi ( t ) =ht\psi ( t ) $ . or in discretised form it is required that the set of vectors $t\psi_0 , t\psi_1 , . . . . . , t\psi_n$ satisfy following linear equations : $-i ( t\psi_1-t\psi_0 ) /\delta=ht\psi_0$ $-i ( t\psi_2-t\psi_1 ) /\delta=ht\psi_1$ . . . . $-i ( t\psi_j-t\psi_{j-1} ) /\delta=ht\psi_{ ( j-1 ) }$ . . . . $-i ( t\psi_n-t\psi_{ ( n-1 ) } ) /\delta=ht\psi_{ ( n-1 ) }$ now since $t$ is linear wrt addition of states so it can be taken out : $-it ( \psi_1-\psi_0 ) /\delta=ht\psi_0$ $-it ( \psi_2-\psi_1 ) /\delta=ht\psi_1$ . . . . $-it ( \psi_j-\psi_{j-1} ) /\delta=ht\psi_{ ( j-1 ) }$ . . . . $-it ( \psi_n-\psi_{ ( n-1 ) } ) /\delta=ht\psi_{ ( n-1 ) }$ in continuum limit these equations are equivalent to : $-it ( \partial /\partial t ) \psi ( t ) =ht\psi ( t ) $ edit : question : consider a one parameter family of states $\psi ( t ) $ which satisfy schrodinger equation $i ( \partial /\partial t ) \psi ( t ) =h\psi ( t ) $ . is it possible to find an invertible linear operator $t$ that commutes with $h$ and such that for any $\psi ( t ) $ as above , $t\psi ( t ) $ satisfies $-i ( \partial /\partial t ) t\psi ( t ) =ht\psi ( t ) $ ? our previous argument ( 2nd one ) extends to one proof that it is not possible ; here is another one : if $t$ is such an operator then $t\psi ( t ) =exp ( ith ) t\psi ( 0 ) $ . ( because $t\psi ( t ) $ solves time reversed schr . equation ) ----- ( 1 ) also $\psi ( t ) =exp ( -ith ) \psi ( 0 ) $ ( because $\psi ( t ) $ solves usual schr . equation ) . ------- ( 2 ) substituting ( 2 ) into ( 1 ) we get $texp ( -ith ) \psi ( 0 ) =exp ( ith ) t\psi ( 0 ) $ now using the fact that t is invertible we get : $exp ( -ith ) \psi ( 0 ) =t^{-1}exp ( ith ) t\psi ( 0 ) $ again using the fact that $t$ is linear and commutes with $h$ we get $exp ( -ith ) \psi ( 0 ) =exp ( ith ) \psi ( 0 ) $ ( note that if $t$ were antilinear then in place of $exp ( ith ) $ on rhs we would have $exp ( -ith ) $ , and hence there would be no problem ) now multiplying on both sides with $exp ( -ith ) $ we get $exp ( -2ith ) \psi ( 0 ) =\psi ( 0 ) $ differentiating with respect to $t$ and putting $t=0$ we get $h\psi ( 0 ) =0$ but $\psi ( 0 ) $ was any arbitrary state in our space of states . so we have $h=0$ identically . hence the required linear operator is not possible unless $h$ vanishes identically .
yes , you can make a unitary asymptotic s-matrix ( so asymptotic measurements ) when the intermediate states do not evolve in a unitary way . this is what ghost fields do--- the intermediate states in ghost-descriptions include negative probability objects , but when you make asymptotic measurements you do not see the ghosts , you only see the positive probability objects . in cases where you have a ghost description , there are often no-ghost formulations , like light-cone or axial gauges . in these formulations , the hamiltonian is well defined , so that you can ask about measurements on the intermediate states and get well defined answers . these formulations have a reduced symmetry compared to the ghost formulation , but they are manifestly unitary . in ghost formulations , you assume that every measurement is made on asymptotic states which have no ghosts . even if it is not true that every measurement is of an s-matrix quantity , the existence of the unitary formulation guarantees that anything you build out of asymptotic states will only end up measuring a quantity which has a reasonable positive probability interpretation .
the most general answer is " we engineer things with relativity . " the most striking example i know is the cebaf accelerator at jlab . like all modern accelerators it uses rf linacs to add energy to the beam . that only works if the size of the cavities and the frequency of the rf match the speed of beam . now , cebaf is special , beams of widely differing energy can propagate simultaneously through the linac at high efficiency , which only works because the speed of the beam is insensitive to the kinetic energy in the ultra-relativistic regime in which the machine works . pre-upgrade a 50 mev beam could coexist with a 4.5 gev beam . post upgrade those numbers will roughly double .
in $y$ direction you have accelerated movement with constant acceleration , thus $$v_y = v_{y0} - g t$$ and after putting initial conditions $$|v_y| = g t$$ i have no idea whatsoever what did you want to do with your calculation .
indeed , your question has nothing to do with the distinction between 1pi and wilsonian . the answer is that the terms which contain nontrivial dependence on $d^2\phi$ are to be dropped if the breaking of supersymmetry is small compared to the natural ( "supersymmetric" ) mass scale in the problem . you can see this by noting that the effective potential has to be of the form $f^2 f ( f/m^2 ) $ where $f$ is the susy breaking scale and $m$ is some supersymmetric scale ( which can the vev of some modulus , too ) . another way to see this is that terms with more powers of $d^2\phi$ have a higher engineering dimension and thus have to be divided by some susy scale , so their effect disappears as f/m^2-> 0 . in some physical scenarios these corrections could be important , but since in dynamical models having rigorous control over the physics usually entails having susy-breaking as a small effect , in most of the literature these terms are dropped .
i think there may be a better forum to ask this question in and it will likely be closed , but information theory is important to many branches of physics in , so here 's a quick answer . the bandwidth of a channel is simply the number of symbols you can send through it per unit time . by symbol , i mean here a single , real number , and this meaning arises through the shannon sampling theorem . see the wikipedia page for this theorem , and go through the proof so you will understand exactly what i mean . now , just one lone noiseless real number can in theory encode as much information as you like . there are $\aleph_0$ digits in a real number ! write out the whole of wikipedia as 0s and 1s and call it a binary fraction between 0 and unity and the whole of wikipedia is still a finite precision , rational binary number ! so you can see in theory that you can send heaps of data over channels that can send only a low number of symbols each second . this theoretical ideal is , of course , limited by noise . it effectively " coarse grains " the real numbers . if i have noise with an amplitude of 0.1units , and can send symbols with an amplitude of up to 1 units , then i roughly have 10 amplitude levels i can encode data on . otherwise put , i can tell apart ten levels . so i can encode $\log_2 10$ bits per symbol in this example . if my noise amplitude is 0.01 units , i can tell apart roughly 100 different levels per symbol . so i can encode $\log_2 100$ bits per symbol in this example . i think you should now be able to see what is going on : the number of bits you can send per unit time is roughly $$b \log_2 s/n$$ the actual shannon-hartley theorem is a little more complicated , but that is the idea . edit : for interest : 64-qam modulation is commonly used for digital communications . this is essentially where the " symbols " are one of 64 points on a regularly spaced grid in the argand plane representing the amplitude and phase of the signal . so this scheme has a spectral efficiency of six bits ( $\log_2 64$ ) per hertz ( i.e. . symbol per second ) . the ultimate spectral efficiency of a typical optical fibre link is of the order of 20 to 25 bits per hertz : see my answer here .
become another unstable element that will again go through beta decay ? yes , this happens . if you start far from the line of stability , many beta decays are needed in order to get to something stable . for heavy elements , you can also have beta and alpha decays intermixed in the chain . could that element ( hypothetically speaking ) go through beta decay , then , once it has too many protons could it immediately go through electron capture to become that same radioactive element there are three processes : $\beta^+$ decay , $\beta^-$ decay , and electron capture . when people refer to beta decay , it means all of these , not just $\beta^-$ . anyway , the kind of process you are referring to is not possible if all the decays we are talking about are beta decays of ground states . first off , the way you are describing it you seem to be imagining the nucleus oscillating back and forth across the line of stability . since almost all mass numbers have at least one stable isotope , this can not happen ; once you hit stability ( in the ground state ) , you can not decay due to conservation of energy . even if you do not cross and recross the line of stability , you can not have a chain like this due to conservation of energy . if the ground state of a can decay to b without violating conservation of energy , then conservation of energy prohibits b from decaying to a . it might be possible for this to happen if a decayed from a state that was not its ground state , went to b , and then b decayed back to a 's ground state . however , this does not seem likely to me , and i do not know of any examples . typically , excited states in nuclei decay electromagnetically on very short time-scales ( nanoseconds or picoseconds ) , so weak decay branches can not compete . to get competition , you typically have to have a state whose electromagnetic decay to the ground state is hindered by the very low excitation energy ( gamma decay is slower if the gamma-ray energy is lower ) , as well as possibly a large difference in spin . this is fairly common in odd-odd-nuclei . however , this only works if you have a low excitation energy in a , which then makes it unlikely that you had have enough energy left in b to get back to the ground state of a . furthermore , the staggering of binding energies between even-even and odd-odd nuclei of the same mass number makes it extremely unlikely that the even-even nucleus b would decay back , away from the line of stability , to the ground state of the odd-odd a .
ok , my initial guess is the votable format , but after some digging i found that the format is one of . cat , table and scat . you can save any catalogue as a local file and open it in the editor and see . generally speaking jskycat should in principle take on many formats as the gaiaskycat or skycat does since it is developed from that . the software also " listens " to the simple application messaging protocol ( samp ) servers , so that is the reason that i think votable is its choice . edit : i found some documentation at the features page for jskycat where it actually confirms my guess .
for a star of a given mass , we can calculate , based on theoretical models , how long it has to live . for example , the sun is currently 5 gyr old and will live another 5 gyr . so if you observe the sun from , say , andromeda , the light would be about 2 million years old and you could therefore conclude that the sun is still alive even though you are observing " old " light . if you flip it around , if we see a sun-like star in andromeda , we could safely say it is still alive . broadly , while fusing hydrogen in their cores , more massive ( and therefore brighter , hotter ) stars live shorter lives . on the one hand , it means that we observe some small stars whose lives will be longer than the current age of the universe . on the other hand , we could theoretically observe stars whose lifetimes are shorter than the distance to them , in light-years . so , a star 80 times as massive as the sun might have a lifetime of a few million years , so if we currently observe it in andromeda near the end of its life , that star is probably gone now . off the top of my head , i would say most of the stars we presently observe still exist as we see them now . we just can not resolve stars that far away . for what it is worth , there are some distant phenomena that we see that happen much faster than the light takes to reach us . for example , the 2011 nobel prize went to the leaders of two teams that observe type ia supernova at redshifts up to about $z=1$ . these are events that lasted less than a year that happened nearly 8 billion years ago . who knows what they look like now !
i will mention just one example of the complexity that curved space introduces into the quantization process . consider minkowski space quantization of the free klein gordon field , which satisfies $$ ( \box+m^2 ) \phi=0$$ a fundamental step in the procedure is performing the mode expansion $$\phi ( x ) =\int{\frac{1}{\sqrt{2\omega_\bf{k}}}} ( a_{\bf{k}}e^{-ikx}+a^*_{\bf{k}}e^{ikx} ) d^{3}\bf{k}$$ here we have a splitting into a negative frequency part $$\phi^- ( x ) =\int{\frac{1}{\sqrt{2\omega_\bf{k}}}a_{\bf{k}}e^{-ikx}}d^{3}\bf{k}$$ and a positive frequency part $$\phi^+ ( x ) =\int{\frac{1}{\sqrt{2\omega_\bf{k}}}a^*_{\bf{k}}e^{ikx}}d^{3}\bf{k}$$ upon quantization , the $a^*_{\bf{k}}$ in the positive frequency parts become creation operators and the $a_{\bf{k}}$ in the negative frequency parts annihilation operators . the splitting is covariant - the exponentials contain lorentz scalars . now if we try to do the same thing in curved space to the ( covariant form of ) the klein gordon equation , we can find spacetimes for which there is no clear way to perform this splitting because in general , there is no " natural " time coordinate . in the minkowski case , we had the action of the poincare group to allow us to deal with the different possible time coordinates - we even had a poincare invariant vacuum state , but here there is no equivalent of the poincare group action . the particle content of the theory depends upon this splitting , and the ambiguity we have in the curved case ( or even in the flat case if we allow non inertial frames ) is the origin of the hawking and unruh effects . that was just a single example of a problem that crops up right from the word " go " in curved space quantization . there has been a lot of effort expended over the last few decades studying quantization on curved spacetimes . for a review , see here .
i call it inertial force , but to be exact it should be called the rate of momentum change . $$ \vec{f} = \frac{{\rm d}}{{\rm d}t} ( m \vec{v} ) = m \vec{a} $$
choosing uniformly distributed points on the two dimensional bloch sphere : $0 \leqslant \phi &lt ; 2\pi , 0 \leqslant \theta \leqslant \pi$ , we can construct a random vector $ |\psi_0\rangle = cos\frac{\theta}{2} |0\rangle + e^{i\phi} sin\frac{\theta}{2} |1\rangle $ in order to obtain a uniform distribution over the sphere 's surface , $\phi$ should be uniformly distributed in the interval $ [ 0 , 2\pi ) $ and $cos ( \theta ) $ should be uniformly distributed in $ [ -1 , 1 ] ) $ . to see that , please notice that in terms of the height of the unit sphere $z = cos\theta$ , the surface element is uniform : $ ds = sin ( \theta ) d\theta d\phi = -dz d\phi$ this is called archimedes ' spherical sampling theorem as it was known already to archimedes . the required expectation : $ p = \mathrm{tr} ( |\psi_0\rangle\langle\psi_0|\phi ) = cos^2\frac{\theta}{2} = \frac{1+z}{2}$ since $p$ is linear in $z$ , it is uniformly distributed in $ [ 0 , 1 ] ) $ .
it is not really a single principle - it is a philosophy and in the context of philosophical discussions about science , it is usually known as positivism . http://en.wikipedia.org/wiki/positivism as any philosophy , it cripples the penetrating power of science if it is extended too far - and every philosophy ultimately fails . the thought experiment about the earth in the universe is just one among millions of examples . positivism could have a problem with the whole concept of thought experiments . while it was very useful and important for the development of quantum mechanics to realize that science does not have to talk about things that can not be measured , i.e. that theories that deny the existence of things that can not be observed are just fine , it still remains true that science also can talk about concepts that can not be observed , such as quarks . it is up to the scientific method to decide whether an auxiliary concept or a theory that is not accessible to observations has an explanatory power that justifies its validity - and the answer may be different in each individual situation .
laser light is spatially and temporally coherent . the stimulated emission is mainly responsible for the temporal coherence . so the answer is yes , you can create an electromagnetic beam that is spatially but not temporally coherent by placing a pinhole close to the source , and then another pinhole in the far field of the first pinhole . this beam will not spread out very much . ( but also remember that laser light does spread out . ) note that for rf frequencies , a " pinhole " is probably several meters in diameter . the far field distance is given by this inequality : $l \gg a^2/\lambda$ , where l is the distance , a is the diameter of the hole , and $\lambda$ is the wavelength . however , creating a rf pencil beam is probably not practical . the term " pencil beam " mentioned in the wikipedia article is explained as being diffraction-limited . the size of a diffraction-limited beam gets larger with , i believe , the square root of the wavelength . it would be more like a gas-pipeline beam than a pencil beam .
neodymium magnets are not made of elemental neodymium . they are $nd_2fe_{14}b$
the fact is , in the context of ideal circuit theory , the inductor voltages are equal in the circuits below : in the lower circuit , the inductor current has a constant component , i.e. , the lower circuit is equivalent to your $c \ne 0$ case . but , there is nothing remarkable or surprising about this . is there something else to your question that i am missing ? [ i ] am asking that why in in elementary physics text books the author directly writes c=0 for ideal inductor without mentioning the initial conditions ? the initial conditions are mentioned when the context is transient analysis . for example , from wikipedia : however , when the context is ac ( sinusoidal ) circuit analysis , the underlying assumptions are ( at least ) : ( 1 ) all sources are sinusoidal and of the same frequency ( 2 ) the circuit is in sinusoidal steady state , i.e. , all transients have decayed . when these conditions hold , can we use voltage and current phasors and the notion of impedance to analyze circuits .
no , the lhc does not violate the uncertainty principle . the principle only affects position and momentum ( not actually velocity ) in the same direction , but in a particle accelerator , you do not have to constrain both position and momentum in any one direction . it is only important to constrain the position in the transverse direction ( perpendicular to the beam ) and the momentum in the longitudinal direction ( along the beam ) .
when the notions of electric and magnetic fields were conceptualized , they imagined that there was an invisible fluid being pushed around by charges , and they leveraged some of the equations and terminology of fluid mechanics . modern understanding of field have largely gotten rid of this picture , but some colorful langauge like " electric flux " remains . if you want to picture positive charge as " amount of fluid added to region per unit time " and negative charge as " amount of fluid removed from region per unit time " , you can , but this thinking only gets you so far . safer to just think of it as an abstract mathematical definition .
okay ! i apply a transformation , which converts my comment into a slightly more self-contained answer . the transformations mentioned here are most naturally described by means of tensor calculus or , more generally , differential geometry . it is needed , roughly speaking , when one wants/needs to introduce a coordinate system at each point in space and study the relations between different points and their coordinate systems . in this case there is a coordinate transformation defined from cartesian system $x^\mu= ( x , y , z ) $ to ellipsoidal coordinates $x^{\mu'}= ( \eta , \mu , \nu ) $ , and an inverse transformation . $\mu$ and $\mu'$ are indices , which can take values from $1$ to $3$ . trasnformation matrix $t^{\mu}_{~\mu'}$ is defined as $t^{\mu}_{~\mu'}=\dfrac{\partial x^{\mu}}{\partial x^{\mu'}}$ . for the given transformation $t^{\mu}_{~\mu'}$ is diagonal , because $x$ depends only on $\eta$ , and so on . metric tensor with matrix $g_{\mu\nu}$ is a quantity , which defines the scalar product of basis vectors $g_{\mu\nu}\equiv \vec{e}_\mu\cdot\vec{e}_{\nu}$ . in cartesian coordinates $x^{\mu}$ , therefore , $g_{\mu\nu}=\textrm{diag}\{1,1,1\}$ . in primed coordinates , for a given point in space ( with its own set of basis vectors ) the same definition holds : $g_{\mu'\nu'}=\vec{e}_{\mu'}\cdot\vec{e}_{\nu'}$ . from tensor calculus , $g_{\mu'\nu'}$ and $g_{\mu\nu}$ are connected by $g_{\mu'\nu'}=g_{\mu\nu} t^{\mu}_{~\mu'} t^{\nu}_{~\nu'}$ , where summation over the same indices is implied . because $t^{\mu}_{~\mu'}$ is diagonal , $g_{\mu'\nu'}$ is also diagonal , as it should be . hence $\vec{e}_{\mu'}$ are orthogonal . now , in orthogonal bases $\vec{e}_{\mu}$ scale factors are defined as $h_{\mu}=\sqrt{\vec{e}_{\mu}\cdot \vec{e}_{\mu}}$ ( which has some physical meaning if one thinks about decomposing a vector in such a basis ) , here summation is not implied . in cartesians , therefore , $h_1=1$ , whereas in ellispoidal coordinates $h_{1'}=\sqrt{\vec{e}_{1'}\cdot \vec{e}_{1'}}=\sqrt{g_{1'1'}}=\sqrt{g_{\mu\nu} t^{\mu}_{~1'} t^{\nu}_{~1'}}=\sqrt{g_{11} t^{1}_{~1'} t^{1}_{~1'}}=t^{1}_{~1'}=t^{1}_{~1'} h_1$ . or , alternatively , one can derive ( without using $h_1 = 1$ ) that $h_1=h_{1'}t^{1'}_{~1} = h_{1'} ( t^{1}_{~1'} ) ^{-1}$ . using the above described , $t^{1}_{~1'}=\dfrac{\eta}{x}=\dfrac{\eta}{\sqrt{\eta^2-a^2}}$ . substituting $t^{1}_{~1'}$ and $h_{1'}$ into $h_1= h_{1'} ( t^{1}_{~1'} ) ^{-1}$ and using $\eta\sim a \rightarrow \infty$ , one gets $h_1 = 1$ . the answer might be hard to read if you have never studied differential geometry , but the key point is that scale factors are not scalars and a simple variable change does not work for them . however , as they are defined for orthogonal systems , the transformation rules that they follow are relatively simple .
you might be interested in skimming the expanding confusion paper by davis and lineweaver , which i have referenced many times on this site . in particular , the last sentence of the abstract states : we analyze apparent magnitudes of supernovae and observationally rule out the special relativistic doppler interpretation of cosmological redshifts at a confidence level of 23$\sigma$ . the basic idea is looking at the luminosity distance using type ia supernovae ( the " standard candles " of cosmology ) . the results you had predict from a doppler shift - even a special relativistic one - do not agree with reality . you had have to have some pretty ad hoc additions to your special relativistic cosmology to get these results . section 4.2 of the paper covers this . it is not surprising , though , that there is so much confusion . the authors themselves got confused , and an earlier version of section 4.1 stated that you could differentiate between doppler shifts and gr based on timing with standard clocks , but in fact the two effects are indistinguishable . perhaps the best part of the paper is appendix b , which gives some 25 examples of misleading or outright erroneous statements on this and related topics made by well-known physicists . i also should add that we also have the cmb radiation to observe , and it provides a unique local at-rest reference frame . that is , if you are moving with respect to the cmb , you will know because it will appear hotter in the forward direction and cooler in the opposite direction . now we have a small velocity relative to it , but nothing close to the speed of light . so if you try to explain redshifts with just sr and the doppler effect , you have to explain why we happen to live in one of the 100 or so galaxies barely moving with respect to the cmb , and why the billions of other galaxies we observe are all moving quite fast with respect to it . in gr , everyone can be locally " at rest " and see an isotropic cmb , while still seeing each other redshifted . so there is a copernican argument for you - the sr description forces you to believe you are in a special location in the universe . if you want to intuitively understand the cosmological redshift , i will shamelessly plug my own excessively verbose answer to a related question .
archimedes ' principle tells us that the upwards force on an object immersed in a fluid is equal to the weight of fluid displaced . so let 's take your initial experiment . you do not tell us the volume of your object , but you do give its weight , $w_g = 100$ g , and density , $\rho_g = 2.6$ g/cm$^3$ , so the volume is : $$ v = \frac{w_g}{\rho_g} = 38.46 cm^3 $$ when you put this in water the volume of water displaced is the same as the above volume , so the mass of water displaced is : $$ m_w = v\rho_w = \frac{w_g}{\rho_g} \rho_w = 38.46g $$ and hence the effective mass of your glass object is 100 - 38.46 = 61.54g as you say . if the density of water changes to 1.010 g/cm$^{3}$ just use the equation above but set $\rho_w = 1.010$ , and you will indeed get the effective weight of the glass equal to 61.15g . though if you want to be really accurate you should note that changing the temperature will change the density of the glass as well , so $\rho_g$ would be slightly different as well .
it is a measure of either the electron 's kinetic energy or it is total energy ( including mass ) . because the electron 's mass is $511\text{ kev} = 0.511\text{ mev}$ , it matters which in this case . by default particle physicists mostly talk about total energy , but nuclear physicist often talk about kinetic energy . in either case you had have to ask to be sure . assuming that is all kinetic energy , then in principle it could be used to ionize more than one hundred-thousand hydrogen atoms with each electron being asymptotically free but having no additional energy ; in practice the energy could not be brought to bear in so ordered a manner . if the particle was directed into a region of high purity hydrogen gas it would actually ionize many fewer atoms , but the ejected electrons would have positive asymptotic kinetic energy .
the reason is because the heat loss occurs mostly in the windows and the fenestration . the idea is that you would like the incoming air to be heated up . also , it creates an air curtain that prevents more heat from being lost through this exposed areas . the final reason is to make the temperature of the room more or less uniform . if the heaters were placed at the center of the room , you would create a large temperature gradient , resulting in drafts and discomfort for the occupant .
the weyl tensor is the trace-free part of the riemann tensor . the latter describes the curvature of spacetime . in the absence of sources , the trace part of the riemann tensor will vanish due to the einstein equations , but the weyl tensor can still be non-zero . this is the case for gravitational waves propagating in vacuum . the physical reason is that even in the absence of sources , there can be curvature of space time . gravitational waves represent small variations of curvature , i.e. non-zero curvature . an analogous situation would be electromagnetic waves propagating in vacuum . one can describe them by the source-free maxwell equations without specifying anything about charges or currents that might have produced them .
you make artificial light with the same spectrum as the sun , and shine it on the crops .
yes , the comination $j_1 + j_2$ determines the spin of the particle . note however , that this is an addition of angular mementum which may be complicated . furthermore , you can count the degrees of freedom : in $ ( j_1 , j_2 ) $ , each contribute $2j_1 + 1$ states and we construct a tensor product , so $ ( j_1 , j_2 ) $ gives $ ( 2j_1 + 1 ) * ( 2j_2 + 2 ) $ degrees of freedom . for the vector we have $ ( 1/2 , 1/2 ) \mapsto 2 * 2 = 4$ degrees of freedom . if the representation is reducible , i.e. of the form $ ( j_1 , j_2 ) + ( k_1 , k_2 ) $ , then you simply add the d.o.f. you get from each pair . the dirac spinor has $ ( 1/2 , 0 ) + ( 0 , 1/2 ) \mapsto ( 2 ) + ( 2 ) = 4$ degrees of freedom , the field-strength tensor has $ ( 1 , 0 ) + ( 0 , 1 ) \mapsto 3 + 3 = 6$ d.o.f. as a sidenote : the representations $ ( 1 , 0 ) $ do not corresond to vectorlike degrees of freedom , but rather to antisymmetric self-dual tensors . $ ( 0 , 1 ) $ is the antisymmetric anti-self-dual tensor . the vector ( and the only way to get a vector out of this ) is $ ( 1/2 , 1/2 ) $ !
the explosion certainly is hemispherical , see , for instance , this explosion caused by the trinity bomb : the gas cloud that you posted , and what many would consider is synonymous to the nuclear weapons , comes after the explosion . nuclear bombs are actually usually ignited above ground for " maximum destruction . " since the nuclear reaction is immensely hot ( about 4000 k whereas the surface of earth is sitting pretty around 300 k ) , the gas rises much the same way a hot-air balloon rises . at some point , the cold air from around the explosion gets sucked under the mushroom cap and causes the thin column you see : thus , for the most part , it is the extreme temperatures that cause the explosion " bubble " to rise in the first place . and it is the convective air currents under the bubble that cause the column to form .
clouds move with the wind , so the cloud velocity is just the wind velocity . the recent storm in the philipines reached wind velocities of 200 mph , though the higest speed reported is apparently 253 mph . the fastest moving clouds known are on neptune , where the winds reach 1340 mph .
yes . also note that in the momentum representation , $x = i\hbar \frac{d}{dp}$ , which is what your commutation relation proved as a special case . you could use this shortcut right off the bat .
i will combine the comments into a guiding answer with some extra details here and there . an empty space containing a single point particle with charge $q$ is defined by two properties : the system 's charge distribution $\rho ( \vec{r} ) $ is zero everywhere except at the location $\vec{r}_0$ of the point particle . the integral over the entire space of the system 's charge distribution is $q$ , i.e. $$\int_{v}{\rho ( \vec{r} ) d\vec{r}} = q$$ let 's use the last property to state $\rho ( r ) = q\tilde\rho ( \vec{r} ) $ where $\tilde\rho ( \vec{r} ) $ is a ' function ' ( distribution ) which integrates to $1$ over all space and has the property $\tilde\rho ( \vec{r} ) = 0$ for all $\vec{r}\neq\vec{r}_0$ . of course this is the dirac delta $\delta ( \vec{r}-\vec{r}_0 ) $ . now imagine we put the origin at the location of the particle : $\vec{r}_0 = \vec{0}$ . then $\rho ( \vec{r} ) = q\delta ( \vec{r} ) $ . next , take a look at the poisson equation : $$\delta\phi ( \vec{r} ) = q\delta ( \vec{r} ) . $$ this is a hard problem to solve . but why exactly ? well , we have a second derivative on the one side and a dirac delta on the other side . the second derivative is not a problem per se , but if we try to integrate the equation , the dirac delta makes life hard for us . note that the definite integral of a dirac delta is easy to evaluate , the indefinite integral is much harder if you do not know the answer . intuitively it might feel natural to you that the integral of the dirace delta is in fact the heaviside step function , but remember that we have to integrate twice to get $\phi$ , so we also have to know how to integrate this step function . all of that is work i would not want to do . not when there is an easier way . and that way is provided by the fourier transform . we know that it has the property of transforming differentiation into multiplication and it involves a definite integral over all space , so the dirac delta will be easy to transform . therefore , it is a great tool to use on differential equations like this one . it transforms a difficult problem in $\vec{r}$-space into an easy one in $\vec{k}$-space . we can then solve the easy problem and inverse fourier transform the solution back to $\vec{r}$-space . this will yield us the correct solution of the problem in $\vec{r}$-space , due to another property of fourier transforms which is sometimes called the fourier inversion theorem . the problem you are trying to solve is actually known as the green 's function problem for the poisson equation . the method of green 's functions for solving a differential equation with a general source term $\rho ( \vec{r} , t ) $ consists of solving the same problem , but with dirac delta source term $\delta ( \vec{r} ) \delta ( t ) $ . the solution to that problem is called the green 's function for that class of problems and often it is useful to fourier transform the equation to find this function . the same green 's function can be used to solve any differential equation with the same structure , regardless of the source term . the underlying idea is to build up the full source term out of lots and lots of point sources and finding that the solution can also be built up in a similar way , out of lots and lots of point source solutions . the building up is done by means of convolution .
when we wish to solve a differential equation like your example , a good guess for when $f ( t ) =0$ is something of the form $x ( t ) = e^{i \alpha t}$ . this just gives us an algebraic equation for $\alpha$ , which we can just solve . i interpret your question to mean : but what is this solution ? why is it complex ? indeed , position should certainly be just a real number . the resolution is that a general solution to the differential equation will not be of the form $x ( t ) =e^{i\alpha t}$ for $\alpha$ solving our algebraic equation . in general there will be a couple solutions to the algebraic equation for $\alpha$--just two in your example--say $\alpha_1$ and $\alpha_2$ . then a general solution to your diffeq with $f ( t ) =0$ is of the form $x ( t ) = a e^{i \alpha_1 t} + b e^{i \alpha_2 t}$ for some complex constants $a$ and $b$ . we then need boundary conditions to choose $a$ and $b$ properly . the crux of the biscuit is we can choose $a$ and $b$ so that an $x ( t ) $ of this form is real for all times $t$ ( this assumes $a$ , $b$ , and $c$ are real in the original equation ) . so using the complex guess $x ( t ) = e^{i\alpha t}$ is just a computational trick which makes things easy because the complex numbers have nice properties with algebraic equations ( they are algebraically closed ) . we could have done the whole thing without $e$ , instead just using sines and cosines and it would have worked out too .
was taught that it has more to do with specific heat than with heat conduction . 1 gr of water requires 1 cal of heat to raise its temperature by 1 °c . by comparison , sand only takes about 0.2 cal to get the same effect . you can find some values of common substances here .
it is conservation rules ( mostly that of energy ) that controls what decays are possible , and what channels are allowed ( the strong interaction respected quark flavor , but the weak interaction does not ) . if the final state has higher energy than the initial state there can be no spontaneous process leading from one to the other . the energy environment of a nucleus is a complicated place because it is affected by the strong nuclear force , electromagnetic forces and by the limits on state occupation imposed by pauli exclusion . there are a lot of question already on the site about when various processes can proceed and related subjects $\alpha$ decay to more than one nuclear state what stabilizes neutrons against beta decay in a neutron star ? how can a proton be converted to a neutron via positron emission and yet gain mass ? why is the ( free ) neutron lifetime so long ? nuclear fission and half life adding many more neutrons to a nucleus decreases stability ? . . . your question about the case where a nucleus has multiple modes have different half-lives is tricky . the measured result is the same half-life for each mode , but we can ask " if we could magically block modes and study the isolated half-life of each mode , what would they be ? " if you perform the ( often approximate ) calculation for that question you generally find different half-lives . the interesting part is the the branching fractions for each decay are related to ratios of the calculated " isolated " half-lives , so you are seeing the effect of the different probabilities when you look at the frequency of different decays .
another technique for this type of question is to assume that the final result is all the water in some specific state , and find the net energy change to achieve this . if it is non-zero , as is likely , you have one uniform system to make any necessary correction . for example , assume that the final situation here is all liquid water at 100° c . this is wrong , as shown by wojciech morawiec , but no matter . to reach this state , we must add heat energy to the kilo of ice:$$heat_{added}=1000\times 334+1000 \times 100\times 4.184=752,400 joules$$ to condense the kilo of steam to liquid water at 100° c , we must remove heat:$$heat_{removed}=1000 \times 2230=2,230,000 joules$$ so , now we have 2 kilograms of boiling hot liquid water , but we need , for zero net heat flow , to stuff back in $ ( 2,230,000-752,000 ) $ or $1,471,000 joules$ . ( see , the guess $was$ wrong ) this is enough to " re-vaporize " $\frac{1,471,000}{2230}=660$ grams of water .
interesting , i was just studying the fourier decomposition of vowels for half an hour yesterday . first , you must distinguish vowels and consonants . words like " bee " ( which is how we spell the letter " b" ) of course are not uniform sounds . they start with a consonant , in this case one created by lips . depending on the way how they are created , consonants are divided to many groups . see a table of consonants here . in general , consonants are types of noise because they do not have a well-defined basic frequency . it means that the fourier series for a consonant is composed pretty much of all sufficiently high frequencies . the color of the noise – whether higher frequencies tend to be more strongly represented than the lower one etc . – determines the type of the consonant . there are consonants with a throat sound added , like l , m , n , which may be fourier decomposed similarly to the vowels , and noise-based consonants such as b , d , g , v , z which are the sound-equipped cousins of p , t , k , f , s , and so on , and so on . the most monochromatic sounds are the vowels . they can be sung so they have a well-defined base frequency . whether one gets u , o , a , e , i – or , in english , oo , aw , ah , eh , ee etc . – depends on how the mouth is opened . this modifies the shape of the resonance cavity and therefore the preferred additional frequencies that are excited by the action of the base frequency coming from the throat . the presence of higher harmonics is essential for the difference between vowels . i recommend you to look at a page about it , for example this one : http://hyperphysics.phy-astr.gsu.edu/hbase/sound/vowel.html the vowel u ( oo in english ) has the lowest representation of the higher harmonics . it is the closest one to the harmonic sound and it is achieved by changing one 's mouth into a passive tube through which the sound penetrates . on the other hand , a ( ah ) and i ( ee ) have a huge , important contribution of higher harmonics and when i say higher , i do not mean the 2nd or 3rd . the 20th harmonic etc . etc . are very important . in fact , it is more accurate to talk about the absolute frequencies . the vowel a ( ah ) has lots of those higher harmonics that are close to 1,000 hz ( cycles per second ) which are already suppressed in u ( oo ) and partly suppressed in o ( aw ) . as you continue to go towards e ( eh ) and i ( ee ) , the contribution from frequencies close to 3,000 hz starts to increase . these frequencies are calculable from the size of the mouth opened in the right way , from the length of the resonant cavity that becomes comparable to the wavelength of the sound waves that become important . in principle , all vowels may be emulated by the fourier expansion using just the base frequency ( pressure goes like $\sin \omega t$ ) plus higher harmonics but the very harmonics including $\sin 20\omega t$ are still very important for the character of the vowel . phonetics is the portion of linguistics that studies how language sounds ; the experts partially learn some physics although fourier series are not their primary tool . still , to understand phonetics , one has to accept various basic things . i found out that native english speakers misunderstand phonetics because they do not really decompose the language into " pure sounds " . your representation of " b " as a sound analogous to " a " may be an extreme example because you may have meant " b " in the sense of " bee " which is clearly composed of two sounds , the consonant " b " and the vowel " ee " . but even when it comes to vowels only , english ( and french and some other languages ) is deliberately obscuring the reality as it pronounces many vowels in a variable way . for example , " my " is pronounced as " mai " where the vowel gradually changes from " ah " at the beginning to " ee " at the end . some native english speakers do not even realize that this " y " in " my " is not a single uniform vowel . there are many other examples , of course .
there are two related but distinct questions : how do you keep a wormhole stable ? how do you make the wormhole in the first place ? courtesy of matt visser we can give one answer to the first question . matt 's example is to make the wormhole cube shaped , and in that case all you need to do is construct a cube from string i.e. the twelve edges of the cube are made from string . however the string would have to have a negative tension , and indeed it would have to have the ridiculously high negative tension of $−1.52 \times 10^{43}$ joules/metre . this is where your exotic matter comes in since the tension in any string made from normal matter would always be positive . the second question is harder . matt 's analysis applies to a time independant wormhole , i.e. one that has existed for an infinite time . constructing your cube of exotic string would warp spacetime in the manner required for a wormhole , but calculating what happens as you tie the strings into a cube is probably impossible at present . response to comment : this is going to be a bit hard to explain , but the space inside the cube does not exist . it is not part of the manifold on which the universe exists . if you travelled towards the cube you would not hit anything - you had just keep going without feeling anything as you past where it is wall is , but now you had be travelling in the other region of spacetime on the other side of the wormhole . re your comment it does not sound like there is a lot of control , the wormhole matt describes is not the same as the sort of wormhole sci-fi writers use to allow interstellar travel . as far as i know there is no theoretical support for the interstellar travel type wormhole . the wormhole matt describes connects two regions of spacetime but makes no statement about the global topology , so the region of spacetime the other side of the wormhole need not be , and almost certainly is not , some distant region of the universe around us . the wormhole does not allow ftl travel to e.g. alpha centauri . it just allows travel ( at up to the speed of light ) to the new region of spacetime on the other side of the wormhole .
he means that since it is a second order differential equation , it is completely determined by two sets of information , namely the value of $\phi$ and ${\dot \phi}$ at some time $t=t_0$ . in other words , $\phi ( t_0 ) $ and ${\dot \phi} ( t_0 ) $ paramaterizes the solution . we can therefore choose them to take any values , some which will imply a negative value of $\rho$ .
the general carelessness with the so-called " principle of least action " it that even in very good and reliable sources it is incorrectly stated that the action must be minimal . while the principle only requires that the action must be stationary , e.g. $\delta s = 0$ . so , more correctly , it should be called a " principle of stationary action " . concerning your example -- both of your trajectories are stationary , therefore both of them might be the true trajectory of your body .
without knowing more about your data set i can only offer a few random suggestions : if you have some kind of pid reason to believe that the tracks might be electron you just assume that they are and compute the energy from the momentum and $m_e$ . in you have a calorimeter in the detector stack you measure the energy , and project the maximum likelihood energy loss back to the vertex . note , however that the measured signal may have error much larger than $m_e$ , so it may be better to use the device for pid and fall back on the previous suggestion .
well the answer is that the body will indeed loose the momentum . but since the mass of the body will decrease as well due to radiation , the velocity should not change .
the uncertainty principle should be understood as follows : the position and momentum of a particle are not well-defined at the same time . quantum mechanically , this is expressed through the fact that the position and momentum operators do not commute : $ [ x , p ] =i\hbar$ . the most intuitive explanation , for me , is to think about it in terms of wave-particle duality . de broglie introduced the idea that every particle also exhibits the properties of a wave . the wavelength then determines the momentum through $$p=\frac{h}{\lambda}$$ where $\lambda$ is the de broglie wavelength associated with the particle . however , when one thinks about a wave , it is clear that the object described by it will not be easy to ascribe a position to . in fact , one needs a specific superposition of waves to create a wave that is essentially zero everywhere except at some position $x$ . however , if one creates such a wave packet , one loses information about the exact wavelength ( since a wave with a single , well-defined wavelength will simply extend throughout space ) . so , there is an inherent limitation to knowing the wavelength ( i.e. . momentum ) and position of a particle . on a more technical level , one could say that the uncertainty principle is simply a consequence of wave-particle duality combined with properties of the fourier transform . the uncertainty is made precise by the famous heisenberg uncertainty principle , $$\sigma_x\sigma_p\geq \frac{\hbar}{2}$$ more generally , for two non-comuting observables $a$ and $b$ ( represented by hermitian operators ) , the generalized uncertainty principle reads $$\sigma_a^2\sigma_b^2\geq \left ( \frac{1}{2i}\langle [ a , b ] \rangle\right ) ^2\ \implies \ \sigma_a\sigma_b \geq \frac{|\langle [ a , b ] \rangle| }{2}$$ here , $\sigma$ denotes the standard deviation and $\langle\dots\rangle$ the expectation value . this holds at any time . therefore , the measurement occurring right now , having occurred in the past or occurring in the future has nothing to do with it : the uncertainty principle always holds .
no , it cannot be enough . stokes ' theorem says that the volume ( $\omega$ ) integral of $d\omega$ , a form that is the exterior derivative of another one ( of $\omega$ ) , may be written as a surface integral . but it does not allow us to rewrite the volume integral of a general integrand ( which is not the exterior derivative of anything ) such as the lagrangian density ${\mathcal l}$ as a surface integral . so the stokes ' theorem is useless for dealing e.g. with the action $s$ that defines the dynamics of a general theory in the volume . one should mention that when the action is topologically invariant , ${\mathcal l}$ may indeed be locally written as a " total derivative " , and in that case , the theory has indeed a provable relationship with lower-dimensional theories ( a major example is chern-simons theory in 3 dimensions and the related wznw theories in 2d ) . but the general theories we know – the standard model coupled to gravity – are not of this special type , at least not manifestly so . what is happening in the volume is general – we surely do care about values of some fields such as the electric field in particular places of the volume – and there apparently is not any " counterpart degree of freedom " on the surface that we could associate it with . some people including leonard susskind and steve shenker etc . do suspect that there exists some " conceptually simple " proof of the holography in which almost all the degrees of freedom in the volume would be unphysical or topological – some huge gauge symmetry that allows one to eliminate all the bulk degrees of freedom except for some leftovers on the surface . but such a proof of holography remains a wishful thinking . meanwhile , we have several frameworks – especially the ads/cft – that seem to unmask the actual logic behind holography . the surface theory is inevitably " strongly coupled " ( i.e. . strongly dependent on quantum corrections ) if the volume description appears at all so things can not be as simple as you suggest , it seems .
the positive energy theorem talks about the lower bound on the total energy/mass , like the adm mass . to be able to define such a concept of the total energy/mass in general relativity , one needs some asymptotic region respecting a time-translational symmetry . that is the region where the gravitational potential ( something like the deviation of $g_{00}$ from the vacuum value ) goes like $gm/r$ . minkowski and anti de sitter space have this global time-like killing vector and the required asymptotic region where the adm-like mass may be measured . however , de sitter space does not have one . so not only there is no positive energy theorem in de sitter space . there is even no well-defined definition of a conserved mass in that spacetime background ! to understand all these things , one has to see why there is no nontrivial conserved energy/mass in cosmology or general backgrounds of general relativity , see e.g. http://motls.blogspot.com/2010/08/why-and-how-energy-is-not-conserved-in.html?m=1
classical electrodynamics mainly deals with two kinds of proplems : a ) the action of a field on a charged particle and b ) the fields arising from the motion of such a field . of course , this can only be approximative but it turns out that a lot of phenomena can be described in this way . however , you are right , an entire treatment would include a ) and b ) simultaniously - including the whole dynamics of such a system with radiative reaction ( or , the abraham-lorentz-force and its relativistic counterpart ) . but as qmechanic pointed out in a comment , there may be no fully consistent way to do so within the framework of classical electrodynamics . jackson ( chapter 16 ) states : the difficulties presented by this problem touch one of the most fundamental aspects of physics , the nature of an elementary particle . although partial solutions , workable within limiting areas , can be given , the basic problem remains unsolved . thus , you will have to search for a really satisfactory answer within the description of quantum electrodynamics . otherwise , you may include the effect phenomenologically . this was done e.g. by barone and mendes in lagrangian description of the radiation damping ( pra , 2007 ) and they give a lagrangian of the form $$\mathcal{l}=\frac{m}{2}\dot{\mathbf{r}}_1\cdot\dot{\mathbf{r}}_2 - \frac{\gamma}{2}\epsilon\left ( \dot{\mathbf{r}}_1 , \ddot{\mathbf{r}}_2\right ) -v\left ( \mathbf{r}_1 , \mathbf{r}_2\right ) , $$ with $\gamma := 2e^2/3c^3$ , with $\epsilon = \epsilon_{ij}dx^idx^j$ beeing the levi-cevita tensor , and the $\mathbf{r}_i$ arise from the special treatment of the problem used in the paper employing some kind of image phase-space representation of the system . furthermore , $v\ , $ is the potential related to the abraham–lorentz–dirac-force which is given explicitely in the paper .
realistically , because the light emitted from the infalling object is quantized , you will observe the " last " photon emitted from the infalling object in ( very much ) finite time . if you keep waiting after that , you will eventually observe the black hole hawking-radiate away , and any " information " carried by the infalling object will be " encoded " in the hawking radiation . here 's an additional argument : when the infalling object passes through the event horizon , it will contribute to the mass of the black hole , which will cause the event horizon to expand outward even more . this ensures even further that the infalling object will no longer be observable from the outside in a relatively short time , compared with the lifetime of the black hole .
diffractive optics are not magic , they are simply another tool that can be used in designing an optical system . they can do things that refractive optics cannot , and they are often lighter and smaller than an equivalent refractive optic . it is important to keep in mind , however , that the benefits of a diffractive optical element ( doe ) are not free . does have limitations of their own . they are harder to produce , and typically produce the desired results only under very specific conditions . for example , lets say you want to produce a circular laser beam with a very uniform intensity profile . what are your options for achieving this ? most laser sources produce a roughly gaussian beam , so you could expand this beam heavily with a refractive expander , and then mask out everything but the center of the beam . this will give you a relatively uniform beam , but you will waste a lot of light . you could use a more complicated refractive design , like a micro-lens array . this is difficult to engineer , and will not give perfect results , but it can do a very good job under a variety of conditions . the beam intensity can be made uniform over a large distance , and the input beam to the micro-lens array will not need to be perfectly collimated . it will also work across a relatively broad wavelength range . finally , you could design a doe beam shaper . these can be designed to give any intensity profile you like , but it will be expensive to produce . it may ( depending on what it is doing ) have certain flaws characteristic of does , like a strong zero-order beam ( where a large fraction of input to the doe passes through without being shaped ) . it may be very sensitive to errors in the input beam wavelength or collimation , and it may only produce the desired intensity profile over a short range of distances . like any tool available to a lens designer , doe 's have their uses . they can have very strong negative dispersion , which is often useful to correct chromatic aberration , and as i said they can be designed to produce arbitrary illumination patterns which would be outrageously difficult to make with purely refractive optics . lastly , while you can say they are " just fresnel lens with smaller features , " it is important to understand that a fresnel lens is a diffractive optic , just a very simple one . in fact , when your understanding of diffraction is deep enough , you will realize that , in some sense , all lenses are diffractive optics . while you can engineer the phase profile of a doe to produce a highly complex optical field , you could also design one to produce a simple focal spot ; the resulting design would be a simple lens !
in qed there are 4 kinds of divergences : ultraviolet divergences . naive calculations depend on the cut-off in such a way that they go to infinity as the cut-off do . however , qed is a perturbatively renormalizable theory so that non-naive , well-done computations ( see regularization and renormalization ) give sensible results . landau pole . the coupling constant $\alpha={e^2\over \hbar \ , c}$ , which is the expansion parameter in the perturbative series , grows with energy and goes to infinity for a finite value of the energy . it turns out that this finite value of energy is larger than the electroweak scale , where qed merges with the weak interaction and qed is not a good theory of nature anymore . therefore , it is not a real ( phenomenological ) problem . infrared divergences . these are due to the fact that photons are massless . they however cancel out once one takes into account all the effects that contribute to a measurable observable . non-convergent series . the $n$-th term of the perturbative expansion is of the form $\left ( {\alpha\over 2\pi}\right ) ^{n}\ , ( 2n-1 ) ! ! $ , so that the series is not convergent but asymptotic because the factor $ ( 2n-1 ) ! ! $ grows very fast for large values of $n$ . this means that we cannot give a non-perturbative definition of qft by summing up all the terms of the series . however , the first terms are meaningful and actually give predictions that accurately agree with observations . the ' first terms ' are approximately $n\sim {\pi\over \alpha}\sim 430$ . and for this value of $n$ , $\left ( {\alpha\over 2\pi}\right ) ^{n}\ , ( 2n-1 ) ! ! \sim 10^{-187}$ . therefore , as long as we are not interested in a precision of one part in $10^{187}$ , this is not a real problem either . note that qed is the theory of nature that has been confirmed with greatest precision — one part in $10^{9}$ in electron 's anomalous magnetic dipole , for which $n=4$ . for qcd points 1 , 3 , and 4 are more o less the same . however , point 2 does not apply since in qcd the coupling constant $\alpha_s$ gets lower with the increasing of energy , and in fact it goes to zero as energy goes to infinity . see asymptotic freedom . to summarize , infrared divergences are due to not taking into account effects that contribute to the observable magnitude . the asymptotic nature of qft perturbative expansions prevents a non-perturvative ( exact ) definition of the theory ( through its series ) , but does not entail a practical problem when comparing predictions with measurements . the lack of perturbative divergences and landau-like poles are a necessary condition for a theory to be well-defined at arbitrarily high energies . however , theories that contain these divergences ( ultraviolet or landau-like poles ) can still be very useful at energies above some scale . on the other hand , theories without these divergencies ( ultraviolet or landau-like poles ) , such as qcd , do not have to be valid to all energies as theories of nature . as m . brown points out in the comments , there is a relation between instantons and renormalons and the asymptotic nature of series . please , see these notes snd the questions instantons and non perturbative amplitudes in gravity and asymptoticity of pertubative expansion of qft reply to graviton 's comment : in my opinion , a fundamental theory of nature ( whatever it means ) should have a non-perturbative definition . if the perturbative expansion is not convergent , it cannot provide this non-perturbative definition . however , in principle , this does not necessarily mean that theory cannot have a non-perturbative definition or an exact solution , but this must be given by other means .
it is not non-sensical at all , except that there should not be a minus sign ( as mentioned in the comments ) and that you took an operator outside of an expectation value , which i think worked out ok in this case but in general should be avoided . more conservatively , $$ \hat x = i \hbar \frac{d}{d \hat p} $$ it follows that $$ \langle q \mid \hat x \hat p \mid q ' \rangle = q ' \langle q \mid \hat x \mid q ' \rangle = i \hbar q ' \langle q \mid \frac{d}{d \hat p} \mid q ' \rangle = i \hbar q ' \langle q \mid \frac{d}{d q'} \mid q ' \rangle $$ remember , an actual physical state is never an idealized momentum ( or position ) eigenket . a slightly more realistic description of a physical state is a wave packet with a narrow width around some momentum : $$ \mid p , \sigma \rangle = \int \frac{dq}{2 \pi \hbar} \ , \left ( \frac{2 \pi \hbar^2}{\sigma^2}\right ) ^{1/4} e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \mid q \rangle $$ you can check that this is normalized to unity , because $$ \langle p , \sigma \mid p , \sigma \rangle = 1 $$ remember that $\langle p \mid q \rangle = 2 \pi \hbar \delta ( p-q ) $ . now , the expectation value of $\hat x \hat p$ for this physical state is $$ \langle p , \sigma \mid \hat x \hat p \mid p , \sigma \rangle = \frac{1}{\sqrt{2 \pi} \ , \sigma } \int dq \ , dq ' \ , e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} e^{- \frac{1}{4} \left ( \frac{q'-p}{\sigma} \right ) ^2} \frac{\langle q \mid \hat x \hat p \mid q ' \rangle}{2 \pi \hbar} $$ using the above result , we can integrate the $q'$ integral by parts , yielding $$ \frac{-i\hbar}{\sqrt{2 \pi} \ , \sigma } \int dq \ , dq ' \ , e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \frac{d}{dq'} \left ( q ' e^{- \frac{1}{4} \left ( \frac{q'-p}{\sigma} \right ) ^2} \right ) \frac{\langle q \mid q ' \rangle}{2 \pi \hbar} $$ since $\langle q \mid q ' \rangle = 2 \pi \hbar \delta ( q-q' ) $ , we can integrate out q ( and then relabel q ' back to q ) , making this a single integral $$ \frac{-i\hbar}{\sqrt{2 \pi} \ , \sigma} \int dq \ , e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \frac{d}{dq} \left ( q e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \right ) $$ we can integrate by parts again , ending up with $$ \frac{i\hbar}{\sqrt{2 \pi} \ , \sigma} \int dq \ , q e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \frac{d}{dq} \left ( e^{- \frac{1}{4} \left ( \frac{q-p}{\sigma} \right ) ^2} \right ) $$ which is a bit easier to evaluate since we do not have to use the product rule . the derivative brings down a factor $- ( q-p ) /2\sigma^2$ , and the two exponentials combine , leaving us with $$ \frac{-i\hbar}{ 2\sqrt{2 \pi} \ , \sigma^3} \int dq \ , q ( q-p ) e^{- \frac{1}{2} \left ( \frac{q-p}{\sigma} \right ) ^2} $$ the first $q$ in the integrand can be rewritten as $ ( q-p ) + p$ . then there are two terms , the latter of which has an odd integrand ( of the form $ ( q-p ) \exp ( \alpha ( q-p ) ^2 ) $ and vanishes . so , changing the integrand to $x = ( q-p ) /\sigma$ , we are left with $$ \frac{-i\hbar}{ 2\sqrt{2 \pi}} \int dx \ , x^2 e^{- \frac{1}{2} x^2} $$ the remaining integral is a textbook gaussian integral , equal to $$ \int dx \ , x^2 e^{-\frac{1}{2} x^2} = \sqrt{2\pi} $$ so , the expectation value of the operator $\hat x \hat p$ for the wave packet is simply $$ \frac{-i\hbar}{2} $$
technically , doppler shift for light happens due to relativistic time dilation , so it is subtly different than acoustic doppler shift . it cannot therefore be used to determine an absolute reference frame . that does not mean there is not one . spacetime as we know it is formed by the big bang , so that might be an absolute reference frame if only we can figure out how to detect it . unfortunately , we can not , and in a very real sense the big bang happened everywhere , since all of space was compressed to a tiny point .
that seems like a fun question ! according to wikipedia the day is currently 2ms too long , so that is a factor of 2.31e-8 . so we need to reduce the angular momentum of the earth by this factor . to make life easy consider a mountain on the equator , with a mass $m$ , treat it as a point mass and assume we manage to move it $d$ meters nearer the centre of the earth . the change in angular momentum is : $$\delta l = m ( r_e - d ) ^2 - m ( r_e + d ) ^2 = -4mr_e d$$ where $r_e$ is the radius of the earth . assuming the earth is a uniform sphere it is angular momentum is : $$l_e = \frac {2}{5} m r_e^2$$ so i get the fractional change of the angular momentum to be : $$\frac {\delta l}{l} = 10 \frac{d}{r_e} \frac {m}{m}$$ bearing mind that we are modelling the mountain as a point mass , i would say about 10km was a reasonable distance to move it , i.e. from 5km above sea level to 5km below sea level , so taking $d$ as 10km and $r_e$ as 6380km and setting the change equal to 2.31e-8 gives : $$\frac {m}{m} = 1.5 \times 10^{-6}$$ so if the mass of the earh is about $6 \times 10^{24}$kg , you had need to move about $10^{19}$ kg of mountain . for comparison , a quick google suggests the mass of mount everest is of the order of $10^{15}$ to $10^{16}$ kg so that is somewhere between 1,000 and 10,000 mount everests .
your question is really " how does the human eye work ? " , since the contact lens is designed to adjust the optics of the human biological lens . this image from the wikipedia article on the anatomical lens shows how the lens focuses incoming light from the left onto the retina ( right ) . for people who are nearsighted or farsighted , the light coming into the eye does not end up in focus . see this picture for the case of nearsightedness : you should imagine the contact lens being placed over the cornea ( left surface ) and causing the rays to adjust so that the image ends up in focus .
the answer is in front of you $$ a = \frac{{\rm d} ( u ) }{{\rm d} t} = \frac{{\rm d}}{{\rm d} t} \left ( \frac{{\rm d} ( r ) }{{\rm d} t} \right ) = \frac{{\rm d}^2 ( r ) }{{\rm d} t^2}$$ as a matter of convention . it is just the way we write 2nd derivatives .
i am afraid the actual situation is much more complicated than you have been told . for one thing , the superconductivity does not occur between neutrons , but between quarks themselves . the topic of high density qcd is a very cool interplay of condensed matter and high energy physics , and a very nice review is available by frank wilczek . however , that article does need some background in qcd and superconductivity simultaneously to appreciate . a shortened version might go something like this : inspiration : free fermions are incredibly unstable to superconductivity , in that any attractive interaction will cause it ( in fact , there is an old theorem by pierls ( ? ) that almost all interactions ( even repulsive ones ) will cause superconductivity if you cool far enough ) . in qcd , quarks naturally attract already ! so at sufficiently high density and low temperature , we can imagine that qcd can cause a strong attractive instability to a fermi gas of quarks . complications : 6 flavours , chiralities , masses of quarks are different , etc . simplification by complication : realise that the normal state of qcd ( i.e. . 3- and 2-quark combinations ) is just that : one possible state . other phases of qcd exist , and we can study the phase boundaries and so forth even if we can not compute things exactly ( universality saves the day ! ) . we find that at really high densities , quarks pair up to give a background diquark condensate , through which single quarks move , and through the anderson-higgs mechanism gains a large mass by eating some goldstone modes . all gluons become gapped ( again , anderson-higgs ) , apart from one which mixes with the photon . the symmetries of this solution is actually the same as that of normal matter --- replace baryons with quarks ( + their diquark condensed background ) and mesons with diquarks ; this suggests that they are really the same phase in theory . in practise , getting from one to another requires some other phases in the middle , which are more complicated and arise due to the quark masses and the number of flavours , etc .
it was an early name of what became the trinity of the international bureau of weights and measures ( bipm ) , the general conference on weights and measures ( cgpm ) and the international committee for weights and measures ( cipm ) . the phrase " international service of weights and measures " or its french equivalent seems to have been used earlier at the cipm meeting of 1887 related to adoption of the centigrade scale of the hydrogen thermometer .
the most elementary reason is that the dirac field hamiltonian is bounded below only when you use anticommutation relations on the creation/annihilation operators instead of commutators . a free quantum field theory with energy unbounded below has no stable vacuum . it is easiest to demonstrate this in two dimensions , where there are no polarization issues . instructive 2d example in two dimensions ( one space one time ) , there is a nice dimensionally reduced analog , which is the right-moving ( necessarily massless ) majorana-weyl fermion ( the argument also works with 2d dirac fermions with two components , but this is the simplest case ) . this is a single component field $\psi$ which obeys the equation of motion $$ ( \partial_t -\partial_x ) \psi = 0 $$ this simple equation is derived from the 2d dirac equation using the ( real convention , explicitly real ) 2d dirac matrices ( 0,1 ; -1,0 ) an ( 0,1 ; 1,0 ) , which are $\gamma_0 = \sigma_x$ and $\gamma_1 = i\sigma_y$ . they square to 1 and -1 respectively , and they anticommute , so they reproduce the 1+1 dimensional metric tensor . the $\gamma_5$ analog , which i will call $\gamma$ to accommodate different dimensions , is diagonal in this explicit representation , and $\gamma=\sigma_z$ . the two eigenvectors of $\gamma$ propagate independently by the 2d massless equation of motion $$ \gamma_i \partial_i \psi = 0 $$ and further , because the $\gamma$ matrices are real , this is a majorana representation ( most physicists write the dirac equation with an i factor in front of the derivative , so that the dirac matrices for a majorana representation are purely imaginary . i am using a mathematician 's convention for this , because i like the equations of motion to be real . others like the k-space propagator to not have factors of i in the k part . unfortunately , physicists never settled on a unique sensible convention--- everyone has their own preferred way to write dirac matrices ) . so it is sensible in the equation of motion to restrict $\psi$ to be hermitian , since its hermitian conjugate obeys the exact same equation . so that the field has a k decomposition $$ \psi ( x ) = \int a_k e^{ikx - ikt }dk$$ and the reality condition ( hermiticity ) tells you that $a^{\dagger} ( -k ) = a ( k ) $ ( one should say that the normalization of the $a$ operators expansion is not completely conceptually trivial--- the $a$ 's are both relativistically and nonrelativistically normalized , because the spinor polarization $\sqrt{w}$ factor cancels the mass-shell hyperbola factor , so that the dk integration is not weighted by anything , it is just the normal calculus integral with uniform measure ) an operator with definite frequency , which ( heisenberg picture ) evolves in time according to $$ \partial_t o = i\omega o$$ has the property that it is a raising operator--- acting with this operator adds $\omega$ to the energy . if $\omega$ is negative , $a$ is an annihilation operator . the condition that the vacuum is stable says that all the annihilation operators give 0 when acting on the vacuum state . but notice that the frequency in the expansion of $\psi$ changes sign at $k=0$ . this came from the linearity of the dirac hamiltonian in the momenta . it means that the operator $a_k$ acts to raise the energy for k> 0 , but acts to lower the energy for $k&lt ; 0$ . this means that the $k&gt ; 0$ operators create , and the $k&lt ; 0$ operators annihilate , so that the right way to $a^{\dagger} ( -k ) $ are creation operators , while the $k&lt ; 0$ operators are annihilation operators . the energy operator counts the number of particles of momentum k , and multiplies by their energy : $$ h = \int_{k&gt ; 0} k a^{\dagger} ( k ) a ( k ) dk $$ and this is manifestly not a local operator , it is defined only integrated over k> 0 . to make it a local operator , you need to extend the integration to all k , but then the negative k and positive k contributions have opposite sign , and they need to be equal . to arrange this , you must take anticommutation relations $$ \{ a^{\dagger} ( k ) , a ( k ) \} = i\delta ( k-k&#39 ; ) $$ and then $$ h = {1\over 2} \int k a^{\dagger} ( k ) a ( k ) = \int \psi ( x ) i \partial_x \psi ( x ) dx $$ note that this looks like it is a perfect derivative , and it would be if $\psi$ were not anticommuting quantity . for anticommuting quantities , $$ \partial_x \psi^2 = \psi \partial_x \psi + \partial_x \psi \psi $$ which is zero , because of the anticommutation . deeper reasons although this looks like an accidental property , that the energy was negative without anticommutators , it is not . the deeper reason is explained with euclidean field theory using a feynman-schwinger formalism , but this requires understanding of the euclidean and path integral versions of anticommuting fields , which requires being comfortable with anticommuting quantities , which requires a motivation . so it is best to learn the shallow reason first .
the other answers are correct , but i think that you might benefit from a more " microscopic " view of what is happening here . whenever one substance ( a solute ) dissolves in another ( a solvent ) , what happens on the molecular scale is that the solute molecules are surrounded by the solvent molecules . what causes that to happen ? as @chris described , there are two principles at work - thermodynamics , and kinetics . in plain terms , you could think of thermodynamics as an answer to the question " how much will dissolve if i wait for an infinite amount of time , " whereas kinetics answers the question " how long do i have to wait before x amount dissolves . " both questions are not usually easy to answer on the macroscopic scale ( our world ) , but they are both governed by two very easy to understand principles on the microscopic scale ( the world of molecules ) : potential and kinetic energy . potential energy on the macroscopic scale , we typically only think about gravitational potential energy - the field responsible for the force of gravity . we are used to thinking about objects that are high above the earth 's surface falling towards the earth when given the opportunity . if i show you a picture of a rock sitting on the surface of the earth : and then ask " where is the rock going to go ? " you have a pretty good idea : it is going to go to the lowest point ( we are including friction here ) . on the microscopic scale , gravitational fields are extremely weak , but in their place we have electrostatic potential energy fields . these are similar in the sense that things try to move to get from high potential energy to lower potential energies , but with one key difference : you can have negative and positive charges , and when charges have the opposite sign they attract each other , and when they have the same sign , they repel each other . now , the details of how each individual molecule gets to have a particular charge are fairly complicated , but we can get away with understanding just one thing : all molecules have some attractive potential energy between them , but the magnitude of that potential energy varies by a lot . for example , the force between the hydrogen atom on one water molecule ( $h_2o$ ) and the oxygen atom on another water molecule is roughly 100 times stronger than the force between two oxygen molecules ( $o_2$ ) . this is because the charge difference on water molecules is much greater ( about 100 times ) than the charge difference on oxygen molecules . what this means is we can always think of the potential energy between two atoms as looking something like this : the " ghost " particle represents a stationary atom , and the line represents the potential energy " surface " that another atom would see . from this graph , hopefully you can see that the moving atom would tend to fall towards the stationary atom until it just touches it , at which point it would stop . since all atoms have some attractive force between them , and only the magnitude varies , we can keep this picture in our minds and just change the depth of the potential energy " well " to make the forces stronger or weaker . kinetic energy let 's modify the first potential energy surface just a little bit : now if i ask " where is the rock going to go ? , " it is a little bit tougher to answer . the reason is that you can tell the rock is " trapped " in the first little valley . intuitively , you probably can see that if it had some velocity , or some kinetic energy , it could escape the first valley and would wind up in the second . thinking about it this way , you can also see that even in the first picture , it would need a little bit of kinetic energy to get moving . you can also see that if either rock has a lot of kinetic energy , it will actually go past the deeper valley and wind up somewhere past the right side of the image . what we can take away from this is that potential energy surfaces tell use where things want ( i use the term very loosely ) to go , while kinetic energy tells us whether they are able to get there . let 's look at another microscopic picture : now the atoms from before are at their lowest potential energy . in order for them to come apart , you will need to give them some kinetic energy . how do we give atoms kinetic energy ? by increasing the temperature . temperature is directly related to kinetic energy - as the temperature goes up , so does the average kinetic energy of every atom and molecule in a system . by now you might be able to guess how increasing the temperature of water helps it to clean more effectively , but let 's look at some details to be sure . solubility we can take the microscopic picture of potential and kinetic energies and extract two important guidelines from it : all atoms are " sticky , " although some are stickier than others higher temperatures mean that atoms have larger kinetic energies going back to the coffee cup question , all we need to do now is think about how these will play out with the particular molecules you are looking at . coffee is a mixture of lots of different stuff - oils , water-soluble compounds , burnt hydrocarbons ( for an old coffee cup ) , etc . each of these things has a different " stickiness . " oils are not very sticky at all - the attractive forces between them are fairly weak . water-soluble compounds are very " sticky " - they attract each other strongly because they have large charges . since water molecules also have large charges , this is what makes water-soluble compounds water-soluble - they stick to water easily . burnt hydrocarbons are not very sticky , sort of like oils . since molecules with large charges tend to stick to water molecules , we call them hydrophilic - meaning that they " love " water . molecules that do not have large charges are called hydrophobic - they " fear " water . although the name suggests they are repelled by water , it is important to know that there are not actually any repelling forces between water and hydrophobic compounds - it is just that water likes itself so much , the hydrophobic compounds are excluded and wind up sticking to each other . going back to the dirty coffee cup , when we add water and start scrubbing , a bunch of stuff happens : hydrophilic compounds hydrophilic compounds dissolve quickly in water because they stick to water pretty well compared to how well they stick to each other and to the cup . in the case where they stick to each other or the cup better than water , the difference is not huge , so it does not take much kinetic energy to get them into the water . so , warm water makes them dissolve more easily . hydrophobic compounds hydrophobic compounds ( oils , burnt stuff , most stains ) do not stick to the water . they stick to each other a little bit ( remember that the forces are much weaker compared to water since the charges are very small ) , but water sticks to itself so well that the oils do not have a chance to get between the water molecules . we can scrub them , which will provide enough energy to knock them loose and allow the water to carry them away , but if we were to increase the kinetic energy as well by increasing the water temperature , we could overcome both the weaker forces holding the hydrophobic compounds together , while simultaneously giving the water molecules more mobility so they can move apart and let the hydrophobic compounds in . and so , warmer water makes it easier to wash away hydrophobic compounds as well . macroscopic view we can tie this back to the original thermodynamics vs . kinetics discussion . if you increase the temperature of the water , the answer to the question " how much will dissolve " is " more . " ( that was the thermodynamics part ) . the answer to " how long will it take " is " not as long " ( kinetics ) . and as @anna said , there are other things you can do to make it even easier . soap for example , is made of long chain molecules with one charged end and one uncharged end . this means one end is hydrophilic , while the other end is hydrophobic . when you add soap to the picture , the hydrophilic end goes into the water while the hydrophobic end tries to surround the oils and burnt stuff . the net result is little " bubbles " ( called micelles ) made up of soap molecules surrounding hydrophobic molecules that are in turn surrounded by water .
yes , coherent radiation means that the phases of two ( or more ) waves representing the radiation differ by a known constant . incoherence means that the phase differences are unknown/random . laser radiation is coherent because stimulated emission assures phase differences are constant . radiation from an incandescent lamp is incoherent because the electromagnetic waves are generated in a statistically random manner depending on which atoms are excited and de-excited . to develop intuition think of the classical example of soldiers marching in step . their motion is coherent and so are the vibrations they set up . they break step over ancient bridges so as to become incoherent , there were cases where ancient bridges resonated to the step and were damaged .
you are operating under a misconception . when a real image is formed , we can see it , provided that our eye is positioned in a location such that rays from the image can enter your eye . compared to a hologram , the situation is different for a couple of reasons . ( 1 ) the possible locations of your eye are more restricted . ( 2 ) you may pick up psychological cues such as the framing of the field of view , which cause your brain not to interpret the real image as being where it actually is . by the way , the word is spelled " lens , " not " lense , " and it is not always true that a convex lens produces a real image .
based on your comment , i think you are indeed asking a more profund question than your teabag suggests : why is it that gravity is so weak compared to the other forces ? the answer is : we do not know . seriously , that is one of the holy grails : to first find the grand unified theory of nature in which all forces except gravity are explained as coming from one single symmetry group which is broken at our normal energies , and to then find the theory of everything that understands all four fundamental forces with a single , unified concept . in such a theory , it is hoped , the differences between the forces emerge naturally , instead of just being put in by measuring the relative strengths experimentally . but we do not know yet . we do not know which of the manifold concepts that are at the forefront of theoretical thinking right now might turn out to be indeed the ones realized in nature . but the question why gravity is so weak is really still unsolved . string theorists let gravity act in other dimensions , thereby weakening it in our four comparative to the others . others employ a form of the anthropic principle to argue that only in a universe where gravity is as weak as here life could have evolved and asked that question , so there is no deeper reason except that , for us to be here , it has to be like that . i apologize to all those whose theory i cannot name and briefly , unjustifiedly compress into a few words off the top of my head . that is it . we really do not know .
( 1 ) first argument an ordinary object that is spinning on an axis has an angular momentum which is determined by how the mass of the object is distributed about the axis , and how fast the object is spinning . for a fixed angular momentum , if the mass is distributed farther from the axis the angular velocity is lower ; if the mass is distributed closer to the axis , the angular velocity is higher . think of a spinning ice skater who turns at one , slower speed with arms extended and at another , higher speed with arms pulled in overhead ( on the axis ) . no size has been found for electrons ; they appear to be point particles . if an electron has finite size ( which happens to be too small to see ) , it introduces issues in any classical description , like charge self-repulsion having infinite energy or having a surface which spins faster than the speed of light . if the electron is a point particle , i.e. , has no finite size , then it cannot have angular momentum due to spinning about it is own center of mass because the entire object is on its rotational axis . how to get out of this conundrum ? you cannot " see " an electron to determine whether it is spinning or not . the " spinning " of the electron is not measurable ; it makes no sense to speak of it in science . however , you can measure an electron 's angular momentum ; it makes sense to speak of angular momentum in science . therefore , do not think of the electron as a " spinning " object ( which we can never know or observe ) ; think of it as simply having " intrinsic " angular momentum . ( 2 ) second argument the usefulness of an analogy in science is determined by whether you can make inferences or understand other features in a first system under study by way of the analogy and knowledge of a second system . thinking of spin as " spinning " introduces conceptual problems classically , does not generalize easily to massless objects , does not handle half-integer spin well ( the representations of a classical spinning object do not look like a spin 1/2 object ) , and allows you to infer almost nothing correctly about the electron , other than feeling like you know where the angular momentum comes from . this point of view does not work well to start with , and is a dead end as you keep studying physics . on the other hand , thinking of spin as intrinsic angular momentum avoids all the above-identified issues and , you will find with further study , fits nicely into the rest of physics . ( 3 ) regarding spin-1/2 and the value ℏ√3/2 , these come from group theory and a particular choice of units for angular momentum . this can not be really explained well in a post ; study group theory for physicists . good luck !
few remarks consider the whole system , this is , gun and bullet . this is an isolated system , so the net momentum is constant . in particular before firing the gun , the net momentum is zero . the conservation of momentum reads $$0=m_{1}v_{1}+m_{2}v_{2} $$ if the call $1$ to the gun and $2$ the bullet : $$\boxed{v_{2}=-\displaystyle\frac{m_{1}v_{1}}{m_{2}}} $$
first , your wave equation is wrong . you can see this from dimensional analysis . it should be \begin{align} \frac{\partial^2 \phi}{\partial t^2} = c^2 \frac{\partial^2 \phi}{\partial x^2} \end{align} second , you made a mistake in the cross terms for the $\partial^2 /\partial x^2$ term . the cross term should have the coefficient $-2\gamma^2 v/c^2$ . third , use the fact that $\gamma = \frac{1}{\sqrt{1 - v^2/c^2}}$ . you will get the desired result .
the symmetry breaking just breaks the symmetry in the electroweak sector of the standard model , it does not turn off the interaction--neutrinos become different from electrons , the gauge bosons get mass , etc , but all of those things are still there . in essence , what the symmetry breaking does is make the weak interaction short-ranged .
i am confused . bernoulli 's eqn . says the static pressure inside the jet should be less than atmospheric . as you go further out and the jet slows down , then it should approach atmospheric pressure . the pressure gradient between the atmosphere outside and the low pressure inside the jet leads to air getting sucked into the jet ( entrainment ) .
i think i can tackle the mechanics aspect . what you can see is an inverted metallic cone with a helical groove running down and around its surface . on zooming in , you the perforations in the cone 's surface . there is a pump which pumps the fluid back to the top as it flows down . as the empty cone begins to refill , the fluid also starts to leak out of the sides . that is why you see that flowery pattern emerge from the bottom and travel upwards . once the fluid reaches the top the cone is filled and at maximum pressure , when the pump is turned off and the cycle begins again . so the purely mechanical aspect of this construction is very clever on its own and presumably substituting other fluids would yield different kinds of fountains . the leafy structure is generated due to magnetic fields . i can not say how exactly . ps : on further reflection it seems that one can have an electromagnetic pump rather than a mechanical one . imagine a inductor running through the center of the cone . a sinusoidal current would generate a sinusoidal magnetic field within the inductor , which would alternately pull the fluid up the cone and then push it back down .
the jiles-atherton model of ferromagnetism is used in some circuit analysis programs . it may be overkill for this question , but it does give pretty pictures . i am going to work in mks units exclusively . the model equations are : $$ b= \mu_0 m \quad , \quad m = m_{rev} + m_{irr} $$ where $b$ is the magnetic flux density , and the magnetization $m$ is composed of reversible ( $m_{rev}$ ) and irreversible ( $m_{irr}$ ) components . physically , during the magnetization process : $m_{rev}$ corresponds to ( reversible ) magnetic domain wall bending ( the s-shaped magnetization curve , but without hysteresis ) $m_{irr}$ corresponds to ( irreversible ) magnetic domain wall displacement against the pinning effect ( the hysteresis ) . these components are calculated according to : $$ m_{rev} = c ( m_{an} - m_{irr} ) $$ $$ m_{an} = m_s \left [ \coth \left ( \frac{h + \alpha m}{a}\right ) -\frac{a}{h+ \alpha m}\right ] $$ $$ \frac{dm_{irr}}{dh} = \frac{m_{an}-m_{irr}}{k \delta - \alpha ( m_{an}-m_{irr} ) } $$ here the anhysteretic magnetization $m_{an}$ represents the magnetization for the case where the pinning effect is disregarded . ( this case corresponds to $c=1$ , where $m=m_{an}$ and $m_{irr}$ therefore does not contribute to $m$ . ) the quantity in the expression for $m_{an}$ in square brackets is the langevin function $\mathcal{l}$: $$ \mathcal{l} ( x ) = \coth ( x ) - \frac{1}{x} \quad , \quad \mathcal{l} ( x ) \approx \left\{\begin{array}{ccc} \frac{x}{3} and , and |x|&lt ; &lt ; 1 \\ 1 and , and x &gt ; &gt ; 1 \\ -1 and , and x&lt ; &lt ; -1 \end{array} \right . $$ and $\delta$ is the sign of the time rate of change of the applied magnetic field $h$: $$ \delta = \left\{ \begin{array}{ccc} +1 and , and \frac{dh}{dt}&gt ; 0 \\ -1 and , and \frac{dh}{dt}&lt ; 0 \end{array} \right . $$ $m_{rev}$ can be eliminated from this system of equations to reduce their number by 1: $$ m = c m_{an} + ( 1-c ) m_{irr} $$ the equations for $m , m_{an} , $ and $m_{irr}$ are inter-dependent and so are to be solved simultaneously . there are 5 parameters ( listed here together with sample values ) : $m_s$ , the saturation magnetization [ 1.48 ma/m ] $c$ , the weighting of anhysteretic vs . irreversible components [ 0.0889 ] $\alpha$ , the mean field parameter ( representing interdomain coupling ) [ 0.000938 ] $a$ sets the scale for the magnetic field strength [ 470 a/m ] $k$ sizes the hysteresis [ 483 a/m ] for the values listed , a crude spreadsheet produced this plot : the horizontal axis is the applied magnetic field h , in a/m , sweeping from 0 up to 2500 , then down to -2500 , and then up again to 2500 . the vertical axis is the flux density b in t . this example comes from a 1999 ieee paper by lederer et al , " on the parameter identification and application of the jiles-atherton hysteresis model for numerical modelling of measured characteristics " . it appears that choosing the parameters to match a given material is a chore , but that is another story . . .
it is easy to forget that , in the context of relativity , there is no universal time . you write : for an outside observer the time seems to stop at the event horizon . my intuition suggests , that if it stops there , then it must go backwards inside . is this the case ? but your intuition does not seem to take into account that , for an observer falling into the hole , time does not stop at the event horizon . the point is that one must be much more careful in their thinking about time within the framework of general relativity where time is a coordinate and coordinates are arbitrary . in fact , within the event horizon , the radial coordinate becomes time-like and the time coordinate becomes space-like . this simply means that , to " back up " inside the event horizon is as impossible as moving backwards in time outside the event horizon . in other words , the essential reason it is impossible to avoid the singularity once within the horizon is precisely that one must move forward through time which , due to the extreme curvature within the horizon , means moving towards the central singularity .
33 mj is the electrical energy . i think the projectile is about 1kg , so the efficiency is about 10% , not so bad . plasma from electrical breakdown , which then gets accelerated the same way the projectile does , with exb force
i understand that a general interpretation of the $1/r^2$ interactions is that virtual particles are exchanged [ . . . ] general relativity does not suppose that zero-mass particles exchanged . you do not need quantum field theory for this . in a purely classical field theory , we have field lines , and the field lines from a spherically symmetric source should radiate outward along straight lines . in a frame where the source is at rest , we expect by symmetry that the field lines are uniformly distributed in all directions . the strength of the field is proportional to the density of the lines , which falls off like $1/r^2$ in a three-dimensional space . this whole description is complicated by the polarization of the field . gravitational fields have complicated polarization modes . nevertheless , the $1/r^2$ result is unaffected . finally , we have an issue unique to gr , which is that the field is the metric , and this means that the field itself affects the measuring tools that we use to measure things like $r$ , the field , and the area of a surface through which we are counting the number of field lines that penetrate . these are all strong-field issues , so for large $r$ , they do not affect the $1/r^2$ argument . is it a postulate ? no . in the standard formulation of gr , the main postulate is the einstein field equations . from it , we can prove birkhoff 's theorem , which says that the schwarzschild metric is the external field of a static , spherically symmetric source . the weak-field limit of the schwarzschild metric corresponds to a $1/r^2$ field .
there are stable orbits around the moon , also nasa , esa , and many more have satellits orbiting the moon , and they do not " decay " towards the earth . wiki list of lunar probes look in the link for orbiters to look for the physics and no only examples search for hill sphere : one google entry hill sphere wiki sphere of influence wiki acording to the last link , the moon 's sphere of influence is 38 moon radii or $66.1\cdot 10^{3}km$ plenty of space to host an asteroid .
[ this is now a long answer . in summary , generally you need a physical assumption , the clock postulate , which people tend to omit , but is necessary , and can not be argued for a priori . however sometimes special relativity plus a restricted version of the postulate suffices . mundane experience is sufficient to verify this restricted version . ] let $\lambda = t$ the time according to inertial $o$ , and let $\vec{x}'$ be the spatial position of $o'$ according to $o$ , while $t'$ is the time measured by $o'$ . if $o'$ is piecewise inertial , then along each piece , $$c^2 ( \delta t'/\delta t ) ^2 = c^2 - ( \delta\vec{x}'/\delta t ) ^2\qquad [ 1 ] $$ and what you are trying to justify is that , even if $o'$ is not piecewise inertial , $$c^2 ( dt'/dt ) ^2 = c^2 - ( d\vec{x}'/dt ) ^2\qquad [ 2 ] $$ so , the problem is , special relativity strictly speaking only makes claims about inertial observers . and if you do not make any assumptions whatsoever about the experience of accelerated observers , then i think you are just stuck , mathematically i do not think you can go from $ [ 1 ] $ to $ [ 2 ] $ . ( for example , we can not rule out that proper acceleration itself further contributes to time dilation . ) i suggest : the motion of $o'$ is smooth . [ a1 ] for every $\epsilon&gt ; 0$ , there is a $\delta&gt ; 0$ such that , if , from the point of view of a certain unaccelerated observer $a$ , the magnitude of the velocity of another observer $b$ never exceeds $c\delta$ betwen time $t_0$ and $t_1$ , then time lapse $\delta t_b$ on $b$ 's clock satisfies $ ( t_1-t_0 ) ( 1-\epsilon ) &lt ; \delta t_b &lt ; ( t_1-t_0 ) ( 1+\epsilon ) $ . [ a2 ] pick $\epsilon&gt ; 0$ , use [ a2 ] to get $\delta$ ; use [ a1 ] to break the motion of $o'$ into intervals small enough such that , from the frame of reference of an interial observer travelling between the endpoints of a piece , the velocity of $o'$ never exceeds $c\delta$ ; use [ a2 ] to make $ [ 2 ] $ true within $\epsilon$ . since this works for all $\epsilon&gt ; 0$ , [ 2 ] is simply true . now [ a1 ] might look suspect , since we have used a piecewise inertial observer , whose motion is obviously not smooth ! so we can not even assume anything about what this piecewise inertial observer experiences at the corners ! but that is okay , [ a2 ] only refers to the individual pieces and not the whole . use a family of ( truly ) inertial observers that meet at the appropriate points . as for [ a2 ] , it is a bit opaque , but what it says that if you are not moving too fast relative to an inertial observer , your experience of time is almost the same . this does not follow logically from anything in particular , it is just a physical assumption . but note that special relativity is so hard for many people to accept precisely because [ a2 ] is a fact of life , for reasonably small $\epsilon$ . to make it true for even smaller $\epsilon$ requires more than everyday experience , but it is still " common sense " , and presumably testable to quite small values . now , to believe it literally for arbitrarily small $\epsilon$ requires quite a leap , but do not take differential equations literally . ( added : ) aha ! i found the clock postulate for accelerated observers , and i do believe [ a2 ] is interchangeable with it . and yes , it is often omitted but cannot be derived from other assumptions . it has been tested . ( second addendum ) : even though they are interderivable , mine is better :- ) i have given the accuracy of [ 2 ] directly in terms of the accuracy of [ a2 ] . for example , we do not need the full clock postulate for the twin paradox ( which you mention as a motivating example in a comment ) : the proper acceleration of $o'$ is continuous and its magnitude is bounded by $a_{max}$ . [ a1' ] ( over any finite interval , [ a1 ] does imply [ a1' ] for some value of $a_{max}$ . and [ a1' ] is sufficient for the above argument . ) now , even with mundane accelerations , the twin paradox can produce a sizeable mismatch in ages within a human lifetime . ( besides , if they are not survivable accelerations , the travelling twin 's lifetime ends ! ) so , there is a usable $a_{max}$ for [ a1' ] . and , mundane experience alone proves [ a2 ] up to that $a_{max}$ and down to fairly small $\epsilon$ . so [ 2 ] holds sufficiently accurately to give the twin paradox . we only need special relativity plus a mundane restricted clock postulate . ( i realise you can bypass the whole acceleration question by altering the paradox so that there are three inertial observers who compare clocks as they pass . but then it is not the twin paradox anymore , duh ! )
$k$ is just a quantum number . $\hbar k$ gets its name " crystal impulse " from the fact , that the formula for a band structure without interaction ( free electrons ) coincides with the formula you get with the definition of classical impulse in terms of $k$ , but it is not an actual impulse . for a free electron we have the energy dispersion : $$ \epsilon ( k ) = \frac{\hbar^2k^2}{2m} $$ the wave definition of classical impulse is $$ p = \hbar k $$ which suggests the connection between energy and impulse $$ p = \frac{m}{\hbar}\nabla_k \cdot\epsilon ( k ) $$ however this is $\neq \hbar k$ in most dispersion relations . take for example the lowest conduction band in silicon : the minimum is not at $k = 0$ . with the classical relation this would mean $p \neq 0$ , but since its a minimum $\nabla_k\cdot\epsilon ( k ) = 0$ and therefore $p = 0$ . this answers your question , whether the electrons impulse at $k = 0$ is also $0$ ( it also has nothing to do with heisenberg 's principle ) . this tells us that the $k$ in dispersion relations does not give anything about the impulse of an electron directly and therefore not about the overal wave packet . to obtain the impulse you have to look at the relation with $\nabla$ given above .
in the good old newtonian world the gravitational acceleration is just : $$ g = \frac{gm}{r^2} $$ the equation you give is just a rewriting of this . if you substitute : $$ r_s = \frac{2gm}{c^2} $$ into : $$ \frac { r^2 }{r_s} \frac {g}{c^2} = \frac {1}{2} $$ you will find it simplifies to the first equation . so there is nothing especially meaningful in this procedure . you say : but then at that point light cannot escape so i would think that $g \rightarrow c/t$ where $t=1$ sec . but this is not a well motivated statement as this is not the relationship between the acceleration and the ( newtonian ) escape velocity . the equation for the escape velocity is : $$ v_e = \sqrt{\frac{2gm}{r}} $$ so if you substitute the expression for $r_s$ you end up with : $$ v = c $$ this is the well known result that at the event horizon the escape velocity calculated from newtonian gravity is equal to the speed of light . however you should not take this result too seriously . if you want to understand why light can not escape from the event horizon you need to look at the general relativistic treatment . as it happens this is dicussed in the question why is a black hole black ?
so , what is antimatter ? even from the name it is obviously the " opposite " of ordinary matter , but what does that really mean ? as it happens there are several equally valid ways to describe the difference . however , the one that i think is easiest to explain is that in antimatter , all of the electrical charges on all of the particles , at every level , have been switched around . thus ordinary electrons have negative charges , so their antimatter equivalents have positive charges . protons are positive , so in antimatter they get the negative charges . even neutrons , which have no overall charge , still have internal parts ( quarks ) that very definitely have charges , and those also get flipped around . now to me the most remarkable characteristic of antimatter is not how it is differs from ordinary matter , but how amazingly similar it is to ordinary matter . it is like an almost perfect mirror image of matter -- and i do not use that expression lightly , since it turns out that forcing ordinary matter into becoming its own mirror image is one of those other routes i mentioned for explaining what antimatter is ! the similarity is so close that large quantities antimatter would , for example , possess the same chemistry as ordinary matter . for that matter there is no reason why an entire living person could not be composed of antimatter . but if you do happen to meet such a person , such as while floating outside a space ship above earth , i strongly recommend that you be highly antisocial . do not shake hands or invite them over , whatever you do ! the reason has to do with those charges , along with some related factors . everyone knows that opposite charges attract . thus in ordinary matter , electrons seek out the close company of protons . they like to hang out there , forming hydrogen . however , in ordinary matter it also turns out that there are also all sorts of barriers -- i like to think of them as unpaid debts to a very strict bank -- that keep the negative charges of electrons from getting too close to the positive charges of the protons . thus while the oppositely charged electrons and protons could in principle merge together and form some new entity without any charge , what really happens is a lot more complicated . except for their opposite charges , electrons do not have the right " debts " to pay off everything the protons " owe , " and vice-versa . it is like mixing positive apples with negative oranges . the debts , which are really called conservation laws , make it possible for the powerfully attracted protons and electrons to get very close , but never close enough to fully cancel out each other 's charges . that is a really good thing , too . without that close-but-not-quite-there mixing of apples and oranges , all the fantastic complexity and specificity of atoms and chemistry and biochemistry and dna and proteins and us would not be here ! now let 's look at antimatter again . the electrons in antimatter are positively charged -- in fact , they were renamed " positrons " a long time ago -- so like protons , they too are strongly attracted to the electrons found in ordinary matter . however , when you add electrons to positrons , you are now mixing positive apples with negative apples . that very similarity turns out to result in a very dangerous mix , one not at all like mixing electrons and protons . that is because for electrons and positrons the various debts they contain match up exactly , and are also exactly opposite . this means they can cancel each other 's debts all the way down to their simplest and most absolute shared quantity , which is pure energy . that energy is given off in the form of a very dangerous and high-intensity version of light called gamma rays . so why do electrons and positrons behave so very badly when they get together ? here 's a simple analogy : hold a rubber band tightly at its two ends . next , place an aaa between the strands in the middle . ( this is easier for people with three arms . ) next , use the battery to wind up the rubber band until it is quite tight . now look at the result carefully . notice in particular that the left and right sides are twisted in opposite directions , and in fact are roughly mirror images of each other . these two oppositely twisted sides of the rubber band provides a simple analog to an electron and a positron , in the sense that both store energy and both have a sort of defining " twistiness " that is associated with that energy . you could easily take the analogy a bit farther by bracing each half somehow and snipping the rubber band in the middle . with that more elaborate analogy the two " particles " could potentially wander off on their own . for now , however , just release the battery and watch what happens . ( important : wear eye goggles if you really do try this ! ) since your two mirror-image " particles " on either side of battery have exactly opposite twists , they unravel each other very quickly , with a release of energy that may send the battery flying off somewhere . the twistiness that defined both of the " particles " is at the same time completely destroyed , leaving only a bland and twist-free rubber band . it is of course a huge simplification , but if you think of electrons and positrons as similar to the two sides of a twisted rubber band , you end up with a surprisingly good feel for why matter and antimatter are dangerous when placed close together . like the sides of the rubber band , both electrons and positrons store energy , are mirror images of each other , and " unravel " each other if allowed to touch , releasing their stored energy . if you could mix large quantities of both , the result would be an unraveling whose accompanying release of energy would be truly amazing ( and very likely fatal ! ) to behold . now , given all of that , how " real " is antimatter ? very , very real . its signatures are everywhere ! this is especially true for the positron ( antimatter electron ) , which is the easiest form of antimatter to create . for example , have you ever heard of a medical procedures called a pet scan ? pet stands for positron emission tomography . . . and yes , that really does mean that doctors use extremely tiny amounts of antimatter to annihilate bits of someone 's body . the antimatter in that case is generated by certain radioactive processes , and the bursts of radiation ( those gamma rays ) released by axing a few electrons help see the doctors see what is going on inside someone 's body . signatures of positrons are also remarkably common in astrophysics , where for example some black holes are unusually good at producing them . no one really understands why certain regions produce so many positrons , unless someone has has some good insights recently . positrons were the first form of antimatter predicted , by a very sharp fellow named paul dirac . not too long after that prediction , they were also the first form of antimatter detected . heavier antimatter particles such as antiprotons are much harder to make than positrons , but they too have been created and studied in huge numbers using particle colliders . despite all of that , there is also a great mystery regarding antimatter . the mystery is this : where did the rest of the antimatter go ? recall those debts i mentioned ? well , when creating universes physicists , like other notable entities , like to start the whole shebang off with pure energy -- that is to say , with light . but since matter has all those unbalanced debts , the only way you can move smoothly back and forth between light and matter is by having an equal quantity of antimatter somewhere in the universe . an amount of antimatter that large flat-out does not seem to exist , anywhere . astrophysicists have by now mapped out the universe well enough to leave no easy hiding places for large quantities of antimatter . recall how i said antimatter is very much like a mirror image of matter ? that is an example of a symmetry . a symmetry in physics is just a way of " turning " or " reflecting " or " moving " something in a way that leaves you with something that looks just like the original . flipping a cube between its various sides is a good example of a " cubic symmetry , " for example ( there are fancier words for it , but they mean the same thing ) . symmetries are a very big deal in modern physics , and are absolutely critical to many of our deepest understandings of how our universe works . so matter and antimatter form an almost exact symmetry . however , that symmetry is broken rather spectacularly in astrophysics , and also much more subtly in certain physics experiments . exactly how this symmetry can be broken so badly at the universe level while being only very subtly broken at the particle level really is quite a bit of a mystery . so , there you have it , a mini-tutorial on both what antimatter is and where it occurs . while it is a bit of overkill , your question is a good one on a fascinating topic . and if you have read through all of this , and have found any of what i just said interesting , do not just stop here ! physics is one of those topics that gets more fascinating as you dig deeper you get into it . for example , some of those cryptic-looking equations you will see in many of the answers here are also arguably some of the most beautiful objects ever uncovered in human history . learning to read them well enough to appreciate their beauty is like learning to read great poetry in another language , or how to " hear " the deep structure of a really good piece of classical music . for physics , the reward is a deep revelation of structure , beauty , and insight that few other disciplines can offer . do not stop here !
if you have watched any of the popular science programmes on the higgs boson you have almost certainly got the wrong idea about it . in particular , the title to your question suggests you have heard the description " god particle " once too often since the higgs is not the root of all elementary particles . physicists believe that particles acquire mass through a process called electroweak symmetry breaking . i am not sure that this has been proven , but the theory fits observations so well that everybody believes it . anyhow , the higgs mechanism is one of many mechanisms by which the symmetry breaking could occur . actually it is the simplest and most elegant mechanism , and all the other suggestions have various problems associated with them , so most people 's money is on the higgs , especially now there is a hint of it at the lhc . but there are various other possible mechanisms including a composite higgs theory known as the little higgs , and it is possible that experiment could prove the little higgs theory to be correct . alternatively another symmetry breaking mechanism like technicolor , that does not have a higgs boson at all , could turn out to be correct . so while it currently seems most likely that the simple higgs boson model is correct , the higgs might not be a fundamental particle or might not exist at all . at the end of the day electroweak symmetry gets broken and particles acquire masses , so all the alternatives to the higgs have to do basically the same thing . failing to find a simple higgs boson would not be the end of physics .
the criterion you mention is roughly the threshold for the formation of the coulomb gap in the hubbard model or the local moment in the anderson model . it is a common break-down region for many approaches starting from one of the limits ( insulator/local moments versus conductor/mixed valence ) . for perturbation theory in $u$ , see the prb 36 , 675 ( 1986 ) by horvatić et al . and references to and form that paper . a more comprehensive discussion can be found in the monograph by hewson . as far as i remember , perturbation in $u$ on the level of self-energy does not give the expected exponential dependence on $u$ for the kondo temperature . unfortunately , i do not know specifics of flex method to help you in more detail .
for clarity , there is a common misconception about plasma here . plasma when being introduced for the first time to someone who does not know what it is , it is called " the fourth state of matter " which is an inaccurate description of it . since this term is used for introducing some one to plasma , it is no big deal . when a material changes from a distinct phase to another , it goes through a physical process called phase transition . when gas becomes plasma , it does not go through the standard phase transition . hence plasma-in a general sense-can not be regarded as a distinct phase as solid , liquid and gas phases . it is a phase of the gaseous state . in certain rare cases however , transition from gas to plasma can be described as phase transition . plasma by definition is a mixture of free electrons and their ions ( possibly negative ions ) . you need enough energy to liberate electrons from atoms . roughly speaking , when you put that energy in a solid , energy might be dissipated as heat . if you put that energy in a liqued , energy might be dissipated in vaporization . if you put it in a gas it goes into breaking atoms and molecules ( creating plasma ) . the following figure makes it clearer hopefully that was useful
this is a difficult concept to talk about without using a few proper definitions . unfortunately these radiometric definitions all sound very similar and have similar meanings but important differences . intensity is the rate of energy transfer per unit area . intensity does not have direction and thus we cannot even talk about radiation having ' different ' intensities in different directions by definition . the radiant intensity , however , is the amount of energy transfer per unit solid angle . the radiance is the amount of energy transfer per unit area per unit solid angle . lambert 's cosine law states that the radiant intensity of an ideally diffuse emitter ( e . g . a perfect black body ) is proportional to $\cos^2\theta$ , where $\theta$ is the angle from surface normal . however , the apparent surface area of a flat object is also reduced by a factor of $\cos^2\theta$ when viewed from an angle $\theta$ from normal . thus the radiance does not vary with angle . when you use the term intensity in your question , you probably mean radiance , which is the most natural term to talk about . it is the same as talking about how bright an object appears . to quote wikipedia : radiance is useful because it indicates how much of the power emitted by an emitting or reflecting surface will be received by an optical system looking at the surface from some angle of view . in summary : is the radiance the same in every direction ? yes . does the radiance in each direction have the same spectral distribution ? yes .
in your post when you say you ' know ' the position and momentum of a single photon you really do not know anything , you are just making a prediction , not making a measurement . in your head you are basically assuming classical physics and using the initial parameters of the system to calculate the final parameters . in order to actually know any properties about a system you will have to perform a measurement , and to really say anything conclusive you will have to do this many times . take your single photon source and measure the momentum and position of the outgoing photons numerous times - the product of the standard deviation in momentum and position will be greater than $\frac{\hbar}{2}$ .
the diagram looks like the fat man bomb that was dropped on nagasaki . the wikipedia article gives lots of info on the design if you are intereted in pursuing it further . the casing is just to make it aerodynamically stable so it fell in a controlled way . the bomb itself is spherical so the case could be spherical as well if it were not for aerodynamic requirements .
a particle as a point mass does not have rotation defined . so the question does not apply to point masses . in fact , rotations are used to describe the position of a point mass riding on a moving coordinate frame . i see rotation as a property of the frame of reference , and not necessarily of the masses tracked .
assuming 1 . your body was given an initial speed , not force ( because forces act over time ) . i am calling it : $$\dot\theta_0$$ 2 . the friction force remains constant . 3 . the dis is spinning along its symmetry axis and is full if a body is spinning then it would remain spinning ( 1st law of newton ) . the constant force is slowing down the rotation of the disc , but to know how long it would take you must know how much the force is needed to change the speed . you must take in to account the moment of inertia . the moment of inertia is usually a matrix and depends on the directions , but since you have a disk , its easy to find its moment of inertia , assuming the disc is uniform ( as in , its not a ring , this matter ) , its moment of inertia is $$ i = \frac{m r^2}{2}$$ taken from here , note it matters in which direction it spins . from this point you can treat this as a free-fall problem where $$ mg \rightarrow f $$ $$ m \rightarrow i $$ $$ v_0 \rightarrow \dot\theta_0 $$ we solve as we would : $$ v_0 - at^2 =0 $$ or $$ \dot\theta_0 - \frac{f}{i}t^2 =0 $$ so $$ t =\sqrt{\frac{\dot\theta_0 i}{f}} $$ i am not 100% i did not miss anything , so please comment so i can fix before down-voting .
it is a stupid mistake . when r is less than the radius of the cylinder , the electric field will be 0 . i solved the problem not taking this into account and just plugged in the numbers . my method is perfectly valid for any r greater than 4cm .
you say : terminal velocity depends on two things- surface area and speed but i think you are getting slightly mixed up about the terminology . the drag ( i.e. . air resistance ) depends on surface area and speed , but the terminal velocity is the speed and it just depends on the surface area ( and air temperature , density , etc , etc that we will assume is constant ) . you say ; with the parachute you have a larger surface area but lower speed and this is quite correct but the speed is the terminal velocity so with the parachute you have a larger surface area but lower terminal velocity .
no . it does not measure time of flight . kinect is , deep down , a structured light scanner , meaning that it projects an infrared pattern ( so invisible for us ) . according to the underlying technology firm primesense , the structured light code is drawn with an infrared laser . this pattern is then read by an infrared camera and the 3d information is reconstructed from the distortions of the pattern . this results in a depth channel which is made available through usb . if you want to see the pattern , you may have some luck by turning off all the lights in the room , turn on the kinect , and try to use your cellphone camera . generally , these camera sensors are sensitive to ir , which appears as green . you may verify if this is the case by trying the same with a tv remote and pressing the buttons . the led should turn green .
yes , your friend is right . within electrostatics , an electric field $\vec{e}$ should be curl-free $\vec{\nabla} \times\vec{e}= \vec{0}$ . the drawn electric field lines looks like the electric field is of the form $$ e_x=e_x ( y ) , \qquad e_y=0 , \qquad e_z=0 , $$ cf . the rule that to depict the magnitude $|\vec{e}|$ , a selection of field lines is drawn such that the density of field lines ( number of field lines per unit perpendicular area ) at any location is proportional to $|\vec{e}|$ at that point . here the $x$-axis is horizontal , the $y$-axis is vertical , and the $z$-axis perpendicular to the plane . this is only curl-free if $e_x=e_x ( y ) $ is independent of $y$ , which it is not on the figure .
i will once again state that string theory , any theory , cannot be proven right by any experiment . the experiment might validate the theory , i.e. come as a result of a prediction from the theory . at the moment there does not exist one string theory in the manner that there exists one general relativity theory . there are many models based on string theory , though . why such an interest ? because at the moment string theories are the only theories that can accommodate the standard model of particle physics and at the same time allow for the quantization of gravity , which has been the holy grail of theoriticians the past fifty years . that is they promise a " theory of everything , toe ) . what might disprove the usefulness of string theories for a toe would be if supesymmetry were falsified at the lhc , for example . if nothing is seen other than the higgs at the lhc , ss would seem as a nice try but bad luck . then the usefulness of strings becomes doubtful . if ss is seen in the lhc and studied as well as the sm in the international linear collider to be built in the future , then strings will be good as candidates of a toe .
0 . caveat lector : this was done before i drank my morning coffee , so there may be some errors in the reasoning ( well , the physical reasoning , the mathematics should be kosher ) . 1 . perfect fluid . so we have two stress-energy tensors here . one is the stress energy tensor for a perfect fluid $$\tag{1}t^{\alpha\beta}_{\text{fluid}} = \rho \ , u^\alpha \ , u^\beta + p \ , h^{\alpha\beta}$$ where we have the worldlines of the fluid 's particles have velocity $u^\alpha$ the projection tensor $h_{\alpha\beta} = g_{\alpha\beta} + u_\alpha \ , u_\beta$ projects other tensors onto hyperplane elements orthogonal to $u^\alpha$ the matter density is given by the scalar function $\rho$ , the pressure is given by the scalar function $p$ . we had need extra terms if there were heat flow or shear involved . 2 . scalar field . now , we have another distinct stress-energy tensor for a massless scalar field : $$\tag{2}t^{\mu \nu}_{\text{scalar}} =\partial^{\mu}\phi\ , \partial^{\nu}\phi-\frac{1}{2}g^{\mu \nu}\partial_{\rho}\phi\ , \partial^{\rho}\phi$$ we would use this equation when modeling , e.g. , massless pions ( or some other massless spin-0 field ) . 3 . problem : are these two related ? now if we take our matter density to be , in the appropriate units , $$\tag{3a} \rho = 1 + \frac{1}{2}\partial_{\rho}\phi\ , \partial^{\rho}\phi $$ and the pressure $$\tag{3b} p = \frac{-1}{2}\partial_{\rho}\phi\ , \partial^{\rho}\phi $$ then ( 2 ) resembles ( 1 ) . this is after pretending $\partial^{\mu}\phi=u^{\mu}$ , which terrifies the original poster ( but that is what condensed matter physicists do , so i suppose i could end here content ) . is this kosher ? we should first note if we wanted to take the derivative of some function along the worldline $x^{\mu} ( s ) $ with respect to the " proper time " ( length ) $s$ we have $$\tag{4} \frac{\mathrm{d}f}{\mathrm{d}s}=\frac{\mathrm{d}x^{\mu}}{\mathrm{d}s}\frac{\partial f}{\partial x^{\mu}}$$ by the chain rule . for general relativity , we use the " comma-goes-to-semicolon " rule , but for a scalar quantity $f$ we have $$ \nabla_{\mu}f = \partial_{\mu}f . $$ ( if this is not obvious , the reader should consider it an exercise to prove it to him or herself . ) the punchline : identifying $\partial^{\mu}\phi=u^{\mu}$ is kosher . how ? observe in equation ( 4 ) the guy in front , the $\mathrm{d}x^{\mu}/\mathrm{d}s$ is just some vector . so in the very , very special case that equations ( 3a ) and ( 3b ) hold , and $\mathrm{d}x^{\mu}/\mathrm{d}s= ( 1,0,0,0 ) $ , we see that we can indeed recover the first stress-energy tensor as a special case of the scalar field 's stress-energy tensor .
in the first experiment , work done is 0 as volume is constant . using the first law of thermodynamics $q=u-w$ , $q=\delta u$ . in the second case extra heat is needed due to the work done which is $\delta ( pv ) =p\delta v$ , as pressure is constant . using the ideal gas equation $p \delta v = nr \delta t=2r\delta t$ . note the change in internal energy depends only on the change in temperature and is same in both the cases .
yes there is . i use the summation convention throughout ; repeated indices are summed over 1,2,3 . begin with the canonical commutation relations ( ccrs ) \begin{align} [ x_i , p_j ] = i\hbar\delta_{ij} i , \qquad [ x_i , x_j ] = 0 , \qquad [ p_i , p_j ] = 0 . \end{align} define the components of orbital angular momentum as follows : \begin{align} l_i = \epsilon_{ijk}x_jp_k \end{align} prove your desired identities by applying the definition of the angular momentum components and by repeatedly using the ccrs . i will leave the details to you ; it is a good exercise to get comfortable with using the ccrs to prove stuff . it actually turns out to prove useful to first prove the following identities which encode the fact that the $x_i$ are components of a " vector operator " and so are the $p_i$ . \begin{align} [ x_i , l_j ] =i\hbar\epsilon_{ijk}x_k \end{align}
your equation is almost correct . updated using x-axis along ab taking the x-axis along ab yields $$ 50 t \sin ( \theta-15^\circ ) = 26 t \sin ( 40^\circ ) $$ $$ \sin ( \theta-15^\circ ) = 0.52 \sin ( 40^\circ ) $$ $$ \theta = 34.527^\circ $$ $$ \cos ( \theta-15^\circ ) = \sqrt{1-\sin^2 ( \theta-15^\circ ) } $$ and the y-axis perpendicular to ab $$ 50 t \cos ( \theta-15^\circ ) = 20 + 26 t \cos ( 40^\circ ) $$ $$ t = 0.735 $$ using x-axis along ac ( interception pt ) taking y-axis perpendicular to ac $$ 50 t = 20 \cos ( \theta-15^\circ ) +26 t \cos ( 55^\circ-\theta ) $$ $$ t = \frac{20 \cos ( \theta-15^\circ ) }{50 - 26 \cos ( 55^\circ-\theta ) } $$ taking x-axis along ac $$ 20 \sin ( \theta-15^\circ ) =26 t \sin ( 55^\circ-\theta ) $$ which when expanded you need to solve an equation of the form $$a\cos \theta + b \sin \theta = c$$ for $\theta$ with the same results as above .
you are right that the force balance is non-zero and that the pendulum-bob is not moving , but this does not mean that the pendulum-bob is not accelerating does it . so , at the moment the ' bob ' is still , it is accelerating back to the centre-line of the oscillation with it is maximum absolute velocity . see here for more information and a nice animation of this phenomenon . i hope this helps .
the localization or delocalization of the excess charge in conductors and insulators can be understood in a way similar to the uncharged case using band theory . please refer to the first diagram in : http://en.wikipedia.org/wiki/work_function for simplicity let us consider the zero temperature case . the fermi energy $e_f$ can be thought of as a level which separates the occupied states from the empty states . now , as you may know , the fermi energy in a metal would lie in a band . in other words , in metals you have half filled bands ; this is why electrons , near the fermi energy , are delocalized in metals . in semiconductors , however , the fermi energy lies within the band ( as shown in the figure ) leaving all the bands either completely filled or empty . as a result , the electrons are localized . since the fermi energy separates the filled and empty states , it can , intuitively , be thought of as the surface of a fluid in a container ; the fluid is analogous to the electrons in the system . now , as you add or remove the fluid , its surface will either rise or drop respectively . changes in the fermi energy can be visualized in the same way . there is , however , one caveat : the vacuum energy $e_{vac}$ will also change , in addition to $e_f$ , as excess charge is introduced . $e_{vac}$ is considered as the energy at which the electron is no longer bound to the solid . if the solid is charged positively or negatively , it will be harder or easier for the electron to escape respectively . as a result , one only considers changes in work function $\phi$ or electron affinity $e_{ea}$ . the former is often used in the case of metals and the latter in the case of semiconductors or insulators . to sum it all up , excess charges will result in changes in $\phi$ and $e_{ea}$ . after these changes have occurred , by the introduction of access charges , it is a question of where the fermi energy sits . depending on that , the electrons will either be localized or delocalized . for a reasonable value of excess charge the fermi energy will only move by a small amount , and will still typically lie in one of the bands or in the band gap in a metal or insulator respectively . this is why the excess charge will be delocalized , and cover the entire surface of the metal ; whereas the excess charge will be localized in an insulator . if you want to get a better feel for how $e_f$ , $e_{vac}$ , $\phi$ , and $e_{ea}$ change as excess charge develops on metals , insulators , and semiconductors , you can take a look at chapter 2 of : http://www.amazon.com/field-effect-devices-volume-edition/dp/0201122987/ref=sr_1_1?ie=utf8qid=1348762717sr=8-1keywords=field+effect+devices
molecules are broken apart when they collide with one another with enough force to break the bonds that hold them together . these collisions happen all the time and depend on the density of the gas . this is what defines the mean free path . the frequency of collisions increases with density . the force involved in the collisions increases with temperature ( because the temperature of a gas is related to the average kinetic energy of an ensemble of particles -- hotter gas == faster molecules == more energy transferred in collisions ) . okay , so that is the background . very , very crudely explained . to the specific question , sound is just pressure waves . where the pressure is high in the wave , the density and the temperature of the gas increase relative to the baseline , and where it is low in the wave , the density and temperature decrease . it is therefore possible for a pressure wave to be strong enough to break apart molecules by increasing the temperature of the gas from the adiabatic compression . this happens all the time in hypersonic ( mach number > 5 or so ) regimes . this is space re-entry bodies , missiles , meteors . . . the real question is when does a pressure wave stop being a " sound wave " and start being a " shock wave . " i am not an expert in acoustics but typically acoustics implies linear wave theory . this means that shocks are not " sound . " since shocks happen when the flow reaches mach 1 , and chemical dissociation starts at mach 5 or so in air , that would seem to imply that no , sound cannot cause chemical dissociation by the definition of sound as a linear pressure wave . but shock waves certainly can . the destructive nature of sound is typically more related to exciting natural frequencies of a material than just obliterating it to pieces from the incident wave itself . the frequency of the sound wave is chosen to match the resonance of the material so it self-amplifies and the material destroys itself .
the morally correct answer is that the measurement of one spin in the epr-entangled pair eliminates the entanglement as it picks a particular factorized basis vector for the measured spin , and the total wave function therefore has to factorize to $\psi_\text{just measured}\otimes \psi_{\rm something}$ . if you parameterize the multi-fermion states in terms of " fermion 1" and " fermion 2" , you will have to antisymmetrize , so no multi-particle wave function will ever tensor factorize . ( this is true even for bosons with the exception of the states where all the bosons are placed to the same one-particle state . ) however , as you say , this obstacle to factorization ( in the form of the required antisymmetrization , and similarly symmetrization for bosons ) is sort of " trivial " . there is a technical way to support the claim that this state is really not entangled . if you write the " subsystems " not as " fermion 1" and " fermion 2" but as " region around $r_a$" and " region around $r_b$" , using your notation , then you will see that the wave function ( al ) for the whole space is almost accurately tensor factorized to the wave functional that only depends on the fields near $r_a$ ( where a spin-down electron excitation is added ) and the fields around the point $r_b$ ( where the spin-up electron excitation lives ) . $$ \psi_{qft} = c^\dagger_\downarrow ( {\rm gaussian}_{r_a} ) \otimes c^\dagger_\uparrow ( {\rm gaussian}_{r_b} ) $$ of course , the formula above is just a suggestive way to show that the ordinary , correct state of a quantum field theory $$ c^\dagger_\downarrow ( {\rm gaussian}_{r_a} ) c^\dagger_\uparrow ( {\rm gaussian}_{r_b} ) |0\rangle $$ is really a simple tensor product of a sort . for this reason , the notion of " entanglement " is usually modified for identical particles so that the unentangled states are not just the states having the form of the simple tensor product but all states obtained as ( anti ) symmetrization of a tensor product .
the only way to observe a meteor shower is naked eye . any optical instrument will limit your field of view , and you will miss most of the meteors . the location of the radiant in the sky has no bearing on where you will see meteors . people do not need to worry about exactly where the radiant is . meteors can occur anywhere in the sky . if you look towards the radiant , the trails will be shorter ; the longest trails are about 90° away from the radiant , which is most of the sky . when i first got into astronomy back in the late '50s , the international geophysical year was in progress , and my club participated in a very active igy meteor program . we observed many really obscure showers and also on selected nights with no showers , to act as a control . what i learned from that experience is that most showers have hardly any more meteors than a non-shower night . as a result , the only showers i bother watching deliberately are the perseids in august , the geminids in december , and maybe the leonids in november if a peak is predicted . these are the only showers where you see enough meteors to make it worth your while . i see enough sporadic meteors just about every night i am observing that i get enough meteor " fixes " all year round .
thermodynamics is a phenomenological description of macroscopic systems , and it is laws are based on empirical observations . the first law , first states that a state function called internal energy $u$ exists for macroscopic systems ( an experimental fact ) , that can be thought of as the analog of potential energy in mechanics , for macroscopic systems ; then defines heat intake of the system : 1 ) for an adiabatically isolated macroscopic system ( i.e. . , when the only sources of energy are mechanical ) , the amount of work required to change the state of the system only depends on the initial and final states . ( an observational fact ) 2 ) when the adiabatic constraint is removed the amount of work is no longer equal to the change in the internal energy , and their difference is defined as the heat intake of the system : ( definition of heat ) $$\delta q=du-\delta w$$ here , $\delta q$ and $\delta w$ are not separately functions of state , but their sum ( internal energy ) is . note that $\delta w$ is the work done on the system .
the maxwell equations do not need to " take into account " that the proper time of light-like paths is zero . the definition of the minkowski metric as $$\mathrm{d}s^2 = c^2\mathrm{d}t^2 - \mathrm{d}x^2 - \mathrm{d}y^2 - \mathrm{d}z^2$$ lorentz invariance means that all physical laws are invariant under the isometries of this metric , which are the lorentz group . the maxwell equations are manifestly lorentz invariant if you simply write them as $\partial_\mu f^{\mu\nu} = 0$ . they also give as a result that the speed of electromagnetic waves is $c$ . since they are lorentz invariant , this is true in all lorentz-related frames . now , if you plug any path that represents motion with the speed $c$ into the metric , it is zero . why ? parametrize the path as $t\mapsto ( t , ct , 0,0 ) $ ( you can certainly lorentz transform so that that is possible ) . plug it in . now , $$\mathrm{d}s^2 = c^2\mathrm{d}t^2 - \mathrm{d} ( ct ) ^2 = c^2\mathrm{d}t^2 - c^2\mathrm{d}t^2 = 0$$ and so the proper time along that path is $\tau = - \int ds^2 = 0$ .
solution 1---guess-work : if the forces applied on the two ends are equal , say both $1.5\ n$ , the spring will get stretched $1.5\ m$ . a natural guess is that the stretch is determined by the average of the two forces at the ends , which in this case are both equal to $1.5\ n$ . therefore , for the case you mentioned ( $1\ n$ applied to one end , and $2\ n$ to the other end ) , the answer is again obtained from the average : the spring is stretched $1.5\ m$ . solution 2---precise analysis : if the spring is massless , the question is ill-posed ; the nonzero net force yields infinite acceleration . if the spring has mass $1\ kg$ ( you can extend the following analysis to arbitrary mass ) , then since the net force on it is $2\ n-1\ n =1\ n$ , the spring will have $1\ m/s^2$ acceleration to the right . now let us , as observers , accelerate along with the spring so at all times we are in rest with respect to each other . so we ( the observers ) have $1\ m/s^2$ acceleration to the right . since we are accelerating to the right , we are no longer inertial observers ; so if we want to use newton 's laws we should think everything we see is subject to a " gravitational field " with strength $1\ m/s^2$ to the left ( this is the push-back force you experience when a car you sit inside accelerates ) . therefore we see a spring with $1\ n$ force applied to its left end , $2\ n$ applied to its right end , and a gravitational pull of $1\ n$ on it to the left ; the total force is zero in our ( non-inertial ) accelerated frame , as it should be , since we see the spring at rest . now , applying newton 's second law ( we are still in the non-inertial frame remember ) to the tiniest bit of spring at the left end , we learn that the tension of the spring is $1\ n$ at the left end ; similarly the tension of the spring is $2\ n$ at the other end . the tension in the middle of the spring linearly interpolates between these two values . this linear gradient in spring 's tension is due to the left-wards gravitational field ; a similar ( gravitational ) effect results in linear vertical pressure gradient in liquids at rest . now you see why this problem is much more difficult than those with equal forces applied to both ends of the spring . here the tension varies along the spring , leading to further complications . however it is not too difficult to overcome these complications . . . the tiniest bit of the spring at the left end is subject to $1\ n$ at its both ends ; if every bit of the spring was like that , the spring would stretch $1 m$ . the tiniest bit of the spring at the right end is subject to $2\ n$ at its both ends ; if every bit of the spring was like that , the spring would stretch $2 m$ . but the parts of the spring in the middle are subject to a tension which linearly interpolates these two values , therefore on average the tiny pieces of the spring stretch in a way that results in the whole spring stretching $1.5\ m$ . this confirms the guess-work in solutions 1 . you can convince yourself that although the ( $1\ kg$ ) mass we introduced for the spring is necessary to make the problem well-defined , the final answer does not depend on the mass of the spring .
einstein 's theory of gravity is already relativistic so i think that what you are asking is this : beginning with newtonian gravity and making an analogy with coulomb 's law ( where mass is analogous to electric charge etc . ) , and taking into account special relativity effects of a ( mass ) current etc . , does the analog of magnetic force pop out ? the answer is : yes . however and unfortunately , the gravitational waves that also pop out , analogous to electromagnetic waves , transport negative energy .
a finite size of the proton induces electromagnetic form factors in the qed interactions and makes them nonlocal .
you dont give magnitude of pounds , you give magnitude of , say mass ( as here ) . so the magnitude is 120 pounds .
1 . not all states produced by $\text{cnot} \ ; ( h \otimes i ) $ are entangled . for the first part of your question : no , not all states which arise as the output of $\text{cnot} \ ; ( h \otimes i ) $ are entangled . specifically , you can consider $$\begin{align*} |\psi\rangle \ ; and =\ ; ( h \otimes i ) \ ; \text{cnot} \ ; \bigl [ \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |10\rangle \bigr ) \bigr ] \\ \ ; and =\ ; ( h \otimes i ) \bigl [ \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |11\rangle \bigr ) \bigr ] \\ \ ; and =\ ; \tfrac{1}{2} \bigl ( |00\rangle + |01\rangle + |10\rangle - |11\rangle\bigr ) , \end{align*}$$ which is a maximally entangled state ( notice from the second line above that it differs from a bell state by a local unitary ) . however , by construction , if you apply $\text{cnot} \ ; ( h \otimes i ) $ to $|\psi\rangle$ , you will get back out the state $$ \text{cnot} \ ; ( h \otimes i ) \ ; |\psi\rangle \ ; =\ ; \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |10\rangle \bigr ) \ ; =\ ; |+\rangle\otimes|0\rangle\ ; , $$ which has no entanglement at all . even if you are interested only in inputs which are product states , we can see that the circuit maps $|+\rangle \otimes |0\rangle$ to another product state &mdash ; specifically , $|0\rangle|0\rangle$ . 2 . all maximally entangled two-qubit states can be easily described by a similar circuit to $\text{cnot} \ ; ( h \otimes i ) $ acting on the standard basis . for the second part of your question : if you allow yourself arbitrary single-qubit unitaries , then for any maximally entangled two-qubit state $|\psi\rangle$ , you can certainly construct a circuit which constructs $|\psi\rangle$ from standard basis states , and which allows you easily to see that the state is maximally entangled . every two-qubit state has a schmidt decomposition , $$ |\psi\rangle \ ; =\ ; u_0|\alpha_0\rangle|\beta_0\rangle \ ; +\ ; u_1|\alpha_1\rangle|\beta_1\rangle \ ; , $$ where $u_0 \geqslant u_1 \geqslant 0$ &mdash ; in particular , $u_1 = 0$ for $|\psi\rangle$ a product state &mdash ; and where $\{ |\alpha_0\rangle , |\alpha_1\rangle \}$ and $\{ |\beta_0\rangle , |\beta_1\rangle \}$ are each orthonormal bases for $\mathbb c^2$ . in order for $|\psi\rangle$ to be maximally entangled , we need $u_0 = u_1 = \tfrac{1}{\sqrt 2}$ . consider single-qubit unitary matrices $a$ and $b$ , such that $$\begin{alignat*}{4} a |x\rangle \ ; and =\ ; |\alpha_x\rangle and \quad and \text{for $x \in \{0,1\}$} , \\ [ 1ex ] b |y\rangle \ ; and =\ ; |\beta_y\rangle and and \text{for $y \in \{0,1\}$} ; \end{alignat*}$$ then we can describe $|\psi\rangle = ( a \otimes b ) \ ; \text{cnot}\ ; ( h \otimes i ) \ ; |0\rangle|0\rangle$ , that is , the effect of applying $ ( a \otimes b ) $ to the bell state $|\phi^+\rangle = \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |11\rangle ) $ . furthermore , it is not hard to show that any standard basis state is mapped by that circuit to some maximally entangled state similar to $|\psi\rangle$ , and that any circuit of this form ( whatever $a$ and $b$ might be ) maps any standard basis state to a maximally entangled two-qubit state .
from wiki : approximately 90% of the power consumed by an incandescent light bulb is emitted as heat , rather than as visible light . so , yes , the incandescent bulb can be used as a heater and , in fact , has been used as a heater . for example , see : easy-bake oven . the original toy used an ordinary incandescent light bulb as a heat source
thermal conductivity of ice is higher than thermal conductivity of snow , so ice is the winner . ice has higher thermal conductivity because it is more dense than snow . when snow is as dense as ice then it is no longer snow , it is ice . thank you for your answers , without them i would not guessed it right : ) .
the scatter direction depends on the size of the particle and the wavelength . very small particles ( e . g . nitrogen atoms of the atmosphere ) scatter isotropically . there is still an effect on the polarisation of the scattered light ( bees use that to locate the sun if they can not see it directly ) . often gaussian beams are used to describe how the intensity propagates in an optical train ( a system of lenses and mirrors ) http://en.wikipedia.org/wiki/gaussian_beam . note that this describes the intensity of a laser with a gaussian intensity profile ( this is a good approximation for many lasers , especially if you focus them through a pinhole ) . if you have an extended light source you will have to add the intensities of several such beams . once you have a numeric intensity profile ( i guess 2d is enough for your usage ) , you can try an exponential decay law to estimate the effects of scatter . your focus spot will look more intense the higher the numerical aperture of your lens is . if you use a lens that has 2.5 cm diameter and f=10cm the spot will not look as intense as with a lens that has f=3cm . have you thought of using fluorescence ? you could use dissolve some colour in water and use laser protection goggles to see the fluorescent light . then you do not have to cope with scattering . you can get polygonal mirrors out of old laser printers . that way you can scan the beam in one direction . if you use a laser diode , you can modulate the intensity very fast . i recently purchased a 405nm laser with 120mw for $120 from lasever.com. 120mw is very dangerous . if you do not have protection goggles or you share your space with other people do not use lasers> 0.5mw !
the basis of the hilbert space in schrödinger 's picture is assumed to be time-independent regardless of any properties of the hamiltonian . the hamiltonian is just another operator . if the hamiltonian is time-dependent , its eigenstates and eigenvalues are obviously time-dependent , too . both equations you write down only express the fact that the basis of eigenstates of $h ( t ) $ is still a basis , so a general ket vector , including the actual state vector of the system , may be expanded as a linear superposition of these basic vectors with some general complex coefficients $c_n ( t ) $ . the two expansions only differ by the phase one includes into the coefficients $c_n ( t ) $ or into the basis vectors $|n ; t\rangle$ . one convention includes the phase $\exp ( i\theta_n ( t ) ) $ , another one does not , and so on . obviously , there is no " universally mandatory " rule that would dictate the right phase of these vectors so there is some freedom about the notation . note that a phase factor times an eigenstate is still an eigenstate . whatever your convention for the phases is , if you carefully follow the maths and remember what the symbols mean – the defining equations – you will be able to derive the invariant claims about the adiabatic theorem . the wikipedia-sakurai conventions treat the phases wisely and naturally , to speed up the derivations .
the riemann tensor encapsulates all information about the 4-dimensional space-time . this information can generally divided into two sectors : information about the curvature of space-time due to the existence of matter . this is given by the ricci tensor according to the einstein equation $$ r_{\mu\nu} - \frac{1}{2} g_{\mu\nu} r = 8 \pi g t_{\mu\nu} $$ information about the structure of gravitational waves in the space-time . this is given by the trace-free part of the riemann tensor , namely the weyl tensor . often , we are not quite interested in the exact structure of the space-time , but only if gravitational waves can exist or their structure . in these cases , one studies the weyl tensor rather than the ricci tensor . for example , in the setup of quantum gravity , one requires to study the asymptotic structure of spacetime . in these theories , a good understanding of the weyl tensor is more important .
i will translate your post into the language of translations . then i will answer this question about translations . then i will answer your original question . translation question i am confused about a trivial concept . let the displacement of a rigid body be described by the equation $\vec{x} ( t ) =\delta \vec{x} ( t ) +\vec{x} ( 0 ) $ , with $\delta \vec{x} ( 0 ) =0$ . then , at each instant there is only one unit vector in the direction of displacement that we may call $\hat{w} ( t ) $ and which we may take to be normalized . that vector $\hat{w} ( t ) $ is what geometrically we would call the ( instantaneous ) direction of velocity [ note : this sentence is wrong ] . kinematically , however , the instantaneous direction of velocity $\vec{v}$ is the derivative $\dot{\vec{x}} ( t ) =\dot{\delta \vec{x}} ( t ) $ . that is the direction of $\vec{v} ( t ) $ . as is obvious ( for example an object not moving in a straight line ) , in general $\hat{w} ( t ) $ and $\vec{v} ( t ) $ are not parallel . so , why are there two directions for velocity , and does $\hat{w} ( t ) $ play any role in the kinematics/dynamics of the motion ? answer to translation question you are wrong that $\hat{w}$ is the direction of velocity . $\hat{w}$ was defined as the direction of $\delta \vec{x}$ , that is , the direction of the displacement . as kevin said the total displacement is not needed because you only need to know a objects current position and velocity to get its motion in the future . answer to the rotation question now we are ready to answer the rotation question . we just translate the answer of the translation question to rotation language . you are wrong that $\vec{v}$ is the direction of angular velocity . $\vec{v}$ was defined as the axis of rotation for $r$ , that is , the direction of the rotation between the initial and final orientation . as kevin said this rotation is not needed because you only need to know a objects current position and velocity ( here we are talking about the velocity everywhere in the object , which for a rigid object can be summarized by a linear velocity and an angular velocity ) to get its motion in the future .
so first of all , the first equation you gave is only correct , if the $|ø\rangle$ form a basis . it has nothing to do with " in which basis they are " . the easiest way to understand this is probably with a 3d vector-analogy . so if $b_i$ , $i=1\dots3$ form a basis , for any vector $v$ it is legitimate to write $$v=\sum_{i=1}^3 b_i ( b_i\cdot v ) $$ there , the $b_i\cdot v$ are the components of $v$ in the representation of the $b_i$ . it is the very same for bras and kets . it is " just " not 3d but has infinite dimension , so if we have a basis of infinite $|ø\rangle$ , $ø\in \mathbb{r}$ or $|r\rangle$ , $r\in \mathbb{r}^3$ , denote the scalar product ( $a\cdot b$ ) using dirac notatiton ( $\langle a|b\rangle$ ) , and write integrals instead of sums we get te formulas given by you ( mathematically this is non trivial ) . therefor the $\langle r|\psi\rangle$ are the components in the position basis .
this is question is both open theoretically and experimentally , although there is some astrophysical data . after a black hole forms , the matter is squeezed as it falls into an accretion disk , and the matter that is absorbed is rotating very fast by the time it falls in . the dynamics of the accretion disk are complicated , because there are horrendous fields caused by ionization and charge separation . this makes it difficult to achieve consensus about whether the matter falling into a small black hole causes its spin to slow down or speeds up . the experimental consensus seems to currently be that the black holes that power active galactic nuclei are spinning at close to extremality , meaning they have close to the maximum amount of angular momentum for their mass . one reason to expect this is that , absent quantum mechanics , the black hole can only generate energy to the extent that it is spinning . the agn 's are generating a huge amount of energy , so something must be maintaining the spin , and this should be the infalling matter . it is my opinion that there is still uncertainty regarding the emission of highly spinning black holes , because there is no certainty about what happens to infalling matter . one has reasons to suspect that this matter is emitted from the black hole nonthermally , more or less as it came in , after doing a traversal of the interior regions . if this is so , you must take into account the spin-up/spin-down effects of the in-out matter , and this requires the classical limit of quantum black hole , something which is nearly , but not quite , available , thanks to string theory .
gravity may be fundamentally different from electromagnetism , as wouter says , but it seems to me that , as far as your question is concerned , gravity is not fundamentally different from electromagnetism : there is gravitational field ( http://en.wikipedia.org/wiki/gravitational_field#general_relativity ) , which is indeed " some " measurable material " of gravity between say the earth and the moon " . afaik , you are right , and " gravity in theory , has an infinite range and moving a flower on earth could move an atom on jupiter ever so slightly . " ( i removed your " maybe " and question marks ) . however , in general , an atom on jupiter does not " feel " a movement of a flower on earth instantaneously - gravity is widely believed to have a finite propagation speed ( http://en.wikipedia.org/wiki/speed_of_gravity ) equal to the speed of light . so again , it seems that , as far as your question is concerned , the situation with gravity is not fundamentally different from that with electromagnetic field : the coulomb field has an infinite range , and there is " measurable material " - electromagnetic field ( in the form of electrostatic field ) between two charges , but , if one charge moves , the other charge does not " feel " that movement instantaneously . thus , both gravitational and electrostatic forces are mediated by fields . one could say that gravitational field is just space curvature , but i do not feel that would change much . as for gravitons . . . again , afaik , while it is not clear yet how gravitational field should be quantized ( http://en.wikipedia.org/wiki/quantum_gravity ) , that does not seem to matter much as far as your question is concerned .
the spiral arms do not mean that the mass is getting sucked to the center . they are just wave-like density patterns . the bodies in orbit around the center of the galaxy are in stable orbit ; just like the earth around the sun and the moon around the earth . what happens is that gravity accounts for the centripetal force ( in the orbiting frame , gravity is balanced by the centrifugal force ) , so there is no net radial acceleration " left over " to suck the body in . the only reason things would fall into the center is if they were headed there . this can happen if two stars pass by each other and are slingshotted in opposite directions , one of which gets sent to the center of the galaxy .
all we can do precisely is give a probability for some physical quantity to have its observed value . for example ( subject to various assumptions ! ) the probability of the cosmological constant having it is observed value is around 1 in $10^{120}$ . since this is absurdly low we say it is fine tuned . but where you draw the line between fine tuned and not fined tuned is a matter of debate . most of us would not consider a 10% probability fine tuned , but what about 1% or 0.1% ? particle physics required a $5\sigma$ probability to be considered proof , and this is about 1 in 3.5 million and this is about 0.00003% , so that seems like a reasonable lower bound for not fine tuned . however i would guess most people would consider considerably higher probabilities than this as evidence of fine tuning . the point is that the probability of an observed value can be calculated precisely , but whether this corresponds to fine tuning is a matter of personal opinion .
yes , this equations applies to all waves . . . with the caveat that you replace c by the speed of the wave you are studying ! in a water wave , the product of the wavelength and the frequency will be the speed of the water wave , not of light . for sound waves in air , it will be the speed of sound , etc . because of this , the general form of the equation you provided is : $$\lambda \nu ~=~ v_{wave} . $$ another interesting thing is that the speed of the wave need not be constant . the equation is always valid , but it might be possible that the wavelength depends on the frequency , in which case the speed will also depend on the frequency . this happens with water waves ; you can notice not all waves travel at the same speed on the ocean . it also happens with light ; light of different colours travel at different speeds through glass , which is what allows a prism to disperse white light into a rainbow . when the wavelength depends on the frequency , we call it " dispersion " . if it does not , then the wave speed is the same for all waves of that type .
as cumrun vafa explains in the video linked to below the picture of him in this article , f-theory works in a total of $10+2$ dimensions . the signature of the last two infinitesimal dimensions is ambiguous , so that they can indeed both be timelike . since these are only infinitesimal dimensions , any causality issues etc are not a problem in this case . and as cumrun vafa nicely explains in his talk , f-theory gives quite a nice phenomenology with an astonishingly realistic ckm-matrix , coupling constants , etc ; so it is not true that theories that operate in more than one time dimension are completely off base , as some people claim . there is no reason to dogmatically dismiss every theory that has more than one time dimension . btw , the talk is very accessible and enjoyable .
here is a link which discuses this problem in some depth . i do not think that this would be a good project for an ode class because temperature and density are not differentially related to each other i.e. one is not a differential form of the other .
there is nothing wrong with your calculations . from the wikipedia article on supermassive black holes : " the average density of a supermassive black hole ( defined as the mass of the black hole divided by the volume within its schwarzschild radius ) can be less than the density of water in the case of some supermassive black holes " given that black hole masses scale with linear size , while objects we encounter in daily lives have a mass proportional to the cube of their linear size , makes it inevitable that beyond a certain size black holes are characterized by mass densities that we label as ' small ' . in other words : when growing an object while keeping it is mass density fixed , there is a maximum to how far you can grow such an object . beyond a certain size , the object acquires a gravitational horizon that starts expanding proportional to the object 's mass , thereby reducing the object 's mass density .
edit : ok , i misinterpreted anku 's question and had a bowl shape in mind whereas he had an upturned bowl in mind . this changes the energy equation to $$ mgr = \frac{1}{2}mr^2\dot{\theta}^2 + m g r ( \sin \theta ) \ ; . $$ i measure the angle from the horizontal here . after similar manipulations as below , i get $$ t = \sqrt{\frac{r}{2g}} \int_0^{\pi/2} \frac{d\theta}{\sqrt{1-\sin\theta}}$$ which is indeed a divergent integral . however , this does not mean that the motion is irreversible . the reverse motion starts from the top of the hill , but the top of the hill is an equilibrium point , albeit an unstable one , which implies that it takes an infinite amount of time to roll down the hill . from conservation of energy , you can write down the following formula : $$ mgr = \frac{1}{2}mr^2\dot{\theta}^2 + m g r ( 1-\cos \theta ) \ ; . $$ the left hand side represents the potential energy at the top of your semi-circular hill , the right hand side the total energy at any point of the trajectory . ( angle measured from the vertical . ) rearranging , you can write this as $$ \frac{2 g \cos \theta}{r} = \dot{\theta}^2$$ or after some additional work and intergrating $$ t = \sqrt{\frac{r}{2g}} \int_0^{\pi/2} \frac{d\theta}{\sqrt{\cos\theta}}$$ a quick check with wolframalpha gives a finite number for the right integral . therefore the time it takes for the ball to roll up the hill is finite .
i finally managed to solve the problem . using that $\delta u _ {cycle} = 0$ , knowing the heats $q_{ab}$ and $q_{cd}$ and the adiabatic equation $pt^3=constant$ . is possible to use the first law to find the total work expression in the cycle , and substitute in the efficiency expression . you can find the problem solved step-by-step here : http://folk.uio.no/yurig/fys203/oppgaver/reichl.tmp.pdf i finally found this on google , after a lot of time .
if a wind hits a wall directly ( in a 90 degree angle ) does any of it bounce back ? no , because air behaves like a continuous fluid , it can not rebound and flow back through itself without interacting with the fluid behind it . the air will all be displaced sideways . there will be a higher pressure in front of the wall . are there any similarities with , say , light rays hitting a mirror ? none that i can think of . vortices in a fluid can bounce off walls . ref . ref . a stream of fluid can bounce off a fluid surface . ref . but none of these are like wind hitting a wall or like light-rays hitting a mirror .
you have to be precise with the definition here . you mention a temperature difference at both sides , but this is a 1d problem . you have different temperatures at both sides , and a temperature difference across the rod . the heat flux is proportional with the temperature gradient , this is referred to a fourier 's law ( see e.g. http://en.wikipedia.org/wiki/fourier's_law#fourier.27s_law ) now i understand your question as : why is $q=-k \frac{du}{dx}$ ( u is temperature in your case ) , and not just proportional to $u$ . now , suppose the flux would be proportional to the temperature . then you get some problems . first , suppose you have a rod of uniform temperature . the heat flux is constant across the rod everywhere , but in what direction ? and what is the new equilibrium than ? just the notion that the energy should flow from high to low temperature , is given by the gradient , making the energy always traveling down the gradient , trying to reach a more uniform state . to make it work out , some constant of proportionality was introduced , which is most often a system ( or material ) property .
you have missed something important about relativity . the rule that all ( inertial ) observers measure the speed of light in vacuum to be the same is really just a special case of the big rule : all inertial frames have the same physics . that is explicitly a claim that there is no experiment that can distinguish one free-falling frame from another without reference to some external frame of reference . so no : there is no universally special frame of reference .
conservation of energy follows from invariance under translation in time , not inversion . this symmetry states that no matter when you do your experiment , it will give the same results . all isolated systems obey this symmetry ( and therefore conserve energy ) and no violation of it has ever been detected . ( needless to say , it would be a huge event if it were . ) in classical physics , only continuous symmetries - that is , symmetries that can be continuously connected to the identity transformation - have a corresponding conservation law . quantum physics does permit conservation laws for discrete symmetries but these laws are far harder to visualize . an example of this is conservation of parity , $p$ , which corresponds to invariance under inversion in space , and which gives the parity - even or odd - of wavefunctions . temporal inversion , $t$ , is even harder to turn into a physical quantity because it requires a full relativistic treatment in which time is a coordinate like space and not a parameter ( as it is in non-relativistic quantum mechanics ) . a third discrete symmetry is charge conjugation , $c$ , which exchanges particles for their antiparticles . it turns out that any consistent field theory must be invariant under all three operations when taken together - i.e. under $cpt$ . thus violation of parity - an experiment and its mirror image behaving differently - is possible , for example , if it comes together with violation of $c$ - i.e. the mirror experiment behaves like the original one if it is made of antimatter - , as was discovered in the sixties . violations of $c$ and $p$ together have also been discovered in recent years , which means that in some situations violations of $t$ must occur . the recent $b$-meson experiments confirm this . since the $t$ symmetry does not correspond to energy but to a far more abstract quantity ( which is not conserved ) , this does not lead to a nonconservation of energy .
generally speaking , when people talk about the universe having a beginning they are talking about the solution to the einstein equations called the flrw metric . this describes a universe that is homogenous and isotropic , and it seems to be a pretty good approximation to our universe . the flrw metric tells us how the density of the universe changes with time . nb it does not give the size of the universe . a homogenous isotropic universe is infinite in size by definition , and it remains infinite in size all the way back to the big bang . anyhow , if you wind time back to the big bang you find the density of the universe becomes infinite and the distance between any two points in the universe falls to zero , even though the universe remains infinite in size . this nonsensical result is why you will often hear it said that gr can not explain the big bang , and it is why we expect some theory of quantum gravity to take over and prevent the density becoming infinite . anyhow , the point of all this ranting is that by definition the flrw universe is the same everywhere . so if you believe the flrw metric is a good approximation to our universe ( and it seems to be ) then the entire universe behaved the same as the bit we can see . the entire universe , including our bit , had a beginning . however , this is a strictly classical perspective and does not describe phenomena like inflation that originate from quantum mechanics . the original theory of inflation did not change the overall behaviour much , it just introduced a period of exponential expansion shortly after the big bang . but there is a more recent theory called eternal inflation that drastically changes our view of the universe 's beginning . in eternal inflation the universe has been inflating for all time and will continue to inflate for all time , so the universe as a whole does not have a beginning . however areas within the universe stop inflating and turn into the sort of slowly expanding spacetime that we see around us . these areas then appear to have a big bang , but this is simply the point at which inflation stopped for that particular area . other areas would have big bangs that happened at a different time , while the universe as a whole had no beginning . so the answer to your question is yes and no depending on which theory you believe . unfortunately at the moment there is no experimental evidence to prove which answer is correct .
calories are the energy released when food is burnt . they are not a very accurate measure of the energy you get form eating them because they do not consider the actual biological process - just what happens if you burn them . if you drink cold water or eat ice then your body must use energy to heat it to body temperature - so eating enough ice could be a diet ! see how much more energy does it take for a human body to heat 0c ice vs 0c water ?
i do not understand all the nitty details , but the gist goes as follows : you have a constant probe beam , which is cast on a sample and analysed later . suppose the sample has a very predictable behaviour , with an event every $24\mu \text{s}$ . then the analysis of the beam will show exactly that . this is the blue line in fig . 4 . now , suppose you want to hide these events , but you do not want to divert the beam around the sample ( for whatever reason ) . they do it in the following way : increase/decrease the wavelength of that part of the probe beam , which would encounter the event . this only serves as " marking " the part of the wave you want to manipulate . speed up waves with lower frequency and slow down waves with higher frequency . this creates a beam gap in the middle . that means , the sample will see the probe beam " turned off " during the event , and the probe beam is not affected . after the sample , reverse step 2 , i.e. slow down waves with lower frequency and speed up waves with higher frequency . reverse step 1 . now your probe beam looks roughly as if it was not messed with at all and probed the sample at all times . note , that this is a layman explanation , and the actual speeding up/slowing down is a rather complicated feat .
when it is in water the buoyant pressures are distributed more evenly over the whale 's natural surface contour , resulting in less internal strain in the whale 's body . on land , the pressures are all concentrated in a planar surface at the bottom . the whale 's body is not naturally planar , so significant strain develops as the body attempts to conform to the planar surface in order to distribute the forces resisting gravity .
hint : use repeatedly : the virasoro algebra . the condition for being a lowest weight representation .
here 's a slightly different but equivalent way to think about it . forces describe interactions between two objects . if two objects are interacting , they exert forces on each other . if two objects are not interacting , they do not exert forces on each other . thus , an object does not " carry around " a force with it . a force is not a property of an object , just as dmckee explains . instead , we describe interactions between two objects using the more-abstract concept of force . in your block-hits-other-block scenario , it is tempting to ask where did the force come from if colliding object had $f_\text{net}=0$ ? but when forces are viewed as interactions , it becomes more apparent that the force did not come from anywhere within one of the objects . there simply was not an interaction before they collided , so we would not ascribe the existence of a force force .
your statement about $|| r-r'||$ is true only if $r$ and $r'$ have the same $\phi$ coordinate . ( same " longitude" ) the denominator does have a $\phi '$ dependance . the value of that modulus will be larger when $\phi \neq \phi '$ .
if i am right , your question is " why does current need to return to the same source ? " electrical current is caused by the flow of electrons , i.e. when an electron moves from the shell of one atom to another , then there is a net current flow . however , this causes the ionization of the other atom , which is naturally in a neutral ( non-ionized ) state . as a result , this newly positively charged ion attracts an electron . the idea of current returning to the same place is not exactly right - it is that current in must equal current out ( see : kirchhoff 's laws ) , most easily represented in a circuit by drawing a loop .
your intuition is right , an object ( at least of the ideal sort these formulas are used to describe ) returns to the same size when it is brought back to the same temperature . the reason your math is not giving that result is that $l ' = l + l\alpha\delta t$ is a first-order approximation , valid for small values of $\alpha\delta t$ . this means , among other things , that when doing calculations with it , your results only have to agree to first order - in other words , if you have two quantities that differ only by a multiple of $ ( \alpha\delta t ) ^2$ ( or any higher power ) , they can be considered the same . by this reasoning , $l'' = l$ to first order ; or , as it would be written in symbols , $l'' = l + \mathcal{o}\bigl ( ( \alpha\delta t ) ^2\bigr ) $ . if you want to be a little more rigorous about this whole business , you can derive the full , non-approximate formula for thermal expansion like this . for a small value of $\delta t$ , the corresponding change in $l$ is $$\delta l = \frac{\mathrm{d}l}{\mathrm{d}t}\delta t$$ from the approximate thermal expansion equation , you know that $\delta l = l\alpha\delta t$ , which gives $$l\alpha = \frac{\mathrm{d}l}{\mathrm{d}t}$$ integrating this gives $$l ' = le^{\alpha ( t ' - t ) }$$ which is the real expression ( or at least a mathematically consistent better approximation ) . you can verify that $l ' = l\alpha\delta t$ is the first-order approximation to this . another way to do this derivation is to think about heating a material from temperature $t$ to $t ' = t + \delta t$ , which takes it to length $$l ' = l ( 1 + \alpha\delta t ) $$ then to a slightly higher temperature $t'' = t ' + \delta t$ , which takes it to length $$l'' = l' ( 1 + \alpha\delta t ) = l ( 1 + \alpha\delta t ) ^2$$ then to a slightly higher temperature $t'''$ , which takes it to $$l''' = l'' ( 1 + \alpha\delta t ) = l ( 1 + \alpha\delta t ) ^3$$ and so on . if your desired final temperature is $t_f = t + n\delta t$ , then the final length will be $$l_f = l ( 1 + \alpha\delta t ) ^n = l ( 1 + \alpha\delta t ) ^{ ( t_f - t ) /\delta t}$$ in the limit as $\delta t\to 0$ , this becomes $$l_f = l e^{\alpha\delta t}$$
if you put a schottky device into very-forward-bias , the iv curve becomes a straight line whose inverse slope is the resistance as usual . so you can still use tlm . of course , people do not normally bother to figure out contact resistance because the schottky aspect of the contact has a much much bigger effect on the device than the resistive aspect . for an ohmic contact that arises from tunneling through a schottky barrier , again , people do not normally bother to figure out the " barrier " height because the " barrier " is irrelevant for the electrical behavior of the device . but if you had to figure it out for some reason , the only method i know of is internal photoemission .
i am not an expert in 2d cft . however i hope following manipulations are valid . assume that your second equation follows from first one . then on rhs of your first equation taylor expansion of $o_2 ( w ) $ at point $z$ gives : $o_2 ( w ) =o_2 ( z ) + ( w-z ) \partial_z o_2 ( z ) + . . . $ taking derivative wrt $w$ on both sides we get $\partial_wo_2 ( w ) =\partial_zo_2 ( z ) + . . . $ using these two results in your first equation we get $o_1 ( z ) o_2 ( w ) = \displaystyle\frac{o_2 ( z ) }{ ( z-w ) ^2}+regular\:terms$ subtracting it from your second equation , multiplying with $ ( z-w ) ^2$ and taking limit $w\rightarrow z$ we conclude that $o_2$ and $o_1$ should be equal . since to begin with we did not assume any such thing regarding fields $o_2$ and $o_1$ so in general your second equation should not follow from the first one . i think equality of $o_2 ( w ) o_1 ( z ) $ and $o_1 ( z ) o_2 ( w ) $ ( assuming fields are ' bosonic' ) within time ordered product only implies that their ope should be symmetric under exchange of z and w . so if your first equation for ope can be realized for some ( bosonic ) fields , then by exchanging z with w on rhs you should get the same result within a regular term .
the acceleration equation needs to include force terms for air drag $f_a$ , and runner friction $f_f$ in addition to the gravity term $g \sin ( \theta ) $ where $\theta$ is the slope of the luge run and $g$ is gravitation . as singh pointed out , gravitation exerts a force proportional to mass , so the total acceleration is $$a = g \sin ( \theta ) - \frac{f_f + f_a}{m}$$ runner friction is more or less proportional to mass , so we can replace it with a constant , i.e. the mass of the rider is not a consideration , giving $$a = g \sin ( \theta ) - k - \frac{f_a}{m} $$ air drag depends on frontal surface area , $a$ and the square of velocity $v^2$ , so $$f_a = dav^2$$ where d is a drag coefficent to account for the rider 's aerodynamic smoothness ( or lack thereof ) . user11865 's answer points out that the rider 's surface area is proportional to $\sqrt{m}$ and this is what gives heavier riders an advantage , especially at higher speeds . let 's wrap the density and shape of the human body into a constant , say , $b$ , and the acceleration equation now looks like $$a = g \sin ( \theta ) - k - \frac{dbv^2}{\sqrt{m}}$$ the acceleration lost to air drag is the only term that depends on mass and having more mass makes it smaller .
i am assuming that this section of the book is talking about the ultraviolet catastrophe , where an ideal black body in thermal equilibrium will emit an infinite amount of power through radiative means . the source goes on to say : the ultraviolet catastrophe results from the equipartition theorem of classical statistical mechanics which states that all harmonic oscillator modes ( degrees of freedom ) of a system at equilibrium have an average energy of kt . following that : according to classical electromagnetism , the number of electromagnetic modes in a 3-dimensional cavity , per unit frequency , is proportional to the square of the frequency . this therefore implies that the radiated power per unit frequency should follow the rayleigh–jeans law , and be proportional to frequency squared . thus , both the power at a given frequency and the total radiated power is unlimited as higher and higher frequencies are considered : this is clearly unphysical as the total radiated power of a cavity is not observed to be infinite , a point that was made independently by einstein and by lord rayleigh and sir james jeans in the year 1905 . essentially , if in a cavity there are an infinite number of electromagnetic modes possible ( think standing waves ) , the equipartition theorem says that a system in equilibrium has an average of $k_{b}t$ worth of energy per mode . this is not what is actually observed , since we do not see an infinite amount of power radiated . how was this solved : max planck solved the problem by postulating that electromagnetic energy did not follow the classical description , but could only be emitted in discrete packets of energy proportional to the frequency , as given by planck 's law . the radiated power eventually goes to zero at infinite frequencies , and the total predicted power is finite . ( 1 ) the formula for the radiated power for the idealized system ( black body ) was in line with known experiments , and came to be called planck 's law of black body radiation . based on past experiments , planck was also able to determine the value of its parameter , now called planck 's constant . the packets of energy later came to be called photons , and played a key role in the quantum description of electromagnetism . ( 1 )
mathematically , qft and dft are exactly the same thing . you can verify this by comparing the first equation on each of the wikipedia pages . on wikipedia these two differ by a minus sign in the exponent and by a normalization factor , but these are just conventions . the difference between quantum fourier transform and classical fast fourier transform is in the speed and in the way the data is represented physically . classically , a dimension $n$ vector is represented using $n$ floating point numbers . on a quantum computer , the qft operates on the wave function of $\log_2 n$ qubits , an exponential space savings . the classical fft runs in time $o ( n\log n ) $ and the qft runs in time $o ( ( \log n ) ^2 ) $ where again $n$ is the dimension of the vector . note that the classical fft must take time $o ( n ) $ to even read the input ! it should also be noted that the classical and quantum versions differ in another important respect , as a consequence of the way the data is stored . after the classical fft algorithm completes , you have the entire output vector to do whatever you want with ( e . g . print out the $100^{\textrm{th}}$ component of the vector or whatever ) . the qft leaves the answer in a quantum state , which you can think of as a sort of generalized probability distribution , and you can do certain measurements to get certain types of information out , but you have to be crafty . shor 's algorithm is of course the canonical place to look for an example here , although simon 's algorithm illustrates many of the same concepts without having to deal with the computational tedium of shor 's algorithm ( simon uses the hadamard rather than fourier transform but conceptually there is not much difference ) . by the way , the circuit that implements the qft is a reflection of cooley and tukey 's divide and conquer algorithm for fft . when cooley-tukey splits the input in half , the qft inspects one qubit . ponder this and you will understand something about the power of quantum computers .
the hamiltonian has the very legal definition that it is the legendre transform of the lagrangian function . so , in any physical case to find the hamiltonian of a system , you have to take $l = t - v$ and then perform the ritualistic legendre transform process shifting coordinates from $q , q'$ to $q , p$ . the time symmetry of the system leads to the conservation of the so called jacobian function and not the hamiltonian . the hamiltonian will however be the total mechanical energy if thesystem is conservative and in this case the jacobian function also equals the total mechanical energy . the jacobian is the mechanical one and not the mathematical one . ref goldstein .
as you are aware , thermal stratification is occurring . once the upper surface of the tub is hot the driving force ( temperature and therefore density difference ) for the thermal siphoning effect is reduced . mixing the tub will definitely help here . all the tubs i have seen are very well insulated ( they take days to cool down . . ) so burying it will not have a great reduction in heat loss . your lid sounds like the weak link as far as reducing heat loss is concerned . any fouling inside the steam pipe can significantly reduce heat transfer to the water . in extreme cases by an order of magnitude . to maintain the ' off the grid ' aesthetic while still improving heat transfer a non-electric pump could be used . a water pump from a dishwasher or washing machine could be rigged up to be powered in another way . some ideas : a wind up mechanism , pedal power , a wind mill ( or a ' fire mill ' powered from the fires exhaust flow ! ? ) , or any other harebrained scheme you can come up with , it will not take much to dramatically improve flow rate from the current thermal siphon rate . motorcycle speedometer cables are a really flexible ( pun intended ) way to get rotational energy from one place to another .
the above equation of $\mathbf{a} ( \mathbf{r} ) $ is not for arbitrary shape loop but for a point magnetic dipole . so the position of the dipole will be well-defined . added : when you look at points close to loop ( close enough that you can not take it as point dipole ) then above $\mathbf{a} ( \mathbf{r} ) $ is not valid and in general it would be an integral over the loop
the issue of particle annihilation is immaterial to the final mass of the merged black hole . if the traditional , " no-hair " view of gravitational collapse holds , and the particles lose their identity when crushed into a singularity , there would be no particle annihilation at all . if some newer and more exotic physics holds , such as string theory or loop quantum gravity , that rescues gravitational collapse from creating singularities , then even if the basic particles retain some sense of identity and can annihilate , these dynamics will still occur inside the event horizon of the black hole , and the energy released from the annihilation event will still be trapped inside the event horizon and register as mass from outside . the only issue at stake , then , is the bulk electrostatic potential energy as the two black holes approach each other . if the holes are oppositely charged , then potential energy will be converted to kinetic energy , and presumably some of this will get radiated away during the collision , resulting in a slightly lower mass for the resulting black hole . if the black holes are of like charge , then it will require more work to bring them together , and this work will probably end up reflected as a slightly larger mass of the resulting black hole . as a practical matter , however , the fractional difference in mass will be minute . all objects of astrophysical-scale masses , including black holes , will be found to have negligible net charge , due to the abundant presence of free electrons and ions in interstellar space . any object in space with a large net charge will rapidly accrete free charged particles , neutralizing itself . for tiny black holes on the primordial or quantum scale , stephen hawking calculated that such a black hole can only have a net charge of a few electrons ( eight , perhaps ? ) ; any more and not enough bound electron states could exist for such a " black hole atom " to be stable against the black hole nucleus accreting charged particles and neutralizing itself . i read this paper early in grad school and remember it relatively clearly , but so far i have not been able to find the reference . will update if i do . however , i did find http://arxiv.org/ps_cache/gr-qc/pdf/0001/0001022v1.pdf in this paper , on similar stability and half-life arguments , they claim that a primordial black hole could not have a charge greater than 70 .
there are a variety of methods used to measure distance , each one building on the one before and forming a cosmic distance ladder . the first , which is actually only usable inside the solar system , is basic radar and lidar . lidar is really only used to measure distance to the moon . this is done by flashing a bright laser through a big telescope ( such as the 3.5&nbsp ; m on apache point in new mexico ( usa ) , see the apollo project ) and then measuring the faint return pulse with that telescope from the various corner reflectors placed there by the apollo moon missions . this allows us to measure the distance to the moon very accurately ( down to centimeters i believe ) . radar has been used at least out to saturn by using the 305&nbsp ; m arecibo radio dish as both a transmitter and receiver to bounce radio waves off of saturn 's moons . round trip radio time is on the order of almost 3 hours . if you want to get distances to things beyond our solar system , the first rung on the distance ladder is , as wedge described in his answer , triangulation , or as it is called in astronomy , parallax . to measure distance in this manner , you take two images of a star field , one on each side of the earth 's orbit so you effectively have a baseline of 300 million kilometers . the closer stars will shift relative to the more distant background stars and by measuring the size of the shift , you can determine the distance to the stars . this method only works for the closest stars for which you can measure the shift . however , given today 's technology , that is actually quite a few stars . the current best parallax catalog is the tycho-2 catalog made from data observed by the esa hipparcos satellite in the late 1980s and early 1990s . parallax is the only direct distance measurement we have on astronomical scales . beyond that everything else is based on data calibrated using stars for which we can determine parallax . and they all rely on some application of the distance-luminosity relationship $m - m = 5log_{10}\left ( \frac{d}{10pc}\right ) $ where m = apparent magnitude ( brightness ) of the object m = absolute magnitude of the object ( brightness at 10 parsecs ) d = distance in parsecs given two of the three you can find the third . for the closer objects , for which we know the distance , we can measure the apparent magnitude and thus compute the absolute magnitude . once we know the absolute magnitude for a given type of object , we can measure the apparent magnitudes of these objects in more distant locations , and since we now have the apparent and absolute magnitudes , we can compute the distance to these objects . it is this relationship that allows us to define a series of " standard candles " that serve as ever more distant rungs on our distance ladder stretching back to the edge of the visible universe . the closest of these standard candles are the cepheid variable stars . for these stars , the period of their variability is directly related to the absolute magnitude . the longer the period , the brighter the star . these stars can be seen in both our galaxy and in many of the closer galaxies as well . in fact , observing cepheid variable stars in distant galaxies , was one of the original primary mission of the hubble space telescope ( named after edwin hubble who measured cepheids in m31 , the andromeda galaxy , thus proving that it was an “island universe” itself and not part of the milky way ) . beyond the cepheid variables , other standard candles , such as planetary nebula , the tully-fisher relation and especially type 1a supernova allow us to measure the distance to even more distant galaxies and out to the edge of the visible universe . all of these later methods are based on calibrations of distances made using cepheid variable stars ( hence the importance of the hubble mission to really nail down those observations .
the answer seems to near 60 degrees . additional description here i have found this mathematica description of the ecliptic plane relative to the galactic plane . wolfram . com
as michael brown mentioned in the comments , no one will explain this as well as feynman ( at least , no one we know of that is alive ) . but that does not mean your question does not deserve at least our attempt . so here is mine and i will try to keep this in the simplest terms i can . ( aside : to all of the physicists reading this , i apologize in advance but in my simplification , i may intentionally omit or contradict the true physics . for instance , i doubt i will be saying how the photon arises as a gauge boson in local u ( 1 ) symmetry ) . see ( 2 ) see ( 1 ) . just kidding . i have lumped 1 and 2 together because to explain what a photon is is to essentially explain where it comes from . hopefully , everyone reading this will be aware of the wave-particle duality that most ( all ) things enjoy . in that way , as mentioned , a photon is a particle in its own right . but does that mean that people can think of the photon as a tiny billiard ball ? no , that would be silly . the photon is a wave packet that , for all intents and purposes , is indivisible . consider a vibrating electron , it is motion one way or the other constitutes a current , which radiates a magnetic field . since this magnetic field is changing as the electron speeds up and slows down , it induces an electric field that radiates outward . since this electric field is also changing continuously , this in turn induces a magnetic field that radiates outward . rinse and repeat . the result is a self-propagating combination of electric and magnetic fields travelling outward from the electron . this is em radiation . the photon is the unit of an em wave . what is one photon ? say we shine a laser , then we block half the beam with a metal plate . the other half still comes through . if there were only one photon in the beam , when you block half of it with the metal plate , none of it would come through . one photon is the largest amount of energy of an em wave where this would still be true . in physics terms , we write it as $e=h\nu$ . the energy of one photon of a wave is equal to h ( a very small constant ) times the wave 's frequency . because the photon is indivisible , we can say it represents the smallest unit of energy of that particular em wave . a different wave would have a different smallest energy . to address what was mentioned in the comments , a photon may have full particle status , but it is not similar to an electron . photons have no mass , they are not matter and , when you examine the properties of a photon , there is no denying that they are packets of energy in every respect ; fluctuations in the background em plane . i will admit , at first glance it does seem very silly for us to say electric and magnetic fields can produce action at a distance and then say , " no , you need photons to cover that distance and actually do the interacting " . but it is true . at least , we can say it is true . without teaching everyone about field theory and symmetries , my short answer to this would be that in advanced physics , we have a certain equation that originally did not work out . as we have done a million times in the past , we had to modify this equation to work and we found that we could only do that by introducing a massless particle that mediates the em force . afterward , we noticed that this particle happened to have the same properties of a photon . in fact , if we tried the hypothetical situation where we assumed this particle was a photon , this one equation produced all of the laws and equations from electromagnetics that we already knew and loved . thus , we said , " we are pretty sure this equation is the right one to use . we assumed this particle we invented was a photon , and it resulted in the equations and laws we have in the real universe . so this must be the way it actually is ! " . having said that , we can never actually observe the photon as it mediates the force . this is simply because if we were to observe the photon , it would no longer be able to mediate the force because we have no method of observing a photon without destroying it . i have already explained how they are created , how they are destroyed is much simpler . when a photon hits something , it can either reflect , transmit , or be absorbed . the latter is destroying it . when it is absorbed , this means whatever it struck ( usually an electron ) absorbs all of the energy of that photon . that is it . how we know they exist . . . we know because we can do experiments with just one photon . we can see the effects of one photon . but most importantly , theoreticians say , " if a photon did not actually exist , what would happen in some experiment ? well jim , we would see outcome . and if they do exists , we should see different outcome . " then experimentalists perform the experiment and 10 times out of 10 we always see the outcome predicted by the existence of photons . this particle/wave packet is called a " photon " because it was first theorized specifically about light . i believe " photo " is from the greek word meaning light and the extra n was added because all of the particles known at that date ended with an n ( proton , electron , neutron . why not photon ? ) i am willing to bet i have missed something important , so let me know . if i can merge what i have missed with the general form of the answer , i will be happy to put it in .
without the internal part : the divergence $$\nabla_\mu ( x_\nu \theta^{\mu\nu} ) = x_\nu \nabla_\nu \theta^{\mu\nu} + \frac12 ( \nabla_\mu x_\nu + \nabla_\nu x_\mu ) \theta^{\mu\nu} $$ where i used that $\theta^{\mu\nu}$ is symmetric . recalling that the energy-momentum tensor is divergence free , the first term drops out . assuming that $x^\nu$ generates a dilation/scaling symmetry ( and not a bona fide symmetry ) , we know that its deformation $$ \nabla_\mu x_\nu + \nabla_\nu x_\mu \propto \mathcal{l}_x g_{\mu\nu} \propto g_{\mu\nu} $$ where $\mathcal{l}$ is the lie derivative . ( in the case $x^\nu$ generates a symmetry the term vanishes from killing 's equation . ) hence in this case for the current to be conserved ( that is , divergence free ) , we need that $g_{\mu\nu} \theta^{\mu\nu} = 0$ ; that is , the energy momentum tensor is tracefree .
you are missing the fact that the truck is still moving forwards during its decelleration interval .
this question was featured on this month 's safari magazine . it says : the electrons of a permanent magnet are aligned in a strict and disciplinary order . these electrons of a magnetic domain rotate harmoniously in the same direction and that is what is responsible for the magnetic power of a magnet . the magnetic waves travel through the domain walls , passing from one domain to the next and provide strength to the domain structure . the structure is so tight and coherent that unless the magnet is subjected to extreme temperatures or extreme shock , the disciplinary order of the magnetic domain is not disturbed . as long as the domain structure remains intact , the magnetic power of a permanent magnet does not diminish . under the normal cause of usage , a magnet made of samarium cobalt will lose only about 50% of its magnetic power in 700 years . i am not sure about what causes it to lose its magnetism .
the longitudinal components are derivatives of scalars , $\partial_\mu\phi$ . for a massive spin-1 $v_\mu$ they appear only in the mass term $m^2_v v_\mu^2$ . that is where the dimension $2$ comes from .
you are missing the rotational kinetic energy at the bottom , $\frac{1}{2}i \omega ^2$ . the key word in the problem is , ' rolling without slipping ' . also remember the equation , $$v=r\omega$$ the cylinder 's moment of inertia should be looked up in a table or given . on wikipedia , the moment of inertia of a thin cylindrical shell is given as : $$i=mr^2$$
your mistake is in forgetting that the verictial component of force f holds up half the stick , so the friction force only needs to hold up the other half of the stick 's weight , not the whole weight .
if i understand the question , you are wondering how to justify the statement that a ( reverible ) adiabatic process is isentropic from the point of view of statistical mechanics ( the classical thermodynamics definition makes sense to you ) . let us then start with the entropic fundamental relationship , s = s ( u , v , n ) , where u stands for energy , v for volume , n for number of particles . in many a statistical mechanics texts you will find the explicit definition of s for a system of particles ( under usual simplifying assumptions ) . inyour example n is constant , but u and v are not : i would be glad to help further if needed , but if you looked at the expression for s as a function of v and n this would answer your question alone . i believe the discussion at isentropic processes be useful .
your question is not quite accurate , this because you thinking of photon as a classical particle ( solid ball ) , but it is not , photon ( same as other particles ) are quantum particles , thus you should consider particle-wave duality , in other hand , because photon has no mass , it can not move at any speed other than speed of light ( in vacuum ) , otherwise it will not exist . finlay , the momentum of massless particle , can be thought of as " amount of motion " , or even more directly , it is related actually not to speed of photon ( like in classical physics ) but to wave length by the following famous de broglie relation : $$ p=\frac{h}{\lambda}$$ thus there is no sense of saying " stopping a photon " in vacuum , it can exist only in motion and strictly at speed of light , or " absorbed " fully or partially , what will change it is energy/momentum , but not it is speed ! p . s the situation is more complicated for photons in some media , anyway one can slow down and maybe " even freeze " them .
but how do you calculate the g factor of a point particle or an extend particle ? this is done for a point particle , and any experimental deviation from the calculated value for a point particle would suggest structure beyond a point particle . dirac theory predicts g=2 . anomaly from g=2 has qed , hadronic and weak contributions , which are each calculated . the hadronic and weak contributions are small and considered to be well understood . the qed contribution to the anomaly is the main contribution and extremely difficult to calculate . hundreds of feymann diagrams are involved . see new determination of the fine structure constant from the electron g value and qed for more information .
let 's begin by choosing coordinates . let the orbit of the earth define the x-y plane . assume that just before the collision the earth 's orbital axis is pointing in the x-direction , that is $ ( 1,0,0 ) $ . i will assume that the earth absorbs the asteroid , and that after the collision the axis has been changed to $ ( \sin ( 27 ) , 0 , \cos ( 27 ) ) $ where 27 is the degree measure of the earth 's axis currently . so the relative change in angular momentum for the earth is $ ( 1,0,0 ) - ( \sin ( 27 ) , 0 , \cos ( 27 ) ) = ( 0.55,0 , - . 89 ) $ which is a vector of length 1.05 and so the angular momentum of the asteroid has to be 1.05 times the angular momentum of the earth . make the approximation that the earth is a sphere of constant density . then its angular momentum is given by $0.4 m_e\omega\ ; r_e^2$ where $m_e , r_e$ are the mass and radius of the earth and $\omega$ is its angular rate of rotation . we have $m_e = 6\times 10^{24}\ ; kg\ ; \ ; \ ; r_e = 6.4\times 10^6\ ; m , \ ; \ ; \ ; \omega = 2\pi/ ( 24\ ; hours ) = 2\pi/ ( 86400\ ; sec ) $ . i am using sloppy approximations here ; the earth is not constant density , i am not using the sidereal day , etc . this gives $7\times 10^{33}\ ; kg\ ; m^2/s$ as the approximate angular momentum contributed by the asteroid . the formula for angular momentum is mass x velocity x radius . this is maximum when the radius is maximum ; for an asteroid hitting the earth this happens when the radius is equal to the earth 's radius , i.e. $ 6.4\times 10^6\ ; m$ . dividing the asteroid 's angular momentum by this give its linear momentum : $p_a = 7\times 10^{33} / 6.4\times 10^6 = 10^{27}\ ; kg\ ; m/s$ the reason a relatively small asteroid can wipe out all life on earth is due to the kinetic energy it carries . upon collision , the kinetic energy is converted to heat . if the asteroid is large and fast enough , the heat will increase the temperature of the atmosphere enough to boil off the oceans and even vaporize the salt deposits left over . since kinetic energy is proportional to the square of velocity while momentum is only proportional to velocity , we will assume that our asteroid is moving as slowly as possible . the escape velocity of the earth is 11 km per second ; to achieve a slower velocity an asteroid would have to be very lucky . so in calculating the kinetic energy of the asteroid , we will assume a very conservative speed of 5 km/sec . putting $10^{27} = 5000 m$ we find the mass of the asteroid as $2\times 10^{23}$ kilograms . assuming a specific density of 5 , or 5000 kg per cubic meter , this gives a radius for the asteroid of 2000 kilometers or a diameter of 5000 km . this is far more than enough to destroy all life on the planet . the asteroid 's kinetic energy is $0.5\times 2\times 10^{23}\times ( 5000 ) ^2 = 2.5\times 10^{30}$ joules $= 6\times 10^{14}$ megatons . in other words , the collision would result in the release of kinetic energy equal to 600 million million hydrogen bombs going off , each with an equivalent energy of one million tons of high explosives . the earth 's surface area is 500 million square kilometers or 500 million million square meters . so the energy release is equivalent to having a 1 megaton bomb going off on each square meter of the earth 's surface . and since the vaporized rock from the blast is lighter than rock , this vapor will condense on the surface and apply its heat to the surface . for a video of a 500 km asteroid hitting the earth ( 1/1000 of the volume necessary to change the earth 's axis ) see : http://www.youtube.com/watch?v=vzizu42sn6w
first of all , when you say that trying to crack a pipe is hard work , what you probably mean ( in physics terms ) is that it takes a large force . but that does not necessarily mean that it requires a lot of energy . the energy used in a physical process like that is equal to the force times the distance over which the force is applied , and you do not have to push very far in order to crack a pipe . in fact , the pipe barely moves at all before it cracks , so even though the force required is quite large , it only acts over a tiny distance , and therefore it barely takes any actual energy . what little energy is required can come from the water itself . to explain the " how " you have to consider molecular interactions . ( well you do not have to , but i am going to . ) the energy of each pair of water molecules varies with the distance between them , in a manner shown ( approximately ) by the following graph ( from wikipedia ) . you will notice that there is a certain distance at which the energy is a minimum . this distance represents the " natural " equilibrium distance between molecules when there is no pressure . however , when the water is under pressure , the molecules get pushed together ( because pressure is roughly akin to force ) , so their actual distance will be a little closer than the minimum of the graph . water has the unique property that its " natural " density at a constant pressure reaches a maximum at a certain temperature , around 4 degrees celsius , and that its frozen form ( ice ) is less dense than its liquid form . in other words , the equilibrium intermolecular distance ( the minimum of the graph ) is smallest at 4 degrees celsius . if the water temperature is going to drop below 4 degrees celsius , the minimum shifts a little to the right , which means one of two things has to happen either the water expands ( if the intermolecular separation stays with the minimum of the graph ) , or its pressure rises ( if the intermolecular separation creeps up the slope of the graph ) . now think about the situation in a pipe . as long as the pipe stays intact , the water can not really expand at all . so the only option is for the pressure to increase . as the pressure increases , the force on the pipe increases as well , and you will notice that because the slope of the curve is very steep , the force increases very rapidly . at some point , the force becomes large enough to overwhelm the bonds that hold the atoms/molecules in the pipe together , and at that point , the pipe cracks . notice that in this theoretical model there is no need for any part of the pipe to have moved , which means the pipe could crack without any energy being used . ( in practice , there are other things going on that do make it take a tiny bit of energy . )
given a arbitrary metric $g_{\mu\nu}$ you can introduce a reference ( background ) metric $\bar{g}_{\mu\nu}$ ( in the paper notation it is just the minkowski metric $\eta_{\mu\nu}$ ) in a way that $\delta{}g_{\mu\nu} = g_{\mu\nu} - \bar{g}_{\mu\nu}$ is small ( in some sense ) . you can reintroduce the background metric in a way that the perturbation remains small , this action is parametrized by an small diffeomorphism ( you can see why in the mukhanov 's review of perturbations : dx . doi . org/10.1016/0370-1573 ( 92 ) 90044-z ) generated by an arbitrary vector field $-\xi^\alpha$ , thus , the background change by $$\bar{g}_{\mu\nu}\rightarrow\bar{g}_{\mu\nu}-\mathcal{l}_\xi\bar{g}_{\mu\nu} , $$ where $\mathcal{l}_\xi$ is the lie derivative with respect to $\xi^\alpha$ . given this transformation , the perturbation transform as $$\delta{}g_{\mu\nu}\rightarrow\delta{}g_{\mu\nu} + \mathcal{l}_\xi\bar{g}_{\mu\nu} . $$ if you choose a covariant derivative compatible with $\bar{g}_{\mu\nu}$ , say $\bar{\nabla}_\alpha\bar{g}_{\mu\nu} = 0$ , the lie derivative can be written as $$\mathcal{l}_\xi\bar{g}_{\mu\nu} = 2\bar{\nabla}_{ ( \mu}\xi_{\nu ) } . $$ in the paper the background metric is just the minkowski metric , then in cartesian coordinates $$\mathcal{l}_\xi\eta_{\mu\nu} = 2\partial_{ ( \mu}\xi_{\nu ) } = 2\xi_{ ( \mu , \nu ) } , $$ where the comma represent the partial derivative . so in general you can write a metric in terms of the background , perturbation and a gauge transformation as $$g_{\mu\nu} = \bar{g}_{\mu\nu} + \delta{}g_{\mu\nu} + 2\bar{\nabla}_{ ( \mu}\xi_{\nu ) } . $$
spin1/2 particle ususally , in this kind of hamiltonian , people uses $s=s_z$ , where $$s=s_z=\left [ \begin{array}{cc} 1 and 0 \\ 0 and -1\end{array} \right ] . $$ then , your unperturbed hamiltonian $h_0$ is : $$h_0=-\mu s\cdot b_0 = -\mu \left [ \begin{array}{cc} 1 and 0 \\ 0 and -1\end{array} \right ] b_{0 , z} . $$ then the eigen vectors of energy are : $$|\psi^0_+\rangle=\left [ \begin{array}{c} 1 \\ 0\end{array} \right ] , $$ $$|\psi^0_-\rangle=\left [ \begin{array}{c} 0\\ 1 \end{array} \right ] . $$ perturbation solution then you want to compute $|\psi_+\rangle$ and $|\psi_-\rangle$ for the perturbed hamiltonian $h=h_0-\mu b_1 s_x$ , where $$s_x=\left [ \begin{array}{cc} 0 and 1 \\ 1 and 0\end{array} \right ] . $$ as you said , you have to compute the following quantities ( note i use $+ , -$ instead of $n=0,1$ . which became : $$\psi^{ ( 1 ) }_+=\sum_{n\neq +} \psi^{ ( 0 ) }_{n'}\frac{\langle\psi_{n'}^{ ( 0 ) }|-\mu b_1s_x|\psi_{+}^{ ( 0 ) }\rangle}{e_+^{ ( 0 ) }-e_{n'}^{ ( 0 ) }}=\psi^{ ( 0 ) }_{-}\frac{\langle\psi_{-}^{ ( 0 ) }|-\mu b_1s_x|\psi_{+}^{ ( 0 ) }\rangle}{e_+^{ ( 0 ) }-e_{-}^{ ( 0 ) }}$$ $$\psi^{ ( 1 ) }_-=\sum_{n\neq -} \psi^{ ( 0 ) }_{n'}\frac{\langle\psi_{n'}^{ ( 0 ) }|-\mu b_1s_x|\psi_{-}^{ ( 0 ) }\rangle}{e_-^{ ( 0 ) }-e_{n'}^{ ( 0 ) }}=\psi^{ ( 0 ) }_{+}\frac{\langle\psi_{+}^{ ( 0 ) }|-\mu b_1s_x|\psi_{-}^{ ( 0 ) }\rangle}{e_-^{ ( 0 ) }-e_{+}^{ ( 0 ) }}$$ put here vectors and matrices we just found and let me know if you get zero .
it is because magnetic and electric fields transform into one another depending on your intertial frame , and therefore field lines are not an invariant intrinsic property of space . in one frame , you could have a region where there is only a uniform magnetic field $\vec b$ , so that any stationary charge remains classically at rest . yet viewed from a frame moving normal to the direction of this magnetic field , the fields now become : $$\vec {e'} = \gamma\vec v\times \vec b , \quad \vec {b'} = \gamma\vec{b} $$ in this frame , a stationary charge will now be accelerated by the electric field $e'$ .
the ideal kelvin boat wake ignores surface tension , and it assumes deep water waves with an ( in general ) broad spectrum of frequencies $\omega$ with dispersion relation $\omega^2=gk$ , where $g\approx 9.8 \frac{m}{s^2}$ . the ideal kelvin wake furthermore assumes that the ship sails with a constant velocity , and that the wave amplitudes of the partial waves are so small that they obey a linear superposition principle . the kelvin wake does not describe the narrow turbulent band behind a ship , nor shock waves . the kelvin wake consists of two types of waves : transverse and divergent waves . there are two characteristic angles $$\alpha\approx 19^{\circ} \qquad \mathrm{and} \qquad \beta\approx 35^{\circ} , $$ corresponding to $$\tan ( \alpha ) = \frac{1}{2\sqrt{2}} \qquad \mathrm{and} \qquad \tan ( \beta ) = \frac{1}{\sqrt{2}} , $$ or equivalently , $$\sin ( \alpha ) = \frac{1}{3} \qquad \mathrm{and} \qquad \sin ( \beta ) = \frac{1}{\sqrt{3}} . $$ in polar coordinates $ ( r , \theta ) $ of a co-moving coordinate system , where the position of the boat is at the origin , the transverse waves are in the region $|\theta|\leq \beta$ , and divergent waves are in the region $\alpha\leq |\theta|\leq \beta$ . the angles $\alpha$ and $\beta$ are constant in at least two ways : firstly , they do not depend on the distance $r$ to the ship . this is because the speed of each partial wave ( with frequency $\omega$ ) is independent of the position $ ( x , y ) $ . secondly , $\alpha$ and $\beta$ are , evidently , universal angles , independent of , for instance , $g$ . this is explained in the references below . references : 1 ) howard georgi , " the physics of waves " , chapter 14 . ( hat tip:user1631 . ) 2 ) mit on-line open course ware , mechanical engineering , wave propagation , lecture notes , fall 2006 , chapter 4.7 . 3 ) wikiwaves .
yes , single photon radio waves have been constructed . radio communication is now possible on the most elementary level : scientists at the eth zurich and the max planck institute for the science of light in erlangen have used two molecules as antennas and transmitted signals in the form of single photons , i.e. light particles , from one to the other . since a single photon usually has very little interaction with a molecule , the physicists had to use a few experimental tricks for the receiver molecule to register the light signal . a radio connection established via individual photons would be ideal for various applications in quantum communication – in quantum cryptography or in a quantum computer , for example . the discussion in this question should enlighten you , and the links given there . there is a one to one correspondence between classical and quantum electrodynamics , both rely on the maxwell equations as you will see if you read the detailed answer . in the link in my answer the way one goes mathematically from one to the other is shown . no need for extra experiments since the mathematics is rigorous . for low energies of the electromagnetic waves it is not very smart to use all the mathematical panoply if one can do the job with the classical representation , except as in this case of single photon transmissions .
obviously things vary from subfield to subfield , so canonical advice is probably to much to hope for . with that in mind , let me try to give you something of an answer . in principle when a person finishes a phd , if they remain in academia , there are a number of ways this might happen . they might end up with : a teaching job with little or no research emphasis , a postdoc supported from a faculty members grant , a personal fellowship , or a junior faculty position . now , ( 4 ) is essentially unheard of these days , so you can more or less forget that . of the rest , what is feasible will depend on a number of factors , but the impact of your research is going to play a big role in your success in applying for either ( 2 ) or ( 3 ) . i presume , since you are only looking at what to apply for now , that you have not finished the phd yet , and have something on the order of six months to a year left ( do correct me if i am wrong ) . with this in mind , it seems quite possible that you will have more than one publication by the end of the phd . if this is possible , you should put in the effort to write up results . if you get 3 or 4 papers ( preprints definitely count , so make sure you are using the arxiv and including them on your cv ) , this will substantially alter your odds of getting a postdoc from someone you have not previously worked with . personal fellowships are the most sought after form of postdoc , as they usually allow substantial autonomy , and so provide something of a springboard for a research career . this means that they are competitive and realistically , i can not see someone getting one on a single publication . i was rejected from some of the ones i applied for with 8 publications including a number of prls . postdocs supported from someones grant are easier to get , but still competitive . if you have only one publication , it might prove hard to convince someone you do not already know of your potential , though it is perhaps still worth trying , especially if your paper is particularly interesting . the best way to get a position if you are really limited on publications is as a postdoc on a grant supported by someone you have worked with before , or who knows your work well . these people are more likely to hire you on potential or other factors , rather than on your publication record . the scope for this is of course limited , so as pieter points out the more people who know you and your work , the better . that said , my advice would definitely be to start applying and get your results written up as soon as possible .
yes , $$\sum_{\mu} f_{\mu}{}^{\mu}~:=~\sum_{\mu , \nu}f_{\mu\nu} g^{\nu\mu}~=~0$$ vanishes because it is a trace of a product of a symmetric and an antisymmetric tensor . it is irrelevant for the argument that $f_{\mu\nu}$ is lie algebra valued .
static friction always opposes relative motion at the point of contact . there are two cases possible : 1 ) it orients itself in direction and magnitude in such a way that the relative acceleration of the contact point is zero . 2 ) if this is not possible ( such as in friction is too small to prevent motion ) , it tries to minimize the relative acceleration .
a large transformation is a transformation that can not be continuously connected with the identity transformation ( with the transformation that does nothing ) though other allowed transformations . so large transformations are grouped into " sectors " that are discretely separated from each other . in electromagnetism , the gauge transformations are $u ( 1 ) $ transformations parameterized by the number $\lambda ( x , y , z , t ) $ that is defined modulo $2\pi$ . charged fields of integral charge $q$ transform as $$\psi\to e^{iq\lambda} \psi$$ so you can see that only $\exp ( i\lambda ) $ matters : shifts of $\lambda$ by $2\pi n$ where $n\in{\mathbb z}$ are unphysical . in the case of the aharonov-bohm effect , there is a closed loop $c$ around the solenoid ( where the magnetic field is localized ) and the relevant " large gauge transformation " is given by $$\lambda = \phi$$ where $0\leq \phi\leq 2\pi$ is a periodic , angular variable parameterizing the closed loop $c$ ( in the simplest parameterization , some angle in the axial or spherical coordinates ) . note that even though this $\lambda$ is not a single-valued function of the spacetime coordinates $x , y , z$ because it jumps when $\phi$ is increased by $2\pi$ ( which corresponds to the return to the original point in space ) , it is an allowed gauge transformation because $\lambda\mod 2\pi$ or , equivalently , $\exp ( i\lambda ) $ is a single-valued function of space , and that is everything that is needed . such a gauge transformation may be ill-defined inside the contour $c$ i.e. inside the solenoid , however . when this minimal large ( topologically nontrivial ) transformation is performed , the gauge potential $\vec a$ is charged by $\nabla\cdot \lambda$ . the contour integral changes by $2\pi$ $$\oint_c \vec{d\ell}\cdot \vec a \to \oint_c \vec{d\ell}\cdot \vec a + 2\pi$$ because the integral is an integral of a gradient of $\lambda$ , so it is just the difference of $\lambda$ between the initial and final points which is $2\pi$ , up to a sign . by stokes ' theorem , $\oint_c \vec{d\ell}\cdot \vec a$ is the same thing as the integral $$\int_\sigma \vec{b}\cdot \vec{ds}$$ over the interior $\sigma$ of the contour $c$ , i.e. inside the solenoid , so this magnetic flux jumps by $2\pi$ as well . i was neglecting the factors $e , c , \hbar$ above . with the correct factors included in the sentences above , the jump of the magnetic flux is $2\pi\hbar / e$ in your units and conventions . so the two physical configurations may differ in their values of the magnetic flux through $\sigma$ but they are physically equivalent i.e. indistinguishable because they are related by a large gauge transformation ( although one that is only well-defined outside the solenoid ) .
is there any physical system which can be kept in mind as a simple example of the same ? yes . consider a single spin $1/2$ particle , like an electron . in this case , the matrix will be $2$-by-$2$ since its a representation of $\mathrm{su} ( 2 ) $ acting on the two-dimensional spin-$1/2$ hilbert space . the idea here is that when you rotate the physical system by a rotation $r$ say , then the spin state " rotates " as well ( the states in the hilbert space " rotate " into each other if you will ) as follows : \begin{align} |\tfrac{1}{2} , m'\rangle\longrightarrow d^{1/2}_{m , m'}|\tfrac{1}{2} , m'\rangle \end{align} in fact , for a rotation by an angle $\theta$ about the unit vector $\mathbf n = ( n_x , n_y , n_z ) $ , we have \begin{align} ( d^{1/2}_{m , m'} ) =\begin{pmatrix} \cos\frac{\theta}{2}-in_z\sin\frac{\theta}{2} and ( -n_y-in_x ) \sin\frac{\theta}{2} \\ ( n_y-in_x ) \sin\frac{\theta}{2} and \cos\frac{\theta}{2}+in_z\sin\frac{\theta}{2} \\ \end{pmatrix} \end{align} so that when $\theta = 0$ , this is the identity matrix ; nothing happens to the state , while when $\theta = 2\pi$ ( a full rotation ) , this matrix is $-1$ times the identity , and the state therefore rotates into itself multiplied by $-1$ ; pretty strange is not it ? a general explanation of the idea of irreps , beyond just the wigner-d matrix , would be appreciated . this is an extremely broad question . the general idea behind a representation of a group $g$ is that it is a mapping that assigns an invertible matrix $d ( g ) $ to each group element $g$ such that the group structure is preserved ( the technical term for this is that it is a group homomorphism ) . the representation is said to be irreducible if it has no nontrivial invariant subspaces . concretely , this means that there is no similarity transformation that puts all of the representation matrices into block diagonal form .
the only dimension-five operators allowed by the sm are neutrino masses , $ ( hl_i ) ( hl_j ) $ . so we mostly talk about dimension-six operators because for almost any question they are the first higher-dimension operators that can appear .
you are right that we do not know if the universe is finite or infinite in space . cosmologists do now think that it has an infinite future because of the accelerated expansion rate due to dark energy but this does not tell us anything about the question of infinite space . to answer the question for space we first have to assume spacial homogeniety , i.e. that space looks the same everywhere on large scales . if this assumption is wrong then we are stuck because we cannot see beyond the horizon of the observable universe which is due to the finite speed of light and the finite age of the universe . if we can not assume anything about what happens beyond the horizon then obviously we cant tell if it is finite or infinite . however , within the observable universe space does appear to be homogeneous so it is usual to assume , rightly or wrongly , that it is homogeneous everywhere . if that is the case then the question of the finiteness depends on the curvature of space and this is something that can be estimated with some precision using cosmological observations , especially that of the cosmic microwave background . if the curvature of space takes a positive value then it must be finite . if it is zero or negative then it is probably infinite though the possibility of a finite universe remains if it has an unusual topology like a tessellation of a finite polyhedron . when we measure the curvature we find that it is close to zero and we cannot tell whether it is positive , zero or negative with the present error bars , so we do not know if the universe is finite or infinite in space . this flatness is expected as a prediction of inflation theory . the best we can do is set a lower limit to how small the universe can be given limits on its measured curvature and that is what the papers you linked to are trying to do . they do not claim to settle the question of whether it is finite or infinite . your final question about quantum mechanics and the outside of the universe is unrelated and should have been asked as a separate question , but the answer is no .
as you said , the case of black holes is conceptually totally analogous to the burning books . in principle , the process is reversible , but the probability of the cpt-conjugated process ( more accurate a symmetry than just time reversal ) is different from the original one because $$ \frac{prob ( a\to b ) }{prob ( b^{cpt}\to a^{cpt} ) } \approx \exp ( s_b-s_a ) . $$ this is true because the probabilities of evolution between ensembles are obtained by summing over final states but averaging over initial states . the averaging differs from summing by the extra factor of $1/n = \exp ( -s ) $ , and that is why the exponential of the entropy difference quantifies the past-future asymmetry of the evolution . at the qualitative level , a white hole is exactly as impossible in practice as a burning coal suddenly rearranging into a particular book . quantitatively speaking , it is more impossible because the drop of entropy would be much greater : black holes have the greatest entropy among all localized or bound objects of the same total mass . however , the hawking radiation is not localized or bound and it actually has an even greater entropy – by a significant factor – than the black hole from which it evaporated . that is needed and that is true because even the hawking evaporation process agrees with the second law of thermodynamics . at the level of classical general relativity , nothing prevents us from drawing a white hole spacetime . in fact , the spacetime for an eternal black hole is already perfectly time-reversal-symmetric . we still mostly call it a black hole but it is a " white hole " at the same moment . such solutions do not correspond to the reality in which black holes always come from a lower-entropy initial state – because the initial state of the universe could not have any black holes . so the real issue are the realistic diagrams for a star collapsing into a black hole which later evaporates . such a diagram is clearly time-reversal-asymmetric . the entropy increases during the star collapse as well as during the hawking radiation . you may flip the diagram upside down and you will get a picture that solves the equations of general relativity . however , it will heavily violate the second law of thermodynamics . any consistent classical or quantum theory explains and guarantees the thermodynamic phenomena and laws microscopically , i.e. by statistical physics applied to its phase space or hilbert space . that is true for burning books but that is true for theories containing black holes , too . so if one has a consistent microscopic quantum theory for this process – but the same comment would hold for a classical theory as well : your question has really nothing to do with quantum mechanics per se – then this theory must predict that the inverted processes that decrease entropy are exponentially unlikely . whenever there is a specific model with well-defined microstates and a microscopic t or cpt symmetry , it is easy to prove the equation i started with . a genuine microscopic theory really establishes that the inverted processes ( those that lower the total entropy ) are possible but very unlikely . a classical theory of macroscopic matter however " averages over many atoms " . for solids , liquids , and gases , this is manifested by time-reversal-asymmetric terms in the effective equations - diffusion , heat diffusion , friction , viscosity , all these things that slow things down , heat them up , and transfer heat from warmer bodies to cooler ones . the transfer of heat from warmer bodies to cooler ones may either occur by " direct contact " which really looks classical but it may also proceed via the black body radiation – which is a quantum process and may be found in the first semiclassical corrections to classical physics . the hawking radiation is an example of the " transfer of heat from warmer to cooler bodies " , too . the black hole has a nonzero temperature so it radiates energy away to the empty space whose temperature is zero . again , it does not " realistically " occur in the opposite chronological order because the entropy would decrease and a cooler object would spontaneously transfer its heat to a warmer one . in an approximate macroscopic effective theory that incorporates the microscopic statistical phenomena collectively , much like friction terms in mechanics , those time-reversal-violating terms appear explicitly : they are replacements/results of some statistical physics calculations . in the exact microscopic theory , however , there are no explicit time-reversal-breaking terms . and indeed , according to the full microscopic theory – e.g. a consistent theory of quantum gravity – the entropy-lowering processes are not strictly forbidden , they may just be calculated to be exponentially unlikely . the probability that we arrange the initial state of the black hole so that it will evolve into a star with some particular shape and composition is extremely tiny . it is hard to describe the state of the black hole microstates explicitly , but even in setups where we know them in principle , it is practically impossible to locate black hole microstates that have evolved from a recent star ( or will evolve into a star soon , which is the same mathematical problem ) . your $u^{-1}$ transformation undoubtedly exists in a consistent theory of quantum gravity – e.g. in ads/cft – but if you want the final state $u^{-1}|initial\rangle$ to have a lower entropy than the initial one , you must carefully cherry-pick the initial one and it is exponentially unlikely that you will be able to prepare such an initial state , whether it is experimental preparation or a theoretical one . for " realistically preparable " initial states , the final states will have a higher entropy . this is true everywhere in physics and has nothing specific in the context of quantum gravity with black holes . let me also say that the " white hole " microstates exist but they are the same thing as the " black hole microstates " . the reason why these microstates almost always behave as black holes and not white holes is the second law of thermodynamics once again : it is just very unlikely for them to evolve to a lower-entropy state ( at least if we expect this entropy drop to be imminent : within a long enough , poincaré recurrence time , such thing may occur at some point ) . that is true for burned books , too . a " white hole " is analogous to a " burned book that will conspire its atomic vibrations and rearrange itself into a nice and healthy book again " . but macroscopically , such " books waiting to be revived " do not differ from other piles of ashes ; that is the analogous claim to the claim that there is no visible difference between black hole and white hole microstates , and due to their " very likely " future evolution , the whole class should better be called " black hole microstates " and not " white hole microstates " even the microstates that will drop entropy soon represent a tiny fraction of this set . my main punch line is that at the level of general reversibility , there has never been any qualitative difference between black holes and other objects that are subject to thermodynamics and , which is related , there has never been ( and there is not ) any general incompatibility between the general principles of quantum mechanics , microscopic reversibility , and macroscopic irreversibility , whether black holes are present or not . the only " new " feature of black holes that sparked the decades of efforts and debates was the causality . while a burning book may still transfer the information in both ways , the material inside the black hole should no longer be able to transfer the information about itself to infinity because it is equivalent to superluminal signals forbidden in relativity . however , we know today that the laws of causality are not this strict in the presence of black holes and the information is leaked , so the qualitative features of a collapsing star and evaporating black hole are literally the same as in a book that is printed by diffusing ink and then burned .
also what will happen if by some means we are able to observe both things accurately ? you are assuming that both ' things ' ( position and momentum ) have definite values simultaneously . but , according to quantum mechanics , they do not . whatever a quantum mechanical particle is , it is not an entity that can exist in a state of definite position and definite momentum simultaneously . so , you need to do some more reading and thinking about the uncertainly principle . from the wikipedia article " uncertainty principle ": historically , the uncertainty principle has been confused with a somewhat similar effect in physics , called the observer effect , which notes that measurements of certain systems cannot be made without affecting the systems . heisenberg offered such an observer effect at the quantum level as a physical " explanation " of quantum uncertainty . it has since become clear , however , that the uncertainty principle is inherent in the properties of all wave-like systems , and that it arises in quantum mechanics simply due to the matter wave nature of all quantum objects . thus , the uncertainty principle actually states a fundamental property of quantum systems , and is not a statement about the observational success of current technology .
you are right , real photons always travel at the speed of light and would carry energy away from a magnet . from a field theory point of view , all static fields , whether electric , magnetic , the weak nuclear force or the strong nuclear force can be thought of as being mediated by virtual particles . so for either electric or magnetic fields , that would be virtual photons that mediate the field . see wikipedia for more information . another great article explaining virtual particles is from john baez and finally see this question and answer : what&#39 ; s the relation between virtual photons and electromagnetic potentials ? .
my memory of peskin and schroder is a little hazy , but they are probably discussing the shifman-vainshtein-zakharov sum rules . the idea is that you can use the opes for composite operators representing mesons/hadrons to derive formulae that express meson/hadron n-point functions in terms of the vevs of various qcd condensates . ( edit : just discovered that shifman has some very nice lecture notes on the subject . ) more generally : opes are always relevant ( if not always easy to use in a given situation ) because they carry almost all the information about a field theory . you can actually define a qft by writing down the set of local observables , the opes between them , and the vevs of the local observables . it is as good a formalism as the hamiltonian or path integral formalisms -- better in some ways , because it applies when the path integral does not .
which superpositions are allowed or forbidden in principle is determined by so-called superselection sectors . http://en.wikipedia.org/wiki/superselection these are virtually absent from the standard textbook literature as they cannot be inferred from the traditional perturbative treatment of quantum mechanics or quantum field theory . the superselection rule easiest to understand is the one that forbids the superpositions of a boson state and a fermion state . the reason is that these states transform differently under the action of the rotation group , hence there is no consistent action of the rotation group on the superposition . ( under a 360 degree rotation the boson half would rotate back to itself , while the fermion half changes its sign . ) this shows that superselection rules are tied to inequivalent representation of lie groups or lie algebras of quantities defining the physics of a system . the charge superselection rule is associated to inequivalent representations of an infinite-dimensional heisenberg group defining the canonical commutation relations ( ccr ) of a relativistic quantum field . [ my discussion assumes the bosonic case . in case of fermions , one needs instead the car , and s similar reasoning applies . ] here the reasoning is more intricate , and requires the nonperturbative setting of algebraic quantum field theory . in this setting , observables form a c^*-algebra , and states are suitable positive linear functionals of this algebra . the analogous standard qm situation is where the c^*-algebra is the algebra of bounded linear operators on a hilbert space , and a state is the expectation mapping $\langle a\rangle =tr ~\rho a$ with a positive semidefinite density matrix $\rho$ of trace 1 . here the uniqueness theorem of von neumann guarantees that the canonical commutation relations have a unique unitary representation up to unitary equivalence . however , this theorem only holds for ccr of finitely many operators , whereas field theory deals with infinitely many of these . the ccr of field theory have infinitely many inequivalent representations , and these live in different hilbert spaces , between the elements of which no sensible inner product is defined . as it makes no sense to consider superpositions between states of two different hilbert spaces ( not embedded in a common hilbert space with a physical meaning ) , inequivalent representations imply a superselection rule . this implies the charge superselection rule for qed , as it can be shown that in qed , states of different charge must lie in inequivaent representations .
it is a common misconception to think that because the higgs mechanism is the origin of mass it is also the origin of gravity . this is a misconception because the origin of gravity is not simply mass . instead it is a quantity called the stress-energy tensor . the stress-energy tensor is usually represented as a 4 $\times$ 4 matrix containing 10 independant entries ( 10 not 16 because the matrix is symmetric ) and in most cases the only significant entry is the top left one , $t_{00}$ , which gives the energy density . the key point is that as far as the stress-energy tensor is concerned mass and energy are the same thing related by einstein 's famous equation $e = mc^2$ . immediately before the electroweak transition all particles were massless , and immediately after they had a finite mass , but this change did not make any difference to the stress-energy tensor and therefore to gravity . before and after the transition the energy density was the same ( well similar anyway ) so the contribution of the particles to the stress-energy tensor and therefore to gravity was the same . so there is no contradiction between the higgs mechanism and the idea of entropic gravity .
if i understand correctly , the question is about whether , in deriving the galilean transformation as an approximate limiting case of the lorentz transformation , it is necessary to impose the requirement that $vx$ is small , in addition to the " obvious " requirement that $v$ is small . the answer is yes , it is necessary . of course , since $vx$ is not a dimensionless quantity , we have to specify what we mean when requiring that it is small . the specific requirement is that $vx/c^2\ll \delta t$ , where $\delta t$ is the precision with which we wish to calculate time intervals . suppose that , in the inertial reference frame in which you are at rest , two stars explode simultaneously , one here and one in the andromeda galaxy . consider the same two events in the reference frame of someone walking at a liesurely pace . the lorentz transformation indicates that they will be separated by a time interval of about 2 days . if you care about levels of precision less than that , you can not use the galilean transformation , even though $v/c$ is small .
starting from the hamiltonian formulation of qm one can derive the path-integral formalism ( see chapter 9 in weinberg 's qft volume 1 ) , where the hamiltonian action is found to be proportional to $\int \mathrm{d}t ( pv - h ) $ . for a subclass of theories with " a hamiltonian that is quadratic in the momenta " ( see section "9.3 lagrangian version of the path-integral formula " in above textbook ) , the term $ ( pv - h ) $ can be transformed into a lagrangian $l_h = ( pv - h ) $ . then the lagrangian action is proportional to $\int \mathrm{d}t l_h$ . both actions give the same results because one is exactly equivalent to ( and derived from ) the other . $$ \int \mathrm{d}t ( pv - h ) = \int \mathrm{d}t l_h$$ moreover , when working in the interaction representation you do not use the total hamiltonian but only the interaction . the derivation of the hamiltonian action is the same , except that now the total hamiltonian is substituted by the interaction hamiltonian $v$ . again you have two equivalent forms of write the action either in hamiltonian or lagrangian form . if you consider hamiltonians whose interaction $v$ does not depend on the momenta , then the $pv$ term vanishes and the above equivalence between the actions reduces to $$ - \int \mathrm{d}t v = \int \mathrm{d}t l_v$$ where , evidently , the interaction lagrangian is $l_v = -v$ this is what happens for instance in qed , where the interaction $v$ depends on both position and dirac $\alpha$ but not on momenta . note : there is a sign mistake in your post . i cannot edit because is less than 10 characters and i have noticed the mistake in a comment to you above , but it remains .
unless i am missing an easy way to do this problem it seems a surprisingly hard one . this diagram shows the problem ( i have exaggerated the altitude of the satellite to make the diagram clearer ) : the satellites are in circular orbits ( dotted line ) at a distance $r$ from the centre of the earth , so their orbital velocity as ( as you say ) : $$ v = \sqrt{\frac{gm}{r}} $$ assuming the two satellites stick to each other when they collide you end up with a single 500kg mass of twisted metal moving at some velocity $v'$ that is less than $v$ . the new orbit will be an ellipse , and the question is whether the new orbit intersects the earth 's surface . if the new orbit does not intersect the earth then the fused satellites will continue to orbit , while if the new orbit does intersect the earth obviously the debris will crash into the earth . the first step is to calculate v ' , and this is done simply by conservation of momentum . at the moment before the collision the momentum of the 400kg satellite is $400v$ and the momentum of the 100kg satellite is $-100v$ ( i am taking velocity positive to the right ) . after the collision the momentum of the wreckage is $500v'$ , so : $$ 400v - 100v = 500 v ' $$ and we get : $$ v ' = \frac{3}{5}v \tag{1} $$ the hard bit is working out the new orbit . for this you need to start with the vis viva equation , and with some head scratching you can work out the equation for the perigee distance ( $r_p$ in the diagram ) : $$ r_p = \frac{r_a}{\frac{2gm}{v'^2r_a} - 1} $$ the question is then simply whether $r_p$ is less than the radius of the earth . you know $v'$ from equation ( 1 ) , and $r_a$ is just the initial orbital radius ( measured from the centre of the earth ) . i will leave it up to you you calculate $r_p$ and answer the question .
provided the intervals between all events are spacelike they can appear in any order . see this article for a popular science level description , or this paper for the full details .
conformal field theories do not have a mass-gap , which is one of the assumptions [ for the strong conclusions of non-mixing of poincare spacetime symmetries vs internal symmetries ] of the coleman-mandula no-go theorem . similarly , for its superversion : the haag-lopuszanski-sohnius no-go theorem . [ in the supercase , the poincare algebra is replaced with the super-poincare algebra . ]
a magnet is in essence made up of atoms with orbital and spin angular momentum , ( mainly due to the electrons ) and the forces that act on these electrons can be derived from dirac 's equation , but give you what is essentially the lorentz force law . you can think of the magnetic field acting on every single electron individually , and all these forces added up will apply a net force and a net torque to the magnet as a rigid body . if you want to do some calculations , you can imagine each atom as having a microscopic current proportional to the dipole moment of that atom ( basically , the magnetization ) . this " bound current " is $\vec{j} = \nabla \times \vec m$ . again , imagine an electron orbiting the nucleus as providing a " current " flowing around the nucleus -- the bohr magneton . it is just an idealization but you can make it rigorous if you want . now , currents of adjacent atoms cancel out where they intersect , because they are going in opposite directions , but on the boundary of the magnet they do not cancel out , because there is no neighboring atom there . thus it is perfectly mathematically and physically sensible to model a permanent magnet as a sheet of current moving along the magnet 's surface , roughly like a solenoid does , and given by $\vec{k} = \vec{n}\times \vec{m}$ , where $\vec{n}$ is the normal to the surface . now this surface current is a real current , and therefore it will experience a force when subjected to an external magnetic field ( and it is own magnetic field , btw . this is why transformers hum ) .
i think you are being a bit hard on scientific american . it is a popular ( if slightly geeky ) magazine so you would not expect its articles to have all the gory details . the best way to find info about areas like this is to search arxiv . org . for example googling for " super wimp site:arxiv . org/abs " finds http://arxiv.org/abs/0812.0432 and this looks like a good place to start . there have been various suggestions for particles that only interact by the gravitational force . one example is the sterile neutrino .
the classical field has a straightforward interpretation in the bosonic case--- it is determined by the density and phase of the superfluid condensate of the particles and both are simultaneously measurable in the thermodynamic limit . if there is no superfluid , the field is zero . where there is a superfluid , it is the square-root of the density , with a phase whose gradient is the local superflow . you can measure the value of psi-squared at x , that is determining the superfluid condensate density , which can be done by shining light through the fluid to get the index . you can also measure the value of the velocity by the exact way light refrects ( or if it is dense enough , by putting little dust specks in the fluid ) . a gaseous bose-einstein condensate , or even liquid helium , is described precisely by the classical limit of this formalism in the thermodynamic limit . if you take the classical limit wrong , by insisting that the particle number n is fixed , you do still get hbars lurking around . in order to take a good classical field limit , you need enormous occupation numbers for the field , which requires that you take n to infinity and the mass to zero keeping the density fixed . in this limit , the conjugate variables density/phase become commuting . as for measuring the " x-projection of operators " , i could not figure out what that meant . you can measure the field momentum too , of course , by just taking the complex conjugate of the measurement of the field ( the imaginary part of the field and the real part are not independent , and you can describe the whole system using only the real part and its time derivative , as described in the wikipedia article ) .
the key here is that you think there is no skidding . in fact , there is skidding , although for normal automobiles this is barely noticeable . for normal cars , the rear wheels simply skid a lot less than would the front wheels when a turn would be fully forced . you can see this also in trucks , where it becomes necessary to have dual or triple-axle steering when doing tight turns while manoeuvring .
first of all , the standard model does not treat bosonic fields as classical . they are quantum mechanical i.e. non-classical , they are just not anticommuting or grassmann-odd . second , a consistent theory just requires the relationship between spin and statistics , see e.g. the http://en.wikipedia.org/wiki/spin-statistics_theorem combining integer spin with fermi statistics leads to ghost or energy or the norm that is not positively definite , and vice versa ( half-integer spin with bose statistics ) . it was proved by pauli . however , your very question did not actually talk about the integer vs half-integer spin at all . it was talking about the relationship between fermions and anticommuting fields . this is almost a tautology . a fermion is a particle whose wave function for many particles is antisymmetric , $\psi ( x_1 , x_2 ) =-\psi ( x_2 , x_1 ) $ etc . , so the fields that create these particles must be anticommuting , $a^\dagger ( x_1 ) a^\dagger ( x_2 ) =-a^\dagger ( x_2 ) a^\dagger ( x_1 ) $ . the multiparticle state in qft is written as $$ |\text{2 fermions}\rangle = \int d^3 x_1 d^3 x_2\ , \psi ( x_1 , x_2 ) a^\dagger ( x_1 ) a^\dagger ( x_2 ) |0\rangle $$ because the wave function $\psi$ is antisymmetric , only the antisymmetric combination $a^\dagger ( x_1 ) a^\dagger ( x_2 ) - a^\dagger ( x_2 ) a^\dagger ( x_1 ) $ contributes to the state , and in fact , only this combination is nonzero . the sum – the anticommutator – vanishes . that is why the antisymmetry of $\psi$ is " automatic": if there were a non-antisymmetric part of $\psi$ , it would vanish in the integral above because the product of the creation operators is antisymmetric . the same for bosons and " commuting " , without the minus sign . the answer to your " why " question is that your statement is really a tautology , pretty much a definition of bosons and fermions , up to the possibly confusing comments about " one antisymmetry " implying the " other antisymmetry " above . of course , you could also ask why one uses commuting or anti-commuting fields to describe particles at all . well , nature just works in this way . quantum fields naturally reduce to multi-body quantum mechanics with the automatic symmetry or antisymmetry – and they may give rise to automatically lorentz-invariant theories , too ( something that would be hard in the " non-relativistically styled " multiparticle quantum mechanics ) .
since you are dealing with an inelastic collision , energy is not conserved when the bullet hits the block . you should try to find a relation between the initial velocity of the bullet and the velocity of the combined system ( bullet+block ) after the collision from conservation of momentum .
you have done the two hard parts : ( 1 ) relate area to force and intensity and ( 2 ) relate distance , acceleration , and time . for connecting these two , i suggest looking at newton 's second law .
i completely agree with scott that this particular " grassmannization " is not equivalent to what supersymmetry is doing in physics . supersymmetry is a constraint that picks a subset of theories – ordinary theories with ordinary bosonic and fermionic fields that are just arranged ( and whose interactions are arranged ) so that there is an extra grassmann-odd symmetry . because supersymmetric theories are a subset of more general theories , of course that all the general inequalities that hold for the more general theories hold for supersymmetric theories , too . and there are many new inequalities and conditions that hold for supersymmetric theories – but not fewer constraints . in supersymmetric theories , what becomes grassmann numbers are never probability amplitudes . only particular observables are fermionic operators – operator counterparts of grassmann-number-valued quantities in classical physics . these fermionic operators only have nonzero matrix elements between grassmann-odd states and grassmann-even states ; for the same reason why bosonic operators only have nonzero matrix elements between states of the same grading . one may introduce a grading on the hilbert space but the amplitudes are still complex commuting $c$-numbers . there is a simple reason why probability amplitudes can not be grassmann numbers . to get physical commuting quantities out of grassmann numbers , one always has to integrate . that is why the grassmann variables may be integration variables integrated over in feynman 's path integral ; but that is also why they have to be set to zero if we are doing classical physics . there are not any particular nonzero values of grassmann numbers . on the other hand , probability amplitudes do not have to be integrated ; their absolute values should be just squared to obtain the probabilities ( or their densities such as differential cross sections ) . so if their construction is consistent at all , it is just a mathematical analogy of superspaces at a different level – amplitudes themselves are considered " superfields " even though in genuine quantum physics , amplitudes are always complex numbers . that is why the inequalities can not be considered analogous to bell-like inequalities and can not be applied to real physics . in particular , once again , tsirelson 's bound can not be violated by theories just because they are supersymmetric ( in the conventional sense , just like the mssm or type iib string theory ) because it may be derived for any quantum theory , whether it is supersymmetric or not , and supersymmetric theories are just a submanifold of more general theories for which the inequality holds . i would point out that it would not be the first time when michael duff and collaborators would be giving wrong interpretations to various objects related to quantum computation . some formulae for the entropy of black holes mathematically resemble formulae for entangled qubits etc . but the interpretation is completely different . in particular , the actual information carried by a black hole is $a/4g$ nats i.e. the black holes roughly parameterize an $\exp ( a/4g ) $-dimensional space of microstates . that is very different ( by one exponentiation ) from what is needed for the quantum-information interpretation of these formulae in which the charges themselves play the role of the number of microstates . so i think that at least michael duff has been sloppy when it came to the interpretation of these objects which was the source of his misleading comments about the " black hole entropy formulae emulating tasks in quantum computation " . there may be mathematical similarities – i am particularly referring to the cayley hyperdeterminant appearing both in quantum computing and black hole entropy formulae – but the black holes are not really models of those quantum algorithms because their actual hilbert space dimension is the exponential of what it should be for that interpretation and they are manipulating pretty much all the qubits at the same moment . the objects in the hyperdeterminant have completely different interpretations on the string theory and quantum computing side ; there is not any physical duality here , either .
i will approach this question theoretically , although i feel the intuition follows nicely . if we talk about kerr black holes - rotating black holes described by their mass and angular momentum , with no additional parameters such as charge etc . - then you can show that the radius of the event horizon is given by $\boxed{r=m + \sqrt{m^2-a^2}}$ where $a=\frac{j}{m}$ . ( this value of $r$ is found by finding where the kerr metric blows up ; hence event horizon . in fact , finding where the metric blows up involves solving a quadratic equation , so we get two values of $r$ and in kerr black holes we therefore have two event horizons ; unlike in schwarzschild black holes . ) regarding your first point about maximum angular momentum , if we set $g=1$ and $c=1$ , the maximum angular momentum you stated is given by $a=m$ and if we plug this into our equation for $r$ above we see that we have $r=m$ . we know that the radius of the event horizon in a schwarzschild black hole ( no rotation ) is $r=2m$ . so therefore we can see that at maximum angular momentum , the radius of the event horizon is half of what it would be if the black hole were not spinning . to this end , we can also see that at zero angular momentum , $a=0$ , we have $r=2m$ which is what we want as at zero angular momentum we of course should have the schwarzschild radius . using the boxed equation for $r$ at the top , it is easy to test out different values of $a$ to see what happens to the event horizon . for example , this equation alone is sufficient to show that for $a&gt ; m$ we do not have an event horizon , in which case we have what is a called " fast kerr " which is just a singularity with no event horizon .
there is a more physically intuitive way to do this in my opinion . note that in your picture , the longer stick has length $l_0 = 1\ , \mathrm{m}$ and the shorter stick has length $l=l_0/\gamma_u$ because of length contraction . therefore , the time $t = 12.5\ , \mathrm{ns}$ given in the problem corresponds to the shorter stick traveling a distance $l_0-l_0/\gamma_u$ . this gives the equation $$ l_0 - \frac{l_0}{\gamma_u} = u t $$ now simply solve for $u$ . i checked this numerically by the way and it gives $u=c/2$ .
i found this to be interesting , because at least pedagogically people will write down long range electronic interactions which totally break gauge invariance . i mean we have all seen someone write down a " general four point interaction": $$\int\psi ( 1 ) \psi ( 2 ) \bar{\psi} ( 3 ) \bar{\psi} ( 4 ) v ( 1,2,3,4 ) $$ . this breaks gauge invariance , which is usually horrible , but frequently this does not seem to lead any obvious issue . why is that ? it is bothered me before . to return specifically to your question , first let 's rewrite in real space : $$\int\delta ( r ) u ( r , r' ) \bar{\delta} ( r' ) $$ where $\delta = c_{\uparrow}c_{\downarrow}$ has $u ( 1 ) $ charge $2$ . this is not gauge invariant unless $u$ transforms properly . now we started with a gauge invariant system at some level , so we must have gotten to this point by " integrating out " out some charged degrees of freedom . in this simplest scenario we did second order perturbation theory and $u$ is just some correlator : $$ u ( r , r' ; a ) = \langle o ( r ) \bar{o} ( r' ) \rangle_{a}$$ where $o$ has the right charge . or it could be something more complicated , the details do not matter . i have explicitly noted this correlator must depend on the gauge field $a$ . this is not surprising since $u$ measures a charge being released at $r$ and destroyed at $r'$ - regardless of the details there must be ahranov-bohm phases . it is this dependence of $u$ on $a$ that maintains gauge invariance , clearer below . there is a sort of minimal coupling prescription for $u$: $$u ( r_1 , r_2 ; a ) = \int \mathcal{dp}\exp ( i\ ! \int_{r_1}^{r_2}\ ! \ ! \ ! \ ! a ( r' ) \cdot dr' ) $$ where the $\mathcal{dp}$ is some measure on the space of paths from $r_1$ to $r_2$ , and this measure does not depend $a$ . this is minimal in the sense of being , well , minimal and in the sense that if you expand $u$ as a polynomial in derivatives you recover the usual minimal coupling prescription . you can see that this has right gauge transformations properties . in the special case where the motion is essentially semiclassical you just a finite sum over wilson lines . okay so that is the way this interaction should be written , now the real question is when can we ignore all this , since we surely do not want to estimate some path integral measure on the atomic scale when we can barely estimate single interaction energies on the atomic scale . now violating gauge invariance leads to horrible things , i need not remind . but clearly there is a sense where if $u ( r , r' ) $ has a small radius of effect and if we probe with a very long wavelength then we should not know that it is not a delta function , and hence we should not know that its violating gauge invariance . this is not really correct : if we follow our " minimal coupling " prescription we realize what we need is not that the distance between $r$ and $r'$ is small , but that the region explored by the bulk of paths in $\mathcal{dp}$ is small . this makes sense , since the issue comes from ahranov bohm phases . it does not help if $r$ and $r'$ are close if we get between them by travelling on circuitous paths that are enormously sensitive to the gauge fields . so the correct criterion is if the magnetic flux threading the region with paths is much smaller than a flux quantum , then we can " straighten out " all the paths in our path integral and just write it as : $$ u ( r_1 , r_2 ; a ) \approx \tilde{u} ( r_1 , r_2 ) \exp ( i\int_{r_1}^{r_2}\ ! \ ! \ ! a ( r' ) dr' ) $$ where the integral is taken over whatever path in the region we choose . note this approximation is gauge invariant , since the error depends on the magnetic field , and its as simple a gauge invariant as thing as we get . now in terms of linear response , at this point one could simply plug in an external field and decide whether or not it was sufficiently small to be ignored . or one could simply make peace with the wilson line and proceed . if you wanted to operate more generally but still wanted to ignore that wilson line you can do it as long as you restrict yourself to a sufficiently smooth gauge . if you are in a gauge where $a$ does not vary appreciably over the range of $u$ then $\int_{r_1}^{r_2}\ ! \ ! \ ! a ( r' ) \cdot dr ' \approx a\cdot ( r_2 - r_1 ) $ . and the point this is approximately pure gauge : it corresponds to the gauge transformation with field $\chi = a ( r ) \cdot r$ . so we may essentially ignore it . to have a smooth gauge one must have slowly varying fields small magnetic field . which are fairly intuitive physical requirements . and then additionally we must not introduce gauge transformations which vary quickly . so one way to think of all this is that we can manipulate such apparently non-gauge invariant expressions because we have actually gauge fixed the high frequency modes of the gauge field . probably could have thought of that without all this work , but such is life .
for what it is worth : http://www.coolmagnetman.com/magmeter.htm - a home-made device based on a hall effect device - for about $40 .
normal matter structure is entirely constructed from the electronic bindings , so it is in the realm of the possible to engineer how the atoms are binded together exactly and this is the aim of lower-level nanotechnology . however , it is with current technogy and physics , impossible to create complex structures at lower scales ( i.e. : nuclear scale ) . and i do not think that is something that is in principle possible unless some dramatic breakthrough occurs in how we obtain larger-scale nuclear matter , which from the experimental limitations that there are to obtain high z nucleus that might probe the regions of higher islands of stability , one can safely infer that this is a very challenging problem in of itself update : this is not entirely related , but it shows an example of how assumptions as the one i have made above about manipulation of small scales being out of engineering reach can be twisted : this slide about non-homogeneous diffraction crystals shows how to do something that most physicists have thought for long to be essentially impossible ; x-ray and gamma-ray optics
i think you will find this article ( with pictures ) helpful for dot convention and this one for mutual inductance . let me know if this does not clear it up .
yes , it is simple to prove using moment generating functions . and yes , the mathematics is very closely related to that of quantum field theory . you compute $g ( j ) = &lt ; exp ( \sum j_i x_i ) &gt ; $ where each $j_i$ is a " source " for the corresponding $x_i$ . this is easily shown to be something like $g ( j ) = exp ( \sum j_i \mu_{ij}^{-1} j_j ) $ to get expectation values you then take $ &lt ; x_i x_j . . . &gt ; = \frac{\partial}{\partial j_i} \frac{\partial}{\partial j_j} . . . g ( j ) |_{j=0}$ . the rest follows simply . in particular , you can see how the variables must be grouped in pairs to get a nonzero results when you set $j=0$ after taking derivatives . you essentially have a feynman expansion of a non-interacting 0-dimensional field theory . this is covered nicely in zee 's " quantum field theory in a nutshell " , where it is a simple application of what he calls the " central identity of quantum field theory "
both formulas are equivalent , if you are in the electrostatic approximation and your dipole vector does not depend on the position $\mathbf{r}$ . let 's consider the expression $\mathbf{f}=\nabla_{\mathbf{r}} ( \mathbf{p} \cdot \mathbf{e} ) $ which can be easily obtained from the potential energy function $u=-\mathbf{p} \cdot \mathbf{e}$ and its relation with the force $\mathbf{f}=\nabla_\mathbf{r} u$ . now , recall the vector identity $\nabla_\mathbf{r} ( \mathbf{a}\cdot \mathbf{b} ) = ( \mathbf{a} \cdot \nabla_\mathbf{r} ) \mathbf{b}+ ( \mathbf{b} \cdot \nabla_\mathbf{r} ) \mathbf{a} + \mathbf{a} \times ( \nabla_\mathbf{r} \times \mathbf{b} ) + \mathbf{b} \times ( \nabla_\mathbf{r} \times \mathbf{a} ) $ for $\mathbf{a}=\mathbf{a} ( \mathbf{r} ) $ and $\mathbf{b}=\mathbf{b} ( \mathbf{r} ) $ two arbitrary vectors . for $\mathbf{p}=\mathbf{a} \neq \mathbf{p} ( \mathbf{r} ) $ [ independent of the position ] and $\mathbf{b}=\mathbf{e} ( \mathbf{r}$ ) we have $\nabla_\mathbf{r} ( \mathbf{p}\cdot \mathbf{e} ) = ( \mathbf{p} \cdot \nabla_\mathbf{r} ) \mathbf{e}+ ( \mathbf{e} \cdot \nabla_\mathbf{r} ) \mathbf{p} + \mathbf{p} \times ( \nabla_\mathbf{r} \times \mathbf{e} ) + \mathbf{e} \times ( \nabla_\mathbf{r} \times \mathbf{p} ) $ as the dipole vector does not depend on the position we can drop the second and the fourth terms . in the electrostatic approximation , faraday 's law reads $\partial_t \mathbf{b}=\mathbf{0}\leftrightarrow \nabla_\mathbf{r} \times \mathbf{e} ( \mathbf{r} ) =\mathbf{0} $ [ this is known as ''carn 's law'' ] so that the electric field is irrotational and the curl vanishes . then we can drop the third term and $\nabla_\mathbf{r} ( \mathbf{p}\cdot \mathbf{e} ) = ( \mathbf{p} \cdot \nabla_\mathbf{r} ) \mathbf{e}$ so that your definitions agree .
well , the von neumann equation holds witin schroedinger picture and it is immediate to prove in quantum physics ( differently from liouville equation which needs to preventively establish the non-trivial liouville theorem for the symplectic measure on the space of phases ) . indeed , as $\rho$ is an incoherent superposition of pure states , one has , $$\rho = \sum_k p_k |\psi_k \rangle \langle \psi_k|\: . $$ hence , $\rho$ evolves in time as effect of the standard schroedinger evolution of pure states of the mixture , $$\rho ( t ) = \sum_k p_k u_t |\psi_k \rangle \langle \psi_k| u^\dagger_t = u_t \rho u^\dagger_t \: , $$ where $u_t = e^{-\frac{it}{\hbar}h}$ is the usual time evolutor . this identity immediately leads to von neumann equation , $$\frac{d}{dt} \rho ( t ) = -\frac{i}{\hbar} [ h , \rho ] \: , \tag{1}$$ where the derivative is computed with respect to the strong operator topology ( or the weak one ) , the same notion of time derivative as that used in schroedinger equation . unfortunately , and in my opinion erroneously , that derivative is very often indicated by $\frac{\partial}{\partial t}$ instead of $\frac{d}{dt}$ . this would make sense if $\rho$ included another time dependence . think of heisenberg picture , when operators already depend on time in schroedinger picture . in that case to distinguish between $\partial/\partial t$ ( referred to some parametric time dependence already present in schroedinger picture ) and $d/dt$ ( referred to the total time dipendence , including the one arising from heisenberg evolution ) makes sense . here , instead , we have only one time dependence . if $\rho ( t ) $ were a time-depending observable in schroedinger picture , passing in heisenberg picture we would obtain $$\frac{d\rho_h}{dt} = \frac{\partial \rho_h}{\partial t} +\frac{i}{\hbar} [ h , \rho_h ] =0\: , \tag{2}$$ i.e. , $$\frac{d\rho_h}{dt}=0\: . $$ actually all that is trivial without computing any derivative , since $$\rho_h ( t ) = u^\dagger_t \rho ( t ) u_t = u^\dagger_t u_t \rho u^\dagger_t u_t= \rho\: . $$ the conserved quantity would be $\rho$ itself , but it does not make much physical sense a priori , because $\rho$ is a state , not an observable and heisenberg evolution is not appropriate for it . ( even if the result is formally correct since states , in heisenberg picture , are constant in time . ) in classical physics the partial derivative $\partial/\partial t$ always makes sense , since $\rho$ is a function of both $t$ and the state in the space of phases . in that formulation ( 2 ) has a precise meaning in terms of a conservation law . it says that , along the integral lines of hamiltonian flow , the density of probability is constant . in other words , the hamiltonian flow is incompressible ( see the pair of final remarks in my answer to this question conserved quantities and total derivatives ? )
when we treat quantum mechanical objects as if they are particles , this is often referred to as a classical treatment . intuitively , this is going to be valid based on a simple argument related to the de broglie wavelength:\begin{equation} \lambda_{db} = \sqrt{\dfrac{2 \pi \hbar^2}{m k_b t}} . \end{equation} most often , when this wavelength is on the order of interatomic ( or inter-'object' ) spacing , then quantum mechanical effects become quite relevant and one must consider the wave-like nature of matter . for wavelengths much smaller than the distance between atoms ( or molecules , elementary particles , etc . . ) quantum effects will be negligible and the classical treatment works just fine . you can notice that $\lambda_{db}$ is a function of both the mass of the object and the temperature , so making either of these larger while the other is constant will decrease the debroglie wavelength . you work in plasma physics so this wavelength will most often be very small due to the high temperatures even for very ' light ' entities such as electrons . as such you need not consider the wave-like properties of the electron to make accurate calculations of certain physical properties of the system . electrons are negatively charged and because of the coulomb repulsion , i would suspect that no matter how much energy they have they will not be a distance apart that is on the order of this wavelength . i study low-temperature condensed matter though most often , so i may be wrong about this spacing . hope this helps give some intuitive picture of when the classical treatment is acceptable without having to refer to empirical evidence .
i think you are misinterpreting the statement that " it does not have any effect " . this statement does not mean that the faddeev-popov methodology " does not work " , as you wrote later . instead , it means that it is completely unnecessary . if you look at the faddeev-popov ghosts ' lagrangian , you will see that for abelian groups , the structure constants $f_{abc}$ vanish and we are left with $$ {\mathcal l}_{\rm ghost} = \partial_\mu \bar c^a \partial^\mu c^a $$ which means that the ghosts are completely decoupled . they do not interact with the gauge fields ( photons ) . you may still use the faddeev-popov machinery and the brst formalism based upon it to identify the physical states as the cohomologies of $q$ , the brst operator . but what this brst machinery tells you is something you may easily describe without any faddeev-popov ghosts , too . it just tells you that the excitations of $\bar c , c$ are unphysical much like the excitations of time-like and longitudinal photons . that is why the brst problem in the case of abelinan gauge groups is " solvable " in such a way that you may simply eliminate the ghosts completely , together with 2 unphysical polarizations of the photon . and that is why qed may be taught without any faddeev-popov ghosts and one may still construct nice feynman rules for any multiloop diagrams . for non-abelian theories , the counting still works – ghosts , antighosts , and two polarizations of gluons etc . are unphysical . however , because there are interactions of ghosts with the gluons in that case , there is no easy way to describe the physical states without the faddeev-popov ghosts .
how galilean transformations which are wrong ( are approximately correct ) give the correct answer for k ? the lorentz prediction and the galilean prediction must agree in the limit that $v \to 0$ ( or in the limit that $c \to \infty$ ) . this is because $v=0$ corresponds to no transformation at all , so they had better both agree there . so if you take the transformation and evaluate it for smaller and smaller $v$ , you will find that $k=1$ still has to be true . why we should assume that there are two electric fields , one in the lab frame and one in the other , but just one magnetic field in both frames ? that is just the galilean transformation of the em field . to see how it relates to the relativistic case , the lorentz transformation of the em field is : $$\mathbf{e}' = \gamma \left ( \mathbf{e} + \mathbf{v} \times \mathbf{b} \right ) - \left ( {\gamma-1} \right ) ( \mathbf{e} \cdot \mathbf{\hat{v}} ) \mathbf{\hat{v}}$$ $$\mathbf{b}' = \gamma \left ( \mathbf{b} - \frac {\mathbf{v} \times \mathbf{e}}{c^2} \right ) - \left ( {\gamma-1} \right ) ( \mathbf{b} \cdot \mathbf{\hat{v}} ) \mathbf{\hat{v}}$$ when you take the limit that $c \to \infty$ , we know that $\gamma \to 1$ , so it just becomes : $$\mathbf{e}' = \mathbf{e} + \mathbf{v} \times \mathbf{b}$$ $$\mathbf{b}' = \mathbf{b}$$
your reasoning contains some errors and unjustified assumptions . the first error is to think that the universe started out with a size of about the plank length . this may not be the case . if it is flat and infinite now then it would always have been flat and infinite , even at the beginning , or at least as far back as the point where it makes sense to talk about space-time in such terms . it is true that the observable universe would have started from a very small point , but the whole universe is likely to be much bigger . even if the universe is curved and finite in size its initial size could have been anything from much smaller than the plank size to much larger . your second error is to think that a flat universe has to be infinite . it is true that a universe with constant positive curvature over space must be finite , but the converse is not true . a flat universe or even one with negative curvature can be finite if it repeats with periodic boundary conditions . for a flat space the simplest topology that can have this property is the 3-torus . you are also making the assumption that the cosmological principle holds on all scales no matter how large . our observations of the observable universe suggest that this principle is quite reasonable on scales up to billions of light years , but we can not say anything for sure about what the universe is like on much larger scales . the curvature of space may vary in quite dramatic ways beyond the horizon that limits how far we can see due to the finite speed of light . all four of your options are possibilities and there are too many in the " other " category to elaborate .
i would expect metals to cool faster through convection because of a related heat property of theirs : conduction . metals are generally good heat conductors so as heat energy is removed from the surface of the metal by air , internal heat energy in the metal can quickly flow to the surface . the rate heat flows between two systems is dependent on their temperature difference . when air removes heat via convection , the surface of the object is cooler than the central portion of the object . if heat in the object flows slowly from the center to the surface like in many plastics then the surface stays closer to air temperature and heat transfers to the surrounding air more slowly . if the object is a metal , the heat removed at the surface is easily replaced by heat flowing from the center to the surface and the temperature delta between the surface and the air is greater so the object cools faster .
as brandon mentioned , two small objects could not orbit each other near a significant gravitational field . the hill sphere " approximates the gravitational sphere of influence of a smaller body in the face of perturbations from a more massive body . " therefore , your pebble 's hill sphere would be too small to permit orbits near earth . the wiki article has a calculation showing that an astronaut could not orbit the 104 tonne space shuttle 300 km above the earth since the shuttle 's hill sphere was only 120 cm .
in the situation you have described in the comment there is a drag force acting on a ball even if water is assumed to be inviscid . however , one should carefully calculate it in order to decide whether it matters at all . the effect responsible for the drag is called added mass . actually i refer you to that wikipedia article to find out the explanation . what matters is whether this drag is strong enough for your case . the force should be $$\boldsymbol f = -\frac{1}{2} \rho_w v \boldsymbol a$$ where $\boldsymbol a$ is the ball 's acceleration , $\rho_w$ is water density and $v$ is the ball 's volume . so if the acceleration is comparable to $\boldsymbol g$ then the drag is comparable to the buoyant force and could not be neglected . since you assume ball 's density to be zero then you have to take the drag into account because otherwise it acceleration would be equal to g .
suppose you had three electrons , with individual wavefunctions $\lvert \psi_1 \rangle$ , $\lvert \psi_2 \rangle$ , and $\lvert \psi_3 \rangle$ . let them all have the same $\vec{x}$ , $l$ , and $m$ , so they can only differ in intrinsic spin . since spin is a two-dimensional hilbert space , as you noted , then three vectors must be linearly dependent . that is , there exist complex numbers $\alpha$ and $\beta$ such that $$ \lvert \psi_3 \rangle = \alpha \lvert \psi_1 \rangle + \beta \lvert \psi_2 \rangle . $$ now the state of all three electrons is given by the tensor product $$ \lvert 1,2,3 \rangle \equiv \lvert \psi_1 \rangle \otimes \lvert \psi_2 \rangle \otimes \lvert \psi_3 \rangle . $$ the tensor product respects the structure of the underlying vector spaces , which is a fancy way of saying we can write $$ \lvert 1,2,3 \rangle = \alpha \lvert 1,2,1 \rangle + \beta \lvert 1,2,2 \rangle . $$ but $\lvert 1,2,1 \rangle$ , as a product state of identical fermions , is antisymmetric under interchange , in particular under interchance of the $\lvert \psi_1 \rangle$ components : $$ \lvert 1,2,1 \rangle = - \lvert 1,2,1 \rangle . $$ thus $\lvert 1,2,1 \rangle \equiv 0$ . similarly , $\lvert 1,2,2 \rangle$ vanishes . we have just shown that our 3-electron wavefunction is a linear combination of $0$-vectors , and so it too vanishes . this argument easily generalizes to more than three electrons by appending the combined wavefunction of all the others to the end of our state and just carrying it through the computations ; that is , just do the same thing splitting up $\lvert \psi_3 \rangle$ , but apply it to $$ \lvert 1,2,3 \rangle \otimes \lvert 4 , \ldots , n \rangle . $$
i would guess that the professor is explaining his/the ( ? ) theory that dark matter is neutrinos , produced via a scattering process he calls " witten 's dog " . it is funny because the neutrinos are coming out of the dog 's butt . in the standard humor classification , this is known as a " poop joke " .
i did a google search and found that : when a glass rod is rubbed with a paper towel , the glass becomes positively charged . therefore : 1 . ) ground the electroscope through touching it with you fingers ( removing any excess charge ) ; one can assume it is neutral/has no net charged at this point . 2 . ) touch one of the charged rods to the electroscope . the leaves should rise with an unknown charge . 3 . ) vigorously rub the uncharged glass rod with a paper towel to give it a positive charge . 4 . ) bring the positively charged rod close to the electroscope . if the leaves are repulsed , then you know that the glass rod of unknown charge was/is of the same charge , and therefore is positively charged . opposite charges attract . therfore if the leaves attract one knows that the glass rod of unknown charge was negatively charged . 5 . ) [ check ] repeat steps 1-4 to verify that that last rod is of opposite charge .
first normalize the state to find $a$ . then you need to express the state as a superposition of the stationary states of the infinite square well : $$ \psi\left ( x\right ) = a x \left ( a-x\right ) = \sum_{n=1}^\infty c_n \psi_n\left ( x\right ) , $$ where $\psi_n\left ( x\right ) = \sqrt{2/a} \sin\left ( n \pi x / a\right ) $ is the $n$-th stationary state . you can do this using the orthogonality of the stationary states , $$ \int_0^a dx \ \psi^*_m\left ( x\right ) \psi_n\left ( x\right ) = \frac{2}{a} \int_0^a dx \ \sin\left ( \frac{m \pi x}{ a}\right ) \sin\left ( \frac{n \pi x}{ a}\right ) = \delta_{mn} , $$ by integrating the equation above : $$ \begin{align} \int_0^a dx \ \psi^*_m\left ( x\right ) \left [ a x \left ( a-x\right ) \right ] and = \int_0^a dx \ \psi^*_m\left ( x\right ) \left [ \sum_{n=1}^\infty c_n \psi_n\left ( x\right ) \right ] \\ and = \sum_{n=1}^\infty c_n \left [ \int_0^a dx \ \psi^*_m\left ( x\right ) \psi_n\left ( x\right ) \right ] \\ and = \sum_{n=1}^\infty c_n \delta_{m n} \\ and = c_m \end{align} $$ i will leave the $c_n = a \sqrt{2/a} \int_0^a dx \ \sin\left ( n \pi x / a\right ) x \left ( a-x\right ) $ integral for you to work out . once you have the $c_n$ 's , the most likely value of a measurement of the energy is the energy corresponding to the stationary state with maximum $c_n$ . to find the probability of measuring $9 \hbar^2 \pi^2 / 2 m a^2$ for the energy , determine the stationary state that this energy corresponds to , and compute $\left|c_n\right|^2$ . for the time evolution , since the potential is $0$ everywhere after $t=0$ , it is a free particle , and the general solution is : $$ \psi\left ( x , t\right ) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty dk \ \phi\left ( k\right ) \exp\left [ i\left ( k x + \frac{\hbar k^2}{2 m} t\right ) \right ] , $$ where $$ \phi\left ( k\right ) = \frac{1}{\sqrt{2 \pi}} \int_0^a dx \ \psi\left ( x , 0\right ) \exp\left ( -i k x\right ) = \frac{a}{\sqrt{2 \pi}} \int_0^a dx \ x\left ( a-x\right ) \exp\left ( -i k x\right ) . $$ so , now you just have to do this integral .
the sign of the gauge part of the covariant derivative is a convention , you can choose it any way you want , it just defines the sign of a . this sign has nothing to do with the metric convention , mostly + or mostly - . its arbitrary in either convention . the second part is just differentiating both sides of the previous equation for $d_0\phi$ , there is a t in the right hand side . so it is $\partial_0 ( t d_0 \phi ) $ , since a_0 is infinitesimal and gives a higher order correction , and he keeps the first part of this , where you differentiate t with respect to t , and ignores the second part , since time derivatives of $\phi$ are small by assumption that the monopole is stationary at t=0 and slowly accelerating .
the blades of a ceiling fan are pitched out of plane slightly . as a result , when the fan spins , the blades push air either up towards the ceiling or down towards the floor . which direction it pushes air is determined by the direction the fan is spinning , and the direction the blades are pitched . the usual convention is given by the right hand rule : if you hold your right hand so that you can curl your fingers in the direction the fan is spinning , then it will push air in the direction that your thumb is pointing . 1 when it is pushing air down on you , it will then be spinning in a counter clockwise direction as you look up at it . you can make it follow a left hand rule by reversing the pitch of the blades . once you have set the direction of the pitch of the blades , you can reverse the airflow by reversing the direction the fan spins . many ( most ? ) modern ceiling fans provide some mechanism to do this . the fans in my house have a small black switch that slides up and down . @ignacio vazquez-abreams mentions using a pull chain in a comment to another answer . in warm weather , you set the fan so that it pushes air down towards the floor . this causes you to feel a breeze , which cools you . in cold weather , you set the fan so that it pushes air up into the ceiling . you do not feel a breeze , 2 but it circulates warm air from the ceiling towards the walls and down towards the floor . box fans usually follow the right hand rule as well . you may find it easier to check some of this if you have a box fan handy . you can walk around the fan and hold a tissue in front of it from both side while watching the blades spin . you can check this with a box fan . set the box fan on the floor , and turn it on . it will pull air from one side and push it out the other . you feel a breeze on the " out " side and feel a much weaker breeze ( if any ) on the " in " side .
$\newcommand{\er}{\hat e_r} \newcommand{\et}{\hat e_\tau} \newcommand{\d}{\dot} \newcommand{\m}{\frac{1}{2}m} $ in radial coordinates , $\d\er=\d\theta \et$ , and ( useless here ) $\d\et= -\d r \er$ . $\er , \et$ are unit vectors in radial and tangential directions respectively . due to this mixing of unit vectors ( they move along with the particle ) , things get a little more complicated than plain ' ol cartesian system , where the unit vectors are constant . for your particle , writing $x+l\to r$ , the position vector is : $$\vec p= r\er$$ $$\therefore \vec v=\d{\vec p}= \d r\er + r\d\er=\d r \er + r\d\theta\et$$ $$\therefore v^2= \vec v\cdot\vec v= \d r^2+r^2\d\theta^2$$ substituting back the value of $r=x+l , \d r=\d x$ ( and mutiplying by $\m$ , we get the above expression ? as you can see in my expression for $\vec v$ , i had two components of velocity--radial and tangential . since they are perpendicular , i can just square and add , akin to $t=\m\left ( \d x^2 +\d y^2\right ) $ . the point is , it may be a scalar , but it contains a vector in its expression:$$t=\m v^2=\m|v|^2=\m \vec v\cdot \vec v=\m ( \dot x^2+\dot y^2 ) $$
classically a non-pointlike spinning charged object possesses a magnetic dipole moment due to the fact that charged particles in the object are spinning around some axis . in contrast , the electron has a dipole moment that arises from its intrinsic spin angular momentum . as you point out , the electron has no internal structure , so the spin does not refer to actual physical spinning . the dipole moment has spatial dimensions outside of the point where the electron exists because it arises from the quantum spin property of the electron , it is not itself a property of the electron . the magnetic dipole moment of the electron is related to its spin in the way described here .
the idea is just to make use of the relationship between luminosity ( the amount of energy emitted per second from the star - in other words , the power ) and flux ( the amount of power hitting the surface ) . because , the flux can be modeled as a large number of ( imaginary ) spherical energetic wavefronts emerging from the star in 3-d space as a function of time . also , this flux is based on the inverse-square law . so , it decreases with distance ( squared ) . and since the power distributed over every sphere is still the same , both are related by the area of spheres . $$\mathrm{flux=\frac{luminosity}{4\pi r^2}}$$ and since the question says that the astronomer idealizes it as a blackbody ( which we always do ) , we can use the stefan-boltzmann equation and say that the flux is $\sigma t^4$ . . . edit for confusion : simply relating the given flux with stefan-boltzmann only gives the temperature of the imaginary sphere at the distance at which the flux was measured . first , you can find the luminosity of the star by using the relation above . then , you need the flux at the surface of the star . given the radius of star , it can be used to determine how much power is transmitted to the surface through the luminosity . finally on relating with $\sigma t^4$ gives the answer . . .
i believe your mistake is with units , and it is the following : $$t=\dfrac{m [ c_{rms} ] ^2}{3r} = \dfrac{\left ( 1 \text{amu} \right ) [ 11.2 \frac{km}{s} ] ^2}{3 \left ( 8.3144621 \frac{\mathrm{j}}{\mathrm{\text{mol} k}} \right ) } $$ this does not even cancel out because you are left with a $\text{mol}$ unit . add avogadro 's number . $$t = \dfrac{\left ( 1 \text{amu} \right ) [ 11.2 \frac{km}{s} ] ^2}{3 \left ( 8.3144621 \frac{\mathrm{j}}{\mathrm{\text{mol} k}} \right ) } \left ( 6.022 \times 10^{23} \frac{1}{\text{mol}} \right ) = 5,028 k $$ this is the case for earth . for the moon : $$t = \dfrac{\left ( 1 \text{amu} \right ) [ 2.4 \frac{km}{s} ] ^2}{3 \left ( 8.3144621 \frac{\mathrm{j}}{\mathrm{\text{mol} k}} \right ) } \left ( 6.022 \times 10^{23} \frac{1}{\text{mol}} \right ) = 230 k $$ this is negative in celsius units . this is not a problem . it is merely saying that even freezing temperatures are enough for a lone hydrogen atom to escape the gravity of the moon with . all we required was that this number be less than the temperature of the sun , which it is . any surface that faces away from the sun will again see those photons within a month , unless it is in a crater on one of the poles , which we know can have ice near the surface . so that makes sense .
in my experience this is entirely possible in a ' warmer ' climate ie southern britain . i can give you advice on that basis , maybe someone else will advise on dealing with the cold ! can i suggest something like the robodome , this is an automated dome to house the scope ( upto 10" aperture is suitable but tight . ) which can be synced to a weather station and the telescope controls . operating can be done fairly simply via r232 and or usb cables , the usb will need to be powered to deal with the distance . remote operation is no different really from a wired connection while stood next to the scope . but ! remeber that a scope drive will have enough power to crush fingers , mains electricity is enough to kill and if something goes wrong you could a lot of equipment at the mercy of the weather . so some kind of safety plan is needed . . . minimum list : dome , scope , ccd , webcam , cabling , weather station , motorised mount . [ to track add a tracking camera/on ccd chip . ] setting up your own observatory is a lot of fun though , sitting in the warm is definitely the way to go !
the electric field is the vector sum $$ {\vec e}~=~\frac{1}{4\pi\epsilon_0}\big ( \frac{q_1{\bf n}_1}{2d^2}~+~\frac{q_2{\bf n}_2}{d^2}\big ) $$ so the components of the field are $$ e_x~=~\frac{1}{4\pi\epsilon_0}\frac{q_1}{2\sqrt{2}d^2} $$ $$ e_y~=~\frac{1}{4\pi\epsilon_0}\frac{q_1}{d^2}\frac{1~+~2\sqrt{2}}{2\sqrt{2}} $$ the rest is plug and grind on numbers .
can vectors in physics be represented by complex numbers ? absolutely . there exists a direct isomorphism between the 2d euclidean vector space and the argand plane , for a start . in fact , it is possible to talk of mathematical objects called quaternions and use quaternion algebra analogously to vector algebra . historically quaternions were used to represent geometrical operations and transformations in 3d space - in the days before vector algebra . ( they still are used , especially in areas such as computer graphics where they offer one or two advantages over the simpler world of vectors . ) in any case , the relationship to vector algebra is a very close one . can vectors in physics be divided ? in general , no , vector-vector division is not a well-defined operation . at least , not within the bounds of linear algebra . i.e. there exist none or multiple solutions to the equation $\vec{y} = \mathbb{a} \vec{x}$ . ( see the wolfram page . ) saying this , the concept of vector division has an interesting relationship with your first question . if we map vectors to complex numbers ( or quaternions in > two dimensions ) , we can use complex division or quaternion algebra respectively to define an analogous " vector division " operation . note that quaternions can in fact be extended to higher dimensions , which allows for interesting possibilities . this study of this falls under the area of clifford algebras . ( note : vector-scalar division is of course well-defined , as is point-wise division of equal-length vectors , but i presume you are not referring to that . )
acceleration is the derivative of velocity with respect to time , which is not the same as just dividing the difference in velocity by the difference in time . $$a=\frac{dv}{dt}\neq \frac{\delta v}{\delta t}$$ $dv$ and $dt$ can be thought of as infinitely small $\delta v$ and $\delta t$ . so if your $\delta v$ and $\delta t$ are quite small , the result of your calculation will be pretty close to the actual acceleration . but in your case , $\delta v$ and $\delta t$ are pretty big . therefore , your calculation is way of . we can see that our result becomes more accurate if we consider an eighth of a circle : $$\frac{\delta v}{\delta t}=\frac{ \sqrt{ ( v_{x0}-v_{x1} ) ^2 + ( v_{y0}-v_{y1} ) ^2} }{5/2}$$ now we plug in $v_{x0}=20$ , $v_{y0}=0$ and $v_{x1}=v_{y1}=20\cdot\sqrt{\frac{1}{2}}$ . $$\frac{\delta v}{\delta t}=2\frac{ \sqrt{20^2 ( 1-\sqrt{\frac{1}{2}} ) ^2 + 20^2 ( 0-\sqrt{\frac{1}{2}} ) ^2} }{5}=40\frac{ \sqrt{1-2\sqrt{\frac{1}{2}} + \frac{1}{2} + \frac{1}{2}} }{5}=8\sqrt{2-\sqrt{2}} \approx a$$ and we use $$a=\frac{v^2}{r} \rightarrow r =\frac{v^2}{a}$$ $$c=2\pi r = 2\pi \frac{v^2}{a} \approx 2\pi \frac{20^2}{8\sqrt{2-\sqrt{2}}} \approx 410.46\space m$$ as you can see , this is much closer to the real value of the circumference . this is because we chose a smaller part of the circle , and because of that a smaller $\delta v$ and $\delta t$ . if you pick smaller and smaller parts of the circle , your value will get closer and closer to the real value . and in the limit of choosing a part of the circle with length 0 , it will be the exact value . this is exactly what the '$d$' in $\frac{dv}{dt}$ means . you can also try choosing a larger part of the circle , and you ranswer will come out a lot worse . in the worst case , if you use the entire circle , you will find that $\delta v=0$ , and therefore $a=0$ , and conclude that the circle has an infinite circumference .
i assume you have no qualms with the " large $\xi$" approximation - it is fairly obvious that $\xi^2-k^2\approx \xi^2$ for large enough $\xi$ . after that you are left with the differential equation $$\frac{d^2\psi}{d\xi^2}\approx-\xi^2\psi . \tag1$$ one way to solve this equation is by the method of divine inspiration : you somehow come up with two linearly independent functions you can write down which solve the equation , after which you know what the general solution is . however , the two functions that griffiths poses are not exact solutions of that equation , as you would know if you had done your proper diligences . indeed , $$ \frac{d^2}{d\xi^2}\left [ e^{\pm \xi^2/2}\right ] =\frac{d}{d\xi}\left [ \pm\xi e^{\pm \xi^2/2}\right ] = ( \xi^2\pm1 ) e^{\pm\xi^2/2} . $$ this means that we are looking for a solution that is approximately valid for the ( already approximate ) equation ( 1 ) . as for how one might get such solutions , there is obviously a myriad different possible paths . one really nice way of deriving the solutions is to factor the offending second-order differential operator into two different first-order operators : $$ \left ( \frac{d}{d\xi}-\xi\right ) \left ( \frac{d}{d\xi}+\xi\right ) \approx \left ( \frac{d}{d\xi}+\xi\right ) \left ( \frac{d}{d\xi}-\xi\right ) \approx \frac{d^2}{d\xi^2}-\xi^2 . $$ these are of course approximate inequalities , and of course you must work these out to see what terms got dropped and why . after that , you can simply work out solutions to the two equations $\left ( \frac{d}{d\xi}-\xi\right ) \psi=0$ and $\left ( \frac{d}{d\xi}+\xi\right ) \psi=0$ , which are in fact the solutions given by griffiths . these are first-order equations and therefore simply solvable by integrating . the ( approximate ) equalities above guarantee that a function in the kernel of either factor will be ( approximately ) in the kernel of the operator you do care about . of course , these factors are far from trivial inventions , and they are at the heart of the operator approach to solving the simple harmonic oscillator .
when alpha particles ( helium nuclei ) or beta particles ( electrons ) are released in a radioactive decay they carry significant kinetic energy . as they go through the surrounding material they bump here and there occasionally kicking off some electrons from the surrounding atoms which are then ionised , until they finally stop .
wall crossing is any discontinuous change of an integer ( or at least rational ) quantity - or , more generally , any qualitative change of the spectrum etc . - that occurs when one moves to the opposite side from a " wall " in a moduli space or parameter space . a would-be index may suddenly change discontinuously . the wall - a codimension one locus in the parameter space - is then referred to as the " wall of marginal stability " . one one side , an object may be stable while it is unstable on the other side . it is typically unstable because a decay suddenly becomes plausible because the hypothetical decay product get light enough , if you wish . the objects are typically bps objects on the stable side and they can also be bps black holes or any other bps objects . to see objects that may be exactly stable , bps , but that are also sufficiently diverse , $n=2$ supersymmetry or eight supercharges is an ideal number of supercharges . that is why those considerations played an important role in the seiberg-witten insights about $n=2$ gauge theories , among related situations . paul aspinwall liked to say that 8 supercharges is the optimum number for nice physics . only nature did not manage to choose the number 8 for supercharges , but that is not aspinwall 's fault .
the reduced matrix is defined as the partial trace of the density matrix . be $a$ , $b$ finite dimensional hilbert spaces and be $t$ $\in$ $l ( a \otimes b ) $ ( linear operators on $a \otimes b$ ) , then the partial trace of t is defined as $\rm{tr}_b [ t ] $ in $l ( a ) $ is defined by \begin{equation} \langle a | \rm{tr}_b [ t ] | b \rangle = \sum_n \langle a | \langle n | t| n\rangle | b \rangle \end{equation} where $| n \rangle$ is an orthonormal basis in $b$ . finally , note that the reduced matrix is not the correct way of the describing a quantum state , is just a way to describe it as seen by looking just at a subsistem . this usually involves ignoring part of the information of the state and therefore the reduced density matrix of a pure state may be a mixed state . this is spectacular for the bell states , as their reduced matrix is $\rm{id}/2$ , the most disoredered state .
the question is essentially just asking you to perform clebsch-gordan ( cg ) decomposition , namely a change of basis on the hilbert space $\mathcal h_{j_1}\otimes\mathcal h_{j_2}$ of the composite system of spins . recall that for each of the hilbert spaces $\mathcal h_{j_1}$ and $\mathcal h_{j_1}$ there exist orthonormal bases of eigenvectors of $\mathbf j_1^2 , j_1^z$ and $\mathbf j_2^2 , j_2^z$ respectively , and these bases are \begin{align} \text{for $\mathcal h_{j_1}$} and :\qquad \{|j_1 , m_1\rangle\ , |\ , m_1=-j_1 , -j_1+1 , \dots , j_1-1 , j_1\} \\ \text{for $\mathcal h_{j_2}$} and :\qquad \{|j_2 , m_2\rangle\ , |\ , m_2=-j_2 , -j_2+1 , \dots , j_2-1 , j_2\} \end{align} it follows that an orthonormal basis of eigenvectors for the composite hilbert space $\mathcal h_{j_1}\otimes\mathcal h_{j_2}$ consists of the set of all tensor products of these basis elements ; \begin{align} \text{for $\mathcal h_{j_1}\otimes\mathcal h_{j_2}$}:\qquad \{|j_1 , m_1\rangle|j_2 , m_2\rangle\ , |\ , -j_1\leq m_1\leq j_1 , -j_2\leq m_2\leq j_2\} \tag{$\star$} \end{align} this is what being referred to as " . . . products of eigenstates of the z components of individual spins . " so once you have found the eigenstates of the hamiltonian , which you will presumably write in terms of the basis \begin{align} \{|j , m , j_1 , j_2\rangle\} , \end{align} you just need to decompose them into the tensor product basis in $ ( \star ) $ above .
here is a table i made for you listing the elements with a density higher than 10 g/cm$^3$ and their approximate price per kg : i could not find any prices for einsteinium or actinium and some of the other prices might come from poor sources , but take it as a rough guide . now you only have to figure out how much you need and your budgetetary constraints , and choose the densest you can afford . as i have learned from the political debate in the us , teachers are apparently raking in big cash , so i suggest you go with osmium or rhenium . note : some of these might be unsuitable/infeasible for other reasons than their price .
when you mention solar power , it makes me think you are thinking about photo-voltaic power or power extracted from solar panels . the power put out by the sun is about $3.95*10^{26}w$ per second . but solar panels can only capture a fraction of that energy . even so , in 2008 humans used about $4*10^{13}w$ per second which is many orders of magnitude less than the sun puts out . the suns life cycle will last billions of years fusing hydrogen , then helium and slowly working its way down to a white dwarf star . as the temperature and radius changes of the sun , it is power output will fluctuate . but it will always put out more power than humans could use until the earth is heated greatly during the red giant phase . considering homo sapiens have only been around 200,000 years and the sun will not expand for another ~7,000,000,000 years that is approximately infinite in terms of human life spans which are about 80 years . what will exactly happen during this red giant phase is still under debate , but it is a long long time from now .
if alice observes that bob travels 1000 lightyears in 1000 years , then bob 's speed is c according to alice and thus , is c in all frames of reference which means that bob does not have a frame of reference at all . let 's try adjusting the numbers a bit . let alice observe that bob , moving with a constant speed , travels 999 lightyears in 1000 years . according to alice , bob has a speed of 0.999 c . bob knows when he has travelled 999 lightyears according to alice because alice has helpfully put up a marker for each lightyear ( according to her ) . how much time elapses on bob 's wristwatch between the event that the 0 lightyear marker flashes by and the event that the 999 lightyear marker flashes by ? according to sr , the elapsed time ( in years ) according to bob is $$\tau = 999\sqrt{1 - ( 0.999 ) ^2} = 44.7$$ but remember , according to bob , it is alice and the lightyear markers that are moving . and , according to bob , the distance ( in lightyears ) between alice 's lightyear markers is $$d = 1\sqrt{1 - ( 0.999 ) ^2} = . 0447 $$ the point is this : we must be careful in our thinking about distance travelled and elapsed time since these quantities are reference frame dependent . now , if we introduce acceleration into the thought experiment , alice and bob are no longer are equivalent . imagine that alice and bob are both on earth , bob instantly accelerates to 0.999 c relative to and away from earth , travels for 44.7 years according to his wristwatch , and then instantly decelerates to zero speed relative to earth . bob and alice will both agree that he is now 999 lightyears from earth , that bob has aged 44.7 years and alice has aged 1000 years .
the number of physical degrees of freedom ( dof ) or dynamical variables is simply the number of generalized positions whose evolution is given by a second order in time differential equation . using the op 's notation , the number of dof is $${1\over 2} ( n-2m-s ) $$ for instance , in electrodynamics the phase-space is six-dimensional $\{a_i , f_{0i}\}_{i=1}^3$ and the gauss law is a first class constraint . thus $n=6 , \ , m=1 , s=0$ . so that there is two dof corresponding to the two polarizations of electromagnetic waves or the two photon 's helicities . one can take an alternative and equivalent point of view in which the phase-space consists of $\{a_{\mu} , f_{0\mu}\}_{\mu=0}^3$ and besides the gauss law one has the first class constraint $f_{00}\approx0$ ( the symbol $\approx$ is read " weakly zero " and means zero when the constraints are verified , you may perfectly write $=$ ) which poisson commutes with the gauss law and both are therefore first class constraints . then $n=8 , \ , m=2 , s=0$ and the number of dof is still two , of course . in the case of the gravitational field , the counting of dof is analogous . the phase-space consists of $\{h_{ab} , p_{ab}\}_{a=1 , b=1}^{a=3 , b=3}$ , with $h_{ab}$ the components of the spatial metric and $p_{ab}$ their conjugated momenta . the four $ ( 0 , \mu ) $ einstein equations are not dynamical equations —since they do not contain second order temporal derivatives— but first class constraints . hence $n=12 , \ , m=4 , \ , s=0$ so that the number of dof is two corresponding to the two polarizations of gravitational waves . however , consider the case of the procca field ( a vectorial field of mass $m$ ) . now the phase-space consists of $\{a_{\mu} , f_{0\mu}\}_{\mu=0}^3$ and there are two constraints $\partial_i\ , f_{0i}=m^2a_0$ —i am considering a theory with no matter fields besides the vectorial field , if one added other fields , then there would be a density of charge $\rho$ in the right hand side— which reduces to the gauss law when $m=0$ and $f_{00}=0$ like in the electromagnetic case . however , now due to the mass term , the two constraints do not poisson commute , thus the constraints are second class . hence $n=8 , \ , m=0 , s=2$ and the number of degrees of freedom is three corresponding to the three helicities of a massive vectorial particle .
the reason is that the exposure on the camera is set so that the main subject of the image is properly exposed , ie not too dim and not too bright . because the typical objects being photographed are quite bright , the image detector ( camera ) will not get enough light from the stars for them to show up .
when you have two operators $\hat{a} , \hat{b}$ satisfying equation $ [ \hat{a} , \hat{b} ] = \imath \hat{c}$ , you can prove with schwarz inequality that $\sigma_{\hat{a} , \psi} \sigma_{\hat{b} , \psi} \geq \frac{1}{2} | \hat{c} |$ . unless there would be stronger inequality that can be used in calculations , it gives us the lower bound of uncertainty principle . there is no upper bound , but if you want , you can get bigger uncertainty by lowering the trueness or precision of measurement .
it seems right . you have $$ \frac{\partial w^{1/2}}{\partial x^\sigma}=\frac1{2\sqrt{w}}\frac{\partial w}{\partial x^\sigma} $$ change the order of the derivatives in the second term $$ \frac{d}{d\lambda}\frac{\partial w^{1/2}}{\partial \dot{x}^\sigma}=\frac{\partial}{\partial \dot{x}^\sigma}\left ( \frac{d}{d\lambda} w^{1/2}\right ) =\frac{\partial}{\partial \dot{x}^\sigma}\left ( \frac1{2\sqrt{w}}\frac{d w}{d\lambda } \right ) $$ product rule $$ \frac{\partial}{\partial \dot{x}^\sigma}\left ( \frac1{2\sqrt{w}}\frac{d w}{d\lambda } \right ) =-\frac1{4w\sqrt{w}}\frac{\partial w}{\partial \dot{x}^\sigma}\frac{dw}{d\lambda}+\frac1{2\sqrt{w}}\frac{\partial }{\partial \dot{x}^\sigma}\frac{dw}{d\lambda} $$ subtract , equate to zero , multiply $2\sqrt{w}$ you get $$ \frac{\partial w}{\partial x^\sigma}+\frac1{2w}\frac{\partial w}{\partial \dot{x}^\sigma}\frac{d w}{d\lambda}-\frac{d}{d\lambda}\frac{\partial w}{\partial \dot{x}^\sigma}=0 $$
you are right that the result you see is due to the chain rule . the author uses either spherical or cylindrical coordinates , so \begin{equation} r = \sqrt{x^2 + y^2 + z^2} \end{equation} or \begin{equation} r = \sqrt{x^2 + y^2} \end{equation} which you can differentiate to obtain \begin{equation} \frac{\partial{r}}{\partial{x}} = \frac{x}{r} \end{equation} hence \begin{equation} f_x ( r ) = -\frac{\partial{u ( r ) }}{\partial{x}} = -\frac{\partial{u ( r ) }}{\partial{r}}\frac{\partial{r}}{\partial{x}} = -\frac{x}{r} \frac{\partial{u ( r ) }}{\partial{r}} \end{equation}
if you consider a homogeneous piece of silicon the total flow of electrons through it is : $$ i = \frac{u}{r} = n \mu \frac{s}{d} u $$ where $r$ - the resistance of the piece , $u$ - external voltage applied to it . the resistance depends on : $n$ - the concentration of electrons ( number of electrons per m$^3$ ) , $\mu$ - the mobility of electrons ( ratio of velocity of the electron and electric field that makes it move ) , $s$ and $d$ - cross-section and length of the sample . the changes of geometric size with temperature are negligible . so the values that affect the resistance are concentration $n$ and mobility $\mu$ . the mobility depends on the temperature and also on the concentration of various defects in the sample . at 100 f the temperature dependence is dominating . the concentration is the most complicated point . there are the following cases : pure silicon . all the electrons ( and the same amount of holes ) are thermally generated . their concentration depends on the temperature exponentially . if you need total current do not forget about the holes . silicon doped with donors . the amount of thermally generated electrons is negligible . the concentration does not depend on the temperature . semiconductor device with p-n junction or/and heterojunction ( connection of different materials ) . the laser/led of optical computer mouse is this case . the sample is not homogeneous and the concentration is determined mainly not by temperature but by more interesting things like voltage polarity . this case requires more formulas and exact data concerning the sample structure . ! the laser is made of gaas and similar materials not silicon . the attempts to make silicon laser never stop though . edited ( 2011/12/15 ) : for the temperature dependence of electron mobility wikipedia gives $$ \mu ( t ) \approx \mu_0 t^{-2.4} $$ where $\mu_0 = 9.46 \cdot 10^{6} \text{m}/\text{ ( v s ) }$ , hope i have calculated it correctly from first point set ( black circles ) here this formula takes into account only electron scattering on the oscillations of the ions of the crystal . this effect is dominating at room temperature and higher . the temperature must be in kelvin degrees here . for the electron $n$ and hole $n_h$ concentration in pure silicon ( case 1 . ) at room temperature and higher one can use the following formula : $$ n = n_h = n_\text{eff} \ ; t^{\ ; 3/2} \exp \left ( -\frac{e_g}{2k_b t} \right ) $$ where $n_\text{eff}$ - some constant describing the shape of conduction and valence bands of silicon ( i have not found explicit value yet ) , $t$ - temperature in kelvin degrees , $e_g = 1.12\ ; \text{ev} = 1.79 \cdot 10^{-19} j$ - energy gap of silicon , $k_b$ - boltzmann constant .
t2k is running right now . they might ( probably ) need to improve their understanding of the distance and timing . lbne is still in the planning stages , but will have a longer baseline which could be very helpful disclaimer : i am vaguely involved in lbne--specifically doing mc work for the near detector design .
first rename your variables to something other than x , y , and z , because you are going to need x and y as coordinates . let v be the aircraft 's velocity vector relative to the air . it has a direction and length . let w be the wind vector relative to the ground . it has a direction and length . add vectors v+w , and you get g , the velocity vector of the aircraft with respect to the ground .
take a look at this article . the authors present an elementary explanation for the two-to-one ratio of wobble to spin frequencies . update : in case you do not have easy access to the ajp , here it is : article .
first of all , in lower dimensions ( 2+1 and 1+1 ) the gravity is much simpler . this is because in 3d curvature tensor is completely defined by ricci tensor ( and metric at a given point ) while in 2d curvature tensor is completely defined by scalar curvature . this means that there are no purely gravitational dynamical degrees of freedom , in particular no gravitational waves . general note : horizon ( which is the defining feature of black hole ) representing our inability to obtain information about events under it would always imply the entropy of corresponding solution . so , in all of black hole models there is some black hole thermodynamics . for hawking radiation one needs to include quantum effects into consideration and also radiative degrees of freedom ( if there are no gravitons or photons or any other '-ons ' than nothing would radiate ) . let us start with case of 3d ( that is 2+1 ) . the einstein equations in 2+1 spacetime without any matter fields would simply imply that spacetime is flat , that is ' constructed ' from pieces of minkowski spacetime . it may have nontrivial topology , so 2+1 gravity is a topological theory , but no black hole solutions exist . this model ( in mathematical sense ) is exactly solvable . to introduce non-trivial 2+1 solutions we can add matter or cosmological constant ( which could be considered the simplest form of matter ) . it turns out that the spacetimes with negative cosmological constant ( which would locally be composed of pieces of anti-de-sitter spacetimes ) do admit the black hole solution : btz black hole ( name after authors of original paper ) . this solution shares many of the characteristics of the kerr black hole : it has mass and angular momentum , it has an event horizon , an inner horizon , and an ergosphere ; it occurs as an endpoint of gravitational collapse ( for that , of course , we need to include matter beyond cosmological constant in the consideration ) ; and it has a nonvanishing hawking temperature and interesting thermodynamic properties ( see , for instance , paper by s . carlip ) . the hawking temperature of btz black hole $t\sim m^{1/2}$ which , in contrast to the ( 3+1 ) -dimensional case , goes to zero as $m$ decreases . additionally , the simplicity of the model allows quantum treatment of it including statistical computation of the entropy ( see references in paper by e . witten ) . there are many other variations of solutions in 2+1 gravity theories ( for instance by including dilaton and em fields , scalar fields etc . ) but all of them require negative cosmological constant . this is because dominant energy condition forbids the existence of a black holes in 2+1 dimensions ( see here ) . now to 1+1 dimensions . locally all gr models in 1+1d are flat . so to include nontrivial spacetime geometry we need to modify gravity . this can be done by including dilaton field . the resulting models often admit nontrivial geometries with black holes ( see paper by brown , hennaux , teitelboim , wiki page on cghs model , paper by witten on bh in gauged wzw model , and this review ) . these black hole solutions also admit nontrivial thermodynamics and hawking radiation . in particular the hawking temperature is proportional to mass , so as the black hole evaporates it becomes colder ( unlike 4d case where $t \sim m^{-1}$ ) . now to higher dimensional gravity . gravity itself is much richer than in lower dimensional cases , so analogues of all 4d black holes also exist in higher dimensions , as well as some new black hole-like solutions such as black strings and black p-branes . there are also multi-black hole configurations where multiple black holes are placed along the ring or line such that the total force on each of them is zero , resulting in equilibrium configuration . since many uniqueness theorems for black holes only work for 3+1 dimensions there are even solutions with nontrivial horizon topologies such as black rings . i suggest to look at the living review recommended by ben crowell or to this lectures by n . obers . the simplest black hole would be schwarzschild–tangherlini solution ( analogue of schwarzschild black hole ) which is vacuum solution to einstein field equations : here $\mu = r_s^{d-3} = \frac{16 \pi g m}{ ( d-2 ) \omega_{d-2}}$ is mass parameter . this gives us the relationship between mass and schwarzschild radius : $r_s \sim m^{1/ ( d-3 ) }$ . the entropy is given by bekenstein-hawking formula : $$ s = \frac {\cal a}{4g}=\frac 14 \left ( \frac{\omega_{d-2} r_s^{d-2}}{ g} \right ) . $$ temperature could be found from the first law $ ds = d m / t $: $$t = \frac{d-3}{4 \pi r_s} . $$ rotating solution ( generalization of kerr metric ) would be myers-perry metrics . note , that rotations in higher dimensions are more complex , so the angular momentum is represented by several parameters . also note , that many solutions with horizons elongated in one direction ( such as black strings or black rings ) turn out to be unstable via the gregory-laflamme instability , where the smooth ' tubular ' horizon evolve growing perturbations of certain wavelengths . so possibly black strings and black rings would tend to decay into droplets-like black hole along them ( the exact mechanics is yet unknown ) . but of course , the second law of thermodynamics would be observed , meaning that total area of the horizons would increase .
firstly , i do not think your convention for the derivative is consistent with how you lower the index on $\lambda^b$ . let us start with $$ \frac{\partial}{\partial \lambda^b} \lambda^a = \delta_b^a $$ if you lower the index on $\lambda^b$ you get $$ \lambda_a = \epsilon_{ac} \lambda^c $$ putting this together gives $$ \frac{\partial}{\partial \lambda^b} \lambda_a = \epsilon_{ac} \frac{\partial}{\partial \lambda^b} \lambda^c = \epsilon_{ab} = -\epsilon_{ba} $$ which has a different sign from what you have . furthermore , the spinor bracket you write is lorentz invariant if you transform both spinors at the same time . the actions of $j^a$ and $j^b$ on $\langle ab \rangle = \epsilon_{cd} a^c b^d$ is given by \begin{align} j_{ab}^a\ , \epsilon_{cd} a^c b^d and = \left ( a_a \frac{\partial}{\partial a^b} + a_b \frac{\partial}{\partial a^a} \right ) \ , \epsilon_{cd} a^c b^d \\ and = \epsilon_{cd} \left ( a_a \delta_b^c + a_b \delta_a^c \right ) b^d = + ( a_a b_b + b_b a_a ) \\ % j_{ab}^b\ , \epsilon_{cd} a^c b^d and = \left ( b_a \frac{\partial}{\partial b^b} + b_b \frac{\partial}{\partial b^a} \right ) \ , \epsilon_{cd} a^c b^d \\ and = \epsilon_{cd} a^c \left ( b_a \delta_b^d + b_b \delta_a^d \right ) = - ( a_a b_b + b_b a_a ) \end{align} so if you sum them the transformation of $a$ is cancelled by the transformation of $b$ .
no , because the wavefunctions are not waves in space . they are waves in enormous high-dimensional spaces of possibilities . if you have two particles , the wavefunction is waving in 6 dimensions ( the two positions of the two particles make a six dimensional space of possibilities ) , if you have three particles , the wavefunction is in 9 dimensions . so it is always wrong to think of it as a wave in space , like a field . there is a field which obeys the schrodinger equation , but this classical field is a classical wave , like e and b , which describes many coherent bosons in the same quantum state all moving together , like a superfluid or a bose-einstein condensate .
quite a simple answer : scattering of light ( rayleigh scattering would be more precise here . . . ) an observer in ground sees the sky as blue due to scattering of light by air molecules present in the atmosphere . for an observer in space , the water bodies reflect the color of sky . . . the water bodies ( ocean , lakes , river ) appear blue ( 'cause water is quite colorless ) because of the way sunlight is selectively scattered as it goes through our atmosphere . taking raman effect into account , water absorbs more of the red light in sunlight . by this way , water also enhances the scattering of blue light in the surroundings . by rayleigh scattering law : ( it is more important here ) the amount of scattering is inversely proportional to the fourth power of its wavelength . due to the larger amount of $n_2$ and $o_2$ molecules ( 78% and 21% ) in the atmosphere , blue light which is having shorter wavelength is scattered to a greater extent . thus , the earth would not be blue if it does not have enough $o_2$ and $n_2$ molecules in its atmosphere . the scattering depends on the characteristics of gaseous molecules in atmosphere . . . this is applicable to other planets also . ( like mars appearing red , venus appears yellow , etc . )
if you give your disk enough ideal properties , such as being infinitely thin and conducting heat perfectly , so that there is no temperature difference between both sides , so that its own radiation has no net effect on its movement , and being able to reflect and absorb radiation perfectly no matter what the wavelength , then methinks it will be the redshift/blueshift thing that will eventually stop it from accelerating . if the disk is moving with velocity $v$ then the radiation hitting the absorptive side will have its frequency modified ( blueshift ) by a factor $f$ , while the reflective side will be ( redshift ) by a factor $f'$ . since radiation pressure is proportional to frequency , the thrust on the disk will be proportional to $2f'-f$ . with the classical approach to doppler effect , we would get $$2f'-f = 2\frac{c-v}{c}-\frac{c+v}{c} = \frac{c-3v}{c} , $$ and there would be acceleration until the disk reaches 1/3 of the speed of light . with the relativistic approach , and $\beta = v/c$ , we would have $$2f'-f = 2\sqrt{\frac{1-\beta}{1+\beta}}-\sqrt{\frac{1+\beta}{1-\beta}}= \frac{1-3\beta}{\sqrt{1-\beta^2}} . $$ the numerator of that expression has a root at $\beta=\frac{1}{3}= 0.33$ , so acceleration would stop at that fraction of the speed of light .
three-phase has two main reasons to exist : driving n . tesla 's polyphase induction motors used throughout industry . reducing the total cost of metal in cross-country power lines : w/single-phase lines , more metal would be needed to transfer the same rate of kilowatts . you are right : lighting as well as ac motors will briefly turn off at 120 times per second ( that is for usa 60hz line frequency . ) for this reason , metal-vapor streetlights and older inductor-ballast fluorescent tubes have significant " hum " modulation in their light output . this strobe effect can be very visible when your eyes sweep across strings of led-based xmas lights . the hum is greatly reduced with incandescent lamps , since the filament does not cool down significantly during the millisecond-long low point in the 60hz wave . and you will probably hear a 120hz sound when using high-speed carbon-brush motors under high mechanical loads , such as electric mixers and carpentry routers . these high-rpm products can not use ac induction motors which are naturally limited to 1800rpm by the 60hz drive frequency . the 120hz variation ends up in the torque output of high-speed brush motors . single-phase induction motors should have much less 120hz mechanical buzz , since they are essentially 2-phase motors with magnetic field torque which rotates rather than oscillates . either capacitance or inductance is used to give the motor a second electromagnet pole having shifted phase .
by the eigenenergies you quote , i imagine you are dealing with an infinite potential well with walls at $x=0$ and $x=l$ . in this case , the wavefunction is zero outside of the well , so $$\phi_n ( x ) = \left\{\begin{array} and \sqrt{\frac{2}{l}}\sin\left ( \frac{n\pi}{l}x\right ) and \text{if }0&lt ; x&lt ; l , \text{ and}\\ \qquad \quad0 and \text{otherwise . }\end{array} \right . $$ the integral is therefore over $x\in [ 0 , l ] $ , and converges without a problem .
in your frame of reference , it does indeed look as though the difference in speed between a and b is greater than $c$ . but the question is - does a think that b is moving away at that speed ? and the answer is " no " . there is a thing called the lorentz transformation which describes how the observed speed of an object is a function of the speed of the observer ; conveniently , this prevents the breaking of the speed limit of special relativity . without giving you the math ( you asked for a " simple explanation" ) , there are two things that happen when you are in a moving inertial frame of reference : length contraction , and time dilation . clocks moving relative to you seem to go slower , and distances become shorter . these changes are described in the lorentz transform .netresult is that velocity is also changed - this is described in the einstein velocity addition equation which states $$u ' = \frac{u+v}{1+\frac{uv}{c^2}}$$ when we put $u=v=c/2$ , we get $u ' = 0.8 c$ , so no speed limit is broken . the key to understand here ( after i re-read your question i realized i needed to add this ) : it is ok for two things to appear to move faster than the speed of light relative to each other - for example , you can see two beams of light , one traveling to the left , and the other traveling to the right , and say " the difference in speed between these photons is 2c " . however , there is no frame of reference in which you can observe anything moving faster than the speed of light - and that is the condition that special relativity imposes . does that difference make sense ?
i am not an expert on nuclear technology or weapons but the wikipedia page on the plant and it is destruction provides some clues . ultimately there were many potential methods that might be used to design a weapon . it was known at the start of the war that bombarding uranium with neutrons resulted in nuclear fission which could be chained together . heavy water was one way to slow , or moderate , the free neutrons ( graphite rods are another option , which is what is used currently in nuclear reactors ) . so while the heavy water would not be used in the final weapon , it was to be used to refine the plutonium that went into the final weapon . the water could be used as a neutron moderator which would enable the production of weapons-grade plutonium , which would then be turned into bombs .
i have called $\beta$ the angle that $\alpha + \beta = 90^o$ so $\tan\alpha = 1/\tan\beta$ since the triangle is not moving , and the situation is symmetrical : the horizontal component of $n$ will cancel out , while the vertical ones must compensate for the weight of the triangle . $$ 2n \cos \beta = m_2g$$ i will call $f_x$ the horizontal component of $-n$ the reaction on the rectangle of $n$ , $f_y$ the vertical component . $$ f_x = n\sin \beta $$ $$ f_y = n \cos \beta $$ friction must compensate the force on the horizontal axis $f_x$ so $$ f_{friction} = f_x \rightarrow \mu r = f_x $$ the reaction of the table is given by the vertical component of the sum of all forces so $r = m_1g + f_y$ from the first you get $$ n = \frac{m_2g}{2\cos \beta}$$ so $$\mu = \frac{f_x}{f_y + m_1g} = \frac{n\sin\beta}{n\cos\beta + m_1g} = \frac{\frac{m_2g}{2\cos \beta}\sin\beta}{\frac{m_2g}{2\cos \beta}\cos\beta + m_1g}$$ simplify you will get $$\mu = \frac{m_2g\tan\beta}{ ( 2m_1 + m_2 ) g} = \frac{m_2}{ ( 2m_1 + m_2 ) \tan\alpha}$$
let 's find the complete solution of the problem . a complete solution of the problem would be the solution to the linear ode , $m \dot{\mathbf{v}} =q\mathbf{v} \times \mathbf{b}-k \mathbf{v}$ assume without loss of generality that the magnetic field is pointed along the z-axis , so $\mathbf{b} = b \mathbf{\hat{z}}$ . so our equation simplifies to , $m \dot{\mathbf{v}} =qb\mathbf{v} \times \mathbf{\hat{z}}-k \mathbf{v}$ dividing both sides of the equation by $m$ and for simplicity in the notation , let $\omega=\frac{qb}{m}$ and $\gamma=\frac{k}{m}$ . so , $\dot{\mathbf{v}} =\omega\mathbf{v} \times \mathbf{\hat{z}}-\gamma \mathbf{v}$ using $\mathbf{v}=\begin{pmatrix} v_{x}\\v_{y}\\v_{z} \end{pmatrix}$ and writng the given eqaution in matrix form we have , $\dot{\mathbf{v}} =\begin{pmatrix} -\gamma and \omega and 0 \\ -\omega and -\gamma and 0 \\ 0 and 0 and -\gamma \end{pmatrix} \mathbf{v}=a \mathbf{v}$ this is linear ode which can be solved using the matrix exponenetial as , $\mathbf{v}=e^{at} \mathbf{v_0}$ to simply this equation we can find the eigenvaules of a , use a similarity transform to convert it to a diagonal matrix which this greatly simplifies the matrix exponential . the eigenvalues and the corresponding eigenvectors are , $\lambda_{1}=-\gamma , \mathbf{v_1}=\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$ $\lambda_{2}=-\gamma-i\omega , \mathbf{v_2}=\begin{pmatrix} 1 \\ i \\ 0 \end{pmatrix}$ $\lambda_{3}=-\gamma+i\omega , \mathbf{v_3}=\begin{pmatrix} i \\ 1 \\ 0 \end{pmatrix}$ using $s= [ \mathbf{v_1} \mathbf{v_2} \mathbf{v_3} ] $ and performing a similarity transform on the matrix a , $s^{-1}as=d$ where d is diagonal matrix with the eigenvalues as the diagonal elements . and here we witness the power of similarity transformations as , $e^{at}= s \begin{pmatrix} e^{\lambda_{1}t} and 0 and 0 \\0 and e^{\lambda_{2}t} and 0 \\ 0 and 0 and e^{\lambda_{3}t} \end{pmatrix} s^{-1}$ ( after some tedious calculations ans using $e^{ix}=\cos{x}+isin{x}$ ) $=\begin{pmatrix} e^{-\gamma t}\cos ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\ -e^{-\gamma t}\sin ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\0 and 0 and e^{-\gamma t}\end{pmatrix}$ therefore , $\mathbf{v}=\begin{pmatrix} e^{-\gamma t}\cos ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\ -e^{-\gamma t}\sin ( \omega t ) and e^{-\gamma t}\sin ( \omega t ) and 0\\0 and 0 and e^{-\gamma t}\end{pmatrix} \mathbf{v_0}$ writing out the components , $v_{x}=e^{-\gamma t} ( v_{x_0} \cos{\omega t}+v_{y_0} \sin{\omega t} ) $ $v_{x}=e^{-\gamma t} ( -v_{x_0} \sin{\omega t}+v_{y_0} \cos{\omega t} ) $ $v_{x}=e^{-\gamma t} v_{z_0}$ this the just the equation of a helix with both the pitch and radius decreasing exponentially with $\gamma$ . however , the angular frequency is the same as that without drag , $\omega$ . here is a sample trajectory ,
assuming you are in orbit around the sun ( presumably a highly elliptical orbit ) you will not feel any force due to gravity . in principle you might feel tidal forces , but for an object the size of a spaceship these are negligable even if you graze the surface of the sun . the most obvious problems are the heat from the sun and the radiation it emits . the radiation is a mixture of electromagnetic radiation and charged particles , both of which are not good for anything relying on it is dna remaining intact . it is difficult to do much about the heat because in space the only way you can cool is by radiation . what you had probably do is surround your spaceship with a mirrored shell and keep a layer of vacuum between the shell and your ship . even with very good mirroring the shell will heat up , but for a while at least it will keep the heat off your spaceship . the messenger probe in orbit round mercury uses a reflective shield , and contains internal refridgeration - i do not knw exactly how this works but presumably it uses a radiator on the side of the probe pointing away from the sun . there is not a lot you can do about the radiation except surround your spaceship with a thick layer of lead , and that much lead would be difficult to put into space . the solar probe plus is planned to get within 4 million miles of the sun 's surface , and this will be the closest we have managed to get any spacecraft . however the spp does not have any human passengers to worry about . i suspect radiation is the real problem for human passengers . even for a hypothetical manned mars mission the radiation dose the astronauts would receive is a worry , and the intensity of the radiation goes up as the inverse square of the distance .
the angular momentum $l_{a/b}$ of a rigid body $a/b$ about its center of mass is $$l_{a/b} = i_{a/b} \omega_{a/b} , $$ where $i_{a/b}$ is the inertia matrix of $a/b$ about its center of mass in the world frame and $\omega_{a/b}$ is the angular velocity of $a/b$ . the angular momentum $l_{a/b}^0$ of a rigid body $a/b$ about the origin of the world frame is $$l_{a/b}^0 = l_{a/b} + x_{a/b} \times p_{a/b} , $$ where $x_{a/b}$ are the coordinates of the center of mass in the world frame . then the total angular momentum in the system with the rigid bodies $a$ and $b$ about the origin of the world frame is $l_{total}^0 = l_a^0 + l_b^0$ , which is supposedly conserved . the total linear momentum is $p_{total} = p_a + p_b$ . the total linear momentum is conserved as soon as the forces $f_a$ and $f_b$ are of equal magnitude and opposite direction ( $f_a = f = -f_b$ ) : $$ \frac{\mathrm d}{\mathrm{d}t} ( p_a + p_b ) = m_a \dot{v}_a + m_b \dot{v}_b = f_a + f_b = f - f = 0 , $$ where $v_{a/b}$ is the translational velocity of $a/b$ in the world frame . the derivative of the total angular momentum with respect to time is $$\begin{split} \frac{\mathrm d}{\mathrm{d}t} ( l_a^0 + l_b^0 ) and = \frac{\mathrm d}{\mathrm{d}t} ( l_a + l_b + x_a \times p_a + x_b \times p_b ) = \dot{l}_a + \dot{l}_b + x_a \times \dot{p}_a + x_b \times \dot{p}_b \\ and = \tau_a + \tau_b + x_a \times f_a + x_b \times f_b = r_a \times f_a + r_b \times f_b + x_a \times f_a + x_b \times f_b \\ and = ( x_a + r_a - x_b - r_b ) \times f . \end{split}$$ thus the total angular momentum is conserved if : $x_a + r_a - x_b - r_b = 0$ , that is the force pair acts at the same coordinates in the world frame , $f = 0$ , that is no force acts , $ ( x_a + r_a - x_b - r_b ) \ ||\ f$ , that is the force acts along the line of connection . fig . 1 satisfies condition number 3 and thus conserves the total angular momentum . fig . 2 satisfies none of the three conditions and thus does not conserve the total angular momentum . edit : the so post is angular momentum always conserved in the absence of an external torque ? contains a proof for point particles , which has an analogous requirement for the conservation to hold ( forces along the line of connection ) . the author of the proof corrected it by now .
according to the american meteor society , the sonic boom of an asteroid or meteor ( sometimes referred to as a ' fireball' ) is due to if a very bright fireball , usually greater than magnitude -8 , penetrates to the stratosphere , below an altitude of about 50 km ( 30 miles ) , and explodes as a bolide , there is a chance that sonic booms may be heard on the ground below . this is more likely if the bolide occurs at an altitude angle of about 45 degrees or so for the observer , and is less likely if the bolide occurs overhead ( although still possible ) or near the horizon . and from caltech 's coolcosmos page when an object travels faster than the speed of sound in earth 's atmosphere , a shock wave can be created that can be heard as a sonic boom . the reason for asteroids causing sonic booms in the lower atmosphere , is according to the article how the falling meteor packed a sonic punch ( klotz , 2013 ) is due to because the meteor is supersonic , the waves , which travel at the speed of sound , can’t get out of the way fast enough . the waves build up , compress and eventually become a single shock wave moving at the speed of sound . looking a bit further in to what a sonic boom ( using a jet as an example ) is and how it occurs is illustrated in the following diagram image source so , if a meteor , asteroid is going faster than the speed of sound for particular part of the atmosphere , then a sonic boom will occur . going back to the american meteor society 's description of the likely cause of a sonic boom , they stated that if a meteor comes in below an altitude of about 50 km ( 30 miles ) then a sonic boom is likely to occur , one of the reasons is that the speed of sound is slower , due to the temperature of the atmosphere at that height and lower . below is a graph showing the speed of sound plotted against temperature as a function of atmospheric elevation : image source .
as far as i know , gallavotti proved the ergodicity of the lorentz gas , while sinai proved that of a system of $n \leq 5$ rigid spheres . anyway , this is a minor detail . for certain aspects , a more suitable model for the drude model is the boltzmann gas . lanford has shown ( in 1970s , i think ) that the entropy for this model is always increasing , but anyone has proved that boltzmann gas is ergodic . so the answer to your question is : if , at our level of accuracy , lorentz gas was an appropriate mathematical scheme for the drude model , than it would be ergodic . else we can not conclude , since sinai 's result is very important but too limited . ( at the present day . ) however , it is an interesting question from a mathematical point of view ( for me , for example , it really is ) , but for a physicist it is not very important , since drude model does not provide an appropriate level of precision for most of calculations carried out nowadays in solid state physics ( at least , concerning what i have seen in my course of condensed matter , i am not a specialist in solid state physics ) . moreover , any of that models takes in account coulombian interactions between charged particles . ( this remark restricts - in principle - a lot the domain of applicability of such schematizations . ) i think you could find very interesting the treatments of this subject ( ergodic theory ) by halmos and arnold in their classical monographs . references . p . r . halmos , lectures on ergodic theory v.i. arnold , ergodic problems of classical mechanics and mathematical methods of classical mechanics g . gallavotti , statistical mechanics and the elements of mechanics
the key for producing a nice tone , is a mix of two facts : first , offering the air stream a somehow geometrical regular hole , where a stationary wave can be born with a certain frequency and other possible frequencies are filtered out , and second , a low enough stream velocity , so that no turbulences can happen and the flow is ordered ( laminar ) . the whistle is produced as an intermediate solution between having a hole that is small enough for the precise stationary wavelengths to happen , and big enough for the air not to flow at much too high a speed . that is why people with their fingers in their mouth can whistle louder , because , by putting the fingers , they create a richer and bigger aperture than a simple , small hole and so air can flow slowly enough but , at the same time , the fingers combined with the lips maintain the involved geometrical distances conveniently small , so that the stationary waves can happen . when you blow too strongly or through an irregular-shaped hole , air flows in a disordered way , thus vibrating with thousands different modes , and that is why you hear the typical " hiss " sound , because a hiss is a mix of lots of frequencies ( technically called white noise ) . physics is not physics without , at least , a little maths . so now , follow me . you surely know newton 's second law : $ma = f$ first , we assume that we take the force per unit volume , so that we use the density instead of the mass . and , at the same time , we write the acceleration as the derivative of the velocity : $\rho \frac{dv}{dt} = f$ because , as you know , the acceleration is the amount of change of the velocity per unit time . when we deal with fluid mechanics , that amount of change is due to two terms : one deals with the change in time of the velocity in a fixed point of space , and the other is due to the change of the velocity from one point to another . that is written in this way : $\rho ( \frac{\partial \mathbf{v}}{\partial t} + \mathbf{v} \cdot \nabla \mathbf{v} ) = f$ ( for those of you who see it for the first time , do not worry about the triangle , it is kind of a sophisticated derivative . just follow what the terms mean ) the $f$ as you know , stands for forces ( due to weight , springs , etc ) . in fluid dynamics , we like to give a special role to two kind of forces , so that we make them appear separately in the equation : $\rho ( \frac{\partial \mathbf{v}}{\partial t} + \mathbf{v} \cdot \nabla \mathbf{v} ) = -\nabla p + \frac{1}{\mathrm{re}} \nabla^2 \mathbf{v} +f$ the term with the $p$ is due to the change in pressure from one point to another . the other one with the $\mathrm{re}$ is due to the viscosity , i.e. a measure of strong the fluid tries to avoid changes in shape ( honey has a higher viscosity than water ) . this is the navier-stokes equation for an incompressible flow ( the air is compressible , but in the range of speeds involved in a whistle , it can be very good approximated by this equation - for the purists : the navier stokes equation is experimentally found to hold , up to a good degree of approximation , in turbulent flows too ) . the $re$ stands for " reynolds number " . it is a somewhat heuristic quantity that goes proportional to the velocity but inversely proportional to viscosity . it depends too on the geometrical dimensions of the problem , so it is not straightforward to derive its value . the important fact when you whistle is that , if you blow strongly , and thus increase the velocity of air , therefore having a big reynolds number , then the forces due to viscosity become less important in the equation ( because the viscose term has $\mathrm{re}$ in the denominator ) . the viscose forces are the ones that most help maintaining the flow geometrically ordered ( laminar ) . when their contribution is not dominant , the flow becomes disordered ( turbulent ) . a turbulent flow has no mechanical properties that are stable enough in time for a stationary wave to establish , so that your nice whistle vanishes and is replaced by a randomly fluctuating mix of thousands frequencies , that is , the white noise of the hiss . . .
newtonian mechanics is not quite as predictable as it is made out to be , both in theory and in practice . in theory , it is an easy task to set up a newtonian system of particles that will eventually violate the lipschitz continuity condition . you can toss reversibility and predictability out the window when that happens . in practice , we can not know state perfectly , and that means the complicated dynamic systems you mentioned eventually become unknowable . they will eventually entire a region of strong chaos , and there is no telling what happens after that . with regard to entropy , consider an interstellar gas cloud . the jeans instability can make a portion of the cloud collapse to form a protostar and a protoplanetary disk . you will not see a protostar and protoplanetary disk undo themselves and reform that gas cloud because of entropy . that is yet another gravitational example , but there are plenty of non-gravitational examples . consider a semi-rigid body with three distinct principle moments of inertia rotating in space . eventually that body will end up spinning about the axis with the largest moment of inertia . this is an irreversible process , and it lies almost entirely within the realm of newtonian mechanics . the only non-newtonian aspect is that the body radiates heat generated by internal friction away into the universe .
no need to make it that complicated--calculate the potential at the surface of a sphere with a uniform mass distribution with mass $m$ . then calculate the energy required to add a bit of mass with mass $dm$ and radius $dr$ to the top of this sphere . express $m$ and $dm$ in terms of $r$ and $dr$ and integrate .
the exhaust and the intake are separate . what they are claiming is that exhaust gases that have just left a piston , may be forced back into it , before the valve closes , due to a high pressure wave originated at a different piston 's exhaust . i am not sure how innovative that is though , since some 20 years ago they were already teaching mechanical engineers like myself that an intelligently designed exhaust manifold can take advantage of reflections of compression and rarefaction waves to improve cylinder emptying .
the mssm has 2 complex doublets i.e. 8 real components in the higgs fields . four of them are electrically neutral ( real bosons , antiparticles to themselves ) , four of them ( i.e. . two particle-antiparticle pairs ) are electrically charged . one neutral real boson and two charged ones ( one charged pair ) get eaten by the gauge bosons because 3 generators are broken , going from $su ( 2 ) \times u ( 1 ) _y$ to $u ( 1 ) _{\rm em}$ . the electrically charged ones are cp-partners of their oppositely charged antiparticles , of course . we do not learn anything if we consider cp for charged particles . one combination of the positive and negative particle is always cp-even and the orthogonal combination is cp-odd . the four neutral ones are evenly split to two cp-even and two cp-odd ones , too . it is the cp-odd ones that are eaten by the gauge bosons . so one is left with the cp-even charged ones , three cp-even neutral bosons , and one cp-odd neutral higgs boson . why are cp-odd bosons eaten ? it is because the gauge bosons pick a minus sign under c , simply because they are linked to generators which are c-odd , too . ( antiparticles need to be transformed by the opposite gauge transformation phases . ) so under p , the gauge bosons are ordinary vectors , not axial vectors , but the c flips their sign , which is why they are " axial vectors under cp " . the goldstone bosons that are eaten are cp-odd . now , are they c-even , p-odd , or vice versa ? this is really a meaningless question because the spectrum of the electroweak theory ( the fermions ) is not p-symmetric . not even the free lagrangian ( not even when mixing is neglected ) . it is because there are left-handed doublets and right-handed lepton singlets , etc . so it makes no sense to try to assign p parity to fields - because no choice will lead to a p-invariant theory . if you remove fermions , you may say that the goldstone bosons are p-even , c-odd . they have the " normal sign " under p because the gauge bosons do , but they have the opposite one under c .
let me give a simpler ( and , surely , more naiive ) answer . given two n-tuplets $x_i$ and $y_j$ , their tensor product is a matrix : $$a_{ij} = x_iy_j$$ so , in your case : $$a_{ij} = \left ( \begin{array}{cc}x_1y_1 and x_1y_2\\x_2y_1 and x_2y_2\end{array}\right ) $$
what does it mean ? the reason they are conservative or non-conservative has to do with the splitting of the derivatives . consider the conservative derivative : $$ \frac{\partial \rho u}{\partial x} $$ when we discretize this , using a simple numerical derivative just to highlight the point , we get : $$ \frac{\partial \rho u}{\partial x} \approx \frac{ ( \rho u ) _i - ( \rho u ) _{i-1}}{\delta x} $$ now , in non-conservative form , the derivative is split apart as : $$ \rho \frac{\partial u}{\partial x} + u \frac{\partial \rho}{\partial x} $$ using the same numerical approximation , we get : $$ \rho \frac{\partial u}{\partial x} + u \frac{\partial \rho}{\partial x} = \rho_i \frac{u_i - u_{i-1}}{\delta x} + u_i \frac{\rho_i - \rho_{i-1}}{\delta x} $$ so now you can see ( hopefully ! ) there are some issues . while the original derivative is mathematically the same , the discrete form is not the same . of particular difficulty is the choice of the terms multiplying the derivative . here i took it at point $i$ , but is $i-1$ better ? maybe at $i-1/2$ ? but then how do we get it at $i-1/2$ ? simple average ? higher order reconstructions ? those arguments just show that the non-conservative form is different , and in some ways harder , but why is it called non-conservative ? for a derivative to be conservative , it must form a telescoping series . in other words , when you add up the terms over a grid , only the boundary terms should remain and the artificial interior points should cancel out . so let 's look at both forms to see how those do . let 's assume a 4 point grid , ranging from $i=0$ to $i=3$ . the conservative form expands as : $$ \frac{ ( \rho u ) _1 - ( \rho u ) _0}{\delta x} + \frac{ ( \rho u ) _2 - ( \rho u ) _1}{\delta x} + \frac{ ( \rho u ) _3 - ( \rho u ) _2}{\delta x} $$ you can see that when you add it all up , you end up with only the boundary terms ( $i = 0$ and $i = 3$ ) . the interior points , $i = 1$ and $i = 2$ have canceled out . now let 's look at the non-conservative form : $$ \rho_1 \frac{u_1 - u_0}{\delta x} + u_1 \frac{\rho_1 - \rho_0}{\delta x} + \rho_2 \frac{u_2 - u_1}{\delta x} + u_2 \frac{\rho_2 - \rho_1}{\delta x} + \rho_3 \frac{u_3 - u_2}{\delta x} + u_3 \frac{\rho_3 - \rho_2}{\delta x} $$ so now , you end up with no terms canceling ! every time you add a new grid point , you are adding in a new term and the number of terms in the sum grows . in other words , what comes in does not balance what goes out , so it is non-conservative . you can repeat the analysis by playing with altering the coordinate of those terms outside the derivative , for example by trying $i-1/2$ where that is just the average of the value at $i$ and $i-1$ . how to choose which to use ? now , more to the point , when do you want to use each scheme ? if your solution is expected to be smooth , then non-conservative may work . for fluids , this is shock-free flows . if you have shocks , or chemical reactions , or any other sharp interfaces , then you want to use the conservative form . there are other considerations . many real world , engineering situations actually like non-conservative schemes when solving problems with shocks . the classic example is the murman-cole scheme for the transonic potential equations . it contains a switch between a central and upwind scheme , but it turns out to be non-conservative . at the time it was introduced , it got incredibly accurate results . results that were comparable to the full navier-stokes results , despite using the potential equations which contain no viscosity . they discovered their error and published a new paper , but the results were much " worse " relative to the original scheme . it turns out the non-conservation introduced an artificial viscosity , making the equations behave more like the navier-stokes equations at a tiny fraction of the cost . needless to say , engineers loved this . " better " results for significantly less cost !
the main question is " relative to what ? " for space probes and the like , the speeds that matter are be either with respect to the earth , the target object ( s ) ( mars , some asteroid , space station , etc . ) , and/or the sun ( or solar system barycenter ) . these speeds are measured mostly by doppler shifts in radio waves emitted by a radar the probe carries , reflected by the surface of some target the communication signal between probe and earth ( see for instance , the deep space network ) . other methods have been used ( image analysis between consecutive images taken by the space probe , the temperature of the heat shield on atmospheric entry , etc . ) but these are all much less precise than doppler measurements . space telescopes will measure redshift to some object ( star , galaxy , etc . ) ( which is very similar to doppler ) , which is more an indication of how fast that object is moving with respect to the entire solar system , rather than just the space telescope . parallax methods are also used ( see @ignaciovazquez-abrams 's answer ) , but such methods can only be used for objects relatively close by ( the parallax for most galaxies is too small to measure ) . other methods include cepheid variables , and of course the famous type 1a supernovae , which were used to conclude that the expansion of the universe is accelerating . but these are primarily measures of distance , and only crude measures of speed -- for objects at large distances , redshift is the only accurate way to measure the speed with respect to those objects .
the terminology of collapse of the wavefunction is an unfortunate one . take an oscillating ac line and use a scope to measure it and display it . is the ac 50 herz wavefunction collapsed because we observe it on the scope ? the ac wave function is just a mathematical description of the voltage and current on the line and allows us to calculate the amplitude and time dependance of the energy it carries . an equally unfortunate concept is the matter wave . the particle is not a continuous soup distributing its matter in space and time the way of an ac voltage or other classical wave . you will never find 1/28th of a particle , it is either there in your measuring instruments or it is not , and it is governed by a probability wave mathematical description , not a " matter wave " even more so , the wavefunction manifestation of a particle does not collapse when we measure it the way a balloon collapses when pierced by a pin , because it is just a mathematical description of the probability to find a particle in a particular ( x , y , z ) with a particular ( p_x , p_y , p_z ) within the constraints of the heisenber uncertainty principle . when wavefunction collapse , does not σx become 0 ? , as we will know the location of the particle . or does standard deviation just become smaller ? we know the location of the particle at that specific coordinate where we had our measuring instrument with the specifice momentum that our insturments measured , within the instrument errors . the probability of finding it there after the fact is 1 . it is the nature of all probability distributions that after the detection they become one . example : the probability i will die in the next ten years is 50% . at the instant of my death the probability is one that i am dead . σx is not a standard deviation in the error sense . σxσp≥ℏ2 says that : if i want to know the location of my particle within a region about the x point with uncertainty/accuracy σx , the σp i can measure simultaneously is constrained to be within an uncertainty that follows the constraint σxσp≥ℏ2 . does the particle resurrect into a wavefunction form ? the particle keeps it dual nature of particle or probability wave according to the momentum it still carries and will be appropriately detected as a particle or a probability wave by the next experimenter . it is not a balloon to have been destroyed by the measurement . what can be an observer that triggers wavefunction collapse ? ( electron wavefunction does not collapse when meeting with electrons ; but some macroscopic objects seem to become observers . . . . ) in principle , any interaction of a particle that changes its momentum and position is an observer except that some interactions are quantum mechanical because of the hup and the nature of the interaction and some are macroscopic manifestations in our instruments of the passage of a particle or probability wave of a particle . we usually call observers the classical macroscopic detectors , be they people or instruments . at the microcosm quantum level we have interactions governed by the probability wave functions . what happens to the energy of a particle/wave packet after the collapse ? energy and momentum are conserved absolutely , so it will depend on what sort of detection of the particle took place . some will be carried off by the particle if it has not been absorbed into the detector , as for example these particles in this bubble chamber photograph which continually interact with the transparent liquid of the bubble chamber . in this case a tiny bit of the energy is taken by kicked off electrons ( first detector atom of liquid , final detector photographic plate ) which show by the ionisation the passage of the particle , which is certainly not idiotically " collapsing " .
it might be worth taking a look to the original text , galileo 's discourses on two new sciences . the reasoning you are looking for is on the third day , a translation of which may be found online . the relevant parts are labelled theorem i and theorem ii in the above-linked translation . to derive that distance in a uniformly accelerated motion ( e . g . free fall ) goes as time squared , galileo first argues that the time in which any space is traversed by a body starting from rest and uniformly accelerated is equal to the time in which that same space would be traversed by the same body moving at a uniform speed whose value is the mean of the highest speed and the speed just before acceleration began . this is argued on a graphical basis ( see above link ) . however , even though the pictures may look pretty similar to modern functional representations ( e . g . of velocity vs . time ) and arguments involve finding equal areas in different situations , the arguments never involve an actual calculation of an " area " with mixed units , which was not yet conceivable at the time ( e . g . $m/s \cdot s = m$ ) . in fact , the whole third day seems very convoluted precisely because the notion of velocity was not clearly numerical yet , since only commensurable ( same unit ) quantities could conceivably be operated with ( added , divided , . . . ) . proportions of non-commensurable quantities , however , could be compared ( today we had say they are dimensionless ) , as in if a moving object traverses two distances in equal intervals of time , these distances will bear to each other the same ratio as the speeds ( earlier in the third day )
the euler number ( often called the euler characteristic ) is given a in terms of nice integral formulas in the gauss-bonnet theorem , but it can be defined in other ways . the difference in the factors simply comes from the fact that the two-dimensional scalar curvature $r$ is twice the gaussian curvature $k$ ( see towards the end of the first paragraph here ) . since you are reading polchinski , you will probably have no problem with most proofs of the gauss-bonnet theorem ( which are all over the internet ) , but i think do carmo 's differential geoemetry has a rather nice , elementary proof .
assuming you are attempting to determine $\langle n|x|n\rangle$ , you are on the right track . here are some hints : determine what the ladder operators do the the energy eigenstates ; \begin{align} a|n\rangle = ? , \qquad a^\dagger |n\rangle = ? \end{align} write the position operator in terms of ladder operators as you have done . what are the inner products of the energy eigenstates : \begin{align} \langle m|n\rangle = ? \end{align} combine this all together to obtain the desired expressions for $\langle x\rangle = \langle n|x|n\rangle$ .
there is a more general statement : all 4d lorentz invariant field theories flow to cfts in the uv and the ir . a proof was given last year by luty , polchinski , and ratazzi in http://arxiv.org/pdf/1204.5221.pdf . their argument has some assumptions but they are fairly weak .
the entropy article in wikipedia includes a paragraph on the statistical mechanics definition of entropy . it is useful for such questions as you pose . specifically , entropy is a logarithmic measure of the density of states : $$s = - k_b\sum_i p_i \ln p_i$$ where $k_b$ is the boltzmann constant , equal to 1.38065×10$^{−23}$ j k$^{−1}$ . the summation is over all the possible microstates of the system , and $p_i$ is the probability that the system is in the ith microstate . for most practical purposes , this can be taken as the fundamental definition of entropy since all other formulas for s can be mathematically derived from it , but not vice versa . ( in some rare and recondite situations , a generalization of this formula may be needed to account for quantum coherence effects , but in any situation where a classical notion of probability makes sense , the above equation accurately describes the entropy of the system . ) in what has been called the fundamental assumption of statistical thermodynamics or the fundamental postulate in statistical mechanics , the occupation of any microstate is assumed to be equally probable ( i.e. . $p_i=1/\omega$ since $\omega$ is the number of microstates ) ; this assumption is usually justified for an isolated system in equilibrium . [ 23 ] then the previous equation reduces to : $$s = k_b \ln \omega$$ this means that any changes in the number of microstates increases entropy . thus radiation does so , each new photon out of the sun increasing its number of microstates . thermal equilibrium means that the temperature is the same in the systems in thermal equilibrium and as @zephyr commented above the earth would have the temperature of the sun , if we were in thermal equilibrium with the sun .
my question is during the initial deceleration , is that simply applied opposite the current orbital velocity vector ? and when the satellite arrives at the new orbit , what direction is the correctional acceleration/deceleration made ? is it the difference of the desired tangential direction that it wants to go and the current direction it is going ? or simply opposite its current vector ? the two directions are the same . the impulses are done at the apoapsis and the periapsis of the transfer orbit , where the velocity is also tangential to the circular orbits ( the larger one at apoapsis and the smaller at periapsis ) . i understand that the point of the hohmann transfer orbits is to use as little velocity change ( and thus fuel ) as possible , but can an orbital transfer be faster or slower if you are willing to burn more fuel to affect the velocity change ? [ snip ] finally , if you had unlimited fuel , and time was more a consideration , would you even bother with this process versus something more " point and shoot , turn and burn " ? if you have enough delta-v and thrust , you can just " point and burn " . you will be doing a hyperbolic transfer orbit that , in the limit of very large velocities , will tend to a straight line ( $e \to \infty$ ) .
in principle , you can charge a conductor indefinitely . but remember that in order to cause a flow of charges from a body ( call it a ' source' ) to another ( the conductor in question ) , the potential of the former has to be lower than the potential of the latter . this potential difference causes a current to flow from the source to the conductor , resulting in a transfer of charges . all the charges that are already in the conductor will exert an electrostatic repulsion on the incoming ones , slowing them down and making it more and more difficult ( as more and more charge accumulates in the conductor ) to charge it further . as charge continues to flow out of the source and into the conductor , the potential difference decreases and the potentials of the two objects become more and more similar . at some point , the potential difference will reach 0 . the current will stop flowing when the charge in the source is equal to the charge in the conductor , which corresponds to the situation in which the electrostatic repulsion from the charges in the conductor is equal to the force attempting to put more charges in . here , the potential difference is 0 . if you want to force more charges to flow out of the source and into the conductor , therefore increasing its charge , you have two options : 1 ) you either increase the potential of the source , so as to create a non-zero potential difference and thus causing current to flow . 2 ) introduce a new force that pushes the charges out of the source and into the conductor : this force ( eg chemical force ) has the job of bringing a charge of the source against the electric field exerted by the charges of the conductor . the real limit to the charging of a conductor would be when there is no physical space available for the electrons . electrons are fermions and they cannot occupy the same position in space , so it will become harder and harder to squash them together . now , in the context of capacitance : imagine a circuit with a generator ( battery ) and a capacitor . dc current will flow only in half of the circuit ( left or right depending on the choice of charge carries , electrons or ions , that is conventional ) . let 's say protons carry the charge ( this is the convention for current , although physically it is the electrons that to the moving ) . protons leave the + terminal and accumulate on the closest plate of the capacitor . the plate is now the conductor , and the + terminal of the battery is the source . the plate has initially 0 charge and therefore 0 potential . the charges accumulating on the plate will exert an electric field that is going to oppose incoming protons . the potential of the + terminal decreases , the potential of the plate increases , the potential difference reaches 0 and current stops flowing : there are as many protons in the + terminal as there are on the plate . unless you increase the charge in the + terminal ( therefore increasing its potential ) or apply another force on the protons , no more current will flow . if you now disconnect the battery and close the circuit , you have one of the capacitor plate charged ( so at a non zero potential ) and the other one with no charge ( 0 potential ) . potential difference => current will flow until the charge on both plates is the same .
it can be justified using distribution theory .
i will attempt an answer , though someone knowing the precise ground realities will most likely improve on my answer . you make a very good point about the speed staying constant if the space ship can be treated as a closed system . that is the sole point that we need to worry about . naturally , the atmosphere within a space ship has to be maintained ( at the values that can support human beings ) . if it was just a case of filling up the shuttle once with $21 \%$ oxygen and being done with it , astronauts would keep consuming it so that its levels would fall , and percentage of ${\rm co}_2$ would keep increasing . that is undesirable and in a simplified description , one can get around this by removing ${\rm co}_2$ via a chemical reaction with lithium hydroxide ${\rm lioh}$ . ( by the way , this is a fairly common use of ${\rm lioh}$ , as a carbon dioxide scrubber in breathing purification systems , as can be seen here . ) upon the reaction , these ''canisters'' can be stored and disposed off later . all the excess water ( i.e. . discounting the potable variety ) is directed to tanks , which can again be disposed later . excess heat is handled by converting to ammonia vapor and subsequent storage . ( though somewhat simplified , a description of this process can be found in the first link of this article . ) so , while space shuttles would ''maintain'' a requisite atmosphere , ( apparently ) nothing gets dumped on there an then basis . now , your question pertained to what would happen if this release happens ( or does not happen ) while the shuttle continues moving ahead at a uniform velocity $v$ - if it got dumped , then $v$ would change . does not seem to be the case . see , everywhere in physics , we make all sorts of approximations , the question is how valid they are in real situations . irrespective of which materials you may choose to build the spacecraft with , it will not make a perfect thermal insulator . while they may try to reduce this radiation loss to as low a value as possible , there will be some amount of heat radiated by the craft . so , while not absolutely ideal , it may be be a good approximation to an insulated body , or in the context , let 's say a closed system . in textbook situations , one always considers simplified descriptions . thus , armed with these links , i think you can safely go and pester your instructor , telling him that his original logic had a flaw ! !
there are two reasons why it may not be necessary . . . firstly the intensity of light from the candle is low unlike that of the laser . secondly , the laser encounters two mirrors while light from the candle only encounters one before reaching the detector .
wind load formula : $f_d = \frac{1}{2} \rho v^2 a c_d$ where $f_d$ is the force of drag ( or in this case force against the flat plate ) $\rho$ is the density of the air $v$ is the speed of the air against the object $a$ is the area of the object which the air is blowing against $c_d$ is the drag coefficient
let me make my comments into an answer : a dalitz plot is a tool for further study of resonances , not for determining their mass . resonances are seen on the plot for the invariant mass distribution , the the square root of the measure of the four vector of the sum of the constituent particles . as with the recent discovery of the higgs . in this plot , which is the invariant mass distribution of the sum of many particles , a number of cuts have been applied to clean up the resonance , and a fit ( red line ) gives the mass . the dalitz plot is for the simpler situation of a decay into three particles . in this case , if there are two resonances for example , the subset of invariant mass plots of pairs will have interference from the kinematic constraints . the plot allows to study this and also gives extra information on the three body parent state , if it is also a resonance , studying the interference patterns .
i can now safely say that it is lte . for anyone interested : there are all sorts of lower frequency carrier waves that are then modulated with the actual hf . for umts , lte or bluetooth , perfectly distinctive sound signatures can be made audible with measurement-tools . or a pair of krk speakers that is , even with magnetic shielding . although readings indicate a really unhealthy dose of radiation ( 6 μw/m2 ) in my room , it is still far below any judicial threshold .
the laughlin state alone does not explain the plateau . there is a lot more to the story . firstly at filling factor=1/3 the many-body ground state of the interacting electron gas is " approximately " the laughlin wavefunction . by this i mean that the overlap between the laughlin state and the numerically found ground state ( for any realistic interaction like coulombic ) is very large , i.e. their inner product is quite close to 1 . using the plasma analogy one can show that this state corresponds to uniform electron density . ( see girvin 's les houches notes for details on plasma analogy . ) secondly the transport phenomena are decided by charged excitations in the system . for the filling factors 1/3,1/5,1/7 , etc . the charged excitations are quasiholes and quasielectrons . while the former has a dip in the density profile at some point z ( say ) in the 2d plane , the latter has the opposite thing in its density profile ( as opposed to the earlier uniform case ) . the plasma analogy can again be used to show that these quasiparticles will have fractional e/3 charge in our case . ( atleast for now let us avoid justifying why they are excited states . ) now lets say we are sitting exactly at 1/3 filling factor and then we add an electron to the system . it will break into 3 quasielectrons which can be separated at no extra energy cost ( the idea of fractionalization ) . similarly if some more electrons are added they will produce more quasiparticles . now start thinking in terms of the ' semiclassical percolation picture ' that is applied to electrons to explain integer qhe ( again see girvin 's notes ) . instead of electrons we give the same arguments using quasiparticles to explain the plateaus around 1/3 filling factor . the conductivities stop changing when the added quasiparticles are either going into the valleys of the disorder potential or are ending up on shorelines at the 2 well separated edges . let me clarify things a bit more . think of starting with the 1/3 filling factor ground state . now let us add adiabatically 1 flux quanta through a thin solenoid at the origin of space ( see laughlin 's nobel lecture ) . he shows that in this process e/3 charge flows towards the origin and gets collected there . thus we have ended up with an exact ground eigenstate of the original hamiltonian + e/3 charge . so quasiholes are ' charged excitations ' , not the excited state when sitting at 1/3 filling . in fact the low energy gapped excitations at 1/3 filling are ' neutral collective excitations ' ( again see girvin 's notes ) and the existence of this gap is necessary for adiabaticity to work fine in the above thought experiment . ( in the words of laughlin the usage of the word quasiparticles here was " unfortunate " . ) now if i just move the filling factor a bit in an experiment the new ground state is made up of new " quasiparticles " .
ref [ 19 ] in the arxiv paper , c . m . caves and b . l . schumaker , phys rev a 31 , 3068 ( 1985 ) , gives a clean description of a parametric amplifier as the prototype of a two-photon device , at the bottom of its second page : in a [ parametric amplifier ] , an intense laser beam at frequency $2\omega$ —the pump beam— illuminates a suitable nonlinear medium . the nonlinearity couples the pump beam to other modes of the electromagnetic field in such a way that a pump photon at frequency $2\omega$ can be annihilated to create " signal " and " idler " photons at frequencies $\omega\pm\epsilon$ and , conversely , signal and idler photons can be annihilated to create a pump photon . one way to think of the present situation would be as a dual of this description . that is , the medium , the josephson junction , oscillates at a pump frequency $2\omega$ , and interacts nonlinearly with the vacuum state . from the arxiv paper itself , there is a clear parallel , quantum theory allows us to make more detailed predictions than just that photons will simply be produced . if the boundary is driven sinusoidally at an angular frequency $\omega_d = 2\pi f_d$ , then it is predicted that photons will be produced in pairs such that their frequencies , $\omega_1$ and $\omega_2$ , sum to the drive frequency , i.e. , we expect $\omega_d = \omega_1 + \omega_2$ . one of the comments on the nature news page , edward schaefer at 2011-06-06 12:39:04 pm , makes a point that i think is worth highlighting , that " the mirror transfers some of its own energy to the virtual photons to make them real . "
i believe this is just a restatement of the first newton 's law .
you are not getting your facts right at all . how do we know from this $\langle w \rangle = \int_{-\infty}^{\infty} \bar{\psi}\left ( -\frac{\hbar^2}{2m} \frac{d^2}{dx^2} + w_p \right ) \psi dx$ or this $\hat{h} = -\frac{\hbar^2}{2m}\frac{d^2}{dx^2} + w_p$ that we have an eigenfunctiuion and eigenvalue . answer : we do not . all i know about operator $\bar{h}$ so far is this equation where $\langle w \rangle$ is an energy expected value : \begin{align} \langle w \rangle = \int_{-\infty}^{\infty} \bar{\psi}\left ( -\frac{\hbar^2}{2m} \frac{d^2}{dx^2} + w_p \right ) \psi dx \end{align} no , you do not . here 's the mathematical side of what an eigenfunction and eigenvalue is : given a linear transformation $t : v \to v$ , where $v$ is an infinite dimensional hilbert or banach space , then a scalar $\lambda$ is an eigenvalue if and only if there is some non-zero vector $v$ such that $t ( v ) = \lambda v$ . here 's the physics side ( i.e. . qm ) : we postulate that the state of a system is described by some abstract vector ( called a ket ) $|\psi\rangle$ that belongs to some abstract hilbert space $\mathcal{h}$ . next we postulate that this state evolves in time by some hermitian operator $h$ , which we call the hamiltonian , via the schrodinger equation . what is $h$ ? you guess and compare to experimental results ( that is what physics is anyway ) . next we postulate for any measurable quantity , there exists some hermitian operator $o$ , and we further postulate that the average of many measurements of $o$ is given by $ \langle o \rangle = \langle \psi | o | \psi \rangle$ . connection to wavefunctions : we pick the hilbert space $l^2 ( \mathbb{r}^3 ) $ to work in , so $\psi ( x ) = \langle x | \psi \rangle$ , and $\langle o \rangle = \int_{-\infty}^{\infty} \psi^* ( x ) o ( x ) \psi ( x ) dx$ . ok , that is the end . the form of $h$ does not follow from the energy expected value . wait ! i have not even talked about eigenvalues and eigenfunctions . this is a useless post ! answer : well you do not have to . but it is useful to find the eigenvalues and eigenfunctions of $h$ , because the eigenfunctions of $h$ form a basis of the hilbert space , and certain expressions become diagonal/more easily manipulated when we do whatever calculations we want to do . so to find the eigenvalues of $h$ , we simply solve the eigenvalue equation as stated above : solve \begin{align} h | \psi_n \rangle = e_n | \psi_n \rangle . \end{align} this is in the form $t ( v ) = \lambda v$ . so as alfred centauri says , we simply want to find the eigenfunctions of $h$ . a more subtle question would be , how do we know they exist ? the answer lies in spectral theory and sturm-liouville theory but nevermind for now , as physicists we assume they always exist . so your additional question : $\hat{a} \psi$ is an eigenfunction of operator$\hat{h}$ with eigenvalue $ ( w-\hbar \omega ) $ . well . . . . that just follows straightaway . you said you already proved that $h a^\dagger \psi = ( w - \hbar \omega ) a^\dagger \psi$ . so here $t$ = $h$ , $a^\dagger \psi = v$ , and $\lambda = ( w - \hbar \omega ) $ . which is an eigenvalue equation $t ( v ) = \lambda v$ . thus , $a^\dagger \psi$ is an eigenvalue of $h$ with eigenvalue $ ( w-\hbar \omega ) $ .
you are assuming that the kruskal–szekeres ( u , v ) coordinates have to be defined in terms of the schwarzschild ( r , t ) coordinates , but there is nothing special or fundamental about the schwarzschild coordinates . general covariance says that we can use any coordinates we like . if the k-s coordinates had been the ones originally chosen by schwarzschild , then someone could have come along later and defined ( r , t ) in terms of ( u , v ) . we would then say that a disadvantage of the ( r , t ) coordinates is that they do not cover all four quadrants . in general , if you have a solution to the einstein field equations , it makes sense to extend it as much as possible . physically , if a test particle reaches a nonsingular point in a finite proper time , then its world-line can and should be extended beyond that point , not just terminated there . it is only at singular points that the laws of physics break down and geodesics can not be extended . in the case of the schwarzschild spacetime , extending it in this way results in all four quadrants of the kruskal diagram . the maximally extended schwarzschild spacetime is not a realistic model of a black hole , however . when a black hole forms by gravitational collapse , you do not get quadrants iii and iv .
it is a direct consequence of the usual ( weak ) spatial markov property enjoyed by the ising model . namely , if $\lambda$ is a ( deterministic ) finite subset of $\mathbb{z}^d$ , and $f$ a local function with support inside $\lambda$ , then the expected value of $f$ in the box $\lambda$ , with a given frozen configuration $\omega$ outside $\lambda$ only depends on the value taken by $\omega$ along the boundary of $\lambda$ ( assuming nearest-neighbor interactions , as in the linked paper ) . this follows immediately from the dlr equation . the strong markov property is the analogous property when $\lambda$ is a random finite subset of $\mathbb{z}^d$ . in order for the property to be satisfied in this case , it is necessary that knowing $\lambda$ only imposes conditions on the spins outside $\lambda$ ( otherwise that would have an effect on the expected value of $f$ ) ; this is the reason for asking that $\lambda$ be determined from outside . an example would be : " let $\lambda$ be the outermost circuit of $+$ spins surrounding the support of $f$ in a region $\delta$ of $\mathbb{z}^d$" . this imposes constraints outside $\lambda$ ( since , e.g. , there cannot be a circuit of $+$ spins surrounding $\lambda$ inside the region $\delta$ ) , but not inside . the proof of the strong version is then a corollary of the ( weak ) markov property : just decompose the expectation of $f$ according to the realization of the random set $\lambda$ , and , for each realization , apply the weak markov property . ps : forgive the plug , but i would like to point out two recent works that provide alternative approaches to the aizenman-higuchi theorem , and yield considerably stronger results : arxiv:1003.6034 ( ising model ) and arxiv:1205.4659 ( potts model ) . edit : i have looked at your slightly more detailed question on math . stackexchange , but i still do not see precisely where your difficulty lies . so , let me try to give an explicit example . to keep things easy , let us only consider the nearest-neighbor model , and work in the finite box $\lambda=\{-n , \ldots , n\}^2$ with $+$ boundary conditions . let us consider the random variable $\sigma_0$ giving the spin at the center of the box . given a contour $\gamma$ ( i assume you know what this is ) surrounding the origin , let $\mathcal{e}_\gamma$ be the event that $\gamma$ is the outermost contour surrounding the origin , and $\mathcal{e}_\varnothing$ be the event that no such contour exist . then the expectation of $\sigma_0$ can be decomposed according to $\gamma$: $$ \mu^+_\lambda ( \sigma_0 ) = \sum_\gamma \mu^+_\lambda ( \mathcal{e}_\gamma ) \mu^+_\lambda ( \sigma_0 \ , |\ , \mathcal{e}_\gamma ) , $$ where the sum is over all contours $\gamma$ in $\lambda_n$ surrounding the origin , including the degenerate case $\gamma=\emptyset$ . the point , now , is that , when $\gamma\neq\emptyset$ , $$ \mu^+_\lambda ( \sigma_0 \ , |\ , \mathcal{e}_\gamma ) = \mu^-_{\rm int ( \gamma ) } ( \sigma_0 ) , $$ since , conditionnally on $\mathcal{e}_\gamma$ , there is a path of $-$ spins along $\gamma$ ( on the " inner " side of $\gamma$ ) , which decouples the configuration inside $\gamma$ from the configuration outside . note , and this is crucial , that the fact that we chose the outermost contour implies that the configuration inside $\rm int ( \gamma ) $ is unconstrained ( while the configuration outside $\gamma$ is very much constrained , since it must not destroy the fact that $\gamma$ was outermost ) . the last identity is the strong markov property for the finite-volume nearest-neighbor ising model . the property in your question is the analogous one for the infinite volume gibbs measure .
when expanded , $ ( 1/8 ) \phi^2 ( \phi-2 ) ^2$ contains quadratic , cubic , quartic ( 2nd , 3rd , 4th degree ) terms in $\phi$ . so it is a polynomial with these monomials . the final form of the potential places general coefficients in front of these terms . the quadratic term is universally written as $ ( 1/2 ) m^2\phi^2$ because it contributes the usual mass term $m^2\psi$ to the klein-gordon equation of motion . note that the potential has units of $m^{d+1}$ where $d+1$ is the spacetime dimension in your conventions because when integrated over $length^{d+1}$ spacetime , we should get a dimensionless action . it follows from the $m^{d+1}$ dimension of $m^2 \phi^2$ that $\phi$ has the dimension of $m^{d/2-1/2}$ . in the cubic term , $\phi^3$ therefore has dimension $m^{3d/2-3/2}$ and we have to multiply it by a coefficient with units $m^{-d/2+5/2}$ to obtain another $m^{d+1}$ term . this $m^{-d/2+5/2}$ coefficient is written as a product of the same power of $m$ , the mass from the quadratic term , and a $\lambda_3$ which is may be kept dimensionless . in the same way , the quartic term contains $\phi^4$ whose dimension is $m^{2d-2}$ but we need the dimension of the whole $v$ to be $m^{d+1}$ so we need to add $m^{d-3}$ , dimensionally speaking , which the formula does , and it adds a new dimensionless coefficient $\lambda_4$ to this term .
( i am considering the speakers are emitting some kind of music or something nonperiodic , the situation gets a bit boring if you consider a uniform source ) it basically means alice hears nothing . atleast , not until bob crosses ( at which time your equation is no longer valid , the $-$ in the denominator becomes a $+$ ) . she hears a sonic boom as bob crosses her , and then hears two sounds at once . the first sound is whatever is being played by bob after he crosses her , at a frequency $\frac{f}{3}$ . the second , more interesting sound , is that whatever sounds were emitted by bob are heard backwards , at a frequency $f$ ( this comes from the $-f$ you derived ) . so , if bob was playing mozart 's symphony 23 , and switched to coldplay 's yellow when he passed alice , alice hears : boom ; yellow at one-third the pitch and simultaneously symphony 23 playing backwards . would probably sound horrible ; - ) why is this ? remember , bob 's speed is greater than the speed of sound . so , wavefronts emitted by bob now are much closer to alice than the wavefronts emitted in the past : here , the moving dot is bob , and assume alice to be another dot to the right of bob in his path . the edge of the cone that you see being formed is the " sonic boom " . it is a region of a rapid rise and fall of pressure ( extremely high pressure ) . right after it passes , you see two kinds of wavefronts passing alice . the first is the " left sides of the circular wavefronts " . these have been emitted after bob passed , and are playhed normally , with a third of the frequency ( yellow in my example ) . the other kind is the right side of the circular wavefronts emitted before bob passed alice . as you can see , these are heard top-down , i.e. , the ones emitted last are detected first . for comparison , here is the same diagram if the relative speed was $&lt ; v_0$: to summarize , the negative frequency just means that the sounds emitted at that time are heard " backwards " at a later time--"reflected " at the point in time when bob crosses alice . bonus : http://what-if.xkcd.com/37/
no , general relativity is based on something called " intrinsic curvature " , which is related to how much parallel lines deviate towards or away from each other . it does not require embedding space-time in a higher dimensional structure to work . you are thinking of something called " extrinsic curvature " . in fact , many examples of extrinsic curvature - including your example of a stick being bent - do not have intrinsic curvature at all . let me try to be a bit clearer : imagine there is an ant who lives on your stick . as far as the ant is concerned , the world is one dimensional . now , suppose we tell the ant that space is really 3d and his little 1d world is inside ( "embedded in" ) that 3d space . there is absolutely no way the ant would be able to figure out if his stick was straight or bent the way you are describing . so , this is not the sort of curvature that interests us in general relativity . basically , intrinsic curvature is just concerned with the geometric relationship between nearby points . it is entirely possible to think about this in terms of embedding space-time into some higher dimensional world , but you do not have to : it works just fine if you confine yourself to the observable four space-time dimensions . " curved " in this sense is just a short hand way of saying " parallel lines do not do what they do in ' flat ' ( euclidean or minkowski ) space / space-time " .
the best way to understand curved space-time is einstein 's way in 1907 . imagine all of space is filled with clocks which are held in place , but they need tick at different rates in order to stay simultaneous with each other . near a massive object , the clocks tick slowly , away from masses , they tick faster . particles travel through space so that they locally take the path of maximum time between fixed endpoints , so that between endpoints which are close to a massive object , their path curves out a little , meaning that they are bent toward the massive object . this is a statement of the einstein 1907 theory of gravity , which he knew then would be the weak field , slow velocity approximation to genera relativity . it is counterintuitive for a few reasons : in geometry , straight line paths are minimum distance . in relativity the path is a local maximum . this is a consequence of the minus sign in the pythagorean theorem in relativity . in relativity , unlike in geometry , the sum of the length of two legs of a triangle ( when these are not imaginary ) is always less than the third , so that straight lines maximize proper time . there is only one function which describes the curving of space time , and this is the clock rate . the curvature is determined by this clock rate , but it is purely a time curvature . space is not curved at all . the geodesic motion is not trivial to see from the clock-rate description . you might naively think that to maximize the proper time you need to move away from massive objects , because time ticks slower near them . but the maximization is holding the endpoints fixed . to give an equation of motion without the concept of maximum proper time , you can just say that objects feel a force of attraction towards regions of slower clock-tick , and leave it at that . but this does not look like a geometrical condition ( although it is ) . i do not believe that there are two pictures of a phenomenon , one appropriate for laymen and a separate one for physicists . a correct picture is a correct picture , and is useful for both , and a misleading picture is misleading for both . this picture is used by all general relativists when they are thinking about the weak field limit . two dimensional relativistic gravity for the two dimensional gravity , with point masses , there is a nice description which can be understood immediately . two dimensional point masses are parallel strings moving perpendicular to the direction of motion in 3d plus time , but these strings are like pencils of light , not stationary line-masses , they are relativistic along their direction of motion . you need to have a relativistic momentum density on the strings for them to reduce to the simple limit of 2+1 gravity . in this limit , the strings are described by 2+1 graity . the point masses in 2+1 gravity are described by cutting out a wedge from a two dimensional paper representing space-time , and gluing it back to form a cone . this description is exact--- this is what the space-time around a relativistic cosmic strings looks like . the space is called locally flat , because if you draw a least distance line it will be straight after unrolling the paper , so that the only curvature is that which can be seen from outside , not to a flat fellow living inside the paper . there is only intrinsic curvature at the tip of the cone , proportional to the deficit angle , the angular size of the wedge . this is is the mass of the string . if you imagine a particle coming in from infinity , it travels in a straight line along the cone , but it comes out deflected in a certain way . this is easiest to see by taking two parallel lines coming in on opposite sides of the cone point--- they will intersect each other . if you make a double-cone by cutting out two wedges , to make a slushie-shape after gluing . the paper is still locally flat , but if you draw two straight lines , the line passing between the cones will intersect the other lines . a collection of n stationary cone points describes an equilibrium stationary configuration of 2d gravity . if you set the cone points in motion , and add some points with negative curvature which evolve in a specific way ( their curvature in the 3d sense is still zero ) you get t'hooft 's description of 2+1 gravity , which is an active research subject today .
comments to the question ( v1 ) : beware that different authors have different conventions for the horizontal order of indices for the christoffel symbols$^1$ $\gamma^{\lambda}{}_{\mu\nu}$ and the riemann curvature tensor $r^{\sigma}{}_{\mu\nu\lambda}$ . some may e.g. write $\gamma_{\mu\nu}{}^{\lambda}$ and $r_{\mu\nu\lambda}{}^{\sigma}$ instead . it may be useful to write objects such as $\gamma^{\lambda}{}_{\mu\nu}$ and $r^{\sigma}{}_{\mu\nu\lambda}$ with covariant and contravariant indices not merely on top of each other a la $\gamma^{\lambda}_{\mu\nu}$ and $r^{\sigma}_{\mu\nu\lambda}$ but horizontally displaced to keep track of the horizontal position of the indices . then the indices can be raised or lowered by a metric $g_{\mu\nu}$ with no ambiguity in notation of which index was raised or lowered . in case of supermanifolds , the indices may correspond to grassmann-odd coordinates , and it may become a tedious exercise in book-keeping to assign consistent transformation laws with correct sign factors for tensor components under coordinate transformations of supercharts . some choices of horizontal orders of indices may be more natural in the sense of minimizing the appearance of sign factors . we mention this 3rd point because both authors barnich and brandt are experts on brst formalism , where grassmann-odd variables play an essential role . -- $^1$ it is covenient to call $\gamma^{\lambda}{}_{\mu\nu}$ christoffel symbols even if the tangent-space connection $\nabla$ is not torsionfree nor compatible with a metric .
halley 's method requires one to measure the timing of the beginning of the transit and the end of the transit ; both pieces of data have to be measured at two places of the earth 's globe whose locations must be known . the picture by vermeer , duckysmokton , ilia shows that the two places on earth have differing locations in two different directions ( the differences in the distance from the sun and venus are too small to be measurable ) : one of them is parallel to the direction of the transit of venus and will be reflected in the overall shift of the timing ; the other component is transverse to it and it will actually shift the line along which venus moves and crosses the sun in the up/down direction i.e. it will make the duration of the transit longer . each of these pieces of data – overall shift in the timing arising from one coordinate 's difference between the two terrestrial locations – and the difference between the length of the transit – due to the other coordinate – are in principle enough to determine the solar parallax . because synchronization of clocks at very different locations was difficult centuries ago , i suppose that the latter – the difference between $\delta t_1$ and $\delta t_2$ – was probably more useful historically . but we are talking about o ( 10 ) minutes differences in both quantities . the calculation of the parallax from $\delta t_1$ and $\delta t_2$ and their difference is a simple exercise in geometry but i want to avoid trigonometric functions here . at any rate , halley did not live to see a proper measurement ( the transit occurs about twice a century and the two events are clumped together with a 8-year break in between ) . the best he could get was 45 angular seconds for the parallax ; the right answer is about 8.8 seconds . he knew that his result was very inaccurate . note that the solar parallax is the angle at which the earth 's radius is seen from the sun , i.e. the difference in the rays needed to observe the sun from the earth 's center and/or a point on the earth disk 's surface . when you convert 8.8 angular seconds to radians , i.e. multiply by $1/3,600\times \pi/180$ , you get $4.3\times 10^{-5}$ . now , divide 6378 km by this small number to get about 150 million km for the au . some orders-of-magnitude estimates for the numbers . venus orbits at 0.7 au so it is actually closer to the earth than it is to the sun during the transit . it means that a shift by 6,000 km up/down on the earth 's side corresponds to about 12,000 km up/down on the sun 's side . so the two horizontal lines crossing the sun on the picture ( places on the solar surface where venus gets " projected" ) may be separated by about 12,000 km . compare it with the solar radius near 700,000 km : you may see that we are shifting the horizontal lines by about 1% of the sun 's radius and the relative difference between $\delta t_1$ and $\delta t_2$ will be comparable to 1% , too . the last transit in 2004 took about 6 hours so the difference in the duration at various places is of order 10 minutes . the 2012 transit of venus on tuesday night utc will take over 6 hours , too ; the timing and duration differs by about 7 minutes depending on the location , too . if you have been dreaming about observing the transit of venus , do not forget about tuesday 22:49 night utc ; the following transits will occur in 2117 and 2125 . there is a blog version of this answer , too .
when one proves that a small part of a greater system is described canonical ensemble , even though the greater system is described by a microcanonical ensemble , the key point is that the density of states of the greater system has the exponential form you mention , over a certain interval of energy . specifically what is important is that $\log \rho ( \eta ) = {\rm const} + \beta \eta$ over the range $\eta = \langle\eta \rangle \pm \delta e$ , where $\delta e$ is the energy fluctuation of the small system , and $\langle \eta \rangle = e - \langle e \rangle$ is the expected energy . the value of $\rho ( \eta ) $ is not really important for energies that are very far outside this range , since those energies never occur . in practice the form of $\rho ( \eta ) $ is usually something like $\log\rho ( \eta ) \approx {\rm const} + n \log \eta$ for some very large $n$ ( such a form is found for gases , in particular ) . this log can be taylor expanded to yield the needed form for the canonical ensemble . for a large environment ( one that is essentially in the thermodynamic limit and much larger than the attached system ) , the first order expansion is very accurate over the relevant energy range .
what is the net force ? newton : $f_{net}=ma=m\frac{dv}{dt}$ so the net work done to accelerate a particle from $v_0=0$ to final velocity $v_f$ is $w_{net}=\int f_{net}ds=\int m\frac{dv}{dt}ds=m \int_{v_0}^{v_f} v dv=\frac{1}{2}mv_f^2$ where in the last step i used $ds/dt=v$ and $m$ constant . if you had some crazy system where $m=m ( v ) $ , then that mass could be a function of velocity . maybe some effective mass due to interactions . . . so what you wrote is $not$ correct since ( as qmechanic said ) , the units do not match ; $mdv$ has units of $\text{kg ms}^{-1}$ , and units of work are $\text{j=kg m}^2s^{-2}$ .
the speed of light is entirely a local concept - it does not care if there are 10 atoms or 10 billion galaxies somewhere in the universe . obviously we can not go to distant galaxies to directly measure the speed of light , so in the absolutely strictest sense this is not directly empirically tested . however , the constancy of the speed of light is one of the most fundamental tenets of physics . in some sense , just about every observation we make in astronomy tests it , for if there were any variation it would manifest in all sorts of crazy ways in every single system we look at . the confusion seems to stem from the term " universal . " the word " universal " means " fundamental " or " unchanging in space and time " or " lies at the heart of our theoretical framework , permeating everything we do . " it does not mean " tied to the universe " or " depends on global properties of the universe . " along the same lines , a scented candle could be said to have an " earthy " scent , but this has nothing to do with it being located on earth the planet .
the total internal energy of a system is completely out of the question as an answer , of course . i would even go as far as saying that it is the quintessential and most important extensive variable of a system . therefore , most things that have to do with ' energies ' within thermodynamics will also be extensive variables . i am not sure of what is being expected from you as an answer , but as a tentative guess from a condensed matter bloke , without prior knowledge about the particular context in which this question was formulated , i would think of ' energy per something ' quantities which are characteristic signatures of the thermodynamic state of a physical system . that would be the case of the bond energy per atom in a condensate ( i.e. . , solid or liquid ) system or the mean thermal energy per degree of freedom which is $\frac{1}{2}k_bt$ from the equipartition theorem ( and an intensive quantity , as temperature is also intensive ) . other possible answers that come to mind would be the fermi energy of the gas of electrons in a neutral solid , or the characteristic gap energies of an insulating , semiconducting or superconducting material , although these are very solid state physics-centered answers .
your calculations are correct . the reason we do not feel it is because the 1 atmosphere of pressure will be applied to all surfaces of our body , including the soles of our feet . in fact the interior of our body is also pushing out against our skin with 1 atmosphere of pressure so there is no net force being applied in either direction at the skin 's surface . the only way you could feel this pressure would be if part of your body was in contact with a vacuum while the rest of your body remained in air at 1 atmosphere of pressure . imagine that you were standing on an opening to a vacuum chamber that was almost , but slightly less than the size of the soles of your feet . if there was a perfectly air tight fit between the vacuum opening and your feet , you would indeed feel that pressure as that much force on your feet just as you calculated . as a simpler experiment , just put your hand over the hose of a vacuum cleaner - that is only a very partial vacuum but the force you need to exert to remove your hand is due to the 1 atmosphere of pressure around us all pushing your hand onto the partial vacuum . in fact if you measured that force needed to pull your hand off of the vacuum cleaner hose , you could compute what the air pressure in the vacuum hose was .
" quasi-something " is " almost something " . in particular , quasicontinuum is a discrete set that contains so many entries ( typically energy eigenvalues ) with so small spacings that it looks like a continuum . for example , if the energy eigenvalues are $$e_n = n\epsilon$$ where $n$ is integer and $\epsilon$ is $0.0001$ times some natural energy difference , then $e_n$ is almost continuous and the set $\{e_n|n\in z\}$ is a quasi-continuum . in many physical questions , a quasi-continuum behaves just like a continuum .
you say your self : the uncertainty principle in quantum mechanics is well known and considered one of most basic properties of natural reality . in fact quantum mechanics and its postulates and laws are the underlying framework on which any classical theory is built . the " laws " of classical theories emerge from the underlying quantum mechanical framework . in the paper you quote they claim that : more precisely , we show that violating the uncertainty relations in quantum mechanics leads to a thermodynamic cycle with positive net work gain , which is very unlikely to exist in nature . as an experimentalist i am in no position to check whether their conclusion is correct , this is the work of peer review in journals , and it has been accepted in nature and , i hope , peer reviewed . well done if it is correct , because it is one more validation of the underlying quantum mechanical framework . i do not know whether it is related to the statement in the wiki article : in statistical thermodynamics , the second law is a consequence of unitarity in quantum mechanics it seems from the references to be connected to the many worlds interpretation , so this new derivation might be a more mainstream connection of the quantum mechanical framework to the second law .
the proton ( $uud$ ) turns into a neutron ( $ddu$ ) . up and down quarks do not have equal charges ; the up is $+\frac{2}{3}e$ and the down is $-\frac{1}{3}e$ . by the way , such an operation has a name - isospin symmetry transformation - corresponding to an approximate su ( 2 ) symmetry that makes the proton and neutron have almost similar masses .
a simplified approach would put a lens with low distortion within its fov at a specific distance to the screen . luckily these screen magnifiers are commercially available for a 12" display , about 40us$\$$ . the principle of a magnifying glass with just one lens is applied . remember the the thick and heavy lenses of a magnifying glass . a frensel lens may be used to get a thinner lens .
i assume you are interested in using light sent from earth . you could use a laser which does not spread out so much as it travels and so less of it would be wasted . if you used some bulb radiating in one direction , you would use the inverse square law to get the power at a distance $x$ , the distance to jupiter . then multiply the power per unit area at that distance by the area of the circle formed by jupiter 's radius to get the reflected power . then use the square law again in the same way to find the power returned to earth . the power you will need will depend on the sensitivity of the photodiode , used to measure the light . the laser is better because the power per unit area goes down way slower with distance .
i think your question is perfectly fine , i do not think this forum is only for advanced research level questions . assuming you are not actually receiving small droplets of water , the air around the droplets is cooled by the water , which will then cool your skin . this is strongly accentuated by the fact the water create air currents . while the effect you suggest of feeling an absence of radiating heat is physically sound , my intuition would tell me it is negligible in this case . radiation of objects at room temperature would not be very strong compared to the diffusive and convection effects induced by your cold shower .
it does not quite work like that . each of the three flavors is a quantum superposition of the three mass states . so if you make an electron neutrino , for example , you get a combination of the lightest neutrino state , the middle mass neutrino state , and the heaviest neutrino state . ( you can look up the coefficients but they are not important here . ) as the neutrino propagates through space , each of these three mass states goes on its way individually , just as a regular particle would . so there is nothing particularly strange about the way neutrinos interact with gravity .
my understanding is that at the speeds involved the shear forces inside the material are so tremendous that the target no longer behaves rigidly , but more like a liquid , which you can see in slow motion videos of bullets hitting metallic targets . because the bullet is moving so fast , the " information " that the bullet has hit the target travels faster than sound , i.e. , a shockwave , giving rise to these enormous forces . once the shear strength of the material is overcome , the friction/drag between the bullet and the target is diminished , so that the majority of the bullet 's momentum makes it out the other end . if you want to send your iphone flying , equip it with a bullet-resist vest , which can handle the forces in question . think of hitting an apple with a cleaver . if you do it slow enough , you are not providing enough shear force for ( some of the ) the cell walls to tear , so the apple stays in tact , and moves with the cleaver . if you do it fast enough , you end up with two halves that barely move .
it is $\sqrt{\frac {gm}{r}}$ , where $m$ is the mass of the body ( earth ) the spacecraft is revolving around , $r$ is the distance from the center of that body ( earth ) , and $g$ is the universal gravitational constant . this can be easily derived from equating centripetal acceleration and gravitational attraction , i.e. $$\frac {mv^2}{r} = \frac {gmm}{r^2}$$ note : this is only applicable for spherical bodies .
this is really an open problem . even for the class of problems which quantum computers are known to be fast at - and shor 's algorithm in particular - there is no ' hard ' proof that classical computers must be slow there . ( to be clear : i do not think any serious computer scientist expects factoring to be in p , but there is no formal proof that it is not . ) it is not clear to me what you mean by ' finite state machine ' . shor 's algorithm does require a universal quantum computer , but any implementation must have finite registers and their size will determine the size of integer they can factor out . from what i can make out , you are asking whether there are special-purpose quantum devices that provide quantum speed-ups , compared to the best classical algorithms , for a particular problem . if that is the case , you may want to look at linear optical quantum computing implementations of the boson sampling problem , which is exactly in that class . some places to look are the computational complexity of linear optics . s aaronson and a arkhipov . proceedings of the 43rd annual acm symposium on theory of computing ( stoc '11 ) , pp . 333-342 . full paper ( 96 pages ) at arxiv:1011.3245 [ quant-ph ] . for a more understandable reference , try this blog entry by aaronson .
physically what is happening is this : when you touch the positively charged source to the conductor ( the metal sphere ) , electrons leave the conductor through the point of contact . this leaves the point of contact on the conductor with a large deficit of electrons , and thus the point has a positive charge density . the positive charge density produces an electric field in the conductor , which immediately pulls on remaining electrons in the conductor . the electrons remaining spread out until they have eliminated all of the electric fields in the conductor ( if there were remaining fields , the electrons would continue to rearrange ) . the electrons will now be ' more spread out ' than the protons ; the difference between the new electron surface density and the original tells you the distribution of ' excess positive charge ' on the surface . i hope this helps , let me know if you have an application in mind for this ; i often times find it helpful in thinking about problems to temporarily ignore the fact that in practice there is only one charge carrier ( the electron ) and just think about excess positive charge as positively charged particles spreading out .
when you see the history of the universe plotted against time , the time used is the comoving time i.e. the time measured by a clock that is at rest with respect to the universe around it . this is the time co-ordinate used in the flrw metric , which is a solution to the equations of gr that , as far as we can tell , gives a good description of the universe back to very early times . earlier than around the planck time after the big bang we expect the notion of time to become imprecise because it is not possible to measure times shorter than the planck time . without a working theory of quantum gravity it is not possible to comment further . however for all times later than the planck time we expect time to be a good co-ordinate and be well behaved . this allows physicists to calculate at what time the various stages in the evolution of the universe happened . response to comment let me attempt to phrase my answer more broadly . the first point is that in relativity ( both special and general ) you need to be careful talking about time . for example you have probably heard that time runs more slowly when you move at speeds near the speed of light . however there is a well defined standard time that cosmologists use for describing the history of the universe . we call this comoving time . so when you hear statements like " the universe is 13.7 billion years old " we mean it is 13.7 billion years old in comoving time . you do not need to know how comoving time is defined , just that it gives us a good timescale for describing the history of the universe back to the planck time . which brings us to . . . you have probably also heard of heisenberg 's uncertainty principle . again i will gloss over the details , but one side effect of the uncertainty principle is that it is impossible to measure times less than about 5 $\times$ 10$^{-44}$ seconds . i do not know of a simple way to explain this to someone who is not familiar with quantum mechanics , so i am afraid you will have to take this on faith . and this brings us back to hawking 's programme . as long as the times we are interested in are greater than 5 $\times$ 10$^{-44}$ seconds we can define the time using comoving time so we can assign reliable times to cosmological events like the electroweak transition . but for times close to 5 $\times$ 10$^{-44}$ seconds the whole notion of a " time when something happens " becomes meaningless because it is fundamentally impossible to measure times that short . i would guess this is what hawking means when he says time ceases to exist .
assume sinusoidal grate -- sin(x) is the simplest possible periodic function . direct axis x horizontally , and y vertically , and lets calculate diffraction at the point (x,y) . according to fraunhofer diffraction we must first calculate distance from the observation point (x,y) to the source of secondary wave (x_1, h*sin(d*x_1))) which is r=sqrt((y-h*sin(d*x_1))^2+(x-x_1)^2) . here the h is the height of the ripple , and d its frequency . this r is plugged in into the amplitude integral Integrate(i*exp(-2*pi*i*r/lambda)/(r*lambda),x1=-R..R) . mathematica expression : Integrate[i*exp(-2*pi*i*sqrt((y-h*sin(x1))^2+(x-x1)^2)/lambda)/(sqrt((y-h*sin(x1))^2+(x-x1)^2)*lambda),{x1,-d,d}]  maple : Int(i*exp(-2*pi*i*sqrt((y-h*sin(d*x1))^2+(x-x1)^2)/lambda)/(sqrt((y-h*sin(d*x1))^2+(x-x1)^2)*lambda),x1=-R..R);  neither succeeded solving it analytically , so one have to regress to numerics . therefore , i assigned the following numeric values : the radius of the flat mirror : R=20 mm  wavelength : lambda=0.0004 mm (=400 nm)  defect density/frequency : d=10 (10 ripples/mm -- with higher values Wolfram alpha times out)  defect height h=0.0001 mm (1/1 wave)  location y=100 mm (10 cm above the mirror).  with all the assignments the only free variable remaining is x , so one should be able to plot intensity graph . unfortunately , wolfram alpha refuse to understand what Plot[Integral[]] is and suggests some stock market graphs instead . i had to calculate pointwise . here is an expression which calculates intensity at the axis ( x=0 ) : Integral[i*exp(-2*pi*i*sqrt((100-0.0001*sin(10*x1))^2+(0.0-x1)^2)/0.0004)/(sqrt((100-0.0001*sin(10*x1))^2+(0.0-x1)^2)*0.0004),x1=-20..20]  which walpfa evaluates to 0.62+3.81i the characteristic angle of periodic grating is lambda*d so the diffraction pattern linear dimension is lambda*d*y which in our case is conveniently 1 . therefore , we could see diffraction pattern by probing only three points : x=0.0 , x=0.5 , and x=1.0 . here are the intensities : x=0.0 -&gt; 0.62+3.81*i x=0.5 -&gt; 7.76+3.0*i x=1.0 -&gt; 3.51+1.8*i  in other words , the diffraction pattern is quite noticeable when defect size is comparable to lambda . to doublecheck , what if we shrink grating height 10 fold ? there : x=0.0 -&gt; 3.4+3.5*i x=0.5 -&gt; 4.1+3.1*i x=1.0 -&gt; 3.88+3.33*i 
since $\mathcal{h}$ is anti-self-adjoint , we have $$\langle f , \mathcal{h}f\rangle=\frac{\langle f , \mathcal{h}f\rangle-\langle \mathcal{h}f , f\rangle}{2}=0$$ since $f=|a|_x^2$ is real . thus $\frac{dp}{dt}=\frac{\alpha}{2}\langle f , \mathcal{h}f\rangle=0$ ( unless i am misreading your notation ) .
the job of the friction is to enforce the no slip constraint . so let us find the relative acceleration of the two parts and find the force $f$ which makes it zero . the cylinder eom are ( positive is to the left ) $ f = m \dot{v} $ and $r f = i \dot{\omega}$ and the tangential cylinder acceleration at p is $\dot{v}_p = \dot{v} + r \dot{\omega}$ to make the tangential acceleration equal to the rug you have $$ a = \dot{v}_p = \frac{f}{m} + \frac{r^2 f}{i} \\ f = \left ( \frac{1}{m} + \frac{r^2}{i}\right ) ^{-1} a $$ where the part in the parenthesis is the effective mass of a rolling cylinder on its surface . for a solid cylinder the mass moment of inertia is $i=\frac{m}{2} r^2$ and thus $$ f = \left ( \frac{1}{m} + \frac{2}{m}\right ) ^{-1} a = \frac{m}{3} a$$
there are plans for encounters with kuiper belt objects ( kbos ) after it passes pluto . ( pluto itself is a kbo . ) quoting the new horizons mission timeline : plans for an extended mission include one to two encounters of kuiper belt objects , ranging from about 25 to 55 miles ( 40 to 90 kilometers ) in diameter . new horizons would acquire the same data it collected at pluto - where applicable - and follow a timeline similar to the pluo encounter : closest approach - 4 weeks : object observations closest approach + 2 weeks : post-encounter studies closest approach + 2 months : all data returned to earth the " one to two " is a limitation imposed by the amount of available fuel . the tricky part ( well , one tricky part ) is finding kbos that new horizons can visit . and the project is asking for help from the public to find suitable kbos . it turns out that the human eye is better than computers at identifying potential kbos in photographs . you can help by visiting http://www.icehunters.org/. how cool is that ?
an interesting question . you are right , the stress in a crystalline solid , or any solid , is treated by engineers as a macroscopic property of matter assuming matter is a continuous medium . it is given in terms of the external forces acting on the solid per unit area at some direction . hence the distinction of $\sigma_{xy} , \sigma_{yz}$ etc . this goes with the definition of stress $\sigma_{ij}={\frac {df_i}{da_j}}$ where $df_i , da_j$ are cartesian tensors ( not like the general contravariant and covariant tensors in general tensor analysis as in gr . ) this derivative is meaningful locally and , as said above , treats the solid as a continuous medium . in order to define stress in terms of the crystal structure you can relate it via the strain tensor , which can be defined in terms of relative infinitesimal displacements of the atoms in the cell . there are a couple of ways of doing this , one of which is the lagrangian description . in this , the coordinates $ ( x_1 , x_2 , x_3 ) $ of the atoms in the unrestrained state are taken as the independent variables , while $ ( u_1 , u_2 , u_3 ) $ are the relative displacements of the atoms , and they are the dependent variables . this leads to the following definition for the strain tensor ( langrangian strain ) $\eta_{ij}={\frac 1 2} ( {\frac {\partial u_i}{\partial xj}}+{\frac {\partial u_j}{\partial x_i}}+\sigma {\frac {\partial u_r}{\partial x_i}} {\frac {\partial u_r}{\partial x_j}} ) $ where the summation is over the index $r$ . note that this is a function of the coordinates of the atoms in the lattice , so $\eta_{ij}$ is a locally defined quantity . macroscopically strain and stress relate via young’s modulus $e$ in the relation $\sigma=\epsilon e$ ( hooke’s law , ) which would be ok for an isotropic material , in which direction of application of the force is irrelevant . for anisotropic materials , such as crystalline solids in condensed matter physics , this relation is generalised to the following $\sigma_{ij}=\sigma c_{ijkl}\eta_{kl}$ and the summation is assumed over the $kl$ indices . the coefficients $c_{ijkl}$ are the second order ‘elastic constants’ or elastic stiffness coefficients of the material , and they are fourth order tensors , defined as derivatives of the elastic energy of the material , i.e. the potential energy of the atoms in the crystal lattice due to their relative displacements . i think this is what tms meant in his comment . i hope this helps to understand the notion of locality of stress in crystalline solids .
consider a conformal theory of fields $\phi$ defined on minkowski spacetime $m=\mathbb r^{3,1}$ and with target space $v$ . let $\rho_m$ denote a representation of the conformal group $g$ on $m$ and let $\rho_v$ denote a representation of $g$ on $v$ . then we can define an action $\rho_f$ of $g$ on fields $\phi$ as follows : $$ ( \rho_f ( g ) \phi ) ( \rho_m ( g ) x ) = \rho_v ( g ) \phi ( x ) $$ this is a more notationally descriptive version of di-francesco eq . 2.114 ; $$ \phi' ( x' ) = \mathcal f ( \phi ( x ) ) $$ in other words , the action of $g$ on fields has two parts , a spacetime part , and a target space ( aka internal ) part . now , suppose that we find that there is another representation $\rho_d$ of $g$ acting on fields ( by way of differential operators for example ) for which $$ \phi ( \rho_m ( g ) x ) = \rho_d ( g ) \phi ( x ) $$ then notice that the first equation above can be written as follows : $$ ( \rho_f ( g ) \phi ) ( x ) = \rho_d ( g^{-1} ) \rho_v ( g ) \phi ( x ) $$ how does this manifest itself in terms of signs and generators ? well , suppose that for any group representation $\rho$ of $g$ , we have a corresponding , linear representation $r$ of its lie algebra $\mathfrak g$ such that if we write an element $g\in g$ as the exponential of a lie algebra element $x$ ; $g = e^x$ , then $$ \rho ( g ) = e^{r ( x ) } $$ then we can write the transformation of the fields as $$ ( \rho_f ( g ) \phi ) ( x ) = e^{-r_d ( x ) }e^{r_v ( x ) }\phi ( x ) $$ for any $x\in\mathfrak g$ . if these lie algebra representations commute ( as they would if , for example , $r_d$ is a representation via differential operators and $r_v$ is some spacetime-independent representation on the target space ) , then we can write $$ ( \rho_f ( g ) \phi ) ( x ) = e^{r_v ( x ) -r_d ( x ) }\phi ( x ) $$ now , suppose that we choose a basis $x_a$ for the lie algebra $\mathfrak g$ such that every element $x\in \mathfrak g$ can be written as $$ x = i\omega_ax_a $$ for some real numbers $\omega_a$ . then using linearity of the representations $r_d , r_v$ we have $$ ( \rho_f ( g ) \phi ) ( x ) = e^{-i\omega_a ( r_d ( x_a ) - r_v ( x_a ) ) }\phi ( x ) $$ so if we use di-francesco 's notation $$ ( \rho_f ( g ) \phi ) ( x ) = e^{-i\omega_ag_a}\phi ( x ) $$ then we have $$ g_a = r_d ( x_a ) - r_v ( x_a ) $$ to see that this leads to consistent signs etc . , let 's look at an example : example . translations consider a field whose target space transforms trivially under translations ; $$ ( \rho_f ( g ) \phi ) ( x+a ) = \phi ( x ) $$ then if $ia^\mu p_\mu$ is the lie algebra element that corresponds to translations $x\to x+a$ , and if we define $$ r_v ( p_\mu ) = 0 , \qquad r_d ( p_\mu ) = -i\partial_\mu $$ then we have $$ g_\mu = -i\partial_\mu $$ so that $$ e^{-ia^\mu g_\mu}\phi ( x ) = e^{-a^\mu\partial_\mu}\phi ( x ) = \phi ( x-a ) = ( \rho_f ( g ) \phi ) ( x ) $$ as desired .
the first thing when you start connecting is that x_1 has 8 lines to connect because otherwise you are leaving the combinantion in which y_1 and y_2 could be exchanged because they are arbitrary .
remember that your expression gives the energy difference per unit volume . so you need an additional factor of $\left [ l^{-3}\right ] $ on the left hand side .
measuring dead-time ( or other hardware efficiencies ) is a non-trivial proposition , and there is no completely general solution . the answer that john gives in the comments ( $\tau \times \text{number of events}$ ) is the best case : a system with few interconnections and no " extensible " contributions to the dead time . " extensible " describes a system where events coming in during the dead time re-set the recovery clock . because very high rates can leave such system dead essentially all the time such systems are also called " paralyzable " . so let 's try to categorize a few cases intrinsic dead-time of individual cascading particle detector elements . there are generally of the non-extensible , fix-time per event variety . often with quite a short $\tau$ . you measure dead time by counting events and you design your experiment to keep the total small . dead-time due to a background veto system ( cosmic ray veto for a activity measurement or some such ) . depending on the electronics design these can be extensible or non-extensible , but they probably should be extensible . yuck . if you are getting much extensions you need to reduce the background . the kamland muon veto was in principle extensible but the rates were so low that it did not happen often eoungh to matter . because muon vetos were recorded in the data stream the total dead time could be simple added up later . dead times that arise from veto signal can be estimated by counting a clock signal as vetoed and raw . of course , you need to have designed this in . computer dead-time due to read-out delays or a rate-limiting by imposed sampling window . this is generally a non-extensible dead time characterized by a simple counting again . the only issue here is that the $\tau$ may need to be evaluated by monte carlo if the system consists of multiple parts with different response times . ( because a trigger that occurs near the transition between " live " and " dead " periods might be missed if one part of the signal is registered on the other side of the cut off ; i needed to do that for a kamland study once . )
consider the system in equilibrium , with the rod hanging straight down . imagine taking a marker and drawing a vertical diameter across the disk . if the rod were fixed to the disk 's center so that the disk could not rotate , then would this marker line still be vertical mid swing ? what does this tell you about the rotation of the disk around its central axis relative to its starting position ? if the disk were not rigidly attached as in the former case , would there now be any torque on the disk around its central axis ? what would the marker line look like mid swing in this case ?
turns out the book ( not paper ) is on google books ( hopefully the link works , otherwise go to books . google . com and search " viscosities of solutions and mixtures" ) . it says for salt ( nacl ) , the values for the three-parameter equation , $$ \frac{\eta_s}{\eta_0} = 1+a\sqrt{c}+bc + cc^2 , $$ are given by $$ a=0.0062 \quad b=0.0793 \quad c=0.0080 $$ unfortunately , the one-parameter values are not given in that book ( at least from my perusal ) , but i imagine you might be able to approximate it by comparing estimations with the above three-parameter values . ps : sorry about not responding to that comment , seems i totally missed that in my feed . hopefully this answer makes up for that !
the sound that reaches your ear is just air pressure fluctuating over time . you can use a transducer of some sort to convert the value of air pressure to some other form - for example : to the depth of a groove being cut into a helical track on a layer of wax on a rotating drum to the depth of a groove being cut into a spiral track on a circular disc of metal from which other plastic disks are pressed . to a strength of magnetisation of a magnetic layer on a plastic tape being wound onto a spool to a series of numbers representing the pressure at regular tiny intervals of time . the idea that the variations in pressure over time are due to , or consist of , a collection of frequencies is just a mathematically equivalent description but it does not represent extra information , it is just a different way of describing the same information . here 's some diagrams from a synthesizer manual above are three very different sounds with apparently the same frequency ( say 440 hz ) above is shown how you can add sine waves of two frequencies to produce a more complex waveform above is shown how you can continue adding sine waves of differing frequencies to construct an arbitrary waveform ( a sawtooth ) . the sawtooth waverform can be recorded directly as depths in a groove on a record . but you could " record " the same thing as a set of numbers that represent the frequencies of a dozen sine waves you could add together to produce a single pressure wave that varies over time in the same way . see fast fourier transform
the white cloud you see in the water is steam bubbles . the grains of salt provide nucleation sites that allow the water to vaporize as they fall through the superheated liquid ( so bowlofred had it right--although it is steam that is forming , not dissolved gasses coming out of solution ) . if you raise a pot of water to near boiling and toss in a handful of salt , the water can explode out of the pot due to this effect . counter to what you might think , the addition of salt to water actually raises the boiling point by about one half degree celsius for every 58 grams of salt dissolved per kilogram of water . so the steam is not caused by salty water around the dissolving salt crystals boiling at a lower temperature . the nucleation effect diminishes as the salt diffuses throughout the water . note that the effect is not limited to salt . if you toss in something that does not dissolve--like sand--you will see the same nucleation effect .
yes , it should be $x/z = tan \theta$ , this is probably a typo . the constraint should be $a - \sqrt{x^2 + z^2}=0$ for the argument to make sense . $r$ is a coordinate which is variable but due to the constraint it will always be equal to $a$ , so we can use $a$ in the equations instead . ( $\dot{a}=0$ ) . you know that a gradient of $f$ is always perpendicular to the surface of constant $f$ , so you can understand the extra term coming from $\partial l'/\partial x_i $ as a force acting perpendicular to the surface of $f=0$ holding the particle on it . however , $\lambda$ has to be solved so that the motion of the system is only along the constant surface . but imagine now a force equal to the solved $\lambda \partial f/\partial x_i$ - it would have the same effect as the constraint , so this is the force by which the constraint actually has to be acting to hold the particle/system . ( this also answers 4 . )
as stated , $\mathbf{n}$ is a unit vector and $n_x$ , $n_y$ and $n_z$ are its cartesian components . $\mathbf{n}$ is just a vector pointing in an arbitrarily direction with magnitude 1 . taking $\mathbf{n} \cdot \mathbf{\sigma}$ , we have \begin{equation} \mathbf{n} \cdot \mathbf{\sigma} = n_x\sigma_x + n_y \sigma_y + n_z \sigma_z \\ = n_x \left ( \begin{array}{cc} 0 and 1\\1 and 0\end{array}\right ) + n_y \left ( \begin{array}{cc} 0 and -i\\i and 0\end{array}\right ) + n_z \left ( \begin{array}{cc} 1 and 0\\0 and -1\end{array}\right ) \end{equation}
the partial pressure of the water in the solution does , indeed , decrease . the total pressure of water plus solute , i.e. the pressure of the solution as a whole , stays the same . is this what you are asking ? how can we see this ? pressure is force per area . since we did not change the area , and because forces from different atoms/molecules in the solution are additive , the partial pressures are additive . this is called dalton 's law . strictly speaking , this is only true , if there are no internal forces between the components of a mixture , so it holds reasonably well for many gas mixtures at low pressure and high enough temperature , but it does not hold for concentrated ionic solutions , for which we also have to calculate strong interactions between the solvent atoms/molecules and the solute ions .
this vector potential can be written in every point on the plane except the origin as : $$ a_x = -\frac{\partial \psi}{\partial y}$$ $$ a_y = \frac{\partial \psi}{\partial x} $$ with $$\psi = \frac{1}{2}\mathrm{log} ( x^2+y^2 ) $$ $a$ is not exact , because $\psi$ is singular at the origin . but this means that the magnetic field is zero at every point except the origin . at the origin itself , the magnetic field must be infinite , because the flux through an arbitrary small loop is nonvanishing : $$\phi = \int a = \int_0^{2\pi} d\phi = 2 \pi$$ such a magnetic field can be generated by a infinite solenoid whose radius is shrunk to zero while keeping the flux constant . in the hydrodynamical terminology , the function $\psi$ is called the stream function , it satisfies the laplace 's equation ( harmonic function ) except at the origin . this specific stream function describes a vortex ( the vector potential describes the velocity field of the vortex ) . the streamlines of this velocity field are circles around the origin and its magnitude is inversely proportional to the radius . in order to see that the stream function is harmonic except at the singularity , and generalize the construction to the case of the sphere , we can use complex coordinates on the plane : $$ z = x+iy$$ in this representation we have : $$\psi = \frac{1}{2}\mathrm{log} ( \bar{z}z ) $$ applying the laplace operator , we get $$\nabla^2 \psi=\partial_{\bar{z}} \partial_z \psi = \delta^2_l ( z ) $$ where we have used $$ \frac{\partial}{\partial \bar{z}}\frac{1}{z} = \delta^{ ( 2 ) }_l ( z ) $$ is the complex coordinate on the plane . $\delta^{ ( 2 ) }_l$ is the two dimensional dirac delta function with respect to the lebesgue measure . i.e. , $$\int_{\mathbb{c}} f ( z ) \delta^{ ( 2 ) }_l ( z-z_0 ) \mathrm{dre} ( z ) \mathrm{dim} ( z ) = f ( z_0 ) $$ the vector potential in the complex representation has the form : $$ a_z = \frac{1}{i}\frac{\partial \psi}{\partial \bar{z}}$$ $$ a_{\bar{z}}= -\frac{1}{i}\frac{\partial \psi}{\partial z} $$ explicitly : $$a = \frac{1}{2i}\frac{zd\bar{z}-\bar{z}dz}{\bar{z}z}$$ this fact describes another physical interpretation of this vector potential as follows : in two dimensions a function satisfying the laplace 's equation ( harmonic function ) ( except at the point singularities ) qualifies to be a stream function whose anti-symmetrized gradient ( which is the vector potential in our problem ) describes the velocity field of a vortex . please observe that that this velocity field is invariant under rotations about the origi n and its magnitude is inversely proportional to the distance from the origin . in this interpretation , the line integral of the vector potential is the vorticity . we can change the location of the singularity ( flux line ) to any other point in the plane , say $ ( x_0 , y_0 ) $ $$a = \frac{ ( x-x_0 ) dy - ( y-y_0 ) dx}{ ( x-x_0 ) ^2+ ( y-y_0 ) ^2} = \frac{1}{2i}\frac{ ( z-z_0 ) d\bar{z}- ( \bar{z}- \bar{z_0} ) dz}{ ( \bar{z}- \bar{z_0} ) ( z-z_0 ) } $$ in this case it is not hard to verify that this vector potential can be derived from the stream function : $$\psi = \frac{1}{2}\mathrm{log} ( ( \bar{z}-\bar{z}_0 ) ( z-z_0 ) ) \equiv \mathrm{log} ( |z-z_0| ) $$ we can add several stream functions centered at various points on the plane with different vorticities to obtain a general solution representing fluxes at these points : $$\psi = \sum_k \gamma_k \mathrm{log} ( |z-z_k| ) $$ the constant $\gamma_k $ expresses the fluxes around the $k$-th center ( or the vorticity in the hydrodynamical terminology ) . one can easily verify that the single centered vector potential ( and also the corresponding stream function ) are invariants under the metric preserving automorphisms of the plane consisting of translations and rotations : ( which can be compactly written in the complex notations as : ) $$z \rightarrow e^{i\alpha} z + v$$ $$z_0 \rightarrow e^{i\alpha} z_0 + v$$ from the expression of the single centered stream function , one observes that the denominator is the geodesic distance on the plane , thus a candidate generalization to the sphere ( $s^2$ ) would be the replacement of this by geodesic distance on the sphere : $$|z-z_0|^2 \rightarrow \frac{|z-z_0|^2}{ ( 1+\bar{z}z ) ( 1+\bar{z}_0z_0 ) }$$ where $z$ is the stereographic projection coordinate on the sphere : $$ z = \mathrm{tan}{\frac{\theta}{2}}e^{i \phi}$$ ( $\theta$ and $\phi$ are the spherical surface coordinates ) . thus the candidate solution on the sphere is : $$\psi= \mathrm{log} ( |z-z_0| - \frac{1}{2} \mathrm{log} ( 1+\bar{z}z ) - \frac{1}{2} \mathrm{log} ( 1+\bar{z}_0z_0 ) ) $$ this solution is invariant under the metric preserving automorphisms of the sphere : $$ z\rightarrow \frac{\alpha z + \beta}{-\bar{\beta} z +\bar{\alpha} }$$ , with $|\alpha|^2+|\beta|^2 = 1$ the matrix : $$\begin{pmatrix} \alpha and \beta\\ -\bar{\beta} and \bar{\alpha} and \end{pmatrix} \in su ( 2 ) $$ which is the automorphism group of the round metric thus the candidate vector potential corresponding to this solution is obtained by applying the gradient operator in the sphere 's curvlinear coordinates : $$ a_z = \frac{1}{i} ( 1+\bar{z}z ) ^2 \frac{\partial \psi}{\partial \bar{z}}$$ $$ a_{\bar{z}}= -\frac{1}{i} ( 1+\bar{z}z ) ^2 \frac{\partial \psi}{\partial z} $$ explicitly $$a = \frac{1}{2i} ( 1+\bar{z}z ) ( 1+\bar{z}_0z_0 ) \frac{ ( z-z_0 ) ( 1+\bar{z}_0 z ) d\bar{z}- ( \bar{z}- \bar{z_0} ) ( 1+\bar{z}_0 z ) dz}{ ( \bar{z}- \bar{z_0} ) ( z-z_0 ) }$$ the laplacian on the sphere is given by : $$\nabla^2 = ( 1+\bar{z}z ) ^2 \frac{\partial}{\partial z}\frac{\partial}{\partial\bar{z}}$$ applying the laplacian operator to the candidate stream function , we obtain : $$\nabla^2 \psi= ( 1+\bar{z}z ) ^2 \delta^{ ( 2 ) }_l ( z-z_0 ) +1 = \delta^{ ( 2 ) }_s ( z-z_0 ) +1$$ where $ \delta^{ ( 2 ) }_s$ is the dirac delta function corresponding to the spherical measure : $$\int_{\mathbb{s^2}} f ( z ) \delta^{ ( 2 ) }_s ( z-z_0 ) \frac{ \mathrm{dre} ( z ) \mathrm{dim} ( z ) }{ ( 1+\bar{z}z ) ^2}= f ( z_0 ) $$ the additional constant term in the laplacian constitutes a problem because it means that this stream function is not harmonic outside the singularities . the solution to this problem on the sphere is to add up several solutions with a vanishing total flux ( vorticity ) . $$\sum_k \gamma_k = 0$$ in this case the constant contributions from all centers will cancel .
no , it is not possible in general , because the particles are almost surely to be entangled ( since they are interacting , as mentioned ) . the reason is that each $\rho_i$ is nothing but the reduced density operator of the $i$ th particle . indeed , in the form you have written , $$\rho_i ( x_i , x_i' ) = \langle x_i | \rho_i | x_i ' \rangle = \langle x_i | \mathsf{tr}_{x_i \neq x_i} ( |\psi \rangle \langle \psi | ) | x_i ' \rangle , $$ where the $x_i$ go over the rest of the system . even if all the $\rho_i$ are known , one cannot find $\rho \equiv |\psi \rangle \langle \psi |$ , the state of the system , because there can exist correlations among the variables of different particles that cannot permit one to write the following : $$ \rho = \rho_1 \otimes \rho_2 \otimes . . \rho_i \otimes . . \ \ \ \ \mathsf{ ( incorrect ) } $$ this is just the continuous variable version of entanglement . as an example with qubits , consider the bell state $$ |\psi \rangle = \frac{|01\rangle + |10\rangle}{\sqrt{2}} $$ and trace out one of the qubits . this yields $$ \rho_1 = \rho_2 = \frac{|0\rangle\langle 0|+|1\rangle\langle 1|}{2} , $$ but even a state with only classical correlations , $$ \rho = \frac{|00\rangle \langle00| +|11\rangle\langle11|}{2} $$ would give the same reduced density operators , this state being much different from the entangled one considered . this shows that there is more information in the full state $\rho$ than in the partial knowledge of each of the subsystems $\rho_i$ .
you are correct to observe that there is an often unstated assumption in the standard setup of this problem . when given this problem you are supposed to assume that the off-major-axis components of angular velocity make a contribution to l which is negligible compared to the on-axis angular velocity . obviously this is a good assumption if the top is rotating fast enough , but it is not exactly true . if you start with the initial condition , that the top 's angular velocity is completely aligned with its major axis at the time of release , you should find that the top 's major axis does not really rotate uniformly in a circle but rather there is a very small sinusoidal variation about the uniform circular motion .
the actual reason why one can not interpret the equation $$ \nabla_\mu t^{\mu\nu}=0 $$ as a global conservation law is that it uses covariant derivatives . if a law like that were valid with partial derivatives , you could derive such a law . but there is a covariant derivative which is one of the technical ways to explain that general relativity in generic backgrounds does not preserve any energy : http://motls.blogspot.com/2010/08/why-and-how-energy-is-not-conserved-in.html the text above also explains other reasons why the conservation law disappears in cosmology . however , despite the non-existence of a global ( nonzero ) conserved energy in general backgrounds , the tensor $t_{\mu\nu}$ is still well-defined . as twistor correctly writes , it quantifies the contribution to the energy and momentum from all matter fields ( non-gravitational ones ) and matter particles . and if you can approximate the background spacetime by a flat one , $g_{\mu\nu}=\eta_{\mu\nu}$ , which is usually the case with a huge precision ( in weak enough gravitational fields , locally , or if you replace local objects that heavily curve the spacetime , including black holes , by some effective $t$ , using a very-long-distance effective description ) , then $\nabla$ may be replaced by $\partial$ in the flat minkowski coordinates and the situation is reduced to that of special relativity and the " integral conservation law " may be restored .
in qft , we reinterpret the probability density as the probability charge density . in other words , negative probabilities correspond to antiparticles . in fact , the dirac equation which describes spin-1/2 also has this property , and it led to the prediction of the positron as the antiparticle of the electron .
i disagree with the premise of your question that energy is a " simply mathematical object " . indeed , the fact that it can have density and flux is one the reasons why it ought to be considered a real , tangible thing instead of just a bookkeeping device . while energy in on all its forms is a property of something else , being a property does not reduce a physical thing to being simply a mathematical object . as dmckee said , the idea is that when you have a field ( any field , not just the em field ) you can meaningfully discuss the energy at a particular point . if a field can store energy , then it makes sense that different sized " chunks " of the field will store different amounts of energy . think , for example , about electromagnetic waves that can deliver power to a receiver . a big box that contains many wave periods will obviously have more energy inside it than a small box that contains very few wave periods , since the wave delivers a certain amount of energy per period . there is nothing special about waves vs . other field configurations , so the same reasoning applies to any form of the electromagnetic , or any other , field . well , if it is meaningful to talk about the total field energy contained in some box then its meaningful to compute the average density of energy by dividing out the box volume . if i take the limit of smaller and smaller boxes around a point , this average density approaches the density at that point . we can go through the same sort of reasoning with momentum density ( which is closely related to the energy flux ) . the fact that , as you observed , energy appears to flow like a fluid is because conservation of energy is actually a much stronger statement than you are imagining . you are used to talking about energy conservation in global terms : the change in the energy of a system is equal to the work done on the system by its environment minus the work done on the environment by the system . in field terms , this means that if you draw any sized box , then the change in the total energy in the box equals the total flux going through the sides of the box . by making the box arbitrarily small , we get ( using the divergence theorem ) a local statement of conservation of energy : if $\rho ( \vec{x} , t ) $ is energy density and $j ( \vec{x} , t ) $ is the energy flux then $\frac{\partial\rho ( \vec{x} , t ) }{\partial t} + \nabla \cdot j ( \vec{x} , t ) = 0$ . this is a very powerful statement and is more fundamental than the global statement . for instance , in general relativity local conservation of energy still holds but , because of the complications introduced by curvature , the global statement fails .
in the picture you posted the collector is on the other side of the tower so you can not see it . have a look at this pdf for detailed pictures of the tower - the collector is shown on page 40 , and it is indeed black .
you expect that the paper will bend downwards due to the decreased pressure applied to the air gap causing an increase in its volume , but your observation is that instead the paper is bent upwards . i think that this is probably caused by water leakage . try the experiment again with a plastic seal over the glass , i expect you to see the film bend down . as an estimate of how much down , air pressure is around 30 feet of water . your glass is holding a few inches . so i would think this fraction ( 2/12 ) /30 = 1/180 would be about the amount of movement , on average . for 2 inches of water this would be about 1/90 of an inch , which is detectable ( look to see how reflections in the film are distorted ) .
the analogy is wrong . a voltage source can only shock us if it is able to pass a considerable amount of current through our body ( ~ 250 ma or so , i dont know the exact value but you can google it ) . the circuit that you are trying to discuss , does indeed have 36 amps of current flowing through it , but once you connect yourself to the circuit , you are in fact adding an additional resistance to the circuit ( resistance of our body is usually within 100 - 150 k-ohms ) . this additional resistance will dramatically reduce the value of current in the circuit . all the potential difference will now drop across the high resistance , and very small amount would be leftover to the parallel resistors . with some rough values , there maybe approximately about 0.008 ma which is definitely not enough to be felt by a human being .
if you leave aside quantum mechanics , the laws of physics are deterministic . that means if you have a perfect description of a system you can calculate the properties of the system arbitrarily far into it is past or future . so if you know the properties of the system at some time you can tell what the system was like in the past . however just because we are calculating past properties from future properties i would not say that makes the properties of the system dependant on it is future . which is all very well , but many interesting systems are chaotic and since it is impossible to characterise even a classical system perfectly we can not calculate the systems properties for more than a short distance into the future . and of course in quantum mechanics we have the collapse of the wavefunction , which means we can not calculate the results of future observations . the best we can do is assign them a probability .
instead of using catapults , magnetic propulsion systems ( especially superconducting ) would be more useful for a non-rocket space-launch . particularly , the energy required for an average guy ( 70 kg ) to get outta this world would be $43.904 × 10^8 j$ your catapult is somewhat comparable to a space gun . but , it cannot place the payload in a stable orbit , since gravity certainly would not allow that . . . mass drivers which use superconducting coils would be a more efficient ( more than 90% ) of attaining such a large amount of kinetic energy ( escape velocity ) with a single lift . but , they are all in the future proposals list . . . they are so much powerful and probably do not have a weight limit . yes , we could launch satellites directly into their geostationary orbits without the use of rockets . even though it would be a lot of success , some million new bills should be invested to obtain it . . . also , i will add that space elevators would be more useful when comparing budget . all those wikipedia links are quite good in this subject . . . edit : okay . . . a trebuchet consists of two arms - projectile arm and main weight arm . the ratio of the length of the main weight arm to the length of the projectile arm is typically between 1/2 and 1/5 . the main weight arm has the counter-weight required to shoot the projectile and it should always be in multiples of the projectile weight ( the height of main weight arms also matters here ) . the angle we use commonly for a projectile is 45° . here to attain the escape velocity ( at least close to ) , you must use at least a million ton counter weight , a 10 lbs projectile , and a trebuchet more than 2 mile long , and use an angle of 90° would be helpful . . ! theoretically possible i would say . . . refer these trebuchet range and projectile range papers . . . edit : your first two questions were reasonable . but now , it is quite impossible to attain . as i have already told , gravity would not allow your projectile to be placed in a geo-stationary orbit . even though there is no air resistance ( friction in atmosphere ) and you have broken the escape velocity , you had escape into outer space and never return . . ! hence in the absence of a rocket propulsion system , you require at least some kinda magnetic systems ( not just wood and clockworks . . ! ) @ehryk : if those papers were not useful , here is an information from a simulation which gives an idea . there are some software such as atreb , wintreb or trebuchet simulator . . . you could refer the list of simulators provided . . .
it is my understanding that whenever an object gains or loses electric charge this actually corresponds to losing/gaining electrons ( protons do not move ) . this is not always the case . if you are talking about everyday static electricity ( getting zapped by a door , for instance ) , then yes , electrons are transferred . however , more generally , ions ( e . g . , protons ) may be move and carry charge with them . for instance , in a lithium-ion battery , lithium ions are moving due to a potential internal to the battery . so how can a positive charge always move from higher to lower potential ? positive charges always move from high potentials to lower potentials . negative charges move in the opposite direction . the dominant charge carrier ( species ) that moves depends on the situation at hand . in many situations , the ion cannot move freely due to , for example , bonds with other ions . electrons in the valence band of insulators are more readily " stripped " off the atom and can become free charge carriers . electrons in the conduction band of metals and semiconductors are also able to move more readily within the material lattice .
lehner and pretorius have recently given some persuasive numerical evidence that there are generic violations of cosmic censorship which arise in the time evolution of black strings in 5d gravity , aka the gregory-laflamme instability http://arxiv.org/pdf/1006.5960 . the failure of cosmic censorship certainly does not imply the downfall of causality . classically it means we can see a singularity without it being hidden behind a horizon . this seems to me like a very good thing . we all believe that there is some uv completion of gravity , although different camps have different views on what this is . this theory will cut off the singularity through some combination of classical modifications to einstein gravity and quantum effects . being able to see such a direct effect of the uv completion would be a wonderful thing , so i think we should all hope that there are also generic violations of cosmic censorship in four dimensions , although as far as i know the jury is still out on this .
first order of business is to find where the heck on earth you are . first , $\omega = 360 \sin ( \phi ) /day$ , where $\omega$ is 216.528 degrees ; $\phi$ is the latitude of your position . north of the equator is positive , south negative . this gives you a band to follow around the earth horizontally , positions where could possibly be . you can further narrow your position down because a foucault pendulum can be used to find the acceleration of gravity at its position . once you figure this out , you can go to nasa websites and check out when this location has its next or last total solar eclipse .
you know the basic spring equation , right ? $f = xk$ , where $k$ is the spring constant , in units of force per distance . you also know work ( energy ) is force times distance , right ? so all you have got to do is integrate $xk dx$ from d1 to d2 . ( hint , you can pull $k$ out of the integral . ) you could do it on graph paper if you happened to know d1 and d2 . added : ok , here 's the graph paper approach : a graph of force $f$ versus displacement $x$ looks like this , right ?  | / | / | / F | / | / | / | / |/_______ x  the slope of the graph is $k$ . the area under the graph is work $w$ , because it is just the sum of a bunch of vertical slivers with area $f$ times the width $dx$ of each sliver . so here 's how you get the answer to your question : that help ?
first , observe that although the non-relativistic lagrangian is not invariant . it changes by a total derivative , thus the equations of motions remain invariant . the reason of the difference between the lorentzian and the galilean cases is that the group action of the lorentz group on the classical variables ( positions and momenta ) is a by means of a true representation , while in the case of the galilean group the representation is projective . in the language of geometric quantization , $exp ( i \frac{s}{\hbar} ) $ , where $s$ is the action is a section in $l \otimes \bar{l}$ , where $l$ is the prequantization line bundle and $\bar{l}$ its dual . in other words , the action needs not be a scalar , only an exprssion of the form : $\bar{\psi} ( t_2 ) exp ( i \frac{s ( t_1 , t_2 ) }{\hbar} ) \psi ( t_1 ) $ , where $\psi ( t ) $ is the wavefunction at time $t$ and $s ( t_1 , t_2 ) $ is the classical action between $t_1$ and $t_2$ . the reason that the representation in the galilean case is projective is related to the nontriviality of the cohomology group $h^2 ( g , u ( 1 ) ) $ in the galilean case in contrast to the lorentz case . i have given a more detailed answer on a very similar subject in my answer to anirbit : poincare group vs galilean group and in the comments therein .
at david zaslavsky 's suggestion i will transfer this from the comments to the answers ( i was a bit hesitant because i do not know how reliable youtube videos are to still be around in , say , 6 months time ! ) : this little youtube video might help . you can only resolve the objects by looking at the reflected waves . the amount of detail you can get in the reflected waves can not be smaller than the wavelength ( roughly speaking ) . edit : the video shows incident waves being reflected off small irregularities in the surface at the bottom of the picture . the first case ( wavelength smaller than the irregularites ) shows information about the irregularities being " fed back " in the reflected waves : the last case ( wavelength larger than the irregularities ) shows much coarser information being fed back , making it not possible to get any information about , for example , the size of these irregularities : of course snapshots are a bit hard to read , you had really have to look at the statistics of the received reflected waves as a function of position to really see what was going on , but the video gives a general impression of the problem .
the direction of the thrown object is not important since there is not air resistance . whatever upward vertical component the object had will be downward when it passes you . also , the horizontal component does not change . so it does not matter if you throw the object straight up , or at 37 degrees , or at 0 degrees , the object will always have the same energy when it is at the same level . so the potential energy plus the kinetic energy at the start equals the kinetic energy at the end . $\frac {1}{2}m{v_i}^2 + mgy = \frac {1}{2}m{v_f}^2$
nothing , your book did it wrong .
the problem is that heat flow ( in or out of an object ) is related to the temperature difference between the object and it is environment . for the sort of cooling usually found in the kitchen ( convective cooling ) the heat flow , and therefore the rate of temperature change is proportional to the temperature difference . so let 's take some example like a bottle of milk . if you want to heat it quickly that is pretty easy because it is easy to generate a large temperature difference on the hot side . just burn some gas . however to cool the milk quickly we need to generate a large temperature difference on the cool side , and that is hard . you mention liquid nitrogen , and indeed that is a good way to cool things quickly . however you are forgetting all the hours the liquid nitrogen supplier had to put in to cool nitrogen enough to make it liquify . in general it is hard to cool things quickly unless you cheat and start with something ( like liquid nitrogen ) that is already been cooled . response to comment : this started as a comment , but it got a bit involved so i thought i would put it in here . the temperature of anything ( e . g . milk ) depends on how much heat it contains . let 's not get into exactly what heat is , but basically if you add heat it increases the temperature and if you remove heat it reduces the temperature . the problem is that the milk is surrounded by the rest of the world , and this is around room temperature . heat will not flow from a cold place to a hot place , so heat will not flow out of the milk into it is surroundings unless we do some work . typically what we do is use energy to pump heat around . the area we have pumped the heat from becomes cooler and the area we have pumped it to becomes hotter . this is what you do to liquify nitrogen . you have to pump the heat out of it so te nitrogen gets cold and liquifies while the rest of the world gets hotter . once we have the liquid nitrogen we can use it to cool the milk , but it took a lot of work to make the liquid nitrogen . if you are interested in pursuing this , the mechanism for pumping heat around is called ( unsurprisingly :- ) a heat pump . it is basically a heat engine that runs backwards . heating things is easy because there are lots of systems that have stored energy that can be easily converted to heat . for example a gas/air mixturer has chemical energy that is converted to heat by burning it . you mentioned a microwave : this uses electricity that came from chemical energy i.e. from a power station burning gas or coal , so the heat froma microwave originally came from chemical energy . you might wonder why we can not easily convert heat to chemical energy e.g. mix carbon dioxide and water and have it convert to gas and oxygen and cool down in the process . if we could do this it would be an easy way to cool things . the reason why we can not do this is the second law of thermodynamics . explaining this would be a long essay in it is own right , but in brief it is highly probable that a gas/air mixture will convert to carbon dioxide and water ( i.e. . burn ) but it is very improbable that a carbon dioxide/water mixture would convert back to gas and air .
rocket fuels initially , followed by a series of gravitational assists ( slingshots ) : http://en.wikipedia.org/wiki/gravity_assist the linked article mentions voyager 1 mission as an example .
i think it is possible but would require advanced electronic sensors and some kind of moterized spinning system inside it . if there are no external forces , angular momentum is conserved . so if the internal mechanisms spin one way the dice will have a torque the opposing way .
as we are discussing above , it seems that the detector you are using seems to be functioning properly when used with verifiable radioactive material . the extra clicks are being caused by current induced in the counter by magnetic flux when you may be moving the detector . in order to get around this , it seems that you should use , in addition to the crinkled foil suggested by dmckee , a setup that involves keeping the business end of the detector and the sample under test stationary , thereby removing the current being induced by the changing magnetic field . many thanks to dmckee for the insights above .
gravitational waves have never been directly detected . gravitational waves are predicted by general relativity and have been inferred from other observations . strong evidence of gravitational waves is the change in period of the hulse-talyor binary star system . energy is being lost from the system at a rate consistent with radiation of gravitational waves . the march 17th 2014 report of evidence of primordial gravitational waves is not a direct observation of the waves . instead photons were being observed and gravitation waves inferred from the photons . now that the research group ( bicep2 ) has published their work in a peer reviewed journal , they state that dust may in fact be responsible for the effect attributed to gravitational waves . quoting their published paper " models are not sufficiently constrained . . . to exclude the possibility of dust emission bright enough to explain the entire excess signal " . so it is not yet confirmed that primordial gravitation waves have been detected . •why gw requires such energy levels ? there is no threshold energy for producing gravitation waves . for example a rotating rod , rotating perpendicualr to the axis of the rod , would produce gravitation waves . however gravitation is relatively weak force and it is very difficult , so far impossible , to detect gravitational waves . •why attractive gravity does not create gw ? at the risk of oversimplifying , time varying gravitational fields produce gravitational waves . or more technically , the nth time derivative of the n-multipole moment ( such as the 3rd derivative of the quadrople moment ) of the stress-energy tensor must be non-zero for gravitation waves to be produced . attractive gravity ( such a two objects attracting each other and orbiting their common center of mass ) can cause graviational waves if there is time dependence such as orbitting .
the thing here is that when a particle decays ( nb here i talk about point-like decays , i will address decaying compound systems later ) , the products were not " in " the original particle in the first place . that is $$a \to b + c + d$$ does not imply that $a$ was made up of $b$ and $c$ and $d$ . it implies the combination of two facts that $a$ and $b+c+d$ have compatible quantum numbers . that $m_a$ is sufficiently larger than $m_b + m_c + m_d$ to allow for the products to escape from each other . in other words a neutron is a particle distinct from the collection of a proton , an electron and a electron anti-neutrino , but those of the neutron 's quantum numbers that are respected by the charged-current weak interaction are the same as the collection of lighter particles and the neutrons mass is high enough to produce them . compound systems with tree-level decays now consider beta decay of a non-trivial nucleus . let 's take tritium for the sake keeping the writing simple . the fact that the system is bound means $$m_t &lt ; m_p + 2m_n \ , . $$ but it is still true that $$m_{^3\mathrm{he}^{+2}} + e^- + \bar{\nu}_e &lt ; m_t\ , $$ and the quantum number of the resulting system are compatible with those of a triton alpha-decay alpha decay is a little different . there is no tree-level transformation there , instead there are multiple masses to think about . i am going to use $m$ to label the mother isotope , $d$ for the daughter isotope and $\alpha$ for the alpha particle . here we have a hierarchy of masses . the mother isotope is bound , so we know that $$ m_m &lt ; z_m m_p + ( a_m - z_m ) m_n \ , . $$ ( here $z_m$ and $a_m$ are the charge number and mass number of the mother isotope . ) but , because the decay is energetically allowed we also know that $$m_d + m_\alpha &lt ; m_m \ , , $$ which is to say that the combined system of the daughter and an alpha is more bound than the mother isotope .
quantum mechanics ( qm – also known as quantum physics , or quantum theory ) is a branch of physics which deals with physical phenomena at microscopic scales , where the action is on the order of the planck constant . quantum mechanics departs from classical mechanics primarily at the quantum realm of atomic and subatomic length scales . quantum mechanics provides a mathematical description of much of the dual particle-like and wave-like behavior and interactions of energy and matter . quantum mechanics is the non-relativistic limit of quantum field theory ( qft ) , a theory that was developed later that combined quantum mechanics with relativity . quantum field theory ( qft ) is a theoretical framework for constructing quantum mechanical models of subatomic particles in particle physics and quasiparticles in condensed matter physics , by treating a particle as an excited state of an underlying physical field . some of the relativistic quantum field theories would be qed , qcd , and the standard model . references : http://en.wikipedia.org/wiki/quantum_mechanics http://en.wikipedia.org/wiki/quantum_field_theory
the schwarzschild metric for $d$ dimensions is the standard form $$ ds^2~=~-e^{2\phi}dt^2~+~e^{2\gamma}dr^2~+~r^2d\omega^2 $$ these metric terms in the einstein field equation gives $$ r_{tt}~-~\frac{1}{2}rg_{tt}~=~g_{tt}~=~ -e^{2\phi}\big ( ( d~-~1 ) \frac{e^{2\gamma}}{r} ~+~\frac{ ( d~+~1 ) ( d~-~2 ) }{2r^2} ( 1~-~e^{2\gamma}\big ) $$ a multiplication by $g^{tt}$ removes the $-e^{2\phi}$ and we equate this with a pressureless fluid $t^{tt}~=~\kappa\rho$ . so we think of the black hole as composed of “dust . ” some analysis on this is used to compute the $g_r^r$ gives the curvature term $$ g_r^r~=~\big ( ( d~-~1 ) ( \phi_{ , r}~+~\gamma_{ , r} ) \frac{e^{-2\gamma}}{r}~+~\kappa\rho\big ) . $$ which tells us $\phi~=~-\gamma$ , commensurate with the standard schwarzschild result , and that $$ e^{-2\gamma}~=~e^{2\phi}~-~\big ( \frac{r_0}{r}\big ) ^{d~-~2} . $$ the entropy of the black hole is then computed by writing the density according to these metric elements and computing the rindler time coordinates $s~=~2\pi ( d~-~2 ) a/\kappa$ . the results more or less follow as with the standard $3~+~1$ spacetime result . the connection with strings is to work with the entropy of the black hole . the $1~+~1$ string world sheet has $d~-~1$ transverse degrees of freedom which contain the field data . the entropy $s~=~2\pi ( d~-~1 ) t$ may be computed with the string length , which reduces to the holographic results in $d~=~4$ spacetime . the event horizon is $d~-~2$ dimensional , which for $10$ dimension means the horizon is $8$ dimensional . the singularity is not considered in these calculations . the factors $e^{-2\gamma}~=~e^{2\phi}$ become extremely large . the metric approximates $$ ds^2~\simeq~\big ( \frac{r_0}{r}\big ) ^{d~-~2}dt^2~+~r^2d\omega^2 $$ which is a $d~-~1$ dimensional surface where the weyl curvature diverges for $r~\rightarrow~0$ . for $d~=~4$ this has properties similar to an anti-desitter space . the theory of black holes essentially follows in arbitrary dimensions . it is interesting to speculate on what the singularity is from a stringy perspective . the event horizon contains the quantum field information which composes the black hole . this may then have some type of correspondence with the interior singularity , with one dimension larger . for a black hole that is very small $\sim~10^3$ planck units , the horizon is a quantum fluctuating region , as is the singularity , and the qft data on the two may have some form of equivalency .
you are correct : everything is in motion ( or not ) based on the reference frame . motion is a relative concept , so you are never " moving " but only " moving with respect to something " . find a good basic primer here : http://en.wikipedia.org/wiki/principle_of_relativity
you are not missing anything . when an object moves through a uniform fluid under the influence of a constant force and drag , the closer it gets to terminal speed , the less the net force on it is and the less it will accelerate . so it only approaches terminal speed asymptotically , never actually reaching it . as you know , the equation works out to $$v = v_t\tanh\biggl ( \frac{t}{t_0}\biggr ) $$ and if you try to find the time at which $v = v_t$ ( terminal speed ) , you do indeed get $\tanh^{-1} ( 1 ) $ which is $\infty$ . but for any given precision $\delta v$ , you can find a time after which $\lvert v - v_t\rvert \leq \delta v$ , so given the precision of your measurements , you can tell when the falling object 's speed will be indistinguishable from terminal speed .
the thin lens formula carries over with a few approximations directly to wave optics . this formula , together with an appropriate analysis of diffraction , will let you get a first approximation to the behaviour of many systems . leftaroundabout 's answer gives the overall picture . a good way to translate this into a wave picture is with the following assumptions : the paraxial approximation : i.e. all fields are a superposition of plane waves that are propagating at small angles to the optical axis ; gaussian scalar fields . these are fields that , on a transverse plane , vary like $\exp\left ( - ( ( x-x_0 ) ^2+ ( y-y_0 ) ^2 ) \left ( \frac{1}{2\ , \sigma^2} + \frac{i\ , k , \kappa}{2}\right ) \right ) $ where $\sigma$ is the spotsize and $\kappa$ the wavefront curvature . the " algorithms " are as follows . you begin with the helmholtz equation in a homogeneous medium $ ( \nabla^2 + k^2 ) \psi = 0$ . if the field comprises only plane waves in the positive $z$ direction then we can represent the diffraction of any scalar field on any transverse ( of the form $z=c$ ) plane by : $$\begin{array}{lcl}\psi ( x , y , z ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \left [ \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) \ , \psi ( k_x , k_y ) \right ] {\rm d} k_x {\rm d} k_y\\ \psi ( k_x , k_y ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \exp\left ( -i \left ( k_x u + k_y v\right ) \right ) \ , \psi ( x , y , 0 ) \ , {\rm d} u\ , {\rm d} v\end{array}\qquad ( 1 ) $$ in words : take the fourier transform of the scalar field over a transverse plane to express it as a superposition of scalar plane waves $\psi_{k_x , k_y} ( x , y , 0 ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) $ with superposition weights $\psi ( k_x , k_y ) $ ; note that plane waves propagating in the $+z$ direction fulfilling the helmholtz equation vary as $\psi_{k_x , k_y} ( x , y , z ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) $ ; propagate each such plane wave from the $z=0$ plane to the general $z$ plane using the plane wave solution noted in step 2 ; inverse fourier transform the propagated waves to reassemble the field at the general $z$ plane . now we make the paraxial approximation to the propagation relationship in step 2 above , i.e. we assume that the plane waves are not skewed at too steep angles relative to the $z$ axis so that $k_x^2+k_y^2 \ll k^2$ . then our two propagation equations above become the fresnel propagation integral : $$\begin{array}{lcl}\psi ( x , y , z ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \left [ \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \frac{k_z^2+k_y^2}{2\ , k} z\right ) \ , \psi ( k_x , k_y ) \right ] {\rm d} k_x {\rm d} k_y\\ \psi ( k_x , k_y ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \exp\left ( -i \left ( k_x u + k_y v\right ) \right ) \ , \psi ( x , y , 0 ) \ , {\rm d} u\ , {\rm d} v\end{array}\qquad ( 2 ) $$ now witness that a beam $\psi ( x , y , 0 ) $ with gaussian dependence on $x$ and $y$ becomes , under the fresnel diffraction integral , $\psi ( x , y , z ) $ with gaussian dependence on $x$ and $y$ . the fourier transform of a gaussian is a gaussian , a fact which does not change when we multiply by the $\exp\left ( i \frac{k_z^2+k_y^2}{2\ , k} z\right ) $ kernel in the fresnel diffraction integral , and of course a gaussian is recovered by the inverse fourier transform . furthermore , thin lenses can be thought of simply as phase masks , i.e. a field whose transverse variation is $\psi ( x , y ) $ passing through them is transformed by : $$\psi ( x , y ) \to \exp\left ( -i\ , k\ , \frac{ ( x-x_0 ) ^2+ ( y-y_0 ) ^2}{2\ , f}\right ) \psi ( x , y ) \qquad ( 3 ) $$ so the gaussian form is preserved by all the diffraction and lensing operations . best of all , for gaussian beams the diffraction integrals split up into a product of separate $x$ and $y$ dependences and these separate dependences are also operated on independently by the phase mask ( 3 ) , so propagation analysis can be done as a product of two decoupled one-transverse dimensional diffraction problems . therefore the propagation through a system comprising homogeneous , diffractive mediums and thin lenses can be done wholly in closed form expressions ( use mathematica though ! ) and you end up with something slightly more general than the thin lens formula that becomes the thin lens formula when the axial distances involved become longer than the rayleigh diffraction length ( of the order of wavelengths ) . this method assumes perfectly unaberrated waves . however , witness that any lens whose surface sag ( height ) near its vertex can be described by an analytic function of distance $r$ from the optical axis ( axis of symmetry ) will impart a phase mask well described by the above in the paraxial limit , i.e. by limiting the numerical aperture ( maximum skew angle ) in any field such that the support of any field in the transverse plane $z=0$ is small enough . as long as the width of the support needed to validate the analysis above stays big compared with a wavelength , the analysis above will be valid in the paraxial limit . this means it works in the paraxial limit for all lenses of practical curvatures in the neighbourhood of the optical axis .
i believe you only need one rule : " the angle of reflection equals the angle of incidence . " draw a triangle on a piece of paper . ( for this situation , the third dimension of the prism is irrelevant ) . draw two parallel lines originating from roughly the direction the apex of the prism is pointing . it is not necessary to be exactly in that direction , as you will see . those lines hit the two sides of the prism and reflect , relative to the normal to the surface , according to the rule i quoted . since you know ( i hope ! ) about complementary angles and the various theorems from trigonometry about the interior and exterior angles of a triangle , you will soon see why the formula works . the second formulation is similar , although you have added the possibility of an error term due to inexact reading of the goniometer .
it does not look possible , because the stopping power starts growing for very high energies even for non-strongly interacting particles ( see fig . 27.1 in passage of particles through matter ) . to do a best case estimate , let 's consider two cubic spaceships colliding , each of them with $1\ , {\rm mm^3}$ of volume , a density of $1\ , {\rm g\cdot cm^{-3}}$ and a number density of $10^{21}\ , {\rm cm^{-3}}$ . in general the atoms of the colliding spaceship will appear as a shower of nuclei and electrons but , for our purposes of lower bounding the deposited energy , let 's consider the colliding spaceship as a shower of muons , one for each atom . then we will have the equivalent of $10^{18}$ muons ( $10^{21}\ {\rm cm^{-3}}\cdot 10^{-3}\ {\rm cm^3}$ ) colliding with a target with an areal density of $0.1\ {\rm g\cdot cm^{-2}}$ . using the minimum ionization energy we get an stopping power greater than $1\ {\rm mev\cdot cm^2\cdot g^{-1}}$ , making each muon deposit approximately $100\ {\rm kev}$ of energy . as the number of particles in both spaceships is equal , the expected temperature of both spaceships after the collision will be around the energy deposited by each particle , $100\ {\rm kev} \approx 10^9\ k$ .
the term sector in quantum field theory typically refers to a portion of the lagrangian . for example , in the standard model lagrangian , we encounter a term , $$\mathcal{l}_{sm}= ( d_\mu \phi ) ^{\dagger} ( d^\mu\phi ) -\frac{m^2_h}{2v^2}\left ( |\phi|^2-v^2/2\right ) +\dots$$ plus others involving couplings with the higgs doublet $\phi$ , and we can refer to the subset as the higgs sector . the hidden sector features all the interactions and terms in the lagrangian which we expect exist , but have not explicitly written in the full standard model lagrangian .
yes it is . the total momentum vector of a system does not change at all ( constant length and direction ) , so the projection of it on a line ( or any function you apply to it ) will not change . projection is a linear operator , so that if you project each particle 's momentum on a line and then sum , you get the same result as summing first ( to get the total momentum ) then projecting .
the quasiparticle description of photons is well known in condensed matter physics as the frequency dependent complex dielectric constant/magnetic-permeability of a material . you are asking if this quantity can be calculated from qed by a first principles method . this seems less difficult compared to other quasiparticles , because the photon usually stays noninteracting in the medium at ordinary temperatures , except for the absorption and dispersion . there is a mismatch of scale between the photon frequency/wavelength and other quasiparticles , which travel much slower than light . even so , i did not find many papers adressing this question , because we have good measurements of the dielectric constant as a function of frequency for any material . the material is doing the calculation for you . for dilute gasses , feynman adresses the question of calculating the dielectric constant from a first principles calculation in his 1963 acta physica polonica article which introduced ghost fields . the frequency dependent dielectric constant is determined from the scattering phase shift in the forward direction , which reproduces the qualitative behavior of even crazy cases like when you pass an atomic resonance . the reference is below . rp feynman , acta physica polonica , 24 ( 1963 ) 697 some classic reference for qed in media , which include calculations of the effects of dielectric properties at finite frequencies , are these original papers : e . m . lifshitz , “"the theory of molecular attractive forces” , sov . phys . jetp 2 , 73 ( 1956 ) . i.e. dzyaloshinskii , e . m . lifshitz , and l.p. pitaevskii , ”general theory of van der waals ' forces” , sov . phys . jetp 10 , 161 ( 1960 ) these papers use the given photon quasiparticle description to calculate the van-der-waals forces for an arbitrary configuration of macroscopic matter .
what happens , essentially , is that the s and p wavefunctions get mixed to produce eigenstates that have shifted centres . this means the atom gets an induced electric dipole moment , whose interaction with the external field either lowers or raises the eigenenergy . more specifically , consider the wavefunctions of the states $|200\rangle$ and $|210\rangle$: the first is spherically symmetric , while the second has two lobes where the wavefunction has different signs . if the field is strong ( i.e. . stronger than the fine interaction , which separates these two levels , but not strong enough that levels with other $n$ get involved ) then the eigenstates will be even mixtures of these , but with different phases , as your diagram indicates . this should really be done properly , but for an intuitive picture adding equal colours amplifies them while adding opposite colours cancels them , and vice versa for substraction . if you do that , you get something like this : note in particular that the electronic centre of charge has moved from the origin , which means the states have nonzero dipole moments . with the electric field pointing downwards , the state to the left has a lower energy and the one to the right is raised . ordinarily these superpositions would " slosh " back and forth between one and the other , as the fine-structure difference in energy between them caused the p state to accumulate phase ( as $e^{-ie_{21}t/\hbar}|210\rangle$ ) slightly faster than the s state . with the field present and pointing downwards , the state to the left stays where it is because the field keeps pulling the electron back . the state to the right is forced to its high-energy position by the requirement that it be orthogonal to the first . to do this a bit more formally , you just need to state the problem a bit more specifically . saying that the field is not strong enough to mix subspaces with different $n$ amounts to saying that you are interested in the hamiltonian 's structure in the subspace $\mathcal{h}=\text{span}\{|200\rangle , |210\rangle , |211\rangle , |21-1\rangle\}$ . the hamiltonian is then fully described by its matrix elements $$ \langle 2l'm'|\hat h|2lm\rangle=e_{2l}\delta_{l'l}\delta_{m'm}+\int \mathrm{d}\mathbf{r} \ \psi^\ast_{2l'm'} ( \mathbf{r} ) e_0 z\ \psi_{2lm} ( \mathbf{r} ) . $$ these integrals you should be able to find yourself , but there are selection rules ( which amount to the parity properties of the above integral ) that require most elements to be zero . the hamiltonian is thus reduced to the form $$\hat h = \begin{pmatrix} e_{20} and e_0 d and 0 and 0\\ e_0 d and e_{21} and 0 and 0\\ 0 and 0 and e_{21} and 0\\ 0 and 0 and 0 and e_{21} \end{pmatrix} $$ with the basis ordering as above and where $d=\int \mathrm{d}\mathbf{r}\ \psi^\ast_{200} ( \mathbf{r} ) z\ \psi_{210} ( \mathbf{r} ) $ can be assumed real . having done that , saying that the field is strong w.r. t the fine interaction means enforcing the condition that $e_0 d\gg|e_{21}-e_{20}|$ , i.e. working in the approximation that $e_{21}\approx e_{20}$ . in this approximation the hamiltonian in the subspace $\text{span}\{|200\rangle , |211\rangle\}$ is ( up to a change of origin in energy ) of the form $\begin{pmatrix}0 and 1\\1 and 0\end{pmatrix}$ and the eigenstates are as above . ( if the field is not quite as strong then there will be small off-diagonal terms , and the mixing will not be quite as good . for the details , diagonalize ! it is relatively easy as it is a simple two-level problem . ) this is really all the formalism one can put into it without overdoing it . to do the problem more rigorously you should expand the physical boundaries of the problem : if the field is weak , consider the effects of fine and even hyperfine structure ; if the field is strong then consider mixing of different principal subspaces , and work to higher than first order . both directions mean quite a bit of work and they do alter the results you quote . ( they have to ! )
yes . from clausius theorem the following inequality can be deduced : $$\delta q \le tds$$ where the equality holds in the reversible case . so , a reversible adiabatic process is necessarily isentropic , but irreversible adiabatic processes are not so . to put it in another way , in an irreversible process , according to the above inequality , either entropy changes , or heat must be somehow removed from the system to make it possible to have zero change in entropy . so an irreversible isentropic process can not be adiabatic .
as mentioned in the comments , you need one more piece of information to determine the magnitude of the velocity . you said that you might use the eccentricity , so in that case you can use the formula given here and deduce a quadratic equation on the velocity which yields : $$ v= \sqrt{\frac{g m}{r \sin ( \alpha ) } ( 1 \pm \epsilon ) } , $$ where $g$ is the gravitational constant , $r$ is the distance between the two masses , $m$ is the bigger mass ( i assumed here that one mass is much bigger than the other ) , $\alpha$ is the angle between the velocity vector and the radius , and $\epsilon$ is the eccentricity . note that since we had a quadratic equation , you still have two options for the velocity , both consistent with the given eccentricity .
if you do not want to use statistichal mechanics , you can view it as a completely mathematical thing . when you write the differential form $\delta q$ , you are not speaking of an exact differential , i.e. it is not really the differential of any function of the thermodynamical state . temperature is , in this case , called the integrating factor , which means that $\delta q/t$ is an exact form , in particular , it is $ds$ , the differential of entropy . this is a way to let entropy come out . on the other hand , much more physical explanations can be given . the first uses obviously stat mech , but without making calculations , i can just tell you that $s$ turns out to be very closely connected with the number of possible microscopical states a thermodynamical ( thus macroscopical ) state can admit . finally , a reason is that the quantity $\int \delta q/t$ is never negative in normal thermodynamical transformations , that is , it allows a simple formulation of the second principle . i hope this is what you were looking for .
the " wave " part of the wave-particle duality for particles such as electrons and protons ( as opposed to em radiation ) is called their wavefunction . it does not have any classical analogue and any attempt at understanding it using classical intuition can only be a crude analogy . however , if you are happy with the concept of a probability wave then it is exactly that . why is not this a problem with the uncertainty principle , then ? well , there is a corresponding uncertainty principle between the wavelength of any wave ( or more precisely its wavenumber $k=2\pi/\lambda$ ) and its position in space . the wavelength of a wave is only precisely definable , to arbitrary precision , if you have an infinite wave ; otherwise , you can only measure a finite number of periods and divide , and that will yield an imprecise measurement of $\lambda$ . ( even worse , the amplitude will taper out near the edges , so it will be hard to tell where each peak or trough is . ) to have a better-defined wavelength , then you need a bigger wavepacket , but this means that the position of the wavepacket in space , which only makes sense to a precision of the wavepacket size , has bigger uncertainty . this trade-off game can be expressed as $$\delta k\delta x \gtrsim1 , $$ and can be made precise using fourier analysis of waves .
i would suggest looking at the formalism of floquet space . the basic idea is that one uses a time-independent but infinite dimensional hamiltonian to simulate evolution under a time-dependent but finite dimensional hamiltonian by using a new index to label terms in a fourier series . a good , short introduction can be found in levante et al . for more details , leskes et al provides a very through review . finally , a simple example of an application of floquet theory is given by bain and dumont .
what i am missing in your question , is the dimension of $\lambda$ . you can see from your second formula , that the dimension of $\lambda$ is $\text{m}^{-2}$ ( because of the laplacian ) . further , to add a bit more information , normally we would write your eigenvalue problem as $$\delta v = - k^2 v . $$ we use $k^2$ here mostly because it will make the answer look nicer ( no square roots ) . we then call $k$ the wavenumber
you are right that gravitational lensing , and really any large-scale stuff involving gravity , tells us directly about spacetime rather than the stuff out there . but spacetime , according to gr , is constrained by the mass , energy , momentum , pressure , and shear . the equation we write down is $$ g_{\mu\nu} = 8\pi t_{\mu\nu} , $$ which unfortunately means almost nothing if you do not already know gr , but it just feels so good to write down people like me can not help it sometimes . basically , the left side encompasses curvature and all that , while the right deals with the " stuff . " inferences about curvature therefore tell us something about the " stuff " there . our observations tell us there is a lot of " stuff " that has mass , moves slowly , and does not interact with light . we call it dark matter . once you believe there is something with mass out there , quantum mechanics tells us we can probably think of it as a particle ( i.e. . an excitation of a quantum field , to be pedantic ) on some level . beyond this , our models of cosmology work incredibly well when we postulate that there are particles out there that have mass and only interact via the weak force . these models include everything from the details of the cosmic microwave background to the clumping of matter into galaxies on large scales . could you come up with some other explanation ? perhaps . but there is a lot of evidence coming from different directions pointing toward wimps of some sort .
if i remember correctly they only do this in the turns and they use both arms in the straights . it is the outer arm that is active . this helps them turn in two ways . it helps accelerate the outer side more than the inner which is what is what turning really is . the reaction force at the shoulder also helps them lean into the turn which helps them stay stable through the acceleration of turning . this the same reason why bicycles and motorcycles lean into turns and why race tracks for some sports are banked . the mechanics of swinging arms in skating are similar to those in running and even walking . the main benefit is during the back/down part of the stroke . since the arm is moving back , down and also out , there is an opposing forward , up and inward reaction force on the torso at the shoulder . the forward component helps with acceleration , upwards helps extend the stride and the inward helps with balance . doing this only on one side helps turn to the opposite side as described above . there are ( at least ) two reasons why the return part of the arm stroke does not completely undo the benefit of the first part . one reason has to with timing . the active part of the stroke occurs while the foot on that side of the body is off the ground and the other foot is planted so it is helping propel the side of the body that is up moving and there is a longer lever from the shoulder to the fulcrum at the planted foot on the far side . during the return stroke that side of the body is on the ground so it does not slide back due to returning forces . also the leverage from the shoulder to the planted foot is shorter . when the opposite foot is off the ground the return stroke actually helps move the opposite site of the body forward . a simple experiment to demonstrate these effects is to swing just one energetically arm back and forth while standing on one foot and then on the other . observe the different ways your body responds to swinging the same arm in while standing each foot . the other possible reason the return strong does not undo the benefit of the initial stroke is that the arm can be in a more relaxed position during the return stroke so it has smaller moment of inertia and thus the reverse effect is lesser . also return stroke can be less aggressive . this is similar to the way falling cats turn themselves in mid air without violating conservation of angular momentum . i am not certain how big a role this mechanism plays in skating or running though . edit my explanation is only valid in cases where the skaters move as i described . according to @pulsar below , this may not be universally true . however , arms do go out of phase with legs and both arm are used at least some of the time
one of the biggest failure of theoretical condensed matter and/or material sciences is that up to now , nobody has ever been able to predict what compounds will be a good superconductors . of course , since we do not really understand high-tc superconductivity , we cannot predict which ceramic will or will not be a nice superconductor . but even in the case of more standard superconductors described by bcs or more refined theories ( like eliashberg 's theory ) , the predictive power of theoretical approaches is close to zero . to summarize , all superconductors are found experimentally , and then theorists try to explain why this particular alloy/compound has these properties .
any object , whether it be magnetized or not ( and in particular whether it be ferromagnetic or para/diamagnetic ) , will only experience magnetic forces when it is placed in an external magnetic field . if you are thinking of a long , straight conductor , then the magnetic field it produces will increase as you get near it ( though it does decrease inside the conductor ) . that said , there are some configurations of currents that produce a magnetic field " outside " and but none " outside " . two examples are a toroidal solenoid , in which the field is confined to the solenoid interior and is zero in the donut hole , and two long cylindrical , coaxial solenoids carrying the same current in opposite directions , for which the field is confined to the middle region and cancels out in the inner region . a coaxial cable is an example of the latter . if you are thinking about an extended distribution of currents such as a wire of nonzero thickness , then yes , the field is zero at the centre and it is small nearby . however , you can hardly place a metallic object inside a wire .
probably not , but it depends on the geometry of your coil . for a couple of dollars at the hardware store you can get a big stack of those coin magnets . if the answer to your question were yes in general , it would be harder to break apart the big stack of magnets that to separate two of them . that is not consistent with my experience . in general for a dipole $\vec m$ in a field $\vec b$ , the force is $\vec f=-\nabla ( \vec m \cdot \vec b ) $ . if the dipole moment is a constant , and the dipoles are free to rotate , they will orient themselves so that $\vec m$ is antiparallel to $\vec b$ . in that special case , the force simplifies to $$\vec f \approx m \vec\nabla b . $$ in other words , the dipoles " want " to align antiparallel to the field , then to move up the gradient into the strongest part of the field . a dipole in a uniform field feels a torque , but not a net force . if your coil is set up to generate a uniform field , your two stacked magnets will feel the same force — but that force will be zero . if your coil is generating a magnetic field with some dipole component , your stacked magnets will rotate to align with the field , then ( since we have already assumed there is a field strength gradient ) one of them will see a weaker gradient than the other . the gradient $\vec\nabla b$ typically vary like $1/r^4$ , where $r$ is the distance from the center of the coil , so the force on the two coin magnets can vary considerably over a short distance . this is why strong magnets like to suddenly " leap " together and pinch your fingers when you are playing with them . now you could put your two coin magnets next to each other , symmetrically about the axis of your coil , and they would see the same $|\vec\nabla b|$ in slightly different directions . but with the two coin magnets next to each other , they would exert torques on each other , since they prefer to be stacked pole-to-pole ; you could engineer a solution like this , but there would be extra parts involved . i actually happened to have some magnets with this shape on my desk . in response to carl 's comment , i built a little string lifting harness to measure the force on a single magnetized paper clip : adding a second magnet increased , but did not double , the weight that the harness could hold : stacked number of clips magnets that stayed up 1 11 2 17 3 23 4 24 5 24  the dominant effect in calculating the force is the field is right at the surface of the stack of magnets ; it looks like that gets saturated , an effect i did not consider in the first part of my answer .
assuming i have understood what you are asking ( ignore this if i have not ! ) the point is that the radius of the hole is equal to the minimum radius of the air-water interface and therefore its maximum pressure . if you place the sphere just under the surface of the water so the pressure difference is almost zero then the air-water interface forms a portion of a very large sphere with the centre positioned some distance away from the sphere . as you move the sphere farther down in the water the air water interface indents more and the air-water surface radius goes down , with its centre moving closer to the surface of the sphere . when the radius of the bubble equals the radius of the hole , the centre of the air-water interface is in the centre of the hole . increasing the pressure further causes the radius of the air-water interface to grow again , which reduces the pressure , so the water floods in . so the last paragraph of your question is quite correct . there is a bubble formed at all depths .
you are on the right track ; there are just a few things i would add . the first is that you have made a slight mistake in your calculation . remember that a joule is 1kg m^2 / s^2 . 100g of h is 0.1 kg ; and since you have h and anti-h there are 0.2kg total . the total mass corresponds to an amount of energy $$ e = ( 0.2 kg ) \times ( 3 \times 10^8 m/s ) ^2 = 1.8 \times 10^{16} j $$ now about $e = mc^2$ itself . before einstein , we knew that objects could have energy due to motion ( kinetic energy ) , due to interaction with external fields ( gravitational and electrical potential energy ) , due to energy stored in internal forces ( e . g . harmonic potential energy in a spring ) , due to the kinetic energy of its microscopic constituents ( i.e. . internal energy , proportional to temperature , etc . ) . einstein added something to this list - any object with mass has a rest energy proportional to its mass given by $m c^2$ . in any process where mass is conserved ( as of the year ~ 1900 , all known physical processes ) , this energy is locked away and undetectable . einstein 's work showed that it must exist , and equivalently that processes violating conservation of mass could be possible as long as energy was still conserved . another important note : before einstein , the energy of some object ( ignoring all the potential energies and internal energy ) is $$ e = \frac{1}{2} mv^2 $$ after einstein , you do not just add $m c^2$ , i.e. , $$ e = mc^2 + \frac{1}{2} mv^2 $$ but instead $$ e = \frac{1}{\sqrt{1 - v^2/c^2}} m c^2 $$ at first this formula looks extremely different from $mc^2 + 1/2 mv^2$ , but it is equivalent at small velocities . for a small number $\epsilon$ , $$ \frac{1}{\sqrt{1 - \epsilon}} \approx 1 + \frac{\epsilon}{2} $$ which you can check with a calculator . in fact , there is a much better expression for the energy than $e = \frac{1}{\sqrt{1 - v^2/c^2}}m c^2$ . this is $$ e = \sqrt{m^2 c^4 + p^2 c^2} $$ here $p$ is the momentum of the object . for a particle with mass $m$ , this is $$ p = \frac{mv}{\sqrt{1-v^2/c^2}} $$ the reason this expression works better is that there are many particles , in particular the photons that make up light , which are massless but still carry momentum . a photon with wavelength $\lambda$ has momentum $2 \pi \hbar / \lambda$ . using the above , its energy is $$ e = \sqrt{m^2 c^4 + p^2 c^2} = p c = 2 \pi \hbar \nu = \hbar \omega $$
the answer you have been given is wrong . the energy after inserting the dielectric is $ku_0$
it is true that time slows down for someone travelling near the speed of light , so you could use this to travel into the future i.e. only age one day while the earth ages one year . however this is not really time travelling in the usual science fiction sense . for physicists time travel generally means a closed timelike curve ( ctc ) . see http://en.wikipedia.org/wiki/closed_timelike_curve for a good article on this . basically it means being able to visit the same point in spacetime more than once . there are solutions of the general relativity equations that allow ctcs , but it is not clear how physical these are . einstein 's equations allow solutions that do not have anything to do with the real universe . there is a suggestion called the chronology protection conjecture that the universe does not allow ctcs , but as far as i know no-one has proved this . as of right now no-one knows for sure if ctcs , and therefore time travel , are possible or not , though i suspect most physicists think not . i did not see the discovery channel programme you mention , but as far as i know no-one has sent anything back in time whether it is a photon or anything else .
turns out there are two methods that i have found out . . . there many be others : 1 ) repeat the simulation n times with different initial conditions and use the usual statistic techniques to calculate the mean and error of the variance quantities . 2 ) bootstrapping , sub sample the data as follows : randomly choose n frames from m frames , where m > n n [ n defined below ] . calculate quantities in the set n . repeat n times calculate statistical variances among the n sets of calculations . source : dr peter olmsted @ leeds university
sadly your tensor would have to obey , $$t_{abc} = -t_{cba} = -t_{bca} = t_{acb} = -t_{abc} , $$ and therefore would have to be equal to zero .
( 1 ) as for the ( a ) the total force of the ground/hinge ( e . g . thrust or normal force + friction ) is generally neither vertical nor horizontal . edit : you can obtain the force of the ground/hinge by calculating the force of the rope first , and then add all three forces together to get zero . as for the ( b ) you have three forces acting to beam , force of the ground , gravitational force and force of the rope . since problem suggests " considering equilibrium " , torques of these three forces must equal zero . ( 2 ) force at point b is simply the force of rod bd to rod ac ( and vice versa ) . effectively , you have three forces acting on rod ac . note also that the force of the rod bd is along its direction ( because it is limited by two joints at its ends and there is no force in between ) .
i believe you have a mistake in your formula as the self-inductance of a coil is given by $$l\approx\mu_0 \frac{n^2 a}{\ell} ; $$ here $n$ is the number of windings , $a$ is area of the cross-section , and $\ell$ is the length of the coil . your task is to maximize $l$ with the constraint that the length of the copper wire is $w$ . assuming that the solenoid is a cylinder , the cross-section read $a=\pi r^2$ with $r$ the radius of the cylinder . a solenoid with $n$ windings needs a wire of length $w= 2\pi rn$ . thus , $$ l \approx \mu_0 \frac{w^2}{\ell} . $$ we see that the inductance of the solenoid decreases with increasing length ( keeping the total length of the wire fixed ) . thus , we obtain the largest self-inductance having the smallest length which is a single loop with $n=1$ . for a single loop the formula given above is not correct ( as it assumes $\ell \gg \sqrt{a}$ ) and thus we have $$l\approx \mu_0 r \ln ( r/r ) \approx \mu_0 \frac{w}{2\pi} \ln ( w/r ) $$ with $r$ the radius of the wire .
surely . lets consider scattering of a 1-d particle on a small potential barrier . to solve the problem , we will find energy eigenstates : $$ h|\psi\rangle=e|\psi\rangle $$ set $h=p^2/2m+\epsilon u=h_0+\epsilon u$ , where $\epsilon$ is a small parameter . consider the equation $$ ( h_0-e ) |\psi\rangle=|\phi\rangle $$ let us write a solution as $$ |\psi\rangle=|\psi_0\rangle +g_0 ( e ) |\phi\rangle $$ where $|\psi_0\rangle$ lies in $e$-eigenspace of $h_0$ and $g_0 ( e ) $ is the operator with kernel ( in coordinate rep ) being the casual green function of $h_0-e$ . now we write the original problem : $$ ( h_0-e ) |\psi\rangle=\epsilon u|\psi\rangle$$ so $$ |\psi\rangle=|\psi_0\rangle +\epsilon g_0 ( e ) u|\psi\rangle $$ we want this to reduce to a free particle moving from left to right for $\epsilon=0$ , so we write in the coordinate representation $|\psi_0\rangle=\exp ( ip_ex ) , \ , p_e^2/2m=e , p_e&gt ; 0$ , and the equation : $$ \psi ( x ) =\exp ( ip_ex ) +\epsilon\int_{-\infty}^x g_0 ( e , x-x' ) u ( x' ) \psi ( x' ) dx ' $$ this is just what you want , with $k ( x , y ) =g_0 ( e , x-y ) u ( y ) , \lambda=\epsilon$ , it is a volterra second kind integral equation , and partial sums of liouville-neuman series give a perturbative solution to 1-d scattering .
if you look at the graphs for the sine and cosine functions , and know about the relation between the two : $\sin ( x ) = \cos{\left ( \frac{\pi}{2}-x\right ) }$ you should be able to understand what happened . the expressions are not completely equivalent , but both are solutions to the wave equation .
the repelling is another way of saying that owing to the strength of the hydrogen bonding between water molecules , the water molecules are better off with themselves alone as compared to with non-interacting non-polar molecules within . a substance dissolves only in a solvent , where the solvent-solute interaction is as strong ( or stronger ) than the solvent-solvent interaction and therefore the solvent finds it better ( energetically and thermodynamically favourable ) to allow the solute molecules to dissolve , i.e. take up spaces between the molecules . but if the solute-solvent interaction is poor , ( as in the case of non-polar/hydrophobic molecules ) , the solvent finds it better to be among itself and not allow the hydrophobic molecules to take up spaces between the molecules , which is equivalent to having repelled the non-polar substances , i.e. their mixing with water is energetically opposed . this is also the reason why polar substances do not dissolve in non-polar solvent . there are hydrophobic surfaces which depend on the surface energy or the contact angle of water on that surface , but the same argument cannot be extended to molecules where there is no surface to account for the surface energy .
well , it is simple . if $\omega_x/\omega_y$ is irrational , then the evolution visits the neighborhood of any allowed point in the phase space arbitrarily closely . this is pretty much a more general form of the claim that $\cos kn$ for $k$ irrational and $n\in{\mathbb z}$ may belong to any interval $r\pm \epsilon$ for any $-1\lt r\lt 1$ and arbitrarily small $\epsilon$ . so in the irrational , aperiodic case , there can not be any extra conservation law . any initial condition is , with the help of some appropriate waiting ( shift in time ) , equivalent to any other within an arbitrarily small $\epsilon$ , so the candidate conserved yet continuous quantity has to change by $o ( \epsilon ) $ where $\epsilon$ is arbitrarily small : it can not change at all . only trivial ( constant ) quantities are conserved . if the frequency ratio is rational , then the trajectory on the phase space is periodic . for example , the vector $$ ( \cos ( p_1/q_1 ) t , \cos ( p_2/q_2 ) t ) $$ where $p_i , q_i$ are integers is periodic in time $t$ with the period $2\pi$ times $q_1q_2$ ( over the greatest common divisor of $q_1 , q_2$ , if you want to make the period as short as possible , the true one ) because both coordinates are periodic with this period . it follows that the closed trajectories on the phase space are non-intersecting ( initial conditions uniquely dictate evolution ! ) compact 1-dimensional curves , topologically circles , and there exist transverse dimensions to these curves that may be used to parameterize these closed curves . these parameters labeling the curves are therefore obviously conserved quantities , by definition , because i assigned one fixed value to each full curve ( i.e. . to the points in the phase space at any value $t$ given some initial conditions ) . so the only remaining work in a particular case is to find a convenient form of these conserved parameters . note that the kepler problem of the planetary motion predicts close elliptical curves . the extra conserved quantity associated with this periodicity is the runge-lenz vector ( pointing from the center to the focus of the ellipse , and this vector may be calculated from $x , p$ at any point along the elliptical orbit ) . that is perhaps the simplest example of the concept . locally on the phase space , one may always define parameters that label the trajectories , as some parameters transverse ( or not parallel ) to particular trajectories in the phase space . but trying to extend these candidate parameters globally ends up in trouble : if we return to the same small region where we started , we get contradictory values of these parameters : they are not single-valued if the trajectory is not periodic .
the oscillatory part is nothing but thomas-fermi approximation or more riguresly , this is a version ( someone should correct me if i am wrong ) weyl 's formula regrading on how to obtain the wkb from the trace formula : you can read the 2 papers by berry and tabor on how they derived a trace formula ( like that of gutzwiller ) but to the case of integrable systems . from the derivation there you can see how the ebk pop up . . .
the mass of a quantum of a field is defined from the second derivative of the potential term $$ m^2 = \left . \frac{\partial^2 v ( \phi ) }{\partial \phi^2} \right|_{\phi=0} $$ and similar for fields with spin ( fields that are not scalar fields ) . the general form of the potential – or the whole lagrangian – is always more complicated but only this leading term determines how non-interacting wave packets and the particles of the field propagate . the higher-order terms only affect the interactions . so the potential may be expanded as $$ v = v_0 + 0\phi +\frac{m^2}{2}\phi^2+ o ( \phi^3 ) $$ here , the constant term does not matter except for causing curvature in general relativity ( the cosmological constant ) . the linear term may be set to zero by redefining the field $\phi$ additively so that its vev is $\phi=0$ , the quadratic term is the first important nontrivial term , and the higher-order terms do not affect small " waves " i.e. masses of the particles at all because the equations of motion are of the form $$\box \phi =m^2 \phi + o ( \phi^2 ) $$ and the higher-order terms may be neglected for a small $\phi$ . the quantization of the field from which the particles ' masses may be extracted effectively deals with the infinitesimal values of $\phi$ , too . to calculate the leading non-trivial ( non-constant and nonzero ) term in the potential , it is enough to linearize the dependence of all other things on the fields at the relevant point . so there is no inaccuracy introduced whatsoever .
we can consider the following model : a tube of constant temperature $t_e$ of lenght l , radius $r$ where water is flowing uniformly at a speed $v$ ( that you can obtain from your flow $p$ ) . a " slice " of water travels an interval $dx$ in a duration $dt = \frac{dx}{v}$ . the tube will contribute to the " heating " of the water by $\frac{dq}{dt} = ( t-t_e ) k 2 \pi r dx$ where $k$ is the conductivity and where we use a very simple model ( in particular for the radius , we do not distinguish external and internal radii ) . during this interval the temperature $t ( x ) $ of the water will vary by $dt = -\frac{dq}{c \rho dv}$ where $c$ is the heat capacity at constant pressure of water , and where $dv = 2 \pi r dx$ . replacing we have $\frac{dt}{t-t_e}=-\frac{k}{\rho c v} dx$ whose solution , if the temperature in the tank ( ie x = 0 ) is $t_t$ : $t ( x ) = ( t_t - t_e ) e^{ ( -\alpha x ) }+t_e$ where $\alpha = \frac{k}{\rho c v}$ . depending on the lenght of the tube you have the temperature at the tap .
i think the dominant effect might actually be the fact that the salt you add might not be at boiling temperature . but this is just based on the fact that the boiling-point elevation due to salt in water is actually quite low for typical amounts of salt used in cooking , say . i am not too familiar with the second effect you mention though .
since all the balls are accelerating together , this problem is equivalent , by the non-relativistic equivalence principle , to the problem of balls moving without gravity , or on a horizontal surface , which are free to sort themselves out according to the same force law . this reduced problem is interesting and widely studied . depending on the force law , you can get either a 1-d integrable model , or a 1-d statistical equilibrium , and both have a massive literature .
for a perfect full moon , simply change am to pm and vice versa . for other phases , there is a different offset . first and third quarters would be six hour offsets , etc . , though i would have to think a while before stating which was plus and minus . during a total solar eclipse , the times would exactly coincide , and approximately so for a new moon . ( good luck finding the shadow cast by new moonlight . ) technically , for best precision you ought to account for the equation of time and the slight change in moon phase over the course of a moon-day . also , since the moon 's orbit is inclined with respect to the earth 's , you can not use the same latitude setting all the time , but this is a frikking repurposed sundial we are talking about . if you are really after precise timekeeping , use a technology invented in the last thousand years .
in the equations as you have written them , the constant of proportionality is an outward-pointing vector for the electric field and an inward-pointing vector for the gravitational field . or in other words , if you take the radial component only : it is a positive constant for the electric force and a negative constant for the gravitational force . the details : gauss 's law for electrostatics actually says $$\iint\vec{e}\cdot\mathrm{d}\vec{a} = \frac{q_\text{enc}}{\epsilon_0}$$ and for newtonian gravity , you can write $$\iint\vec{g}\cdot\mathrm{d}\vec{a} = -4\pi g m_\text{enc}$$ for a spherically symmetric surface and mass/charge distribution , letting $\hat{n}$ represent the outward-pointing normal vector at each point on the surface , these simplify to $$\vec{e} = \frac{q_\text{enc}}{4\pi\epsilon_0 r^2}\hat{n}$$ and $$\vec{g} = -\frac{gm_\text{enc}}{r^2}\hat{n}$$ note that the constant of proportionality in the first case is $\hat{n}/4\pi\epsilon_0 r^2$ , which points outward , and in the second case is $-g\hat{n}/r^2$ , which points inwards .
just because the maximum speed is $6\pi\text{ cm/s}$ does not mean that $6\pi = 6\pi \cos ( 3\pi t ) $ . keep in mind that speed is the absolute value of velocity $x&#39 ; $ .
one alkaline aa cell has about 11 kj of energy . for a laptop battery , it is 360 kj . chevrolet equinox fuel cell has 58 mj of energy . one kilogram of tnt carries about 4.184 mj of energy . divide the numbers from the previous paragraph by this constant to see that the aa cell , laptop battery , and electric car battery have 2.6 grams , 86 grams , or 14 kilograms of tnt . note that tnt usually releases all the energy abruptly . gunpowder has 3 mj per kg or so . it means you have to add about 35% to get the right estimate for the mass of equivalent gunpowder . if you could release the energy from the batteries very quickly , the explosion could be equally devastating as the corresponding gunpowder and tnt except that batteries can not release energy this quickly .
you need to know that the $f ( x ) $ is asymptotically constant in order to conclude this , and that the potential is asymptotically zero to the left , and that the wavefunction is an energy eigenstate . then you can conclude that the energy of the eigenstate is the same as the energy of the eigenstate far away and the only energy is kinetic . $k^2\over 2m$ is called the kinetic energy . for your wavefunction , since $\tanh$ is asymptotically constant , you can conclude that there is no reflected wave , that the transmitted wave is phase shifted by the angle in the complex plane of the ratio of the two asymptotic values , $ ( -1+ik ) \over ( 1+ik ) $ , and you can conclude that the potential of scattering is that which is required to make the schrodinger equation work : $$ mv ( x ) = - {1\over \cosh^2 ( x ) }$$ where m is the mass of the particle . this is a well-known exactly solvable potential , called the paschl-teller potential . general reflectionless potential at wavenumber k you are looking for the most general potential with no reflection at wavenumber k when you make this ansatz for the wavefunction . for a general energy eigenstate of this form , $${- \psi&#39 ; &#39 ; \over 2m} = {k^2\over 2m}\psi + \psi ( - {ik\over m} {f&#39 ; \over f} - {1\over 2m}{f&#39 ; &#39 ; \over f} ) $$ so that the potential you get is $$ v ( x ) = - ( {f&#39 ; &#39 ; \over 2m f} + ik {f&#39 ; \over f} ) $$ but this is not an answer , because this potential is in general complex , while a physical potential is real . to find the constraint on f that $v$ is real , write $f$ as $e^\alpha$ and define $\beta=\alpha&#39 ; $ . then the potential in terms of $\beta$ is $$ v ( x ) = \beta&#39 ; + 2ik\beta + \beta^2 $$ and in order for this to be real , if $\beta=p+iq$ with p , q real functions of x , then $$ p = -{q&#39 ; \over 2 ( k+q ) }$$ this , for any choice of q , with the appropriate boundary condition , give the class of potentials reflectionless at wavenumber k . the paschl-teller potential is exceptional for being reflectionless at all k simultaneously .
migdal 's book recommended in comments is good . if you cannot find it , some migdal 's problems can be found in the standard textbook l.d. landau and e.m. lifshitz , quantum mechanics , non-relativistic theory , §41 . transitions under a perturbation acting for a finite time . there are five problems considered in the end of the section : a uniform electric field is suddenly applied to a charged oscillator in the ground state . determine the probabilities of transitions of the oscillator to excited states under the action of this perturbation . the nucleus of an atom in the normal state receives an impulse which gives it a velocity $v$ ; the duration $\tau$ of the impulse is assumed short in comparison both with the electron periods and with $a/v$ , where $a$ is the dimension of the atom . determine the probability of excitation of the atom under the influence of such a " jolt " ( a . b . migdal 1939 ) . determine the total probability of excitation and ionization of an atom of hydrogen which receives a sudden " jolt " ( see problem 2 ) . determine the probability that an electron will leave the $k$-shell of an atom with large atomic number $z$ when the nucleus undergoes $\beta$-decay . the velocity of the $\beta$-particle is assumed large in comparison with that of the $k$-electron ( a . b . migdal and e . l . feinberg 1941 ) . determine the probability of emergence of an electron from the $k$-shell of an atom with large $z$ in $\alpha$-decay of the nucleus . the velocity of the $\alpha$-particle is small compared with that of the $k$-electron , but the time which it takes to leave the nucleus is small in comparison with the time of revolution of the electron ( a . b . migdal 1941 , j . s . levinger 1953 ) . another application , which is very close connected to the problem 4 , is the so called molecular/atomic effects in tritium beta decay . one can measure the electron neutrino mass by studying the $\beta$-electron energy spectrum in the process $\ , ^{3}h\to \ , ^{3}he^{+}+e^{-}+\bar{\nu}_{e}$ near the end-point , where the electron energy is very close to 18.6 kev . the electron is very fast so it can hardly influence all environment around . thus the main effect is the sudden change of the charge of nucleus . there were a lot of studies of possible molecular excitations due to such sudden perturbation . google finds a good thesis about this subject : natasha doss , calculated final state probability distributions for $t_{2}$ $\beta$-decay measurements .
the electric field energy is indeed positive for the case of two opposite charges , but it is smaller than the electric field energy when the charges are separated . the difference is the potential energy of interaction of the two charges , which includes the self-energy , the field energy for a single charge , inside the mass of the charge . you can see that the energy is decreasing when you bring to equal charges together , because when they are right on top of each other , the field is zero . to see mathematically that the decrease is the potential energy , consider the energy integral $$ \int |\nabla \phi|^2 dv = - \int \phi \nabla^2 \phi = \int \phi \rho $$ where the first equality is an integration by parts , and $\rho$ is the charge density . when you can write the field $\phi$ as a sum of two contributions from two separate small charged spheres , the pontential energy of the mutual interaction is given by the charge of one times the potential generated by the other . there is also the self-pontential , which is the energy of the charge in its own field , which is classically divergent for a point charge , and can be neglected because you assume the charge radius is unchanged as the particles move around , so that this energy is constant .
i am not sure what you mean by medium here , but i believe i can still provide an answer to your question . a fuel-thruster works by pushing the fuels reactants back and thus pushing the thruster forward . a human cannot swim in the vacuum because there arms do not push anything back . in water , a human can swim by pushing water back and thus pushing the human forward . by your use of words , i guess you can consider the " fuel " in this case as a " medium " for the reactive force . in answer to your first question , which i interpret as " does a force require both something that caused the force and something that receives the force " , the answer is yes . however , we can write quantities , such as the potential , that only depend on a source .
according to wigner , the wave function of a quantum particle can be multivalued , i.e. , can acquire a nontrivial phase around a closed loop . a phase is nontrivial when it cannot be removed using a gauge transformation by $e^{i \alpha ( \theta ) }$ , with a true function $\alpha$ , i.e. , $\alpha ( 2\pi ) = \alpha ( 0 ) $ . the wave functions having this property are sections of nontrivial line bundles over the configuration manifold . the reason that a wave function is not required to be a true function is because its overall phase and magnitude are nonphysical ( if one defines quantum expectations as : ) $&lt ; x&gt ; = \frac{\int \psi \hat{x} \psi}{\int \psi \psi}$ such wave functions arise when the configuration manifold is not simply connected with a nontrivial cohomology group $\mathcal{h}^{1} ( m , \mathbb{r} ) $ ( this is the case of the circle ) . in this case , there will exist vector potentials on the manifold which are not the gradients of a true function on the manifold . $a \ne d\alpha ( \theta ) $ . with , $\alpha ( 2\pi ) =\alpha ( 0 ) $ . however , there is no need for the flux to be quantized as the wave function needs not be a true function on the configuration manifold . on the contrary , if the flux had been quantized , then no aharonov-bohm effect would not have observed . a quantization condition occurs when $\mathcal{h}^{2} ( m , \mathbb{r} ) $ ( the dirac quantization condition ) , but this is the case of a particle moving on a sphere rather than on a circle . however , this is not the case in superconductivity . the difference between the two situations lies in the fact that the " macroscopic wave function " of a superconductor is not a " wave function " . i.e. , it is not the coordinate representation of a state vector in a hilbert space . it is a quantum field describing goldstone bosons ( cooper pair ) of the superconducting phase ( usually called an order parameter ) . the modulus of the macroscopic wave function $|\psi ( \theta ) |^2$ describes the number density operator of the goldstone bosons . its two point functions describe the ( long range ) correlations . this quantum field couples minimally to electromagnetism , and this is the reason why its equation of motion is similar to the schrodinger equation of a particle coupled to electromagnetism . but the main difference this field is a true scalar field and not a section of a line bundle . this gives us the reason why the phase it acquires in a full loop should vanish because otherwise for example , its correlation functions would depend on how many times the circle was wrapped .
no . i can take a ball and swing it back and forth periodically with my hand . the motion is periodic , but the situation is not conservative - my body generates a lot of heat . a simple mathematical example is a forced , damped harmonic oscillator . it has a steady-state periodic solution that dissipates energy . if you want to know whether a force field is conservative , take its curl . time-independent force fields ( force is a function of position but not time ) are conservative iff their curl is zero .
the main idea behind polaroid sunglasses is that reflexion from water , snow and other glary reflectors is mainly polarized in one direction . to understand this , witness the behaviour foretold by the fresnel equations ( the graph below taken from the wikipedia " fresnel equations " page ) : so that you can see for a wide range of scattering angles from these surfaces , the reflected light reaching your eyes is mainly in the $s$-polarized direction ( electric field vector orthogonal ( "senkrecht " in german ) with the plane of polarization ) , so if you quell this polarization , you get rid of most of the glare from these surfaces . why are your lenses twenty degrees off in their polarization axes ? i would say that this is a simple question of production economics . the power through a polaroid varies like $ ( \sin \theta ) ^2$ , where $\theta$ is the angle between the actual polarizing axes and their ideal directions for quelling a given linear polarization . this functional dependence is very flat for a wide angle range around the null , so , if there is a twenty degree error , the attenuation ratio is still 0.1 . so a polarizer that is twenty degrees off is still almost as good as an ideally aligned one for the lower-the-glare-in-human-sight application . therefore , a manufacturer simply will not go to the extra cost of the quality control needed to align the polarisers more accurately : it really would not make the product any better for the application at hand .
in quantum field theory and its extensions including string theory , the electric charge is a generator of a $u ( 1 ) $ symmetry which should be promoted to a local symmetry i.e. gauge symmetry . in string theory , the $u ( 1 ) $ symmetry and the gauge field often appear as parts of the low-energy effective action . this could be enough to answer the question : we reduce the problem to the same problem in the approximate theory - quantum field theory . except that we do not have to end at this point . string theory produces many geometric pictures how to " imagine " or " visualize " the electric charge . those " visualizations " are often dual to each other : it means that even though these ways to present the charges superficially look totally different , one may actually demonstrate that their physical implications are totally equivalent and indistinguishable . kaluza-klein theory the oldest picture embedded in string theory goes back to 1919 and a discovery by theodor kaluza , later refined by brilliant physicist oskar klein . five-dimensional general relativity , with the new dimension compactified on a circle , produces $u ( 1 ) $ electromagnetism aside from the four-dimensional general relativity . the mixed components of the metric , $g_{\mu 5}$ , may be interpreted as the gauge field $a_\mu$ in the large dimensions . the isometry rotating the circle ( compact fifth dimension ) at each point is interpreted as the $u ( 1 ) $ gauge symmetry . and charged particles are particles that carry a momentum in the new , fifth direction . by quantum mechanics , the momentum has to be quantized ( for the wave function to be single-valued ) , $p=q/r$ , where $r$ is the radius of the circle ( $2\pi r$ is the circumference ) and $q$ is an integer that may be identified with the electric charge . a particle with the opposite charge is simply a particle that moves in the opposite direction along the hidden circular dimension . this works not only for strings but even for point-like particles in higher-dimensional spacetimes . windings string theory offers a special , more intrinsically stringy origin of the charges , too . closed strings may wrap around a non-contractible loop in spacetime - such as the circle from the kaluza-klein theory . they obey boundary conditions on the string : $$ x^5 ( \sigma+\pi ) = x^5 ( \sigma ) +2 \pi r w . $$ those $w$ times wound strings would not exist in a theory without strings . the winding number $w$ - how many times the string is wrapped around the circle - is interpreted as another type of charge . $b_{\mu 5}$ , a component of an antisymmetric tensor field , is interpreted as a new gauge field $a_\mu$ for this $u ( 1 ) $ symmetry . the oppositely charged particles are strings wrapped in the opposite direction ; to be distinct , the closed strings have to be oriented ( carry an arrow ) . this winding number origin of the charge is equivalent to the kaluza-klein origin by an equivalence we call t-duality . the gauge groups in the heterotic string theory combine the kaluza-klein-like charges and the winding-like charges and promote them to large non-abelian groups such as $so ( 32 ) $ or $e_8\times e_8$ . generalizations of wound strings exist for higher-dimensional branes : the total " wrapping number " of some membranes or branes around non-contractible cycles in spacetime ( homology ) are also manifesting themselves as electric charges . many non-perturbative dualities exist . some cycles on which the branes may be wrapped may be shrunk to zero size but they still exist : in those cases , the charged objects are localized in space ( the gauge field only lives on a singularity which may be extended just like a brane ) . that is the case of the ade singularities . in all cases , oppositely oriented branes correspond to oppositely charged particles . note that the orientation may be defined for the " whole world volume " so the reversal of the spatial orientation may be mimicked or compensated by the reversal of the world volume in the temporal dimension . open strings and d-branes when open strings are allowed , they can carry charges ( historically known as " chan-paton factors" ) at the end points - these are the points stuck on the d-branes . so the end points behave as quarks : if the string is oriented and carries an arrow from the " beginning " to the " end " , the beginning may be called a quark and the end may be called an antiquark . in this setup , the charges are most analogous to those of point-like particles . the world line of the quark and antiquark ( going backwards in time ) is nothing else than the boundary of the open world sheet as embedded in the spacetime . even this seemingly point-like origin of charges may be dual - exactly equivalent - the purely stringy ways to produce the charges .
as an engineer , i have had to get pricing on stuff i was not going to buy many times in the past . i doubt you will find a website which will have the prices , but you can find them yourself in a few hours work . what you do is this : call them up and ask for a " sales rep " ( abbreviation for " sales representative " but nobody calls them by the full name ) tell him/her that you are working at a physics lab ( i presume you are ) and that you need " budgetary pricing " . this is pricing that includes no discounts . budgetary pricing is not an offer to sell . so if the price suddenly goes through the roof because of a revolution in some obscure country budgetary pricing will not get them into trouble . they should give this to you without a lot of trouble as it is not secret information . i have always assumed that they do not put it on their websites because ( a ) it changes , and ( b ) it might be construed as being an offer to make a contract . and if you can not get information from their website , ask them to give you a " data sheet " . that should be all you need . hey , i will get you started . here 's a company that sells wafers for the valley : http://www.svmi.com/ here 's their extensive non silicon product line : http://www.svmi.com/non-siliconwafers.aspx here 's their contact info : tel : 408.844.7100 fax : 408.844.9470 email : sales@svmi . com now when you talk to them , do not sound like a complete idiot . make sure you know everything about their product before you pick up the phone . make yourself a temporary expert on their product line . know what size you are interested in and what level of quality , etc . decide in advance what year you are asking for ( new stuff gets cheaper with time ) , and the approximate quantity . if possible , use their website to determine the actual product number for the item you are interested in . by giving them a part number you are making their job very easy . sales people are not , by nature , inclined to annoy potential customers . make it easier for them to give you a price than it would be for them to tell you to go away . if you give them a part number it will be very easy to look up a number for you . do not waste their time by asking for them to speculate about exactly how these prices will change in the future . ( but if they are bored they might have time to talk to you about this . ) they should be able to tell you that they " expect that these prices will drop as production ramps up " , or give you a vague idea , and maybe you will get lucky . but remember that you are asking industry to do you a favor for no real good reason .
when you focus light from the sun you are actually creating an image of the sun . if the focal length of the lens is $f$ the radius of the image is given by : $$ r = \frac{r_s}{d_s} f $$ where $d_s$ is the distance to the sun and $r_s$ is the radius of the sun . the fraction $r_s/d_s \approx 10^{-3}$ , so if you choose a lens with a focal length of 10cm the radius of the image is about 0.1mm ( assuming the lens is perfect ) . the intensity of sunlight is around 1kw per square metre - the exact value depends on latitude , season , time of day , cloud cover , etc , etc so let 's just take 1kw/m$^2$ as a representative figure . all the light falling on your lens is being concentrated into the 0.1mm radius image of the sun , so if the radius of your lens is $r_l$ the power per unit area in the image is : $$ i = \left ( \frac{r_l}{0.1 mm} \right ) ^2 1kw/m^2 $$ so if the lens radius is 5 cm , which seems a fairly standard size for a lens , then the power per unit area in the focussed image of the sun is about 250mw/m$^2$ or 250,000 times the intensity of sunlight on the earth . that is why it is hot ! of course the total power is not very great , because even though the focussed light is very intense the area of the 0.1 mm image is only about $3 \times 10^{-8}$ square metres . the total power is just the area over which light is being collected ( the area of your lens ) times 1 kw . a bigger lens will capture more sunlight and focus more power .
your diagram is not correct as far as camera is concerned . actually in a multifocus camera the focal length is actively changing , which causes the field of view to change . as you can see from the image , a wide angle lens ( having small focal length ) has widest range while the reverse is true for telephoto lens .
i work with stellar models , so i thought i would chip in here . my instant reaction is that you should not worry too much : determining the age of a star is difficult and different models will disagree ( sometimes significantly ! ) on that age . how reliable is this research ? i can not see an obvious reason to doubt the conclusion . what method do they use to measure the age of such a star as methuselah ? basically , one tries to measure as many properties about the star as accurately as possible , and then find the best fitting stellar model . these models are solutions to a set of differential equations ( in time and one spatial dimension ) that tries to capture all the relevant physics that determines how stars evolve . the bulk physics is a fairly well-defined problem but there are several potentially important components that are lacking in these models . ( i will expand on this if desired . . . ) the usual difficulty here is breaking down the degeneracy between brightness and distance . that is , a distant object is fainter , so it is hard to know whether a certain object is intrinsically faint or just further away . the principal result in this paper is the hubble-based parallax measurement , which makes a big improvement on that distance measurement and , therefore , the brightness of the star . the other things they use are proxies for the surface composition and the effective temperature of the star , as far as i can see . incidentally , this is where i would suspect the tension can be resolved . if you look at fig . 1 of the paper , they show the evolution of different stars for different compositions . what you are looking for , roughly speaking , is lines that go through the observed points . that figure shows that if the oxygen content is underestimated , then the best fit is actually about 13.3 gyr , which is no longer at odds with the age of the universe . take note of table 1 , where the sources of error ( at 1$\sigma$ ) are listed . it is interesting that , not only is the star 's oxygen content the largest source of error , but even the uncertainty of the oxygen content of the sun is a contributor ! which is more likely to be wrong , the age of methuselah or the current estimate of the age of the universe ? the age of methuselah , definitely . i would describe our estimates of the age of the universe as in some way " converegent": different methods point to consistent numbers . sure , planck shifted the goalpost by 80 myr or so , but it would be a real shock to see that number change by , say , half a billion years . could relativistic effects account for some of the age ? i have no idea and have not really thought about it . since i am pretty sure this is not a big problem , i do not think relativistic effects are necessary to explain the discrepancy .
when you have a matrix $\phi = \begin{pmatrix} \phi_1\\ \phi_2\end{pmatrix}$ , with one column and two rows , and its transpose matrix $\phi^t = \begin{pmatrix} \phi_1 and \phi_2\end{pmatrix}$ , with one row and two columns , the product of the two matrix $\phi^t \phi$ is a matrix $p$ with one column and one row : $p =\phi^t \phi = \begin{pmatrix} \phi_1 and \phi_2\end{pmatrix}\begin{pmatrix} \phi_1\\ \phi_2\end{pmatrix} = ( \phi_1^2+\phi_2^2 ) $ because this matrix $p$ has one row and one column , it may considered as a scalar . for your particular problem , you have : $ ( \vec{\nabla}\phi ) ^t . ( \vec{\nabla}\phi ) = \sum\limits_{i=1}^n ( {\partial_i}\phi ) ^t ( {\partial_i}\phi ) = \sum\limits_{i=1}^n ( ( \partial_i \phi_1 ) ^2 + ( \partial_i \phi_2 ) ^2 ) $ the first equality comes from the definition of the inner product and the gradient , and the second equality comes from the definition of the transpose operation $t$ , and the manipulation of these matrices , as seen at the beginning of the answer .
the wikipedia article on the system says $\alpha$ cen a has a luminosity of $l = 1.5~\mathrm{l}_\odot$ , and that the a-b system has a period of $80$ years . at a distance of $d = 11~\mathrm{au}$ ( which is not mentioned in the wiki , so i am trusting the op has a good source for this ) , the power per unit area received at the location of b from a is $$ x = \frac{l}{l_\odot} \left ( \frac{d}{1~\mathrm{au}}\right ) ^{-2} = 0.012 $$ times that which the earth receives from the sun , which is certainly not much . now , let 's assume the dark side of the planet is indeed cold . the $80$-year period means the thermal equilibration timescale is much shorter than the timescale of variation in power received . 1 if we just consider the case when a is in opposition , the ratio of light-intercepting cross-sectional area to heat-emitting blackbody area will be $1$ , not $1/4$ as it is for a spinning planet . throw in a little stefan-boltzmann law , and you find the planet 's temperature to be $$ ( 4x ) ^{1/4} t_\text{earth} = 120~\mathrm{k} , $$ where $t_\text{earth} = 254~\mathrm{k}$ is the non-greenhouse average temperature of the earth . this is a rough calculation of course , but it shows that there is no significant heating , even under the best circumstances , from $\alpha$ cen a . this makes sense , since the distance from a to the planet is further than from the sun to saturn . thus : no , there will not be liquid water due to this effect . a quick check of the phase diagram of water assures us it is not liquid at any pressure at $-150^\circ\mathrm{c}$ . just as there is very little heat , there is very little lighting . however , the amount is not vanishingly small . as this blog points out , you can read a book on pluto just using the sun 's light , so this planet would not be completely dark to our eyes with their remarkable dynamic range . i will interpret " calendar " to mean " progression of seasons , " in which case . . . i suppose there might be some changes as certain gasses sublimate , similar to how pluto or comets start outgassing when they approach the sun . the more interesting " seasonal " variation will be spatial , not temporal . there could be a very thin strip of nice temperatures near the terminator , 2 though i would not be surprised if this moved around too much for there to be a permanent region conducive to liquid water . 1 if you think the planet might have more thermal inertia than this , consider how quickly the earth cools off as the seasons change . 2 this seems like it would make for a good science fiction setting .
look at faraday 's law $$\oint \vec e\cdot \vec {dl} = - \frac d{dt}\int \vec b . \vec {da}$$ we use the right hand rule to get the correct directions of the path and surface $\vec e$ and $\vec b$ are integrated over respectively . now suppose $\vec b$ points into the screen and increases over time . using the right hand rule , we integrate $\vec e$ around the path clockwise to to get an induced emf of v volts , say . if there was no minus sign in faraday 's law , this would imply the induced current would be driven around the loop in a clockwise direction , and the direction of its changing magnetic field would add to the original changing magnetic field , producing a postive feedback effect and the creation of energy from nothing . so the minus sign ensures the induced emf works against the change causing it , summarized in lenz 's law , and that energy is conserved .
i am not a physicist either . as i understand it , heat can be lost by conduction , by convection and by radiation , the purpose of the bottle is to reduce all three . if you half the amount of liquid , the question is whether you also half the loss of heat , or do more or less . analysis is difficult because the weak part of the bottle is the cork . if it is full , there is hot liquid near the cork that looses heat faster , and then gets conduction and convection heat fron the rest . there is also a lesser problem with the bottom , since it is an additional surface where heat can be lost . when the bottle is half full , the liquid is further away from the cork . but the air inside will conduct some of the heat ( conduction , and convection ) to the empty part of the bottle , and radiation may internally add some . if the empty part became as hot as the liquid , the heat loss would be the same as before , for a lesser mass of liquid . hence it would cool faster , if it does not get as hot , it means that some heat is lost to keep it cooler . if the bottle were homogenous ( no cork effect , no bottom effect ) , that would mean that , in addition to its normal heat loss through the side , the remaining liquid has to provide for the heat loss in the empty space above it . hence it cools down faster . the bottom is a disadvantage for the half full bottle , since its loss is the same in both case , and thus contributes comparatively more to cooling when the liquid mass is lower . now , i would need more data and/ or knowledge to analyse the effect of the cork . with a very conducting cork , the full bottle would loose heat quickly ( assuming the liquid touches it ) through convection and conduction in the liquid . with a totally insulating cork , it would at worse balance the effect of the bottom of the bottle , so that the analysis without cork or bottom would be valid . so with a reasonnably good cork , my conclusion is that a half full bottle will cool faster .
it sounds like you may have been confused because intuitively , you would think a reaction occurs after the action , in response to it . as you have found , that is not what newton 's third law is saying . the reaction force is not a response to the action . for this reason , some people do not like the statement " for every action , there is an equal and opposite reaction . " a better statement of newton 's third law is to say that forces always occur in pairs . it is impossible for object a to exert a force on object b without object b also exerting a force on object a . the two forces are simultaneous , of equal magnitude , and in opposite directions . mathematically , this is stated as $$\mathbf{f}_{ab} = -\mathbf{f}_{ba}$$
yes , the summation is taking over all possible integer value of $m=0,1,2 , . . . $ except $m=n$ . it can be easily seen by following the derivation of the first order perturbation theory . in your example $\psi_1^{ ( 1 ) }$ , it is sum over $m=0,2,3,4 , . . . $ . note that sometimes the index start from 1 instead of 0 such as infinite square well , then you should skip 0 .
the only connection between the " wave on a string " antenna and an e/m antenna is the relative position of the wave and the antenna . the way they detect waves is completely different , since " what is waving " is completely different . for e/m waves , antennas are conductors which rely on potential differences to measure the frequency and amplitude modulation of incoming waves . these potential differences come from the oscillation of the electric field , and can be measured in any sized conductor ; it need not be of a specific length . a typical configuration for an antenna ( by which i mean " dipole antenna" ) is formed by two quarter-length conductors for a half-wavelength total length . the ideal position of the conductor to receive , say , plane waves , is exactly what you would expect for the wave on a string ; you want the antenna to be perpendicular to the direction of motion . of course , there is polarization , relative angle between the electric/magnetic field vector , and receiver properties to consider , but that is the basic picture . i intend this post to be editable ; if anyone is interested and is more knowledgeable about simple antennas , please feel free .
the field inside the sphere will not be zero if it is hollow and there is a point charge in the hollowed out part . the field will be zero in the conductor , because the field is always zero in a conductor in electrostatics . what you might be refering to is that the field will be zero inside the hollow sphere if it is charged , because the charges will distribute symmetrically over the sphere .
usually " quantum liquid " refers to the ground state of a hamiltonian that do not break translation symmetry of the hamiltonian . ( in a sense , " quantum gas " = " quantum liquid " . ) " quantum spin liquid " refers to the ground state of a spin hamiltonian that do not break spin-rotation and translation symmetries of the hamiltonian .
i think you generally have the wrong formula : $t = \frac{1}{2}m\vec{a}\cdot\vec{a}+\frac{1}{2}\vec{\omega}\cdot\vec{\omega}i+m\vec{a}\cdot ( \vec{\omega}\times\vec{r} ) $ , where $\vec{a}$ is the linear velocity of the point you are calculating i around . so for the point of contact of a rotating wheel , you have $\vec{a}=0$ , as the instantaneous linear velocity of the point of contact is zero , $t= \frac{1}{2}m\vec{a}\cdot\vec{a}+\frac{1}{2}\vec{\omega}\cdot\vec{\omega}i+m\vec{a}\cdot ( \vec{\omega}\times\vec{r} ) = 0 + \frac{1}{2}\vec{\omega}\cdot\vec{\omega}i + 0 = \frac{1}{2}\omega^2i$ which should give you the same answer as before .
there are at least two ideas involved . first is that the expansion of the universe is not linear . while the big bang happened around 14b years ago , that does not mean that 13b years ago , the universe is 1/14th of its present size . current theory suggests that a large portion of the cosmological inflation ( where the universe increased by 26 or more orders of magnitude in linear dimensions ) happened within much , much less than a second after the big bang . and as another example , the current theory estimates that at the time that the cosmic microwave background was emitted ( which was about 0.5 million years after the birth of the universe , placing it about 1/30000 the current age of the universe ) , the universe is already about 1/1000 its current size ( in length ) . second is that the apparent recession of far away objects from us is not so much objects flying apart from each other . rather , it is space being added in between objects . imagine you being the photon , and two turtles ( moving slower than you ) being the galaxies . put turtle one in the first carriage of a train , and put turtle two on the 10th carriage of a train . and you start walking . say it takes you 1 minute to traverse a carriage , and it take the turtles 10 minutes . then in the case where the turtles walk away from each other , it will take you a bit under 12 mintues to get from the first turtle to the second ( you walk 10 minutes to the tenth train , and the turtle has gotten to the 11th . you walk another minute to the 11th train . the turtle is just a few steps in front of you . ) but that is not how the universe expands . the expansion of the universe is more like the following : suppose every 6 minutes , all the carriages decouple , and between each pair of the original carriages plops one more car ! so you walk for 6 minutes ( having traversed 6 cars ) , and you look up , and see that the second turtle is 8 cars in front of you ( and the first turtle is 12 cars behind ) . and you walke another 6 minutes . plop comes the extra cars , and now you are 4 cars from the second turtle and 36 cars from the turtle behind . and finally after another 4 mintues you catch up to the second turtle . from the point of view of the second turtle though , you would have travelled from a turtle that is now 40 cars away from him , while taking only 16 minutes ! this ties back into the funny idea that light emitted from an object 13b lightyear away can take quite a bit less than 13b years to get here , due to the inflationary universe . this is why cosmologists and astronomers use red-shift to measure distance , because there is no reasonable intrinsic notion of distance that is free from ambiguity : should distance be described by how far away the turtles are when you started walking ? or when you finished walking ? or the number of carriages you ( the photon ) have traversed ? instead of that , they measure it using red-shifts , which can roughly fit into this turtle-you framework as how flushed your cheek is from all that walking when you reached turtle number two . based on the redness of your cheeks , the turtles can calculate how much you exerted yourself , and thus for how long you have been traveling , and using known rules of the addition of new cars ( the value of hubble constant ) , the turtles can estimate the distances to other turtles . :- ( i am going to skip discussion of standard turtles , which are turtles from which you will always depart well rested and not flushed , nor how the turtle simiano-ferroequinologists found out about their rates of locomotive expansion . )
the truck is indeed moving up the hill , and the tires are not slipping . there are a couple of ways to see why the friction points in the direction of the motion of the truck . one way is to keep in mind some external agent must be acting on the truck to get it moving to the right . that is , in your free body diagram there has to be some force acting to the right to get the truck accelerating to the right . you might say that the engine and tires are the thing ( s ) doing this , but keep in mind , if the road was perfectly frictionless , then the truck would just sit there with its tires spinning . as we turn on friction slowly , the truck would not all the sudden start moving backwards , it would start moving forwards . there is a force responsible for this , and its friction . another way is to circumnavigate the above all together and note that the truck is pushing on the road down and to the left , hence by newton 's third law , the road must be pushing on the truck up and to the right . the force that points upward on the truck is the normal force , and the force that points to the right is the ( static ! ) friction force . edit : as per your comment you still seem confused , thats fine . in which case , forget the truck for the moment and let me build up the logic more orderly : ( 1 ) first of all , we have to agree how static friction works . picture a block on a table with a string attached to the middle of it , and you pull that string to the right . friction is what opposes the motion of a block sliding on the table , and it always points opposite the motion of the block . if the block is sliding its kinetic friction thats opposing it . if the block is not sliding , its static friction . static friction opposes whatever motion the pull intends for the object . for example , if you are pulling the block to the right on a friction-full table , and its not moving , the static friction is what points the left that opposes the motion . its critical that we agree that this is what static friction is , and how it works . ( 2 ) now replace the block with a wheel so it can rotate where the string it attached to it ( that is , it can roll if it wants to ) . if the table is ( perfectly ) frictionless , and you pull on the string to the right , the wheel will just slide as the block did . ( 3 ) now lets say we turn on friction . the very bottom of the wheel wants to go to the right , but it will not - it will stick ( i.e. . will not slip ) because of friction and the wheel will roll . remember from the block example above friction is the force on a surface that will oppose the motion of the object if there was no friction . the bottom of the wheel wants to slide to the right , so ( static ! ) friction opposes this and points to the left . if you still have further questions feel free to let me know , but let me know if you disagree with ( 1 ) , ( 2 ) , or ( 3 ) .
you just use the lens formula : $$ \frac{1}{u} + \frac{1}{v} = \frac{1}{f} $$ do the calculation for the first lens to find the position of the image , then do the calculation for the second lens taking $u$ to the distance of the first image from the second lens . make $u$ negative if the image is on the far side of the second lens .
the mass of the black hole only grows by $ ( 1-\epsilon ) m$ , i.e. the mass that has not been radiated away yet . that is guaranteed by the mass conservation . however , one must be careful about dividing mass and energy to " individual places " in general relativity ; in this case , it can be kind of done , but more detailed questions " where the mass/energy resides " could be meaningless . only the total mass/energy is conserved in general relativity ( in asymptotically flat and similar spaces ) . the local physics of electrons moving in magnetic fields etc . is always the same . the electron mass is always the same constant . to describe what electron is doing in a situation like this , go to a freely falling frame , find out what the values of the electromagnetic fields are in this frame , and use exactly the same electron mass etc . as you would use in the absence of any black hole . if you wanted to use a non-freely-falling frame ( or coordinate system ) to describe the behavior of an electron near the event horizon , you must be very careful to do it right . for example , the gravitational field near the event horizon makes the usual static coordinates extremely deformed relatively to the flat metric – a component of the metric tensor goes to zero or infinity near the event horizon – and there is a nonzero curvature etc . so i am sure that all people who think that general relativity is still essentially the same newtonian mechanics – and in between the lines , you make it likely that you belong to this set – would almost certainly make the calculations incorrectly in a curved system . that is why i am urging you to go to a freely falling frame .
i think you are on the right track . there are a couple of bits of advice you may follow : you may simply note that if $a \geq b$ , then it follows that $a = b$ is a valid solution , thus $a$ and $b$ must have the same units . therefore $\delta{p}\delta{x}$ has the same units as $h$ which has the same units as $\delta{e}\delta{t}$ . the method you used is called dimensional analysis and it is perfectly correct to use it .
i think you should try these books : 1 ) david tong : lectures on classical dynamics ( http://www.damtp.cam.ac.uk/user/tong/dynamics.htm ) 2 ) v . i . arnold , mathematical methods of classical mechanics 3 ) l . landau an e . lifshitz , mechanics
theoretically ? sure . practically ? no . the primary problem is not lack of knowledge about how to manipulate individual atoms , though this is very tricky and it might not currently possible to manipulate the right kind of atoms in the right kind of way for this sort of task . the central problem is one of scale . for an item like a milky way bar , you are talking about billions and billions of atoms . your problem would not be that the candy bar was melty , it would be that you would die thousands of years before there was enough of it put together to take a bite . also , the techniques for atom manipulation work reasonably well for placing atoms on some sort of substrate like printing circuits on silicone chips . when you start talking about things like assembling complex sugars atom by atom , the situation gets significantly more difficult .
if anyone is interesed , the answer to my question is $$i\mathcal{m}=\frac{-ie^4\epsilon_{\eta} ( k ) \gamma^{\eta}g_{\eta \delta}\gamma^{\delta}\epsilon_{\delta} ( k' ) g_{\delta \beta}\gamma^{\beta}\epsilon^*_{\beta} ( k'' ) g_{\alpha \beta}e^*_{\alpha} ( k''' ) \gamma^{\alpha}g_{\alpha \eta}}{ ( q^2+i\epsilon ) ^4}$$
yes , the field is infinite , but it is only log divergent near the plate , so that it is hard to see the divergence numerically . you can see this easily by solving the problem of a uniformly charged infinite plate , which is a 2d problem . here the charges are uniform along the negative real axis , where the 2d space is imagined to be the complex plane . this problem can be understood as follows : the 2d electrostatic field of a point charge at the origin , written as a map from c to c can be written in complex form as : $$ e_x + i e_y = \frac{z} { 2\pi |z|^2 }= \frac{1}{ 2\pi \bar{z}}$$ it points radially outwards . this is a pure antiholomorphic function , except at the origin . it is more familiar to deal with holomorphic functions , so conjugate it ! $$ e_x - ie_y = e ( z ) = {1\over 2\pi z} $$ now you want to superpose all the charges on the negative z axis . this is a simple integral : $$ \int_{-\infty}^0 e ( z-a ) da = - {1\over 2\pi} \log ( z ) $$ where i threw away an infinite additive constant ( you should think of this as calculating the potential difference between the point z=1 and any other point ) . this is the function with a given fixed cut discontinuity on the negative real axis . so at the point $r , \theta$ , the electric field is $$ e_x = - {\rho\over 2\pi} \log ( r ) $$ $$ e_y = {\rho \theta\over 2\pi} $$ where i have restored the $\rho$ . the part in the y-direction is finite , as your intuition says--- the discontinuity is equal to the charge density ( this charge density is the cut discontinuity of the electric field analytic function , which is a way of making it obvious that the electric field goes as the log--- the log function as a constant cut discontinuity ) . the divergent part is in the x direction , and it is only invisible in the bulk disk because when you get close to the surface , you have cancellations from the left and from the right that wash it out . so the answer is yes , the e field is log divergent , but only the component in the plane of the disk pointing out . the solution of the disk asymptotes to the plane solution in the near disk limit .
it is a perfectly well-defined expression because the tensor product is a linear space . the vectors $|v\rangle\otimes |w\rangle$ form a basis of the whole tensor-product vector space , so any vector ( including bell 's state ) in this space may be written as linear combinations of such basis vectors . $$ |\psi \rangle = \sum_{ij} c_{ij} |v_j\rangle\otimes |w_j\rangle $$ because the operators are linear and we know how to act on each term , the result of the action of the operator $l$ is $$ l |\psi \rangle = \sum_{ij} c_{ij} l |v_j\rangle\otimes |w_j\rangle $$ where your formulae already say how to evaluate the individual terms e.g. for $l = e\otimes i$ . the natural inner product of two vectors on the tensor product space is given by the simple product of the factors . choose a basis like above , and write the inner product of two basis vectors as products in the most straightforward way . $$ \langle v_i| \otimes \langle w_j| \cdot |w_k\rangle \otimes |v_m\rangle = \langle v_i|v_m\rangle \cdot \langle w_j|w_k\rangle$$ this again defines the inner product for any two vectors , by linearity . decompose each of the two general vectors in the tensor product space that enter the inner product as a linear combination of the simple $vw$ basis vectors above , apply the distributive law to calculate the inner product of each term , and sum the terms with the same coefficients .
as chris and leftaroundabout said , these are really independent things . the $1/r^2$ law comes from the fact that as the energy in the sound wave travels away from its source , it gets spread out over a larger and larger sphere . since the total energy must remain the same ( assuming no dissipation ) , the amount of energy in each unit of area on the sphere must decrease . the rate it must decrease is $1/r^2$ . so this law is really a result of conservation of energy . the other law says what happens if there is dissipation . then as the energy travels through the medium ( say , air ) , then some of the energy in the sound will be lost to heat . the way it works is that after a sound wave has traveled a distance $l_d$ , the amplitude of the wave decreases by a factor of $e$ . putting these two together , we expect intensity $i$ as a function of distance to the source $r$ to have the form $i ( r ) = i_0 r_0^2 \frac{e^{-2r/l_d}}{r^2} . $ now why did not your teachers tell you about this . well if you calculate $l_d$ according to the formula on wikipedia , you get about six miles . so four sounds that you ordinarilly hear , it is not an important effect . now as leftaroundabout said , the attenuation law was derived assuming a plane wave , but i am fairly certain that you will get the functional form i said for the spherical wave . the decay length might be slightly different , but it will just be an $o ( 1 ) $ correction .
with a lot of intuition you are well suited to all areas of physics . i think this is from the educational and the applying side a key difference to a lot of other sciences . for biology , chemistry , medicine and other sciences that are more concerned with the macroscopics than with the small details learning lots of facts is unavoidable . i do not think that there is any branch of physics where you can be successful by following a given path without a lot of intuition whether the idea might finally work or not . while you can ask yourself in which area your intuition is often the right answer a key point is : intuition comes from doing it and thinking deeply about it . a small remark to the last part of your question : there is hardly any problem that can be tackled by pure intuition . from my experience physicists with a very good intuition can pick the ten most likely ideas from a hundred possibilities , whereas the other ones have to try twenty or thirty ideas . only a few gifted ones working in a specific field sometimes for decades might narrow it down to just a few from intuition and experience . in the end the intuitive idea still has to be proven by experiment , calculation or both ; there is no shortcut around .
i am not an expert on analysing classical pulsators , but the thing to remember is that , given a good signal-to-noise ratio , the fourier transform will still have it is strongest amplitude at the correct period . you will just see secondary peaks at other frequencies that are integral multiples of the main frequency . i.e. overtones . plus , in many systems you can see the pulsation periods quite clearly by eye . e.g. the curves here . the first thing i would add is that this does break down a bit when the star oscillates at more than one frequency . in that case , the distribution of peaks gets more complicated , although you can still try to work out what the two primary frequencies are , and which of the other peaks are beats , overtones , or combinations of beats and overtones . as you can guess , this is not a simple process . in some systems , it is a real headache ! the second thing is that , nowadays , we can model the pulsations , and these models are able to reasonably reproduce the basic shape , which we can then fit . i do not know the details of the modelling but i have seen convincing results presented at conferences . ( a quick search did not turn up details but i will have a closer look when i have more time . ) the third thing is that , indeed , there are many other methods for trying to find the main frequency . this is a review from a conference i attended a few months ago and this is the cited paper that compares a variety of methods .
i think what would happen is that any water molecule with enough energy to escape the surface tension would escape . because there is no air to provide the water molecule with a way of turning round and going back in , it would permanently leave . this means that the highest energy molecules would selectively evaporate , lowering the average energy of the remaining water . this process is known as evaporative cooling , where you selectively remove the most energetic molecules ( and yes , it is what happens when you blow on a hot drink ) . depending on the size , shape and method of getting it into the vacuum at that temperature ( in a jar , take the jar away , blasted out a tube with gas ) it may or may not freeze on its way to total evaporation . this is assuming zero pressure in outer space and close enough to zero temperature . if you look at the pressure / temperature phase diagram for water ( wikipedia page of generic phase diagram ) it depends if the rapid drop in external pressure causes the evaporative cooling to drop the internal temperature fast enough to briefly solidify on it is way to pure gas . carbon dioxide will go from solid straight to gas at atmospheric pressure , ( smoke machines ) and , as the pressure is so low , if the water did freeze solid , it would not be for long , as it would continue evaporating in such a strong vacuum . edit : fixed spelling typos .
yes it is , because null areas ( and only null areas ) are invariant to the time-slicing , although it is not completely obvious to someone who is not day-to-day familiar with minkowki geometry . this is proved here in the first titled section of this answer : second law of black hole thermodynamics perhaps i should say how this proves the intended result : no matter what spacelike surface you slice a stationary black hole with , so long as matter did not fall in ( so that the geodesics on the black hole surface did not spread apart ) , the area you cross with the slice is invariant . a boosted observer has a tilted simulteneity plane , so this observer crosses the horizon with a different natural coordinate ( although which natural coordinate to use is not specified by gr--- you can use isotropic schwarzschild coordinates , then boost these by a naive lorentz transformation ( this keeps the asymptotic metric flat ) , and then continue a little bit into the interior by any method , this requires bending the t-coordinate away from the symmetry direction so that it can cross the horizon ) . the area of each little triangle on the crossing surface can be slid up and down to match each little triangle on another crossing surface , so the area is independent of the frame .
a colleague in astronomy had a student a few years ago who did a calculation about the possibility of primordial black holes , created in the big bang . if the size of these was just right , they could be evaporating into nothing due to hawking radiation right " now " ( scare quotes because this would necessarily include distant black holes that evaporated many years ago , whose light is just reaching us now ) . the last burst of hawking radiation for these would look basically like a faint gamma-ray burst , in which case it ought to be directly detectable . i am not sure of the current status of this-- their preliminary result was , if i remember correctly , that you might be able to test this by measuring the probability distribution for gamma-ray bursts of the appropriate size and duration , but we did not have any telescopes capable of picking them up at the time . i am not sure if that is changed or not . anyway , that would give you a direct way to detect black holes of a certain size , though they would not be around after the detection , so it might not really fit the spirit of the question . . .
i have experienced something similar . i am not 100% sure if it is the same phenomenon you are describing , but i suspect so . it happens if you use coffee that is ground too finely , and does not have to do with boiling . what happens is that the fine coffee grains block all the holes in the mesh . this means that the water is under more pressure than usual , since it can no longer pass through the plunger . because of this the water ends up escaping by forcing a small part of the mesh away from the side of the carafe and squirting out at high velocity . the reason for the high speed is just that it is passing through a small aperture - it is the same effect as when you put your finger over a hose . to prevent this from happening , you could try a coarser grind , or if you already use coarse-ground coffee , try pressing even more gently on the plunger . if you meet resistance then try lifting the plunger slightly before continuing - this should redistribute the grounds slightly and hopefully unblock the holes in the mesh . from a physics point of view it is worth mentioning that boiling would be a very unlikely response to compressing hot coffee . it is possible for a liquid to be in a " superheated " state , where it is above its boiling point yet remains liquid . when water is in such a state it can indeed boil very suddenly . but this state can only be reached if there are no nucleation sites available to allow steam bubbles to form , and the coffee grounds would probably provide excellent nucleation sites , so if the water were superheated it would boil as soon as you poured it onto the grounds . ( this would probably produce very bitter coffee . ) liquids can also suddenly boil if their boiling point decreases below their temperature - but pressing the plunger increases the pressure , which increases rather than decreases the boiling point . this is true for all liquids ( by le chatelier 's principle ) , so we would never expect boiling to result from an increase in pressure .
for visible light , it is not a hadron that is absorbing the light . the energy levels of electrons in atoms and molecules are quantized . the photon causes an electron to transition from one atomic orbital to a higher energy orbital , or from one molecular orbital to a higher energy orbital . another way matter can absorb light is through plasmon resonance .
let 's look to your own statements . first , time derivative after transformations is not equal to an " old " derivative : for $\mathbf r ' = \mathbf r - \mathbf u t = \mathbf r - \mathbf u t ' \rightarrow \mathbf r = \mathbf r ' + \mathbf u t'$ $$ \partial_{t'} = ( \partial_{t'}\mathbf r ) \partial_{\mathbf r} + ( \partial_{t'}t ) \partial_{\mathbf t} = ( \mathbf u \cdot \nabla ) + \partial_{t} , \quad ( \mathbf u \cdot \nabla ) = u^{i}\partial_{x_{i}} . $$ so , with $\nabla ' = \nabla$ , " bianchi " equations transforms to $$ ( \nabla \cdot \mathbf b' ) = 0 , \quad [ \nabla \times \mathbf e ' ] + \frac{1}{c}\partial_{t}\mathbf b ' + \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf b ' = 0 . \qquad ( . 1 ) $$ second , the form of $\mathbf {e}' ( \mathbf r ' , t' ) , \mathbf b ' ( \mathbf r ' , t' ) $ is not equal to $\mathbf e ( \mathbf r , t ) , \mathbf b ( \mathbf r , t ) $ . let 's use the lorentz force expression , $$ \mathbf f = q\mathbf e + \frac{q}{c} [ \mathbf v \times \mathbf b ] . $$ it does not depend on acceleration , so the statement that $\mathbf f ' = \mathbf f$ under galilean transformation is true . it means that $$ \mathbf e + \frac{1}{c} [ \mathbf v \times \mathbf b ] = \mathbf e ' + \frac{1}{c} [ \mathbf v ' \times \mathbf b' ] . $$ by using galilean transformation for speed , $\mathbf v ' = \mathbf v - \mathbf u$ , this equation can be rewritten as $$ \mathbf e + \frac{1}{c} [ \mathbf v \times \mathbf b ] = \mathbf e ' + \frac{1}{c} [ \mathbf v \times \mathbf b ' ] - \frac{1}{c} [ \mathbf u \times \mathbf b' ] , \qquad ( . 2 ) $$ so the statement that $\mathbf e = \mathbf e ' , \quad \mathbf b = \mathbf b '$ is not correct . so you need to find expressions $\mathbf e ' $ and $\mathbf b'$ via $\mathbf e $ , $\mathbf b$ . by rewriting $ ( . 2 ) $ , $$ \mathbf e + \frac{1}{c} [ \mathbf v \times ( \mathbf b - \mathbf b ' ) ] = \mathbf e ' - \frac{1}{c} [ \mathbf u \times \mathbf b ' ] , $$ in a reason of arbitrary $\mathbf u $ you can get the solution : $$ \mathbf b ' = \mathbf b , \quad \mathbf e ' = \mathbf e + \frac{1}{c} [ \mathbf u \times \mathbf b ] . $$ by substitution these equations to $ ( . 1 ) $ you will get $$ ( \nabla \cdot \mathbf b ) = 0 , \quad [ \nabla \times \mathbf e ] + \frac{1}{c} [ \nabla \times [ \mathbf u \times \mathbf b ] ] + \frac{1}{c}\partial_{t}\mathbf b + \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf b = [ \nabla \times \mathbf e ] + \frac{1}{c}\partial_{t}\mathbf b = 0 , $$ because for $\mathbf u = const$ $$ [ \nabla \times [ \mathbf u \times \mathbf b ] ] = \mathbf u ( \nabla \cdot \mathbf b ) - ( \mathbf u \cdot \nabla ) \mathbf b = - ( \mathbf u \cdot \nabla ) \mathbf b . $$ so the first pair of maxwell 's equations is clearly invariant under galilean transformations . let 's look to the other pair of maxwell 's equations : $$ [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e = 0 , \quad ( \nabla \cdot \mathbf e ) = 0 . \qquad ( . 3 ) $$ by using an expressions which were derived above , you can rewrite $ ( . 3 ) $ as $$ [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e ' - \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf e ' = $$ $$ = [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e - \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf e - \frac{1}{c^{2}}\partial_{t} [ \mathbf u \times \mathbf b ] - \frac{1}{c^{2}} ( \mathbf u \cdot \nabla ) [ \mathbf u \times \mathbf b ] = 0 , $$ $$ ( \nabla \cdot \mathbf e ) + \frac{1}{c} ( \nabla \cdot [ \mathbf u \times \mathbf b ] ) = ( \nabla \cdot \mathbf e ) -\frac{1}{c} ( \mathbf u \cdot [ \nabla \times \mathbf b ] ) = 0 . $$ the requirement of galilean invariance of second equation leads to te state that $\frac{1}{c} ( \mathbf u \cdot [ \nabla \times \mathbf b ] ) $ , which is not true in the general case . analogically reasoning can be used for the first equation . so the second pair of maxwell 's equations is not invariant under galilean transformations .
with quantum mechanics , you have to ask your questions very , very carefully . is it possible to have a sun-sized star in your pocket ? it depends on what you mean by " in " . do all of the atoms of the star need to be entirely in your pocket , or is it sufficient that some part of each atom 's wave function be inside your pocket ? it is possible to have any number of atoms counted as being in your pocket and they could just appear there . a nonzero probability of that could be calculated , but perhaps not contemplated . there is a small chance that a dropped coin will come to rest on its edge - a rare event that can be calculated and contemplated . there is a small chance that one atom will disappear from one side of the room and reappear and the other side . that can be calculated . the odds of a sun 's worth of atoms disappearing from nearby stars into your pocket in a second is nonzero and calculable , but probably not contemplatable . that said , if you said " is it possible there is a sun-sized star in my pocket right now ? " and a sun 's worth of particles just found themselves located in your pocket as you uttered " now " , your friends could not respond : " yes , there is very very tiny possibility this can be true ! " because they would be sucked into the black hole that just came into existence before they could speak those words . the way you ask the question will lead to different answers .
note : the blockquotes only apply with a gradual increase in acceleration starting from $0\ \text m\text s^{-2}$ . the trajectory that it makes depends almost entirely on the coefficient of friction between the two surfaces . this is because the ' net ' normal force will become less and less decreasing friction until the force : $\vec f = mg\sin\theta$ is larger than the static friction force : $\vec f = \vec n\mu$ , after which the block starts sliding down . now , the block will leave the ground if the acceleration , $\vec a_x$ , is large enough . using simple trigonometry it is found that the object leaves the ground when : $$\frac{\vec a_x}{\sin\theta} &gt ; \vec g\cos\theta$$ since the maxima of $\sin x *\cos x$ is $0.5$ , the object will instantly leave the incline . ( because $\sin\theta$ and $\cos\theta$ are positive in the first quadrant and the accelerations are of the same magnitude ) . if the static friction is large , the trajectory will look like the block jumps to a lower level . ( this is where your last diagram is completely wrong , because the net force acts down - the only force acting on it then is $mg$ ) . if on the other hand , the coefficient of friction is low , the lock will slide before it finally releases , and thus make a much longer jump . friction affects how much horizontal velocity it gains ; i.e. low friction will cause the block to ' move away ' from the incline much faster , because its velocity compared to that of the incline ( which is also accelerating ) is a lot lower . note that the block will always catch up with the incline eventually , because you defined $|\vec a|$ to be equal to $|\vec g|$ , and $$\vec g&gt ; \vec a\sin\theta$$ so essentially what happens is that every time the block touches the incline it leaves again . also do not speak about the " inertial force" ; simply call it friction . the only forces initially acting on the block are the : normal force , $\vec n=m\vec g\cos\theta$ ; the weight , $\vec w=m\vec g$ ; the force along the incline due to its weight , $\vec f=m\vec g\sin\theta$ ; and lastly friction , which is equal the previous force until that is greater than $\mu \vec n$ .
the problem is you have the wrong relations between $\{n_\mathrm{h} , n_\mathrm{he}\}$ and $\{n_\mathrm{p} , n_\mathrm{n}\}$ . every hydrogen contains 1 proton , and every helium contains 2 , so $n_\mathrm{p} = n_\mathrm{h} + 2 n_\mathrm{he}$ . the neutrons are only contributed to by helium in the accounting : $n_\mathrm{n} = 2 n_\mathrm{he}$ . inverting these relations yields \begin{align} n_\mathrm{h} and = n_\mathrm{p} - n_\mathrm{n} \\ n_\mathrm{he} and = \frac{1}{2} n_\mathrm{n} . \end{align} it looks like you got the direction wrong , in the sense that there should be fewer helium nuclei than protons or neutrons ( the 2 's are on the wrong side ) . also , " hydrogen " means ${}^1\mathrm{h}$ not ${}^2\mathrm{d}$ unless otherwise stated .
finally , i got it . the sketch is as following : in order to understand the fermionic measure , suppose that we did something suitable so that the spectrum of the appropriate dirac operator is discrete ( for exapmle , take the volume to be finite , work on compact manifold ) . let $\psi_n ( x ) $ be the eigenbasis for the operator . then we use $\det c = \exp ( tr \ln c ) $ for $c$ the transition operator . in our case it multiplies by $\exp ( \phi ) $ , so $\ln c$ is multiplication by $\phi$ . trace $\sum_n\int d\mu ( x ) \phi ( x ) \bar{\psi}_n ( x ) \psi_n ( x ) $ is regularised either by $\zeta$-regularization or by introducing $\exp ( -d^2/m^2 ) $ with $d$ the dirac operator . basically the same as the chiral anomaly in inspirehep . net/record/17430 , but we take the finite transformation instead of infinitesimal one .
let $\mathbf u ( t , \mathbf x ) $ represent the velocity vector field of the fluid . let $\mathbf x ( t ) $ denote the position of a particle moving with the fluid , then the velocity $\dot{\mathbf x} ( t ) $ of the particle at a time $t$ will be equal to the velocity of the fluid flow at the point $ ( t , \mathbf x ( t ) ) $ , namely $$ \mathbf u ( t , \mathbf x ( t ) ) = \dot{\mathbf x} ( t ) $$ now suppose that $\mathbf u ( t , \mathbf x ) \cdot\nabla h ( t , \mathbf x ) = 0$ . he want to show that this implies that $h$ is constant along the path of a particle moving with he fluid . notice that for any path $\mathbf x ( t ) $ we have $$ \frac{d}{dt}h ( t , \mathbf x ( t ) ) = \frac{\partial h}{\partial t} ( t , \mathbf x ( t ) ) +\dot{\mathbf x} ( t ) \cdot\nabla h ( t , \mathbf x ( t ) ) $$ assuming then that $\partial_t h = 0$ , and assuming that the path $\mathbf x ( t ) $ is that of a particle moving with he fluid , the equations written above imply $$ \frac{d}{dt}h ( t , \mathbf x ( t ) ) = \mathbf u ( t , \mathbf x ( t ) ) \cdot\nabla h ( t , \mathbf x ( t ) ) = 0 $$ so the quantity $h$ is constant along a flow line , as desired !
( i am copy-pasting from @mitchellporter 's comments into this community wiki . ) off the mark . in qm you have states and you have " observables " and the states give the probabilities for the observables . the states ( wavefunctions ) evolve deterministically , the probabilities only pertain to how they manifest in observables . a state can imply a 100% probability for a particular outcome , then it is an " eigenstate " of that observable . . . ' t hooft considers wavefunctions which , as they evolve , are in an eigenstate , then , after a certain period of time , another eigenstate , and which keep passing through eigenstates at a fixed rate . these " moments when the wavefunction is in an eigenstate " correspond to the discrete timesteps of the cellular automaton .
special relativity " says " that a clock with a straight worldline through two events records a larger elapsed time between the events than a clock with any other worldline through the same two events thus , the " stay at home " lc oscillator shows the most counts .
the colour charge of quantum chromodynamics is , as far as we can tell , not experimentally measurable , because of quark confinement . more specifically , quantum chromodynamical systems are always colour neutral . quarks do have a color charge , but they are always observed in groups of two ( color + anticolor ) or three ( red + green + blue ) for which the total colour charge is zero ( i.e. . " white" ) . if you were to observe a lone quark then you had be able to measure its charge and ( up to a messier conventions hassle than with electric charges ) its " sign " , i.e. colour . what you can measure , on the other hand , is the coupling between different colour charges . this has a direct equivalent in qed : electric charge carries only artificial units , and in natural units the relevant parameter is the coupling $$\alpha=\frac{e^2}{\hbar c} . $$ similarly , there is a qcd coupling constant , which is complicated as it depends on distance $\leftrightarrow$ enegy . just as $e^2$ , it is dimensionless in natural units , which are the only ones you want to be working with anyway . ( in other units , $e^2$ has units of $\text{energy}\times\text{length}$ , but i would think this does not carry over to qcd . )
for single particles , s orbitals have angular momentum quantum number $\ell = 0$ , and p orbitals have $\ell = 1$ . the magnetic quantum number $m$ runs from $-\ell$ to $+\ell$ in integer steps , so s orbitals can only have one value of $m=0$ ( hence a singlet state ) , and p orbitals can have $m=-1,0,1$ ( hence a triplet of states ) . is this what you are asking ? edit what i think your instructor meant is along the following lines . for a two-electron state , the combined wavefunction of the two electron system needs to be antisymmetric under exchange of electrons . the total wavefunction is the tensor product of the spin and spatial parts , so if one is symmetric , the other needs to be antisymmetric . note that the singlet $|\downarrow \uparrow\rangle - |\uparrow \downarrow\rangle$ , with $s=0$ , is antisymmetric under particle exchange , so the wavefunction of the electrons needs to be symmetric . so we can arrange $\psi ( x_1 , x_2 ) $ for the spatial part of the wavefunction to be symmetric . the theory of addition of angular momentum tells us that states with even angular momentum are symmetric under exchange , so in this case $\ell = \ell_1 + \ell_2 = 0 , 2 , \dots$ is allowed . for the s state ( $\ell = 0$ ) , we can just write $\psi ( x_1 , x_2 ) = \psi_s ( x_1 ) \psi_s ( x_2 ) $ where $\psi_s$ is a single-particle wave-function with $\ell_{1,2} = 0$ . for the triplet states , with $s=1$ , each spin state is symmetric under particle exchange , so we need to arrange a spatial wavefunction that is antisymmetric . allowable spatial wavefunctions have $\ell = 1 , 3 , \dots$ . the simplest is $\ell=1$ which is a symmetrized linear combination of $\psi_s$ and $\psi_p$ . the exact form of the wavefunction depends on $m_\ell$ . note though that we are doing all of this from the point of view of the spins . so that while the $s$ state has both particles individually in an $s$ state , the two-particle $p$ state has individual particles in a superposition of $s$ and $p$ states .
among the base units of the international system , the kilogram is the only one whose name and symbol , for historical reasons , include a prefix . names and symbols for decimal multiples and submultiples of the unit of mass are formed by attaching prefix names to the unit name " gram " , and prefix symbols to the unit symbol " g " ( cipm 1967 , recommendation 2 ) . bipm the reason why " kilogram " is the name of a base unit of the si is an artefact of history . louis xvi charged a group of savants to develop a new system of measurement . their work laid the foundation for the " decimal metric system " , which has evolved into the modern si . the original idea of the king 's commission ( which included such notables as lavoisier ) was to create a unit of mass that would be known as the " grave " . by definition it would be the mass of a litre of water at the ice point ( i.e. . essentially 1 kg ) . the definition was to be embodied in an artefact mass standard . after the revolution , the new republican government took over the idea of the metric system but made some significant changes . for example , since many mass measurements of the time concerned masses much smaller than the kilogram , they decided that the unit of mass should be the " gramme " . however , since a one-gramme standard would have been difficult to use as well as to establish , they also decided that the new definition should be embodied in a one-kilogramme artefact . this artefact became known as the " kilogram of the archives " . by 1875 the unit of mass had been redefined as the " kilogram " , embodied by a new artefact whose mass was essentially the same as the kilogram of the archives . the decision of the republican government may have been politically motivated ; after all , these were the same people who condemned lavoisier to the guillotine . in any case , we are now stuck with the infelicity of a base unit whose name has a " prefix " . bipm the international bureau of weights and measures ( french : bureau international des poids et mesures ) , is an international standards organisation , one of three such organisations established to maintain the international system of units ( si ) under the terms of the metre convention ( convention du mètre ) . the organisation is usually referred to by its french initialism , bipm . wikipedia
so , buckling is the bifurcation of static equilibrium . and thus : more technically , consider the continuous dynamical system described by the ode $\dot x=f ( x , \lambda ) \quad &gt ; f:\mathbb{r}^n\times\mathbb{r}\rightarrow\mathbb{r}^n . $ a local bifurcation occurs at $ ( x0 , λ0 ) $ if the jacobian matrix $\textrm{d}f_{x_0 , \lambda_0}$ has an eigenvalue with zero real part . this also happens to coincide with astm e-9 standard , section 3.2.1 that says : bucklig -- ( 3 ) a local instability , either elastic or inelastic , over a small portion of the gage length in plain english , the above mathematical formula implies : at time t , when constantly increasing a load to a column , the position of the column has more than one solution ( e . g . the column bends right or left ) . before time t , there is only one solution for the position .
i will stick my neck out and say that the answer to your question is simply " yes . " first off , these detailed thermal models are complex and hard to do , so we want confirmation from independent groups . we have that : rievers and lämmerzahl , " high precision thermal modeling of complex systems with application to the flyby and pioneer anomaly , " gr-qc/1104.3985 second , we could ask whether these results contradict previous work . the answer is basically no . previous work was simply sloppy . there is a nice talk on this topic here by toth : http://streamer.perimeterinstitute.ca/flash/a2cc528b-1d36-4a2e-af73-5f81b8b17477/viewer.html there is a long history where people did back-of-the-envelope estimates of the thermal effects and said , " look , the order of magnitude is too small to matter ! " it just turns out that the back-of-the-envelope were wrong . finally , all of this stuff is very tough to be sure of , because there are so many uncertainties about things like the degradation of the white paint on the rtgs . therefore it would be good to have independent ways of testing the hypothesis of a gravitational anomaly , without having to use the pioneer data at all . we do have these independent tests . if the effect obeyed the equivalence principle , it would have had effects on the outer solar system that are not in fact observed : iorio , " does the neptunian system of satellites challenge a gravitational origin for the pioneer anomaly ? , " gr-qc/0912.2947 it is dead , jim .
i will try to give a simple answer without going too deep into physics details . localization of light is the confinement of a light wave so that it is very intense in one particular location . usually it vanishes rapidly as you get farther away from that location . ( note that by " light wave " i am not talking about the traveling plane waves that you would use to describe a light beam ; this is more of a stationary oscillation mode . ) this crops up in the context of plasmonic nanoparticles because surface plasmons ' wavelengths are smaller than normal light waves of the same frequency , allowing surface plasmons to be confined into even tinier spaces . there are two major advantages that lead to applications . one is that confining the light into a small space allows easy manipulation of it , for example by tailoring the shape of your nanoparticle . the other is that confining the light into a small space also squeezes all the energy it carries into that small space . this is called field enhancement . you can then do processes that require a lot of electric field strength ( e . g . nonlinear things ) without needing a huge amount of light to achieve that field strength . ( i adapted some of this answer from the introduction to my doctoral dissertation . )
the event horizon is a lightlike surface , and so its area is coordinate-invariant . for a schwarzschild black hole , $$ds^2 = -\left ( 1-\frac{2m}{r}\right ) dt^2 + \left ( 1-\frac{2m}{r}\right ) ^{-1}dr^2 + r^2 ( d\theta^2+\sin^2\theta\ , d\phi^2 ) $$ the horizon suface is at $r = 2m$ of schwarzschild radial coordinate , and so at any particular schwarzschild time ( $dt = 0$ ) has the metric $$ds^2 = ( 2m ) ^2 ( d\theta^2 + \sin^2\theta\ , d\phi^2 ) , $$ which is just the metric on a standard 2-sphere of radius $2m$ . you can find the square area element explicitly as the determinant of the metric , $da^2 = ( 2m ) ^4\sin^2\theta\ , d\theta^2d\phi^2$ , and integrating : $$a = 16\pi m^2 = \frac{16\pi g^2}{c^4}m^2 . $$ the volume of the black hole is not invariant , as jerry schirmer says . if you try to apply anything the above schwarzschild coordinates above , then since the coefficients of $dt^2$ and $dr^2$ switch signs across the horizon , $t$ is spacelike and $r$ is timelike . therefore , since the black hole is eternal , it could be said to have infinite volume ( classically , but a real astrophysical black hole would have a finite but still extraordinarily high lifetime ) , as you will be integrating $dt$ across its lifetime . technically , the above argument is a bit flawed , because the schwarzschild coordinate chart is not defined across the event horizon , so one should be more careful how they are continued across the horizon ( e . g . , with kruskal-szekeres coordinates ) . but this can be made more rigorous . in another coordinate chart , e.g. , the gullstrand-painlevé coordinates adapted to a family of freely falling observers , $$ds^2 = -\left ( 1-\frac{2m}{r}\right ) dt^2-2\sqrt{\frac{2m}{r}}\ , dt\ , dr + \underbrace{dr^2 + r^2 ( d\theta^2 + \sin^2\theta\ , d\phi^2 ) }_{\text{euclidean in spherical coord . }} , $$ at any instant of time ( $dt = 0$ ) , space is precisely euclidean ; since the horizon is still $r = 2m$ in these coordinates , " the " volume is $$v_{\text{gp}} = \frac{4}{3}\pi ( 2m ) ^3 . $$ if you pick yet another coordinate chart , you may get a yet different answer .
all you need to do is set up a multipole expansion of the gravitational waveform . you will find that the monopole moment is proportional to the time derivative of the mass of the stress-energy tensor , and the dipole moment is proportional to the second time derivative of the momentum from the stress-energy tensor , both of which are conserved . thus , the first nonzero moment comes from the quadrupole moment . this is worked out in great detail in mtw .
the outcomes are not supposed to be the same . there are two ways to interpret your question : 1 . you want to calculate the kinetic energy in different reference frames . let 's think for example of a point-like body moving in a constant velocity $\mathbf{v}$ . it is kinetic energy is $\frac{1}{2}mv^2$ , but if we calculate it in a reference frame that is moving with the body , in that frame the body is at rest and we get zero . so we do not expect the same outcome when calculating kinetic energy in different reference frames . 2 . you want to stay in the laboratory reference frame , but pick different points as the axis of rotation for your calculations . here there is a subtle point we need to be aware of . it is true that some calculations involving the rotation of rigid bodies can be done in several different ways , each time picking a different axis , and still resulting in the correct outcome . for example this works if we want to calculate the linear and angular acceleration of a body when a given force is applied to it . however , if a rigid body has linear movement and rotation simultaneously , and we want to calculate it is kinetic energy , we need to be careful when using the formula : $$ e_k = \frac{1}{2}m v^2 + \frac{1}{2} i \omega^2 , $$ where $\mathbf{v}$ is the velocity of the axis point and $\omega$ is the angular velocity of rotation around that point . this is the formula you used in your calculations , but in fact it is valid only in the following cases ( and i give a proof of this below ) : when we use the center of mass as the axis of rotation . when the axis of rotation is at rest ( i.e. . $\mathbf{v}=0$ ) . when the velocity is parallel to the line connecting the axis point to the center of mass . regarding the calculations you showed in your question : when you used the fixed point of the cylinder as the axis case 2 applied . when you used the center of mass case 1 applied . when you used the moving extreme none of the cases applied , and you cannot use the above formula in this case . proof : we model the rigid body as a collection of point-like masses $m_i$ with their positions relative to the axis of rotation denoted as $\mathbf{r}_i$ . the velocity of mass $i$ is : $$\mathbf{v}_i = \mathbf{v} + \boldsymbol\omega \times\mathbf{r}_i . $$ the total kinetic energy is then : $$e_k = \sum_i \frac{1}{2} m_i v_i^2 = \frac{1}{2} mv^2 + \frac{1}{2} i \omega^2 + m \mathbf{v} \cdot ( \boldsymbol\omega \times \mathbf{r}_{cm} ) , $$ where $m=\sum_i m_i$ is the total mass , $i=\sum_i m_i |\hat{\boldsymbol\omega} \times \mathbf{r}_i|^2$ is the moment of inertia and $\mathbf{r}_{cm} = ( \sum_i m_i \mathbf{r}_i ) /m$ is the center of mass relative to the axis of rotation . we see that we need the last term to vanish in order to get the formula we want to prove , and we can get this if $\mathbf{r}_{cm}=0$ ( case 1 ) , $\mathbf{v}=0$ ( case 2 ) or $\mathbf{v} \cdot ( \boldsymbol\omega \times \mathbf{r}_{cm} ) =0$ ( case 3 ) .
if you are just working with $\hat p_{x}$ , you really only care about the integral over x , rather than the entire volume ( $d^{3}r=dxdydz$ ) . anyways , a hermitian operator is one such that $a^{\dagger}=a$ . this means that $\hat p^{\dagger}= ( \hat p^{*} ) '=\hat p$ where the prime indicates a transpose . a transpose in this case really means that the operator acts to the left . assuming the wavefunctions vanish on the integration boundary , you should be able to show that \begin{equation}\int dx \ , \psi^{*} ( x , t ) ( \hat p_{x} \psi ( x , t ) ) =\int dx \ , ( \psi^{*} ( x , t ) \hat p_{x}^{\dagger} ) \psi ( x , t ) \end{equation} which means that the momentum operator is hermitian . it may be instructive to work this out in 3d where $\hat p=-i\hbar \vec \nabla$ and the integral runs over the whole 3d volume .
in ideal circuit theory , kvl holds period . consider the series rlc circuit driven by an arbitrary voltage source $v_s$ . the canonical differential equation for the series current $i ( t ) $ is : $$\dfrac{d^2i}{dt^2} + \dfrac{r}{l}\dfrac{di}{dt} + \dfrac{1}{lc}i = \dfrac{1}{l}\dfrac{dv_s}{dt}$$ where does this equation come from ? it comes from writing the kvl equation around the loop in terms of the series current $i$: $$v_s ( t ) = ri ( t ) + l\dfrac{di ( t ) }{dt} + \dfrac{1}{c}\int_{-\infty}^ti ( \tau ) d\tau$$ now , it is true that if the assumptions of ideal circuit theory do not hold , kvl does not hold . however , understand that ac circuit analysis is under the umbrella of ideal circuit theory thus , in that context , kcl holds for ac circuit analysis . in response to a comment : then say for example , the current in a circuit with resistance and a capacitance is $i$ . then $v_r=ir$ and $v_c=i/ωc$ . but the supply voltage $v=\sqrt{v^2_r+v^2_c}$ and not $v=v_r+v_c$ . does this not violate kvl ? first , the phasor voltage across the capacitor is $\vec v_c = \dfrac{1}{j \omega c}\vec i$ . phasors are complex numbers . the sum of magnitudes is generally not equal to the magnitude of the sum : $$|z_1| + |z_2| \ne |z_1 + z_2|$$ thus , the sum of resistor and capacitor phasor voltage magnitudes is not meaningful but the sum of the resistor and capacitor phasor voltages is . let $v_1 ( t ) = v_1 \sin \omega t$ and $v_2 ( t ) = v_2 \cos \omega t$ be the time domain voltages across two series circuit elements . by kvl , the voltage across the series combination is $$v_s ( t ) = v_1 ( t ) + v_2 ( t ) = v_1\sin \omega t + v_2 \cos \omega t = \sqrt{v^2_1 + v^2_2}\cos ( \omega t - \phi ) $$ where $$\tan\phi = \frac{v_1}{v_2}$$ note that , using phasors , the above is $$\vec v_s = \vec v_1 + \vec v_1 = -jv_1 + v_2 = e^{-j \phi}\sqrt{v^2_1 + v^2_2}$$ thus , the sum of the phasor magnitudes , $v_1 + v_2$ , is not meaningful and certainly is not an application of kvl .
the torque is in the same direction in either case , so the torque wants to lift the back-wheel and flip you over the front whichever wheel you brake with , but when the braking is on the back-wheel , if you lift the back-wheel , the braking force goes away , so the torque goes away . this means that a brake applied to the back-wheel will never be enough ( unless it is applied extremely suddenly ) to flip the bike , it will just lift the back wheel and reduce the braking to zero , or else make a bouncing back-wheel .
infinite potential barrier – which is not of dirac delta form – will not allow tunneling is not quite right . a barrier where there is a finite region of infinite potential will not allow tunneling , nor will potentials with singularities going suffiently fast $\to \infty$ . but it is easy to construct a non-dirac potential with singularity that still permits tunneling ; in particular the one-dimensional singularities of the $\tfrac{1}{|r|}$ peaks as which you might model the nuclei 's coulomb potential are not much of a problem . you can basically model the ball 's cm amplitude as a bloch wave there . so , yes , a macroscopic ball can in fact tunnel through a brick wall . of course , the probability is exponentially small in the thickness , so indeed an enourmously large number of throws is required . more problematically , it is far more likely for the ball to , say , spontaneously disintegrate into two identically-shaped halfs , to develop a tight chemical connection to the wall , or perhaps catch fire .
use cgs units : grams and cm , as you asked .
to say the same thing david zaslavsky said in slightly different words , the second law implies that entropy cannot be destroyed , but it does not prevent you from moving it around from place to place . when we write the equation $\delta s = \int_a^b \frac{dq}{t}$ , we are assuming that this $dq$ represents a flow of heat into or out of the system from somewhere else . therefore $s$ ( which , by convention , represents only the entropy of some particular system ) can either increase or decrease . since we are talking about a reversible process , the entropy of some other system must change by an equal and opposite amount , in order to keep the total constant . that is , $\delta s + \delta s_\text{surroundings} = 0$ . one other thing : in thermodynamics , " closed " and " isolated " mean different things . " isolated " means neither heat nor matter can be exchanged with the environment , whereas " closed " means that matter cannot be exchanged , but heat can . in your question you say the second law " prohibits a decrease in the entropy of a closed system , " but actually this only applies to isolated systems , not closed ones . when we apply the equations above , we are not talking about an isolated system , which is why its entropy is allowed to change . i mention this because you said you are teaching yourself , and in that case it will be important to make sure you do not get confused by subtleties of terminology .
the majority opinion is that einstein was wrong . however , i see some problems with the standard quantum mechanics ( sqm ) approach that you outlined . sqm contains two major parts : unitary evolution ( described , e.g. , by the dirac equation ) and the measurement theory ( e . g . , collapse , or the projection postulate , which , loosely speaking , states that , after measurement of some observable , the system stays in an eigenstate of that observable with the relevant eigenvalue ) . both of these parts are used in your reasoning : on the one hand , unitary evolution ensures that the particles have zero total spin all the time , and the projection postulate ensures that , if particle a was measured to have spin up , it is in the relevant eigenstate . so far so good . the problem is that the two parts of sqm are mutually contradictory ( the notorious measurement problem in quantum mechanics - http://plato.stanford.edu/entries/qt-measurement/ ) . for example , unitary evolution cannot introduce irreversibility or turn a superposition of two states into their mixture , whereas the projection postulate does just that . so if you include the instrument ( and the observer , if you wish ) into the system containing particle a and then run unitary evolution to describe the measurement process , you will get a superposition of states with different spin projections and , strictly speaking , will not get a definite outcome . actually , your reasoning is close to that used in the proof of the bell theorem to prove that the bell inequalities can be violated in quantum mechanics . however , such proof contains mutually contradictory assumptions . on the other hand , there have been no loophole-free experiments demonstrating violations of the bell inequalities , so there are both theoretical and experimental difficulties with your reasoning , although it is widely used . therefore , i tend to think that your question has not been resolved yet , as its resolution may demand either resolution of the measurement problem in quantum mechanics or loophole-free bell experiments .
once the core has solidifed it can no longer generate a magnetic field . there may be some frozen in field , but i would guess that the strength of the magnetic field with decrease as more and more of the core freezes , so any residual field is likely to be small . the core of mars is thought to be frozen and it is magnetic field is negligable . anyhow , the main problem would be that without the magnetosphere to protect it the atmosphere will gradually be stripped by the solar wind and we will be left with nothing to breathe . in addition , without an atmosphere the surface will be subject to wide temperature swings ; just as the moon is in fact . so no , by any reasonable definition of the word liveable once the core has frozen the earth will not be liveable . i would not worry for a while yet though .
an electric field is said to be static if it does not change with time , i.e. the the charges that produced that field are stationary . this does not imply any constriction on its spatial dependence . in particular , no spherical symmetry is implicit in the definition of electrostatic field , and that field may not depend only on $r$ , as your example shows . this is common when you consider examples of field produced by more than one point charge , e.g. an electric dipole : $$\mathbf{e} ( \mathbf{r} ) ={3\mathbf{p}\cdot\hat{\mathbf{r}}\over 4\pi\varepsilon_0 r^3}\hat{\mathbf{r}}-{\mathbf{p}\over 4\pi\varepsilon_0 r^3}$$ depends on $\mathbf{r}$ and $\mathbf{p}$ .
i believe it is a concave spherical mirror that is very delicately made . it is possible it is not a glass mirror but a more pure material . notice that the outside is very bright : this is necessary to achieve such a projection from a mirror .
ondřej 's answer is partially correct . in reality , you actually do have to worry about how far your eye is from the " eye lens " as the secondary lens system is called in a telescope . this is due to the need to match or over fill the eye 's entrance pupil with the telescopes exit pupil . if you do not do this , you will see a small circle where the telescope is actually working , and it can limit your field of view . you can also think about the telescope in the following way : the objective lens ( the first lens or lens system ) is forming an aerial image ( a really tiny one ) near the shared focal point . the eye lens ( the second lens or lens system ) is magnifying that image , in exactly the same way that you would use a simply magnifier to observe beetles or something . so , to select an eye lens first you want the most magnification which you can achieve without distortion or blurriness . you can do this by taking your eye lens system and looking at normal objects as if it were a magnifying glass . the calculation for the magnification of the eye lens is : $$ m_{\text{eye}} = \frac{250mm}{f_{\text{eye}}} $$ where $f_{\text{eye}}$ is the focal length of the lens ( or lens system ) and $250mm$ is the near point of the average relaxed human eye . the telescope magnification is then given by $$ m_{\text{telescope}} = -\frac{f_{\text{objective}}}{f_{\text{eye}}} $$ the second thing that you have to worry about is the exit pupil matching as i stated above . a galilean telescope will always have a narrower field of view and a little circle that you can see . for a reflective or keplerian telescope , you can add field lenses to increase the field of view . this is basically what huygens or ramsden eyepieces do . since you want your eye 's field of view to be maximized , you can do this by making sure that your exit pupil is located a reasonable distance outside of the telescope system . you will have to do calculations to find this distance , i suggest buying grievenkamp 's field guide to geometrical optics for a complete description of first order optics and a decent account of aberrations .
first and foremost , the bec systems studied in detail today do not involve the formation of any bonds between atoms . bose-einstein condensation is a quantum statistical phenomenon , and would happen even with noninteracting particles ( though as a technical matter , that is impossible to arrange , but you can make a condensate and then manipulate the interactions so they are effectively non-interacting , and the particles remain a condensate ) . the " high school physics " version of what happens at the bec transition is this : particles with integer intrinsic spin angular momentum are " bosons , " and many of them can occupy the same energy state . this is in contrast to particles with half-integer spin , such as electrons , termed " fermions , " which are unable to be in exactly the same quantum state ( this feature of electrons accounts for all of chemistry , so it is a good thing ) . when we talk about a confined gas of atoms , quantum mechanics tells us that we must describe it in terms of discrete energy states , spaced by a characteristic energy depending on the details of the confinement . because of this , the two classes of particles have very different behaviors in large numbers . the lowest-energy state for a gas of fermions is determined by the number of particles in the gas-- each additional particle fills up whatever energy state it ends up in , so the last particle added goes in at a much higher energy than the first particle added . for this reason , the electrons inside a piece of metal have energies comparable to the hot gas in the sun , because there are so many of them that the last electron in ends up moving very rapidly indeed . the lowest-energy state for a gas of bosons , on the other hand , is just the lowest-energy state available to them in whatever system is confining them . all of the bosons in the gas can happily pile into a single quantum state , leaving you with a very low energy . it turns out that , as you cool a gas of bosons , you will eventually reach a point where the gas suddenly " condenses " into a state with nearly all of the particles occupying a single state , generally the lowest-energy available state . this happens with material particles because the wave-like character of the bosons becomes more and more pronounced as you lower the temperature . the wavelength associated with them , which at room temperature is many times smaller than the radius of the electron orbits eventually becomes comparable to the spacing between particles in the gas . when this happens , the waves associated with the different particles start to overlap , and at some point , the system " realizes " that the lowest-energy state would be for all the particles to occupy a single energy level , triggering the abrupt transition to a bec . this transition is a purely quantum effect , though , and has nothing to do with chemical bonding . in fact , strictly speaking , the dilute alkali metal vapors that are the workhorse system for most bec experiments are actually a metastable state-- at the temperatures of these vapors , a denser gas would be a solid . they form a bec , though , because the density of these gases is something like a million times less than the density of air . the atoms are too dilute to solidify , but dense enough to sense each others ' presence and move into the same energy state . the underlying physics is described in detail in most statistical mechanics texts , though it is often dealt with very briefly and in an abstract way . there are decent and readable descriptions of the underlying physics in the new physics for the twenty-first century edited by gordon fraser , particularly the pieces by bill phillips and chris foot , and subir sachdev .
the key output of the flrw metric is the scale factor $a ( t ) $ as a function of time . from this we can calculate the time derivative $\dot{a} ( t ) $ ( which is what the red shift measures ) then check whether or not it satisfies the equation : $$ \left ( \frac{\dot{a}}{a}\right ) ^2 = \frac{8\pi g}{3} ( \rho_{radiation} + \rho_{matter} + etc ) $$ where the etc includes dark energy and anything else you may wish to throw in . so basically the test is to measure the redshift as a function of distance . the problem is that this is extraordinarily hard to do on the scales where the matter distribution is homogeneous . there are various approaches being tried such as baryon acoustic oscillations and properties of galaxy clusters but it is still early days .
first , note that this comes from the heisenberg uncertainty principle , $$ \delta x\delta p\geq \frac\hbar2 $$ where $\hbar\approx10^{-34}$ j$\cdot$s ( i.e. . , a very small number ) . this is a constraint on the simultaneous measurements of momentum and position . if you know the position of the coin , then it can not actually be anywhere else because it is measured to be there . next , i quote sean carroll : quantum mechanics features a " classical limit " in which objects behave just as they would had newton been right all along , and that limit includes all of our everyday experiences . for objects such as cats that are macroscopic in size , we never find them in superpositions of the form "75 percent here , 25 percent there" ; it is always "99.9999999 percent ( or much more ) here , 0.0000001 percent ( or much less ) there . " classical mechanics is an approximation to how the macroscopic world operates , but a very good one . the real world runs by the rules of quantum mechanics , but classical mechanics is more than good enough to get us through everyday life . it is only when we start to consider atoms and elementary particles that the full consequences of quantum mechanics simply can not be avoided . for your coin , we can describe it correctly by classical mechanics ( meaning we can measure its position and momentum simultaneously ) , so there is no need to invoke quantum mechanics in regards to this thought experiment .
1 . hawking-bekenstein entropy of a black hole is given by $s_{\text{bh}} = \frac{kac^3}{4\hbar g}$ where $a$ is the area of the event horizon . assuming a non-rotating black hole , there holds $r_s=\frac{2gm}{c^2}$ for the schwartzschild radius , and therefore $a=4\pi r_s^2=\frac{16\pi g^2m^2}{c^4}$ , which results in $$ s_{bh}=\frac{4kgm^2}{\hbar c} $$ for rotating black holes , the calculation is basically the same , just your schwartzschild radius and the geometry you use to calculate $a$ is a bit different . your second question is not perfectly clear . if you assume the microcanonical ensemble , your entropy is proportional to the logarithm of your partition function , therefore a symmetry of the system , resulting in a degeneracy of the energy states , will increase your entropy as there are more possibilities to realize one microcanonical state .
if you start with a finite amount of gas in the inner sphere and then deposit a massive amount of energy , the molecules of the gas begin moving rapidly outwards and piling up , creating the blast wave . however , the rate at which the gas is moving outwards may not be balanced by the amount of gas molecules being created by the explosive . if this is the case , then the pressure must decrease below ambient as the molecules are pushed outwards with the blast wave . you can see this in videos of blast waves . the initial wave continues to move outwards , but the smoke/dirt/debris caused by the explosive will move outwards initially , then inwards as the lower pressure region sucks it back in towards the center . there is actually considerably banging that goes on where the low pressure behind the blast wave moves inwards and outwards until it relaxes back to atmospheric pressure . here is a great video that shows the blast and resulting banging as the pressure relaxes .
let $y_1 , y_2$ $2$ complex vectors and let $&lt ; , &gt ; $ be a complex inner product defined by $&lt ; y_1 , y_2&gt ; = \vec y_1^* . \vec y_2$ . let $\vec a$ and $\vec b$ the real and imaginary part of $\vec y$ : $\vec y = \vec a + i \vec b$ then : $$&lt ; y_1 , y_2&gt ; = ( \vec a_1 . \vec a_2 + \vec b_1 . \vec b_2 ) + i ( \vec a_1 . \vec b_2 - \vec b_1 . \vec a_2 ) = u ( y_1 , y_2 ) + iv ( y_1 , y_2 ) $$ the cauchy-schwartz inequality gives : $$&lt ; y_1 , y_1&gt ; &lt ; y_2 , y_2&gt ; ~~\ge ~~|&lt ; y_1 , y_2&gt ; |^2$$ we note that : $&lt ; y_1 , y_1&gt ; = u ( y_1 , y_1 ) $ , so we have : $$u ( y_1 , y_1 ) ~~\ge ~~ \frac{u^2 ( y_1 , y_2 ) + v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }$$ now , fixing a particular $y_1$ , we limit the set of $y_2$ to those which respect $u ( y_1 , y_2 ) =0$ . so , we have now : $$u ( y_1 , y_1 ) ~~\ge ~~ \frac{ v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }~~ ~~ ~~ ~~ ~~ ( 1 ) $$ now , take explicitely $y_2$ defined by $ \vec a_2 = - \vec b_1 , \vec b_2 =\vec a_1 , $ , we see that $\vec a_1 . \vec a_2 + \vec b_1 . \vec b_2 = 0$ , that is $u ( y_1 , y_2 ) = 0$ , so this choice is coherent with our previous hyphothesis . morevoer , we have $v ( y_1 , y_2 ) = \vec a_1^2 + \vec b_1^2 $ , and $u ( y_2 , y_2 ) = \vec a_1^2 + \vec b_1^2 $ , so we have , for this particular $y_2$ . $$u ( y_1 , y_1 ) ~~= ~~ \frac{ v^2 ( y_1 , y_2 ) }{u ( y_2 , y_2 ) }~~ ~~ ~~ ~~ ~~ ( 1 ) $$ so , we see , that the inequality $ ( 1 ) $ is effectively saturated by our choice of this particular $y_2$
if you have a smart phone with a flash , you probably have a strobe feature at your disposal . you can put a drinking straw into it and record the sound it makes , and measure the frequency in sound-editing software . imagine the fun explaining this to paramedics . similarly , a laser-pointer , a mirror , and a photo-diode cleverly hooked up to your computer 's sound card can be coaxed into doing science . finally , a laser pointer , two mirrors and knowledge of the speed of light in your room can be used to to determine the rotational speed of the fan , provided your room is big enough , and the fan is moving fast enough . if this is going to work , it will probably be the least accurate but most fun .
here is the proof that i think you are looking for . as ali remarks in his answer , the results holds true for a rigid body undergoing rotation with constant angular velocity . let $\vec r_i$ denote the position of some particle in a rigid body . suppose this rigid body is undergoing rotation with angular velocity $\vec \omega$ , then $$ \dot {\vec r}_i = \vec \omega\times\vec r_i $$ see the appendix for a proof of this . by taking the derivative of both sides with respect to time and multiplying both sides by $m_i$ , the mass of particle $i$ , we obtain $$ \dot {\vec p}_i = \omega\times \vec p_i $$ now we simply note that if $\vec f_i$ denotes the net force on particle $i$ , then newton 's second law gives $\vec f_i = \dot{\vec p_i}$ so that \begin{align} \vec\tau_i and = \vec r_i\times \vec f_i \\ and = \vec r_i\times\dot{\vec p_i} \\ and = \vec r_i\times ( \vec\omega\times\vec p_i ) \\ and = -\vec p_i\times ( \vec r_i\times \vec\omega ) - \vec\omega\times ( \vec p_i\times\vec r_i ) \\ and = \vec p_i\times ( \vec\omega\times\vec r_i ) + \vec\omega\times ( \vec r_i\times\vec p_i ) \\ and = \vec p_i\times \dot{\vec r}_i + \vec \omega\times \vec l_i \\ and = \vec\omega\times \vec l_i \end{align} this is basically the identity you were looking for . in the fourth equality , i used the so-called jacobi identity . now , by taking the sum over $i$ , the result can readily be seen to also hold for the net torque $\tau$ on the body and the total angular momentum $\vec l$ of the body ; $$ \vec \tau = \vec\omega\times\vec l $$ appendix . the motion of a rigid body undergoing rotation is generated by rotations . in other words , there is some time-dependent rotation $r ( t ) $ for which $$ \vec r ( t ) = r ( t ) \vec r ( 0 ) $$ it follows that $$ \dot{\vec r} ( t ) = \dot r ( t ) \vec r ( 0 ) = \dot r ( t ) r ( t ) ^t\vec r ( t ) = \vec\omega ( t ) \times \vec r ( t ) $$ in the last step , i used the fact that $r ( t ) $ is an orthogonal matrix for each $t$ which implies that $\dot r r^t$ is antisymmetric . it follows that there exists some vector $\vec \omega$ , which we call the angular velocity of the body , for which $\dot r r^t \vec a = \vec \omega\times\vec a$ for any $\vec a$ .
to find the bound states for the potential $$v ( x ) ~=~\left\{\begin{array}{ccc}ae^{cx} and \text{for} and x&gt ; 0 , \\ \infty and \text{for} and x\leq 0 , \end{array} \right . $$ where $a , c&gt ; 0$ are two positive constants , one should solve the time-independent schrödinger eq . with the two boundary conditions $$ \psi ( x=0 ) ~=~0 \qquad \text{and} \qquad \lim_{x \to \infty}\psi ( x ) ~=~0 . $$ this boundary value problem does only have solutions for certain discrete values of the energy $e$ .
$n$ depends on the wave frequency . for high frequencies $n\rightarrow 1$ . so for a general electromagnetic field each harmonic propagates with its own speed . edit : in order to speak of a front propagation , you have to have a whole spectrum of harmonics that exist always and everywhere . each harmonic does not have a front in space , it is their superposition who has . in other words , a front " spreads out " due to frequency dispersion of waves in material .
the action $$s= - e_0 ~ \delta \tau $$ of a relativistic massive particle is minus the rest energy $e_0=m_0c^2$ times the change $\delta \tau=\tau_f-\tau_i$ in proper time .
in particle physics the core tool for this purpose these days is root ( a few years back there were still a significant number of people using paw ( part of cernlib ) ) . both of these choices suffer somewhat from being big , heavy tools to install just to get some graphing done---we use them because they are primarily the environments in which we did analysis . a lighter and less specific ( but still surprisingly capable ) tool is gnuplot . i have also seen a lot of activity on stack overflow from people using python based tools like scipy for plotting . root also provides python bindings .
you are absolutely right that in principle each scattered photon undergoes a change in energy due to the recoil of the object it scatters off . in rayleigh scattering , that change is very small , though , and can generally be neglected . in general , when the energy of the photon is much smaller than the rest energy of the massive particle , the photon undergoes only a small change in energy . here 's a qualitative argument for why this is true . let $p$ be the momentum of the incoming photon . the momentum transferred to the massive particle is at most of order $p$ ( specifically , it is at most $2p$ ) . for a nonrelativistic particle , the kinetic energy is $p^2/ ( 2m ) $ , so the energy transferred is of this order . so the fractional energy lost by the photon is of order $$ {\delta e\over e}\sim{p^2/2m\over pc}={pc\over 2mc^2}={e\over 2mc^2} , $$ where $e =pc$ is the original photon energy . so when $e\ll mc^2$ , the change in energy is small in comparison to the original energy . one common way to express this more quantitatively is via the compton scattering formula , $$ \delta\lambda={h\over mc} ( 1-\cos\theta ) , $$ where $m$ is the mass of the massive particle and $\theta$ is the angle through which the photon scattered . this formula is usually used for much higher-energy photons scattering off charged particles , but it is derived just from energy and momentum conservation , so it applies to any situation in which a photon scatters off a massive particle initially at rest . if i have done the arithmetic right , when $m$ is the mass of an atom or molecule , $\delta\lambda$ comes out to be of order $10^{-16}$ m , which is certainly small enough to be ignored for visible light . to answer your last question , even though atoms and molecules are neutral , they are made of charged parts . the photon can and does interact with those parts .
extra-dimensional scenarios may be described as " inspired " by string theory but they are independent hypotheses and they may be true even if string theory is not . however , one has to reduce the ambitions and standards of consistency . sociologically , it is surely true that the research of models with extra dimensions has been adopted and pursued by many people who have never take studied proper string theory or taken a course in it . despite the academic independence , a confirmation of experimentally accessible extra dimensions - which is extremely unlikely to occur , due to their likely tiny size - would be a huge evidence supporting string theory because it is the only framework in which the extra dimensions actually have a justification ( many ) .
you have the right idea . let 's use index notation for convenience . we have \begin{align} p^i ( t ) = \int_{\mathbb r^3} d^3x \ , x^i\rho ( t , \mathbf x ) \end{align} and as you note , we have the continuity equation ; \begin{align} \partial_jj^j = -\frac{\partial\rho}{\partial t} \end{align} so we get \begin{align} \dot p^i = -\int_{\mathbb r^3} d^3 x \ , x^i\partial_j j^j \end{align} where i am using the summation convention for the dot product . this is the point to which you have basically gotten . the trick is now to essentialy perform an integration by parts . in other words , we use the product rule for differentiation to note that \begin{align} \partial_j ( x^i j^j ) and = ( \partial_j x^i ) j^j + x^i \partial_jj^j \\ and = \delta^i_jj^j +x^i\partial_jj^j \\ and = j^i + x^i\partial_jj^j \end{align} and plugging this into the above expression for $\dot p^i$ gives \begin{align} \dot p^i = -\int_{\mathbb r^3} d^3 x \partial_j ( x^ij^j ) + \int_{\mathbb r^3} d^3x j^i \end{align} the first term is the volume integral of the divergence of $x^ij^j$ . since we are integrating over all space , this turns into a boundary term at infinity which vanishes for any finite ( or sufficiently rapidly decaying ) charge density . the result you want then follows \begin{align} \dot p^i = \int_{\mathbb r^3} d^3 x j^i \end{align}
these statements show great confusion in the concepts of modern physics . i read that the reason solids emit continuous spectra is that they do not have time to let their electrons decay-they are too close together . it is confusing to be talking of time with respect to emissions and you give no link . to start with at the atomic level , in any phase of matter , gas , liquid , solid , plasma , the framework is quantum mechanics . quantum mechanics works with potentials of the electrons in the atom , and between atoms/molecules and with the intermolecular van der waals forces in the lattice of solids . gas atomic spectra come from excitations of the electrons and possible vibrational transitions of the atoms as they move in the gas scattering off each other . note the high excitation values needed from the power source , 5000 volts . high excitation values are needed to see emission spectra from solids too , but long before the input energy reaches the atomic level energies needed to excite the electronic atomic orbits the intermolecular energy lines become excited . iron in the forge glows , mostly in the infrared . the radiation appears continuous to the eye and the instruments because there are very many energy levels between molecules overlapping in value due to the complexity of the ~10^23 molecules per mole in matter , all compressed in " touch " densely with neighbors . it is effectively the black body radiation that dominates from solids . this shows the quantum nature not in individual lines but in the avoidance of the ultraviolet catastrophe , where the model is of harmonic oscillators changing energy levels . given that electrons decay on the order of 100 nanoseconds electrons do not decay . decay can be attributed to the de-excitation of the atom by emission or the de-excitation of the lattice in solids . the time of de-excitation depends on the energy and conforms with the heisenberg uncertainty principle bounds . also , do electromagnetic waves move the electrons , or the atom , or both ? both . when the frequency is right for the energy level an electron can be kicked up , or a molecule go to a higher rotational level , or an ensemble of molecules go to a higher level . if it is simply exciting the electrons , i do not know why is should also give way to the vibration of the atoms . see above if it does give way to vibration , then should not gases also give way to continuous spectra ? if gases are molecular , they have molecular vibrational levels , but the frequencies will not be optical as these levels are of much softer energy . matter in the gas phase is very diffuse and inter molecular forces exist transiently , when they scatter and transfer kinetic energy to molecular levels which then decay to ground state . the appearance of continuum to the eye can be obtained as with mercury vapor lamps . the lines are discrete .
the seasons on earth are caused by the rotation of the earth around the sun in addition with the inclination of the axis of the earth . this causes the angle of incident sunlight to be lower in winter and higher in summer . now , the inclination of the axis of the earth is pretty constant . but if that axis would shift in some funny way , you could have any season any time . so , for a summer that lasts several years , the axis would have to shift so that the light incident on a given hemisphere remains at a high angle . however , even defining a year might be hard on such a planet . on earth , we think of a year as the time it takes for the earth to circle the sun once . but that is not how ancient civilizations measured it . they looked at the height of the sun in zenith and noted that there was a day when the sun stood highest , and a day where the sun stood lowest , and these are summer and winter solstice , respectively . with the earth 's axis 's angle fixed , the time span from one summer solstice to the next is the same as the time it takes for the earth to completely circle the sun , but in our imaginary planetary system , these two movements would not be related anymore . but with random axis movement , it might be impossible to tell when a circle around the sun is completed . . .
the azimuthal momentum $$p_{\phi}~:=~\frac{\partial l}{\partial \dot{\phi}}$$ is the ( polar ) $z$-component of the angular momentum $l_z$ of the point mass $m$ relative to the heliocentric reference frame . it is a constant of motion because the azimuthal angle $\phi$ is a cyclic coordinate .
when you snap your fingers there are multiple sound waves , but the speed of sound is so fast you can not distinguish individual waves . the frequency of sound waves is around 100hz to 10khz so each wave completes one oscillation in 0.01 to 0.0001 seconds . what you are hearing when you snap your fingers is the envelope i.e. the overall amplitude of the sound waves . when you throw a stone into still water you get an expanding ring of waves moving out , so at the centre it is still then as you move outwards you pass through the waves and beyond them the water is still again . hearing the finger snap is the same as being struck by the expanding ring of ripples .
the invariant process is the speed of light $c$ which should be ( measured ) the same for every observer in relative motion . and indeed einstein 's 1905 paper derives from first principles , the lorentz transformations ( i.e. the transformations of special relativity ) using ( what some would derogatorily say ) ' high-school mathematics ' . yet the concepts behind these simple relations is the whole point . if one wants to characterise the geometry of minkowski space in terms of f . klein 's erlangen program of groups and invariants , then the geometry of special relativity is exactly this geometry which leaves the lorentz metric invariant ( using a signature of $ ( + , - , - , - ) $ ) : $$ds^2 = ( cdt ) ^2-dx^2-dy^2-dz^2$$ this is exactly analogous to the euclidean geometry as the geometry which leaves the quadratic metric invariant ( e . g in 4 dimensions , $x , y , z , w$ ) : $$ds^2 = dx^2 + dy^2 + dz^2 + dw^2$$
assuming cooling is mainly by convection , the cooling will be described by newton 's law of cooling . this states that the rate of temperature change is proportional to the temperature difference , and result is that the difference between the temperature of your object and the room decays exponentially : $$ t_{room} - t_{object} = ( t_{room} - t_{0} ) e^{-kt} $$ where $t_{0}$ is the initial temperature of your object and $k$ is some constant . you can take the temperature of the object as a function of time and then fit the expression above , but the problem is that this type of fit gives rather large errors in the final temperature $t_{room}$ unless you measure for long enough that you have almost reached the final temperature . you may also find newton 's law of cooling breaks down when the temperature difference is very small , because there is no longer effective convection .
in the many-worlds interpretation of quantum mechanics , there is at all times just one state vector for the entire universe . it is a vector in a certain ( infinite-dimensional ) vector space , and that vector space is always the same . so there is nothing in the theory whose dimension changes when the number of particles in the universe changes . to be a bit more precise , the space in which the state vector of the universe lives is ( something like ) a fock space . vectors in that space include states with all possible different numbers of particles , as well as superpositions containing different numbers of particles . so if a particle-antiparticle pair is created , the state vector simply " wanders " from one part of that space to another ; the space itself need not get any bigger .
there is hardly a book covering all physics , but for particular subjects there is some . for example : jammer : the conceptual development of quantum mechanics . whittaker : a history of the theories of aether and electricity .
this is in addition to my answer posted elsewhere since op wanted a more general answer . that example captured the essence based on the idea of how information can be encoded -- it is a somewhat constructive argument in spirit . another way of thinking about the amount of information is as follows : if an event that is very probable happens , then you cannot get much information out of it . it was going to happen any way . on the other hand , if something unusual happens , then that should give you something to think about . such an event carries more " information " . ( for eg : the occurrence of a certain event conveys no information ) if two independent events happen , then the information you glean from them must " add up " . since their probabilities multiply to give the probability of the combined event , the information gleaned from each event must be proportional to the $\log$ of its probability . in typical treatments , one solves a functional equation for the dependence of the entropy on the probability distribution -- with the two conditions mentioned above . the latter gives the $a \log [ ] + b $ while the former fixes the additive constant $b$ to zero . the scale factor $a$ depends on the base to which you take the logarithm .
i think the harvard physics problems of the week are pretty nice : http://www.physics.harvard.edu/academics/undergrad/problems.html
faddeev has implicitly dropped a total 4-divergence term $d_{\mu} ( a_0 f^{0\mu} ) $ in the lagrangian density ${\cal l}$ . this does not affect the equations of motion , i.e. , maxwell 's equations .
this is more of a psychology question . after you start seeing things , you notice that a certain side of your vision is the part that will see your finger if you touch your forehead , and the other side will see your finger if you touch your lips . we designate the first as " up " and the second as " down " . the brain just gets a bunch of signals . " up " and " down " are artificial tags we attach , where " up " is the side of our forehead and " down " is the side of our lips . besides , if one was to wear glasses that inverted vision , the mind would indeed slowly adjust .
the correct equation for position at time $t$ : $\vec{r ( t ) } = \vec{p} + \vec{v} . t + {1\over2}\vec{a} . t^2$ where $\vec{v}$ accounts for both initial speed and wind speed , thus : $\vec{v} = 30 . \hat{\vec{d}} + 15 . \hat{\vec{v}} = ( 18,15,24 ) $ and $\vec{a} = ( 0,0 , -9.81 ) $ is gravity acceleration assuming it is pointed toward -z axis . it hits the ground when z equates the radius , so : $2 + 24t - {9.81\over2}t^2 = 0.015$ solving for the above equation give you the time . mass does not play a role because we know the acceleration from the start , also the same for wind velocity as long as it has no z component . edit : we could calculate the drag force for a sphere : $f_d = {1\over2}c_d \rho v^2 a$ in which $c_d$ is a geometrical constant that is equal to 0.47 for sphere , $\rho$ is the density of the medium ( air in this case ) , $v$ speed relative to air , $a$ is the cross section area ( $\pi r^2$ ) . from that you could calculate acceleration due to drag ( that is where mass comes in ) and you need to integrate manually or ( numerically ) ( look at jj fleck solution for this part ) acceleration equation to get position equation as the above equation for $\vec{r ( t ) }$ is valid only for the case of constant acceleration .
you can think about some critical phenomena is in terms of analytic continuation and fisher zeros . as you probably know , the taylor series expansion an analytic function can only converge within a disk that does not contain singularities . however , you can find taylor expansions by ' working around ' the singularity by means of analytic continuation . fisher ( and others ) realized that the boundary between two phases is separated by a line of zeros . even if you know a thermodynamic function exactly in one part of the phase diagram , you cannot analytic continuate into another . see fig . 1 . i mention this because it sounds similar to a paper and talk i recently heard by anatoli polkovnikov , who was asking similar questions in regard to a dynamic phase transition . if that does not help , other signs to look for are : critical slowing : it takes longer and longer for dynamics to converge . this applies to simulations as well , which is a good hint ! a point where correlations are algebraic , not exponential . scaling of dynamical phenomena , such as quenches or ramps . this is another one where simulations might help out .
i supposed you are in a context of bound states , with normalized eigenfunctions $\psi_n ( x , t ) = \phi_n ( x ) e ^{ie_nt}$ . of course , if you calculate $\langle x ( t ) \rangle_{\psi_n} = \int dx \bar \psi_n x \psi_n$ , you will find a position expectation value which does not depend on time . now , this is not the general case , if you take a linear combination of the $\psi_n$ : $\psi = \sum a_n \psi_n$ , and calculate $\langle x ( t ) \rangle_{\psi} = \int dx \bar \psi x \psi$ , you will find a positition expectation value which depends on time .
rather than looking at one orbit of io , consider observing io and jupiter for around 200 days , starting when the earth is exactly between the sun and jupiter , and ending when the earth is opposite jupiter , with the sun in between . in the 200 days , io will make around 110 orbits of jupiter . but , importantly , the light from that last orbit of io will need to travel an extra distance equal to the diameter of the earth 's orbit around the sun , making it arrive about 1000 seconds later than expected . this would add about 9 second to the average observed orbital period for io over that 200 days . and in the next 200 days , as the earth caught up to jupiter again , the average observed period of io would be about 9 second less than expected . the earth 's orbital velocity is about 30 km/sec . during one orbit of io around jupiter , anout 1.8 days , the earth-io distance could at most increase by $\frac{30000\times86400\times 1.8}{c}$ light-seconds ; adding 15 seconds to io 's apparent period ; still an observable amount . . .
there is one and only way to cancel something : add its negative to itself . however , there is an alternative to cancellation for shielding a region from external electromagnetic fields . generally speaking , methods of isolating a region from external electromagnetic fields ( em shielding ) can be divided into two categories , passive and active . a passive shield prevents the external field from reaching the isolated internal region . whatever the field is outside , the field is zero inside . this is convenient if the strength of the external field is variable or unknown . faraday cages ( shields made from a mesh of conducting material ) are examples of passive shields against static ( and non-static ) electric fields . alternatively , if you know the value of the external field from which you want to isolate a region , you can generate an equal and opposite field to the external field to actively " cancel it out " . the active alternative to a faraday cage for blocking electrostatic fields is a capacitor , whose geometry is precisely shaped so that the electric between the two charged plates exactly cancels the external field in the region of interest . the magnetostatic analog for active shielding is field cancellation using solenoids with the appropriate geometry . the passive alternative for magnetostatically shielding a region ( analog to the faraday cage ) is an enclosing surface made of material ( metal alloys ) with high magnetic permeability . they do not exactly block the external magnetic field per se in the same way a faraday cage blocks an external electric field , but rather draw the field into themselves , providing a path for the magnetic field lines around shielded cavity . however , the effectiveness of passive magnetostatic shielding diminishes for very weak fields . from a practical standpoint , this usually makes active shielding by electromagnets ( solenoids , helmholtz coils , etc . ) the more useful of the two options .
making your own eyewear for direct viewing of the sun is probably not too practical . you essentially need a filter that blocks well over 99% of the incoming light . you can use #14 or #16 welder 's goggles for this , or obtain specialized filters designed for this purpose . these filters are a glass substrate with a metal deposition layer on them that is designed to transmit only a small fraction of the light . as an alternative to direct viewing , it is much easier to make a system for projection viewing of the sun . in this case , you would create a projected image of the sun on a surface , and observe the image . you can look up " solar pinhole viewer " or check out these instructions for details on how to put together a simple solar viewing system with stuff you may have around the house ( cardboard , paper and aluminum foil ) : http://www.exploratorium.edu/eclipse/how.html for more on observing the sun , see this article : http://www.skyandtelescope.com/observing/objects/sun/viewing_the_sun_safely.html
the fact that $k_b \ln \omega$ coincides with entropy $s$ defined in thermodynamics comes from microcanonical ensemble . there are many resources out there on microcanonical ensemble , for example , this . after you come to the conclusion that $$\beta=\left ( \frac{ \partial \ln \omega }{ \partial u }\right ) _{n , v}$$ fully characterizes thermal equilibrium , you know that it must be a function of thermodynamic temperature , and thermodynamic temperature alone , by virtue of zeroth law of thermodynamics . so $\beta=f ( t ) $ . compare this to $$\frac{1}{t}=\left ( \frac{ \partial s }{ \partial u }\right ) _{n , v}$$ and you get that s must be the function of $\ln \omega$ . the remaining question is the exact form of this function , and you already derive it from special cases .
i have not seen the term electrostatic pressure used explicitly before , but i can explain how to think about the problem . you need to consider the total force on each hemisphere , which is of course the integral over the sphere of the ( vector ) force per unit area . take , then , a surface element $da$ , with charge $\sigma da$ . as is nicely explained by purcell , the force on such a surface element is given by the average of the electric field inside and outside . since the field inside vanishes , the total force on the surface element is then $$d\mathbf{f}=\frac{1}{2}\sigma da\times\frac{4\pi r^2\sigma}{4\pi\epsilon_0}\frac{\hat{\mathbf{r}}}{r^2}=\frac{\sigma^2}{2\epsilon_0}\hat{\mathbf{r}}\ , da . $$ by symmetry , the total force on each hemisphere will be along the axis of the problem , which i take in the $z$ direction . this total force will then be $$\mathbf{f}=\int d\mathbf{f}=\hat{\mathbf{z}}\int\frac{\sigma^2}{2\epsilon_0}\hat{\mathbf{z}}\cdot\hat{\mathbf{r}}da=\hat{\mathbf{z}}\frac{\sigma^2}{2\epsilon_0}r^2\int\cos ( \theta ) d\omega=\frac{\sigma^2\pi r^2}{2\epsilon_0}\hat{\mathbf{z}} . $$ the effect is indeed like having a gas inside exerting an outward pressure $p=\frac{df}{da}=\frac{\sigma^2}{2\epsilon_0}$ , but this is hardly general - it depends on the precise , global arrangement of charges of this particular problem , while giving the impression of being a purely local thing ( since it depends only on the " local " charge density , which is of course also a global parameter ) . if you do accept this " pressure " then yes , the total force is this constant pressure times the area vector of the surface , which is $\pi r^2\hat{\mathbf{z}}$ .
the question as posed in the title ( "heating a volume of water ) has a clear answer : with thermal expansion the mass of the water becomes less and so the specific heat goes down . but in the body of the question you ask about a constant quantity ( "a pot of" ) of water and several effects start to play : water evaporates - this means it loses heat . this heat loss rapidly gets worse as you get closer to the boiling point of the water . thus unless you have an isolated system , it will definitely take longer to heat from 80 to 90 than from 50 to 60 - because heat input is not the only heat term . heat transfer : if you have a constant flame ( for example ) the heat transfer to the pot will be a function of the temperature of the pot - the hotter the pot , the less efficient the transfer . when you want to boil water efficiently , you do two things : cover the pot ( limit loss due to evaporation ) and put the heat inside if you can : for example the submerged heater element in electric kettles . other forms of boilers also put the heat in the middle of the water ( think water heaters for homes ) so most of the hot gas gets to give off its energy to the water . but if you have a flame , the best you can hope to to is transfer all it is internal energy to the water - so when the water is hotter a flame is always less efficient . very efficient systems use counter flow - the hot air moves left to right , and the water to be heated right to left : in that way the colder gas meets even colder water so when the gas finally is exhausted it has no heat left . same principle is used in efficient gas furnace for homes , etc .
displacemtns are vector quantities which are determined on the basis of the final and initial position of the object . when you have a cube let 's say the insect traveled form one of the blue balls to another blue ball . let 's represent each of them by vectors from a reference origin $o$ . then the initial position vector $r_0$ and final position vector $r_1$ . hence your displacements would be $r_1 - r_0$ . which in magnitude will be $|r_1-r_0|$ i.e. the distance between the two blue balls . one can use pythagoras theorem to get the desired displacement in magnitude and direction of the vector would be from initial positron to final position .
the circuit likely was closed by his body , or by a grounding wire he was holding . at least that is one way the demonstration has been done . i assume the other end was in contact with the generator . another way is to suspend the tube so it is not in electrical contact with anything , and swing it around , so that you observe a momentary discharge when the tube is orthogonal to the field . in this case the discharge shuts itself off because there is no closed circuit path as you observe . yes . think of v as like the height of a hill for a positive charge . e wants to push the charge down the hill , in the direction of lower v . hence the minus sign .
you need the two initial gluons in your #2 because each one comes from the sea of one of the participating particles ( i.e. . the proton and the anti-proton ) . there is not enough energy in a ( anti- ) proton to produce a top quark , but the energetic collision supplies enough . the vertical line in your #3 is a space-like ( anti- ) quark line . it is meaning is very clear in the connection between quantum field theories and feynman diagrams , but about the best you can say without the math is that it represents a virtual fermion . you may recall that anti-particles are mathematically equivalent to particles moving backward in time , with allows an interpretation in which it takes two interaction with the gluon to " reverse " it and then you have a single particle that comes from the in as an anti-particle , gets turned around and heads back to the future as a particle .
first , note that the bohr model of the atom has some shortcomings . physicists have developed more accurate models since this one . but let 's ignore them for this . from the wording of your question i believe you are curious about the nature of energy when the electron is in a uniform circular orbit . that is , no energy transitions . ( in modern language , you are restricting yourself to a single atomic energy level . ) is this correct ? if so , read on . if you apply the classical idea of a force on an object moving in a uniform circular orbit , you will find that the corresponding force does not do any work . this is because the force and velocity are perpendicular . ( if this sounds unfamiliar , look up the dot-product definition of work . ) no work , no energy change . so i think your premise is not correct : the force by the proton on the electron does not " give " energy .
the hamiltonian of this system lives in a 4-dimensional hilbert space since you have two spin $1/2$ . therefore , you should represent the spin matrix in this four dimensional space like this : $s_1^z=\begin{pmatrix} -0.5 and 0 and 0 and 0 \\ 0 and -0.5 and 0 and 0 \\ 0 and 0 and 0.5 and 0 \\ 0 and 0 and 0 and 0.5 \end{pmatrix}$ , $s_2^z=\begin{pmatrix} -0.5 and 0 and 0 and 0 \\ 0 and 0.5 and 0 and 0 \\ 0 and 0 and -0.5 and 0 \\ 0 and 0 and 0 and 0.5 \end{pmatrix}$ the order of the four states along the rows and columns is $|dd\rangle , |du\rangle , |ud\rangle , |uu\rangle$ where $u$ stands for spin up and $d$ stands for spin down . in this case $s_1^z . s_2^z=\begin{pmatrix} 0.25 and 0 and 0 and 0 \\ 0 and -0.25 and 0 and 0 \\ 0 and 0 and -0.25 and 0 \\ 0 and 0 and 0 and 0.25 \end{pmatrix}$
all the following is simply a simple application of math . if i am wrong anywhere ( and i am bound to be ) , please feel free to point it out to me or just edit the post , if you like . the largest planetoid is ceres , at almost 1000 km in diameter . if we assume that your planetoid was a bit larger than this , and that ceres was the asteroid you are trying to slingshot around , we can do some maths and estimate a limit to see if it is truly possible to slingshot the rocket back into the spacecraft , using the largest gravity fields that we can . at a mass of 940*10^18 kg , your asteroid will exert a gravitational force on your ( perhaps ) 100kg missile that is f = ( 6.27262 *10^12 ) /$r$^2 . this will be for any value of $r$ . if you slingshot a body by a planet , the speed of the missile will then be twice the speed of the planet plus the missile 's initial speed . so you should probably be orbiting against the revolution of the asteroid belt . anyway , the final speed of the missile will be $2u + v$ , where $u$ is the speed of the planet and $v$ is the speed of the missile . if we let this quantity be $w$ , then you only have 2 variables now determining if you can slingshot back to the spacecraft : $w$ and $r$ . the critical condition here is that given a w ( determining your kinetic energy ) and an r ( determining your potential energy ) , can we reach an energy quantity that : a ) enters the orbit of the asteroid such that it is captured in the orbit b ) escapes the gravity of the planetoid so the missile does not actually end up orbiting the asteroid c ) gains enough speed that it can slingshot back to the spacecraft in a few minutes . the escape speed of the asteroid is : $$v_{escape} = \sqrt{{\frac{2gm}{r}}}$$ so entering the orbit , your $v &lt ; v_{escape}$ . but since you are exiting the planet , your $w &gt ; v_{escape}$ . since $2u + v = w$ , then we want everything expressed in terms of $v$: so $$v &lt ; v_{escape} &lt ; w $$ $$v &lt ; v_{escape} &lt ; 2u + v $$ we are going to assume that the missile will be whizzing just above the surface of the planetoid ( for maximum energy ) , so $v_{escape} = 0.51$ km/s and $r = 490$ km . plus , we will assume the missile is going at the fastest speed of satellite we have ever produced , which is $v = 70$ km/s . substituting : $$70 &lt ; 0.51 &lt ; 2u + 70 $$ so , this can not happen . your missile should be going slower than this . what we need is a speed that is less than 0.51 km/s when we enter , so let 's go for 0.50 km/s . $$0.50 &lt ; 0.51 &lt ; 2u + 0.50 $$ to determine $u$ , we need to estimate the distance your missile is going to be travelling . if we let $x$ be the distance of the spacecraft from the planetoid , and the planetoid is not moving , then if the missile is whizzing just above the surface of the planetoid , your missile will be travelling a distance of $2x + \pi r$ . the missile will be travelling at different speeds throughout this course , so let 's sum up the times required . let 's let the acceleration of the missile experiences be $a$ . the acceleration is the square root of the sum of the squares if the tangential and radial acceleration . $$a = \sqrt{a_t^2 + a_r^2}$$ we know the acceleration at the surface of ceres , which is our $a_r$ . we also know $a_t$ because that is just the acceleration to get from $v$ to $w$ . $$a = \sqrt{ ( \frac{w - v}{t} ) ^2 + ( 0.028g ) ^2}$$ playing around with this equation , we will get $$t = \frac{w - v}{\sqrt{a^2 - a_r^2}}$$ and this is the time it will take the missile to whiz by half the surface of ceres . so summing up all the times : $$t_{tot} = \frac{x}{v} + \frac{w - v}{\sqrt{a^2 - a_r^2}} + \frac{x}{w}$$ $$t_{tot} = \frac{x}{v} + \frac{2u + v - v}{\sqrt{a^2 - a_r^2}} + \frac{x}{2u + v}$$ $$t_{tot} = \frac{x}{v} + \frac{2u}{\sqrt{a^2 - ( 0.028g ) ^2}} + \frac{x}{2u + v}$$ since we just added a new variable , let 's define a new constraint . let $t = 300$ seconds , for example . so it takes 5 minutes for the missile to whizz just above the surface of ceres . then we have : $$300 = \frac{2u}{\sqrt{a^2 - ( 0.028g ) ^2}}$$ let 's also say that we do not want the entire trip to take more than ten minutes in total . so we let $t_{tot} = 600$ . $$600 = \frac{x}{0.50} + 300 + \frac{x}{2u + 0.50}$$ we can turn even more " knobs " with these variables and say that the spacecraft is 500 km away , so $x = 500$ now having these equations , we can " solve " for $u$ and $a$ . because there is 1 equation and 2 unknowns , we are not really going to get just one answer . $u = -0.607$ $a$ = 4.05*10^-3 so , that looks like a very slow acceleration . and ceres seems to be going the other way around , away from you . this probably means that there is an error in the formulas . or , your scenario is impossible . i graphed the equation above ( though i dont know how to do that here ) , with x = 500 and t = 300 , and it showed me that you can not have a positive u unless your $t_{tot}$ is above 1300 . and the graph looks the same if i change any of the constants i set . i believe that this should be enough to give you something to play with and determine for yourself if it is possible or not . besides , you get to decide where the ship is and how fast the missile is going . and anyway , the final equation that i got was way too complex for me to solve completely ( and it has a lot of possible values , besides ) . i do not know any software that can help . but the good news is , it is probably possible to slingshot the missile , but you have to allow a larger time for it to whizz by the asteroid . feel free to change the dimensions of the asteroid in the formula . some good data can be found on wikipedia . hope this is something though .
first , note that the light bulb is essentially just a glorified resistor . as current flows through the filament , joule heating causes the filament to get hot and emit light . when one places a capacitor in a circuit containing a light bulb and a battery , the capacitor will initially charge up , and as this charging up is happening , there will be a nonzero current in the circuit , so the light bulb will light up . however , the capacitor will eventually be fully charged at which point the potential between its plates will match the voltage of the battery , and the current in the circuit will drop to zero . this is when the light bulb will dim and then fizzle out . when the battery is removed from the circuit , there is nothing to maintain the potential difference between the plates , and the capacitor will discharge . as this happens , there will once again be a nonzero current flowing through the circuit , and the bulb will light up . however , the current will steadily decrease as the capacitor discharges and will eventually drop to zero at which point the bulb will go off .
in this hypothetical situation , you can transform the unit vectors in the global reference frame $\hat{x}$ and $\hat{y}$ using the same rotation matrix , to the unit vectors in the transformed coordinate system : $$ \hat{x}&#39 ; = r \hat{x}\\ \hat{y}&#39 ; = r \hat{y} $$ this is what you did implicitly by transforming $x\hat{x}+y\hat{y}$ in the same way , you can do the back transformation $$ \hat{x}=r^{-1}\hat{x}&#39 ; \\ \hat{y}=r^{-1}\hat{y}&#39 ; $$ which can be used for the back-transformation of the dislpacement in the local frame to displacement in the global frame . a practical issue will be that errors in you integrated angle $\alpha$ will accumulate , making the mouse annoying to use .
i think i worked it out myself . what i found is that trying to arrive at the hartree-fock hamiltonian via this mean-feald procedure is not really that appropriate . instead , one says : given a two-particle operator $\hat o = c_1^\dagger c_2^\dagger c_3 c_4$ , how can i find a single-particle operator o^{mf}$ that is a good approximation ? if we have a complete set of single-particle basis states that include states $1$ to $4$ of the two-particle operator , we would then look at the basis elements of $\hat o$ and $o^{mf}$ and try to find the best possible agreement . now , the two-particle operator can do three different things : 1 . leave a state as is . this is the case if either $1 = 3$ and $2 = 4$ or $1 = 4$ and $2 = 3$ . 2 . move one particle from a previously occupied state into a previously unoccupied state , e.g. if $1 = 3$ but $2 \not = 4$ or $1 = 4$ and $2 \not= 3$ . 3 . move two particles into new states , i.e. if $1$ and $2$ are different from $3$ and $4$ . with our single particle operator , we can only hope to somehow reproduce the behavior of case 1 and 2 , but not case 3 . so , given states of the type $|\psi_0\rangle$ that can be written as $\prod_i c_{n_i}^\dagger |0\rangle$ , we look at all matrix elements of $\hat o$ between $|\psi_0\rangle$ and states of the type $c_i^\dagger c_j |\psi_0\rangle$ , $i \not= j$ , that arise from $|\psi_0\rangle$ by exchanging two particles . it is a bit tedious to work these matrix elements out in detail , but one can then indeed confirm that by chosing $$\hat o^{mf} = \langle c_1^\dagger c_4\rangle c_2^\dagger c_3 + \langle c_2^\dagger c_3\rangle c_1^\dagger c_4 - \langle c_1^\dagger c_3\rangle c_2^\dagger c_4 - \langle c_2^\dagger c_4\rangle c_1^\dagger c_3$$ the matrix elements for the two-particle operator $\hat o$ and the mean-field operator $\hat o^{mf}$ will turn out to be the same ( for the states of the type introduced above ) .
from special relativity we know that a lorentz transformation : \begin{equation} x'^\mu = \lambda^\mu {}_\nu x^\nu \end{equation} preserves the distance : \begin{equation} g^{\mu \nu} \delta x_\mu \delta x_\nu = g^{\mu \nu} \delta x_\mu ' \delta x_\nu ' \end{equation} the above two equations imply : \begin{equation} g^{\mu \nu} = g^{\rho \sigma}\lambda_\rho {}^\mu \lambda_\sigma {}^\nu \end{equation} now , let us consider an infinitesimal transformation : \begin{equation} \lambda_\nu {}^\mu = \delta_\nu{}^\mu + \omega_\nu{}^\mu + o ( \omega^2 ) \end{equation} such that we can write : \begin{equation} \begin{aligned} g^{\mu \nu} and = g^{\rho \sigma}\lambda_\rho {}^\mu \lambda_\sigma {}^\nu \\ and = g^{\rho \sigma} \left ( \delta_\rho{}^\mu + \omega_\rho{}^\mu + \cdots \right ) \left ( \delta_\sigma{}^\nu + \omega_\sigma{}^\nu + \cdots \right ) \\ and = g^{\mu \nu} + g^{\mu \sigma} \omega_\sigma{}^\nu + g^{\rho \nu} \omega_\rho{}^\mu + o ( \omega^2 ) \\ and = g^{\mu \nu} + \omega^{\mu\nu} + \omega^{\nu \mu} + o ( \omega^2 ) \end{aligned} \end{equation} and so : \begin{equation} \omega^{\mu\nu} = - \omega^{\nu \mu} \end{equation} thus , the matrix $\omega$ is a $4 \times 4$ antisymmetric matrix , which corresponds to $6$ independent parameters ( i.e. . the $3$ parameters corresponding to boosts and the $3$ parameters corresponding to rotations ) .
i ) two square matrices $a$ and $b$ are similar matrices if they are connected via a relation $$\tag{1}ap~=~pb$$ for some invertible matrix $p$ . ii ) two square matrices $a$ and $b$ are unitarily similar matrices if $p$ in eq . ( 1 ) is a unitary matrix .
a plasma sheath is the interaction of a plasma with a boundary . as long as you let enough time for the plasma to be created , you let enough time for the plasma to create a self-consistent electric field in the sheath . in stationary plasmas , the only case when there is no sheath , is when the surface that the plasma is in contact to , is at the plasma potential ( i.e. . same potential as the plasma ) , which means you have to supply current to this surface ( in langmuir probes in dc glow discharges , for example ) in order to compensate for the fact that electrons arrive more quickly - and hence , at a higher rate - than positive ions , to the boundary , producing a net negative charge flux and thus a current . and in this case , there is no electric field either . in all other cases , the sheath is necessarily accompanied by an electric field of its own , because the electric field is what allows the sheath to exist , as an entity with different properties and structure than the plasma bulk . if you increase or decrease the electric field applied to your discharge , the sheath will change , and adapt to that field . basically , if you try to increase the voltage in a dc glow discharge , the cathode sheath will first cover all of the cathode area , then the current across the sheath will increase to allow the potential drop across the sheath to increase . then the cathode will heat up during the transition to arc , and the sheath will change to take into account the increase ion and electron emissivity of the cathode . all these processes are rather complex and depend highly on the situation . so i think one might say : sheath and electric field are bound together by many relations , so it is difficult to answer without knowing a little more about your particular case !
you only need to average over initial conditions if your initial state is mixed . since your initial state is pure this is not necessary . for each trajectory $i$ , at each point in time $t$ , you will have some specific realisation of the stochastic wavefunction represented by the ket $\lvert\psi_i ( t ) \rangle$ . if you perform $n$ trajectories in total , the density matrix at time $t$ is given by $$\rho ( t ) = \frac{1}{n}\sum_{i=1}^n \frac{\lvert\psi_i ( t ) \rangle\langle\psi_i ( t ) \rvert}{\langle\psi_i ( t ) \rvert\psi_i ( t ) \rangle} , $$ where i have included an explicit normalisation factor in the denominator .
in theory , there is one easy way to decide whether a quantum circuit can be realized in principle : can we implement a universal gate set with the system ? if yes , then we can implement any circuit , if no , then we can just implement circuits with the gates we know how to implement . so much for simple theory . the question however is much harder . first of all , we will have to worry about errors piling up and destroying our computer . so we need the theory of error correction - but the question remains the same : can we fault tolerantly implement a universal gate set with the system ? if we can , then yes , we can implement any circuit , otherwise , only the gates we know how to implement fault tolerantly . okay , so much for that . now , sadly that is still not really an answer to the question of " physical realizability " , it is still theory . so we have to go to the system in question and actually see whether all the gates can be implemented - and then , whether they can be implemented fault tolerantly . for most systems , this is only possible with a very small number of ( logical ) qubits at the moment ( there are systems that can do more than nmr , i would not consider it a very hot approach at the moment - then again , i am relly not an experimentalist ) . then , implementing single qubit gates is often not a big deal , multiple qubit gates , however , usually is a problem - to the point where we can not just " scale " them up to dozens or even hundreds of qubits . to sum up : there is two sides . first , the theoretical , which is simple : can you fault tolerantly implement the gates in the circuit ? second , the experimental , which is hard . there is really no way to say something is possible or not until everything has been checked out .
this is sort of the same as anna 's answer , but i would like to put a slightly different spin on it . as anna points out , there are two different co-ordinate systems involved : one for the observer sitting on earth and one for an observer in the freely falling spaceship , and the situation looks very different for the two observers . each observer can ( in principle ) measure the stress energy tensor then solve the einstein equation to give the curvature tensor . the key thing to note is that these tensors are co-ordinate independent i.e. both observers will calculate the same stress energy and curvature tensors . however , although the tensors are co-ordinate independent the representations of them in the two co-ordinate systems will be different . we normally write the tensors as a 4 x 4 matrix , and the two different observers will calculate different values for the elements in the matrices because they are using different bases . so it is not correct for the observer in the spaceship to think the galaxy is somehow being deflected by his gravity . actually strictly speaking it is also incorrect for the observer on earth to think the spaceship is being deflected by the sun 's gravity . the gravity , i.e. the curvature , is not attached to any particular body . the solar system and the spaceship together ( and in principle the rest of the universe ) produce a curvature then both of them move in response to that curvature . the difference seen by the observers is purely down to them using different bases to represent the tensors .
so is it correct to say that magnetic field are ultimately caused by currents ? no , think of a magnetic field as a field that permeates all of space and time , existing independent of anything else . ( however an empty field does not do anything so changes in $\vec b$ field are what matter . ) the ( change in ) magnetic field can be created by currents but also by other stuff too . using the gauss 's law : $$ \nabla \cdot \vec b = 0 $$ we can see that the magnetic field does not have any divergences i.e. no sources or sinks ( mono-poles ) . you can imagine it as an incompressible fluid ( like a tank of water , the water can move but you can not create high density water or vacuum volumes ) . so we have established there are no divergences however there can be curls in the field , i.e. the field can flow , but since there are no divergences these flows must be closed loops . the equation you describe , ampère 's circuital law ( with maxwell 's correction ) , is given by : $$ \nabla \times \vec b = \mu \vec j + \epsilon \mu \frac {\partial \vec e}{\partial t} $$ this states that there are two ways to create a curl in the $\vec b$ field . the first is with a current density i.e. movement of charge which necessarily requires electric charges . so yes ( curls in ) magnetic fields can be created by electric charges . however there is a second part which states that curls in magnetic fields can also be created by $\vec e$ fields changing in time . this usually requires a charge particle but it can also be caused by a changing magnetic field . this is how light propagates , changing $\vec b$ creates a changing $\vec e$ field which creates a changing $\vec b$ etc . if magnetic mono-poles exist then they can be used to create the first changing $\vec e$ field . which eliminates the need for charges ( well they are magnetic charges ) .
multiply both sides of your equation \begin{equation} \det ( i-u ( t ) e^{ite} ) =0 \end{equation} by $e^{-inte}$ where $n$ is the number of dimensions of the state vector space . we obtain \begin{equation} \det ( e^{-ite}i-u ( t ) ) =0 \end{equation} ( see below for how this works . ) this is a special case of the following equation \begin{equation} \det ( \lambda i - a ) = 0 \end{equation} whose solutions $λ_k$ are precisely the eigenvalues of operator $a$ . hence , the meaning of your equation is : each $e^{-ite}$ satisfying your equation is an eigenvalue of the unitary operator $u ( t ) $ . note that all eigenvalues of unitary operators are complex numbers with absolute value 1 . above we used the following property of determinant : for any scalar $b$ \begin{equation} b^n\det ( a ) = \det ( ba ) \end{equation} determinant of operator $a$ is defined by leibniz formula as \begin{equation} \det ( a ) = \sum_{\sigma \in s_n} \operatorname{sgn} ( \sigma ) \prod_{k=1}^{n}a_{\sigma ( k ) , k} \end{equation} which implies that for scalar $b$ \begin{equation} b^n\det ( a ) = \sum_{\sigma \in s_n} \operatorname{sgn} ( \sigma ) \prod_{k=1}^{n}ba_{\sigma ( k ) , k}=\det ( ba ) \end{equation}
to charge an object , you first need to make sure that it is insulated , so the charge cannot leak away . that is easy if you are charging an insulator , but if you want to charge a metal object you need to mount it on an insulator as metals conduct electricity . then you can charge the object . to do this you need to add or remove electrons to/from the objet . you can easily do this by rubbing it with a dielectric material . see for example the van de graaff generator . you always create equal and opposite charges : if your metal becomes positive , the rubbing material will be negative . the positive object has electrons removed , the negative one has them added . i do not know what you think of the article you linked , but controlling gravity is something that cannot be done with electricity - if it is possible at all , and there are good reasons for thinking it is not .
not necessarily . consider this function as an example : $$\psi ( x ) = \frac{c\sin x^2}{\sqrt{x^2 + 1}}$$ this function is square-integrable and asymptotes to zero as $x\to\pm\infty$ , but its derivative goes to $2\cos x^2$ in the same limit . in quantum mechanics , we often assume that real systems are represented by wavefunctions which have no interesting features once you get far enough away from the origin . in practice , this means that the function and all its derivatives have " compact support:" $$\lim_{x\to\pm\infty}\frac{\partial^n\psi}{\partial x^n} = 0\ \forall\ n\in\mathbb{z}_{0 , +}$$ this mathematical statement corresponds to the physical assumption that you can ignore anything happening far enough away from your experiment . there are situations where it is useful to drop this assumption , though . for example , if you are analyzing a crystal lattice , it makes the calculations easier to assume the lattice extends infinitely far in all directions , and in that case you would use a wavefunction which is periodic all the way out to infinity . of course , such wavefunctions usually have nonzero values in addition to nonzero derivatives at large $|x|$ . i do not know of an example offhand which would use a wavefunction which asymptotes to zero but whose derivatives do not , though it would not surprise me at all to learn of one .
i do not have a solution , but i have objections ; ) lets first consider the category given . you work with one object ( lets denote it $x$ ) which is a set with two elements $x = \{\text{on} , \text{off}\}$ . you have some morphisms ( the identity which you call " do nothing " plus the following ) : $$ \text{flip}\colon x\to x $$ so there are two morphisms . but look , we expect $$ \text{flip} ( \text{on} ) =\text{off}\quad\mbox{and}\quad \text{flip} ( \text{off} ) =\text{on}$$ in other words , the flip morphism is an automorphism . so you have the collection of morphisms be precisely the cyclic group with two elements ! we can improve the situation by " vertical categorification " . this sounds scary , but what happens is we promote $x$ to be a category now , and things get a little better . how to categorify the situation ? first we make " on " and " off " objects , lets call them $a$ and $b$ respectively . then we have an invertible morphism which " flips on " the switch : $$ \text{flip}\colon a\to b $$ its inverse would flip off the switch $$ \text{flip}^{-1}\colon b\to a$$ if we add another object $\omega$ which is intuitively a set with two elements ( true or false ) , then we can introduce a morphism that checks if the lights are on . that is , we have one morphism $$ f\colon a\to\omega $$ which checks if the lights are on while the switch is flipped on . we have another morphism $$ g\colon b\to\omega $$ which checks if the lights are on while the switch is flipped off . if we assume that the wiring is correct , and the light is off when the switch is flipped off , then $g$ factors through a terminal object ( $g$ will always have " false " as its output ) . this might allow us to suggest there is a $g^{-1}$ function . can we say this ? no ! if we can , then we have $b$ be isomorphic to $\omega$ , and the flip morphism is an isomorphism . so that would imply that $\omega$ is isomorphic to $a$ . . . if we allow this , then flipping the switch on would be " the same " as the lights being on . this may be too much for your model ( what if the light burns out ? ) .
using gradshteyn and ryzhik ( seventh edition ) 3.876 ( 1 ) $$\int_0^\infty \frac{\sin{ ( p \sqrt{x^2+a^2} ) }}{\sqrt{x^2+a^2}} \cos ( b~x ) dx=\frac{\pi}{2} j_0 ( a\sqrt{p^2-b^2} ) ~~ [ 0&lt ; b&lt ; p ] \\ = 0~~ [ 0&lt ; p&lt ; b ] $$ differential of both sides with respect to $b$ will give the integral you want to calculate .
the attraction is that between a charge and an induced dipole . if the charged object is a sphere , the field is $qe\over 4\pi r^2$ , in units where $\epsilon_0=1$ . this field is what induces the dipole and causes the attraction . once you get a dipole moment in the dust , the dust dipole is attracted to regions of stronger field with a force that goes like the derivative of the e field . the total dipole is neutral , so you get net opposite positive and negative forces which only fail to cancel out to the extent that the e field is stronger in the regions of induced positive charge . the magnitude of the force is ( in the nearly perfect small-dust approximation ) $ d \cdot \nabla e$ , the dipole moment can be thought of as defined by this equation ( although that is not the definition--- the dipole moment vector d is the sum of qx over all the little infinitesimal regions in the dust , where q is the charge of the region , which sums to zero over the dust particle , and x is the position ) . so if the dipole moment is of fixed size , the force is $2qd\over r^3$ . but the dipole itself is proportional to e , with a coefficient that i will call " p " for polarizability , so you get a force which is $$ {2 q \over 4\pi r^3} { pq\over 4\pi r^2} = {2pq^2\over 16\pi^2 r^5}$$ and this is the force between a sphere and a dipole ( in units where $k={1\over 4\pi\epsilon_0} = 1$ ) . you see it falls off as $1/r^5$ , two powers of r for the induced dipole strength , proportional to the e field , and another three powers from the force , which is as the gradient of e . to calculate p requires knowing something about the material , namely it is dielectric polarizability constant for dc electric fields . this is radically different for different materials , depending on whether the molecules are polar themselves , and how mobile they are , whether it is liquid or solid . in general , you can figure this out from the extent to which the electric field in the interior of the dust is reduced from the exterior , assuming a bulk block of dust material . this is the dielectric constant $\epsilon$ of the dust . for a solid block of dust in a constant electric field e perpendicular to the plane surface of the dust-block , the electric field in the interior of the dust block is $e\over \epsilon$ where $\epsilon$ for dc fields is always bigger than 1 . for a metal , the field in the interior is zero , and this corresponds to infinite $\epsilon$ . i will calculate the polarization constant in an order of magnitude for metal dust , but for the largely nonpolar typical carbon-chain dust material , the actual answer will range from 1%-10% of the metal answer . for the metal , there is a nice useful trick for getting the induced dipole moment magnitude , which is the method of images/conformal maps . a dipole at the origin is conformally equivalent by an inversion around a sphere of certain radius to a constant electric field at infinity , so if you place a dipole at the origin , and the equal electric field inverted , the result makes the sphere have the same potential . the electric potential of a dipole is by differentiating the electric potential of a point charge : $$ \phi = {dz \over 4\pi r^3}$$ the potential of constant z-direction electric field is $$ \phi = ez$$ adding the two , the surface at potential zero ( only a zero potential stays an equipotential under inversion ) is a sphere : $$ ez - {dz\over 4\pi r^3} = 0$$ this is solved by a sphere ( away from the z=0 plane which is an accidental equipotential by symmetry ) $$ d = 4\pi e r^3 $$ and so the induced dipole moment is proportional to the cube of the sphere size . the coefficient p is $4\pi a^3$ for a metal spherical dust of radius a . so the answer for an ideal resistive metal dipole ( perfectly screening object , zero response time to adjust to a new field , these are good assumptions for this situation ) is $$ f = {q^2 a^3 \over 4\pi \epsilon_0 r^5} $$ where i restored $\epsilon_0$ in the final answer . for a non-metal resistive dipole , the induced moment is less by the facor $ ( 1-{1\over \epsilon} ) $ . for hydrocarbons , the dielectric constant is about 2 , so you get 1/2 the force . note than when r is of order a , the maximum force goes up as the object gets small .
non-associative algebras represent a challenge in quantum theory because they cannot be realized as linear operators on a hilbert space which are automatically associative by construction . these algebras appear in the classical description of a particle moving under the influence of a magnetic monopole in $\mathbb{r}^3$ as well as in string theory in backgrounds with nonvanishing $h$-field ( i.e. . , with nonconstant $b$-field ) . please , see for example the following review by dieter lüst . in both cases the classical phase spaces become 2-plectic manifold instead of symplectic . in the case of the magnetic monopole , the poisson bracket of two generalized momenta of a particle moving in a magnetic field background is given by : $\{\pi_i , \pi_j\} = ie\frac{\hbar}{c}\epsilon_{ijk}b^k ( x ) $ in this case , the finite translation operators $t ( \mathbf{a} ) = exp ( \frac{1}{i \hbar}\mathbf{a} . \mathbf{\pi} ) $ satisfy : $ ( t ( \mathbf{a_1} ) t ( \mathbf{a_2} ) ) t ( \mathbf{a_3} ) = exp ( -\frac{ie}{\hbar c} \phi ( \mathbf{a_1} , \mathbf{a_2} , \mathbf{a_3} ) ) t ( \mathbf{a_1} ) ( t ( \mathbf{a_2} ) t ( \mathbf{a_3} ) ) $ where $\phi ( \mathbf{a_1} , \mathbf{a_2} , \mathbf{a_3} ) $ is the flux through the tetrahedron generated by the three translations $\mathbf{a_1} , \mathbf{a_2} , \mathbf{a_3}$ . and the jacobi relation of three generalized momenta becomes $ \epsilon_{ijk}\{\pi_i , \{\pi_j , \pi_k\}\}= \frac{2e\hbar^2}{c}\mathbf{\nabla} . \mathbf{b}$ in the presence of a magnetic monopole , the flux through the tetrahedron is nonvanishing , leading to the loss of associativity of the finite translation operators . also $ \mathbf{\nabla} . \mathbf{b} \ne 0$ leading to the violation of the jacobi identity . due this violation , the poisson brackets are called twisted poisson brackets . in the magnetic monopole case , the quantization of the magnetic charge leads to the quantization of the flux through the tetrahedron and the algebra of the finite translation operators becomes associative . however , the jacobi identity is still violated in this case . this problem is avoided if the sources of magnetic charge lie outside the configuration space , then the jacobi relation is satisfied on it . in this case the configuration space will not be $\mathbb{r}^3$ . for example , if the particle is restricted to move on a two dimensional sphere , these problems would have been avoided . another way out is to declare the generalized momenta as nonohysical and work only with subalgebras of operators which satisfy the jacobi identity , for example , the angular momenta $j_k = \epsilon_{ijk} x_i\pi_j$ as mentioned above , nonassociative algebras cannot be represented by linear operators on a hilbert space . however , it is well known that the hilbert space description is not the most general description of quantum theory . in the case of nonassociativity , mainly due to the string theory applications , there are attempts to find quantization methods , in which nonassociative algebras can be represented . this subject is very new and not fully understood yet , please see for example the following article by mylonas schupp and szabo . the main tool used is deformation quantization which is one of the most general methods of quantization . the reason why this method works is that star products can be nonassociative , for example the moyal star product becomes nonassociative when the poisson bivector is not the reciprocal of a closed two form .
let 's set $c=1$ for simplicity . using your observations , it suffices to show that ( just combine the second and third equations you write down ) $$ \dot \gamma = \vec u \cdot \frac{d}{dt} ( \gamma \vec u ) . $$ to prove this , the following facts are useful : $$ \dot \gamma = \gamma^3\vec u \cdot\dot{\vec u} , \qquad \gamma^2\vec u^2 +1 = \gamma^2 . $$ now just compute \begin{align} \vec u \cdot \frac{d}{dt} ( \gamma \vec u ) and =\vec u \cdot ( \dot \gamma \vec u + \gamma \dot{\vec u} ) \\ and =\vec u \cdot ( \gamma^3 ( \vec u \cdot \dot{\vec u} ) \vec u + \gamma \dot{\vec u} ) \\ and = \gamma \vec u \cdot \dot{\vec u} ( \gamma^2 \vec u^2 + 1 ) \\ and = \gamma^3\vec u \cdot\dot {\vec u} \\ and = \dot \gamma \end{align}
this is somewhat controversial issue . but let me present the reasons , as far as i understood , why people like sir penrose thinks so . their arguments are roughly as follows : 1 ) the basic microscopic laws of physics are perfectly time symmetric . they are not biased in any time direction past or future . 2 ) second law follows from the fact that that given an initial condition of a system which is not in the most probable state will tend to go towards the most probable state by the same microscopic laws . since number of disordered states are much higher the system will become more and more disordered with time . accordingly its entropy will increase until a maximum value when the system comes to the thermal equilibrium . 3 ) since the microscopic laws are time symmetric the same argument can be made towards the past time direction as well . given an an initial condition of a system which is not in the most probable state should go towards more disordered ( high entropy ) states towards past as well . that is what the mathematics of the laws tells us . 4 ) this is against our experience . either all the parts of the universe we are observing ( including our memories of past ) has just undergone a huge fluctuation right now to give the impression that there was a more ordered past ( which is crazy ) or the system was already even more ordered ( low entropy ) and more special in the past . but that means even more huge a fluctuation . this reasoning will lead us to conclude that at the moment of big bang the universe was extra ordinarily ordered and most special . it should be so special that it requires explanation . critics often point out that prediction and retrodiction is not the same thing forgetting that when one talks about the very " arrow of time " no one can say with justification which is prediction and which is retrodiction . other than that it is also questionable whether second law can be applied this way to the whole universe or not .
you are asking so many questions that the only way to answer satisfactorily would be to completely rejustify special relativity . i suggest you take a look at a book like special relativity which does such a thing ! i will try to answer your first question : how does this happen ? if a photon is at position $ ( 0,0 , ct ) $ at time $t$ , that is , it has a spacetime event at time t of $e= ( ct , 0,0 , ct ) $ , one has to apply the lorentz transform , which would yield $e'= ( ct ' , x ' , y ' , z' ) = ( \gamma c t , -\beta\gamma ct , 0 , ct ) $ . so $ct=ct'/\gamma$ , and therefore : $$e'= ( c t ' , -\beta c t ' , 0 , ct'/\gamma ) $$ now , if we want to find the speed of the photon in this frame , that will be the magnitude of the spatial portion of the event $e'$ , differentiated by $t'$ ( and in this case , since everything is linear , that has the same effect as dividing by $t'$ ) : $$\| ( -\beta c , 0 , c/\gamma ) \|=\sqrt{c^2 ( \beta^2+ ( 1-\beta^2 ) ) }=c$$ it moves at the speed of light , still . the reduction in the y-direction , in this frame of reference , is a consequence of the lorentz transformation , which can be viewed as a consequence of the constancy of the speed of light . of course you can notice that if the motion in the y direction did not decrease , motion in the $x$ and $z$ directions would have to stay the same for the speed of light to stay constant , but this does not make sense , because it implies the beam of light is being dragged along with your reference frame . also , in lorentz transformation , suppose two observers measure a rod , can one of the observers see that the other person who is moving w.r.t. him , measure the rod to be less than his own measurement ? in other words , can the metre sticks of a moving observer get bigger than the observer who views the moving observer and sees himself to be at rest ? i will limit my reply to this . try to work it out from the definition of a lorentz transformation . the answer to " can one of the observers see that the other person who is moving w.r.t. him , measure the rod to be less than his own measurement " is yes . in your next sentence , if you mean to imply that some observer measures a meter stick at longer than a meter , the answer is no . a meter stick at rest is a meter , and a meter stick flying by ( along its axis ) in either direction is slightly shorter than a meter , by a factor of $1/\gamma$ .
i will assume you have heard something about proton decay . proton decay is a hypothetical form of radioactive decay in which the proton decays into lighter subatomic particles , such as a neutral pion and a positron . 1 there is currently no experimental evidence that proton decay occurs . there are a number of experiments which test whether such decays occur . for example a current one is here . there exist limits and in this review they are given depending on the theory that would predict proton decay . neutrons decay when free anyway into a proton an electron and an e-antineutrino . reappearance would require something like a new big bang , i.e. all matter/energy become a plasma and then by expansion the reappearance of nucleons .
this link shows the massive calculation of the sunset diagram which is the name of the diagram you want to look at . the massless limit is simple . i suspect this question will be closed soon for being too specific and not relating to any physics concepts , though . . .
the reflector focuses the light from the bulb which is a point source to a straight line .
i am on record of having the opinion that there is no real argument against us being a simulation in a general sense , however we frequently find people jumping to quick into the simulation pool and stating there new what-ever-it-is proves the universe is a simulation . the example given above sounds like one of them . first off , quantum error correcting code ( qecc ) are mathematical approaches to allow for stable transfer of quantum information by correcting for decoherence effects . if some version of qecc is apparent in any formulation of quantum mechanics , it is interesting but probably not very meaningful in proving we exists in some sort of emulation . second , just because it shows up in one theory , unless that particular version is shown to have the ability to predict physical effects then it is hard to make the claim about its relevance . whether these things are testable is a matter of debate . however , there are people who are proposing to look for " glitches " in the universe . some would hypothesize that if we lived in a simulation based on lattice quantum chromodynamics ( lqcd ) we should be able to find places where the lattice work becomes apparent . this is clearly far-fetched but who am i to judge ? for the world of warcraft , although i do not play that game , the first evidence of a simulation is along the same lines as the theory to test for lqcd latticework . the pixelation of the characters would be the first indication of potential emulation . the universe as we know it has a continuous spacetime ( versus discrete ) , so any sign of blockiness is a good indicator . generally , anything that is an inconsistency with basic laws of physics ( e . g . perpetual motion , decreasing entropy , etc ) would be the first indicator something was wrong . now , in wow one can assume that they might operate under a slightly different set of physics than our real world . so ultimately inconsistencies in some portion of the world relative to the laws of physics would be a flag . something you should look into is the equivalence principle . in a nutshell it is a statement that the laws of physics should be the same regardless of you location in spacetime . it is very critical to our notion of the world around us , but a similar rule should be applicable wow , and significant inconsistencies would be cause for exploration .
i cant speak in the case of full generality , but at least for schwarzschild and reissner-nordstrom ( electrically charged ) non-rotating black holes , there exists a coordinate system called ' isotropic coordinates' ; it turns out that in these coordinate systems the ' interior ' and ' exterior ' regions are actually isomorphic ; this means that representing the solution in these coordinates gives rise to two identical yet causally disconnected regions of space-time . these two space-times share a common boundary , namely the event horizon . as such , the two regions can be thought of representing the same physics and the answer to your question is then a definite ' yes': the information contained within the ' interior ' section also amasses on the horizon .
the quantum number $n$ of the harmonical oscillator runs from $0$ to $\infty$ . ( your sum starts at 1 . ) $\sum_{n=0}^{\infty} e^{-\theta ( n+1/2 ) } = \frac{e^{-\theta/2}}{1-e^{-\theta}} = \frac{e^{\theta/2}}{e^\theta - 1} = \frac{1}{e^{\theta/2}-e^{-\theta/2}}$ . i guess there just is an error in your exercise . ( tas make mistakes , too . ) the faq says no homework questions . let 's hope they do not tar and feather us . ; - )
the rule of thumb for me is how would the solutions of the schrodinger equation look for this potential . when the energy of the electron is above the potential well the solutions can be continuous ( although there can be discrete resonances with an energy width ) . below the lip of the potential , as for example in the hydrogen atom , the solutions will be discrete , quantized . if the walls of the potential go to infinity , as in your example , then all the solutions will be quantized as in this example of the particle in a box . in the limited in energy potential ( not going to infinity ) the energy levels with large energy quantum numbers are very dense and resemble a continuum , although they can be labeled with a discrete number , as for example in the hydrogen atom . above the energy of the well are the continuum solutions , not discrete .
it depends on the type of wormhole . the simplest type of wormhole to understand is the sort described by matt visser . i have talked about these briefly in negative energy and wormholes , and also on the sci-fi stack exchange . this type of wormhole has minimal tidal forces . if you pushed your protesting pachyderm towards this type of wormhole it would try to pull the elephant apart , but no more strongly than if you pushed the elephant towards a door too small for it . the force to tear the elephant apart would have to be supplied by your push , so unless you pushed very hard indeed the elephant is unlikely to be troubled by the experience - it simply would not go through the wormhole . i am glossing over the possibility of the elephant contacting the string of exotic matter you used to make the wormhole because to be honest i am not sure what that interaction would be like . the other type of wormhole , and probably the one most people think of is the morris-thorne wormhole . this type of wormhole has enormous tidal forces at its mouth , so you could only survive unscathed if its mouth was enormous . if a morris-thorne wormhole was too small for your elephant it would convert the elephant to a great length of elephant string , a process quaintly knwn as spaghettification . if you want some detail there is a nice article here ( nb it is a microsoft word file ) .
there is actually at least one very big clue that is been accessible to skygazers since the earliest times : the first quarter moon at dusk . every child in the northern hemisphere going back to 30,000 bce likely would have been familiar with how 1st-quarter moons always tend to rise at noon , reach its highest point at sunset ( with an azimuth directly south ) , and set at midnight . form a triangle out the observer , the sun and the moon : $\triangle osm . $ the only angle the observer can measure directly is of course the angle between the sun and moon , the observer forming the vertex . the sun is in the direction of the horizon , and the 1st-quarter moon is near zenith , hence $\angle som \approx 90° . $ the angle with vertex at the moon , $\angle oms$ , could not be measured in general , but it does not take too much imagination to infer that the shape of the sunlit portion of a 1st-quarter moon results whenever $\angle oms \approx 90°$ . hence , $\triangle osm$ is an acute , nearly isosceles right triangle , whose legs are practically parallel and much , much greater in length than the base . this small base length is the earth-moon distance $|om|$ is itself much greater than any terrestrial distances we measure on the earth 's surface . thus , with extremely little effort we can be reasonably confident that eratosthenes ' condition of parallel sunlight rays holds to good enough approximation for the purpose of his measurements ( uncertainties in the measurements of distances between cities would have been the limiting factor towards overall precision anyway ) .
the wind is certainly doing work , because it applies a force and the point where the force is applied is displaced . however it is not doing any work on the boat , it is doing the work on the water . the key point is that the net force on the boat is zero . we know the net force on the boat is zero because the boat is moving at constant velocity - if the net force were non-zero the boat would be accelerating . since the net force on the boat is zero no work is being done on the boat . the velocity of the boat is constant because the drag of the water is balancing out the force applied by the wind . so overall the wind is applying a force to the water - the boat is just the instrument through which the force from the wind is communicated to the water . so the wind is doing work on the water , but not on the boat .
imagine the anchor is hanging from the bottom of the boat , dangling mid-water like this : ( there is no difference between the anchor hanging in the water and sitting in the boat , since the system boat + anchor weighs the same either way . ) the water level depends on how heavy the anchor is . if you make the anchor heavier , it pulls the boat down further , pushing the water out of the way . this water goes out to the sides and raises the water level a bit . if you make the anchor lighter , the boat rises up some , leaving a gap . water rushes in underneath the boat , and the water level goes down . imagine making the chain longer until at last the anchor starts to rest on the bottom of the tank . since the bottom of the tank is supporting the anchor , it does not pull down on the boat as much . from the boat 's perspective , it is as if the anchor got lighter . thus , the boat rises and the water level falls . we must assume that the anchor is more dense than water , but that is all . if you wanted to calculate how much the water level falls , you would need to know the density and weight of the anchor .
the rutherford model of the atom did not respect any quantization : it was a classical planetary model . the bohr-sommerfeld model had the quantization of the allowed orbit from your first picture ; however , you conflated these two models and spoke about " rutherford-bohr " model which has never existed . the third thing that you conflated is the actual quantum mechanical equation that describes the atom correctly - in the non-relativistic limit - while neither the rutherford model nor the bohr model are correct in details . the states $1s , 2s , 2p , 3s , 3p , 3d , \dots$ that you refer to on your second and third picture only exist in the correct quantum mechanical model that predicts three quantum numbers for the electron , $n , l , m$ ( if we ignore the spin ) . the bohr model only predicts ( incorrectly ) one quantum number $n$ , so it would only have states $n=1,2,3,4$ and no extra $s , p , d$ labels that distinguish different values of $l$ . in some sense , the bohr model has angular momentum $l=n$ and it does not allow any values $l&lt ; n$ while the correct quantum mechanical models only allows $l&lt ; n$ but all of them - it predicts $l=0,1,2 , \dots n-1$ . so you should recognize the different models . the rutherford model is classical and hopeless - and only included the insight that the nuclei are smaller than the atoms . it did not know anything correct about the motion of the electron . the bohr-sommerfeld model knew something " qualitative " about the motion of the electron , namely that there was something quantized about it , but it was still too classical and it was the wrong model that only happened to " predict " the right energies after some adjustments but this agreement for the hydrogen atom was completely coincidental and related to the fact that the hydrogen atom may be solved exactly ( and the answer for the allowed energies is very simple ) . so the answer how you can see $1s , 2s , 2p , \dots$ states in the bohr ( or even rutherford ) model is obviously that you can not see them because the bohr and rutherford models are invalid models of these detailed features of the atom . if you decided to learn quantum mechanics and abandoned the naive ideas such as the rutherford and bohr-sommerfeld models , you could also discuss other properties of the electron states in the hydrogen atom . for example , the states $2px , 2py , 2pz$ from your first picture are particular complex linear combinations of the usual basis of states $2p_{m=-1} , 2p_{m=+1} , 2p_{m=0}$ . in fact , $2pz=2p_{m=0}$ while $2px\pm i\cdot 2py = 2p_{m=\pm 1}$ , up to normalization factors .
you are correct that it is impossible to change the frequency of every component of a waveform while ( a ) preserving all the phase relationships between the frequencies and ( b ) preserving the length of the waveform . however , while it is mathematically impossible to do this precisely , it is possible to do things that sound quite a lot like it to the human ear . the most basic algorithms for this are fairly simple , but doing it well is pretty difficult , and the algorithms are being improved all the time . it is referred to as " pitch shifting " if the goal is to change the pitch but not the time , and " time stretching " if you want to change the time while keeping the pitch . the reason this is possible is that the ear distinguishes rhythm and pitch as quite distinct things . a sequence of clicks will sound like a sequence of clicks until its frequency passses about 20-30 hz , at which point it transitions into a buzzing sound that is perceived as having a pitch . because of this , as dumpsterdoofus said , you can chop the sound up into little segments of short duration , change the pitch of each one individually , and then stitch them back together again , repeating things or leaving gaps where necessary . by itself this does not sound so good , because you get a click at the end of each segment , where the waveforms do not line up . it can be improved a lot by overlapping " windows " of sound , which each have a fade-in and fade-out applied . that way you do not hear the clicks that the ends of the segments , but you do tend to hear phase-cancellation effects where thr windows are added back together again . for this reason this technique requires quite a bit of tuning to get it to sound good on any given input sound . there are also fourier-domain techniques , as brandon enright said . these also usually use overlapping windows ; i guess the advantage of using the fourier domain is that you have more control over the phase relationships . ( i do not know very much about these techniques . ) a fairly recent development is the " pitch synchronous overlap-add " ( psola ) algorithm . this uses overlapping windows as well , but it synchronises the length of the windows to the pitch of the input sound . this makes it much harder for the ear to perceive the individual windows . i call it " the algorithm that ruined music " , because it is responsible for that nasty " autotune " effect that is been overused on the vocals of every pop record for the last ten years or so . however , it does also have peaceful applications , and the pitch dial on your amplifier probably uses some version of it . you can find more details about these algorithms on wikipedia .
if all you have is the data there is not a lot more you can do . i doubt the difference in fit between the $v:d$ and $v:d^2$ fits is significant . i note that neither fit goes through the origin , which makes me suspect that neither fit captures the physics behind the data . you need to have a look at your system and see if you can write down some ( maybe approximate ) equation to model it is behaviour . i would guess that you have some system with friction/drag involved , so the acceleration starts out high and falls with increasing speed . in that case if you can make a guess at the relationship between velocity and drag you can write down an approximate equation of motion and then fit that to the data .
first , the earth 's capacitance is not " huge " , less than 1f , so it would not take long to charge earth to a significant potential with moderate current . it is important , however , that earth is conductive , so , unless the terminal has the same potential as the earth , the current will be maintained in the circuit , as the circuit will be closed somehow through the earth and the plant equipment maintaining the terminal potential - it is difficult to assume " that no conductive path exists going back to the power plant ground spike " , unless earthing is very bad either at the plant or at the contact with the terminal , in which case some limited area will be charged and the current will stop . how large the current will be , depends on such things as the resistance in the circuit , including the resistance of the earthing at the plant and at the contact with the terminal . edit ( 07/13/2013 ) actually , earth 's capacitance is not just less than 1f , it is much less - about 0.0007f
1 ) motors orient the camera independently of the satellite . the satellite has to be oriented so its big dish antenna is pointed toward earth . 2 ) yes they know the position , and they pre-program the photographs , because instructions take hours to get there . 3 ) is there a gravitational sensor ? not for pointing cameras . 4 ) for orientation , they have inertial ( gyroscope ) sensors and visual trackers of known stars , like canopus . check out voyager spacecraft .
conservation laws typically hold only for closed systems . in this case the inner drum is losing mass so it can not be considered a closed system ; instead , you must also include the angular momentum of the outer drum . on the other hand , you are right that there is no torque on the inner drum , and that therefore its angular momentum - that of the drum itself , ignoring the sand - is also constant . together , these two constraints are enough to determine the two final velocities .
vector spaces because we need superposition . tensor product because this is how one combines smaller systems to obtain a bigger system when the systems are represented by vector space . hermitation operator because this allows for the possibility of having discrete-valued observables . hilbert space because we need scalar products to get probability amplitudes . complex numbers because we need interference ( look up double slit experiment ) . the dimension of the vector space corresponds to the size of the phase space , so to speak . spin of an electron can be either up or down and these are all the possibilities there are , therefore the dimension is 2 . if you have $k$ electrons then each of them can be up or down and consequently the phase space is $2^k$-dimensional ( this relates to the fact that the space of the total system is obtained as a tensor product of the subsystems ) . if one is instead dealing with particle with position that can be any $x \in \mathbb r^3$ then the vector space must be infinite-dimensional to encode all the independent possibilities . edit concerning hermitation operators and eigenvalues . this is actually where the term quantum comes from : classically all observables are commutative functions on the phase space , so there is no way to get purely discrete energy levels ( i.e. . with gaps in-between the neighboring values ) that are required to produce e.g. atomic absorption/emission lines . to get this kind of behavior , some kind of generalization of observable is required and it turns out that representing the energy levels of a system with a spectrum of an operator is the right way to do it . this also falls in neatly with rest of the story , e.g. the heisenberg 's uncertainty principle more or less forces one to have non-commutative observables and for this again operator algebra is required . this procedure of replacing commutative algebra of classical continuous functions with the non-commutative algebra of quantum operators is called quantization . [ note that even on quantum level operators can still have continuous spectrum , which is e.g. required for an operator representing position . so the word " quantum " does not really imply that everything is discrete . it just refers the fact that the quantum theory is able to incorportate this possibility . ]
you only need to assume the schrödinger equation ( yes , the same old linear schrödinger equation , so the proof does not work for weird nonlinear quantum-mechanics-like theories ) the standard assumptions about projective measurements ( i.e. . the born rule and the assumption that after you measure a system it gets projected into the eigenspace corresponding to the eigenvalue you measured ) then it is easy to show that the evolution of a quantum system depends only on its density matrix , so " different " ensembles with the same density matrix are not actually distinguishable . first , you can derive from the schrödinger equation a time evolution equation for the density matrix . this shows that if two ensembles have the same density matrix and they are just evolving unitarily , not being measured , then they will continue to have the same density matrix at all future times . the equation is $$\frac{d\rho}{dt} = \frac{1}{i\hbar} \left [ h , \rho \right ] $$ second , when you perform a measurement on an ensemble , the probability distribution of the measurment results depends only on the density matrix , and the density matrix after the measurement ( of the whole ensemble , or of any sub-ensemble for which the measurement result was some specific value ) only depends on the density matrix before the measurement . specifically , consider a general observable ( assumed to have discrete spectrum for simplicity ) represented by a hermitian operator $a$ . let the diagonalization of $a$ be $$a = \sum_i a_i p_i$$ where $p_i$ is the projection operator in to the eigenspace corresponding to eigenvalue ( measurement outcome ) $a_i$ . then the probability that the measurement outcome is $a_i$ is $$p ( a_i ) = \operatorname{tr} ( \rho p_i ) $$ this gives the complete probability distribution of $a$ . the density matrix of the full ensemble after the measurment is $$\rho&#39 ; = \sum_i p_i \rho p_i$$ and the density matrix of the sub-ensemble for which the measurment value turned out to be $a_i$ is $$\rho&#39 ; _i = \frac{p_i \rho p_i}{\operatorname{tr} ( \rho p_i ) }$$ since none of these equations depend on any property of the ensemble other than its density matrix ( e . g . the pure states and probabilities of which the mixed state is " composed" ) , the density matrix is a full and complete description of the quantum state of the ensemble .
the field strength is the force on a unit charge , so the field strength at the surface of sphere 1 is : $$ f_1 = \frac{1}{4\pi\epsilon_0} \frac{q_1 . 1}{r_1^2} $$ and the field strength at the surface of the second sphere is : $$ f_2 = \frac{1}{4\pi\epsilon_0} \frac{q_2 . 1}{r_2^2} $$ lets take the ratio $f_1/f_2$ to see which is greater . the constants cancel to give us : $$ \frac{f_1}{f_2} = \frac{\frac{q_1}{r_1^2}}{\frac{q_2}{r_2^2}} $$ and i am going to rewrite this slightly to make it obvious how you use your equality $q_1/r_1 = q_2/r_2$: $$ \frac{f_1}{f_2} = \frac{\frac{1}{r_1}\frac{q_1}{r_1}}{\frac{1}{r_2}\frac{q_2}{r_2}} $$ because $q_1/r_1 = q_2/r_2$ we can cancel them on the top and bottom of the fraction and we are left with : $$ \frac{f_1}{f_2} = \frac{r_2}{r_1} $$ and because $r_2 &lt ; r_1$ this means the field strength at the surface of sphere 2 is greater than at the surface of sphere 1 .
conservation of energy is a consequence of time-translational symmetry of the system . if this symmetry is broken , there would be no conservation of energy .
the sentence in peskin 's and schroeder 's book that " the weak interactions preserve cp and t " is a bit misleading but there is a sense in which it is right . experimentally , cp and t is known to be violated and cpt is always a symmetry . theoretically , cpt is always a symmetry , too – it is proven by the cpt theorem . the cpt transformation is effectively a rotation by $\pi$ in the $t_ez$ plane in the euclideanized spacetime which is a symmetry due to the lorentz symmetry and analyticity of the theory ( the charge conjugation is automatically added because by sending the particles backwards in time , they become antiparticles , so the geometric operation is physically interpreted as cpt and not just pt ) . theoretically , we know that cp and t may be violated . note that because cpt is always a symmetry , a theory is symmetric under cp if and only if it is symmetric under t . there are various potential physical phenomena that violate cp and t – including the " theta angle " of qcd ( which would mean that the strong force also breaks cp and t ) – but the only experimentally observed source of cp violation is the " complex phase in the ckm matrix " . the ckm matrix is a unitary transformation that transforms the upper-type quark mass eigenstates to the $su ( 2 ) _w$ upper partners of the down-type quark mass eigenstates . all the quark masses are generated by the higgs mechanism – from the yukawa couplings and the vev – and all the higgs-related things may be incorporated into the " weak interaction " . but at least up to some extent , the known breaking of the cp occurs due to mass terms which are not " interactions at all " ( they are quadratic terms in the lagrangian while interactions have to be higher-order ) . in this sense , the cp and t violation is not caused by the " weak interaction " only – only by a subtle combination of the weak interaction and subtleties in the mass matrices that would not matter separately if there were no interactions . at any rate , the cp and t are known to be violated much more " unambiguously " than the formulation in the peskin-schroeder textbook seems to suggest . the breaking of this symmetry is there ; its effects are just a few orders of magnitude weaker ( and less qualitative ) than the effects of the breaking of c and p . the latter symmetry violations are " immediately obvious " when we consider the weak force – for example because the observed neutrinos are always left-handed ( and antineutrinos are right-handed ) .
is not it just generally true that in the absence of any potential , the momentum eigenfunctions are also energy eigenfunctions ? in other words , when there is no potential , ( in the right units ) $$ h = p^2/2m + v ( x ) = p^2/2m $$ since the hamiltonian is proportional to the momentum operator squared , it is easy to see that any eigenket of the momentum operator having eigenvalue $p$ will be an eigenket of the momentum operator with eigenvalue $ p^2/2m$ . it turns out that the momentum eigenkets ( and linear combinations thereof ) form the complete solution set of the particle on a ring problem .
the mass of the original polonium atom is 209.9828737 ( 13 ) au , while the mass of the lead atom is 205.9744653 ( 13 ) au and the mass of the helium is 4.00260325415 ( 6 ) au . the mass deficit gives you the amount of energy released .
yes it is certainly possible to construct a universal clock . if you are at rest with respect to the average matter in the universe ( basically this means being at rest with respect to the cosmic microwave background ) , and not in a gravitational field , then you are a comoving observer . every comoving observer will agree on the time since the big bang ( 13.798 $\pm$ 0.037 billion years ) , and they will all agree on the rate that time passes i.e. the length of the second ( defined using your caesium standard ) . so we can use the number of seconds since the big bang as a universal clock that applies everywhere in the universe . this is not a particularly practical clock , since we have little hope of every knowing the age of the universe to an accuracy comparable to one second . still , in principle it could be done . if you get in a spaceship and fly around the universe then your local clock will get out of sync with the universal clocks due to relativistic time dilation , but you can in principle calculate the loss of sync and correct your clock as you go . likewise if you are in a gravitational field your local clock will run slower than the universal clock , but again this can in principle be corrected .
kinematically , yes . in terms of describing the positions of objects , it is equivalent to say " a is accelerating away from b " and " b is accelerating away from a " . however , it is an observed fact that the universe treats these two situations differently . a and b can check whether they feel artificial gravity in their reference frame . if so , it is accelerating . as far as i know , the " way the universe decides " to break this symmetry is a topic of continuing speculation . check out some related questions : inertia in an empty universe is acceleration an absolute quantity ? is rotational motion relative to space ? acceleration in special relativity newton&#39 ; s bucket what if the universe is rotating as a whole ?
in an ideal situation ( no air resistance ) there will be absolutely no difference in the place where the coin lands ! whether you toss the coin up from inside the train or while standing on the roof , the coin will land back in your hand ( provided you have tossed it perfectly vertically ) . however , in practice , while standing on a fairly fast train 's roof , there is a lot of wind because you are moving at high speed . the moment you toss the coin , the wind force acts on it , and creates an acceleration in the backward direction , making it go slower than the train ( and you ) . however , it will not land in exactly the same place from where you tossed it every time ! this purely depends on the retarding force acting on the coin - in this case , the wind .
i have found the solution to my problem . first of all , i had a factor 2 discrepancy due to the pixel dimension and , more important from an optical point of view , the laser beam was underfilling the entrance pupil of the objective . this means the focusing was worse !
there are a few different ingredients going into this : firstly , the ( holomorphic ) current generating the infinitesimal conformal transformation $\delta z=\epsilon v ( z ) $ is $j ( z ) =i v ( z ) t ( z ) $ ( there is a similar antiholomorphic one too ) . the general ward identity ( at $z=0$ ) for this current gives $$ \frac{1}{i\epsilon}\delta\mathcal a ( 0,0 ) =res_{z\rightarrow 0}j ( z ) \mathcal a ( 0,0 ) + \text{antiholomorphic}\\ =res_{z\rightarrow 0}i v ( z ) t ( z ) \mathcal a ( 0,0 ) + \text{antiholomorphic}\\ =res_{z\rightarrow 0}i v ( z ) \sum_{n=0}^\infty\frac{1}{z^{n+1}}\mathcal a^{ ( n ) } ( 0,0 ) + \text{antiholomorphic} $$ using the definitions in the $t\mathcal{a}$ ope ( 2.4.11 ) . we can drop regular terms because they will not contribute to the residue . in the $n$th term of the sum , we need to find $res_{z\rightarrow 0}z^{-n-1}v ( z ) $ , which is $\partial^n v ( 0 ) /n ! $ , the nth term in the taylor expansion of $v$ . this finally gives us the transformation property of $\mathcal{a}$ under a general conformal tranformation : $$ \delta\mathcal a ( 0,0 ) =-\epsilon\sum_{n=0}^\infty\left [ \frac{1}{n ! }\partial^n v ( 0 ) \mathcal a^{ ( n ) } ( 0,0 ) + \text{antiholomorphic}\right ] . $$ ( there is nothing special about 0 , so shift it to wherever else to get ( 2.4.12 ) ) . now to actually find out what some of those $\mathcal a^{ ( n ) } ( 0,0 ) $s are , we can use what we know about how $\mathcal{a}$ transforms under some specific conformal transformations . we can work out the left hand side of the ward identity and learn about the right . firstly , translations , where $\delta z=\epsilon v$ , where $v$ is actually constant . we have $$\delta\mathcal{a}=-\epsilon v^a\partial_a\mathcal{a}=-\epsilon ( v\partial\mathcal{a}+v^*\bar\partial\mathcal{a} ) $$ for the left hand side , and $$-\epsilon ( v\mathcal{a}^{ ( 0 ) }+v^*\tilde{\mathcal{a}}^{ ( 0 ) } ) $$ for the right . match coefficients and you are away : you have got $\mathcal{a}^{ ( 0 ) }$ and $\tilde{\mathcal{a}}^{ ( 0 ) }$ . secondly , we are talking about a quasi-primary operator , so we specify its behaviour under dilations and rotations , the infinitesimal form of which is $\delta z = \epsilon \xi z$ for some $\xi$ . so $v ( z ) =\xi z$ here . translating to the finite form given , this is $z'=\zeta z= ( 1+\epsilon \xi ) z$ , and $$\mathcal{a}' ( z ' , \bar{z}' ) = ( 1+\epsilon \xi ) ^{-h} ( 1+\epsilon \bar\xi ) ^{-\bar{h}}\mathcal{a} ( z , \bar{z} ) = ( 1-\epsilon h\xi-\epsilon\bar{h}\bar{\xi} ) \mathcal{a} ( z , \bar{z} ) . $$ so $$\delta\mathcal{a} ( 0,0 ) =-\epsilon ( h\xi+\bar{h}\bar{\xi} ) \mathcal{a} ( 0,0 ) $$ is the left hand side , and $$-\epsilon ( \xi\mathcal{a}^{ ( 1 ) }+\bar{\xi}\tilde{\mathcal{a}}^{ ( 1 ) } ) $$ is the right . the upshot : you now have the $z^{-1}$ and $z^{-2}$ coefficients of the $t ( z ) \mathcal{a} ( 0,0 ) $ ope ( where $h$ , the holomorphic weight , appears ) and of the $\tilde{t} ( \bar z ) \mathcal{a} ( 0,0 ) $ ope ( where $\bar h$ , the antiholomorphic weight , appears ) .
let $ [ a , a^{\dagger} ] ~=~1 , $ and let $|0\rangle$ be the vacuum state : $a|0\rangle=0$ . define $$|n\rangle~:=~ \frac{1}{\sqrt{n ! }} ( a^{\dagger} ) ^n|0\rangle . $$ then $$ a |n\rangle ~=~ \sqrt{n} |n-1\rangle , \qquad a^{\dagger} |n\rangle ~=~ \sqrt{n+1} |n+1\rangle , \qquad\langle n |m\rangle ~=~ \delta_{n , m} . $$ an arbitrary linear operator is of the form $$t= \sum_{n , m\in\mathbb{n}_0} |n\rangle t_{nm} \langle m| , \qquad t_{nm}~:=~\langle n|t |m\rangle~\in~ \mathbb{c} , $$ so it is enough to study operators of the form $|n\rangle \langle m|$ . it is straightforward to see that $$ |n\rangle \langle m|~=~\sum_{k\in\mathbb{n}_0} c^{nm}_k ( a^{\dagger} ) ^{n+k} a^{m+k} , $$ where there exist unique coefficients $c^{nm}_k\in \mathbb{c}$ , which can be recursively obtained from the relations $$\delta_k^0~=~\sum_{r=0}^k c^{nm}_{k-r} \sqrt{ \frac{ ( n+k ) ! }{r ! } \frac{ ( m+k ) ! }{r ! } } . $$
the difference is that the classical doppler effect assumes a static background . in atmosphere , there is a marked difference between a moving observer and a stationary one - a gentle ( or not so gentle ) breeze . to exaggerate these effects , consider two jets flying above mach 1 . if the first jet is ahead of a second , the first jet will not hear any of the noise the first one makes , even if their relative speed is 0 . note that with the relativistic doppler effect , the situation is symmetric , and the doppler shift of light will depend only on relative measures of velocity/position between the two objects . this is because relativity does not assume a static/fixed background in the same sense as an atmosphere .
this question can be split into two parts : how do three generations affect the way the universe runs within the standard model , and how do they affect beyond standard model physics ? it is likely that the three generations are important in beyond standard model physics and without them particle physics would be very different . however we do not have any definite idea about how that works so let 's concentrate on standard model physics . it is true that almost all physics that affects the way the universe runs depends on only the first generation of matter particles , i.e. the up and down quarks , electron and electron neutrino , plus the gauge bosons . although we can now easily observe particles from the second generation i.e. muons , strange particles and charm , these have almost no significance in chemistry or even main processes of nuclear physics . hence their effect on how the universe runs is very small . the third generation has even less influence . however there are some exceptions and possible exceptions . ( 1 ) muons are an important component of cosmic rays and it is now thought that cosmic rays can play an important part in weather and climate . there have been studies to observe correlations between muon flux and upper atmosphere temperature . even if this is observed it does not necessarily mean that muons have a causal affect on weather but it is possible . atmospheric muons could have other important influences such as gene mutation , but this is speculative as far as i know . ( 2 ) cp violation can occur in the standard model at observable rates only through the ckm and pmns mixing matrices for three generations . with only one or two generations cp violation in the standard model can only happen via unobservable non-perturbative effects . while cp violation does not have any affect on the way the universe runs now , it is thought to be essential in big bang cosmology in order to create an imbalance between matter and anti-matter . without it all matter would have annihilated with antimatter as the universe cooled . however , it is not known if other cp violating processes from beyond standard model physics that do not depend on the three generations are more important . ( 3 ) in calculations of particle masses and decay rates the existence of second generation particles as virtual particles has some effect . you could argue that the masses of protons and neutron might be slightly different etc if there were no muons etc . ( 4 ) the three generations of neutrino have observable effects on decay rates of weak gauge bosons and also on cosmological parameters measured in the cosmic microwave background . whether this affects the running of the universe in some significant way is another question . ( 5 ) the top quark mass is an important factor affecting the stability of the vacuum and the allowable mass range of the higgs boson for stability . you could therefore argue that the top quark and perhaps some of the other heavy flavour particles are needed to stop the vacuum decaying . in several of these possibilities you could make the point that without the second and third generation the parameters of the standard model could be adjusted to produce the same effect , so it is not certain that the second and third generations have any real necessity in the running of the universe .
i think , on would need a model to understand how it works in principle . let us consider the simplified system represented on the figure . you have two metallic spheres with radii $r_1$ and $r_2$ respectively . they are linked via a metallic wire as well . because all these guys constitute a single metallic object , they must have the same potential that i noted $v$ . let us assume for simplicity that the charges are uniformly distributed on the two spheres with two different surface charge densities $\sigma_1$ and $\sigma_2$ . now , that the stage is set , there are two competing effects which have opposite influence on the increase of the charge density : the charge on a metallic object , in the linear regime , is proportional to its potential which is denoted by $q = cv$ where $c$ is the capacitance . the capacitance is such that , for simple convex shapes , it increase with the system size such that for a sphere $c \propto r$ . from this , we can assert that $\frac{q_1}{q_2} = \frac{r_1}{r_2}$ . hence the total charge carried by sphere 1 is bigger than that of sphere 2 . there is a second effect ( which is a simple one ) that consists in noticing that the charge density is inversely proportional to the square of the size of the object essentially $\sigma \sim r^{-2} = \frac{q}{4 \pi r^2}$ for a sphere . hence $\frac{\sigma_1}{\sigma_2}=\frac{q_1}{q_2}\frac{r_2^2}{r_1^2}$ overall , we finally get that $\frac{\sigma_1}{\sigma_2}=\frac{r_2}{r_1}$ and hence " points " have a larger charge density than bigger objects .
the statement " a , b , c " are cyclic rotations ( more often : cyclic permutations ) of " e , f , g " means that " a , b , c " is either " e , f , g " or " f , g , e " or " g , e , f " , in one of these three orders ( but not in the remaining orders " e , g , f " , " f , e , g " , " g , f , e" ) . relatively to " e , f , g " , these three letters were ordered by one of the three elements of the so-called cyclic group ${\mathbb z}_3$ . similarly for cyclic rotations of $k$ elements and the group ${\mathbb z}_k$ . in your example , if you used a non-cyclic permutation of the pauli matrices , you would get $j*k-k*j = -i*l$ with the minus sign on the right hand side because the pauli matrices anticommute . in particular , let me stress that no operation is applied " inside " the matrices . they are treated as wholes , as elements of a set , that are being permuted with other elements .
the z-projection of the angular momentum of a completely unpolarized system is distributed uniformly both classically and quantum mechanically . in quantum mechanics , the density matrix of a three state system is given by : $\rho = \frac{1}{3} ( i_3+\overrightarrow{n} . \overrightarrow{\lambda} ) $ , where $i_3$ is the three dimensional unit matrix . $\overrightarrow{n}$ is the polarization vector and $\overrightarrow{\lambda}$ is a vector of gell-mann matrices . in a fully polarized system $\overrightarrow{n} . \overrightarrow{n}= 1$ , while in the case of a completely unpolarized system : $\overrightarrow{n} = 0$ . in this case : $\rho = \frac{1}{3} ( i_3 ) $ . therefore , the three states are equally distributed . in classical mechanics , the phase space of a spin system is the unit sphere . the z-component of a unit angular momentum system is given by : $ j_z = cos ( \theta ) $ . where $\theta$ is the inclination angle . a completely unpolarized classical angular momentum should be distributed uniformly with respect to the surface area of the sphere which is the classical phase space . the area of a spherical zone of thickness $h$ is $s = 2 \pi r h$ , where $r=1$ is the sphere radius ( this result was already known to archimedes ) . therefore the z-coordinate is uniformly distributed on the sphere and the z-component of the angular momentum expressed as : $ j_z = \frac{z}{r}$ is linear in $z$ , therefore also uniformly distributed .
i do this sort of work in uhecr anisotropy . there are numerous techniques ( which i am currently working on advancing ) . expansion in spherical harmonics is popular ( and what i am working on ) . for that we use the fact that the spherical harmonics are complete to write $$f=\sum_{\ell , m}a_{\ell m}y_{\ell m}$$ and then from orthogonality we can write ( assuming a given normalization ) $$a_{\ell m}=\int d\omega fy_{\ell m}$$ once we have the coefficients $a_{\ell m}$ we have a number of approaches . one obvious one is to follow the approach of the cmb people and write the power spectrum as $$c_\ell=\frac1{2\ell+1}\sum_ma_{\ell m}^2$$ which is axis independent . then you can compare this to that from isotropy . in the pure case you can write for monopoles that $c_0&gt ; 0 , c_\ell=0\forall\ell&gt ; 0$ where the value depends on the normalization . the remaining terms all show deviations from anisotropy . for the sum of discrete events a mc simulation is useful here . another common standard definition that is quite simple is to define $$\alpha=\frac{\max f-\min f}{\max f+\min f}$$ isotropy gives $\alpha=0$ as desired . in addition , for the simple dipole case , we can write $f=1+a\cos\theta$ where $a$ measures the strength of the dipole . $a=0$ is obviously isotropy . $a=1$ gives $f=2$ at $\theta=0$ ( the " direction " of the dipole ) and $f=0$ at $\theta=\pi$ . note that the 1 ( and the implicit requirement $a\le1$ ) is to ensure that $f$ is positive-definite ( which may or may not be necessary for your situation ) . then , at $a=1$ , we get $\alpha=1$ ( and in fact it is easy to show that $a=\alpha$ ) . note that the three $ ( \ell , m ) = ( 1 , m ) $ spherical harmonics are all degenerate . a similar thing can be done for the quadrupole case , at least for the $m=0$ case , with $f=1-b\cos^2\theta$ where $b$ and $\alpha$ have a more complicated ( but still simple ) relationship . for more complicated shapes you can try to match your geometry with spherical harmonics with what is known as the $k$-matrix approach presented here ( arxiv abs ) in section 3 . in that example they consider a sphere with non-uniform exposure . i suspect that the same ( or a similar ) approach could be used for your case . i should warn though that the reconstruction power falls off very quickly with $\ell_{\rm{max}}$ in the $k$-matrix , so do not go any higher than you have to .
if you use lenses to refocus a beam , with high quality optics , the limiting factor will most likely be the quality of the anti-reflection coatings of your lenses and not the absorption or scattering from their optical material ( at least for visible/nir light ) . it is not easy to lose less than 0.1% per surface from fresnel reflections . if you are desperate to get the smallest losses possible , then you may want to use a refocusing system that involves mirrors . losses while reflecting of a dielectric mirror can be less than 0.01% , taking into account scattering , absorption and transmission .
$$y = x\tan\theta - \frac{g}{2v_0^2 \cos^2\theta}x^2 . $$ $$y=x . \tan\theta-\dfrac{gx^2}{2v_0^2}-\dfrac{gx^2\tan^2\theta}{2v_0^2}$$ $$a\tan^2\theta+b\tan\theta+c=0$$ as $\tan\theta\in\bbb r$ , so $b^2-4ac\ge0$ must hold , for the above equation to have real roots for $\tan\theta$ . use that and you will get $$y\le \dfrac{v^2}{2g}-\dfrac{g}{2v^2} . x^2 $$ that defines a area under the parabola , within which any target can be hit .
the initial phase $\phi$ should be $-\pi/2$ , or in other words , the solution should be $a\sin ( \omega t ) $ because $\cos ( \theta-\pi/2 ) =\sin ( \theta ) $ . how did you find that $\phi$ is zero ? you are supposed to use the fact that $x$ is zero when $t$ is zero . which is what let 's you set $\phi $ to $-\pi/2$ ( modulo $\pi$ ) .
" restoring " forces refer primarily to forces that try to return a system to equilibrium . so a spring has a restoring force of $f = -k\delta x$ . this means that if you choose the origin as being $x = 0$ , then compressing the spring would correspond to a negative $x$ ( displacing the spring to the left ) , and stretching the spring would correspond to a positive $x$ ( stretching the spring to the right ) . in that sense , by extending the spring , a positive $\delta x$ creates a negative force ( $-1 \times \delta x$ ) that acts to restore the spring to equilibrium ( pulling back on the spring extension ) and by compressing the spring , you would have a negative $\delta x$ ( $-1 \times \delta x$ ) , which creates a positive force that restores the spring equilibrium . so hooke 's law is actually $f=-k \delta x$ hope that helps .
instead of thinking about one field changing in response to the change of the other , it is more correct to say that whenever the magnetic field is changing , so is the electric field , and vice versa . the way these fields change is governed by maxwell 's equations . this way , we do not arrive at the confusion op has .
congratulations to your cute and solid paper and your new loophole that is morally on par with a loophole circumventing the coleman-mandula theorem itself – almost . ; - ) i am confident you did the algebra correctly so let me offer you the form of the lore that i usually present and the way how you circumvented it . the lore says that the local fermionic transformations are generated by the density of a locally conserved quantity – the supercharge : and you may call it this way in your case , too . the anticommutator of such supercharges must be a " spacetime vector " of a sort and this statement must hold at the level of the densities , too . however , one must be careful what is the " spacetime vector " and how many components of it are participating in the algebra . normally , the lore assumes that the " real large spacetime dimensions " i.e. the momenta and energies must be included on the right-hand side of the fermionic anticommutator algebra . when localized , this would inevitably lead to a gravitating theory . nevertheless , it is not strictly necessary that all components of a spacetime vector are included on the right hand side in this way and it is not true that the energy-momentum is the only conserved quantity that transforms as a vector . as your example shows , one may split the 11 dimensions of m-theory and the " necessary appearance of a vector-like generator " on the right hand side does not have to include the regular momentum along the m5-brane directions at all . instead , your example has different things that transform as a vector , but they are not the energy-momentum . instead , they are the " winding charge of the boundary of an m2-brane " or the density of the " self-dual strings " dissolved within your m5-brane . this " winding charge " is a heuristic way how to describe the generator of the $$\delta b_{\mu\nu} ( x ) = \partial_\mu \lambda_\nu ( x ) - \partial_\nu \lambda_\mu ( x ) $$ local gauge transformation for the two-form potential . much like the regular electromagnetic gauge transformations are generated by a density of the electric charge , these extended $p$-form transformations are generated by densities of various strings and branes ' winding and wrapping numbers . so the algebraic requirement of having the density of a vector on the right hand side is protected and this part of the lore is preserved ; however , you debunk the assumption of the lore that the conserved vector has to be energy-momentum . instead , your conserved vector is a sort of the winding number of the self-dual string and its density enters the right hand side . let me note that having at least 2 of the minimal spinors of supercharges is a necessary condition for you to be able to avoid the energy-momentum : with extended ( chiral ) supersymmetry like the ( 2,0 ) supersymmetry , you may get purely " central charges " on the right hand side and send the coefficient of the normal energy-momentum to zero . the minimal ${\mathcal n} = 1$ supersymmetry does not have any central charges so the energy-momentum ( and therefore gravity , in the local case ) has to appear on the right hand side . the lore has therefore incorrectly generalized the experience from the minimal supersymmetry , not acknowledging that the energy-momentum and central-charge terms in supersymmetry algebras may " decouple " and beat each other in different ways .
dear wade , your good question is easily answered if you consider " pressure " to be a derived quantity , and let us derive it . an average molecule ( or atom ) of an ideal gas - and your proposition only holds for an ideal gas - has kinetic energy equal to $$mv^2/2=3kt/2$$ it is because every degree of freedom carries $kt/2$ and there are three degrees of fredom in translations . note that lighter molecules will move faster than the heavier ones . how do we calculate the pressure ? well , put the molecule in a cubic box of volume $a^3$ . it will hit the walls - the total surface of the cube is $6a^2$ . we need to compute the average ( over molecules ) transfered momentum per unit time - this is called the force - and divide it by the area to compute the pressure of one molecule . the force on the wall may be in $x , y , z$ directions . let 's consider the $x$ direction . the velocity of a particular molecule in the $x$-direction is $v_x$ , so it takes $a/v_x$ of time to get from one side to the other side of the box in the $x$ direction . once it gets to the other side , it bounces off the wall and changes the sign of the $x$-component of the velocity ( and momentum ) . at this moment , the momentum $p_x$ clearly changes the sign - i.e. changes by $2p_x$ . so the change of momentum $p_x$ per unit time is $$f_x = 2p_x / ( a/v_x ) = 2p_x/ ( am/p_x ) = 2p_x^2/am$$ that is equal to $4/a$ times the kinetic energy $k_x$ in the $x$-direction . the pressure is $$p = ( f_x+f_y+f_z ) / 6a^2 = 3\times 4/a\times k_x / 6a^2 = 4k/a / 6a^2 = 2k/3a^3 $$ but as i have said , the average kinetic ( motion ) energy per molecule is $k=3kt/2$ where $t$ is the absolute temperature , so the pressure is $$p = 2k/3a^3 = kt/a^3= kt/v$$ note that we have just derived $pv=kt$ for one molecule or $pv=nkt$ for $n$ molecules - which is what we wanted . the mass of the molecule canceled : if the molecule is heavier , the average velocity at a given temperature is slower . but that does not matter - because the molecule has a greater momentum ( because of the higher mass ) which is compensated by the longer time it needs to get from one side to the other to transfer this momentum . so for a fixed temperature , the pressure is independent of the molecule type .
the confusion arises because there are two different versions of what earth 's surface looks like , and two different models of how gravity works between the case where the ball goes into orbit and the case where both balls hit the ground at the same time . we often approximate gravity near the earth 's surface by saying that it is constant everywhere . this approximation is pretty good if we are only interested in exploring distances much smaller than earth 's radius . in this approximation , the two balls will always hit the ground at the same time , no matter what speed they leave the surface with . but , in reality , the earth is round , and if you push the ball so fast that it travels a distance which is comparable to the radius before it lands , you must take a more sophisticated picture of gravity and the earth into account . in this picture , it is possible that the fast moving ball will remain in orbit forever , or even fly away from the earth never to return .
its questions like this one that keep me coming back to this site ! your first question is : is there an easy way to understand and/or visualize the reciprocal lattice of a two or three dimensional solid-state lattice ? yes ! the reciprocal lattice is simply the dual of the original lattice . and the dual lattice has a simple visual algorithm . given a lattice $l$ , for each unit cell of $l$ find the point corresponding to that cell 's " center of mass " ( see below ) . connect each such " center of mass " to its nearest neighbors . the resulting lattice is the dual of $l$ . to find center of mass of unit cell ( we consider 2d case , generalizes to arbitrary dimension ) : draw the perpendicular bisectors of the edges which bound the unit cell . for regular lattices these lines should intersect at a single point in the interior of the cell . this point is the " center of mass " of the cell . performing these simple steps you find that the dual of a square lattice is also a square lattice , and that the triangular and hexagonal lattices are each others duals ! you can see a nice illustration of this fact here . your second question is : what is the significance of the reciprocal lattice , and why do solid state physicists express things in terms of the reciprocal lattice rather than the real-space lattice ? as mentioned by others this has to do with fourier transforms . in solid-state physics we want to understand the excitations ( waveforms ) that a certain material , whose structure is given by some lattice $l$ , can support . for a lattice only certain momenta are allowed due to its discrete structure . these allowed momenta correspond to the vertices of the dual lattice ! for more see the wikipedia page or check out the first couple of chapters of little kittel or ashcroft and mermin .  Cheers,  edit : this to clarify some doubts about my answer @wsc has expressed in the comments . first of all , it is incorrect that reciprocal lattice vectors in 3d have dimensions $1/l^2$ . consider a 3d lattice with basis vectors $\{a_i\}$ . the reciprocal lattice has basis vectors given by $$ b_i = \frac{1}{2v} \epsilon_i{}^{jk} \ , a_j a_k $$ in index notation , with summation convention . a more familiar way to write this is in vector notation : $$ \mathbf{b}_i = 2\pi \frac{\mathbf{a}_j \times \mathbf{a}_k}{\mathbf{a}_i \cdot ( \mathbf{a}_j \times \mathbf{a}_k ) } $$ where $ ( i , j , k ) $ are cyclic permutations of $ ( 1,2,3 ) $ . we can see that $$ \dim [ \mathbf{b}_i ] = \frac{\dim [ \mathbf{a} ] ^2}{\dim [ \mathbf{a} ] ^3} = \frac{1}{l} $$ and in terms of the lattice spacing $a$ , $\vert\mathbf{b}\vert \sim \frac{1}{a}$ . in fact , this is a basic fact true in any dimension . we can also understand the normalization of the reciprocal lattice vectors by the factor $\mathbf{a}_i \cdot ( \mathbf{a}_j \times \mathbf{a}_k ) $ as being nothing more than $v$ - the volume of the unit cell . why ? so that the transformation between the lattice and reciprocal lattice vector spaces is invertible and the methods of fourier analysis can be put to use . for all regular lattices afaik the " dual " and " reciprocal " lattices are identical . for irregular lattices - with defects and disorder - this correspondence would possibly break down .
the article 's preprint mayer h . c . , krechetnikov r . " walking with coffee : why does it spill ? , " phys . rev . e 85 , 046117 ( 2012 ) . is available from the ucsb site . from a glance of the article the phenomenon is not specific only to coffee . the authors make use of the next formula : the natural frequencies of oscillations of a frictionless , vorticity-free , and incompressible liquid in an upright cylindrical container ( cup ) with a free liquid surface are well known from liquid sloshing engineering : $$\omega_{mn}^2 = \frac{g \epsilon_{mn}}{r} \tanh\left ( \epsilon_{mn} \frac{h}{r} \right ) \left [ 1 +\frac{\sigma}{\rho g} \left ( \frac{\epsilon_{mn}}{r} \right ) ^2 \right ] $$ $h$ is the liquid height , $r$ is the cup radius , $\rho$ is the liquid density , $\sigma$ is its surface tension , $g$ is thethe gravity acceleration . $\epsilon_{mn}$ are coefficients connected to bessel functions . the only parameter that can be significantly different between water and coffee is surface tension $\sigma$ , but then the authors rule it out : for a typical common size of a coffee cup , $r$ $3.5$ cm and $h$ $10$ cm , which is studied here , the surface tension $\sigma$ effect is negligible . that is their calculations are applicable both to water and coffee . their work seems to be all about biomechanics , the way human moves with unwanted frequencies .
for symmetric or antisymmetric tensor product , the most useful definition of an unentangled state is a state of the form $$ |\psi\rangle = \sum_{p} ( -1 ) ^{2j\cdot |p|} \prod^\otimes_i |\psi_{p ( i ) }\rangle $$ which is just a convoluted symbolic expression for the totally symmetrized ( for bosons ) or totally antisymmetrized ( for fermions ) tensor product . the sum goes over all permutations $i\mapsto p ( i ) $ . the power of $ ( -1 ) $ is $1$ for bosons i.e. $j\in {\mathbb z}$ while it is $ ( -1 ) ^{|p|}$ , i.e. the sign of the permutation , for fermions with $j\in {\mathbb z}+1/2$ . the product is a tensor product . all states that can not be written in this form are entangled . if we kept the usual definition of unentangled states as " strict tensor products " , there would be too few unentangled states because most multi-boson states and almost all multi-fermion states refuse to be strict tensor products . so the definition above makes the form of unentangled states more general . however , it may be too general for various purposes . in general , the tensor factors $|\psi_{p ( i ) }\rangle$ should correspond to states where objects are localized in different regions of space , otherwise the state that is unentangled according to the definition above could be entangled " morally " . in particular , in quantum field theory , unentangled states may be obtained as $$ a^\dagger b^\dagger c^\dagger \dots |0\rangle $$ where $a^\dagger$ and others are polynomials in creation operators but $a^\dagger , b^\dagger , c^\dagger$ are only made of creation operators acting on disjoint , non-overlapping regions . the actual wave function of the state will have to be symmetrized or antisymmetrized so it will not be a strict tensor product but the state of the form above will still enjoy many features of unentangled states . the discussion about the non-overlapping regions may get important and subtle because , for example , the simple singlet state of two spins , $|up\rangle |down\rangle - |down\rangle |up\rangle$ , is entangled according to the normal definition but it could end up being unentangled because it is an antisymmetrized tensor product . however , it usually only makes sense to call such a state unentangled if the up-spinning and down-spinning electron are also located at different positions . if they share the same location , the strict singlet state should be called entangled in all conventions .
single ℂ$^n$ is a state space for single photon , let $h_p$ stands for a hamiltonian of a single photon . you are going to struggle here because there is no " hamiltonian of a single photon " . photons are not conserved particles like electrons or protons , they can only be described as excited states of a quantum field . so the idea of an atom interacting with one photon at a time does not really make sense ( if we are considering the free radiation field , not a cavity ) , since extra photons may be created as a consequence of the interaction . nevertheless , a single two-level atom interacting with a heat bath of photons is a canonical theoretical problem , treated extensively in almost any text on quantum optics or open quantum systems . see breuer and petruccione , for example . without going into details , the bound electron in the atom interacts with the quantum and thermal fluctuations of the electric field at the position of the atom . the temperature of the electromagnetic field state determines the mean number of photons flying around for the atom to interact with . after a long time , the internal electronic state of the atom reaches thermal equilibrium with the radiation field via processes of absorption and emission . the atom-field hamiltonian describing this situation is a specific instance of the so-called spin-boson model . study of the spin-boson model is basically a research sub-field in itself . the standard review article for the field is dynamics of the dissipative two-state system , a . leggett et al . , rev . mod . phys . 59 , 1-85 ( 1987 ) , unfortunately behind a paywall . however , this article is quite old now and many new developments and extensions have been made , so it is worth just googling " spin-boson model " to see what you can get .
the quickest way would be to use the right hand grip rule . from symmetry you may conclude that the magnetic field around each wire forms concentric circular loops around the wire . so now it remains to determine whether clockwise or anticlockwise . to do this , just imagine that you are gripping the wire with your right hand in such a way that your thumb points in the direction of current . then the remaining fingers point in the direction of the magnetic field . this follows directly from the convention used for doing path integrals - if the closed line integral is performed in the anti-clockwise direction , then the area vector for doing the corresponding surface integral is taken to point towards you . of course , having thus found the contributions from each individual wire , you would then have to take their vector sum . on a side note , similar conventions are adopted in mechanics for finding the direction of angular velocity corresponding to clockwise or anticlockwise rotation . thankfully , the convention is more or less uniform through all areas of physics !
there is two kinds of vibration that would make this " work": thermal vibration and actual shaking . actual shaking is out of the question because that would involve pushing and pulling a person back and forth such that their average velocity was near-light speed . the " back and forth " part of this would involve way more acceleration , hence force , than the human body could handle . to have a look at thermal vibration , i.e. heating , we use the following formula for the average velocity of molecules in a body of temperature $t$ ( see below for note ) : $$v = \sqrt{\frac {3kt}{m}}$$ and in so doing make the assumptions that the body is made up of molecules with the same mass $m$ with uniform temperature $t$ . let 's take a human made entirely of carbon-12 for which $m$ is 12 kilograms divided by avogadro 's number . for $v \approx c$ , $$t \approx \frac {mc^2}{3k} \approx \frac {2*3^2}{3*1.38}*10^{-23+2*8- ( -23 ) } \approx 4.4*10^{16}k$$ which is pretty hot . so i am not really answering your question . do vibrations work the same as normal movement with regards to time dilation ? yes . so a person could walk into such a machine , and walk out hundreds of years in the future , even though a much smaller amount of time would have passed from their perspective ? well you could do it in principle , and the particles you started with would have travelled into the future , but it would be a stretch to say that the thing you end up with in the future is the person you started with in either method of vibration . note : this formula only holds for ideal gases , which a relativistically heated gas is not . but it gives an estimate of the ballpark of temperatures we are working in . if someone has the expression for the hyper-relativistic thermal velocity i would appreciate a comment or edit : )
the total incident energy per second , $w_{tot}$ , is the energy per unit area multiplied by the area , so : $$ w_{tot} = 2 w/m^2 \times 10^{-4} m^2 $$ the total number of photons is $w_{tot}$ divided by the energy per photon ( in joules ) , and the energy per photon is : $$ e = 10.6ev \times 1.602 \times 10^{-19} $$ where $1.602 \times 10^{-19}$ is the conversion factor from ev to joules . so the total number of photons per second is : $$ n = \frac{w_{tot}}{e} = \frac{2 w/m^2 \times 10^{-4} m^2}{10.6ev \times 1.602 \times 10^{-19}} $$ multiply by 0.53/100 because only 0.53% of photons expel a photoelectron and you get your formula .
you say in particular i have had a lot of trouble understanding how something like interatomic potentials can be seen as mass . which i agree sounds confusing . there is a relationship between energy and mass , but there is a murkier relationship between potential and mass . interatomic interactions are a bad place to start because the energies involved are so small . if you have a molecule that can be dissociated with visible light , its binding energy is a few electron volts . its mass ( in energy units ) is probably a few billion electron volts . it is just not a very big correction . it is probably a little easier to think about in nuclear interactions , where the binding energies may be a percent-scale correction to the mass . suppose i have a carbon-12 atom , which has a mass of exactly 12 atomic mass units , or 12 u . ( this is exact , because carbon-12 is the definition of the atomic mass unit . ) let 's split this atom into helium-4 atoms . i know that the mass of each helium atom will be approximately 4 u . but if i measure the masses , with a spectrometer that is calibrated so that carbon-12 gives 12.000 000 u , i find that each helium atom has a mass of 4.002 602 u . in order to make my three helium atoms , i had to generate roughly 0.0078 u of mass that did not exist before ! furthermore , you have probably noticed that carbon atoms do not spontaneously convert into helium atoms . in your experiment where you are splitting the carbon to make helium , you will discover that you get zero production of helium while the energy that you put into your interaction is less than about 7.3 mev . but when you " turn on " the helium production by putting in , say , 10.3 mev , you find that the three helium nuclei that you produce have a kinetic energy of about 1 mev each . this is because energy and mass are equivalent . adding 1 mev of energy to a nucleus has the same effect on its dynamics as increasing its mass by 0.001 703 u . a carbon-12 nucleus can be thought of as three helium nuclei that are stuck together ; to unstick them you have to add at least 7.3 mev of energy , and that energy will continue to be stored in the system as additional mass in the helium nucleus . conversely , when three helium nuclei fused in a long-ago star to form that carbon nucleus , that new carbon nucleus released a 7.3 mev photon which wandered out into the heat bath of the core of the star and eventually made it to the surface as a few million quanta of visible light . similarly , if you have a uranium-235 atom , it'll weigh 235.043 908 u . put it in with its friends and it may fission into a strontium-90 ( mass 89.907 784 u ) , a xenon-143 ( mass 142.935 191 u ) and two neutrons ( mass 1.008 660 u ) . add these up and you have lost nearly 0.1 u ! this reaction also releases lots of energy , as you are probably aware .
the ontological phrasing you use to describe quantum teleportation should be a red flag . there is no sense it which the quantum state " appears " . it may be the case that you are confounding the physical system itself ( real , ontological ) with the quantum state assigned to it by the experimenter ( subjective , epistemic ) . in other words , think of the quantum state as the information one has about a physical system and not the physical system itself . if the fidelity between the final state and the input is not 1 , then some operation in the protocol was not perfect . of course , this will always be the case . so , you should understand " quantum teleportation " as the protocol itself and not the act of transferring exactly the quantum state of one physical system to another . for any practical application of the protocol , it need not be perfect to be useful .
to study the spectra of earth-like planets in transit across their stars , we will need an observatory in space so that we will not be affected by spectral lines from the earth 's atmosphere . unfortunately , we will not then be able to use large apertures on earth to smooth down the noise . we will also want to go to wavelengths of several microns in the infrared to get deep absorption lines , and also get to a weaker portion of the spectrum of the host star . then it is simply a matter of integrating for a long time during transits and subtracting the spectrum of the star during transit from the spectrum when there is no transit . this will give the absorption spectrum due to the ring of atmosphere visible around the disk of the planet . but transits are few and far between so it takes a long time to build up integration time . one way around needing an observatory in space may be to choose a star with a high radial velocity so that a planet 's spectral lines will be red or blue-shifted relative to absorption from the earth 's atmosphere . then the largest ground-based observatory could be used , or even a consortium of all the largest telescopes be employed at the same time to get the best possible signal . using interferometric techniques in space to localize on only a portion of the star being transited , would one day offer a good way to improve the signal-to-noise ratio .
an obvious example is the hubble bubble though this does not invalidate the flrw model , it just means the homogeneity is on a larger scale than the observable universe . however i get the impression the hubble bubble idea is not widely considered likely .
i tend to think that you are right that the phase - but not the absolute value - may be decomposed into infinitesimal pieces of solid angle . and for each tiny solid angle , the phase you get is the same . it is not hard to see what happens with the $|jm\rangle$ eigenstate under a parallel transport around an infinitesimal solid angle $\omega$ . it simply gets transformed by the phase $$\exp ( i m \omega ) . $$ in your notation , $n_{jm}=m$ . it is because it reduces to exponentials of small rotations around $x , y , -x , -y$ axes , and the commutator boils down to $j_z$ for the usual reasons . for your example , $\omega$ was $4\pi/8=\pi/2$ and $m$ was $1/2$ . that is why you got $\exp ( \pi i /4 ) $ . of course , i realize that this answer seems to contradict the previous problem you posed because $\exp ( 4\pi i m ) $ equals one for any conceivable value of $m$ , so there should be no obstruction ever . so at this moment , i am not sure which answer is the right one . i would guess that my answer to your previous problem is completely wrong - there may exist bundles that can not be obtained simply as products of the non-existing low-spin bundles . and there could also be an issue with the overall sign . note that if the inner products between the two states remain real for any angle , you eventually hit the point where the inner product goes negative , but at this point , the absolute value changes the sign relatively to the inner product . cheers lm
your first criterion $ \underline{g}^l=\underline{g}^v $ does not apply for a mixture , though possibly not for the reasons you think . with a pure material " a mole " has an unambiguous meaning , but for a mixture you have to choose what you are taking to be a mole . you are taking a weighted average of the components present , but unless the composition of the liquid and vapour phases are the same ( in general they will not be ) " a mole " of the liquid is not the same as " a mole " of the vapour . that is why your equation does not apply . the only conditions under which " a mole " would be the same for the liquid and vapour phases is if the compositions of the liquid and vapour phases were the same , i.e. $x_i^l=x_i^v$ . that is why you have ended up concluding that your criterion could only apply if $x_i^l=x_i^v$ , and as you say that is usually wrong .
the do not disappear with zero energy . their energy ( both that originating from rest mass and any kinetic energy ) appears somehow . as photons , a spray of other ( lighter ) particles , etc . for instance , when an electron meets a positron ( that is , a anti-electron ) the most common result is a pair of gamma rays each of 511 kev ( in the center of momentum ( com ) frame of the $e$--$e^+$ pair ) . it has to be at least two because you have to conserve both energy and momentum ( and angular momentum , too , but that is a complication we will ignore for the moment ) , and that also tells us that there have opposite momenta in the com frame .
it is mainly an electrostatic interaction through a dipole-dipole interaction . however , the dipole moment can be permanent or induced . depending on its nature the force has a different name : force between two permanent dipoles ( keesom force ) force between a permanent dipole and a corresponding induced dipole ( debye force ) force between two instantaneously induced dipoles ( london dispersion force ) you can also refer vanderwaals force an atom or molecule is usually globaly neutral : ie there is exactly the same number of positive and negative charges . however the center of charge ( barycenter of the qi ) for the positive and negative charges does not always coincide . this gives rise to an electrostatic dipole that can interact with an external electric field . to answer to your first bullet , the electrostaic dipole is not linked to electrons changing orbital . the cohesive forces in solids have two different origin : orbital coupling ( not of electromagnetic nature ) or ionic bonds in ionic crystals ( na+ , cl- ) for instance . in the latter the cohesion of the crystal is due to electrostaic interaction ( not dipolar ) . for the liquids , the electrostatic forces are responsible for the observed properties . polar solutions are made of molecules with a dipolar moment and are able to dissolve ionic crystals .
this is a solenoid and its magnetic field lines . this is a toroid and its magnetic field lines a solenoid by construction has two magnetic poles at the edges when current is flowing through its windings . one can think of a toroid as a solenoid that has been curved and joined so no poles are open . a toroid can have magnetic fields outside its geometrical boundary according to the way the currents are flowing , if there is a circumferential current that has not been neutralized . once neutralized there is magnetic field only inside . ( as described in the link given above ) . a neutralizing design is shown below . in contrast a solenoid will always have two open poles .
if $\theta$ is the angle between the arms , displaced from the equilibrium $\theta_0$ by $\delta \theta$ and the torque applied is $\tau =-\kappa \delta \theta$ , assuming equal masses of $m$ with initially motionless parts . the first step is the kinematics , whereas the acceleration of 2 and 3 is related to the acceleration of 1 and the common angle . for simplification we have that 1 is not accelerating in the horizontal direction $\ddot{x}_1=0$ ( as seen in figure below ) . $$ \begin{aligned} \ddot{x}_2 and = \ddot{x}_1 - \ell \cos \left ( \frac{\theta}{2} \right ) \frac{ \ddot{\theta}}{2} and \ddot{x}_3 and = \ddot{x}_1 + \ell \cos \left ( \frac{\theta}{2} \right ) \frac{ \ddot{\theta}}{2} \\ \ddot{y}_2 and = \ddot{y}_1 + \ell \sin \left ( \frac{\theta}{2} \right ) \frac{\ddot{\theta}}{2} and \ddot{y}_3 and = \ddot{y}_1 + \ell \sin \left ( \frac{\theta}{2} \right ) \frac{\ddot{\theta}}{2} \end{aligned} $$ now for the equations of motion of each part . we start with free body diagrams in order to sum up the forces on each part . $$\begin{aligned} -fr_2 \sin \left ( \frac{\theta}{2} \right ) + fr_3 \sin \left ( \frac{\theta}{2} \right ) + fn_2 \cos \left ( \frac{\theta}{2} \right ) + fn_3 \cos \left ( \frac{\theta}{2} \right ) and = m \ddot{x}_1 = 0 \\ -fr_2 \cos \left ( \frac{\theta}{2} \right ) - fr_3 \cos \left ( \frac{\theta}{2} \right ) + fn_2 \sin \left ( \frac{\theta}{2} \right ) + fn_3 \sin \left ( \frac{\theta}{2} \right ) and = m \ddot{y}_1 \end{aligned} $$ the eom are done in a direction along the arm $$\begin{aligned} m \ddot{x}_2 \cos \left ( \frac{\theta}{2} \right ) - m \ddot{y}_2 \sin \left ( \frac{\theta}{2} \right ) and = -fn_2 \\ m \ddot{y}_2 \cos \left ( \frac{\theta}{2} \right ) + m \ddot{x}_2 \sin \left ( \frac{\theta}{2} \right ) and = fr_2 \\ 0 and =\ell fn_2 + \tau \end{aligned} $$ with $\rightarrow fn_2 =-\frac{\tau}{\ell}$ $$\begin{aligned} m \ddot{x}_3 \cos \left ( \frac{\theta}{2} \right ) + m \ddot{y}_3 \sin \left ( \frac{\theta}{2} \right ) and = -fn_3 \\ m \ddot{y}_3 \cos \left ( \frac{\theta}{2} \right ) - m \ddot{x}_3 \sin \left ( \frac{\theta}{2} \right ) and = fr_3 \\ 0 and =\ell fn_3 - \tau \end{aligned} $$ with $\rightarrow fn_3 =\frac{\tau}{\ell}$ combined the all of the above equations substituted into the kinematics are $$ \begin{aligned} -fn_2 \cos \left ( \theta \right ) + fr_2 \sin \left ( \theta \right ) - 2 fn3 and = m \ell \frac{\ddot{\theta}}{2} \\ fr_2 \cos \left ( \theta \right ) + fn_2 \sin \left ( \theta \right ) + 2 fr_3 and = 0 \\ - fn_3 \cos \left ( \theta\right ) - fr_3 \sin \left ( \theta \right ) + 2 fn_2 and = - m \ell \frac{\ddot{\theta}}{2} \\ fr_3 \cos \left ( \theta \right ) - fn_3 \sin \left ( \theta \right ) + 2 fr_2 and = 0 \end{aligned} $$ the above is solved with $$\boxed{\frac{3 \tau ( \cos\theta-2 ) }{\ell ( \sin^2\theta+3 ) } = m \ell \frac{\ddot{\theta}}{2}}$$ and $$\begin{aligned} fr_2 and = \frac{\tau \sin\theta ( 2-\cos\theta ) }{\ell ( \sin^2\theta+3 ) } \\ fn_2 and = -\frac{\tau}{\ell} \\ fr_3 and = \frac{\tau \sin\theta ( 2-\cos\theta ) }{\ell ( \sin^2\theta+3 ) } \\ fn_3 and = \frac{\tau}{\ell} \end{aligned}$$
the spin referred in condensed matter is the spin of the electrons least bound to the atoms ( usually valance electrons ) . the atoms reside on the lattice sites . a spin half problem means the atoms have only one valance electron . but there are other possibilities like spin 1 , 3/2 and all . as qeb has already mentioned it can also be used for nuclear spins also . i just want mention that any two level quantum system can be represented as a effective spin 1/2 problem .
the definition for point particles is : $t = \frac{p^2}{2m}$ , where $t$ denotes my kinetic energy . if you are dealing with classical physics , the momentum for a point-particle equals $\vec{p} = m\vec{v}$ , which would give you $t = \frac{mv^2}{2}$ . in the theory of special relativity we notice that objects become heavier when they move , the momentum becomes : $\vec{p} = {m\vec{v}\gamma}$ , where $\gamma = \frac{1}{\sqrt{1- ( v/c ) ^2}}$ . hence the kinetic energy becomes : $t={mc^2\gamma}$ , this is derived below . if you try to make a non-relativistic electromagnetic lagrangian , it is not possible to incorporate the magnetic field in terms of a potential energy . so the solution is to define the momentum equal to $\vec{p} = m\vec{v}-\frac{q\vec{a}}{c}$ , where a is the vector potential so that $\vec{b} = \nabla\times\vec{a}$ . this would give an kinetic energy ( or kinetic term in your hamiltonian ) equal to : $t = \frac{ ( m\vec{v}-\frac{q\vec{a}}{c} ) ^2}{2m}$ . one often calls $m\vec{v}$ the kinematic momentum and $\vec{p}$ the canonical momentum ( since this is the momentum that follows from a lagrangian ) . it are the canonical momenta that are called " thé momenta " . and last but not least , in quantum mechanics you work on a different kind of mathematical structure ( in hilbert-space in stead of the classical phase-space ) . by equivalence with wave-theory the momentum is derived as $\hat{p} = \frac{\hbar}{i}\frac{\partial}{\partial x}$ , where we have an operator $\hat{p}$ which works on that hilbertspace . note : from dj_mummy note that the given relations for kinetic energy all assume the particle was originally at rest at a given time $t=t_0$ . the kinetic energy is derived by using the work-energy theorem ( difference in kinetic energy = work done on the point particle ) . using the laws of classical physics ( newton 's second law ) we get : $t = \int\limits_{x_0}^{x_1}\vec{f}\cdot d\vec{x} = \int\limits_{t_0}^{t_1}\vec{f}\cdot \vec{v}\ , dt = \int\limits_{t_0}^{t_1}\left ( m\frac{d\vec{v}}{dt}\right ) \cdot \vec{v}\ , dt= m\int\limits_{t_0}^{t_1} \vec{v}\cdot d\vec{v} = \frac{m}{2}v^2$ . where $\vec{v}$ is the velocity at time $t_1$ . this gives us the kinetic energy in the case of classical physics ! for the relativistic particle we define a 4-momentum $p^\alpha = m\frac{du^\alpha}{d\tau}$ . where $u^\alpha$ is the four-velocity and $\tau$ is the proper time . the derivation is actually done here and we get the formula above !
entropy is an extensive property . if you double the size of your system but keep the conditions within it the same then you will double the entropy . you can not do this without changing other properties of the system because lots of things , e.g. free energy , are also extensive . if you keep the size of your system constant but pack in twice as much gas then the entropy will change for two reasons , firstly because you have more gas , but secondly because you have changed the pressure and/or temperature . you need to specify the state of your system after addition of the gas to work out the change in the entropy .
brian cox was talking about stellar nucleosynthesis . in theory you could combine the nucleus of a helium atom ( also called an alpha particle ) and the nucleus of a carbon atom to produce an oxygen nucleus and some energy in the form of gamma radiation $$ ^4_2he + ^{12}_6c \rightarrow ^{16}_8o +\gamma$$ this is an alpha process , and does happen inside certain stars . in practice , you are unlikely to be able to do this personally , as you probably cannot get the nuclei moving fast enough or accurately enough to enable fusion .
temperature is proportional to the average kinetic energy , not velocity , of the particles . kinetic energy is unbounded ; it goes to infinity as velocity approaches light-speed , proportional to $ ( 1 - v^2/c^2 ) ^{-1/2}$ .
a neutron bomb is still a hydrogen bomb , just designed in such a way as to allow much of the neutron radiation to escape , instead of remaining trapped to enhance the chain-reaction . a neutron bomb explosion would be basically the same as a hydrogen bomb , just with a little less explosive energy , and a little more neutron radiation---making it more harmful to people ( especially because its harder to shield from neutrons ) .
this can be solved in a single line of math : $$\left\langle\rho_1 , \phi_2\right\rangle=\left\langle\rho_1 , \nabla^{-2}\rho_2\right\rangle=\left\langle\nabla^{-2}\rho_1 , \rho_2\right\rangle=\left\langle\phi_1 , \rho_2\right\rangle$$ where the self-adjointness property of $\nabla^{2k}$ has been used , and $\left\langle\right\rangle$ represents the integration inner product . alternately , you can also try your approaches " a " and " b " , but they will be far more complicated than the method i gave above . method " a " proceeds via a physical pictoral argument ; in short , what is the self-assembly energy of the non-overlapping distribution pair ? the self-assembly energy ought to be invariant with respect to its manner of construction , and there are two obvious ways to do it . try it and see what you get . method " b " is a direct application of the multivariate product rule ; just apply calculus . it'll give the answer you want .
the vacuum dirac equation automatically implies the klein-gordon equation . it means that every solution to the vacuum dirac equation is automatically a solution to the klein-gordon equation . the converse of course does not hold . the most basic reason is that the klein-gordon equation should really act on scalars , a single bosonic field , while the minimum number of components for the $d=4$ dirac equation is four ( and they should be fermionic fields ) . so a general ( or generic ) valid solution to the klein-gordon equation is a valid solution to the klein-gordon equation ( this much is a tautology , but you were asking about it ) , but it is not a solution to the dirac equation . even if you combine 4 solutions to the klein-gordon equation , declare that they are 4 components of a dirac spinor , and ask whether they solve the dirac equation , the answer is no . it is because the dirac equation is really " stronger " than the klein-gordon equations for its components . effectively , the dirac equation is first-order while the klein-gordon equation is second-order . the dirac equation implies certain correlations between the spin ( up/down ) of the particle and the sign of the energy ( positive/negative ) . the quadruplet of klein-gordon equations allows all combinations of spin up/down and the sign of the energy . however , the most general quadruplet of solutions to the klein-gordon equation may be written as a solution of the dirac equation with a positive mass and a solution to the dirac equation with a negative ( opposite ) mass . the dirac equation describes spin-1/2 ( and therefore " fermionic" ) particles such as electrons , other leptons , and quarks , while the klein-gordon equation describes spin-0 " scalar " ( and bosonic ) particles such as the higgs boson . however , before they do the proper job , the " wave functions " have to be promoted to full fields and these fields have to be quantized .
you will have two forces that act on an elementary mass element $dm$ on the surface . the force in the $x$-direction will be $df_{x}=\omega^{2}xdm$ and in the $y$-direction $df_{y}=gdm$ . also , we know that the slope of a curve is $\tan{\alpha}=dy/dx$ . however , the tangent is equal also to $\tan{\alpha}=df_{x}/df_{y}$ . so from this you have that $$\frac{dy}{dx}=\frac{\omega^{2}x}{g}$$ after integration you get $$y=\frac{\omega^{2}}{2g}x^{2}$$ which is just the equation for a parabola . this is a two-dimensional derivation based on the stagnant interface . a more general solution would be as follows . consider the axis $oz$ along the cylinders axis . in this case , the velocity components will be $v_{x}=-\omega y$ , $v_{y}=\omega x$ , $v_{z}=0$ . taking euler 's equation $$\frac{\partial\vec{v}}{\partial t}+ ( \vec{v}\cdot\nabla ) \vec{v}=-\frac{1}{\rho}\mathrm{grad}p$$ considering that $\partial\vec{v}/\partial t=0$ , the projections on the three axis on euler 's equation are $$x\omega^{2}=\frac{1}{\rho}\frac{dp}{dx}$$ $$y\omega^{2}=\frac{1}{\rho}\frac{dp}{dy}$$ $$\frac{1}{\rho}\frac{dp}{dz}+g=0$$ the general solution of these equations is $$\frac{p}{\rho}=\frac{1}{2}\omega^{2} ( x^{2}+y^{2} ) -gz+c$$ on the free surface , where the pressure is constant , the surface will have the shape of a paraboloid . $$z=\frac{\omega^2}{2g} ( x^{2}+y^{2} ) $$
muonic atoms should be stable in electron-degenerate matter ( white dwarf material ) as long as the fermi energy is more than $m_\mu - m_e$ . this is more or less exactly a analogy with neutron stability in the nucleus where the the protons are effectively in a degenerate state . any answer has to forbid electrons ( which is not going to be possible as they share almost all quantum numbers with the muon ) or have electron be degenerate with the stated fermi energy .
the zitterbewegung is more of a relic of the early dirac equation days . it does not exist in the standard position , velocity and acceleration operators of the single particle field , only in alternatively derived versions . these alternative versions were developed because people thought the standard operators were wrong . in fact they did not understand the standard operators . the standard method is using : $\frac{\partial {\cal \tilde{o}}}{dt} ~=~ \frac{i}{\hbar}\ ! \ ! \left [ ~\tilde{h} , \tilde{o}~\right ] $ where the misunderstanding comes from is easy to see in the modern chiral representation . we will show that the standard operators are correct . if we define a position , a velocity and an acceleration operator for the dirac field then the ( averaged ) position and velocity and acceleration are given by : position , velocity and acceleration operators applied on the dirac field : $\vec{x}_{avg} ~=~ \frac{1}{2mc}\int dx^3 ~~ \psi^* \vec{x}~\psi ~~~~~~~~~ ( \vec{x}: \mbox{position operator} ) $ $\vec{v}_{avg} ~=~ \frac{1}{2mc}\int dx^3 ~~ \psi^* \vec{v}~\psi \ , ~~~~~~~~~ ( \vec{v}: \mbox{velocity operator} ) $ $\vec{a}_{avg} ~=~ \frac{1}{2mc}\int dx^3 ~~ \psi^* \vec{a}~\psi \ , ~~~~~~~~~ ( \vec{a}: \mbox{acceleration operator} ) $ velocity operator now $\vec{x}$ is simply the position $\vec{x}$ of each point of the wavefunction . the velocity operator can be derived by commutating with the hamiltonian . $\tilde{v}^i\psi\ =\ \frac{i}{\hbar}\left [ ~\tilde{h} , \tilde{x}^i~\right ] \psi\ =\ c \left ( \begin{array}{cc} -\sigma^i and 0 \\ 0 and \sigma^i \end{array} \right ) \psi$ this velocity operator is in fact totally correct but it was thought to be erroneous in the early days because people misunderstood it to mean that the electron can only move with $\pm\ , c$ , and therefor it must be wrong , they thought . what they were actually expecting was something like the $\vec{v}=\vec{p}/m$ as they got in non relativistic theories , but they found something which only contained $\pm\ , c$ . however , if we evaluate the expression for $\vec{v}_{avg}$ then we get . $\vec{v}_{avg} ~=~ \frac{c}{2mc}\int dx^3 ~~ \psi^* \left ( \begin{array}{cc} -\sigma^i and 0 \\ 0 and \sigma^i \end{array} \right ) \psi ~~=~~ \frac{c}{2mc}\int dx^3 ~~ \bar{\psi} \gamma^i \psi ~~=~~ \frac{c}{2mc}\int dx^3 ~ j^i$ this is an integral over the current density , or the momentum with the appropriate units . now the momentum $\vec{p}$ is a factor $\gamma$ larger as the velocity $\vec{v}$ but the integral over the lorentz contracted field compensates this so we end up with the velocity of the particle as required ! the velocity operator is perfectly fine . the other big misunderstanding was that the x , y and z-components of the velocity operator do not commute while they do so in the non-relativistic theory and therefor the operator must be wrong , they thought . you can still find this quoted in many textbooks . but as you see the expression derives the velocity from the momentum and as we know the momentum components ( the boost components ) should not commute . in fact they should commute just like in the velocity operator . again the operator behaves exactly in the right way , and it does not show a zitter-bewegung at all acceleration operator we will briefly handle the standard acceleration operator as well and show that there is no zitterbewung and that the result transforms in the right way under lorentz transform . it can be actually be shown that it transforms like the lorentz force $\psi^*\tilde{a}^i\psi ~~=~~ \frac{i}{m}\frac{d\vec{p}}{dt} ~~\mbox{transforms like:}~~ \frac{iq}{m}\left ( \vec{v}\times\vec{b} ~+~ \vec{e}\right ) $ because $\psi^*\tilde{a}^i\psi $ gives rise to two terms which transform like the electron 's magnetization and polarization . the construction which therefor transforms like the lorentz force is thus actually . $\psi^*\tilde{a}^i\psi ~~\mbox{transforms like:}~~ \frac{iq}{m}\left ( -\vec{v}\times\mu_o\vec{m} ~+~ \frac{1}{\epsilon_o}\vec{p}\right ) $ if you note that $\vec{v}\times\vec{m}~\propto~\vec{p}\times\vec{j}_a$ then you can recognize the two terms in the standard acceleration operator which is . $\psi^*\tilde{a}^i\psi ~~=~~ c~\bar{\psi}\left [ \ , \gamma^i\gamma^5\times ( \partial_i-i\frac{e}{\hbar}\ ! a_i ) ~\right ] \psi~+~ \frac{imc^3}{\hbar}~\bar{\psi}\gamma^0\gamma^i\psi$ the acceleration is zero in a plane-wave in the absence of a b or e field . in this case the electron field has its own inherent m and p values and the two terms cancel each other . if the inherent m and p values change because of external b and e fields ( by addition ) then the electron accelerates . chiral representation now what about the c in the velocity operator . this behavior of the propagator is easy to understand in the modern chiral representation and the propagator of the field . in principle all fields are massless and propagate with c . due to coupling however propagators can have any speed between +c and -c . the electron has two such massless components . $\psi~~=~~\left ( \begin{array}{c}\psi_l\\ \psi_r \end{array}\right ) $ so , these two components do move at the speed of light . in the rest frame they move exactly opposite to each other and the combined speed is zero . the big difference with the zitterbewegung is that they both happen at the same time . there is no overall alternating net velocity . now the time evolution in the restframe is . $e^{-ht}\left ( \begin{array}{c}\psi_l\\ \psi_r \end{array}\right ) ~~=~~ \left ( \begin{array}{c}\psi_l\cos ( mt ) -i\psi_r\sin ( mt ) \\ \psi_r\cos ( mt ) -i\psi_l\sin ( mt ) \end{array}\right ) $ so , you see the $\psi_l$ and $\psi_r$ alternating but is there a zitterbewegung of $\psi$ or the individual components $\psi_l$ and $\psi_r$ ? the answer is : no for electrons and no for positrons . this is because these are exactly the only two solutions of the dirac equation which do not show a zitterbewegung . the reason for this is . electron at rest : $\psi_l=+\psi_r$ positron at rest : $\psi_l=-\psi_r$ the other " exotic " states where $\psi_l\neq\pm\psi_r$ at rest do show a zitterbewegung , for instance $\psi_l=i\psi_r$ or $\psi_l=\sigma_z\psi_r$ . this is actually the reason why these states are not allowed . they would radiate away electromagnetic energy with the frequency corresponding to their mass . hans .
note : this started out as a quick answer and kind of got out of hand . . . but everything below the second line is probably optional actually , everything is quantum . the term " classical object " in that sense just refers to a system in which the classical approximation is valid , i.e. where quantum effects like uncertainty are negligible . think about it like this : when you make a measurement , it changes the wavefunctions of both the measured particle and the detector . for example , suppose you are measuring the momentum of an elementary particle . when you make the measurement , the particle is going to lose some momentum and the detector gains the corresponding amount of momentum ( or vice versa ) . however , the detector consists of a large number of particles , usually something in the $10^{20}$-ish range , so if you add or subtract one particle 's worth of momentum , the change in the detector 's wavefunction is pretty negligible . because of this , you can use the classical approximation that the detector is unchanged by the measurement , and thus you say it is a classical object . you can even do a quantitative analysis of this . but first , a quick aside on decoherence . suppose your particle starts out in a state $\int\mathrm{d}p_p\ f_p ( p_p ) |p_p\rangle$ and the detector starts out in a state $\int \mathrm{d}p_d\ f_d ( p_d ) |p_d\rangle$ , where $f_p$ and $f_d$ are the momentum-space wavefunctions of the particle and detector , respectively . the measurement process basically convolves the two states with a parameter that represents the momentum transferred , so the final state winds up being $$\int\mathrm{d}p_p\mathrm{d}p_d\mathrm{d}q\ f_p ( p_p ) f_d ( p_d ) f ( q ) |p_p - q\rangle |p_d - q\rangle$$ where $f ( q ) $ is some wavefunction describing the relative probabilities of various results of the measurement . in practice , we will read off a particular value of $q$ , call it $q_0$ , from the measuring device , which is mathematically represented by inserting a delta function $\delta ( q - q_0 ) $ under the integral . that is what makes the states of the detector and particle appear to decohere , $$\int\mathrm{d}p_p\mathrm{d}p_d\mathrm{d}q\ f_p ( p_p ) f_d ( p_d ) f ( q ) \delta ( q - q_0 ) |p_p - q\rangle |p_d + q\rangle$$ $$\propto\int\mathrm{d}p_p\ f_p ( p_p ) |p_p - q_0\rangle\int\mathrm{d}p_d\ f_d ( p_d ) |p_d + q_0\rangle$$ now , suppose the detector contains 150 particles of the same type as the one being measured . each of these 151 particles has a gaussian momentum distribution $g ( p ) $ , all with the same variance . for the particle wavefunction , $f_p ( p_p ) = g ( p_p ) $ . but the detector wavefunction is a little more complicated , since you have to obtain it by adding up 150 copies of the wavefunction $g ( p ) $ using the arithmetic of random variables ; it winds up being a gaussian with $\sqrt{150}$ times the width and $150$ times the mean . anyway , with the setup established , let 's move on to the results . before the measurement , the particle ( green ) and detector ( red ) wavefunctions look like this , here i have scaled the detector wavefunction down by 150 on the horizontal axis , so you are really looking at the average momentum per particle in the detector . this way it is easier to get a sense of the relative effect of any change in momentum . now suppose i happen to measure $q_0 = 5$ in whatever units we are working with . using the formula above , here 's what the decohered wavefunctions look like : notice that the particle wavefunction has shifted quite a bit , but the ( scaled ) detector wavefunction has barely moved at all . so it is still probably a reasonable approximation to say that the detector has zero momentum , i.e. you can treat it like a classical object .
it depends how good of an approximation you want . if you just want something that looks like starlight to the human eye then it is not too hard - you can buy solar spectrum bulbs at any hardware store . but of course , this is not going to give you a great approximation , and it is only going to be anywhere close in the visible wavelengths . if you want something that is going to be a fairly good approximation across a very wide range of wavelengths then just heat any random object up to between $3600\:\mathrm{k}$ and $50,000\:\mathrm{k}$ , depending on the star . ( those massive blue stars at $50,000\:\mathrm{k}$ will present a challenge , but i think it is within the bounds of experimental possibility . ) this works because stars and other hot object both emit spectra that are close to the ideal black body spectrum . you can get an idea of how close by comparing the curve on this graph to the edge of the yellow region : ( image source . ) if you want to reproduce all those deviations from the ideal black body curve then it is going to be a bit harder , but it is probably doable if you have a good enough reason to bother . i would guess that a good technique would be to surround your black body with gases similar to the star 's corona , in order to reproduce the absorption lines . emission lines will be a bit more tricky , but i guess if there is no other way you could simply heat those gases up to the appropriate temperature . the uniqueness of a star 's spectrum comes mostly from its temperature and its composition , i.e. the gases that make it up , so by using this method you could probably more or less simulate the spectrum of a specific star . this method would simulate the spectrum of light that the star emits , but if you wanted to simulate the spectrum that we actually see it would be much harder . this is because the spectra of distant stars are modified by a redshift , caused by the fact that distant galaxies are moving away from us . the redshift is basically the optical equivalent of the doppler effect , and it causes us to see frequencies lower than what the star emits . if you wanted to simulate this in the laboratory you would have to use a different method than the one i have described , such as the customised diffraction gratings described in rod vance 's answer . of course , if you meant " simulate on a computer " then it is a different question . i think this is probably not too hard - you just need to look up the appropriate emission and absorption spectra and add them up in the right way . i am sure people researching stars ' spectra do this all the time .
you can not see clearly underwater for a couple of reasons . one is the thickness of your lens , but the main one is the index of refraction of your cornea . for reference , here 's the wikipedia picture of a human eye . according to wikipedia , two-thirds of the refractive power of your eye is in your cornea , and the cornea 's refractive index is about 1.376 . the refractive index of water ( according to google ) is 1.33 . in water , your cornea bends light as much as a lens in air whose refractive index is $$\frac{1.376-1.33}{1.33} + 1 = 1.034$$ that means you are losing about 90% of your cornea 's refractive power , or 60% of your total refractive power , when you enter the water . the question becomes whether your lens can compensate for that . i did not find a direct quote on how much you can change the focal distance of your lens , but we can estimate that your cornea is doing essentially nothing , and ask whether your lens ought to be able to do all the focusing itself . for a spherical lens with index of refraction $n$ sitting in a medium with index of refraction $n_0$ , the effective focal length is $$f = \frac{nd}{4 ( n-n_0 ) }$$ the refractive index of your vitreous humor is about 1.33 ( like water ) , and the refractive index of your lens , according to wikipedia , varies between 1.386 and 1.406 . let 's take 1.40 as an average . then , plugging in the numbers , the effective focal distance of a spherical eye lens would be five times its diameter . the wikipedia picture of a human eye makes this look reasonable - a spherical lens might be able to do all the focusing a human eye needs , even without the cornea . the problem is that your eye 's lens is not spherical . from the same wikipedia article in many aquatic vertebrates , the lens is considerably thicker , almost spherical , to increase the refraction of light . this difference compensates for the smaller angle of refraction between the eye 's cornea and the watery medium , as they have similar refractive indices . [ 2 ] even among terrestrial animals , however , the lens of primates such as humans is unusually flat . [ 3 ] so , the reason you can not see well underwater is that your eye lens is too flat . if you wear goggles , the light is refracted much more as it enters the cornea - the same amount as normal . if you want to wear some sort of corrective lenses directly on your eye like contact lenses , they should have a refractive index as low as possible . googling for " underwater contact lens " , i found an article about contact lenses made with a layer of air , allowing divers to see sharply underwater .
a fluid motion in a vortex creates a dynamic pressure that is lowest in the center increasing radially ( $p \propto r^2$ ) . the gradient of this pressure that forces the fluid to rotate around the axis . this is usually represented by a vector called vorticity , and defined by $\omega = \nabla \times v$ . in simple terms , this means that the fluid is rotating around a certain point . if you placed a small ball on the flow , you would observe that is would rotate about the center and the direction of vorticity vector is given by the right-hand rule . the formation methods are many . for example , in the wake of an engine , there air has been given rotational momentum and will continue to have vorticity . when two opposite flows meet , they can also form as in planetary systems , like tornadoes or jupiter great red spot
the assumption is , that the spin $s$ is a large parameter . a conjecture that is apparently not valid for $s=1/2$ . the expansion is in $1/s$ , which is assumend to be close to zero . $$ s^+_j = \sqrt{2s-n_j}a^\dagger_j = \sqrt{2s}\sqrt{1-\frac{n_j}{2s}}a^\dagger_j\approx\sqrt{2s}\cdot\left ( 1-\frac{n_j}{4s}\right ) a^\dagger_j$$ the second term , being of order $s^{-1/2}$ , is neglected . assuming $s$ to be large , amounts to a semiclassical approximation . in this limit the relative uncertainty of the spin-operators becomes vanishingly small . ( use the spin-algebra to see this . ) $$ \frac{\delta s^i\delta s^j}{s^2} \longrightarrow 0$$ the working hypothesis is , that low-energy excitations are realized as small deviations around the fully aligned groundstate . for small spin , e . g $s=1/2$ there is no way to only slightly deviate from let 's say $s^z_i=+1/2$ which brings us back to your objection/observation , that the expansion is not valid in the ' small-spin ' limit .
line integrals of the magnetic field strength are magnetic voltage drops . just google for " magnetic voltage drop " ( including the double-quotes ) . in the quasi-static case ( $\dot{\vec{d}}=\vec{0}$ ) the $\vec{h}$-field within a simply path-connected domain with zero current density has a magnetic potential . in this case you can calculate the magnetic voltage drops as potential differences . the magnetic voltage caused by a winding is called current-linkage or ampere-tuns . from the mathematical point of view the closed path integral of the magnetic field strength is a circulation .
well , vectors ( 3d vectors ) do not really transform linearly . unlike galilean transformations , you need now know anything " extra " when transforming a vector . here , due to the " mixing " of space and time , you do . to transform displacement , you need to know time , and vice versa . same with energy and momentum . four-vectors , on the other hand , transform linearly . these are four-dimensional vectors which transform linearly via the lorentz matrix ( $\beta=\frac{v}{c} , \gamma=\frac{1}{\sqrt{1-\beta^2}}$ ) : $$l=\begin{bmatrix} \gamma and -\beta \gamma and 0 and 0\\ -\beta \gamma and \gamma and 0 and 0\\ 0 and 0 and 1 and 0\\ 0 and 0 and 0 and 1\\ \end{bmatrix}$$ for example , if you want to transform position and/or time , you use the four-position $$x=\begin{bmatrix} c t \\ x \\ y \\ z \end{bmatrix}$$ this can be compactly written as $ ( ct , \vec x ) $--this just means that you can expand the second " vector " term to get the next three four-vector components . anyway , the four vector transforms as : $$x'=l\times x$$ ( matrix product ) this , expanded , is : $$\begin{bmatrix} c t ' \\ x ' \\ y ' \\ z ' \end{bmatrix} = \begin{bmatrix} \gamma and -\beta \gamma and 0 and 0\\ -\beta \gamma and \gamma and 0 and 0\\ 0 and 0 and 1 and 0\\ 0 and 0 and 0 and 1\\ \end{bmatrix} \begin{bmatrix} c\ , t \\ x \\ y \\ z \end{bmatrix} , $$ which are your normal lorentz transformations . a property of four vectors is that if we are talking about the same four vector $ ( a , \vec b ) $ in two frames , the value of $a^2-|\vec b|^2$ is the same in both . for four-position , you get $c^2t^2-x^2-y^2-z^2=c^2t'^2-x'^2-y'^2-z'^2$ other four vectors are : four-velocity : $ ( \gamma c , \gamma\vec u ) $ four-momentum : $ ( e/c , \vec p ) $ four-current density : $ ( \gamma\rho , \vec j ) $ four-potential : $ ( \frac\phi{c} , \vec a ) $ ( and a few more which i can not remember )
the interaction of a particle with its own field depends of the position and retardation depends of the distance between the source particle and the test particle . in principle the field is present at the particle position and the retardation is exactly zero in this one-particle case , therefore the particle always interact with its own field with independence of its velocity
1 ) you are correct in how you transform the fields , but the condition you derived for scale invariance is incorrect . each piece of the action must be invariant under scale transformations in order that the whole action is scale invariant . you should get $\lambda^{-2-2\delta}=\lambda^{-d}$ and $\lambda^{-k\delta}=\lambda^{-d}$ . you can check you get the expected mass dimensions for a free scalar field in d-dimensions . 2 ) to test invariance under conformal transformations you either have to calculate how your fields transform under special conformal transformations or under inversion . invariance under inversion will imply conformal invariance since k=i*p*i where k is the generator of scts , i is the inversion operator , and p is the translation operator . inversion is a conformal transformation not smoothly connected to the identity and not all conformally invariant theories are inversion invariant ( but these are theories that include spinors which you do not have to worry about here ) .
$$ \delta \rho \cong -\frac{\int_v \ ! \mathrm{d}^3r \phi ( \mathbf{r} ) \sigma_a ( \mathbf{r} ) \phi ( \mathbf{r} ) } {\int_v \ ! \mathrm{d}^3r \phi ( \mathbf{r} ) \sigma_f ( \mathbf{r} ) \phi ( \mathbf{r} ) } $$ it seems like because of a result of one group perturbation theory . page 223 of j . duderstadt nuclear reactor analysis . since the temperature feedback is changing the thermal utilisation , which is the ratio between the absorbed and utilised thermal neutrons .
anomalies ( not anamolies ) are a whole subject whose basics are covered by one or several chapters of almost any good enough quantum field theory textbook so it is counterproductive to retype this whole chapter here . but generally , in quantum field theory , anomalies are quantum mechanical effects breaking symmetries that exist in the classical theory – quantum mechanical contributions that are zero in the classical theory because of the symmetry but that are inevitably nonzero in the quantum theory because the quantum completion does not allow all the symmetries to be preserved . an anomaly in a global symmetry is a physical effect that changes the dynamics but keeps it logically consistent ; a gauge anomaly – the quantum mechanical breakdown of a gauge symmetry – renders the theory inconsistent because the gauge symmetry is needed to decoupled the unphysical ( negative-norm ) polarizations of the gauge boson so it should never be broken . anomalies appear because certain divergent integrals cannot be simultaneously set to zero . they appear in theories that admit left-right asymmetry , especially in even spacetime dimensions . the simplest feynman diagrams that quantify the anomaly are $n$-gons , polygons , where $n=d/2+1$ where $d$ is the spacetime dimensions . so importantly enough , anomalies in the $d=10$ dimensional effective theories resulting from superstring theory are calculated using hexagon feynman diagrams . in the undergraduate example of $d=4$ , anomalies are given by $n=3$ i.e. triangle feynman diagrams . a fermion – left-right-asymmetric particle – is running in the loop of the feynman diagram . three gauge bosons are attached . the diagram is proportional to something like $${\rm tr} ( q^3 ) $$ where $q$ is a generator of a $u ( 1 ) $ subgroup of the gauge symmetry , a charge . all such traces of cubic expressions have to vanish for consistency , and they indeed vanish in the standard model – but a cancellation between quarks and leptons is necessary for that . the standard model with leptons only or quarks only would be inconsistent . the mssm has the same chiral spectrum as the standard model with one exception : there are extra higgsinos . they contribute some extra term to the anomalies that are not cancelled by anything . the arguably simplest way to cancel the anomaly is to add not one higgs doublet ( one doublet of superfields ) but two doublets , with mutually opposite values of the charges ( assuming the same chirality ) . so these two mssm higgsinos cancel the anomalies against one another . the two higgs doublets in the mssm are important for another reason : one higgs doublet is only able to give masses to the upper quarks only ; or the lower quarks only . one actually needs both higgs doublets , the up-type and the down-type , to allow all quarks to become massive .
first off , physics tends to provide a very good background for people who move on to study problems in other areas , which is perhaps why there is a lot of cross-over to computer science . however , there are also a number of areas at the interface of computer science and physics which attract people from both sides : computer hardware ( which is generally based on semiconductor physics ) . large scale simulations physics of computation ( quantum computing , reversible computing , etc . ) theoretical computer science etc . of these , perhaps the last one ( tcs ) seems the most surprising . however , in recent years , there has been significant success in applying ideas from thermodynamics and statistical mechanics to problems in computational complexity . an example of this would be the simulated annealing algorithm which works extremely well for optimization problems , as well as work done on phase transitions in 3sat .
the use of the term " heisenberg limit " is somewhat misleading for outsiders ( that is non-quantum interferometers ) . if we recall the heisenberg uncertainty principle is a limit on simultaneous measurement of two complementary variables . in the case of ( quantum ) metrology one is only interested in the measurement of a single variable to high accuracy , and this does not ( directly ) conflict with the hup . in the case of interferometry the variable of interest is $\delta \phi$ the phase difference between two waves detected in two arms . in basic interferometry there were some limits as to how accurately this could be measured : quantum shot noise : $\delta \phi = 1/n^{1/2}$ heisenberg limit : $\delta \phi = 1/n $ here the n corresponds to how many quanta are required for the given accuracy , so the second is more accurate when it can be achieved , as was eventually done using entangled states , and perhaps squeezed light . if you cannot use these features of qm one gets just the quantum shot noise accuracy . well a few years ago it was noticed that the assumption behind the heisenberg limit calculation was that the hamiltonian was quadratic in its ( key ) variables : this corresponded to the assumption of linearity amongst the measuring quanta . if the hamiltonian could be made non-linear then an improvement on the heisenberg limit would be possible . this interaction between the measuring photons is discussed in the given paper , in arxiv form here .
i will stick to pure total internal , specular reflexion in this answer . when tir happens , both linear polarisation components are fully relfected , but the phase change ( the goos-hänchen shift ) is different for the two states . in scalar theory the goos-hänchen shift ( see my answer here ) for the two states , but the full vector theory shows a subtle difference . what this means practically is that the two polarisation states seem to reflect from ever so slightly different depths into the denser medium beyond the totally internally reflecting interface . the fresnel equations still apply in this situation . now , of course , we get $\sin\theta_t&gt ; 1$ so that $\cos\theta_t = \sqrt{1-\sin\theta_t^2}$ is imaginary . we interpret the trigonometric functions exactly as they are interpreted in the derivation of the fresnel equations ( e.g. in reference [ 1 ] ) , to wit , the sine and cosine are the ratios $k_x/k$ and $k_z/k$ of the wavevector components tangential and normal to the interface , respectively . the cosine is imaginary beyond the interface , so the wave is evanescent and exponentially decaying with depth as in my answer cited above . so , from the fresnel equations : $$r_s = \frac{n_1 \cos \theta_i - n_2 \cos \theta_t}{n_1 \cos \theta_i + n_2 \cos \theta_t} = \frac{n_1 c_1 - i\ , n_2 c_2}{n_1 c_1 + i\ , n_2 c_2}$$ $$t_s = \frac{2 n_1 \cos \theta_i}{n_1 \cos \theta_i + n_2 \cos \theta_t}= \frac{2 n_1 c_1}{n_1 c_1 + i\ , n_2 c_2}$$ $$r_p = \frac{n_2 \cos \theta_i - n_1 \cos \theta_\text{t}}{n_1 \cos \theta_t + n_2 \cos \theta_i} = \frac{n_2 c_1 -i\ , n_1 c_2}{n_1 c_2 + i\ , n_2 c_1}$$ $$t_p = \frac{2 n_1\cos \theta_i}{n_1 \cos \theta_t + n_2 \cos \theta_i}= \frac{2 n_1 c_1}{n_1 c_2 + i\ , n_2 c_1}$$ where $c_1 = \cos\theta_1$ and $c_2 = \text{im} ( \cos\theta_2 ) $ are both real numbers . take heed that the reflexion co-efficients are both complex numbers of the form $z^*/z$ and thus have unity magnitude , so all the power is reflected . the goos-hänchen shifts for the two orthogonal linear polarisations are : $$\phi_s = -2\ , \arg ( n_1 c_1 + i\ , n_2 c_2 ) = -2\ , \arctan\left ( \frac{n_2 c_2}{n_1 c_1}\right ) $$ $$\phi_p = -2\ , \arg ( n_1 c_2 + i\ , n_2 c_1 ) = -2\ , \arctan\left ( \frac{n_2 c_1}{n_1 c_2}\right ) $$ which are almost equal in most cases : they deviate more significantly from one another for highly glancing angles $\theta_1\approx\pi/2$ ( which condition invalidates the scalar theory of my other answer ) . so , for your random polarisation , you are going to either represent it as a known , pure polarisation with a $2\times1$ jones vector $x = \left ( \begin{array}{c}x_p\\x_s\end{array}\right ) $ and transform it by : $$x\mapsto u\ , x ; \ ; \text{where}\ ; u = \left ( \begin{array}{cc}e^{i\ , \phi_p} and 0\\0 and e^{i\ , \phi_s}\end{array}\right ) $$ or if the light is depolarised ( a mixed state ) you will use the mueller calculus / density matrix formalism : $$\rho\mapsto u\ , \rho\ , u^\dagger ; \ ; \text{where}\ ; \rho = \sum\limits_{j=0}^3 s_j \sigma_j$$ $\sigma_0 = {\rm id}$ is the $2\times 2$ identity matrix , $\sigma_j$ the pauli spin matrice and the co-efficients $s_j$ are the four stokes parameters as described in my answer on dealing with calculations with depolarised light . reference : [ 1 ] §1.5 " reflexion and refraction of a plane wave " in the seventh edition of born and wolf , " principles of optics " .
yes , spring elongation is $\delta= \frac{f}{k}$ regardless of where $f$ comes from .
dirac 's derivation of the existence of positrons that you described was a totally legitimate and solid argument and dirac rightfully received a nobel prize for this derivation . as you correctly say , the same " sea " argument depending on pauli 's exclusion principle is not really working for bosons . modern qft textbooks want to present fermions and bosons in a unified language which is why they mostly avoid the " dirac sea " argument . but this fact does not make it invalid . the infinite potential charge of the dirac sea is unphysical . in reality , one should admit that he does not know what the charge of the " true vacuum " is . so there is an unknown additive shift in the quantity $q$ and of course that the right additive choice is the choice that implies that the physical vacuum $|0\rangle$ ( with the dirac sea , i.e. with the negative-energy electron states fully occupied ) carries $q=0$ . the right choice of the additive shift is a part of renormalization and the choice $q=0$ is also one that respects the ${\mathbb z}_2$ symmetry between electrons and positrons . it is bizarre to say that dirac missed the lagrangian formalism . dirac was the main founding father of quantum mechanics who emphasized the role of the lagrangian in quantum mechanics . that is also why dirac was the author of the first steps that ultimately led to feynman 's path integrals , the approach to quantum mechanics that makes the importance of the lagrangian in quantum mechanics manifest . it would be more accurate to say that dirac did not understand ( and opposed ) renormalization so he could not possibly formulate the right proof of the existence of the positrons etc . that would also correctly deal with the counterterms and similar things . still , he had everything he needed to define a consistent theory at the level of precision that was available to him ( ignoring renormalization of loop corrections ) : he just subtracted the right ( infinite ) additive constant from $q$ by hand . your sentence the decay of electrons to positrons is then supressed by the u ( 1 ) gauge symmetry of the lagrangian forcing conservation of electrical charge . is strange . since the beginning – in fact , since the 19th century – the u ( 1 ) gauge symmetry was a part of all formulations of electromagnetic theories . it has been a working part of dirac 's theory from the very beginning , too . the additive shift in $q$ , $q=q_0+\dots$ , does not change anything about the u ( 1 ) transformation rules for any fields because they are given by commutators of the fields with $q$ and the commutator of a $c$-number such as $q_0$ with anything vanishes : $q_0$ is completely inconsequential for the u ( 1 ) transformation rules . all these facts were known to dirac , too . the fact that the u ( 1 ) gauge symmetry was respected was the reason that there has never been anything such as a " decay of electrons to positrons " in dirac 's theory , not even in its earliest versions . an electron can not decay to a positron because that would violate charge conservation while the charge has always been conserved . for historical reasons , one could mention that unlike dirac , some other physicists were confused about these elementary facts such as the separation of 1-electron state and 1-positron states in different superselection sectors . in particular , schrödinger proposed a completely wrong theory of " zitterbewegung " ( trembling motion ) which was supposed to be a very fast vibration caused by the interference between the positive-energy and negative-energy solutions . however , there is never such interference in the reality because the actual states corresponding to these solutions carry different values of the electric charge . their belonging to different superselection sectors is the reason why the interference between them can not ever be physically observed . the " zitterbewegung " is completely unphysical .
that is exactly the case . if you look at the trajectory of any given spacecraft , you will see that it has a few burns of the rocket engines punctuating very long periods just coasting along in orbit around some other body . for example , the flight path of apollo 8 has something like eight different rocket burns : launch , translunar and transearth injection ( to get out of orbit and go towards the other body ) , three course correction burns , lunar orbit insertion to catch up with the moon , and one orbit correction burn on the moon . image source : wikipedia the rocket engines spend most of their time turned off , and carry just enough fuel for all of this plus a little extra for safety . this still means that the initial rocket needs to be huge , because the translunar injection requires quite a bit of fuel and that fuel needs a huge other load of fuel to get into orbit .
neglecting air resistance ( whether this is a good idea or not is besides the point ) , suppose you drop the egg from height 10.0m , and it fell to a height of 0.10m , and then rebounded to a height of 9.0m . what can you deduce from this fact ? edit : since you want something a bit more along the modelling side , if you know how the tension changes with temperature , you can deduce the change in entropy with change in length at fixed temperature using maxwell relations . you can assume the rubber band obeys $du = \delta q + t dl$ where $t$ is the tension and $l$ is it is length . you can then use more thermodynamic cleverness to determine the change in temperature with length for adiabatic stretching , which allows you to figure out the irreversible component ( lost as heat ) . google for thermodynamics+rubber+elasticity should get you started . more links : http://physics.oregonstate.edu/~roundyd/courses/ph423/lab-2.pdf ftp://ftp . ccmr . cornell . edu/tmp/mse-4020/4020-notes-7-text-book-rubber-elasticity . pdf
it is true that by noether 's theorem energy is conserved when the action is depednent on position only and not time . however , i think they want you to write down the equation of motion ( use $f=ma$ ) and then use that to show that the total energy is a constant . write down the expression for total energy and take its derivative w.r.t. time .
assuming a relatively small heatsink ( you are not trying to conduct heat to a fin a meter away ) then the thermal conductivity is not a major factor . and as the heatsink has a constant heat inflow and outflow the heat capacity does not really matter . what is important is the ratio of surface area of the fins , the thermal contact to the hot side supporting the weight on the component . that is why aluminium is so popular , it is cheap , easy to extrude/mould/machine , does not need any surface coating/painting and is light .
assuming the projectile had enough velocity to escape the gravitational potential of the " projector " ( system that fired it ) . and assuming that space-time is otherwise basically flat . then , yes .
the wedge is tangent to the sphere . using that it touches at height r/5 you can easily work out the slope of the wedge . the velocity of the sphere follows directly ( 20m/s times slope ) .
perhaps my ensuing answer will be a little too simple to be satisfying to you , but i only have a firm grasp of things when i keep it simple . let 's just consider statistical mechanics and look at the canonical ensemble ( where there is a constant number of particles [ n ] , volume [ v ] , and temperature [ t ] ) . the components that make up this system want to reach equilibrium because that is how thermodynamic systems behave . the equilibrium point in the canonical ensemble is defined as the point at which the helmholtz free energy , a , is minimized , and it is defined as : a = u - ts where u is potential energy ( a negative value ) and s is entropy . entropy is also defined in stat mech as s = -k ln ( w ) where k is the boltzmann constant and w is the number of microstates in a system . what does this mean ? a system in equilibrium does not always increase its entropy , rather it minimizes its free energy and one way to minimize free energy is to maximize entropy . let 's consider the three different states of h2o : at low temperatures , hydrogen bonds ( i.e. . potential energy ) dominate free energy and the system exists as a low entropy solid . at intermediate temperatures , neither potential energy or entropy dominate and its a liquid . at high temperatures , the entropy term dominates and the water molecules exist far away from each other so as to have an extremely high number of microstates , and very little potential energy .
there are 3 observations that support the big bang theory , i.e. origin of the universe in a singularity : the redshift of galaxies , as you already mentioned . the cosmic background radiation . the amounts of different nuclei in the universe , notably the preponderance of light elements like hydrogen and helium . each of these alone would probably not be sufficient to support the big bang theory . the redshift of galaxies could be explained by some other theory , some have been suggested by hoyle and narlikar in the past . probably the other two phenomena could be explained independently as well , but it is the conjunction that fits so well with the big bang hypothesis . does that settle the matter once and for all ? short answer is no . since these 3 observations have been made and confirmed , more detailed observations have been added to the mix and this has complicated the story for the big bang model . but that would take us into a longer post . the current model which is the most widely accepted is the so-called lambda-cdm model . as for the problem of the universe starting in a real singularity , instead of a very dense state , this is still an open problem related to a yet to be invented ( or completed ) theory of quantum gravity . our current understanding of singularities in general relativity is going back to the penrose-hawking singularity theorems . they are of the kind " here be dragons ! " in that they delineate the conditions for singularities to form and point where our knowledge ends . more can not be done , because a singularity is basically a failure of the theory .
one should not expect to have a " good " formula for the local isometric embeddings of a constant negative curvature surface in euclidean $\mathbb{r}^3$ . this is due to a little theorem proved by david hilbert around 1901: theorem there does not exist a smooth immersion of the hyperbolic plane into euclidean 3 space . the theorem has been further studied in the years following . in 1961 efimov showed that any complete surface with curvature strictly bounded above ( that is to say , if there exists a negative number $k_0 &lt ; 0$ such that the gaussian curvature is always strictly less than $k_0$ ) cannot admit a smooth ( twice continuously differentiable ) isometric immersion into euclidean three space . that is to say , if you try to " extend " any surface in euclidean 3 space that satisfies constant negative curvature , you are guaranteed to hit a singularity . in particular , you cannnot expect the surface to be described by $f ( x , y , z ) = 0$ where $f ( x , y , z ) $ has a nice algebraic expression ( say , polynomial ) and has smooth level sets . typically the image one usually use to illustrate the notion of negative ( but not constant ) curvature is the graph $$ z = x^2 - y^2 $$ which produces a classical saddle , or the catenoid whose gaussian curvature , while everywhere negative , is not constant . ( though it has constant [ in fact everywhere vanishing ] mean curvature . ) lastly , however , despite the above , it is possible to embed " patches " of hyperbolic plane into euclidean 3 space . there are many ways of doing so ( one can search for the term pseudosphere ; though some people use the same term for the hyperboloid/de sitter spaces embedded in higher dimensional minkowski space ) , but one of the more well-known is the tractricoid . ( see wiki entry here . ) parametrically in cylindrical coordinates $ ( z , r , \theta ) $ the surface can be described by : $$ \mathbb{r}_+\times\mathbb{s}^1 \ni ( t , \omega ) \mapsto \left ( z=\frac{1}{\cosh t} , r=t-\tanh t , \theta = \omega\right ) $$ and has constant negative curvature .
first of all , i think you are correct that mercury 's precession is only useful as a check on classical , large-scale theories of gravity , like mond , which would actually govern astronomical interactions . as pointed out in the comments , quantum gravity mainly concerns itself with an entirely different domain , namely planck-scale distances , times , and densities , since those are the scenarios in which classical gr seems to break down . accordingly , the typical checks on theories of quantum gravity are things like black hole entropy , and predicting some sort of non-singular behavior for the universe at the big bang . using lqg to calculate a property of a complex real-world system like the solar system is at worst impossible and at best absurdly complicated , if it in fact reduces to classical gr ( which i have also heard is yet to be proven ) . on the other hand , a theory of quantum gravity could perhaps handle the " interesting " part of the calculation of planetary precession without incorporating mass directly . all it really needs is a connection , which in lqg is provided by the su ( 2 ) gauge field $a^i_a$ defined on the edges of a graph . given a graph and a connection , one might be able to compute something akin to a geodesic through the graph , which in the continuum limit would correspond to the orbit - though my knowledge of lqg falls short here , i am not sure whether that is how you would actually get from quantum geometry to an orbit . and anyway , as far as i know , in order to get the connection for a given mass distribution ( i.e. . the sun ) in the first place , you would have to use classical gr to find the christoffel symbols .
i am not sure , but i think your objection here is that the times measured by these observers for the interval between the " a1 meets b1" and " a2 meets b2" events is the same , even though they are in frames that are moving relative to one another . so should not there be some kind of time dilation ? this is not a problem , though . the familiar relativistic time dilation formula has to do with the time t ' you will measure between two events , relative to the time t between the events in the frame where they occur at the same location--that is , their rest frame , the frame in which a single stationary clock could mark time for the two events . t'/t = gamma , the time dilation factor . what is the rest frame defined by these two events ? it is not the frame moving with either train . it is a third frame : the frame in which the two trains are moving in opposite directions with the same speed ! ( call it the " center of velocity " frame . ) in this frame , a1 passes b1 and a2 passes b2 at exactly the same place . relative to this center-of-velocity frame , both train frames are moving at the same speed , just in opposite directions . so the elapsed time between the two events gets time-dilated to exactly the same extent to observers on the two trains . the measured time will not be the same in every frame ; but it will be the same in those two particular frames !
starting without the beam splitter , just to establish notation , label the slits 1 and 2 , the laser l and a point on the screen x . then $\langle 1| l \rangle$ is the ( complex ) amplitude to go from the laser to slit 1 . $\langle x| 1 \rangle$ is the amplitude to go from slit 1 to a point on the x on the screen . the amplitude to do that path combination is $\langle x| 1 \rangle\langle 1| l \rangle$ . given that it must land somewhere on the screen we know $$ \sum_x ( |\langle x| 1 \rangle\langle 1| l \rangle + \langle x| 2 \rangle\langle 2| l \rangle |^2 ) =1$$ ( or integral . . . ) if we now place a beamsplitter b right after slit 2 , then instead we have$$|\langle c| b \rangle \langle b| 2 \rangle \langle 2| l \rangle|^2+\sum_x ( |\langle x| 1 \rangle\langle 1| l \rangle + \langle x| b \rangle \langle b| 2 \rangle\langle 2| l \rangle |^2 ) =1$$ now we know a few things about the constituents : $$|\langle 1|l \rangle |^2=0.5 $$ $$|\langle 2|l \rangle |^2=0.5 $$ $$|\langle b|2 \rangle |^2=1 $$ $$|\langle c|b \rangle |^2=0.5 $$ so the first term is $\frac{1}{4}$ ( propagation into the camera ) and the sum of the other terms must be $\frac{3}{4}$ ( propagation to somewhere on the screen ) .
in special relativity , hertz per dioptre is an excellent unit for showing the joint invariance of electromagnetic phenomena in the behavior of all types of lenses , reflective or refractive , under the effects of the lorentz transformation along the axis of motion . i am not aware of any other unit that links those two domains in quite that way . in the case of refractive lenses with chromatic dispersion , the invariance turns out to be non-trivial and a bit surprising , since it asserts that the atomic materials in a lorentz compressed lens must maintain a very specific relationship in how they interact with a spectrum of gamma-shifted light frequencies . here 's how it works for the easier reflective-lens case . first , imagine a sphere 4 meters across with an $f=280$ thz resonant infrared light wave inside . why 4 meters ? well , i am trying to use the correct definition of dioptre . that is the focal length of a refractive or reflective lens , which means the distance it requires to converge parallel light down to a single focal point . in this case , the lens is reflective and has spherical curvature . looking only at a region small enough ( e . g . 2 cm across ) to avoid spherical aberration , the focal length of the $d=4$ m sphere is $l=\frac{1}{2}r=\frac{1}{4}d=\frac{1}{4}4=1$ m . so , a 4 m diameter sphere thus correctly gives a dioptre ( curvature ) of $\delta=1/l=1/1=1$ d , where d $=m^{-1}$ . next , accelerate the sphere along it x axis to a velocity of $v=\sqrt{\frac{3}{4}}$ c , which gives a lorentz factor of $\gamma=2$ . that means that both the sphere and the resonant light pattern within it will be compressed to $\frac{1}{2}$ their original lengths along the x axis , from the perspective of a viewer " at rest " relative to the moving sphere . for the small reflective lens regions around either end of where the x axis crosses the sphere , the pre-acceleration curvature was $\delta_0=1d$ ( the zero subscript indicates the rest frame ) . after acceleration to $\gamma=2$ the sphere becomes an oblate spheroid , and the curvatures of the two reflective lens areas have been reduced to $\delta_1=2d$ , where higher dioptre numbers indicate flatter curves . ( the proof of that is left as an exercise for the reader , but it is not difficult . ) now let 's examine what happens to the frequency of the light within the sphere . the neat thing about special relativity is that physics must remain invariant for both the observer and the observed system . so , if there were n wavelengths of resonant light crossing the sphere along the x axis prior to it being accelerated , there must also be n wavelengths along that same length after the compression . in other words , the wavelengths of the radiation must also be cut in half along x ( only ) , resulting in twice the frequency as before . that transforms the original x-axis $f_0=280$ thz light of the at-rest sphere into $f_1=560$ thz light in the moving sphere . an observer in the rest frame would see this as bright green . observant readers may now be saying " hey , that can not be right ! the lorentz factor also slows time . . . so should not the light in the moving sphere be slower and thus less energetic ? " while it is true that time will pass more slowly within the moving sphere , it is not correct to think that this same light will be slower when viewed from the rest frame . for that situation the geometry of the wavelengths wins , and the light looks green . however , a simpler way to think of it is that since the light is being emitted and reflected by an object traveling at $\gamma=2$ ( or equivalently $v=\sqrt{\frac{3}{4}}$ c ) , the ordinary doppler effect will double its frequency . ( @colink has correctly noted that the above explanation glosses over some important complications . please see his excellent comment for more info . i may try to address that soon . ) now it is time to put this all together . the original light and sphere had an eta factor of : $\eta_0=f_0/\delta_0 = ( 280 thz ) / ( 1 d ) = 280\times{10}^{12}$ hpd where 1 hpd = 1 hz/d ( hertz per dioptre ) . the moving light and sphere has an eta factor of : $\eta_1=f_1/\delta_1 = ( 560 thz ) / ( 2 d ) = 280\times{10}^{12}$ hpd . in other words , the eta factor $\eta$ , which relates the lorentz-transformed electromagnetic waves to the lorentz-contracted physical mirrors from which they reflect , has remained invariant for this example of $\gamma=2$ . it is not an isolated case . it is easy to show that $\eta$ is a universal invariant of special relativity : $\forall{v_i} ( \eta_i=\frac{f_i}{\delta_i}= c ) $ where c is a constant in units of hpd = hz/d = hertz per dioptre . now the remarkable generalization of all of this is that by the same kinds of geometric arguments and application of the " physics must be preserved in both frames " principle , refractive lenses must also fall under the above argument . if a refractive lens has chromatic dispersion ( the colored fringes seen in cheap lenses ) , then the constant c in the above equation will become a frequency-dependent value $c ( f ) $ . yet the eta invariance remains intact ! that is surprising because light dispersion is a pretty complicated phenomenon , yet from the rest frame these messy compressed atoms must nonetheless maintain eta invariance . that is . . . unexpected . thus hpd units not only have real physical meaning , but a meaning that relates directly to the original intent of both the hertz and dioptre units ( versus just being $m/s$ in disguise ) . this meaning in turn provides an easy way to express an invariant relationship in special relativity that links together the electromagnetic and mechanical lorentz transformations in an unexpected and non-intuitive fashion . and finally , despite all the above unexpectedly interesting ( to me at least ! ) sr relationships involved , the hpd unit really did originate as a bit of humor in ( as best i could uncover ) this xkcd discussion posting back in 2007 . so , shrodingersduck from the people 's democratic republic of leodensia , wherever you are six years later , i thank you for inadvertently creating an interesting and quite fun opportunity to explore special relativity in a rather unusual context . addendum 2013-01-31 the generality of the hpd unit in special relativity can i think be stated even more broadly . so , here goes : light frequency , geometric forms , and frequency-dependent refractive indices all change when systems undergo lorentz transformation , so they are not individually lorentz invariant . theorem : if the optical characteristics of an optical system are instead described using hpd ( hertz per dioptre ) and/or its inverse unit dph ( dioptres per hertz ) , the resulting description of its optical properties will remain constant ( "eta invariance" ) regardless of relativistic frame or orientation from which the optical system is analyzed . that is a theorem only . @colink 's excellent observation that the doppler argument i made could be bogus because the shift works differently depending on whether the light is moving with or against the velocity still concerns me . so , i want to look at that a lot more closely and see if i can disprove my own theorem . still , would not it be delightful if a unit defined as a joke turned out to be relativistically invariant when the common units for the same phenomena are not ? the other obvious generalization question is this : does eta invariance ( if it exists ) apply to other wave phenomena ? and finally , @joezeng , i think i misunderstood your question about whether the eta factors ( descriptions of optical components using hpd units ) are related to the velocity of light . well , hpd does have dimensional equivalence to a velocity ( $m/s$ ) , but if there is a meaningful way to re-interpret an hpd value as a velocity , i sure do not see it . intriguing question , though . . .
let 's split up the forces , it gives an easier view on what should happen . horizontal forces what we see is a centrifugal force . centrifugal force ( from latin centrum , meaning " center " , and fugere , meaning " to flee" ) represents the effects of inertia that arise in connection with rotation which are experienced as an outward force away from the center of rotation so , outward force away from the center of rotation . this means that the water would only be able to appear in place if there is a force in the opposite direction , which is known as friction . where is the friction if you take the glass away ? there is no friction anymore , thus that video shows special effects . ; - ) vertical forces there is also the gravitational force . what would keep the water up so long ? nothing . furthermore , if you look at his other videos , you can confirm it is cgi . . . :- )
we can calculate charge on baloon by assuming it a perfect sphere with uniform charge distribution . $$v=\frac{q}{4\pi\epsilon_0\times r}$$ where $r$ is radius of ballon .
there is another question on this site about whether the laws of physics change over time . i think that the answers to that one ( including mine ) apply pretty much perfectly to this question about whether the laws change in space . we expect the fundamental laws of physics to be the same throughout space . in fact , if we found that they were not , we would strongly expect that that meant that the laws we had discovered were not the fundamental ones . it is very sensible to ask whether the laws as we currently understand them vary with respect to position . people do try to test these things experimentally from time to time . for instance , some experiments to test whether fundamental constants change with time are also sensitive variations in the fundamental constants with position . some cosmological theories , especially some of those that come under the heading of " multiverse " theories do allow for the possibility that the laws are different in different regions of space , although generally only on scales much larger than what we can observe . in general , in such theories , the truly fundamental laws are the same everywhere , but the way the evolution of the universe played out in different regions is so different that the laws appear quite different . one way this can happen is by the mechanism of spontaneous symmetry breaking . when the universe cooled down from very high temperatures , it probably underwent various transitions , more or less like phase transitions , in which an initially symmetric state turns into a less-symmetric state . in those transitions , there may be different ways that the final state can come out , and they may be quite dramatically different -- completely different sorts of particles may exist , for instance . there could be different regions of the universe in which the symmetry breaking went different ways , in which case the " apparent " laws would be utterly different in different regions , but probably only on scales many orders of magnitude larger than what we can see .
in his original work , fermi considered only vectors $f^{\mu}$ which are orthogonal to the curve $f^{\mu} v_{\mu} = 0$ . his analysis is relevant to the spin or photon polarization vectors which are orthogonal to the four-velocity by definition . walker generalized fermi 's work to vectors which are not necessarily orthogonal to the velocity . ( thus the second term in the fermi-walker transport is not identically $0$ ) . this fact and more historical remarks on fermi 's work on accelerated bodies in general relativity , the fermi-walker transport and their connections to modern works on spinning bodies in general relativity as well as references to their original work can be found in the following article by : bini and jantzen .
i put an extra answer , since i believe the first jeremy 's question is still unanswered . the previous answer is clear , pedagogical and correct . the discussion is really interesting , too . thanks to nanophys and heidar for this . to answer directly jeremy 's question : you can always construct a representation of your favorite fermions modes in term of majorana 's modes . i am using the convention " modes " since i am a condensed matter physicist . i never work with particles , only with quasi-particles . perhaps better to talk about mode . so the unitary transformation from fermion modes created by $c^{\dagger}$ and destroyed by the operator $c$ to majorana modes is $$ c=\dfrac{\gamma_{1}+\mathbf{i}\gamma_{2}}{\sqrt{2}}\ ; \text{and}\ ; c{}^{\dagger}=\dfrac{\gamma_{1}-\mathbf{i}\gamma_{2}}{\sqrt{2}} $$ or equivalently $$ \gamma_{1}=\dfrac{c+c{}^{\dagger}}{\sqrt{2}}\ ; \text{and}\ ; \gamma_{2}=\dfrac{c-c{}^{\dagger}}{\mathbf{i}\sqrt{2}} $$ and this transformation is always allowed , being unitary . having doing this , you just changed the basis of your hamiltonian . the quasi-particles associated with the $\gamma_{i}$ 's modes verify $\gamma{}_{i}^{\dagger}=\gamma_{i}$ , a fermionic anticommutation relation $\left\{ \gamma_{i} , \gamma_{j}\right\} =\delta_{ij}$ , but they are not particle at all . a simple way to see this is to try to construct a number operator with them ( if we can not count the particles , are they particles ? i guess no . ) . we would guess $\gamma{}^{\dagger}\gamma$ is a good one . this is not true , since $\gamma{}^{\dagger}\gamma=\gamma^{2}=1$ is always $1$ . . . the only correct number operator is $c{}^{\dagger}c=\left ( 1-\mathbf{i}\gamma_{1}\gamma_{2}\right ) $ . to verify that the majorana modes are anyons , you should braid them ( know their exchange statistic ) -- i do not want to say much about that , heidar made all the interesting remarks about this point . i will come back later to the fact that there are always $2$ majorana modes associated to $1$ fermionic ( $c{}^{\dagger}c$ ) one . most has been already said by nanophys , except an important point i will discuss later , when discussing the delocalization of the majorana mode . i would like to finnish this paragraph saying that the majorana construction is no more than the usual construction for boson : $x=\left ( a+a{}^{\dagger}\right ) /\sqrt{2}$ and $p=\left ( a-a{}^{\dagger}\right ) /\mathbf{i}\sqrt{2}$: only $x^{2}+p^{2} \propto a^{\dagger} a$ ( with proper dimension constants ) is an excitation number . majorana modes share a lot of properties with the $p$ and $x$ representation of quantum mechanics ( simplectic structure among other ) . the next question is the following : are there some situations when the $\gamma_{1}$ and $\gamma_{2}$ are the natural excitations of the system ? well , the answer is complicated , both yes and no . yes , because majorana operators describe the correct excitations of some topological condensed matter realisation , like the $p$-wave superconductivity ( among a lot of others , but let me concentrate on this specific one , that i know better ) . no , because these modes are not excitation at all ! they are zero energy modes , which is not the definition of an excitation . indeed , they describe the different possible vacuum realisations of an emergent vacuum ( emergent in the sense that superconductivity is not a natural situation , it is a condensate of interacting electrons ( say ) ) . as pointed out in the discussion associated to the previous answer , the normal terminology for these pseudo-excitations are zero-energy-mode . that is what their are : energy mode at zero-energy , in the middle of the ( superconducting ) gap . note also that in condensed matter , the gap provides the entire protection of the majorana-mode , there is no other protection in a sense . some people believe there is a kind of delocalization of the majorana , which is true ( i will come to that in a moment ) . but the delocalization comes along with the gap in fact : there is not allowed propagation below the gap energy . so the majorana mode are necessarilly localized because they lie at zero energy , in the middle of the gap . more words about the delocalization now -- as i promised . because one needs two majorana modes $\gamma_{1}$ and $\gamma_{2}$ to each regular fermionic $c{}^{\dagger}c$ one , any two associated majorana modes combine to create a regular fermion . so the most important challenge is to find delocalized majorana modes ! that is the famous kitaev proposal arxiv:cond-mat/0010440 -- he said unpaired majorana instead of delocalised , since delocalization comes for free once again . at the end of a topological wire ( for me , a $p$-wave superconducting wire ) there will be two zero-energy modes , exponentially decaying in space since they lie at the middle of the gap . these zero-energy modes can be written as $\gamma_{1}$ and $\gamma_{2}$ and they verify $\gamma{}_{i}^{\dagger}=\gamma_{i}$ each ! to conclude , an actual vivid question , still open : there are a lot of pseudo-excitations at zero-energy ( in the middle of the gap ) . the only difference between majorana modes and the other pseudo-excitations is the definition of the majorana $\gamma^{\dagger}=\gamma$ , the other ones are regular fermions . how to detect for sure the majorana pseudo-excitation ( zero-energy mode ) in the jungle of the other ones ?
this types of problems are solved by observing projectile movements in $x$ and $y$ direction separately . in $x$ direction you have constant velocity movement $$v_x = v_{x0} = v_0 \cos ( \theta ) , \ ; ( 1 ) $$ $$x = v_{x0} t +x_0 = v_0 \cos ( \theta ) \ ; t +x_0 , \ ; ( 2 ) $$ and in $y$ direction you have constant acceleration movement with negative acceleration $-g$ $$v_y = - g t + v_{y0} = - g t + v_0 \sin ( \theta ) , \ ; ( 3 ) $$ $$y = - \frac{1}{2} g t^2 + v_{y0} t + y_0 = - \frac{1}{2} g t^2 + v_0 \sin ( \theta ) \ ; t + y_0 . \ ; ( 4 ) $$ your initial conditions are $$x_0 = 0 , \ ; y_0 \ne 0 , $$ and final conditions ( at moment $t=t$ projectile falls back on the ground ) are $$t = t , \ ; x = d , \ ; y = 0 . $$ if you put initial and final conditions into equations ( 2 ) and ( 4 ) you end up with two equations and two unknowns $v_0 , t$ . by eliminating $t$ you get expression for $v_0$ . my calculations show that $$v_0 = \frac{1}{\cos ( \theta ) }\sqrt{\frac{\frac{1}{2} g d^2}{d \tan ( \theta ) +y_0}}$$ which is i believe equal to your equation . maybe your problem is that $d$ means displacement in direction $x$ , while the total displacement is $\sqrt{d^2+y_0^2}$ ?
the weld acts on the rod with force components $b_x$ and $b_y$ as well as a moment $m$ . the equations of motion for the rod are $$ b_x = m_{rod} \left ( - \cos\theta \ , \ddot{q} \right ) $$ $$ b_y = m_{rod} \left ( \sin\theta \ , \ddot{q} + g \right ) $$ $$ m - \frac{l}{2} b_y = i_{rod} \dot{\omega}_{rod} = 0 $$ where $q$ is the distance along the guide of travel and the last equation is the sum of moments about the center of gravity of the rod . gravity is included above as $g$ . the equations of motion for the block are $$ -f\ , \cos\theta + n \sin\theta - b_x = m_{block} \left ( - \cos\theta\ , \ddot{q} \right ) $$ $$ f\ , \sin\theta + n \cos\theta - b_y = m_{block} \left ( \cos\theta\ , \ddot{q} + g\right ) $$ where $n$ is the contact normal force . combined the above is $$ m_{rod} \left ( \ddot{q} + g\sin\theta \right ) - f = -m_{block} \left ( \ddot{q} + g\sin\theta \right ) $$ $$ n-m_{rod} g \cos\theta = m_{block} g \cos\theta $$ with solution $$ n = \left ( m_{rod}+m_{block}\right ) \ , g \cos\theta $$ $$ \ddot{q} = \frac{f}{m_{rod}+m_{block}} - g \sin \theta $$ now going back to moment , $m=\frac{l}{2} b_y$ which you can solve now .
$y = y_0 + ut + 0.5at^2$ since $y_0$ and $u$ are $0$ , we have $y = 0.5~at^2$ . in your calculation $t = 2.95 \times 10^{-9}$ , which is correct . so , $y = 0.5\times5.30×10^{17}\times ( 2.95 \times 10^{-9} ) ^2$ therefore , $y = 2.3~\text{cm}$
your query is valid : how is voltage a ' wave ' that reflects and creates standing waves ? well , the answer is quite simple when you stop and think about it . all signals travelling across transmission lines are merely electromagnetic waves . now these lines are commonly driven by voltage sources , hence they are ' voltage waves ' . this makes perfect sense : if a simple circuit is driven by a sinusoidal voltage source , you expect that the resultant voltage ( amplitude ) will vary like the sine wave . if you extend your thinking a bit , you will see that the same question can apply to any circuit with a sinusoidal voltage source . we do not usually apply the concept of ' waves ' to a simple circuit ( the concept is only useful for the analysis of tls since reflections can be a bit of nuisance and at radio frequencies are important ) , but its present . ever heard of lc circuits ? these circuits are basically resonators where the energy from the source ( a voltage source ) keeps on changing forms . but how can that happen , if the source is a ' voltage ' ? this analogy would help you understand that all signals are just electromagnetic waves . the reason we call them ' voltage waves ' in rf engineering is simply because at those frequencies the wavelike behavior of the source helps us in the analysis/design of the circuit ( through the use of matching circuits , tuning circuits etc ) . hope this answers your query . feel free to inquire more . ok it seems you have a few more queries . let 's get to them . 1 ) yes , an em wave does not require any medium to travel ( that is how antennas in space work ) . however , ' containing ' an em wave inside a conductor is not a big deal . its a simple matter of creating a potential difference and letting it do all the work . you could , ofcourse , ask the same question when you connect a simple wire to a voltage source . the explanation remains the same . 2 ) now why does the wave reflect at an open circuit ? the answer to this question you will encounter in your course , but i will give a simple version of it here . at the open circuit point the current in the line is zero ( by the definition of an open circuit ) . since charge continues to arrive at the end of the line through the incident current , but no current is leaving the line , then conservation of electric charge requires that there must be an equal and opposite current into the end of the line . essentially , this is kirchhoff 's current law in operation . this equal and opposite current is the reflected current and by ohm 's law , it creates a reflected voltage wave . 3 ) what brandon enright posted about is the tried and tested , age-old analogy between electricity/electronics and hydraulics , which every engineer has used at some point . it is perfectly correct . please refer this link ( http://en.wikipedia.org/wiki/hydraulic_analogy ) to further understand it and relate it to your query .
i did some research and calculations : to summarize : the relativistic rocket will not break apart , uniform acceleration along it is possible . but the observers will measure different accelerations due to the gravitational time dilation . in more detail : let 's assume the observer at the bottom measures $\alpha$ acceleration . so for an inertial observer outside ( who draws the minkowski chart ) this accelerating observer 's motion will be hyperbolic . the semi mayor axis of this hyperbola will be $c^2/\alpha$ . let 's say the length of the relativistic rocket is $h$ this is measured before the launch , and the mechanical stresses during the travel try to keep this value constant in the reference frame of the rocket , otherwise the rocket would break apart ( we assume it do not break apart ) . as the rocket accelerates the plane of simultaneity rotates from the viewpoint of the inertial observer . so the two ends of the rocket will not trace two identical hyperbolas . but the two ends always connect two points on the two hyperbolas whose slope is the same ( you can see this on the rindler chart ) . so all parts of the rocket travel with the same speed at the local frame simultaneously , so the rocket 's acceleration will be uniform and it will not break apart . but the two hyperbolas are different . the bottom traces a hyperbola whose semi-mayor axis is $c^2/\alpha$ , the top traces a hyperbola whose semi-mayor axis is $c^2/\alpha + h$ . the acceleration that corresponds to the second hyperbola is $1 + \alpha h / c^2$ times smaller than $\alpha$ . this a bit paradoxical situation , because i stated that the acceleration is uniform along the rocket , now i state it is different due to the different hyperbolas . this paradox can be resolved if i introduce gravitational time dilation , so i assume that the clock of the observer at the top ages faster with the rate i mentioned above . so the top observer measures less acceleration this way . there is an event horizon at the rindler-horizon where $h = -c^2/\alpha$ , there the time stands still . this is somewhat analogous with the black hole 's event ho+rizon . the gravitational time dilation formula mentioned in the wikipedia and the one mentioned in the comment is the same , but that exponentional formula never reaches zero , that would mean the rindler-horizon does not exist . . . which would be a bit odd . so i still need some research . update : the wikipedia article has been fixed since last update . so the general formula to the gravitational time dilation is $e^{\int^h_0 g ( h ) dh / c^2}$ , where $g ( h ) $ is the measured gravitational acceleration at the given level . for rindler observers $g ( h ) = c^2/ ( h+h ) $ where $h = c^2/\alpha$ . doing the integral gives $e^{ln ( h+h ) - ln ( h ) } = ( h+h ) /h$ . substituting $h$ back i will get the original $1 + \alpha h / c^2$ i mentioned earlier .
there is no point in space where the big bang took place . it happened everywhere , simultaneously . centered on earth , since everything is moving away from us with a uniform velocity ( or is stationary with respect to the cmb , if you prefer ) , the net momentum of the universe is approximately zero .
the state $|\phi\rangle$ is a coherent state , which has a non-zero overlap with the vacuum state $|0\rangle$ . the vacuum state is defined by $\hat \phi ( x ) |0\rangle=0$ for all $x$ . the vacuum state is also given by the coherent state $|\phi ( x ) =0\rangle$ for all $x$ . furthermore , one should keep in mind that the coherent state basis is overcomplete , and therefore $\langle \phi|\phi'\rangle\neq0$ for any $\phi$ and $\phi'$ . all this properties solve the op 's problem . to see that , one can look at the simpler case of the harmonic oscillator , and everything should be clear .
the question says : no standing waves are observed for any other mass between these values so if you increase the mass smoothly from 16 to 25kg no standing waves are seen . if there was more than two nodes difference between 16 and 25kg you would have seen one or more standing waves as the mass was increased .
the infinite length is not really infinite but it is infinite relative to the very small radius of loops . if we consider radius to be relatively comparable , then the field will depend upon the radius and the point where the field is to be calculated . ( can be done by adding field due to all loops by integration ) then field inside at a point comes out to be $$\dfrac12 \mu_0ni\bigg [ \dfrac{b}{\sqrt{b^2+r^2}}+\dfrac{a}{\sqrt{a^2+r^2}}\bigg ] $$ where $a$ and $b$ are distances from the point to the ends of solenoid . we can see if $r&lt ; &lt ; a , b$ then field comes out to be$$\mu_0ni$$ and this condition is called infinite length .
if one is a mathematician one can study any set of theories to his/her heart 's content and end up with a qed at the end . physics is about studying understanding and modeling nature ( physis in greek ) preferably with mathematical models which are predictive of new behaviors . the standard model ( sm ) is one such mathematical model . what does this mean ? it means that it is a shorthand for a huge number of physical data painstakingly gathered from individual experiments . it fits the results and all the predictions it has given for the lhc are within the experimental errors . why are physicists interested in string theories and not satisfied with the sm ? because they believe that all interactions in nature should be modeled by a single model and the sm describes and predicts data only for three out of the four . gravity is not within sm . for many years now theoretical physicists have been studying various mathematical models that will include gravity in one unified theory with all four interactions . string theories are the best candidate for modeling all interactions because their group structures can accommodate the groups of the sm , and thus all known data can be embedded in a string theory model . if you do not know in depth what your are trying to model , you may be a good technician in mathematical modeling , but not a good physicist with a physicist 's intuition . it all depends on your further goals , if you want to be a research physicist or are just studying some physics for other requirements . if the former , yes the sm course is necessary . the work for theoretical physicists in string theory models lies in finding which specific branch and finally form of a string theory embeds best the sm and predicts new phenomena to be found and studied . if one has no data bank of the known phenomena and the way they are connected mathematically one will have developed no intuition on finding the needle in the haystack of string theory possibilities , or be able to suggest experiments that will lead to a validation for a specific st model . .
paragraphs " creating a general single particle state ( discrete solution form ) " ( $3.108 \to 3.110 $ ) and " destroying a general single particle state ( discrete ) " ( $3.111 \to 3.112 $ ) are two independent paragraphs , are should not be mixed . it is not possible to start with a normalized state $c\equiv\sum_\mathbf{k}a_\mathbf{k}a_\mathbf{k}^\dagger$ , with $\sum_\mathbf{k}\left|a_\mathbf{k}\right|^2=1$ , then applying the operator $d\equiv\sum_\mathbf{k}a_k$ , and find that the resulting state $\sum\limits_i a_i|0\rangle$ is also normalized ( except in the trivial case where there is only one term in the sum ) . the main reason is that the operator $d$ is not unitary , so there is no reason why it should transform a normalized state into an other normalized state . or said , differently : $\sum_\mathbf{k}\left|a_\mathbf{k}\right|^2 \neq |\sum\limits_{k} a_\mathbf{k} |^2$
if the acceleration is zero at the contact point and $a$ at the edge , increasing linearly the acceleration of the centre is $a/2$ . starting from the linear velocities of various points on the solid , say one at the top edge ( let 's call it a ) , the centre ( o ) and one at the contact point ( b ) , all at a certain time : if the wheel was pinned at the centre $v ( a ) $ would be $r\omega$ where $\omega$ is the angular velocity of the cylinder ; $v ( b ) $ would be the same and $v ( o ) $ would be 0 . however the contact point is constrained by the ground - as a result , all speeds are offset by $r\omega$ meaning that now $v ( a ) =2\times v ( o ) $ .
you want to find the eigenfrequences of this system . first , note the existence of zero mode : $$ q_l=vt+\phi_l , $$ it is the ' equilibrium ' rotating around with arbitrary velocity . next , we have the equations $$ \ddot{q}_l=-\omega^2 ( 2q_l-q_{l-1}-q_{l+1} ) $$ and the periodicity condition $q_{l+n}=q_l$ . let us use the ansatz for the eigenvectors $$ q_l=\re a\exp ( ikl-i\omega t ) . $$ substituition reads $$ \omega^2=\omega^2 ( 2-e^{-ik}-e^{ik} ) =2\omega^2 ( 1-\cos k ) , $$ while the periodicity condition is $nk=2\pi m , \ , m\in\mathbb{z}$ . let us restrict $k$ to $ ( 0,2\pi ) $ , in order to exclude double-counting the eigenvectors and the zero mode . then $m=1 , \dots , n-1$ , and the eigenfrequency reads : $$ \omega^2=2\omega^2 ( 1-\cos \frac{2\pi m}{n} ) , \ , m\in\{1\dots n-1\} . $$ thus we have $n-1$ vectors of the form $q_l=\re a\exp ( ikl-i\omega t ) $ with the above eigenfrequency , and one zero mode given by $q_l=vt+l\delta$ , $\delta$ being the equilibrium distance , with zero frequency . so , the full spectrum reads as $$ \omega^2=2\omega^2 ( 1-\cos \frac{2\pi m}{n} ) , \ , m\in\{0\dots n-1\} . $$ ( note that this is the full spectrum since we have found all $n$ eigenvectors ) . edit : note that while $n-m$ corresponds to the same eigenvalue as $m$ , we have two different eigenvectors for each eigenvalue , because $a$ can be complex . e.g. we can take $q_l=\cos ( kl-\omega t ) $ and $q_l=\sin ( kl-\omega t ) $ , $k=\frac{2\pi m}{n}$ .
there is a quite instructive paper g . a . alekseev and v . a . belinski , equilibrium configurations of two charged masses in general relativity , phys . rev . d76 ( 2007 ) 021501 ; arxiv:0706.1981 [ gr-qc ] , e.g. they mentioned a work about non-existence of static equilibrium configurations of two charged black holes by p . chrusciel and p . tod , commun . math . phys . , 271 577 ( 2007 ) ; arxiv:gr-qc/0512043 and found condition for equilibrium of two charged masses : $m_1 m_2 = ( e_1-\gamma ) ( e_2+\gamma ) $ with $\gamma = ( m_2 e_1-m_1e_2 ) / ( l+m_1+m_2 ) $ .
the ordinary bessel functions are perfectly well defined for complex arguments . for example , here is a plot of $\re [ j_2 ( x + i y ) ] $: the difference between the ordinary and modified bessel functions is that they satisfy different equations : $$ z^2 y'' + z y ' + ( z^2 - n^2 ) y = 0 , $$ for the ordinary bessel functions and $$ z^2 y'' + z y ' - ( z^2 + n^2 ) y = 0 , $$ for the modified bessel functions . note that there is a relationship between them : $$ j_{\nu } ( z ) =\frac{z^{\nu } i_{\nu } ( i z ) }{ ( i z ) ^{\nu }} $$ with similar identities going the other way . it is all very similar to the relationship between the trig functions $\sin ( z ) , \cos ( z ) $ with the hyperbolic functions $\sinh ( z ) , \cosh ( z ) $ .
yes , a color is a point in lms space . at least , that is the signal that the eye tells to the brain starting signal which is post-processed by neurons in the eye and brain . for example , the brain does some inferences on what the lighting condition is etc . , so that an object looks like it has one color even if half of it is in sunlight and half of it is in shadow . there is a linear transformation between lms and the xyy of the cie color-space . it is some 3x3 matrix . the horseshoe / sharkfin / shoe-sole is a slice of xyy space at constant y surface in xyy space projected onto the xy plane . below left is the slice at y ~ 1 , below right is the slide at y ~ 0.5 below left is the surface showing the highest possible y for each xy , below right is the surface showing a somewhat lower y at each xy . the slice at y = 0 would be a completely black horseshoe . if there was a frequency of light that only stimulated the l cells , and a different frequency that only stimulated m , and a different frequency that only stimulated s , then it would be possible to create a kind of rgb space that fills the entire visual gamut . unfortunately , that is not the case . if you are making a projector with three color lights , the rules are ( 1 ) each of those three colors has to be composed of a non-negative amount of each frequency , and ( 2 ) each pixel on the screen has to have a non-negative amount of each color . these two requirements are mathematically incompatible with recreating every possible sml color stimulus . ( the lack of negative numbers means that your linear algebra intuition does not apply here . ) the " old painters belief " is certainly not true , and i doubt that any painters really believe that . ask a painter to grab red , yellow , and blue tubes of paint , and then mix them to get black , and then mix them a different way to get white . they will surely acknowledge that it is impossible . basically , i agree with what you said .
i would guess that the article is referring to solitons . i am not sure if every non-linear system gives soliton solutions , but many do . the wikipedia article i have linked gives lots of examples of classical solitons , but i am not sure to what extent ( if at all ) they are important in the standard model . perhaps one of the qft specialists hereabouts could comment .
as you approach the event horizon , the doppler shift of any electromagnetic signals you send back out approaches infinity . any distant observer will get old and die while monitoring your signals that get slooower and sloooooower . your own experience is that you exist for some finite time ( not very long , for a black hole of a typical size ) , and then you hit the singularity and die . ( you may die earlier because of tidal forces . that depends on the size of the black hole . ) to you , the experience is that the doppler shifts of signals coming in from the outside become greater and greater . however , you do not get to see the arbitrarily distant future of the outside universe . for any location outside the event horizon , there is some latest time at which their signals can reach you before you go splat into the event horizon . i have never liked descriptions of this sort of thing that use phrases like " it seems to you like " or " you see " or " in your frame of reference . " general relativity does not have global frames of reference . it is meaningless to talk about whether something far away is happening " now " as judged " according to you . " all you can do is receive or transmit signals . you know they are just signals . btw , there is a classic science fiction novel , gateway , by frederik pohl , in which the protagonist makes himself really , really miserable by imagining that his lover , whom he abandoned to fall into a black hole , is forever cursing him " now . " a better understanding of general relativity would have been the best therapy for the poor guy -- but i guess that would have spoiled the book .
work done by the electric force is positive and not negative . the change in the potential energy of the electric field $\delta u$ is equal to the negative of the work done $w$ by the electric force . you have $$\delta u = -w$$ $$u_1=u_0-w&lt ; u_0$$ $$\therefore w&gt ; 0$$ and as $$w=\int f\cdot dl&gt ; 0$$ , we can say that $f$ acts in the same direction as $dl$ , i.e. it is attractive .
$\partial_i \frac{f}{1-f^2}=- ( \frac{f}{1-f^2} ) \partial_i ( \frac{1-f^2}{f} ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) \partial_i ( \frac{1}{f}-f ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) ( \partial_i ( \frac{1}{f} ) -\partial_if ) ( \frac{f}{1-f^2} ) $ $=- ( \frac{f}{1-f^2} ) ( - \frac{1}{f}\partial_i f \frac{1}{f} ) -\partial_if ) ( \frac{f}{1-f^2} ) $ $= ( \frac{1}{1-f^2} ) \partial_i f ( \frac{1}{1-f^2} ) + ( \frac{f}{1-f^2} ) \partial_i f ( \frac{f}{1-f^2} ) $
a hydrogen atom can be made to show chaotic behaviour if it is excited to very near it is ionisation energy . the maths is somewhat beyond me , but this paper discusses calculations of the hydrogen atom showing the onset of chaotic behaviour . you can find more by googling for " rydberg atom " combined with " chaos " or " chaotic behaviour " .
there seem to be a lot of human body mechanical models , such as this one : as for applications , i have heard that sub-audio frequency vibrations have been considered as nonlethal weapons for riot control .
grassman $d\theta$ has opposite mass dimension to $\theta$ , which is why the notation is not 100% optimal , it confuses on this issue . but if you know how to evaluate the integral , that it goes like the derivative , then you know how change of scale works , and it is the opposite of normal change of scale : $$\int d ( k\theta ) f ( k\theta ) = {1\over k} \int d\theta f ( \theta ) $$ and this is why the volume determinant for the integration ends up being the reciprocal of the bose case .
these are selection rules for the electric dipole radiation . the transition from the excited state to a lower state of an atom is governed by the following matrix element : $$c \cdot \langle f | \hat{p} | i\rangle$$ you could also write the sum of positions of electrons $\sigma\hat{r}$ instead of their total momentum $\hat{p}$ in the middle . this simple form of the operator in the middle is because at low enough frequencies , i.e. long enough wavelength , the atom simply finds itself in a uniform field , anf the electric potential $\phi$ for a uniform field is linear in $\hat{r}$ . equivalently , the matrix element may be converted from $\hat{r}$ to $\hat{p}$ and vice versa by realizing that the commutator of the hamiltonian with $\hat{x}$ is proportional to $\hat{p}$ . at any rate , the operator in the middle is a 3-dimensional vector that only acts on the positions or momenta of the electrons , not their spins . so it has to commute with the total spin operator $\hat{s}$ of the electrons , and $\delta s=0$ as a consequence . on the other hand , it is a vector , i.e. a $j=1$ object as far as the so ( 3 ) transformations go , and by combining its $j=1$ angular momentum with the orbital angular momentum $l_i$ of the initial state , you may get the final $l_f$ being $l_i\pm 1$ or $l_i$ according to the basic rules of the addition of angular momentum . you may imagine that you are just adding two vectors of specified lengths , $l_i$ and $1$ , and depending on their relative angle , the length of the sum may go from $l_i-1$ to $l_i+1$ . your list did not allow $l_f=l_i$ and i think you are right that it is typically forbidden as well because it violates parity . the angular momentum to parity goes like $ ( -1 ) ^l$ , i guess , but because the vector operator is parity-odd , the initial and final states have to differ by their parity as well , which means that only $\delta l=\pm 1$ is allowed . everyone , please correct me if i am saying something incorrectly . best wishes lubos
i think what you are getting at is not some kind of mathematically rigorous equivalence , but more what it means for a particle physics experiment like atlas to collect 1 inverse femtobarn of data . and actually this is computable quire easily . the design frequency of the lhc is 40mhz ( which corresponds to 25ns bunch spacing , but now i is at 50ns ) . but since most events are uninteresting background all modern experiments have a system called a " trigger " which only records events which pass some rough requirements which would render them interesting ( maybe a high-momentum electron or jet ) . atlas is routinely recording at 300hz ( a $10^5$ reduction of rate from the initial collision rate ) . that is 300 events per second . the size of an event in terms of storage space varies from experiment to experiment and depends on the software it uses , but for atlas it is something of the order of 1.5mb/event . currently the lhc runs at peak luminosities of 12600 $\mu b^{-1}/s$ ( microbarn per second ) , this decreases over time since the beam intensities decrease , let 's just run with 1000$\mu b^{-1}/s$ . an inverse femtobarn is $10^9\mu b^{-1}$ so we have : $$\frac{300 \text{ events}}{s}\frac{1.5 \text{ mb}}{\text{ event}}\frac{s}{1000\mu\text{b}^{-1}} \approx 0.5 \frac{\text{mb}}{\mu\text{b}^{-1}}$$ so for $10^9 \mu b^{-1}$ we have $$0.5\cdot10^9\text{mb}$$ so 500 tb of data ps : this is just an back-of-the-envelope calculation of course . the rates are constantly changing and the luminosities as well . so collecting 1/fb of data in a low lumi setting requires much more data ( since one would still max out the bandwidth of 300hz recording ) than in high lumi settings ( where one is still bound by the 300hz boundary , so the trigger would have to do a tighter selection )
it is called the landau-zener formula : wittig , curt . the landau-zener formula . j . phys . chem . b 109 ( 2005 ) pp . 8428-8430 . doi:10.1021/jp040627u clarence zener found it correctly , landau had a factor of $2\pi$ error , they were independent . there are two papers by landau on this : landau , lev d . a theory of energy transfer on collisions . phys . z . sowjet . ( physikalische zeitschrift der sowjetunion ) 1 88 ( 1932 ) and landau , lev d . a theory of energy transfer ii . physik . z . sowjet . 2 46 ( 1932 ) . they can both be found in landau 's collected papers , which may be easier to get hold of than physik z sowjet . see also [ 1 ] in the first paper above for the zener reference .
daylight does not contain equal amounts of all colours . the spectrum of daylight peaks around yellow/green , and the amount of energy falls off quite sharply as you move to the blue end of the spectrum . that means even if the width of the blue filter is the same as a green filter , you will measure less light coming through the blue filter . if you are using artificial light the effect will be even more pronounced as artificial light generally has a lower colour temperature than sunlight . incidentally , the response of your solar cell will also vary with the colour of the light , and this may well also be affecting your results . i can not lay my hands on a typical frequency response for a silicon solar cell , but i think the response falls off at the blue end of the spectrum .
with no resistance , the full voltage is applied to the fan , and you get mechanical work done , at whatever efficiency the fan itself is capable of . never minding the fan itself , so far as the electrical aspect goes , you could say it is 100% efficient . with resistance in the system , for example about equal to the resistance of the fan , you have less current flowing through the system . half as much . same voltage . so the system is using half the power . the fan , now one part of a voltage divider circuit , is getting half the voltage , and getting half the current . it runs slower , doing less , doing about 1/4 the work . 1/4 of the fan action divided by 1/2 power going into the system means that the system is 50% efficient . imagine a lot of resistance in the system , megohms . you will have only a trickle of current , say a microamp . the fan barely moves . the system will be using very little power , but most of that little power is just making the resistor hot . or rather , a small fraction of a degree warmer than ambient temperature . the system efficiency is close to zero .
i get the impression that op is referring to normalized ricci flow ( nrf ) : $$ \frac{1}{2} \partial_t g_{\mu\nu} ~=~ -r_{\mu\nu} + \frac{\langle r \rangle}{n} g_{\mu\nu}~ . $$ here $\langle r \rangle$ is the average scalar curvature over the full space-time $m$ . the average procedure is often weighted with an einstein-hilbert boltzmann factor . it is just a number ( as opposed to a space-time dependent scalar quantity ) . also $n$ is the space-time dimension , which is fixed , and hence cannot be easily varied as op suggests .
the problem with this question is that static friction and kinetic friction are not fundamental forces in any way-- they are purely phenomenological names used to explain observed behavior . " static friction " is a term we use to describe the observed fact that it usually takes more force to set an object into motion than it takes to keep it moving once you have got it started . so , with that in mind , ask yourself how you could measure the relative sizes of static and kinetic friction . if the coefficient of static friction is greater than the coefficient of kinetic friction , this is an easy thing to do : once you overcome the static friction , the frictional force drops . so , you pull on an object with a force sensor , and measure the maximum force required before it gets moving , then once it is in motion , the frictional force decreases , and you measure how much force you need to apply to maintain a constant velocity . what would it mean to have kinetic friction be greater than static friction ? well , it would mean that the force required to keep an object in motion would be greater than the force required to start it in motion . which would require the force to go up at the instant the object started moving . but that does not make any sense , experimentally-- what you would see in that case is just that the force would increase up to the level required to keep the object in motion , as if the coefficients of static and kinetic friction were exactly equal . so , common sense tells us that the coefficient of static friction can never be less than the coefficient of kinetic friction . having greater kinetic than static friction just does not make any sense in terms of the phenomena being described . ( as an aside , the static/kinetic coefficient model is actually pretty lousy . it works as a way to set up problems forcing students to deal with the vector nature of forces , and allows some simple qualitative explanations of observed phenomena , but if you have ever tried to devise a lab doing quantitative measurements of friction , it is a mess . )
this is only being posted as an answer because i cannot comment ( not enough points yet ? ) : mark everitt is wrong . . . the simplest argument would be appealing to this explanation of a microphone : http://www.ccmr.cornell.edu/education/ask/index.html?quid=1212 you can always evaporate masses to membranes and have them oscillate . . . this was in fact used in my research to study gravity at extremeley small length-scales . that being said , yes you can couple a membrane to a circuit . check out the field of optomechanics , which does this with laser cooling .
what you are describing is the difference between brittle and ductile behaviour . most materials show both properties under the appropriate conditions . for example glass becomes ductile as the temperature rises towards the glass transition , while metals become ductile at low temperatures . a brittle material may be superficially unaffected by a blow , but it is likely that there will be some effect at the atomic scales . if you study the surface with an electron microscope you will probably find the blow has caused small defects on the surface , and these could nucleate a crack under stress . alternatively the blow could have caused very small cracks , that again could lead to failure under stress . to what extent this happens depends on the force of the blow . in metals repeated small deformations can lead to the well known phenomenon of metal fatigue . analogous processes do happen with brittle materials , though i have to confess this is outside my area of expertise . a quick google for something like " fatigue in brittle materials " will find lots of related articles like this one .
the question was motivated because i have a suspicion that the electron does not participate to the source of a gravitational field , and eventually not even responds to such a field . in 1908 milikan measured the charge on a single electron . the charge-to-electron mass ratio $q/m_{e}$ was calculated by thomson in 1897 using the angular momentum and the deflection due to a perpendicular magnetic field . i think that this value reflects the inertial mass . i found this old ( 1964 ) doc gravitational and resonance experiments on very low-energy free electrons by fairbank , and this entry experimental comparison of the gravitational force on freely falling electrons and metallic electrons by wittborn and fairbank , 1967 , with the abstract : a free-fall technique has been used to measure the net vertical component of force on electrons in a vacuum enclosed by a copper tube . this force was shown to be less than 0.09mg , where m is the inertial mass of the electron and g is 980 cm/sec2 . this supports the contention that gravity induces an electric field outside a metal surface , of magnitude and direction such that the gravitational force on electrons is cancelled . it seems that the issue remains unsettled . - docs of 1992 and 2007 - ( tests of the weak equivalence principle for charged particles in space ) fairbank , as everyone else back then , and even now , believed that the electron must participate in gravity , contrary to my suspicion , preferring to imagine the existence of an imaginary induced electric force , possibly because in the 50s and 60s existed some hype about possible effects relating electricity and gravity . experimentation is central to advancement of physics , and the solution of this unsettled issue may prove important in the genesis of a sucessful theory encompassing both the so called particles and the gravitation field . i can understand that performing the test of fairbank under microgravity conditions can discriminate if the electron responds to the gravity field . to test if the electron has an active gravitational mass a possible test that may work ( 'i do not know the tecnological limits . . . ' ) could be done using a collimated beam of slow neutrons with a path traversing between the plates of a very large high-voltage capacitor . the possible deflection , or not , of the beam may prove very interesting .
the assumption is that there is a frictional force at point a that resists swinging of the gate . the amount of frictional force available is proportional to the downward force at that point . it is just like if you have a block on the ground . you push on the block horizontally , the block resists moving due to the frictional force . the heavier the block , the more force you need to overcome friction . if you could make the block weightless , then it would move freely .
this is a resonance in the circuit--- when you have a bunch of different frequencies driving a resonant system , the response is only strong for those frequencies which are close to the natural frequency of the resonant oscillator . you can see the same phenomenon in mechanical systems . if you have a mechanical mass on a spring , and you apply a force which varies with time , the amplitude of oscillation is $$ { f ( \omega ) \over \omega^2 - \omega_0^2 + i\gamma} $$ where $f ( \omega ) $ is the fourier component of the force at the frequency $\omega$ , $\omega_0$ is the natural frequency of the oscillation , and $\gamma$ is a small damping parameter . in the limit of small $\gamma$ , you pick out only the fourier component of f near the resonant frequency , those components which are different in frequency cancel because they push and pull at the wrong time given the natural vibration frequency of the oscillator . this natural fourier transform property of linear oscillators is the basis of human hearing , where the hairs in the ear are tuned to resonate only very close to one frequency . it is also the basis for radio tuning , or any other linear frequency sensitive response .
there is a nice reason for this , which witten often explains . imagine that your three dimensional space is the boundary of a four-dimensional space , for example , you can imagine that space is the surface z=0 of regular four dimensional space x , y , z , t . further , you can imagine that space is closed into a sphere , which does not affect things except for some boundary conditions at infinity ( the physics should not care about such things , also note that this is implicitly euclidean ) . if you close the three dimensional space-time into a sphere , the interior of the sphere is like the rest of the values of z for the plane case . you can extend any 3 dimensional gauge field configuration to the imaginary fourth dimension arbitrarily , so that any gauge field on the surface of the sphere can be extended to many different gauge fields on the interior . on the interior , you can construct the manifestly gauge invariant operator : $$ \epsilon_{\mu\nu\lambda\sigma} f^{\mu\nu}f^{\lambda\sigma} = f\tilde{f}$$ it is important to note that this quantity is a perfect divergence : $$ f\tilde{f} = \partial_\mu j^\mu_\mathrm{cs} $$ where j is the chern-simons current in 4-dimensions . using stokes theorem , for any four-dimensional gauge field configuration $$ \int f\tilde{f} = \int d ( *j ) = \int_\partial *j $$ where the last equality is stoke 's theorem , and the previous equality is writing the diverence of a current as the poincare dual of a three-form . so the manifestly gauge invariant $f\tilde{f}$ integral on any gauge field on the interior of the sphere is equal to the integral of the three form *j on the boundary of the sphere . so the integral of *j must be gauge invariant . i did not work out the actual form of *j , but it is the quantity you are trying to prove gauge invariant . although witten 's argument is conceptually illuminating , so it is the correct argument , verifying gauge invariance explicitly is not much more difficult than understanding all parts of the argument . still , it is good to know the conceptual reason , because the reason the chern-simons style things are important is exactly because they are the boundary terms of integrals of those gauge invariant field tensor combinations which are perfect derivatives .
for 2d/3d cases the ellipse has 5/7 degrees of freedom . you say you have 3 points and their " tangents " . a single 2d/3d point gives 3/5 equations . 3 points give 9/15 equations . so that in both 2d and 3d cases you have an overdetermined equation system ( actually it is overdetermined even with 2 points ) . in the general case it may be solved for instance by one of those methods .
there is quite a big controversy these days about the correct definition of the entropy in the microcanonical ensemble ( the debate between the gibbs and boltzmann entropy ) , which is closely related to the question . everyone agrees that the correct definition of the density matrix is given by $$\rho ( e ) =\frac{\delta ( e-h ) }{\omega ( e ) } , $$ where $h$ is the hamiltonian and $$ \omega ( e ) =tr\ , \delta ( e-h ) . $$ then the question is the correct definition of the entropy . boltzmann says $s_b=\ln \omega ( e ) $ , whereas gibbs argued $s_g=\ln \omega ( e ) $ where $$ \omega ( e ) =\int_0^e\omega ( e ) de . $$ in the text quoted by the op , the partition function corresponds to $\omega ( e ) $ . note that in most cases , in the thermodynamics limit , both entropies gives the same result . the question arises in the case of small systems and special cases with bounded from above spectra . hilbert et al . ( arxiv:1408.5382 and arxiv:1304.2066 ) argue that only the gibbs entropy is thermodynamically consistent . i must say that i find their arguments compelling , and that of their opponents , given in at least two comments of their papers , not at all .
if $f_{ii}=0$ then you are right that the $i\neq j$ constraint is unnecessary although it does make the physical interpretation clearer : all atoms ( other than $i$ ) act on $i$ . however , in practice , the force is usually given as a function of separation , $f ( r ) $ . and so when you evaluate $f_{ii}$ , you effectively evaluate $f ( 0 ) $ . the problem is that $f ( 0 ) $ is almost never zero . for instance , the coulomb and lennard-jones interactions are undefined for $r=0$ and so , when evaluating them ( either by hand or in code ) , you must explicitly skip the $i=j$ case .
the einstein-hilbert action of general relativity , to make the variational principle fully rigorous , must be supplemented by a boundary term , $$s = \frac{1}{8\pi g} \int_{\partial m} d^3 x \sqrt{-h} \ , k$$ where $h_{\mu \nu}$ is the first fundamental form of a submanifold which we take to be $\partial m$ , the boundary of the spacetime manifold . the cuvature $k$ is the trace of the extrinsic curvature . so your concerns are justified , strictly speaking , one should include a boundary term , unless the manifold has no boundary . ( the boundary term was first derived by gibbons , hawking and york . for additional information , i highly recommend the gravitational physics lectures online from the perimeter institute by professor ruth gregory - her lectures are excellent . )
maybe a way for you to be not confused is to imagine a time dependence . for instance , let suppose three times $t_i , t , t_f$ with $t_i &lt ; t &lt ; t_f$ . one may suppose that the particle is in the initial state $|a\rangle$ at time $t_i$ , is in the final state $|z\rangle$ at time $t_f$ , and , at the intermediary time $t$ is in one of the $2$ states $|s_1\rangle$ or $|s_2\rangle$ . the law of composition of amplitudes say that : $\langle z , t_f|a , t_i\rangle =\langle z , t_f|s_1 , t\rangle\langle s_1 , t|a , t_i\rangle + \langle z , t_f|s_2 , t\rangle\langle s_2 , t|a , t_i\rangle$ this is true for all $z$ , so we have : $|a , t_i\rangle = |s_1 , t\rangle\langle s_1 , t|a , t_i\rangle + |s_2 , t\rangle\langle s_2 , t|a , t_i\rangle$ now , you may interpret this equation as follows : given that the particle is in the state $|a\rangle$ at time $t_i$ , the probability amplitude to find the particle in the state $|s_1\rangle$ , at time $t$ , is : $\langle s_1 , t|a , t_i\rangle$
as a technical term in quantum mechanics , two particles are indistinguishable if they cannot be distinguished even in principle . typically , this means that they are identical other than their spin , momentum , position , energy and the time at which they exist . what this implies is that the wave state that describes the pair of particles has to be symmetrized or anti-symmetrized . in particular , different isotopes are distinguishable . one writes : $$|ab\rangle = ( |a\rangle|b\rangle \pm |b\rangle|a\rangle ) /\sqrt{2}$$ where the sign is + for symmetrization of bosons , or - for the antisymmetrization of fermions . on the other hand , if the two states are not identical , then the wave state is not ( anti ) symmetrized : $$|ab\rangle = |a\rangle|b\rangle$$ you can think of indistinguishability as arising naturally from the wave nature of particles . you might be able to write descriptive letters on particles , but how can you distinguish between two identical waves ? now suppose you had a system where spin was conserved and never altered . an electron would be either spin-up or spin-down and never change and so you could distinguish between two such electrons . it would be as if they were distinct particles . this concept shows up in isospin where a neutron is treated as distinguishable from a proton even though they form an isospin doublet that is similar to the spin doublet ( spin-down , spin-up ) . in such a restricted system , with a state of two particles you have the choice of writing your wave states either symmetrized , anti-symmetrized , or unsymmetrized . any ( consistent ) way you choose , you will get the same answer when you make your calculations as the two states form separate hilbert spaces . however , if the instructor looks carefully at your derivation , it could be marked wrong even though it gives the correct answer -- it is better to follow the rules . to learn more about this subtlety , read up on superselection sectors . what is going on is that when your hilbert space divides into sectors which cannot physically interact , you do not have to symmetrize or antisymmetrize . but as i say above , you do not need to know this detail in order to calculate . just follow the rules ; only identical particles ( in the sense of identical other than position , energy , momentum , orientation , etc . ) are either symmetrized or antisymmetrized .
i think this is a case of the mathematics being designed to model reality . as you say , making the time component of the metric positive would give a space that does not match what we observe . in particular , the negative component for time allows us to disconnect regions of space that are not causally linked . in other words , the fact that the speed of light is finite and a maximum means that we must describe space-time with a shape that keeps causally disconnected regions separate . the necessary shape is reflected in the choice of the sign of $\eta_{00}$ . that is how i understand anyway . . .
this question is actually one of the lab exercises i teach . for a spring-mass system , if the damping force is friction , then it is independent of velocity ( verified experimentally ) . however , as mentioned in the comments , the damping force may not always be friction . for example , if the mass is a material like aluminium and it is oscillating over some magnets , the damping force will be linearly dependent on velocity .
as the comments have mentioned , acceleration , torque and force can all be zero , but the key point is that some zeros are more fundamental than others . take acceleration : you state in a comment that newton 's law of gravity has an infinite range so the force/acceleration generated by some gravitating body can never be zero . this is quite true , but acceleration/force can be positive or negative so two non-zero accelerations can sum to zero . this is what dmckee means in his comment . even though the gravitational fields of the earth and moon are infinite , somewhere in between them is a point where the net force is zero . but ( ignoring some technical definitions ) you can not have a negative temperature . so i can not take a chunk of matter and make it is temperature zero by mixing it with another chunk of matter that has a negative temperature . the only way i can cool my chunk of matter is by putting it in contact with something that is colder , but of course still has a positive temperature . so 0k is special because there is nothing colder than it .
i suppose a lot of what constitutes the " friedmann equation ( s ) " is just up to definitions . however , with the 3 equations you listed , there will be redundancy . here is the normal derivation i usually see ( feel free to skip the first two paragraphs if you are not familiar with tensors/gr ) : given einstien 's equation in the form $r_{\mu\nu}=-8 \pi g s_{\mu \nu}$ , ( where $s_{\mu \nu}$ is related to the stress energy tensor by $s_{\mu \nu}=t_{\mu \nu} - \frac{1}{2}g_{\mu \nu}t^\lambda_\lambda$ ) and with some playing around with the spatial portion of the robertson walker metric ( see weinburg cosmology ) , we can get a riemann tensor $r_{ij}=\tilde{r}_{ij}-2\dot{a}^2 \tilde{g}_{ij}-a\ddot{a}\tilde{g}_{ij}$ ( tilde means the spacial metric and it is curvature tensor ) to $r_{ij}=- [ 2k+2\dot{a}^2+a\ddot{a} ] \tilde{g}_{ij}$ ( where $k$ is the curvature constant ( -1,0 , +1 ) ) . we then decide on a stress energy tensor , using the principles of homogeneity and isotropy ( we do not want there to be some strange asymmetry ) , to get one of the form $t_{00}=\rho$ , $t_{i0}=0$ , and $t_{ij}=a^2p\tilde{g}_{ij}$ . which gives us $s_{ij}=\frac{1}{2} ( \rho-p ) a^2\tilde{g}_{ij}$ and $s_{00}=\frac{1}{2} ( \rho+3p ) $ . using all this , plugging back into the efes , we get two equations ( one for i=j=0 , and one for the rest ) : ( 1 ) $-\frac{2k}{a^2}-2\frac{2\dot{a}^2}{a^2}-\frac{\ddot{a}}{a}=-4\pi g ( \rho - p ) $ ( 2 ) $\frac{3\ddot{a}}{a}=-4\pi g ( 3p+\rho ) $ then , cause the first equation is sorta unwieldy , we can add three times the first equation to the second to get the nicer ( and more familiar ) : ( 3 ) $\dot{a}^2+k=\frac{8}{3}\pi g a^2$ we can also get the following equation from ( 1 ) and ( 2 ) : ( 4 ) $\dot{\rho}=-\frac{3\dot{a}}{a} ( \rho + p ) $ which is really no surprise , as this conservation law is found in any solution to the efes . so really what equations you decide to call the " friedman equations " is up to whatever your doing . ( 1 ) and ( 2 ) are the direct consequences of derivation , which you can then use to derive ( 3 ) and ( 4 ) . it just turns out for most cosmology calculations , we do not want to use ( 1 ) , and the last 3 ( ( 2 ) ( 3 ) ( 4 ) ) are more useful . however there will be redundancy within these 3 equations ( after all , they came from just two equations ) ! edit : just to adress the ops question : lets take ( 2 ) and ( 4 ) : ( 2 ) $\frac{3\ddot{a}}{a}=-4\pi g ( 3p+\rho ) $ ( 4 ) $\dot{\rho}=-\frac{3\dot{a}}{a} ( \rho + p ) $ lets multiply ( 2 ) by $\frac{2a\dot{a}}{3}$ . $\frac{2a\dot{a}}{3} [ \frac{3\ddot{a}}{a} ] =\frac{2a\dot{a}}{3} [ -4\pi g ( 3p+\rho ) ] =&gt ; 2\dot{a}\ddot{a}=-\frac{8}{3}\pi g ( 3p a\dot{a} + \rho a\dot{a} ) $ some tricky algebra here : $=&gt ; 2\dot{a}\ddot{a} = \frac{8}{3}\pi g ( -3 a\dot{a} ( \rho+p ) + 2\rho a\dot{a} ) $ we now substitute in the conservation of energy equation . ( 5 ) $2\dot{a}\ddot{a} = -\frac{8}{3}\pi g ( \dot{\rho}a^2+2\rho a \dot{a} ) $ but hey ! this looks sorta like what you would get if you differentiated the friedmann equation ( 3 ) , note that $k$ is not time dependent ( if it were , that would be crazy ! ) . since your integrating instead of differentiating you will have constants of integration , but you can just tie that into the $k$ term ( which is a sorta interesting way to view what $k$ means ) . edit : doing a similar process will show you a way to derive ( 4 ) to begin with .
you can see jupiter in the night sky with your naked eyes due to its reflected sunlight ( although i believe that in july and august of 2014 jupiter is very close to the sun in the sky and is visible only for a little while near twilight ) . you can take a picture of jupiter in the sky with any old camera . if you want a high-quality picture , your camera needs to have a lens arrangement that will make the image of jupiter on the camera 's ccd larger than the image of jupiter on your retina . the thing to look for is a lens with a long focal length . if the focal length of the lens 1 is long enough , it will need to stand some distance away from the camera 's ccd on a rigid mount ; this is usually called a telescope . you can replace the camera with your eye and see jupiter 's cloud bands directly . 1 actually most telescopes use a curved mirror rather than a lens , for several technical reasons . images as nice as that one usually come ( possibly ) from professional astronomical observatories on the ground , or from the hubble telescope , probably nasa 's most successful instrument ever ( after a rocky start ) . your particular image seems to have been taken by the robotic spacecraft cassini when it passed near jupiter en route to saturn , where it has been orbiting and collecting data for the last ten years . in that case the camera had the advantage of being much closer to jupiter than i will ever be :- (
same charges repulse each other . so when they are confined in a system , they try to have stable distance among them as much as possible . for a hollow conducting sphere this stable maximum distance is equal distribution of charges in the outer surface . if any charge try to go the inside conducting surface it automatically decrease distance which increase repulsion . that is why charge inside the conducting surface zero .
actually , most research suggests that interactions of two galaxies ( i.e. . , " mergers " or collisions ) is what determines the shapes of galaxies , and not necessarily gravity alone . this astronomy now article from a few years ago goes a bit more into the details . it is true that there is a strong force pulling us inwards ( on the order of a billion newtons ) , so you indeed could say that a galaxy is collapsing , but it would take something like $10^{100}$ years before the stars would fall into the supermassive black hole in the center . on that time-scale , it really is not a statement worth making , imo .
a generalized free field is one for which ( modulo field redefinitions ) the connected $n$-point functions $g_n ( x_1 , . . . , x_n ) $ vanish whenever $n &gt ; 2$ . this means , basically , that a generalized free field is one for which the euclidean functional measure is gaussian . the field is completely specified by its 2-point functions $g_2 ( x , y ) $ . generalized free fields are usually discussed using the parametrization given by the kallen-lehmann decomposition of the 2-point function $g_2 ( x , y ) $ , which says that ( for scalar fields ) $g_2 ( x , y ) = \int_0^\infty d\rho ( m ) \delta_m ( x-y ) $ , where $\delta_m ( x-y ) $ is the real-space propagator for a free real field of mass $m$ and $\rho$ is a positive measure . this parametrization makes it easy to write down examples of generalized free scalar fields . just pick a positive measure $\rho$ on the mass line $ [ 0 , \infty ) $ . the simplest example is the free field of mass $m$ , which corresponds to $\rho ( m ) = \delta_m ( m ) $ , the dirac delta function supported at $m$ . you get other examples by picking other measures . for example , you can take a purely continuous measure , like $d\rho ( m ) = \theta_m ( m ) dm$ or $d\rho ( m ) = m^2dm$ . ( here , $\theta_m ( m ) = 0$ if $m &lt ; m$ and $1$ otherwise . ) these examples where the measure has continuos support are somewhat difficult to think of in the usual lagrangian formalism ; it is as if you have a continuum of fields of different mass which are all constrained to move together . but giving the correlation functions is enough to define a field theory ; you can use wightman 's reconstruction theorem to recover the hilbert space and the field operators . if you are working in axiomatic field theory , then you usually impose some sort of growth conditions on your correlation functions . in the case of generalized free fields , these growth conditions translate into conditions on $g_2$ , or equivalently on $rho$ . for example , if the free field 's euclidean measure obeys the osterwalder-schrader axioms , then $\rho$ must be a tempered measure of polynomial growth at $\infty$ ( and not too unreasonable behavior near $0$ ) .
no , you are not missing the obvious : this is a good question . the simple harmonic oscillator shoved down your throat in freshman physics and engineering courses is a linear system : any solution scaled by a scale factor is also a solution . so the period cannot depend on amplitude . the basic equation defining this beast is $$\ddot{x} = - \omega^2 x\tag{1}$$ where $\omega$ is the frequency ( in radians per second ) and $x$ the displacement . in this system , the restoring force is proportional to the displacement ( and directed against the latter ) . given that the bottle is curved , you are almost certainly witnessing the amplitude dependent variation of period that arises in a nonlinear simple harmonic oscillator . you have something akin to a pendulum , where gravity acts normal to the motion at the bottom of the swing , and the component of gravity in the direction of motion is proportional to the sine of the swing angle . thus we get the equation : $$\ddot{\theta} = - \omega^2 \sin ( \theta ) \tag{2}$$ where now $\theta$ is the swing angle . notice that the restoring force in this model is ( realistically ) limited : it cannot be greater than $\omega^2$ . for small oscillations we again get the same equation as in ( 1 ) . but watch what happens when i put all this into the trusty mathematica slave below : i am plotting the solution to ( 2 ) for $\omega^2=1$ for two beginning speeds : $\dot{x} ( 0 ) =1$ and $\dot{x} ( 0 ) =1/2$ . witness that the bigger amplitude one has the longer period . this is exactly what you have just seen with your bottle .
the correct statement is that we can always construct a geodesic such that $x ( s ) =y ( s ) =0$ for every value of the affine parameter $s$ . all that independently from our initial choice of the origin and orientation of orthogonal cartesian coordinates $x , y , z$ in the $3$-manifolds normal to $\partial_t$ ( the natural rest space of the considered spacetime ) . the geodesics are solutions of the euler-lagrange equations of the lagrangian $${\cal l} = \sqrt{|-\dot{t}^2 + a ( t ( \xi ) ) ^2 ( \dot{x}^2+\dot{y}^2+\dot{z}^2 ) |}\: , \tag{1}$$ where the used parameter is a generic one $\xi$ ad the dot denotes the $\xi$-derivative . as ${\cal l}$ does not explicitly depend on $x , y , z$ , from e-l equations , we have the three constants of motion : $$\frac{\partial {\cal l}}{\partial \dot{x}}\: , \quad \frac{\partial {\cal l}}{\partial \dot{y}}\: , \quad \frac{\partial {\cal l}}{\partial \dot{z}}\: . $$ passing to describe the curves with the geodesical length $s$ , with $$ds = \sqrt{|-\dot{t}^2 + a ( t ( \xi ) ) ^2 ( \dot{x}^2+\dot{y}^2+\dot{z}^2 ) |} d\xi$$ these constants read , in fact , $$a ( t ( s ) ) ^2 \dot{x} ( s ) \: , \quad a ( t ( s ) ) ^2 \dot{y} ( s ) \: , \quad a ( t ( s ) ) ^2 \dot{z} ( s ) \: , $$ where now the dot denotes the $s$-derivative . in other words , there is a constant vector $\vec{c}\in\mathbb r^3$ , such that , for every $s$: $$a ( t ( s ) ) ^2 \frac{d\vec{x}}{ds} = \vec{c}\tag{2}$$ where $\vec{x} ( s ) = ( x ( s ) , y ( s ) , z ( s ) ) $ . the geodesics are described here by curves $$\mathbb r \ni s \mapsto ( t ( s ) , \vec{x} ( s ) ) \tag{3}\: . $$ looking at the lagrangian ( 1 ) , one sees that it is invariant under spatial rotations . that symmetry extends to solutions of e-l equations . in other words we have that , if ( 3 ) is a geodesics , for $r\in so ( 3 ) $ , $$\mathbb r \ni s \mapsto ( t ( s ) , \vec{x}' ( s ) ) := ( t ( s ) , r\vec{x} ( s ) ) \tag{4}$$ is a geodesic as well . correspondingly , due to ( 2 ) we have the new constant of motion $$a ( t ( s ) ) ^2 \frac{d\vec{x}'}{ds}= a ( t ( s ) ) ^2 \frac{dr\vec{x}}{ds} = a ( t ( s ) ) ^2 r\frac{d\vec{x}}{ds} = r\vec{c}\tag{2'}$$ unless $\vec{c}=0$ ( * ) , we can rotate this constant vector in order to obtain , for instance , $r\vec{c} = c \vec{e}_z$ . this means that the new geodesic verifies $$a ( t ( s ) ) ^2 \frac{d\vec{x}'}{ds}\:\: ||\:\: \vec{e}_z$$ the spatial part is parallel to $\vec{e}_z$ . i will omit the prime $'$ in the following and i assume to deal with a geodesic with spatial part parallel to $\vec{e}_z$ and thus , as $a\neq 0$ , it holds $x ( s ) = x_0$ , $y ( s ) =y_0$ constantly . let us finally suppose that the initial point of the geodesic is $\vec{x} ( 0 ) = \vec{x}_0$ . as the lagrangian is also invariant under spatial translations , we also have that if ( 3 ) is a geodesic , for $r\in so ( 3 ) $ , $$\mathbb r \ni s \mapsto ( t ( s ) , \vec{x}' ( s ) ) := ( t ( s ) , \vec{x} ( s ) + \vec{r}_0 ) \tag{5}$$ is a geodesic as well . choosing $\vec{r}_0 := - \vec{x}_0$ , we have a geodesic with $x ( s ) =y ( s ) =0$ as requested . ( * ) we can always choose $\vec{c}\neq 0$ assuming that the initial tangent vector of the geodesic verifies this requirement ( notice that $a^2 \neq 0$ ) . and we know that there is a geodesics for every choice of the initial conditions .
any siphon in your house will be affected in this way . the inside surface ( that you can see ) is affected by the air pressure in your house , the other level is connected to the sewer piping system , wich gets his pressure from a vent of the sewer system . both pressures can be affected in many ways by the stormy wind , depending where some passages ( like windows not really tight ) for the air are located , eg at the windward or leeward side of your house . the vent of the sewer system can develop pressure variations depending on wind speed by some water-aspirator-like action . so the siphon in your toilet works as a pressure differential indicator .
energy exchange is quantized when moving a electron from one bound state to another bound state . this is not because the exchange is inherently quantized , but because the states the electron may occupy are quantized . thus the standard photo-electric effect in which a photon can not excite an atom unless it has a minimum energy . however , . . . there are multi-photon processes by which sub-threshold light can excite transitions . cross-section for them go by intensity-squared ( or worse ) and are very small for any reasonable light intensity . to study or employ them you get powerful , short pulsed laser systems . where short pulsed means nano-second or faster pulses and powerful means " do not look into beam with remaining eye " . even then you do not get a lot of rate . these processes are utterly negligible for the kind of benchtop experiment we use to teach the photoelectric effect : you just can not get enough intensity . ( see below for how negligible . ) the conceptual model here is that the first photon bumps the electron to a short-lived , unstable state without well defined quantum numbers , and the second comes along before that state decays and finishes the job . we are currently exploring the application of such a process to calibrating light yields , opacities in a large volume of scintillating material . from new j . phys . 12:113024 , 2010 : for gases the one-photon absorption cross-section $\sigma_1$ is typically of the order of $10^{−17}\text{ cm}^2$ , whereas the two-photon and the three-photon cross-sections are of the order of $\sigma_s = w/f_2 \approx 10^{−50}\text{ cm}^4\text{ s}$ and $\sigma_3 = w/f_3 \approx 10^{−83}\text{ cm}^6\text{ s}^2$ , respectively . where $f$ is intensity in photons/second and w is excitation rate in reciprocal seconds .
well to be hones both of your questions are related . let me start by rewriting your equations of $y_1$ and $y_2$ ( this will make the discussion easier ) , your version of $y_1$ and $y_2$ can be rewritten as : $$y_1=a\sin\left ( \frac{2\pi}{\lambda} ( x+vt ) \right ) , $$$$y_2=-a\sin\left ( \frac{2\pi}{\lambda} ( x-vt ) \right ) , $$where i put a minus outside of $y_2$ by using $\sin ( -x ) =\sin ( x ) $ . let 's now first look at question 2 . question 2 : the answer to this question can be found by looking at the wavefronts of $y_1$ and $y_2$ , which in its turn can be done by looking at a constant value for the arguments of $y_1$ and $y_2$ ( since a constant argument yields a constant value of $y_1$ and $y_2$ and hence a wavefront ) . let 's call this constant value of the argument $x_0$ , then the arguments of $y_1$ and $y_2$ become : $$x_0=x+vt \text{ for the argument of $y_1$} , $$$$x_0=x-vt \text{ for the argument of $y_2$} . $$ both arguments can be rewritten as:$$x=x_0-vt \text{ for the argument of $y_1$} , $$$$x=x_0+vt \text{ for the argument of $y_2$} , $$ where we see that the introduced constant $x_0$ denotes the position of the wavefront at $t=0$ . what this tells us is that $y_1$ represents a wave travelling in the negative $x$-direction ( when the time increases the value of $x$ decreases ) and $y_2$ represents a wave travelling in the positive $x$-direction ( when time increases the value of $x$ increases ) . in general . you can do this analysis for every kind of wave , and you will find that waves with an argument of the form $ ( x+vt ) $ are waves travelling in the negative x-direction ( als called ''left travalling waves'' ) and waves with arguments of the form $ ( x-vt ) $ are wave travelling in the positive x-direction ( also called ''right travelling waves'' ) . on hyperphysics a few more drawings and discussions are available ( should you be interested ) . $$$$ question 1 : the answer to question 1 can be given by the fact that te amplitudes of waves can be summed . this is because of the fact that each wave tells you what displacement $y$ is causes at a given point $x$ on a time $t$ , when you have two waves which interact , the displacements should be summed . now when you look at reflection ( of sound our light or whatever wave you are looking at ) , there are 3 things that can happen : ( first case on the figure ) : when you reflect the reflected wave picks up a phase $\phi=\pi/2$ ( this happens when you reflect on a dense medium ) . in that case your reflected wave picks up a minus sign since $\sin ( x+\pi/2 ) =-\sin ( x ) $ , this is probably the case you are looking at ) . ( second case on the figure ) : when you reflect the reflected and indecent wave are in anti-phase , so they cancel eachother out . ( third case on the figure ) : when you reflect , the reflected wave does not pick up a sign ( and they are in phase ) , the amplitude of the wave doubles .
when integrating in ( 2d ) polar coordinates you need to use a surface element : $$da = r\ ; dr\ ; d\theta$$ the reason for this can be seen geometrically : the surface element has the same shape as one of the spaces between two red and two blue lines ( a sort of curved rectangle ) . in the infinitesimal limit the area of one such segment is just its length multiplied by its width . the length is easy , it is $dr$ , and is always the same ( notice that the length of a blue segment between the evenly spaced red lines is always the same ) . the width is a bit more subtle . first , you might notice that the " inner width " and " outer width " are different . we do not need to worry about this because in the infinitesimal limit , they approach the same length . but the length of the arc between two evenly spaced blue lines clearly increases as the radial coordinate increases . it should be easy to convince yourself that the length of an arc that spans angle $theta$ is $r\theta$ ( of course $theta$ in radians ) . it follows that the width of the surface element is $r\ ; d\theta$ ( the angle shrinks to an infinitesimal , but the radial coordinate does not - it simply takes the value of the radius wherever we place our surface element ) . so why does not your integral using a line work ? well , this treats surface elements at all distances from the origin as having the same size ( i am speaking very loosely here , since infinitesimals do not really have a size ) . but looking at the diagram , clearly surface elements close to the origin need to be smaller than ones further away , otherwise you end up " overcounting " area near $r=0$ and under-counting area further out . put another way , take a bunch of long thin rectangles of the same size cut out of paper and try to arrange them into an approximate segment of a circle without overlapping them . you should find it is impossible . even as you make them infinitesimally thin this fails . you need to use little wedge shapes pieces .
very basically , a speaker driver has a magnet and a coil which move relative to each other when electric current is applied . ( the speaker cone is attached to the moving parts and this is what moves the air that causes the audible sound ) . when placed close to a guitar pickup , the tiny coil movement itself ( not the cone/diaphragm movement ) and/or magnetic flux generated by the movement acts on the magnetic field of the pickup in a manner similar to the movement of the guitar strings themselves . it is also possible , depending on the design of the headphones , to use them as microphones . many years ago , i used a set of can-style headphones plugged into an reel-to-reel tape recorder as a sort of acoustic guitar pickup .
i meanwhile found the constant by intensive googling ( for example in this article ) . it is ( depending on the source ) around 144 k .
it is not an uncommon interpretation that the alcubierre drive acts as a multiplier of an existing subluminal velocity . the trouble is that the alcubierre metric describes the drive in constant motion that does not change with time . it tells us nothing about how the drive accelerated to that speed or decelerated from it . the drive is moving in whatever direction the metric says because that is how the metric was constructed . i have seen very few papers on the alcubierre metric that even mention the question of acceleration , and none that treat it in detail . i would guess the ratio of hardness of the problem to interest in the outcome is too high for most researchers . the paper the alcubierre warp drive : on the matter of matter examines it briefly although the authors ' interest is really on the effect of matter caught up in the drive . so i am afraid there is no answer to your question .
i think it is bizarre that a particle does not have a definite composition . yeah , it is . as qftme said , that is quantum mechanics for you . it really does not make sense until you immerse yourself in the subject for long enough ( and even then , only somewhat ) . but it does appear to be the way the universe works . anyway , just so everyone is on the same page , let me start from the basics . if you are familiar with linear algebra , you know that a vector in a 2-dimensional vector space , for example , can be written as a linear combination $\alpha|0\rangle + \beta|1\rangle$ of two basis elements $|0\rangle$ and $|1\rangle$ . for example , a direction vector of length 1 that points northeast can be written as $$\frac{|\text{north}\rangle + |\text{east}\rangle}{\sqrt{2}}$$ or it could be written as $$|\text{northeast}\rangle$$ or $$\alpha|\text{north-northeast}\rangle + \beta|\text{east-southeast}\rangle$$ etc . you could figure out what the coefficients $\alpha$ and $\beta$ are in that last case , but it does not matter . the point is , there are an infinite number of ways to decompose any vector . the pion state is an example of such a vector . it is often considered to be a member of a three-dimensional vector space . one possible basis for that vector space is $u\bar{u}$ , $d\bar{d}$ , and $s\bar{s}$ . but another possible basis is $$\pi^0 = \frac{u\bar{u} - d\bar{d}}{\sqrt{2}}$$ $$\eta = \frac{u\bar{u} + d\bar{d} - 2s\bar{s}}{\sqrt{6}}$$ $$\eta&#39 ; = \frac{u\bar{u} + d\bar{d} + s\bar{s}}{\sqrt{3}}$$ this basis is useful because these particular combinations happen to be relatively stable ; in other words , when a particle consisting of any combination of $u\bar{u}$ , $d\bar{d}$ , and $s\bar{s}$ is detected in a cloud chamber ( if you are old-school ) or a calorimeter or something like that , it will behave like one of these three particles . it is possible that what was actually emitted was the quantum state $u\bar{u}$ , but in terms of the " stable " states , that is $$u\bar{u} = \frac{1}{\sqrt{2}}\pi^0 + \frac{1}{\sqrt{6}}\eta + \frac{1}{\sqrt{3}}\eta&#39 ; $$ ( hopefully i did the math right ) . so you would have a probability of $\frac{1}{2}$ that it acts like ( or technically , collapses to ) a pion , $\frac{1}{6}$ that it collapses to an eta meson , and $\frac{1}{3}$ that it collapses to an eta prime meson . one of those three possibilities is what you had actually observe in your detector . you can do this the other way around , too : suppose that instead of $u\bar{u}$ , you started with a pion , and instead of measuring the " stable " meson type , you were able to directly measure the quark content . since the pion state contains equal components of $u\bar{u}$ and $d\bar{d}$ , your hypothetical quark flavor measurement would give you one of those outcomes with 50% probability each : half the time you had find that you had an up quark and an anti-up quark , and the other half of the time you had find a down and anti-down quark . that is what the state $\frac{u\bar{u} - d\bar{d}}{\sqrt{2}}$ actually means : it governs the probabilities that the pion will interact with a quark flavor measurement as each particular quark type .
the main difference between qm and qft w.r. t measurements etc . is the kind of question you ask . usually in qft you simply start your calculation assuming a plane wave of incident particles and determine the probablility of scattering in different directions . you do this to learn something about the interactions of the particles , not about the state the particle is in . you infer the interactions that took place from classical measurements of momentum and energy , not from any idealized quantum measurement . in order to describe the evolution of one specific particle , you would need to convolute your results for plane wave states with the wave function of your particle . still , once you have determined the state your particle ends with , the wave function collapses and this process is not described dynamically by the theory , but put in by hand . tl/dr : in qft the measurement process is still not described , but the state of the system is also not the object of interest .
texts on qcd do not divide the generators of $su ( 3 ) $ – and therefore " bicolors of gluons " – into two groups because this separation is completely unphysical and mathematically artificial ( basis-dependent ) . moreover , the number of " bicolors of gluons " i.e. generators of $su ( 3 ) $ , the gauge group of qcd , is not nine as you seem to think but only eight . the group $u ( 3 ) $ has nine generators but $su ( 3 ) $ is the subgroup of matrices with the unit determinant so one generator is removed . at the level of gluons or lie algebra generators , the special condition $s$ means that the trace is zero . so the combinations $$ a ( r\bar r ) + b ( g\bar g ) + c ( b \bar b ) $$ are only allowed " bicolors of gluons " if $a+b+c=0$ . now , in this 8-dimensional space of " bicolors of gluons " , there are no directions ( "bicolors" ) that are better than others . for any direction in this space , there exists an $su ( 3 ) $ transformation that transforms this direction into a direction non-orthogonal to any chosen direction you choose . this is true because the 8-dimensional representation is an irreducible one ( the adjective " irreducible " means that one should not try to split it to two or several separated collections ! ) . and there does not exist any consistent yang-mills theory that would only contain the six off-diagonal " bicolors " because the corresponding six generators are not closed under the commutator . the actual calculations of the processes with virtual gluons ( "forces " between quarks etc . ) therefore never divide terms to your two types because this separation is just an artifact of your not having learned group theory . instead , all the expressions are summing over three colors of quarks , $i=1,2,3$ indices of some kind , and there is never any condition $i\neq j$ in the sums because such a condition would break the $su ( 3 ) $ symmetry . now , the $r\bar r , g\bar g , b\bar b$ " bicolors of gluons " ( only two combinations of the three are allowed ) are actually closer to the photons than the mixed colors . so it is these bicolors that produce an attractive force of a very similar kind as photons – they are generators of the $u ( 1 ) ^2$ " cartan subalgebra " of the $su ( 3 ) $ group and each $u ( 1 ) $ behaves like electromagnetism . that is why these components of gluons cause attraction between opposite-sign charges and repulsion between the like charges . the six off-diagonal " bicolors of gluons " ( and let me repeat that the actual formulae for the interactions never separate them from the rest – they are included in the same color-agreement-blind sum over color indices ) cause neither attraction nor repulsion : they change the colors of the interacting quarks so the color labels of the initial and final states are different . it makes no sense to compare them , with the idea that only momentum changes , because that would be comparing apples and oranges ( whether the force looks attractive or not depends on the relative phases of the amplitudes for the different color arrangements of the quarks ) . at any rate , particles like protons contain quarks of colors that are " different from each other " so they are closer to the opposite-sign charges and one mostly gets attraction . however , the situation is more complicated than it is for the photons and electromagnetism because of the six off-diagonal components of the gluons ; and because gluons are charged themselves so the theory including just them is nonlinear i.e. interacting .
i think this is related to elastic fatigue . here 's the wikipedia page .
op wrote ( v1 ) : in the beginning of proof of this theorem one says that permutation of fields in the left and right side of ( 1 ) does not change it . answer : this is essentially due to the fact that operator ordering prescriptions ( such as e.g. time ordering $t$ or normal ordering $::$ ) are ( graded ) symmetric $$ t ( \hat{a}_{\pi ( i_1 ) }\ldots\hat{a}_{\pi ( i_n ) } ) ~=~\pm t ( \hat{a}_{i_1}\ldots\hat{a}_{i_n} ) , $$ $$ :\hat{a}_{\pi ( i_1 ) }\ldots\hat{a}_{\pi ( i_n ) }: ~=~\pm :\hat{a}_{i_1}\ldots\hat{a}_{i_n} : , $$ where $\pi\in s_n$ is a permutation of $n$ elements . see also this phys . se answer . so more generally , the statement reads : permutation of fields in the left and right side of ( 1 ) does not change it , up to possibly an overall sign on both sides in the case of grassmann-odd operators .
the observation of particle tracks is in terms of a sequence of ionizations of atoms , which are subsequently magnified by an appropriate mechanism . how particle tracks arise in quantum mechnaics is described in the following famous old paper : n.f. mott , the wave mechanics of alpha-ray tracks , proc . royal soc . london a 126 ( 1929 ) , 79-84 . http://rspa.royalsocietypublishing.org/content/126/800/79.full.pdf he shows that once some atom isionized in a track chamber , the following ionizations will be overwhelmingly in the same direction , thus explaining the track . the detectors , in principle atoms , are reduced to essentially just single valence electron , and only two states are of interest : ionized or not . but since we do not know in advance where a particle will be recorded first , one needs a large array of such electrons . the hamiltonian can thus be considered to be that of an array of localized 2-state systems , each initially in the same state , with an interaction that allows a transition of this state to an ionized state .
nothing as easy from basic principles as for conduction or radiation . you can multiple the mean heat carried by the convecting liquid by it is mass rate of flow $$ \delta t = c_p * ( t_{in} - t_{out} ) * \frac{\delta v}{\delta t} \rho \delta t . $$ where $t_{in}$ is the mean temperature of the liquid moving toward the sink ( cool side ) and $t_{out}$ is the temperature of the liquid moving the other way . the problem is that you can not get the temperature ( s ) of the convecting liquid or the flow rate without detailed calculations or measurements . for the generaly case you pretty much have to go to cfd .
well , to be really pedantic , almost anything is data . any measurement or observation is data . i suppose i would define data as any piece of information , regardless of how it was obtained or whether it is valid . of course , i think your question was really getting at what would be considered evidence , in the context of providing support or opposition to a theoretical model . i would say that the answer is very situation dependent . using simulation results as an example since it was in your question , i would certainty take simulation results in support of some theory , but only on the condition that the simulation has been very well validated against experiment . the details of how and why the simulation results are favorable to the theory is also certainly important . ultimately , all science must be validated by experiment or observation , but that can sometimes be a long journey , and there is definitely a place for other sorts of data along the way .
i do not have enough reputation to post this as a comment , which it should be . should not the product in your definition of $\left|\psi\right&gt ; $ go over $i , i'$ ? is it restricted to $i&gt ; i'$ ? is there any restriction that $\alpha\neq\beta$ ? could you provide details of the solution you have tried so far ? edit as you say in your comment , $$s_−s_+=a^\dagger_{p\beta}a_{p\alpha}a^\dagger_{q\alpha}a_{q\beta} . $$ anticommute the $a^\dagger_{p\beta}$ through $a_{p\alpha}a^\dagger_{q\alpha}$ . $$s_−s_+=a_{p\alpha}a^\dagger_{q\alpha}a^\dagger_{p\beta}a_{q\beta} . $$ now anticommute $a^\dagger_{q\alpha}$ through $a_{p\alpha}$ , but do not forget the possibility that $p=q$ . $$s_−s_+= ( \delta_{qp}-a^\dagger_{q\alpha}a_{p\alpha} ) a^\dagger_{p\beta}a_{q\beta} , $$ and you have got , $$s_−s_+=a^\dagger_{p\beta}a_{p\beta}-a^\dagger_{q\alpha}a_{p\alpha}a^\dagger_{p\beta}a_{q\beta} . $$ and you are done .
neither of those statements are true . it is an easy approximation to make : a neutron star has all of that ' space ' removed from between nucleons --- so we just need to know how big a neutron star of mass equal to the solar system would be . well , the only significant mass is the sun ( jupiter is about 1% the mass of the sun---negligible ) . if the sun were compressed into a neutron star , it would have a radius of about 10km ( up to 50% or so accuracy ) . see this nice talk about neutron star radii . solar system : so if you removed all of the ' space ' between all of the atoms in the solar systems , it would form an object about the size of a large town , or small city . universe : obviously collecting all of this mass would yield a black-hole . but conceptually , using some very order of magnitude estimates for the universe as a whole , if we assume there are roughly $10^{20}$ - $10^{22}$ stars ( i think this estimate is quite high ) , then the radius would be something like a 1-100 mpc or roughly 10 million to 1 billion light-years . edit ( to address the question itself ) : the concept of ' size ' for atoms and nuclei has some grey area , but you can define the size of a hydrogen atom , or the size of a proton/neutron to an order of magnitude . a statement like ' remove all of the empty space ' is much more nebulous , and ends up being largely a question of semantics . a more accurate way of phrasing the underlying concept being addressed might be something like : ' roughly how much volume do the dominant mass-constituents of matter take up ? ' the idea is that nucleons ( protons and/or neutrons ) are 2000 times more massive than electrons , and thus the important component of mass . at the same time , the electrons are the dominant volume-fillers ( by a factor of about $10^{15}$ ) .
it may not be the answer you are looking for but i recommend you get a thermos or a well insulated flask . these are what mountaineers use and you do not have to change the chemical composition of water this way .
for a geometrical argument , you are looking for basically what ron posted . but you can also argue this one mathematically : as you may know , the difference between two spacetime events is represented by a time difference $\delta t$ and a spatial difference $\delta x$ . under a lorentz boost , these quantities transform like this : $$\begin{align}c\delta t&#39 ; and = \gamma ( c\delta t - \beta\delta x ) \\ \delta x&#39 ; and = \gamma ( \delta x - \beta c\delta t ) \end{align}$$ now , the spacetime interval is $\delta s^2 = c^2\delta t^2 - \delta x^2$ . for a timelike interval , $\delta s^2 &gt ; 0$ , this means $c\delta t &gt ; \delta x$ , assuming that both differences are positive ( and you can always arrange for that to be the case ) . using the lorentz boost equations , you can see that in this case , $c\delta t&#39 ; $ has to be positive . so for two events separated by a timelike interval , if one observer ( in the unprimed reference frame ) sees event 2 later than event 1 , any other observer ( in the primed reference frame ) will also see event 2 later than event 1 . on the other hand , suppose you have a spacelike interval , $\delta s^2 &lt ; 0$ . in this case , $\delta x &gt ; c\delta t$ , so it is possible to get $c\delta t&#39 ; &lt ; 0$ for a specific velocity ( namely $\beta > \frac{c\delta t}{\delta x}$ ) . so if one observer ( in the unprimed reference frame ) sees event 2 later than event 1 , it is still possible for another observer ( in the primed reference frame ) to see them in the reverse order .
in the weak-field case , $$\mathrm{d}s^2 = -\left ( 1+2\frac{\phi}{c^2}\right ) c^2\mathrm{d}t^2 - \frac{4}{c}a_i\mathrm{d}t\mathrm{d}x^i + \left ( 1-2\frac{\phi}{c^2}\right ) \mathrm{d}s^2\text{ , }$$ where $\phi$ is the newtonian potential and $\mathrm{d}s^2 = \mathrm{d}x^2 + \mathrm{d}y^2 + \mathrm{d}z^2$ is the euclidean metric . in the static case , $a_i = 0$ , which is the form used for gps calculations , but in general it is more interesting as being a direct analogue to classical electromagnetism , first formulated for gravity by heaviside in 1893: $$\begin{eqnarray*}\mathbf{e}_\text{g} = -\nabla\phi - \frac{1}{2c}\frac{\partial\mathbf{a}}{\partial t} and \quad\quad and \mathbf{b}_\text{g} = \nabla\times\mathbf{a}\end{eqnarray*}$$ $$\begin{eqnarray*} \nabla\cdot\mathbf{e}_\text{g} = -4 \pi g \rho_\text{g} and \quad\quad and \nabla \times \mathbf{e}_\text{g} = -\frac{1}{2c}\frac{\partial\mathbf{b}_\text{g}}{\partial t} \\ \nabla\cdot\mathbf{b}_\text{g} = 0 and \quad\quad and \nabla\times\frac{1}{2}\mathbf{b}_\text{g} = -\frac{4\pi g}{c}\mathbf{j}_\text{g} + \frac{1}{c}\frac{\partial\mathbf{e}_\text{g}} {\partial t} \end{eqnarray*}$$ this particular version was taken from einstein 's general theory of relativity by grøn øyvind and sigbjørn hervik ; a few variations in defining these fields exist in the literature . but probably more importantly , the post-newtonian formalism gives a more general approximation scheme , the first few terms of which are : $$\mathrm{d}s^2 = - ( 1+2\phi+2\beta\phi^2+\ldots ) \mathrm{d}t^2 + ( 1-2\gamma\phi+\ldots ) \mathrm{d}s^2 + ( \ldots ) \mathrm{d}t\mathrm{d}x^i\text{ , }$$ with many other potentials that i am omitting here . this is very useful for understanding the general predictions of gtr and comparing them to alternative theories of gravity ( e . g . , gtr predicts $\beta = \gamma = 1$ , other theories might not ) .
when electron " orbits " nucleus it is trapped in potential barrier caused by nucleus : electron needs some energy $e_0$ to escape ( overcome ) that barrier ( $e_0$ is same as work function $\phi$ ) , when photon with frequency $f$ ( energy of that photon is $e=hf$ ) comes and hits electron , it gives it energy ( $e=hf$ ) , and if it is greater than $\phi$ then electron can escape the nucleus ( overcome the potential barrier ) : and it will have some kinetic energy ( $ke$ ) too . so in order to electron escape the nucleus photon which will hit it must have greater energy than $\phi$ ( it is same as greater frequency than $f_0$ ( where $f_0$ is minimum frequency for photon which will hit electron so it will escape the nucleus and you can calculate it using this equation $\phi = hf_0$ and $f_0=\frac{\phi}{h}$ ) ) . but when photon which will hit electron has same energy as minimum energy required for electron to escape the nucleus ( $\phi = e$ or $f=f_0$ ) then electron will just " go up " to the top of potential barrier and then it will " go down " back to the bottom of the potential barrier : and it wont be able to escape the nucleus . ( sorry for my poor drawings )
i think this is a typo in morse and feshbach methods of theoretical physics . the correct expression is $-\frac{1}{6}r^2\nabla ^2\psi$ or $-\frac{1}{6} ( dx^2+dy^2+dz^2 ) \nabla^2 \psi$ .
i agree with @pauljgans on that order/disorder is not a good analogy . moreover , the thermodynamic entropy of your room is the same when the room is ordered that when is not . in a first approximation the entropy of your room is $s = u/t$ with $u$ internal energy and $t$ temperature . entropy will change as $ds = du/t$ if you heat up your room from 15 ºc to 21 ºc . in a more general model you will be considering other changes in entropy due to flows of mass , chemical reactions , diffusion . . . yes , entropy is related to energy , but entropy is not a conserved quantity and its nonconservation is guided by the second law , which says that entropy production is non-negative .
i think you can get a estimate like this . for a semiconductor with no split in the quasi-fermi levels , the electrons and holes take their intrinsic values ( carrier density ) $n_0$ and $p_0$ ( $cm^{-3}$ ) . the charge carriers are in equilibrium with the thermal photons being absorbed and emitted inside the material . so if we calculate the emission rate of thermal photons then we know the time constant for how long the thermally generated carriers will last before recombining ( because at equilibrium upwards rates and downward rates must balance ) . let 's assume perfect bimolecular recombination , then the rate of thermal emission is , $\frac{\partial n}{\partial t} = bn_0p_0$ , where $b$ is the bimolecular recombination coefficient , for gaas , $b=7\times10^{-10}$$cm^{6}s^{-1}$ , and the intrinsic carrier density is , $n_i=n_0=p_0=2\times10^{6}$$cm^{-3}$ . this gives a transition rate of 2800 $s^{-1}$ . this seems a bit slow . but it is correct for the assumptions , namely because we assumed an un-doped intrinsic semicondutor ( the carrier density is very low ) . for more information i recommend ' light-emitting diodes by e . fred schubert’ , search for the vanroobroeck-shockley equation .
the explicit eigenfunctions of the harmonic oscillator hamiltonian are given here , but i would highly discourage you from explicitly doing an integral using these expressions to determine $a$ . it is significantly easier to use the fact that the eigenfunctions are orthogonal ; $$ \int_{-\infty}^\infty dx\ , \phi_m^* ( x ) \phi_n ( x ) = \delta_{mn} $$ if you use this fact , then the integral on the left hand side of the $t=0$ normalization condition you wrote down will be very easy . try this out , and if you still have trouble we can give you more guidance since this is a homework question .
without friction , the forces during the collision ( glancing or head-on ) are applied exclusively through their centres of mass . ( illustration available on wikipedia . ) the torque is given by $\tau=\mathbf r \times \mathbf f$ - but if the forces are applied through the centre of mass , then $\mathbf r$ and $\mathbf f$ are parallel , and hence $\tau=0$ . without a torque , angular momentum cannot change ( because $\frac{\text{d}l}{\text{d}t}=\tau$ ) , so that each ball will keep its angular momentum . with friction , depending on the relative movement of the balls ' surfaces during the collision , there could be a tangential component of the force , which would cause a torque on each ball . therefore , angular momentum could be transferred . however , as joshphysics mentioned in a comment , the total angular momentum of the system would still be conserved , as there is no external net torque .
the attraction does happen at all temperature , but it is negligible if the temperature is too high . so the electrons do attract each other , but the thermal fluctuations do not allow for cooper pairs to be stable . to give an heuristic example : imagine a lot of hydrogen atoms . the electrons are bound to their protons at zero temperature . if now you put the hydrogen atoms in a medium with a temperature high compare to 13ev ( the binding energy ) , the electrons can take some energy from environment to leave their protons , and you do not have hydrogen atoms anymore , but a plasma ( free protons and free electrons , interacting without forming bound states ) . the same thing happens with cooper pairs : the temperature needs to be small enough to allow the physics to be dominated by these very weakly bounded pairs .
i think that you are confused . when you rotate something by 360 degrees , you will not change the direction in space of anything . you will only change the wave function to minus itself - if there is an odd number of fermions in the object ( which is usually hard to count for large objects ) . if you have electrons with spins pointing up and you rotate them around the vertical axis by any angle , whether it is 360 degrees or anything else , you will still get electrons with spin pointing up . this is about common sense - many spins with spin up give you a totally normal , " classical " angular momentum that can be seen and measured in many ways . the flip of the sign of the wave function can not be observed by itself because it is a change of phase and all observable probabilities only depend on the density matrix $\rho=|\psi\rangle \langle\psi|$ in which the phase ( or minus sign ) cancels . the phase - or minus sign - has nothing to do with directions in space . it is just a number . in particular , it is incorrect to imagine that complex numbers are " vectors " , especially if it leads you to think that they are related to directions in spacetime . they are not . you would have to prepare an interference experiment of an object that has not rotated with the " same " object rotated by 360 degrees - and it is hard for macroscopic objects because the " same " object quickly decoheres and you must know whether it has rotated or not , so no superpositions can be produced . ; - ) however , all detailed measurements of the spin with respect to any axis indirectly prove that the fermions transform as the fundamental representation of $su ( 2 ) $ . in particular , if you create a spin-up electron and measure whether it is spin is up with respect to another axis tilted by angle $\alpha$ , the probability will be $\cos^2 ( \alpha/2 ) $ . the only sensible way to obtain it from the amplitude is that the amplitude goes like $\cos ( \alpha/2 ) $ and indeed , this function equals $-1$ for $\alpha$ equal to 360 degrees .
the spit horizon in a rindler wedge occurs at a distance $d~=~c^2/g$ for the acceleration $g$ . in spatial coordinates this particle horizon occurs at the distance $d$ behind the accelerated frame . clearly if $d~=~0$ the acceleration is infinite , or better put indefinite or divergent . however , we can think of this as approximating the near horizon frame of an accelerated observer above a black hole . the closest one can get without hitting the horizon is within a planck unit of length . so the acceleration required for $d~=~\ell_p$ $=~\sqrt{g\hbar/c^2}$ is $g~=~c^2/\ell_p$ which gives $g~=~5.6\times 10^{53}cm/s^2$ . that is absolutely enormous . the general rule is that unruh radiation has about $1k$ for each $10^{21}cm/s^2$ of acceleration . so this accelerated frame would detect an unruh radiation at $\sim~10^{31}k$ . this is about an order of magnitude larger than the hagedorn temperature . we should then use the string length instead of the planck length $4\pi\sqrt{\alpha’}$ and the maximum acceleration will correspond to the hagedorn temperature .
assuming you are using c code : this implements a simple integration of the equations of motion : $$v = \frac{dx}{dt}\\ a = \frac{dv}{dt}$$ i made the time step very small : you can get away with bigger steps ( takes less time ) . also i keep the loop going until the velocity has reached a certain downward value : you can use any other criteria ( time , position , etc ) . update as johannes pointed out , you get slightly more accurate results ( even when you use larger time steps ) when you use the average velocity during a time step to compute the next position . this leads to a small change in code : . below , i show both the old and the new calculation side by side - as you can see , when the time step is coarser , the new method continues to give good results : here are the results at time = 1.80 seconds for different time steps : finally , the " correct " result from integrating the equations of motion gives velocity = 8 - 1.8*9.8 = -9.64 position = 0 + 8 * 1.8 - 0.5 * 9.8 * 1.8^2 = -1.4760  as you can see , smaller steps get better results , and using the " new method " ( as suggested by johannes ) gets you closer to the true position than the " lazy method " i first proposed . last update to implement other forces , you can do something like this : i hope you can see how you could implement lots of different things that might change the total force on the object and that will change the motion . in this case , the last term ( onsupport ) sets both velocity and acceleration to zero , meaning that after a small time step it will still be zero - but if you set onSupport = 0 it will once again be able to move . and like this you could have multiple objects doing different things under the influence of gravity , rocket packs , etc . you obviously want to experiment with the drag=200 factor - i just picked a number from thin air , and with a value like 200 you will end up with a terminal velocity of about 3.5 m/s ( you would survive that no problem ) . as you make drag lower , the terminal velocity will be higher ( also depends on the mass of the object ) .
the short answer is that the two principal value definitions agree on sufficiently well-behaved functions , but may disagree on sufficiently singular functions . for instance , on one hand $$\lim_{\epsilon\searrow 0} \int_{\mathbb{r}\backslash [ -\epsilon , \epsilon ] } \frac{\mathrm{d}x}{x^3}~=~0$$ is zero , while on the other hand $$\lim_{\epsilon\searrow 0} \int_{\mathbb{r}} \frac{\mathrm{d}x}{x ( x^2+\epsilon^2 ) }$$ is not well-defined , since the integrand is not integrable at $x=0$ . i ) here we would like to investigate further the definition of principal value $p\int\ ! \mathrm{d} x$ . definition . let $\chi= ( \chi_{\epsilon} ) _{\epsilon&gt ; 0}$ be a family of functions $\chi_{\epsilon}:\mathbb{r}\to [ 0,1 ] \subseteq \mathbb{r}$ that are : even functions $\chi_{\epsilon} ( x ) ~=~\chi_{\epsilon} ( -x ) , $ lebesgue measurable functions , $\chi_{\epsilon} ( x ) \nearrow 1$ pointwise almost everywhere for $\epsilon \searrow 0$ . let us refer to such a function $\chi_{\epsilon}$ as a kernel function . examples of kernel functions $\chi_{\epsilon}$ are for instance : the characteristic function $$\chi_{\epsilon} ( x ) ~=~\chi^{\rm std}_{\epsilon} ( x ) ~:=~ 1_{\mathbb{r}\backslash [ -\epsilon , \epsilon ] } ( x ) $$ for the set $\mathbb{r}\backslash [ -\epsilon , \epsilon ] $ . ( this choice $\chi^{\rm std}$ will lead to the standard definition of principal value . ) the continuous function $$\chi_{\epsilon} ( x ) ~=~ \chi^{a , b}_{\epsilon} ( x ) ~:=~ \frac{|x|^a}{|x|^a+ \epsilon^b} , $$ where $a , b&gt ; 0$ are two positive constants . ( the choice $\chi^{2,2}$ will lead to the other definition of principal value mentioned by op . ) the constant unit function $\chi_{\epsilon} ( x ) ~=~ 1$ . ( unsurprisingly , this latter choice will turn out to be not so useful . ) ii ) definition . define the set $v ( \chi ) $ of $\chi$- admissible functions as $$v ( \chi ) ~:=~\left\{ f : \mathbb{r} \to \mathbb{c} ~\left|~ \begin{array}{c} f~\text{is lebesgue measurable} , \cr \forall \epsilon&gt ; 0~:~~ \chi_{\epsilon} f~\in~ {\cal l}^1 ( \mathbb{r} ) , \cr \text{and} \cr \left ( \int\ ! \mathrm{d}x~ \chi_{\epsilon} ( x ) f ( x ) \right ) _{\epsilon&gt ; 0} \text{is convergent for}~ \epsilon\searrow 0 \end{array}\right . \right\} . $$ definition . if a function $f\in v ( \chi ) $ is $\chi$-admissible , we define the $\chi$- based principal value as $$p ( \chi ) \int\ ! \mathrm{d} x f ( x ) ~:=~\lim_{\epsilon\searrow 0} \int\ ! \mathrm{d}x~ \chi_{\epsilon} ( x ) f ( x ) . $$ here ${\cal l}^1 ( \mathbb{r} ) $ denotes the set of functions that are lebesgue integrable , i.e. functions that are lebesgue measurable and whose absolute value has a finite integral . ${\cal l}^1 ( \mathbb{r} ) $ is an example of an ${\cal l}^p$ space . iii ) it is not hard to see that : if $f\in{\cal l}^1 ( \mathbb{r} ) $ is lebesgue integrable , then it is $\chi$-admissible $f\in v ( \chi ) $ , and the principal value $$p ( \chi ) \int\ ! \mathrm{d} x ~f ( x ) ~=~ \int\ ! \mathrm{d} x ~f ( x ) $$ is just the ordinary lebesgue integral because of the lebesgue dominated convergence theorem . the set $v ( \chi ) $ of $\chi$-admissible functions is a $\mathbb{c}$-vector space . if a function $f\in v ( \chi ) $ is $\chi$-admissible , so is the mirrored function $ ( x\mapsto f ( -x ) ) \in v ( \chi ) $ , with same principal value . if an $\chi$-admissible function $f\in v ( \chi ) $ is odd , then $p ( \chi ) \int\ ! \mathrm{d} x~f ( x ) ~=~ 0$ . thus it is enough to investigate even and odd functions . finally , let us investigate power functions $x\mapsto x^p$ , $p\in\mathbb{r}$ , which play an important role in practice as building blocks . iv ) even functions . let $$g_{p , k} ( x ) ~:=~ 1_{ [ -k , k ] } ( x ) |x|^p~=~g_{p , k} ( -x ) $$ be a truncated power function , where $p\in\mathbb{r}$ is a real power , and where $k&gt ; 0$ is a positive truncation constant . it is not hard to show that in the case of example 1 , 2 , or 3 , $$g_{p , k}\in v ( \chi ) \qquad \leftrightarrow \qquad p&gt ; -1\qquad \leftrightarrow \qquad g_{p , k}\in {\cal l}^1 ( \mathbb{r} ) . $$ in the affirmative case $p&gt ; -1$ , the principal value definitions based on the three examples 1 , 2 , and 3 agree : $$p ( \chi ) \int\ ! \mathrm{d} x ~g_{p , k}~=~\int\ ! \mathrm{d} x ~g_{p , k}~=~ \frac{2k^{p+1}}{p+1} . $$ v ) odd functions . let $$h_{p , k} ( x ) ~:=~ {\rm sgn} ( x ) 1_{ [ -k , k ] } ( x ) |x|^p~=~-h_{p , k} ( -x ) $$ be a truncated power function , where $p\in\mathbb{r}$ is a real power , and where $k&gt ; 0$ is a positive truncation constant . in the three examples 1 , 2 , and 3 , we get $h_{p , k}\in v ( \chi^{\rm std} ) $ always , $h_{p , k}\in v ( \chi^{a , b} ) \qquad \leftrightarrow \qquad p+a&gt ; -1$ , $h_{p , k}\in v ( 1 ) \qquad \leftrightarrow \qquad p&gt ; -1\qquad \leftrightarrow \qquad h_{p , k}\in {\cal l}^1 ( \mathbb{r} ) . $
( i try to answer to my own question , after some reflections made with the help of luboš . ) for an incompressible and irrotational flow , the conditions $\nabla\times \boldsymbol u=\boldsymbol 0$ and $\nabla\cdot \boldsymbol u = 0$ imply , $\nabla^2\boldsymbol u =\boldsymbol 0$ . indeed : $$\nabla^2\boldsymbol u = \nabla ( \nabla\cdot\boldsymbol u ) -\nabla\times ( \nabla\times \boldsymbol u ) = \boldsymbol 0$$ this forces us to write down the navier-stokes equation for the motion of the fluid without the viscous term $\mu\nabla^2\boldsymbol u$ , no matter the viscosity : $$ \rho ( \partial_t \boldsymbol u + u\cdot\nabla \boldsymbol u ) = -\nabla p \ \ \ , \ \ \ \nabla\cdot \boldsymbol u = 0$$ now , it could seem that this implies the flow is automatically a high-reynolds number flow ( for which we could have wrote down the same equation , but for a different reason : $\mu=\rho\nu\simeq 0$ , and this would have been an approximation ) . but , even if the viscosity is far from neglectable , we can make another kind of approximation , saying that the inertia , represented by the left-hand-side terms in n-s equation , can be neglected because of $re=ul\rho/\mu\ll 1$ ( this can happen in a lot of situations : microobjects , extra-slow flows , and - of course - high viscosity . in this case , the equations of motion become : $$-\nabla p = \boldsymbol 0\ \ \ , \ \ \ \nabla\cdot \boldsymbol u = 0$$ which are , in fact , the equations we would arrive at if we started by the stokes equation ( for inertia-less flows ) for irrotational flows . then , an irrotational flow is not necessarily governed by the euler equation , i.e. , it is not necessarily inviscid .
the entropy of the ( density ) matrix $a$ , usually denoted $\rho$ , is evaluated as the supremum ( i.e. . maximum that may never be realized , just arbitrarily closely approached ) of the trace of the product of matrices $ah$ minus the natural logarithm of $\exp ( h ) $ , the exponential of $h$ . the supremum is taken over all hermitean matrices $h$ of the same size as $a$ . in practice , when you try to maximize this expression , you will find out that the best choice is $$ h = \ln ( a ) + c\cdot {\bf 1} $$ in words , the ideal matrix $h$ that gives you the supremum is the logarithm of the matrix $a$ ( there should be a minus sign somewhere to get the right conventions for entropy but i will overlook this detail to agree with the literature below ) . the choice of $c$ does not matter because the piece proportional to the unit matrix gets subtracted . see a proof of this formula e.g. as theorem 2.13 in http://www.mathphys.org/azschool/material/az09-carlen.pdf which also provides you with some background , as much as you need .
that is exactly right . a fundamental tenet of physics is that all inertial reference frames are equivalent and indistinguishable . 1 furthermore , given one inertial frame ( standing at rest 2 ) , any other frame moving with respect to it with a constant velocity is also inertial . the frame " moving at terminal velocity " is just as inertial as " sitting still " and so you would not even be able to tell you were moving . by definition you feel no acceleration at constant velocity . thus the acceleration due to gravity must be exactly balanced by some other force . by construction that force is not air resistance for you ( as would be the case of a sky diver at terminal velocity ) but simply the normal force of the elevator floor , which would make the experience feel exactly like standing in a non-moving elevator in the same gravitational field . 1 at least locally , meaning that any experimental apparatus and things you measure are confined to objects also in that frame . 2 to be pedantic , standing " still " in a gravitational field is considered inertial in newtonian mechanics but not general relativity . i am speaking in newtonian terms here , but the conclusion would be just the same if analyzed with the machinery of gr .
being bulk neutral neutrons participate only weakly in electromagnetic interactions which is the dominate interaction for charged particles . instead neutron scattering can be thought of as primarily a contact interaction with the nuclei of atoms in the way . light atoms ( and hydrogen in particular ) have a larger cross-sectional area per nucleon than heavy ones take up more of the energy of the interaction in recoil than heavy ones making them much more effective at reducing the kinetic energy of non-thermal neutrons per unit areal mass density . historically waxes , water and plastics have been the neutron shielding materials of choice , though concrete or rammed earth are cheap and not too bad . once down to thermal energies neutrons get as much kinetic energy as they lose on average and you just have to wait for them to decay or capture . doping your absorber with boron , chlorine or even gadolinium will help to capture the thermalized neutrons faster . pvc gets you the chlorine for free in your plastic , and boron can be added easily to concrete or to a number of plastics . it should not be overlooked that it takes a lot of space to slow , thermalize and capture neutrons ( that contact interaction thing means they go through more material before interacting than charged particles ) ; especially if you need to get them all . they are notorious for penetrating large quantities of shielding , and distance is one of your best friends when it comes to neutron shielding .
i will not derive this exact formula for you , but instead show you the derivation for a single solid ( following schroeder 's ' introduction to thermal physics ' , where i learned it ) . the generalization to two solids is then very simple ( mostly a conceptual difference ; the math hardly changes ) . consider an einstein solid with $n$ oscillators and $q$ units of energy . the problem of calculating the multiplicity can be visualized as follows given $n-1$ vertical lines - representing partitions between different oscillators - and $q$ dots - representing units of energy - how many ways are there of arranging these symbols ? this problem is quite simple : there are a total of $q+n-1$ symbols , and one just has to pick the location of the $q$ dots ( or , equivalently , the $n-1$ lines ) to uniquely specify an arrangement . thus , there must be $$ \omega=\binom{n+q-1}{q}$$ different arrangements corresponding to the same macrostate . i will continue to consider a single solid , which is qualitatively the same thing . one considers a solid , about which is only known that it consists of $n$ oscillators , sharing $q$ units of energy . this specifies the macrostate . given that the system is in some macrostate , the entropy is $$s=k_b\ln \omega$$ where $\omega$ is the number of microstates corresponding to that particular macrostate . about long term and short term entropy : consider a system like the one you are asking about - where two subsystems share a fixed amount of energy . then , one can say that on any practical timescales , the total system will be in the entropy- ( and therefore multiplicity- ) maximizing configuration . denoting the corresponding multiplicity by $\omega_1$ , we can define the ' short term entropy ' as $s=k_b\ln\omega_1$ . this is really the most practically useful concept of entropy , and therefore the prefix ' short term ' is often dropped . however , if we leave the system for extremely long times , one can say that , since all microstates are probed , we eventually lose any knowledge of the state of the system - it could be in any state . therefore , one can define the ' long term entropy ' , which is defined as $s_\text{long}=k_b\ln\left ( \sum_{\text{macrostates}}\omega\right ) $ , i.e. one uses the total multiplicity , which sums over all possible states .
two books to get you started in general computational radiation transport : computational methods of neutron transport , written by e.e. lewis , edited by w.f. , jr . miller , isbn 0-89448-452-4 monte carlo particle transport methods : neutron and photon calculations , written by ivan lux and laszlo koblinger , isbn 0-8493-6074-9 the supporting materials for the mcnp ( monte carlo n-particle ) transport code maintained by los alamos national laboratory may also be useful : http://mcnp.lanl.gov/ these references are not targeted at nuclear rockets , instead they focus on the basic principles of computational radiation transport . application to nuclear rockets , or any other particular system , is just a matter of constructing the right set of assumptions and constraints to answer your specific question .
welcome to the community wizzphiz . you do not state your age in your profile , but i would think &lt ; 20 ? there are no spontaneously physical electron positron pairs created from the vacuum . the reason is called " conservation of energy " it would take energy to create such a pair . the vacuum sea consists of " virtual particles " and the electrons going around the accelerator cannot " see " them as in addition to being virtual they are created and annihilated in a small delta ( time ) . in this link , which is a festschrift for a scientist , in paragraphs 2 and later there are a lot of explorations of electrons scattering off radiation , even very low black body radiation that exists in a vacuum , but these are not the dirac sea pairs you are asking about . the vacuum pairs do have experimental signatures in the casimir effect , and in the widening of the lamb shift . when one goes to general relativity the vacuum particles theoretically may become physical in accelerated systems but there is no solid experimental evidence of this . the beam energies in the accelerators we have are not in that ball park of acceleration .
there is absolutely no danger from the electromagnetic waves coming from the phone . they are nowhere near powerful enough to heat up the gasoline , much less cause it to explode . they also do not produce more static electricity than anything else . if there is any part of the phone that is hypothetically " dangerous " , it might be the battery , as you say . if the battery happens to fall out , and land precisely ( terminals-first ) onto a conductive surface , it can create a tiny spark which can ignite the vapors . there is much more danger from the static electricity from your own body when handling the gas nozzle . the best practice should be to lay one hand onto your car , then pick up the nozzle with the other hand and insert it into your tank . ( the same when removing the nozzle )
if i had to give a one sentence answer to this question , it would be as follows : *that the phase and amplitude alone on one plane is enough to wholly define a three-dimensional light field arises from the various uniqueness theorems for maxwell 's equations within a connected volume given the solution on the volume 's boundary ; otherwise put : once you know a solution on a boundary , then the values within must follow from " reasonable " physical assumptions . for simplicity let 's sit with the scalar diffraction theory , so now we are essentially talking about uniqueness theorems for the helmholtz equation $ ( \nabla^2 + k^2 ) \psi = 0$ . uniqueness theorems when $k^2 &gt ; 0$ or when $k^2 \in \mathbb{c}-\mathbb{r}$ are much more complicated than when $k^2\leq0$ . the latter case corresponds to static solutions of the klein gordon equation or to static solutions of the maxwell equations with or without an assumption of a massive photon ; see my answer here for more details . such cases have very strong uniqueness theorems : once a solution 's values are set on a compact volume 's boundary , there is only one possible solution within the volume . this situation even extends to semi-infinite volumes . however the former situation includes $k^2&gt ; 0$ , the case for scalar diffraction in freespace or a lossless dielectric : uniqueness theorems need further strong assumptions about the field to make them work . thankfully , some of these assumptions are reasonable physically . we can restore simplicity to the solutions of the freespace helmholtz equation ( i.e. to the situation we have with a hologram ) by making reasonable physical assumptions such as the sommerfeld radiation condition or that the field is a tempered distribution ; for more information on the latter condition , see my answers here or here . given these assumptions , together with the assumption that the field is propagating purely left-to-right , we can reconstruct a field from the hologram as follows . you begin with the helmholtz equation in a homogeneous medium $ ( \nabla^2 + k^2 ) \psi = 0$ . if the field comprises only plane waves in the positive $z$ direction then we can represent the diffraction of any scalar field on any transverse ( of the form $z=c$ ) plane by : $$\begin{array}{lcl}\psi ( x , y , z ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \left [ \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) \ , \psi ( k_x , k_y ) \right ] {\rm d} k_x {\rm d} k_y\\ \psi ( k_x , k_y ) and = and \frac{1}{2\pi}\int_{\mathbb{r}^2} \exp\left ( -i \left ( k_x u + k_y v\right ) \right ) \ , \psi ( x , y , 0 ) \ , {\rm d} u\ , {\rm d} v\end{array}$$ to understand this , let 's put carefully into words the algorithmic steps encoded in these two equations : take the fourier transform of the scalar field over a transverse plane to express it as a superposition of scalar plane waves $\psi_{k_x , k_y} ( x , y , 0 ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) $ with superposition weights $\psi ( k_x , k_y ) $ ; note that plane waves propagating in the $+z$ direction fulfilling the helmholtz equation vary as $\psi_{k_x , k_y} ( x , y , z ) = \exp\left ( i \left ( k_x x + k_y y\right ) \right ) \exp\left ( i \left ( k-\sqrt{k^2 - k_x^2-k_y^2}\right ) z\right ) $ ; propagate each such plane wave from the $z=0$ plane to the general $z$ plane using the plane wave solution noted in step 2 ; inverse fourier transform the propagated waves to reassemble the field at the general $z$ plane . if you can understand these steps you should be other see how the solution to helmholtz 's equation , i.e. the full three-dimensional scalar light field , is reconstructed from its values on a plane . the latter of course is what a phase and intensity mask hologram encodes . what hinders holography ? i am not up with the latest hologram production techniques , but essentially making a hologram is a kind of interferometry and as such calls for low vibration and building of an interferogram between transmitted and reference light . one can not simply " snap " a hologram like one can with a digital camera ( or even with an older style film camera ) . moreover , the phase masking needed to make the equations above work is highly colour-dependent , so that any kind of colour holography is even more restrictive than the making of one-colour holograms . the holography wiki page gives you a good overview ; the " rainbow " holographic technique is the nearest i know of to colour holography . aside from this technique , most holograms need high coherence in the light source for reconstruction . another interesting technique is the manipulation of light by computer generated holography , where one computes by solving maxwell 's equation the phase and amplitude mask needed for e.g. nulling out the mean aberration from a lens before analysis by an interferometer .
if want to describe the dynamics of the ball , you need to use the so ( 3 ) matrix which describes the ball 's orientation . this is a 3 by 3 matrix whose transpose is its inverse . these may be parametrized by euler angles , and most of the literature on rigid rotating bodies uses this convention , but i think it is best just to use the matrix entries themselves because euler angles are hard to work with . the orientation matrix itself has a velocity vector which is described by an antisymmetric matrix . the center of mass motion of the ball , ignoring air , will be a straight parabolic arc in the direction of the original motion , and the spin has no effect . the ball will not go off center just by spinning it , unless it collides with some part of the machine going out . there is asymmetric air drag on the ball when it is rotating , which will lead the ball to arc its trajectory a little bit , this is a small effect . you should not use spin to do this , because the main effect of the spin will be on the bounce when landing , which is going to be impossibly wild given the enormous spin you intend to put on the ball . human servers can not put too much spin on the ball , because they are limited by the racket design . you are better off just aiming the ball in a different direction for different serves , and if you want to disguise the direction , do it with a reflector plate which the ball bounces off . you should match to experiment , not theory , because it will be easier to fit the experimental data with a curve than to predict it from mechanics . given your current spin design , you will make inhuman and unanswerable serves . reflection of spinning tennis balls since you keep asking for this , i worked it out . it is somewhat interesting , because there are several issues which one has to keep straight . most of the answer is determined independent of tennis-ball details , but one quantity , the change in rotation in the plane of reflection , is impossible to predict well . consider a tennis ball impacting the z=0 plane , travelling freely with velocity $v_x , v_z$ , and with rotation vector $\omega_x$ , $\omega_y$ , $\omega_z$ . this is the general case , since you choose the z axis perpendicular to the wall , and the x axis parallel to the velocity in the plane of the wall . at the moment of impact , the friction force of the ball will quickly and irreversibly enforce the no-slip condition , before any significant deformation of the ball . the reason is that the total impulse ( momentum transfer ) from the impact is about $2mv_x$ , so that the available friction impulse is of the order of $2\mu mv_x$ , where $\mu$ is the coefficient of friction , which is of order 1 , and this is much bigger than the impulse required to enforce no slip or a reasonable range of $\omega$ , say 10-500 . no-slip-on-contact means that the friction impulse imparted to the ball in the x-y plane $p_x , p_y$ must satisfy the following : $$ v_x + {p_x\over m} = r\omega_y - {\alpha p_x\over m}$$ $$ {p_y\over m} = r\omega_x - {\alpha p_y\over m}$$ these conditions enforce that the velocity and the angular velocity after the impulse are those required for no-slip . this gives the impulse . the constant $\alpha$ is the coefficient of the moment of inertia , $$ i = {mr^2\over \alpha}$$ for a shell , like a tennis ball , $\alpha=3$ . these conditions determine the outgoing velocity in the x , y directions and the outgoing angular velocity in the x , y directions . $$\delta v_x = {r\omega_y - v_x\over 1+\alpha}$$ $$\delta v_y = {r\omega_x \over 1+\alpha}$$ the z direction is executing a reflection independent of the x and y , because once no-slip is established , the elastic process happens as it would for a non-rotating ball . the final z velocity is $\kappa v_z$ in the opposite direction , so that $$\delta v_z = - ( 1+\kappa ) v_z$$ where $\kappa$ is a phenomenological bounce-loss parameter , for a tennis ball , i would guess about . 8 ( from bouncing tennis balls , they go back to about 64% of their original height each bounce ) . the final values of $\omega_x$ and $\omega_y$ are determined by no-slip $$\omega_y^f = {v_x\over r}$$ $$\omega_x^f = {v_y\over r}$$ the undetermined quantity is the final value of $\omega_z$ , the rotation in the plane of the impact wall . this rotation is reduced by the friction force as the ball elastically deforms , and bounces off . i will write this as $$\omega_z^f = q ( v_z , \mu ) \omega_z$$ where $q ( v_z , \mu ) $ is a phenomenological function which you need to parametrize . the friction torque is reduced by the impact area from the friction force , and this is a factor of maybe 1% for a ball at 60 m/s , but the total friction impact available is $2\mu mv_z$ , which is about 100 times the amount needed to stop a reasonable rotation . so these are comparable , and it is not easy to predict how the impact will affect this component of rotation , except that it will reduce it , perhaps by as little as 1% , perhaps as much as 90% per bounce . this needs to be measured for a real tennis ball at real impact speeds .
quantum field theory is the framework that we normally use to study particle physics . it is the idea that the world is described by fields and each field can move into excited states which correspond to particles . since we are working with fields , you not constricted to a fixed particle number as in quantum mechanics . this is very useful since due to $e=mc^2$ , particles constantly appear and disappear in the real world . particle physics on the other hand , is the study of the particles that make up our world . since qft 's are terrific at describing nature , we use them to describe particle physics ; they are tool we need to study particle physics . however , some people study qft for the sake of qft since its a very deep and subtle subject .
it is easiest to directly derive the form of the vector fields from the boundary conditions for asymptotically flat spacetimes . see for example this paper http://arxiv.org/abs/1001.1541 or this one http://arxiv.org/abs/1106.0213 infinitesimally diffeomorphisms act on the metric via lie derivative $\mathcal{l}_{\xi}g_{\mu \nu}$ . in the case of bms , you require this transformation to respect the boundary conditions on $g_{\mu \nu}$ , for example $g_{uu}\approx -1 +\mathcal{o} ( r^{-1} ) $ . so the vector field should satisfy $\nabla_u \xi_u =\mathcal{o} ( r^{-1} ) $ . you may set up such equations for each component of the metric and its corresponding boundary conditions . in each case you require the vector field not to touch the " leading " minkowski part of the metric . this is a system of differential equations , which you may then solve . as far as the generators go , this is sort of addressed in the second paper i linked to . though as far as eqn 3.3 goes , you may heuristically think of this just as the adm hamiltonian weighted by a function on the sphere which turns the uniform time translation into a supertranslation ( the spaces andy is studying are christodoulou-klainerman spaces for which $i^0$ is a non-singular point and so you may hope to match $\mathcal{j}^+_-$ to $\mathcal{j}^-_+$ through $i^0$ where the adm hamiltonian is defined ) . demonstrating that these charges generate the correct transformations quantum mechanically is actually very subtle , as is discussed here http://arxiv.org/abs/1401.7026 hope this helps .
for 1d potentials , the sequence of bound state energy eigenvalues $e_n$ cannot grow faster than what happens in the case of an infinite well , i.e. $e_n$ cannot grow faster than $n^2$ .
short answer : this is just feynmann parametrization , so you could demonstrate the formula by recurrence .
ah , this gives me a chance to give a proper home to an analysis i first posted on reddit . ( i would much rather have first posted it here :-p ) mathematical derivation it all starts with a blog post i have written that comes very close to addressing the exact question you are asking . in the post , i calculated how fast an object would be moving after falling a given distance , assuming quadratic drag . but one of the formulas i used to get to that result is the time it takes for an object to fall a certain distance . here 's the argument from my post . if you write out newton 's second law for an object falling through the air , you get $$\frac{1}{2}ca\rho \dot{y}^2 - mg = m\ddot{y}$$ i.e. drag force minus gravitational force equals mass times acceleration . in this equation , $m$ is the object 's mass , $a$ is the cross-sectional area it presents , $c$ is the object 's drag coefficient , $\rho$ is the density of the fluid it is falling through , $g$ is the acceleration of gravity , and $y$ is its height at any given moment . solving this equation for $\dot{y}$ gives $$\dot{y} = -\sqrt{\frac{2mg}{ca\rho}}\tanh\biggl ( \sqrt{\frac{gca\rho}{2m}}t\biggr ) $$ you can then integrate this with respect to time and solve it for $t$ to get $$t = \sqrt{\frac{2m}{gca\rho}}\cosh^{-1}\exp\biggl [ \frac{ca\rho}{2m} ( h - y ) \biggr ] $$ a couple more steps are shown in my blog post , but they are not really important . the point is that this formula gives the time $t$ it takes for an object to fall a distance $h - y$ . you will notice that the properties of the falling object occur in this formula only as part of the particular combination $\frac{ca\rho}{2m}$ . so the behavior of a falling object can be entirely characterized by that ratio . if you call this ratio $r$ , then the formula becomes $$t = \sqrt{\frac{1}{rg}}\cosh^{-1}\exp [ r ( h-y ) ] $$ for a few sample values of $h - y$ , here 's what this looks like as a function of $r$: you will notice that the time to fall a given distance always increases with increasing values of $r$ . so the larger an object 's value of $\frac{ca\rho}{2m}$ , the longer it takes to fall . conversely , an object with a smaller ratio of cross-sectional area to mass ( i.e. . smaller $r$ , assuming the same shape ) will fall faster . now , roughly speaking , a fat person tends to be larger than a skinny person in all three dimensions . so their mass will roughly be bigger by a factor of $k^3$ for some $k$ , whereas their cross-sectional area will only be bigger by $k^2$ . ( this is a huge approximation , of course , but it should still work for the question of " faster " vs . " slower . " ) accordingly , $r$ for a fat person will be smaller ( by a factor of $k$ ) , which means they take less time to fall . physical interpretation that is all well and good , but just going through the math does not necessarily make it clear why ( physically ) fat people fall faster . the crux of the explanation is in that last paragraph : a fat person has a larger mass in proportion to their surface area . since the drag force is proportional to area , but weight is proportional to mass , as a person gets fatter , the weight ( going down ) increases more than the drag force ( going up ) , which means the person accelerates more . more math : slowdown factor now what about this " slowdown factor " that wolfram alpha is coming up with ? if you look down toward the bottom of the results , it tells you that the slowdown factor is just the ratio of the time actually taken to fall , which i showed you how to calculate above , to the time that would be taken without air resistance . you can get the latter time by setting $c$ , $a$ , or $\rho$ to zero , or taking the limit as $m\to\infty$ . ( does it make sense why all of these assignments correspond to making the effect of the air insignificant ? ) or , of course , you could just as well take $r\to 0$ . now , before you start wondering how you are going to get away taking the square root of $\frac{1}{0}$ , you actually need to take a limit , and the limit of $t$ as $r\to 0$ is well defined : $$t_0 = \lim_{r\to 0}\sqrt{\frac{1}{rg}}\cosh^{-1}\exp [ r ( h-y ) ] = \sqrt{\frac{2 ( h - y ) }{g}}$$ the formula for the slowdown factor is then $$\text{slowdown factor} = \frac{t}{t_0} = \sqrt{\frac{1}{2r ( h - y ) }}\cosh^{-1}\exp [ r ( h-y ) ] $$ this depends only on the product of the " drag ratio " $r$ and the height fallen $h - y$ . essentially it is a way of characterizing how much the presence of air resistance affects the time of flight .
for all intents and purposes , you can use an incompressible equation of state : $$ v = constant $$ that is it . no matter what pressure and temperature , you have the same volume . it is not completely true , but in relation to gasses it is true enough to make it that pressure work is negligible in liquids compared to gasses , and for liquids , you can just deal with the heat content without considering any work done in the expansions and contractions required to change temperature .
the potentiometer is not a normal resistor . they are often used simply as variable resistors , but in this example its use does its name justice -- " potentiometer " = voltage measure . the float is attached to a type of slider which contacts the resistor at some point in the middle . the result is that you can treat the potentiometer as two resistors ( i will call them r1 on top and r2 on bottom ) in series , where r1 and r2 always add up to the same total resistance , and therefore where the potential drop across r1 and r2 always add up to the voltage of the source on the left . as an example , if the float is at the top , you might find that r1 = 0 and r2 = 1000 ohms . if the voltage source is 5v , then v1 ( the voltage drop across r1 ) = 0 and v2 = 5v . if the float is exactly in the middle , r1 = r2 = 500 ohms . then v1 = v2 = 2.5v . if the float is at the bottom , then r1 would be 1000 ohms and r2 would be 0 , v1 would be 5v and v2 would be 0 . the movement does not change the voltage across the one whole resistor , but it changes how much of that voltage drop is above and how much is below the float . as drawn in your diagram , the voltmeter/level indicator measures v2 , the voltage drop that is below the float .
as your calculations show , yes , it does . the reason for this is that the work performed by a force $f$ on an object is proportional to the displacement through which it is applied , $$w=f \delta x . $$ if an object is going faster , then for an given time interval $\delta t$ ( and thus a given impulse $i=f\delta t$ ) the displacement $\delta x=v\delta t$ is bigger , and you must perform more work .
as with many discussions about string theory , it is sometimes good to recall some reality : it was over 50 years ago that the higgs mechanism was proposed . compared to fully-fledged theories such as string theory , the higgs mechanism is a tiny add-on to the observed standard model ( as it was then ) . it took 50 years for experiment to get to the point of seeing it , and in fact so far just a first glimpse of it . for 50 years , the higgs mechanism was speculation not confirmed by experiment . it had all theoretical backing behind it , theory all pointed to it being true , but it could not be checked experimentally for 50 long years . for 50 years , you were free to make tv documentaries about particle physics without mentioning the higgs mechanism , if you thought it was too outlandish a proposal to have a chance of being confirmed . then finally experiment reached its energy scale and there it was . 50 years later . as you all know , there are scenarios thinkable where more beyond-the-standard-model-physics is right around the corner , but nothing to rule out that it takes another 50 years to see the next piece of " new physics " . that is just a fact of our short life . but that is not necessarily as bad as it may sound . while " new physics " may remain specuative for a long time to come , here is a well-kept secret to take note of : even old physics is not fully understood yet . and string theory can help here , and theorists know ( though tv stations may not yet have gotten the message ) . for instance , computation of scattering amplitudes even in the known and confirmed standard model is still a challenge , if only you are ambitious enough . string theory has helped with understanding some subtle points in plain yang-mills perturbation theory . see the links at string theory applied elsewhere -- qcd scattering amplitudes . in particular check out the remarkable story linked to there , told by matthew strassler in his post from string theory to the large hadron collider , which is about how string theory insights into qcd scattering amplitudes helped raise the precision of loop computations to the level that it was possible in the first place to separate signal from background in the lhc . he cites people who were involved as saying that without these string theory insights the higgs might have been produced , but not identified at the lhc . have a look , it is an interesting story . another thing may be worthwhile to remember from time to time : while we are all fond of proclaiming that we understand fundamental particle physics via quantum yang-mills theory , fact is that quantum yang-mills theory is still an open theoretical problem . we know that we do not understand some very fundamental facts about qauntum yang-mills . it is a " millenium problem " yang-mills existence and the mass gap . now , one thing that string theory has become after its " second revolution " is something like a map of the space of yang-mills like-field theories and various " dual " theories . via d-brane physics , kk-reduction , ads/cft , etc . yang-mills like theories appear in various guises in various corners of string theory , and their embedding into string theory geometrically explains subtle equivalences between these , such as electric/magnetic duality , etc . if you have not seen it before , check out at http://ncatlab.org/nlab/show/gauge+theory+from+ads-cft+--+table at least part of this string-theoretic " map " of the space of quantum field theories related to yang-mills theory . while this has not solved the mass gap problem yet , clearly , one may start to feel that the deeper nature of yang-mills theory is slowly but surely being probed here . the punchline here is the following : besides being a framework for models of quantum gravity and gauge unification , string theory is a piece of theoretical physics that sheds light on the nature of quantum field theory as such . while experimentalists and public media are busy with indulging in the higgs physics now that they waited for for half a century , maybe theoreticians can use the time before the next accelerator to step back and think a bit more about the still open more fundamental issues of quantum physics . that is where string theory has already helped , and i think will help in the future . of course you will not see this on public tv . ( generally , it is surprising these days how not only the public media but also the broad community 's attention is consistently attracted to the shallow and ignoring the deep advances that do happen in fundamental physics . for instance there is loads of excitement about , say , the firewall essay contest , but the really interesting advances , such as for instance in genuine mathematical characterization of string theory vacua here remains a topic among a tiny group of specialists . at the same time everybody has an opinion about the " landscape " , and everbody else has the opposite opinion . what is needed instead is more decent theoretical work on the foundations of quantum field theory and , inevitably then , string theory . )
there is a simple reason why we can consider variations on the whole $a$ rather than the quotient $c=a/g$ and the reason is following : all configurations that are $g$-equivalent have the same value of the action $s$ . that is what we mean by the statement that the theory has the symmetry $g$ . so the variation of the action $s$ in the directions that are equivalent to the action of a gauge transformation in $g$ vanish automatically , by the gauge invariance of the action ! the variation of the action $\delta s$ is therefore a combination of the variations $\delta a_\mu$ only of those kinds that are independent of the directions along $g$ . that is also why the equations of motion that we derive from $\delta s=0$ do not determine the evolution of the fields such as $a_\mu$ unambiguously out of the initial conditions : the equations of motion only constrain $f_{\mu\nu}$ and they always allow us to change $a_\mu$ in the future , by a gauge transformation . this ambiguity arises from the " flat directions of the action " . you could be studying $\delta s =0$ on the quotient $c=a/g$ only but it would be cumbersome and $\delta s$ would be physically and literally the same thing as it is on $a$ . it is the very point of introducing degrees of freedom which include one redundant one ( because of the gauge symmetry ) to simplify the picture . in fact , the equations of motion from $\delta s= 0$ on $a$ are manifestly lorentz-covariant etc . if you were trying to parameterize the space $c=a/g$ by some fields , you would probably have to impose some lorentz-breaking or otherwise unnatural conditions , e.g. $a_0=0$ , and the whole formalism would lose the manifest lorentz symmetry even though the actual phenomena , when looked at properly , would still obey the laws of relativity . so yes , the methods on $a$ and on $c=a/g$ are equivalent , and it is the calculus on $a$ that is the smarter one . if the formalism using $c=a/g$ were more convenient from all points of view , we would never talk about the gauge symmetry because it would be a totally counterproductive concept ! be sure that it is a very useful concept . i can not make sense out of the second part of the question . when we discuss infinitesimal variations of quantities such as energy , they should be linear combinations of the infinitesimal variations of the fields , like $du=\vec e\cdot d\vec d$ . but your expression is " doubly infinitesimal " , it is bilinear in $\delta x$ where $x$ is something , and terms this small can be neglected in the usual infinitesimal calculus in which $\delta a$ is sent to zero because they are of higher order . the energy ( and stress-energy tensor ) is gauge-invariant in electrodynamics , too , so its ( linear , first-order ) variation induced by gauge transformations is equal to zero . talking about some second-order " variations " seems completely misguided to me . also , less seriously , i feel uneasy about your usage of the word " orthogonal " . in general , one does not have an inner product ( needed to determine orthogonality ) on the full configuration space and it is really not needed for most questions of this sort . the directions in $a$ away from a slice that may be used as representatives of $c=a/g$ come in two types : those that are pure gauge transformations , in the direction of the $g$ " fibers " , and those that are not . most of them " are not " but any combination of those that are and those that are not " is not " again and no combination is really " fundamentally different " than others . so it is a bit meaningless to look for " orthogonal " directions to the directions along $g$ . finally , you ask whether " a " gauss law is related to the standard gauss law taught at school but you have not really explained what you mean by " a " gauss law . there is only one gauss ' law . it is the equation of motion obtained by varying the action with respect to $a_0$ , and it gives us something like ${\rm div}\ , \vec d = \rho$ . it is always the same law – which may be generalized to more complicated theories than electrodynamics , e.g. yang-mills theory . this equation ${\rm div}\ , \vec d =\rho$ is interesting because it does not contain any time derivatives . so it really does not dictate the time evolution of anything : it already constrains the initial state . one may prove that if the equation holds at $t=0$ , it will hold at any time : the time-derivative of the gauss ' law may be derived as a spatial derivative of other maxwell 's equations involving $\vec d$ . this non-dynamical = constraint character of the gauss ' law ( the fact it does not contain time derivative ) is related to the fact that this equation of motion is derived from the variation of a field that may be interpreted as a completely redundant one in a particular convention how to fix the gauge symmetry . that is why we can identify it with the statement that the states related by gauge transformations are treated as equivalent states by the theory . this is particularly clear in the quantum theory where we may a priori have states not annihilated by ${\rm div}\ , \vec d - \rho$ but only states that are annihilated by this operator , i.e. states that respect the equivalence of the states related by gauge transformations , may be considered physical .
when physicists say that a particle has electric charge , they mean that it is either a source or sink for electric fields , and that such a particle experiences a force when an electric field is applied to them . in a sense , a single pair of charged particles are a battery , if you arrange them correctly and can figure out how to get them to do useful work for you . it is the tendency for charged particles to move in an electric field that lets us extract work from them . a typical electronic device uses moving electrons to generate magnetic fields ( moving electrons cause currents , and currents generate magnetic fields ) and these magnetic fields can move magnets , causing a motor to turn . what is happening at a fundamental level is that an electric field is being applied ( via the potential across the battery ) that is causing those electrons to move . if i wanted a magnetic field to be generated , i could get one from a single pair of charges , say , two protons placed next to one another . the protons will repel ( like charges repel ) and fly away from each other . these moving protons create a current ( moving charge ) which creates a magnetic field . your author is right when he says that charges attract or repel other charges . to help connect it to more familiar concepts , consider this : the negative end of your battery terminal attracts electrons and the positive end repels them . ( the signs of battery terminals are actually opposite the conventional usage of positive and negative when referring to elementary charges . as a physicist , i blame electrical engineers . ) the repelled and attracted electrons start moving , and these moving electrons can be used to do work .
i think the real question is actually posed most directly in your comment ( so you might want to consider editing some of this into the original question ) : i am more concerned about understanding whats going on and making sure that i know how to do it . i have heard that you should not worry about significant figure rules until you have your final answer . how many decimal places should you round a number like 50.0 cos ( -20.0 ) to ? do you always round to 2 decimals as in my problem ? yes , you are correct that you should never actually round a number off until you are done with the calculation . however , when you are writing out your intermediate steps , it is common practice to write rounded values , rather than copying every digit your calculator shows you , just to avoid burdening the reader with a lot of extra digits that do not really add anything interesting . keep in mind that this convention only affects what you write . you still keep the number to full precision in your calculator . as for choosing the number of digits to write out , you can use the significant figure rules , which go like so : addition and subtraction : find the last significant digit of each number , and choose the one with the larger place value . that place should be the last significant digit of your result . another way to think of this rule is that a digit in the sum ( or difference ) is not significant unless both digits that were added to produce it are significant . so , using gray shading to designate insignificant digits : $$\begin{align*}3 and . 146309\\+2 and . 71\\ =5 and . 85\color{red}{6309}\end{align*}$$ so you would round to the last significant digit in this case , i.e. you would write out $5.86$ . but if you use this result again : $$\begin{align*}5 and . 85\color{red}{6309}\\+4 and . 93101\\ =10 and . 78\color{red}{7319}\end{align*}$$ this time you would again round to the last significant digit , and write out $10.79$ . multiplication and division : your result should have the fewest number of significant digits of either of the numbers you are multiplying or dividing . $$\begin{align*} and 253.1\\\div and 45\\ = and 5.6\color{red}{2444\ldots}\end{align*}$$ and if you multiply this by the earlier result , $$\begin{align*} and 5.6\color{red}{2444\ldots}\\\times and 10.78\color{red}{7319}\\ = and 60 . \color{red}{67268\ldots}\end{align*}$$ these rules are a simplification of a slightly more complex ( but more accurate ) system , the error propagation rules , which physicists normally use in research . unfortunately , the error propagation rules for functions like the sine and cosine can not be simplified quite so easily , so in practice people often just use the multiplication/division rule ( write out the fewest number of significant digits ) for everything else not mentioned here . of course , you have to remember that , except for final results , it is really not that important how many digits you write , since you should never be rounding " behind the scenes " in your calculator anyway .
there is a crucial difference between the newtonian time-varying field effect and the long distance effect , in that the newtonian effect is what is called " near field " and the radiative transmission of energy is by a " far field " . it is the difference between an electrostatic force and a radio wave ( lubos motl 's answer gets at this , but it is possible to elaborate using electromagnetism as a direct analog . gravity has more components , and is less intuitive , but it is the same idea ) . not all time-varying field responses are true waves . if you hold two charges , they have an electrostatic force . if you move one of the charges around the other , you get a time-varying electrostatic field on the other . this effect can lead to all sorts of oscillations on the second object . but this time varying field is , when the objects are separated by less than the speed of light divided by the typical oscillation period , not an electromagnetic wave . it is just a time-varying electrostatic field . the electrostatic field dies off as $1/r^2$ , and so the energy density in the field dies off as $1/r^4$ , which means that the total energy going past a sphere of radius r dies off as $1/r^2$ . if there were radiation going out , the amount of energy going past concentric large spheres would be roughly constant , as the ratiation passed the spheres , and this requires fields which fall off like $1/r$ , not $1/r^2$ . the difference in falloff of the two kinds of fields is important . there are proposals for near-field electrostatic and magnetostatic communication . in practice , this just means using a radio wavelength bigger than the distance between the objects , so that you would have them nearly touching , and then you can synchronize them with signals that are too small to be registered from far away ( because static fields fall off much quicker than energy carrying waves ) . the magnetic fields generated when you move an electric charge in a circle , together with the induced electric fields from the magnetic field , does only die off as $1/r$ , meaning that the total energy density carried across larger and larger spheres is constant . this energy flux is the electromagnetic wave energy , and it is the far-field , or radiative field component of the electrostatic situation . the near and far field are not continuously related , they cross over , so that the far-field responses are not intuitive as compared to the near field . gravity also has a near-field $1/r^2$ force , and this is also carries negligible energy and has zero detection possibility at long distances . the induction of components other than time-time of the metric tensor is is required to have gravitational waves , and this is not possible in newton 's conception . so it is not correct to say that newton was considering gravitational waves , even when you make gravity propagate at finite speed , because the effects you are considering are all near field effects , while the true gravitational radiation is far-field .
you have $2$ atomic ensembles . for each atomic ensemble , the ground state is $|\psi_0\rangle$ , and the excited state is $|\psi_1\rangle$ . the excited state is signaled by an idler photon . without beam splitter , if you detect only one photon ( for the whole set ) , you know which atomic ensemble is excited , so you have the state $|\psi_0\rangle |\psi_1\rangle$ or the state $|\psi_1\rangle |\psi_0\rangle$ however , if you merge the optical path of the two idlers photons ( with a beam splitter ) , and if you detect a photon , you are not able to say which atomic ensemble is in the excited state . the global state is entangled : $$|\psi\rangle = \frac{1}{\sqrt{2}} ( |\psi_0\rangle |\psi_1\rangle + e^{i\phi} |\psi_1\rangle |\psi_0\rangle ) \tag{4}$$ where $\phi$ is a phase factor due to the difference of optical path .
i found a compact version of the course notes , re-written in english by the same professor who gave the original lectures . you can find them at this link .
this would be a dyson sphere , there should be no difference , in terms of gravitational forces , if the central object is a star or a black hole ( of the same mass ) . see also niven ring - where rotation is used to provide artificial gravity .
given that leftaroundabout and vonjd have addressed the fundamental place of the fourier transform in the formalism , let me talk a little about an experimental application . what is the shape and size of a atomic nucleus ? from rutherford we learned that the nucleus is rather a lot smaller than the atom as a whole . now , electron microscopy can just about provide vague picture of a medium or large atom as a out-of-focus ball , but there is no hope of employing that technique to something orders of magnitude smaller . what we do is scatter things off of the component parts of the nucleus . a nice reaction here is $$ e + a \to e + p + b $$ where $a$ represents that target nucleus and $b$ the remnant after we bounce a proton out . ( this is what nuclear physicists call " quasi-elastic scattering " . ) now , if ( 1 ) we are shooting a beam of electrons at a stationary target , ( 2 ) we have a precision measurement of the momenta of the incident and scattered electrons and the ejected proton , ( 3 ) we are willing to neglect excitation energy of the remnant nucleus , and ( 4 ) we assume that $p$ mostly did not interact with the remnant after being scattered , we know the momentum of the proton inside the nucleus at the time it was struck . collect enough statistics on this and we have sampled the proton momentum distribution of the nucleus . now , here 's the fun part : you can show that the spacial distribution of protons in the nucleus is the fourier transform of the momentum distribution . and bingo , a measurement of the size of the nucleus . do it with a polarized target and you can get info on the shape as well .
it will depend on a lot of factors . summer typically , otherwise-identical flats near the top , will be hotter than those near the bottom , for two reasons : 1 ) heat rises - so heat will rise from lower flats to upper flats . more accurately , the density of air decreases as temperature rises , so hotter air will tend to rise up through buildings , where convection is possible . 2 ) overshading is likely to be less , higher up : in summer , most of the heat in a typical flat will come from solar gain ( rather than , say , from internal gains from cooking , people , appliances ) . the more that windows are overshaded , the lower this solar gain is . flats low down will have their windows overshaded by neighbouring buildings , trees , and so on . flats higher up will see more sky from their windows ; so will have higher solar gain . winter much of the stuff above , particularly about heat rising , but also about solar gain , still applies in winter : although heating systems may now be the single largest source of heat , solar gains can still be relevant , if there are large south-facing windows . basements obviously , there is little or no solar gain - there may be some small windows at the footway ground level , but not much . however , as @anna-v says , there is the moderating effect of the ground itself , which acts as a large thermal store . this large thermal mass will act as a seasonal buffer , heating very slowly through spring and summer , and cooling slowly through autumn and winter , thus typically moderating both the hottest summer temperatures and the coldest winter temperatures . there are software packages , such as energyplus , and phpp that can model solar gain and the effects of thermal mass , at any time of year , for any location ; but note that will need a lot of input parameters to do a decent job of it .
1 ) op is basically wondering how weinberg on the middle of p . 112 can extend the integration region from$^1$ $${\cal j}^{\pm}_{\beta}~=~ \int_{m_{\alpha}}^{\infty} \ ! de_{\alpha}\frac{e^{-ie_{\alpha}t}g ( e_{\alpha} ) t_{\beta\alpha}^{\pm}} {e_{\alpha}-e_{\beta}\pm i0^{+}}$$ to include the negative real axis $${\cal j}^{\pm}_{\beta}~=~ \int_{-\infty}^{\infty} \ ! de_{\alpha}\frac{e^{-ie_{\alpha}t}g ( e_{\alpha} ) t_{\beta\alpha}^{\pm}} {e_{\alpha}-e_{\beta}\pm i0^{+}} , $$ where $g:e_{\alpha}\mapsto g ( e_{\alpha} ) $ is a meromorphic function ? 2 ) that weinberg ( implicitly ) assumes meromorphicity of the $g:d\subseteq \mathbb{c}\to \mathbb{c}$ function can be deduced further down on p . 112 , where he writes that [ . . . ] we can close the contour of integration for the integration variable $e_{\alpha}$ [ . . . ] , which is a clear reference to the residue theorem , which in turn assumes meromorphicity . also weinberg writes on the same page$^1$ [ . . . ] the functions $g ( e_{\alpha} ) $ and $t_{\beta\alpha}^{\pm}$ may , in general , be expected to have some singularities at values of $e_{\alpha}$ with finite [ . . . ] imaginary parts [ . . . ] so there is little doubt that weinberg assumes meromorphicity of $g$ . 3 ) on the other hand , on the bottom of p . 109 , weinberg writes$^1$ [ . . . ] therefore , we must consider wave-packets , superpositions $\int\ ! de_{\alpha}~g ( e_{\alpha} ) \psi_{\alpha}$ of states , with an amplitude $g ( e_{\alpha} ) $ that is non-zero and smoothly varying over some finite range $\delta e$ of energies . [ . . . ] now according to the identity theorem for holomorphic functions , if a function $g:d\subseteq \mathbb{c}\to \mathbb{c}$ is zero on a subset $s\subseteq d$ that has an accumulation point $c$ in the domain $d$ , then $g\equiv 0$ is identically zero . however , any interval $i\subseteq \mathbb{r}$ on the real line of non-zero length has accumulation points . so if weinberg in above quote literally means that $g$ is mathematically zero outside some finite interval $i\subseteq \mathbb{r}$ , then $g\equiv 0$ would be identically zero in the whole complex plane . of course weinberg does not mean that . he just means that $g$ outside some finite range takes so small values , that to the precision $\epsilon$ that we are working , it does not matter whether we include the integration region $\mathbb{r}\backslash i$ , or not . in particular , mathematically speaking , weinberg has only proven the condition $$\tag{3.1.12} \int_{m_{\alpha}}^{\infty} \ ! de_{\alpha}~ e^{-ie_{\alpha}t} g ( e_{\alpha} ) \psi^{\pm}_{\alpha}~\longrightarrow~ \int_{m_{\alpha}}^{\infty} \ ! de_{\alpha}~ e^{-ie_{\alpha}t} g ( e_{\alpha} ) \phi_{\alpha} ~\text{for}~ t\to\mp\infty \qquad $$ within some precision $\epsilon$ . however , the precision $\epsilon$ can be made arbitrarily fine by preparing more and more sharply defined wavepackets $g$ . 4 ) if one would like to have a concrete example of a $g$ function , one may think of a lorentzian function ( aka . breit–wigner or cauchy distribution ) , $$g ( e_{\alpha} ) ~=~ \frac{1}{\pi}\frac{\delta}{ ( e_{\alpha}-e_0 ) ^2+\delta^2} , \qquad \int\ ! de_{\alpha}~g ( e_{\alpha} ) ~=~1 , $$ for appropriate choices of constants $e_0$ and $\delta$ . 5 ) finally , one should not loose sight of weinberg 's main goal in section 3.1 , namely to argue the $\pm i0^{+}$ prescription in the lippmann-schwinger equations $$\tag{3.1.17}\psi^{\pm}_{\alpha} ~=~\phi_{\alpha}+\int\ ! d\beta\frac{t_{\beta\alpha}^{\pm}\phi_{\beta}} {e_{\alpha}-e_{\beta}\pm i0^{+}} . $$ the lippmann-schwinger equations ( 3.1.17 ) are not an approximation , and they are independent of the choice of wavepacket $g$ . -- $^1$ to simplify the discussion , we have taken the liberty to replace weinberg 's more general $\alpha$-integration with just an $e_{\alpha}$-integration . here $$\tag{3.1.4} \int \ ! d\alpha \cdots \equiv \sum_{n_1\sigma_1n_2\sigma_2\cdots}\int d^3p_1 d^3p_2 \cdots$$ changing integration variable from $e_{\alpha}$ to momenta does not solve op 's problem , essentially because we still have to pick the branch of the pertinent square root that has positive real part , so that it does not bring us any closer in understanding negative energies .
the answer to your questions requires that you understand how em waves are generated . imagine an electron which is not moving and stationary . according to coulomb 's law , a field is produced by this electron . the field will be static and not changing as long the electron is not moving . imagine now you start to vibrate the electron in a sinusoidal way . what is going to happen to the field ? the field will be changed in a way that conforms to the motion you are doing to the electron . basically this change in the field is an em wave , which is travelling at the speed of light . so yes , the em field 's change moves , like the book has told you . a cute simulation is available at this website : http://www.colorado.edu/physics/2000/waves_particles/wavpart4.html hope this helps .
well , after symmetry breaking , all that remains is electromagnetic $u ( 1 ) $ , so the only generator that is truly a symmetry generator is $q$ . the fermions couple to the " higgs " via the yukawa coupling : $\mathcal{l}_y = -y_e^{ij} \bar l_{l , i} \phi e_{r , j} - y_u^{ij} \bar q_{l , i} \tilde{\phi} u_{r , j} - y_d^{ij} \bar q_{l , i} \phi d_{r , j} + h.c. \ , $ which mixes left and right handed fermions . here $l$ is the left-handed doublet $ ( e_l , \nu_l ) $ , and $e_r$ is the right-handed singlet . because both $l$ and $\phi$ transform under $su ( 2 ) _l$ , there is a symmetry . after symmetry breaking , $\mathcal{l}_m = -\frac{y_e^{ij} v}{\sqrt{2}} \bar e_{l , i} e_{r , j} -\frac{y_u^{ij} v}{\sqrt{2}} \bar u_{l , i} u_{r , j} -\frac{y_d^{ij} v}{\sqrt{2}} \bar d_{l , i} d_{r , j} + h.c. $ where $v$ is the higg 's vev . this is not invariant under $su ( 2 ) _l$ . the same thing happens with the gauge bosons that become massive , although there the interaction term comes from the covariant derivative acting on $\phi$ . finally , the potential for $\phi$ , ( the mexican hat ) is symmetric under su ( 2 ) , but the vacuum is not , because for the vacuum state , $\langle 0 | \phi | 0\rangle = ( 0 , v/\sqrt{2} ) $ , which is not invariant .
it is a fundamental particle with the properties of an electron ( one electron charge , one electron mass etc . ) . electrons are extremely small and considered to be point particles , but their wave function can be dispersed over a large area ( entire atom or even molecule ) . a proton is not a fundamental particle since it is composed of three quarks ( these quarks are fundamental ) . all electrons are identical to each other . so if i have two electrons there is no experiment that i can perform to tell them apart . if there was they would behave drastically differently . electrons are fermions which means that no two electrons are allowed in the same place at the same time . if they were then there would be no chemistry since all electrons would drop into the lowest energy atomic shell . electrons also have a property called spin . it is like the particle is spinning on its axis ( despite being a point particle with no axis ) , it is only allowed to spin at one speed and in one of two directions . but we were not actually told what the particles were ! well this is a deep philosophical question with no universally agreed answer . classically we imagine it is just a tiny charged billiard ball that bounces around , but this view is very wrong . in quantum mechanics an electron is considered to be a wave function that interacts with other wave functions , all of which is treated very mathematically , with no classical analog . in quantum field theory and string theory the electron is considered an excitation of a string or field . imagine a belt with a twist in it , the twist is the electron , it can move from one end to the other , it can be in a very spread out or exist in one spot , and if it meets a twist in the opposite direction then the both disappear . the twist in the opposite direction being an anti particle positron . and if we do not know , how can we fire them from guns , like they do in experiments . . . etc if we do not understand something there is nothing stopping us from doing experiments and firing them from guns . in fact once we do understand something that is when we stop doing experiments . if i do not know what a platypus is there is nothing stopping me firing it in a gun .
that is a pretty good way to look at it . to be more mathematically explicit , notice that the energy of an electric dipole ( see here ) with dipole moment $\mathbf p$ in an electric field $\mathbf e$ is $$ u = -\mathbf p\cdot\mathbf e = -|\mathbf p||\mathbf e|\cos\theta $$ this expression is minimized when $\cos\theta = 1$ , which is when the angle between the dipole moment vector ( which for a physical dipole points from the negative to the positive charge ) and the field is $0$ . this is precisely the condition for the dipole to be aligned with the electric field as you described . if the dipole is not aligned with the field , then it will experience a torque that tends to align it with the field . you can see why this happens in the physical dipole ; the positive charge feels a force in the direction of the field , while the negative charge feels a force in the direction opposite the field , and these both tend to rotate the dipole to align in with the field .
because $\left ( i\gamma^\mu\frac{\partial}{\partial x^\mu}+m\right ) _{\xi\xi&#39 ; }i\delta ( x-x&#39 ; ) $ is a symbolic expression for a given analytical right-hand side . it is written so for convenience ( not yet calculated ) but it is a specific expression like $\delta ( x-x&#39 ; ) $ or $\delta ( x-x&#39 ; ) &#39 ; $ . it should not acquire any " gauge extension " by definition . this expression does not contain a " particle momentum " .
that rather depends on the accelerator and the intended purpose of the beam . but to choose one particular example the main electron accelerator at jefferson lab is the continuous electron beam accelerator facility ( cebaf ) which ( duh ! ) functions in a continuous fashion ( thought the electrons still comes in " bunches " because at any given time a rf klystron is only adding energy to particles found in part of it is length . the bunch spacing is ~2 ns in each of the three experimental halls and can be clearly reconstructed in some experiments . that said , the ability of cebaf to recirculate electrons in a continuous mode like that is a very special feature of the machine ( which to my knowledge is not duplicated anywhere else at this time ) . other than that , linear machines and synchrotrons can run in continuous mode , but accelerator rings machines must fill-n-spill .
the pressure coefficient at a certain point ( at which the value of the pressure is $p$ ) is defined as $c_p=\frac{p-p_\infty}{\frac12\rho_\infty u_\infty^2} , $ where the $\infty$-symbol denotes freestream quantities . for an incompressible and steady fluid and assuming zero viscosity , bernoulli 's equation is given by $p+\frac12\rho u^2=p_\infty+\frac12\rho u_\infty^2 , $ which we can rearrange as $\frac{p-p_\infty}{\frac12\rho u_\infty^2}=1-\frac{u^2}{u_\infty^2} . $ in order for this expression to be valid for given fluid , we have to show that it is incompressible and steady . incompressibility means that the laplacian of the flow potential $\phi$ vanishes , this can be shown to be true for the problem at hand . furthermore , a fluid is steady if its flow does not depend explicitely on time , which is also the case .
the short version is that you are using an incorrect expression for the energy of a particle in motion . the correct , general expression for the kinetic energy of a particle of mass $m$ is $$ t = ( \gamma - 1 ) m c^2 \ , , $$ where $\gamma$ is the lorentz factor $$ \gamma = \frac{1}{\sqrt{1 - \left ( \frac{v}{c} \right ) ^2 }} \ , . $$ the version that you use , $t = \frac{1}{2}mv^2$ is only valid when $v \ll c$ .
hooke 's law is frequently used to model multi-dimensional materials because the stress tensor is simple ( linear ) . the full expression can be found on wikipedia . the simplification for 2d is straight forward ( drop any terms with a 3 in the subscript ) . note that whether deformation in one dimension affects the others is a property of the material and shows up through poisson 's ratio ( $\nu$ ) . independence between deformations in x and y imply $\nu = 0$ if you imagine two perpendicular springs only , then the terms with $\gamma$ ( or different subscripts like 12 , 23 , 31 , depending on the form of the equation ) drop out of the expression as those are shear terms . the shear terms can be thought of as a spring across the diagonal . the stress tensor $\sigma$ is defined as the force per unit area .
there are a few different ways to establish this correspondence , eg , using lie groups or fourier transform . but , in the end of the day , the notion that this takes one from classical to quantum mechanics is nothing but an ' axiom ' . so , in this sense , it is a bit weird to use the word " derive " the principle : this ' principle ' is used as one of the axioms that defines quantum mechanics . in any case , von neumann was probably the one to first formalize this construction in terms of what he called " transformation theory " , which is the theory of fourier transforms for distributions ( generalized functions ) .
the short answer is that you are basically correct ; you just need to be more careful with your notation and your minus signs . here 's the long answer . by the definition of a conductor , the sphere is at some constant potential . additionally , the potential of the system goes to zero as $r\rightarrow \infty$ . there are a lot of different ways to think about this problem , but the simplest is probably to say that since the problem is spherically symmetric the equipotential surfaces ( i.e. . the surfaces of constant potential ) are concentric spheres centered around the conductor . therefore , the only important length in determining a potential difference is the difference in radius from the center of the sphere . ( think about it : you can move freely over any sphere concentric with the conductor without changing your potential . ) as a result , $a$ and $b$ may as well both lie along the same line connecting them to the origin . as you correctly noted , if the sphere carries a charge $q$ , then the potential at a given point outside of the sphere is given by $v=\frac{kq}{r}$ where $r$ is the distance from the center of the sphere to that point . by definition , $\vec e=-\vec \nabla{v}$ . therefore , $\vec e=-\frac{\partial{v}}{\partial r}\hat r=\frac{kq}{r^2}\hat r$ . now , if you wanted to , you could integrate $-\int_b^a\vec e\cdot d\vec r=-\int_b^a\frac{kq}{r^2}dr$ which would give you back $v|_b^a=v_a-v_b$ . ( since $d\vec r=\hat rdr$ and $\hat r\cdot\hat r=1$ ) but we already know what the functional form of $v$ is , so we could just take that difference immediately . you get the same answer both ways : $$ \delta v=kq\big ( \frac{1}{a}-\frac{1}{b}\big ) $$
the formula you have specified $$ \delta k = \sqrt{ ( \delta k_1 ) ^2 + ( \delta k_2 ) ^2} $$ is the formula to obtain error of quantity $k$ , as being dependent on $k_1$ and $k_2$ according to the following expression $$ k = k_1 + k_2 . $$ generally , to obtain experimental error of a dependent quantity ( and the expression stated in your question ) , you start with the expression for dependent quantity $$k = f ( k_1 , k_2 , . . . ) $$ and use statistical expression $$\delta k = \sqrt{\sum_i \left ( \frac{\partial f}{\partial k_i} \delta k_i \right ) ^2} . $$ if $$k = \frac{k_1 + k_2}{2}$$ then $$ \delta k = \frac{\sqrt{ ( \delta k_1 ) ^2 + ( \delta k_2 ) ^2}}{2} $$ so the generalized answer might be : you have to divide with $n$ and not $\sqrt{n}$ . however , bare in your mind that the statistical expression above might be used when measured quantities are " independent " of each other . if $k_1$ and $k_2$ are the same quantity measured in two measurements , this is not exactly true , so the exact statistical expression is much more complicated .
these concepts refer to completely different aspects of reality . supersymmetry is a ( possible ) symmetry of the microscopic laws of nature , much like the rotational symmetry . entropy is the quantity counting the disorder of a given ( usually macroscopic ) system , the number of rearrangements that do not change the macroscopic appearance of the physical system ( well , the logarithm of the number of these rearrangements ) . entropy may be zero or nonzero for a system that preserves some supersymmetry or does not preserve any supersymmetry , entropy may be zero or nonzero in a theory that is supersymmetric and in a theory that is non-supersymmetric . all the combinations are surely possible . supersymmetry is a particular property of theories ( or states ) , a constraint , but it does not prohibit macroscopic objects . entropy is a measure of any macroscopic physical object . some calculations of entropy may simplify in a supersymmetric theory , especially if the state of the physical system preserves some of the supersymmetries . ( for example , the strominger-vafa black hole in 5d is the first one whose entropy was computed microscopically , and it is largely because it is the simplest black hole with a classically nonzero horizon that still preserves some susy , i.e. it is bps , we say . ) but that is true for all calculations , not only entropy calculations : susy often constrains and simplifies things .
the experiment detected more than just solar neutrinos , it also detected those produced by interactions with muons from cosmic rays , radioactive decays in the rocks surrounding the mine , internal radioactive contaminants in their tetracholorethylene fluid , and atmospheric argon decay production , but like any good experiment they controlled and subtracted all of these backgrounds . the full paper is actually available for free here and in section 6 of the paper ( nonsolar production of ${}^{37}$ar ) there is a full discussion of all of their background subtraction methods and calibration measurements . in particular , section 6.1 ( cosmic rays ) discusses how they subtracted the cosmic ray background from direct depth intensity measurements .
the usual approximation for arithmatic of quantities with uncorrelated uncertainties that for small ( ish ) uncertainties $\delta x_i$ or measurements $x_i$ let 's us write for multiplicative operation $$ \begin{array}~ y and = and \left ( \frac{x_1}{x_2}\right ) \ , \text{ or }\ , \left ( x_1 x_2\right ) \\ \frac{\delta y}{y} and = and \sqrt{ \left ( \frac{\delta x_1}{x_1}\right ) ^2 + \left ( \frac{\delta x_2}{x_2}\right ) ^2 } \end{array} $$ ( i.e. . add relative uncertainties in quadrature ) . you can get this kind of result from a bastardization of the chain the rule . given $y = f ( x_1 , x_2 , \dots ) $ or each input measurement $x_i$ , compute $\left ( \frac{\partial f ( x_1 , x_2 , \dots ) }{\partial x_i}\right ) \delta x_i$ and add all the resulting terms in quadrature . you have also been a little free with your nomenclature here . call $s$ the underling signal and $n$ the random noise ( this can be counting statistics or any other random process such as shot noise in the detector , but not a constant bias which must be subtracted off--our noise is assumed to have a mean of zero ) with a distribution whose width is characterized by $\sigma$ . a single measurement is then $m_i = s_i + n_i$ , and the population has a signal to noise ration of $\frac{s}{\sigma}$ . the win from addition is that the sum of $n$ such measurements is $$ m = \sum_{i=1}^n m_i = ns + \sqrt{n}\sigma $$ meaning that the signal to noise ratio of the sum is $\frac{ns}{\sqrt{n}\sigma} = \sqrt{n}\left ( \frac{s}{\sigma}\right ) $ , an improvement .
dear john , note that 23.85 å is equal to 2.385 nm , while the observed 4.3 nm is approximately two times larger . there is a simple error in your calculation that exactly fixes the factor of two . note that the actual calculation you should have done has a radius proportional to $1/m$ and the correct $m$ that you should substitute is the reduced mass of the two-body problem governing the relative position of the two particles . http://en.wikipedia.org/wiki/reduced_mass the reduced mass is $m_1 m_2 / ( m_1+m_2 ) $ . now , the important point is that an exciton is not a bound state of the effective electron and a superheavy nucleus : instead , it is a bound state of an effective electron and an effective hole - a larger counterpart of the positronium ( an electron-positron bound state ) . http://en.wikipedia.org/wiki/exciton assuming that both the electron and hole masses are equal , 0.26 $m_0$ , the reduced ( and still also effective ) mass is 0.26/2 $m_0$ = 0.13 $m_0$ , and the resulting $a$ is twice as big as your result , 4.77 nm - assuming that your arithmetics is right . the deviation from 4.3 nm is not too large but i can only handwave if i were trying to pinpoint the most important source of the discrepancy . it could be a different effective mass of the hole ; finite-size effects caused by the fact that the silicon atoms were not quite uniformly distributed inside the exciton , and so on . update oh , in fact , i noticed that your properties table does include a special figure of the effective hole 's mass and it differs from the electron mass : 0.38 $m_0$ . so the reduced mass is $$\frac{0.38\times 0.26}{0.38+0.26} m_0 = \frac{0.0988}{0.64} m_0 = 0.154 m_0 $$ and the calculated radius is $$ \frac{11.7}{0.154} \times 0.53\ &#197 ; = 40.3\ &#197 ; . $$ well , this is 7 percent too small , much like the previous one was 7 percent too big . ; - ) hydrogen atoms with composite heavy fermions concerning your second question , as you clearly realize , the calculated radius of the " atom " with such " heavy electrons " would be much smaller than the ordinary atom . this also proves that the assumptions of such a calculation fail : the heavy fermions ( in condensed matter physics ) are the result of the collective action of many atoms on the electron and its mass . so the large mass of the heavy fermions is only appropriate for questions about physics at long distances - much longer than the ordinary atom . if you look at very short distances - a would-be tiny atom with the heavy fermion - you cannot use the long-distance or low-energy effective approximations of condensed matter physics . you have to return to the more fundamental , short-distance or high-energy description which sees electrons again . at any rate , you will find out that there can be no supertiny atoms created out of the effective particles such as heavy fermions . the validity of all such phenomenological effective theories - such as those with heavy fermions - is limited to phenomena at distances longer than a certain specific cutoff and highly sub-atomic distances surely violate this condition , so one must use a more accurate theory than this effective theory , and in those more effective theories , most of the fancy emergent condensed matter objects disappear . non-relativistic effective theories just a disclaimer for particle physicists : in this condensed matter setup , we are talking about non-relativistic theories so the maximum allowed energy $e$ of quasiparticles does not have to be $pc$ where $p$ is the maximum allowed momentum in the effective theory . in other words , we can not assume $v/c=o ( 1 ) $ . quite on the contrary , the validity of such effective theories in condensed matter physics typically depends on the velocities ' being much smaller than the speed of light , too . so the mass of the heavy fermions is much greater than $m_0$ which would make $m_e c^2$ much greater than $m_0 c^2$ ; however , the latter is not a relevant formula for energy in non-relativistic theories . instead , $p^2/2m_e$ , which is ( for heavy fermions ) much smaller than the kinetic energy of electrons , is relevant . the maximum allowed $p$ of these quasiparticles is much larger than $\hbar/r_{\rm bohr}$ - the de broglie wavelength must be longer than the bohr radius . that makes $p^2/2m_e$ really tiny relatively to the hydrogen ionization energy .
you say : by setting γ to 1 , we obtain the result in galilean relativity ( ie : " the time between the lightning strikes is $\tfrac{vd}{c^2}$" ) , which is the theory of space time before einstein came out with special relativity . but remember that $\gamma$ is not some independant parameter . it is just shorthand for : $$ \gamma = \frac{1}{\sqrt{1 - \tfrac{v^2}{c^2}}} $$ so you can not just set $\gamma = 1$ without changing either $v$ or $c$ or both . if you set $v = 0$ then $t = t ' = 0$ . in this case the events are simultaneous in both frames , but that is not surprising because if $v = 0$ both frames are the same inertial frame . if you want to use the galilean limit but keep $v$ non-zero the way to do this is to increase the speed of light to infinity ( obviously this is a thought experiment ) . in that case $\gamma\tfrac{vd}{c^2} = 0$ and again $t = t ' = 0$ so the lightning strikes are simultaneous in both frames .
see the wiki article on polarized 3d glasses . most likely , you have a pair of circularly polarized glasses . the mirror reverses the circular polarization . the article on circular polarization does it better than i would be likely to achieve in less than an hour or two . or hyperphysics , or google .
as far as i have understood from this paper , they have given some observational limits to the value of $\omega_{k , 0}$ , but this article concludes asserting that " there is no evidence from planck for any departure from a spatially flat geometry " . taking $\omega_{k , 0}=0$ and the value for $\omega_{r , 0}$ given at this post , one can compute the above integral obtaining $t_0 = 0.947797 \ , h_0^{-1}$ , which , taking $h_0 = 67.3 \ , \text{km} \ , \text{s}^{-1} \text{mpc}^{-1}$ , gives $t_0 = 13.78 \times 10^9$ years .
all matter is made of waves —at least it can be represented that way , and it behaves that way . of course , matter also behaves like particles . this is one of the odd-but-true conclusions of quantum mechanics . the de broglie wavelength gives the wavelength of any " matter wave . " these waves are not waves in the classical sense with amplitude and the like ; they are wave functions , which express the probable location of a particle as something that looks like a wave . you can think of it in a sense that , looking very closely , the location and motion of a particle becomes blurry and starts to look like a wave instead of just one point . we usually think of matter as a wave only when making observations on a scale comparable to the de broglie wavelength , which is very small for most things . using the de broglie relation $\lambda=\frac{h}{p}$ , you can calculate the wavelength of a particle with momentum $p$ . as an example , a electron with a kinetic energy of 10 ev has a wavelength of 0.39 nm . that is the wavelength of the electron 's wavefunction . if you perform an experiment that would highlight that wave behavior , such as pass a beam of electrons of that energy through a diffraction grating with a spacing comparable to that wavelength , you would see an interference pattern , just as you do with light , because electrons behave like waves ( when we want them to ) .
it just happens to be a coincidence . the current popular theory for how the moon formed was a glancing impact on the earth , late in the planet buiding process , by a mars sized object . this caused the break up of the impactor and debris from both the impactor and the proto-earth was flung into orbit to later coallesce into the moon . so the moon 's size just happens to be random . plus the moon was formed closer to the earth and due to tidal interactions is slowly drifting away . over time ( astronomical time , millions and millions of years ) it will appear smaller and smaller in the sky . it will still always be roughly the size of the sun but total solar eclipses will become rarer and rarer ( they will be more and more annular or partial ) . likewise in the past , it was larger and total eclipses were both longer and more common .
the time airborne is dependent only on the vertical component of velocity . it is described by the following equation when gravity is uniform : $t_a = \frac{2v_{oy}}{g}$ , where $v_{oy}$ is the vertical component of the projectile 's velocity and g is the acceleration due to gravity . if you launch an object horizontally , then it has no vertical component of velocity and its airborne time is only dependent on it is height above ground when it is launched : $t_a = \sqrt{2h/g}$ , where h is the height of the object when it is launched . if the object has a larger component of horizontal velocity , it will travel farther during its time in the air , but as the above two equations show , the amount of time it spends in the air is not dependent on the value of its horizontal velocity . this is only relevant to a pole dance if the pole dance is a physicist : ) actually , what is probably happening on the pole is that a larger velocity around the pole contributes to a higher centripetal force for the dancer . friction is proportional to the force between the dancer and the pole , so increased centripetal force leads to increased friction which means its harder to slide down .
the average of any quantity $s$ is $\frac{\sum\limits_{r=0}^ns_r}{n}$ . if the distribution is continuous , lets say as a function of x , then it becomes $\lim\limits_{n\to\infty}\frac{\sum\limits_{r=0}^ns_r}{n}$ . this can be rewritten as $\frac{\int s ( x ) dx}{\int dx}$ , taking limits as the length of the wire . in your formula , i do not see any $x$ term in the rhs , nor anything that could depend on x , so i do not see how we can proceed . please specify what is constant and what is a function of x . so the final formula is $$\frac{\int t ( x ) dx}{\int dx}$$ if your wire is infinite , you may need to take limits 0 to y , and then limit the expression for average as $y\to\infty$ . update : with the updated formula , assuming the wire spans from x=0 to x=l , $$\langle t\rangle=t_\infty- \frac{\dot{q}}{km^2}\left ( \frac{\tanh ( ml ) }{ml}-1\right ) $$ if the wire spans from 0 to y , $$\langle t\rangle=t_\infty- \frac{\dot{q}}{km^2}\left ( \frac{\sinh ( my ) }{my\cosh ( my ) }-1\right ) $$ . limiting y to infinity gives us an infinite answer . so i am assuming that i have interpreted it correctly in my previous answer .
lee , low , and pines mention " low lying energy states of the system ( $p^2/2m\ll\omega$ , where $p$ is the total momentum of the system , $m$ is the mass of the electron , and $\omega$ is the frequency of lattice oscillations ) . " so yes , on the one hand , these states are " close to the ground state " , on the other hand , they have electrons " with small momentum " in the conduction band . they discuss dielectrics , as far as i can understand , so the conduction band is empty in the ground state , so electrons can have low momentum in the low-lying energy states ( the momentum of these electrons is low compared to the debye momentum and , therefore , compared to the fermi momentum as well . )
the uv divergences have ( most the time ) nothing to do with perturbation theory ( or , stated otherwise , free particles ) . they are also present in non-perturbative approach ( see for example non-perturbative renormalization group , or exact renormalization group ) . divergences , or better , cut-off dependence of observables means that the quantity you are looking at depends on the high energy physics , and is , in some sense , a free parameter of the theory . by that , i mean that stating that the system is described at low energy by a $\phi^4$ theory is not enough to completely define the theory , you also need to give the value of the mass ( for example ) . one main thing to keep in mind when studying qft , especially in the context of high-energy physics , is that there is no reason that having a cut-off dependence is problematic . in the case of statistical mechanics and condensed-matter , the cut-off dependence is representative of the microscopic physics . in qed , say , there are also no good reason to think that the cut-off is unphysical , since we know that in fact at high energy , qed is a subpart of the electro-weak force , etc . one of the qft where people really want that there are no infinities is the case of general relativity ( if you think that a qft should be enough , and you do not need fancy strings or who knows what ) . that is the famous case of the non-renormalizability of gr . but that is a perturbative statement , which does not mean that gr in non-renormalizable , implying that you have to invent something else . indeed , the asymptotic safety scenario assumes that in fact gr is non-pertubatively renormalizable in the uv , meaning that there is a uv fixed point that allows to send the cut-off to infinity without divergences ( you still have to fix one or two quantities to be on the critical surface , but that is much better than the infinity of coupling constant to be fixed in the perturbative approach ) . interestingly , there are good evidences that such uv fixed point indeed exists .
this is a very intuitive way of arriving at helicity-1 representations , but it is not totally correct . essentially , you are ignoring the continuous spin representations of the poincare group . when you do induced representations of a semi-direct product like the poincare group , you fix some momenta as a representative element of an orbit of $so ( 3,1 ) $ and then look for the little group that stabilizes this momenta . in the case of massive particles for example you choose $ ( m , 0,0,0 ) $ and find that the little group is su ( 2 ) giving rise to the spin degrees of freedom of massive particles . little group transformations then transform spin states into other spin states . but in the case of a massless representation , choose for example $ ( e , 0,0 , e ) $ . if you play around for a while you will find that a much larger group , $iso ( 2 ) $ , the isometries of the plane , stabilizes this momenta . the representation theory of this group can be obtained as it is for the poincare group ( $iso ( 3,1 ) $ ) . but because of the two " momenta " parameters in $iso ( 2 ) $ most of these representations will be infinite dimensional . this would mean that such a massless particle has infinitely many internal degrees of freedom . since this is not observed , people choose the " representative momentum " ( 0,0 ) i.e. the vacuum state for $iso ( 2 ) $ . this corresponds to setting the $iso ( 2 ) $ " momentum generators " to zero , and the little group here is then $so ( 2 ) $ . irreducible representations of $so ( 2 ) $ are one dimensional , but you have to include two such representations by cpt , giving rise to the two degrees of freedom of the photon . so what i am saying is that the $m \to 0$ limit for massive particles is not really well defined , you could land on the discrete helicity representations or the continuous spin representations . because of this it is safer to just start with the correct representations of the poincare group . say that you want the helicity-1 representation . try to put it in $a_\mu$ with polarizations $\epsilon_\mu$ . note that even if you choose only two polarizations to be nonzero when you do a little group transformation they shift by something proportional to the momenta . interpret this as gauge invariance and make the identification on physical states . also , all representations of the poincare group will be characterized by the casimirs $p^2 , w^2$ . it is a straightforward calculation to show that $w^2=m^2j^2$ for massive states . your idea basically amounts to just setting m=0 in this expression , which allows you to show $w^\mu =\lambda p^\mu$ with $\lambda$ the helicity that labels the states . however , you should actually go back through the calculation and plug $ ( e , 0,0 , e ) $ into the expression for the pauli-lubanski vector . then you will find that to show $w^2=0$ you need certain linear combinations of the lorentz generators to vanish on the states . these are exactly the "$iso ( 2 ) $ momenta operators " that you need to set to zero to get the helicity representations . hope this helps !
the first issue i see : your speed is in miles/hour , and your time is in seconds .
edit : note that i am doing only the first variation , and i am not doing each and every step , mainly those pertinent in understanding how the general shape equation is determined . if you want to see the full derivation , you will need to understand the geometric mathematic primer discussed in sections 2 and 3 of the book . geometric methods in the elastic theory of membranes in liquid crystal phases by zhong-can ou-yang , ji-xing liu , yu-zhang xie , xie yu-zhang $c_{0}$: spontaneous curvature of the membrane surface $k_{c}$: bending rigidity of the vesicle membrane $h$: mean curvature of the membrane surface at any point $p$ $k$: gaussian curvature of the membrane surface at any point $p$ $da$: area element of the membrane $dv$: volume element enclosed by the closed bilayer $\lambda$: surface tension of the bilayer , or the tensile strength acting on the membrane $\delta p$: pressure difference between the inside and outside of the membrane . the shape energy of a vesicle is given by : $$ f = f_{c} + \delta p \int dv + \lambda \int da $$ where $$ f_{c}=\frac{k_{c}}{2} ( 2h-c_{0} ) ^{2} = \frac{k_{c}}{2} ( c_{1}+c_{2}-c_{0} ) ^{2} $$ the variation of $da$ and $dv$ are needed , refer to the book to locate those . next we will calculate the first variation of $f$ . and we can break this into components by starting with the first variation $f_{c}$ . $$ \delta ^{ ( 1 ) }f_{c} = \frac{k_{c}}{2}\oint ( 2h+c_{0} ) ^{2} \delta ^{ ( 1 ) } ( da ) + \frac{k_{c}}{2}\oint 4 ( 2h+c_{0} ) ^{2} ( \delta ^{ ( 1 ) }h ) da $$ where the first order variation of $\psi$ gives us : $$ \delta ^{ ( 1 ) }da = -2h\psi g^{1/2}dudv $$ $$ \delta ^{ ( 1 ) }dv = \psi g^{1/2}dudv $$ $$ \delta ^{ ( 1 ) }h = ( 2h^{2}-k ) ) \psi + ( 1/2 ) g^{ij} ( \psi _{ij}-\gamma _{ij}^{k}\psi_{k} ) $$ note : $\gamma_{ij}^{k}$ is the christoffel symbol defined by ( for reference ) : $$ \gamma_{ij}^{k} = \frac{1}{2}g^{kl} ( g_{il , j} + g_{jl , i} - g_{ij , l} ) $$ and we plug those into the variation of $f_{c}$: $$ \delta ^{ ( 1 ) }f_{c} = k_{c}\oint [ ( 2h+c_{0} ) ^{2} ( ( 2h^{2}-k ) \psi + ( 1/2 ) g^{ij} ( \psi_{ij}-\gamma_{ij}^{k}\psi_{k} ) ) ] $$ $$ = k_{c}\oint [ ( 2h+c_{0} ) ( 2h^{2}-c_{0}h-2k ) \psi + ( 1/2 ) g^{ij} ( 2h+c_{0} ) \psi_{ij} - g^{ij}\gamma_{ij}^{k} ( 2h+c_{0} ) \psi_{k} ] g^{1/2}dudv $$ and there are two relations ( $i , j = u , v$ ) $$ \oint f\phi_{i}dudv = -\oint f_{i}\phi dudv $$ $$ \oint f\phi_{ij}dudv = \oint f_{ij}\phi dudv $$ so then we have : $$ \delta ^{ ( 1 ) }f_{c} = k_{c}\oint \left \{ ( 2h+c_{0} ) ( 2h^{2}-c_{0}h-2k ) g^{1/2} + [ g^{1/2}g^{ij} ( 2h+c_{0} ) ] _{ij} + [ g^{1/2}g^{ij} ( 2h+c_{0} ) \gamma_{ij}^{k} ] _{k} \right \}\psi dudv $$ and we can re-write : $$ [ g^{1/2}g^{ij} ( 2h+c_{0} ) ] _{ij} = [ ( g^{1/2}g^{ij} ) _{j} ( 2h+c_{0} ) ] _{i} + [ g^{1/2}g^{ij} ( 2h+c_{0} ) \gamma_{ij}^{k} ] _{k} \psi dudv $$ and for functions $f ( u , v ) $ , where $u , v = i , j$ , we can directly expand : $$ [ ( g^{1/2}g^{ij} ) _{j}f ] _{i} = - ( \gamma_{ij}^{k}g^{1/2}g^{ij}f ) _{k} $$ a laplacian operator for these surfaces is defined in the book , and is given as : $$ \bigtriangledown^{2} = g^{1/2}\frac{\partial }{\partial i} ( g^{1/2}g^{ij}\frac{\partial }{\partial j} ) $$ so then we have : $$ [ g^{1/2}g^{ij} ( 2h+c_{0} ) _{j} ] _{i} = g^{1/2}\bigtriangledown^{2} ( 2h+c_{0} ) $$ using these methods in the first variation : $$ \delta^{ ( 1 ) }f_{c} = k_{c}\oint [ ( 2h+c_{0} ) ( 2h^{2}-c_{0}h-2k ) + \bigtriangledown^{2} ( 2h+c_{0} ) ] \psi g^{1/2}dudv $$ and now we want the variation of $f$ . $$ \delta^{ ( 1 ) }f = \delta^{ ( 1 ) }f_{c} + \delta^{ ( 1 ) } ( \delta p\int dv ) + \delta^{ ( 1 ) } ( \lambda\int da ) $$ which gives us : $$ \delta^{ ( 1 ) }f = \oint [ \delta p-2\lambda h + k_{c} ( 2h+c_{0} ) ( 2h^{2}-c_{0}-2k ) + k_{c}\bigtriangledown^{2} ( 2h+c_{0} ) ] \psi g^{1/2}dudv $$ and since $\psi$ is a very small , well smooth function of $u$ and $v$ , the vanishing of the first variation of $f$ requires that : $$ \delta p = 2\lambda h + k_{c} ( 2h+c_{0} ) ( 2h^{2}-c_{0}h-2k ) + k_{c}\bigtriangledown^{2} ( 2h+c_{0} ) = 0 $$ which is the general shape equation of the vesicle membrane . $c_{0}$ is a constant unless the symmetry effect of the membrane and its environment varies between each point ( we assume it does not ) otherwise $c_{0}$ becomes a function of $u$ and $v$ . so we can reduce to : $$ \delta p = 2\lambda h + k_{c} ( 2h+c_{0} ) ( 2h^{2}-c_{0}h-2k ) + 2k_{c}\bigtriangledown^{2}h = 0 $$ hope this helps . again i would locate that book to see the full derivations . i do not know if the visible section of the book on google shows you everything that you need to know , but i surely hope this points you in the right direction to understanding the problem .
presumably each axle is rigidly attached to its gear ( no bending or breaking ) . this means that the torque on the gear is the same as the torque on its axle . so you can ignore the axles and just think about the gears themselves . and in that case , you can just use the torque ratio of the gears themselves .
i take the heart of the heisenberg cut to be the way we calculate probabilities and expected values in qm . for elementary qm , for some measurement described by an operator $\hat m$ , the expected value in a state described by a density matrix $\hat\rho$ is given by the trace $\left&lt ; \ ! m\right&gt ; =\mathsf{tr}\left ( \ ! \hat m\hat\rho\ ! \right ) $ . what we put in the $\hat\rho$ is what is in our model universe . the measurement operator describes our measurement apparatus , which is not in the model universe , but instead describes how our measurement apparatus gets information out of the model universe . there is an almost-symmetry between the ways $\hat m$ and $\hat\rho$ appear ; it is almost as if there is a measurement apparatus universe as well as the model universe . different measurement apparatuses can affect each other in the measurement apparatus universe without changing the model universe , which is called measurement incompatibility ( did i just create an interpretation of qm ? do i know this one ? i guess it is too glib , sadly . ) edit : we can extend the mathematics in many different ways , but one deserves mention because it is of great practical value . we can introduce transformations $\hat t_i$ that operate between the preparation apparatus and the measurement apparatus , in which case we have $\left&lt ; \ ! m\right&gt ; =\mathsf{tr}\left ( \ ! \hat m\hat t_n\cdots\hat t_2\hat t_1\hat\rho\ ! \right ) $ . we can at any time say that $\hat m\hat t_n\cdots\hat t_5$ , say , or any other part of this list ( without changing the order ) , is our measurement . anyway , to some extent we can move stuff from the model universe into the measurement apparatus universe and vice versa , although we may have to get into technical stuff like povms to do it . once we go to quantum field theory , there is a tight relationship between the measurement apparatus universe and the model universe , because we use the same lego blocks to build measurements and states . the separation into states and measurements is absolutely fundamental in qm . it is how the relationship between hilbert spaces and experimental results works , which causes trouble when people want to do cosmology , with everything in the model universe . although i had not previously thought about calculating where one should put the separation , i can see that if one chooses a particular accuracy that one wants one 's model of an experimental apparatus to achieve relative to one 's real apparatus , that might put a limit on where one can put the heisenberg cut . i am not sure , however , that one can not always improve the sophistication of one 's description of a measurement , particularly if one is willing to go to povms . i suppose , however , that putting people inside your model universe is always going to be in the realm of toy models . the separation into states and measurement famously comes under the microscope in bell 's article `against " measurement " ' . incidentally , i see you went to willem de muynck , who is perhaps a little idiosyncratic , but i have often found his a good counterpoint of view .
if you mean that qft operators are matrices $m_{ij}$ whose indices $i , j$ are really points in space ( or spacetime ) , so that the operator is really represented by the function $m ( x , x&#39 ; ) $ , then the answer is no . the actual matrices corresponding to qft operators are much much larger than that . an object expressed by $m ( x , x&#39 ; ) $ is pretty much equivalent to operators in ordinary quantum mechanics of a single particle : $$m ( x , x&#39 ; ) = \langle x| \hat{m} |x&#39 ; \rangle$$ the function $m ( x , x&#39 ; ) $ knows everything about the operator $\hat{m}$ because we have evaluated all matrix elements of this operator with respect to a basis - in this case , the position eigenvector basis . ( it is just space , not spacetime . ) however , quantum field theory has a much larger hilbert space . instead of the simple functions above , you may imagine that an operator is expressed by the following functional : $$m [ \phi ( x , y , z ) , \phi&#39 ; ( x , y , z ) ] = \langle \phi ( x , y , z ) |\hat{m}| \phi&#39 ; ( x , y , z ) \rangle$$ note that the object on the left hand side is a functional - it is a function whose two arguments are functions of 3 variables themselves - field configurations of a klein-gordon field , in this case . again , the left hand side knows everything about the operator $\hat{m}$ that acts on the hilbert space of the klein-gordon quantum field theory . the formula above chose a specific basis - a " truly " continuous basis of field configurations of $\phi ( x , y , z ) $ . however , this prescription may make the operators of qft look more complicated than they really are . any operator in any quantum mechanical theory may be specified by its matrix elements with respect to any basis . and there are usually more intuitive bases . in particular , in free quantum field theories , a simple-to-imagine basis is the fock space basis . $$|0\rangle , a^\dagger_{i}|0\rangle , a^\dagger_{i} a^\dagger_{j}|0\rangle , \dots$$ it is the vacuum and all of its excitations by an arbitrary number of creation operators that add particles into any state . in this way , the hilbert space is represented as an infinite-dimensional harmonic oscillator . the transformation " matrix " from the continuous basis of the field configurations to the fock space basis may be found by copying the same transformation for a normal harmonic oscillator infinitely many times and taking the tensor product . ( you do not have to mechanically repeat the same work infinitely many times , so this prescription may sound more intimidating than it is . ) if you meant that the quantum fields $\hat{f} ( x , y , z ) $ are operators and for each $x , y , z$ , then the answer is almost yes . they are operator distributions - that generalize operators much like the distributions such as the delta-function generalize the concept of a function . however , they are damn large matrices . they are matrices expressed with respect to a basis - for example one of the two bases above . but if this paragraph captures what you meant , then you were just asking whether operators are matrices . indeed , at least morally , they are . but this fact is true in any quantum mechanical theory and is not specific to qft . an operator contains a lot of numbers and may always be represented as a matrix . however , the values of the matrix elements always depend on the basis you chose - sometimes in ways that can not be recognized quickly . however , there is something physical about each operator that does not depend on any basis : you may " feel " an operator . you may learn many of its properties . some of them are more easily understood in one basis ; other properties naturally lead to another basis . so it is useful to think about operators in a more general way than matrices in a particular fixed basis - because the latter viewpoint usually discourages one from gaining the insights that are more obvious in another basis . one should not forget that the transition from one basis to another is just a " trivial " linear algebra that does not contain any special " physics " knowledge .
i think it is not quite true that modern physics tries to " avoid big bang singularities " . among many other things , modern physics tries to determine what is happening in the vicinity of the region that the big bang cosmology views as a singularity . by saying that the big bang singularity is completely avoided , you are already presupposing an answer . this possible answer may be a " working hypothesis " but it may also be incorrect . so it is not a " goal of modern physics " . classical general relativity breaks down - and becomes unpredictive - in the presence of singularities . but that does not mean that the full theory that also knows everything about the short-distance physics has to avoid the singularities completely . in string theory , the big bang singularity has not been understood yet - at least to the satisfaction of most string theorists . but many other singularities have been fully understood and it is not true that all of them disappeared . some of them did not disappear but the physics around them began well-defined , anyway . in particular , time-like singularities - such as orbifold singularities ; orientifold singularities ; and conifold points - have been pretty much fully understood . there are new degrees of freedom and new phenomena that take place in their vicinity but in some sense , physics understands them as well as it understands the smooth space today . from some viewpoint - e.g. according to some " probes " ( objects whose reactions we use to evaluate what is happening in the region ) - the singularity may get regulated , replaced by a regular manifold with some typical length scales . other probes see the singularity replaced by a non-commutative geometry that is also regulated . but some singularities according to some probes still look singular ; the best geometrical description is still a geometry that has a full-fledged singularity at the original point . however , it is not necessarily a problem . despite the singularity , physics at some ( not quite ) manifolds such as the conifold may be shown to be completely equivalent to physics at a completely smooth manifold ( e . g . by mirror symmetry ) . this equivalence shows that physics at singular manifolds may be well-defined and predictive . the space-like singularities - like the singularity inside the schwarzschild black hole and the big bang singularity - remain confusing . some people even think that all questions involving the interior of a black hole or the very beginning of the universe are inevitably ill-defined , at least to some extent , so there will never be a set of sharp observables that can be exactly calculated and discussed . i am personally not sure whether it is the case : it could be . but of course , the working hypotheses what is happening near the big bang - probably before inflation - also include some models with bounces ; cyclic universes ; non-commutative geometry ; a beginning of the universe from " zero " that looks totally smooth after the wick rotation to the euclidean space ( the hartle-hawking state ) ; but also an abrupt tunneling into another universe , and many other things . the collection of possibilities is pretty rich and some of them are more motivated than others . of course , physics still does not know for sure which of the tools are truly relevant for the birth of the universe .
electrons in the $n$-type conduction band diffuse across the boundary into the empty conduction band of the $p$-type semiconductor . once there they recombine with holes in the $p$-type valence band . so the net motion of electrons is from the $n$-type conduction band to the $p$-type valence band . this means that near the boundary there are no electrons in the conduction band and no holes in the valence band i.e. the material becomes insulating . the transfer of electrons causes the $p$ side to become negatively charged and the $n$ side positively charged . as electrons move , the charge separation creates an electric field that opposes the electron diffusion .
a possible answer to the last part of the question : the article six easy roads to the planck scale , adler , am . j . phys . , 78 , 925 ( 2010 ) contains multiple " derivations " that you might ( or might not ) find more satisfactory than the one you mention . as far as the rest of the question is concerned , others have made the most relevant points . i think a fair summary of what magueijo is getting at is something like the following : one frequently hears that " interesting new physics " happens when some length $l$ is less than the planck length . the planck length is manifestly lorentz invariant . the other length $l$ , if it is the physical length of some object , is manifestly not lorentz invariant . what meaning , then , can one assign to such statements ? it seems to me that reasonable people can differ over whether this is an interesting question . i do not find it manifestly insane , myself .
for incoherent light , yes , though you would do well to measure the transmission spectrum to protect yourself from nasty surprises . for a laser , you probably need professional filters with optics-quality surface roughness .
i originally had something about the constellations changing in the sky to show that the earth orbits the sun , but that would still be the case if the sun orbited the earth instead . now that i think about it , there is one thing that conclusively proves that the earth orbits the sun : parallax . over the course of one year many of the stars will move relative to each other . at the end of the year they will be back where they started . this is because the earth moves around in a 2au diameter circle , so that six months from your first observation , you will be standing 2au away from where you were then , and are viewing the stars from a ( slightly , but observably ) different angle . to show that the moon orbits the earth you could observe its location at the same time every night , and see that it moves , and is always nearly the same distance from earth . it never goes into a retrograde motion . assuming the earth is spherical , the only way this could be true is if the moon orbits the earth . you might also take the phases of the moon into account and model the sun-earth-moon system to explain it .
without going into the details of it , a conventional bwr gives a power density of $50 kw/liter$ inside the core , which is about half that of pwr . mind you , this includes the coolant inside the core and not the fissile material only . ref
it is very important to distinguish whether the symmetry is broken explicitly or spontaneously . i think that the sentence " now when i break this symmetry spontaneously ( or explicitly ) " indicates that its author is not quite distinguishing these things . an explicit symmetry breaking generally lifts the degeneracy because the different parts of the multiplets no longer have the same energy . however , spontaneous symmetry breaking increases the degeneracy , particularly of the ground state . it is really how spontaneous symmmetry is defined . it is a fate of the symmetry that remains the symmetry of the laws of physics , but in practice , the environment we encounter , starting from the ground state , is no longer invariant under the symmetry . its not being invariant means nothing else than the fact that if we act with a generator $g$ of the continuous symmetry on the ground state , we get $$ g|0\rangle \neq 0 $$ we would get zero if the symmetry were not spontaneously broken . if it is broken , we get a nonzero vector that is independent from the original $|0\rangle$ , so we get another " copy " of the ground state . here , $g$ still commutes with the hamiltonian $h$ so these copies have the same energy – we have degeneracy . for example , if the electroweak symmetry were global , just to simplify things , the ground state of the higgs field could have vev $ ( 0,246 ) $ in the units of gev , but it could have any other vev with the same magnitude , too . so there are infinitely many vacua . ( in gauge theory , they are made equivalent , but if we spontaneously break a global symmetry , they are states related by the symmetry and therefore having the same energy , but distinct elements of the hilbert space . ) so the claim that the op is dissatisfied with really holds completely generally , by definition of the spontaneous symmetry breaking ! misunderstanding that the ground state of a spontaneously broken symmetric theory is degenerate is the misunderstanding of the basic idea of the spontaneous symmetry breaking . note that spontaneous symmetry breaking still effectively means that the " symmetry is broken for most practical purposes " because the " copies " mentioned above may be imagined to be physically identified and we split the hilbert space into " superselection sectors " each of which is built upon one copy of the ground state . the action of the symmetry generator on any excited state gives us a vector from another superselection sector which can not be identified with zero so from the perspective of a single superselection sector , the symmetry is simply broken . none of these questions and discussions about explicit vs spontaneous symmetry breaking has anything to do with the time-reversal symmetry ( or the absence of it ) which is just another symmetry ( one given by an antiunitary transformation , so some of the comments above would not apply to this symmetry ) . both systems with and without time-reversal symmetry satisfy the claim that the ground state is degenerate if a symmetry is spontaneously broken .
a mixed state is mathematically represented by a bounded , positive trace-class operator with unit trace $\rho : \cal h \to \cal h$ . here $\cal h$ denotes the complex hilbert space of the system ( it may be nonseparable ) . the set of mixed states $s ( \cal h ) $ is a convex body in the complex linear space of trace class operators $b_1 ( \cal h ) $ which is a two-side $*$-ideal of the $c^*$-algebra of bounded operators $b ( \cal h ) $ . convex means that if $\rho_1 , \rho_2 \in s ( \cal h ) $ then a convex combination of them , i.e. $p\rho_1 + q\rho_2$ if $p , q\in [ 0,1 ] $ with $p+q=1$ , satisfies $p\rho_1 + q\rho_2 \in s ( \cal h ) $ . two-side $*$-ideal means that linear combinations of elements of $b_1 ( \cal h ) $ belong to that space ( the set is a subspace ) , the adjoint of an element of $b_1 ( \cal h ) $ stays in that space as well and $ab , ba \in b_1 ( \cal h ) $ if $a\in b_1 ( \cal h ) $ and $b \in b ( \cal h ) $ . i stress that , instead , the subset of states $s ( \cal h ) \subset b_1 ( \cal h ) $ is not a vector space since only convex combinations are allowed therein . the extremal elements of $s ( \cal h ) $ , namely the elements which cannot be decomposed as a nontrivial convex combinations of other elements , are all of the pure states . they are of the form $|\psi \rangle \langle \psi|$ for some unit vector of $\cal h$ . ( notice that , since phases are physically irrelevant the operators $|\psi \rangle \langle \psi|$ biunivocally determine the pure states , i.e. $|\psi\rangle$ up to a phase . ) the space $b_1 ( \cal h ) $ and thus the set $s ( \cal h ) $ admits at least three relevant normed topologies induced by corresponding norms . one is the standard operator norm $||t||= \sup_{||x||=1}||tx||$ and the remaining ones are : $$||t||_1 = || \sqrt{t^*t} ||\qquad \mbox{the trace norm}$$ $$||t||_2 = \sqrt{||t^*t||} \qquad \mbox{the hilbert-schmidt norm}\: . $$ it is possible to prove that : $$||t|| \leq ||t||_2 \leq ||t||_1 \quad \mbox{if $t\in b_1 ( \cal h ) $ . }$$ moreover , it turns out that $b_1 ( \cal h ) $ is a banach space with respect to $||\cdot||_1$ ( it is not closed with respect the other two topologies , in particular , the closure with respect to $||\cdot||$ coincides to the ideal of compact operators $b_\infty ( \cal h ) $ ) . as $s ( \cal h ) $ is closed with respect to $||\cdot ||_1$ , it is a complete metric space with respect to the distance $d_1 ( \rho , \rho' ) := ||\rho-\rho'||_1$ . when $dim ( \cal h ) $ is finite the three topologies coincide ( though the norms do not ) , as a general result on finite dimensional banach spaces . concerning your last question , there are many viewpoints . my opinion is that a density matrix is physical exactly as pure states are . it is disputable whether or not a mixed state encompasses a sort of physical ignorance , since there is no way to distinguish between " classical probability " and " quantum probability " in a quantum mixture as soon as the mixture is created . see my question classical and quantum probabilities in density matrices and , in particular luboš motl 's answer . see also my answer to why is the application of probability in qm fundamentally different from application of probability in other areas ? addendum . in finite dimension , barring the trivial case $dim ( {\cal h} ) =2$ where the structure of the space of the states is pictured by the poincaré-bloch ball as a manifold with boundary , $s ( \cal h ) $ has a structure which generalizes that of a manifold with boundary . a stratified space . roughly speaking , it is not a manifold but is the union of ( riemannian ) manifolds with different dimension ( depending on the range of the operators ) and the intersections are not smooth . when the dimension of $\cal h$ is infinite , one should deal with the notion of infinite dimensional manifold and things become much more complicated .
aeroplanes fly by thrusting air downwards and by thus being borne up by the newton 's-third-law begotten upwards reaction force of the downthrusted air on the aeroplane . there are many excellent answers to the physics se question " what really allows airplanes to fly ? " that you should read . but basically the simplest estimates arise from calculating the ram pressure thrust upwards on the aeroplane given the above principle . the variables you need to know are density of the air at the height , the relative speed of the aeroplane to the air , the angle of attack that the wing makes with the velocity vector of the air relative to a frame comoving with the aeroplane and the scale factor that yields the effective surface area of the wing - which at subsonic speeds is considerably larger than the wing itself because the disturbance to the fluid flow pattern that arises from the wing is felt over a region that is considerably bigger than the wing . the last variable - effective area - can also be expressed as the wing 's coefficient of lift . to illustrate these points , we can do a back of the envelope estimation of ram pressure in this case : see my drawing below of a simple aerofoil with significant angle of attack being held stationary in a wind tunnel . this is the kind of analysis you should do to get an idea of your specific situation . your air density is going to be rather less than that for the following calculation ( commercial jetliners reach their top speed at heights of about 8000m ) : lets suppose the airflow is deflected through some angle $\theta$ radians to model an aeroplane 's attitude ( not altitude ! ) on its last approach to landing or as it takes off , flying at $300\mathrm{km\ , h^{-1}}$ airspeed or roughly $80\mathrm{m\ , s^{-1}}$ . i have drawn it with a steep angle of attack . air near sea-level atmospheric pressure has a density of about $1.25\mathrm{kg\ , m^{-3}}$ ( molar volume of $0.0224\mathrm{m^{-3}} ) $ . the change in momentum diagram is shown , whence the change in vertical and horizontal momentum components are ( assuming the speed of flow stays roughly constant ) : $$\delta p_v = p_b \sin\theta ; \quad\quad\delta p_h = p_b \ , ( 1-\cos\theta ) $$ at the same time , the deflecting wing presents an effective blocking area to the fluid of $\alpha\ , a\ , \sin\theta$ where $a$ is the wing 's actual area and $\alpha$ the scale factor to account for the fact that in the steady state not only fluid right next to the wing is distrubed so that the wing 's effective area will be bigger than its actual area . therefore , the mass of air deflected each second is $\rho\ , \alpha\ , a\ , v\ , \sin\theta$ and the lift $l$ and drag $d$ ( which force the engines must afford on takeoff ) must be : $$l = \rho\ , \alpha\ , a\ , v^2\ , ( \sin\theta ) ^2 ; \quad\quad d = \rho\ , \alpha\ , a\ , v^2\ , ( 1-\cos\theta ) \ , \sin\theta$$ if we plug in an angle of attack of 30 degrees , assume $\alpha = 1$ and use $a = 1000\mathrm{m^3}$ ( roughly the figure for an airbus a380 wing area ) , we get a lifting force $l$ for $\rho = 1.25\mathrm{kg\ , m^{-3}}$ and $v = 80\mathrm{m\ , s^{-1}}$ of 200 tonne weight . this is rather less than the takeoff weight of a fully laden a380 airbus ( which is 592 tonnes , according to the a380 wikipedia page ) but it is an astonishingly high weight just the same and within the right order of magnitude . we see that the wing 's effective vertical cross section is bigger than the actual wing by a factor of 2 to 3 . this is not surprising at steady state , well below speed of sound flow : the fluid bunches up and the disturbance is much bigger than just around the wing 's neighbourhood . so , plugging in an $\alpha = 3$ ( given the experimental fact that the a380 can lift off at 592 tonnes gross laden weight ) , we get a drag $d$ of 54 tonne weight ( 538kn ) - about half of the airbus 's full thrust of 1.2mn , so this ties in well with the airbus 's actual specifications , given there must be a comfortable margin to lift the aeroplane out of difficulty when needed .
the two resistors are in parallel . this means that at $a$ the current splits between them relating to their reistance . so the current throw the top resistor is $3\ , a$ and throw the bottom resistor is $1 \ , a$ . if we use kirchhoff 's current law which states , that in any node ( like $a$ ) the current flowing into the node is equal to the current flowing out of it . you have $1+3=4\ , a$ flowing out of node $a$ and thus must have $4\ , a$ flowing into the node .
one of the standard texts for this kind of thing ( the quantum mechanics of lasers , without all the technical details you had need to know to design a real one ) is loudon 's quantum theory of light . i have got the 2nd edition , i think he is up to the 3rd . in my edition he does the " pre-quantum " explanation of lasers ( i.e. . in terms of einstein a and b coefficients ) in about 40 pages . over the next 200 pages or so he does the fully quantum treatment of light ( quantization of the field , creation and annihilation operators , etc ) and then revisits the laser . the first 40 pages should cover the " clearly written description " criteria , and - as far as it goes - it is relatively rigorous as well .
let the minkowski metric $\eta_{\mu\nu}$ in $d+1$ space-time dimensions be $$\tag{1}\eta_{\mu\nu}~=~{\rm diag} ( 1 , -1 , \ldots , -1 ) . $$ let the lie group of lorentz transformations be denoted as $o ( 1 , d ; \mathbb{r} ) =o ( d , 1 ; \mathbb{r} ) $ . a lorentz matrix $\lambda$ satisfies ( in matrix notation ) $$\tag{2} \lambda^t \eta \lambda~=~ \eta . $$ here the superscript "$t$" denotes matrix transposition . note that the eq . ( 2 ) does not depend on whether we use east-coast or west-coast convention for the metric $\eta_{\mu\nu}$ . let us decompose a lorentz matrix $\lambda$ into 4 blocks $$\tag{3} \lambda ~=~ \left [ \begin{array}{cc}a and b^t \cr c and r \end{array} \right ] , $$ where $a=\lambda^0{}_0$ is a real number ; $b$ and $c$ are real $d\times 1$ column vectors ; and $r$ is a real $d\times d$ matrix . now define the set of orthochronous lorentz transformations as $$\tag{4} o^{+} ( 1 , d ; \mathbb{r} ) ~:=~\{\lambda\in o ( 1 , d ; \mathbb{r} ) | \lambda^0{}_0 &gt ; 0 \} . $$ the proof that this is a subgroup can be deduced from the following string of exercises . exercise 1: prove that $$\tag{5} |c|^2~:= ~c^t c~ = ~a^2 -1 . $$ exercise 2: deduce that $$\tag{6} |a|~\geq~ 1 . $$ exercise 3: use eq . ( 2 ) to prove that $$\tag{7} \lambda \eta^{-1} \lambda^t~=~ \eta^{-1} . $$ exercise 4: prove that $$\tag{8} |b|^2~:= ~b^t b~ = ~a^2 -1 . $$ next let us consider a product $$\tag{9} \lambda_3~:=~\lambda_1\lambda_2$$ of two lorentz matrices $\lambda_1$ and $\lambda_2$ . exercise 5: show that $$\tag{10} b_1\cdot c_2~:=~b_1^t c_2~=~a_3-a_1a_2 . $$ exercise 6: prove the double inequality $$\tag{11} -\sqrt{a_1^2-1}\sqrt{a_2^2-1} ~\leq~ a_3-a_1a_2~\leq~ \sqrt{a_1^2-1}\sqrt{a_2^2-1} , $$ which may compactly be written as $| a_3-a_1a_2|~\leq~\sqrt{a_1^2-1}\sqrt{a_2^2-1}$ . exercise 7: deduce from the double inequality ( 11 ) that $$\tag{12} a_1\neq 0 ~\text{and}~ a_2\neq 0~\text{have same signs} \quad\rightarrow\quad a_3&gt ; 0 . $$ $$\tag{13} a_1 \neq 0~\text{and}~ a_2\neq 0~\text{have opposite signs} \quad\rightarrow\quad a_3&lt ; 0 . $$ exercise 8: use eq . ( 12 ) to prove that $o^{+} ( 1 , d ; \mathbb{r} ) $ is stabile/closed under the multiplication map . exercise 9: use eq . ( 13 ) to prove that $o^{+} ( 1 , d ; \mathbb{r} ) $ is stabile/closed under the inversion map . the exercises 1-9 show that the set $o^{+} ( 1 , d ; \mathbb{r} ) $ of orthochronous lorentz transformations form a subgroup . $^1$ $^1$a mathematician would probably say that eqs . ( 12 ) and ( 13 ) show that the map $$o ( 1 , d ; \mathbb{r} ) \quad \stackrel{\phi}{\longrightarrow}\quad \{\pm 1\}~\cong~\mathbb{z}_2$$ given by $$\phi ( \lambda ) ~:=~{\rm sgn} ( \lambda^0{}_0 ) $$ is a group homomorphism between the lorentz group $o ( 1 , d ; \mathbb{r} ) $ and the cyclic group $\mathbb{z}_2$ , and a kernel $$ {\rm ker} ( \phi ) ~:=~\phi^{-1} ( 1 ) ~=~o^{+} ( 1 , d ; \mathbb{r} ) $$ is always a normal subgroup .
moseley , the physicist who ' fixed ' the periodic table at the start of the 20th century , did it by measuring x-ray spectra . the energy of the $k_\alpha$ x-ray emission line is proportional to $ ( z^2 - 1 ) $ , where $z$ is the atomic number . the results of moseley 's experiment fitted his formula so perfectly that he was able to predict the existence of several as-yet-undiscovered elements by looking at the gaps in his graphs . he also re-ordered the controversial placement of nickel and cobalt . sadly he was killed in world war one before he was able to become the great scientific figure he surely would have been .
neither of the interaction terms would appear were the atom not present , and you cannot simply set $\hat{p}=0$ as it is a dynamically fluctuating quantum variable . therefore , both of the terms must describe scattering of the light from the atom . roughly speaking , the term $\sim \hat{p}\hat{a}$ encodes inelastic scattering , while the term $\sim \hat{a}^2$ encodes elastic scattering . in first order perturbation theory , transitions due to the first term are controlled by the matrix elements $$ \mathcal{m}_1 = \langle m , e_f| \hat{p}\hat{a} |n , e_i\rangle = \langle m|\hat{a}|n\rangle\langle e_f|\hat{p}|e_i\rangle . $$ here , $n , m$ are the initial and final number of photons in the light field ( assume for simplicity there is only one mode , obviously in a real calculation you will also have to consider different wavevectors and polarisation states of the field ) , while $e_{i , f}$ are the initial and final atomic eigenstates . since the operator $\hat{a}$ is linear in annihilation/creation operators , the matrix element is zero unless $m = n\pm 1$ . meanwhile , since atomic eigenstates have definite parity , the matrix element is zero unless $|e_f\rangle \neq |e_i\rangle$ ( or more precisely , it is zero unless the two states have opposite parity ) . so to first order , the interaction vertex $\hat{p}\hat{a}$ describes processes in which a photon is absorbed or emitted by the atom , changing its internal state . at higher orders in perturbation theory you will see processes where the atom 's internal state changes multiple times . this includes , for example , an inelastic scattering process where the atom absorbs a photon at one frequency and an electron becomes excited , then the electron drops in energy by emitting a photon at another frequency . ( you could also have elastic scattering processes where the atom ends up in the same state , hence the " roughly speaking " disclaimer above . ) matrix elements of the second interaction vertex look like $$\mathcal{m}_2 = \langle m , e_f|\hat{a}^2 |n , e_i\rangle = \langle m| \hat{a}^2 |n\rangle \langle e_f| e_i\rangle . $$ therefore , these matrix elements vanish unless the initial and final state of the atom is the same . however , the vertex is now quadratic in annihilation/creation operators , so you will see , for example , elastic scattering processes where a photon is absorbed and then re-emitted at the same frequency ( but different direction , in general ) . at higher orders you will start to see really interesting optical non-linearities where , for example , two photons are absorbed , and then two photons are re-emitted at different frequencies . hopefully this explains your idea about " perturbing the hamiltonian of the free em field " . the strong interaction between light and electrons can produce an effective interaction in the presence of matter ( aka optical non-linearity ) between otherwise non-interacting photons . finally , do not forget that the full interaction operator contains both contributions so at higher orders in perturbation theory you will also get cross terms between $\hat{p}\hat{a}$ and $\hat{a}^2$ .
the function $\sin 3\theta$ on the unit sphere is not an eigenfunction of the laplacian on the sphere , i.e. the angular part of the laplacian , i.e. of $l^2$ , so it is not convenient a basis vector in problems whose hamiltonian involves the laplacian . the function $\sin 3\theta$ may be written as a combination of spherical harmonics $y_{lm}$ with many different values of $ ( l , m ) $ so it is a " mixture " of multipoles of different " rank " . for the more natural basis of functions on the sphere that may see as basis vectors , see the table of spherical harmonics https://en.wikipedia.org/wiki/table_of_spherical_harmonics for example , the spherical harmonics $y_{3 , \pm 3}$ are proportional to $$ y_{3 , \pm 3} \sim \exp ( \pm 3i\phi ) \sin^3 \theta $$ which is very similar to $\sin 3\theta$ but has the extra $\phi$-dependence . similarly , one may look at the function $y_{30}$ which is similar to $\sin 3\theta$ but prefers cosines and so on . either $\sin^3 \theta$ or $\cos^3\theta$ ( check it ! ) without any $\phi$-dependence is a combination of $y_{30}$ and $y_{10}$ . once one realizes why the spherical harmonics are the preferred , more natural basis , we may carefully discuss the spherical harmonics ' association with the multipole expansion . for example , we learn that $y_{3 , m}$ for any $m$ , including the functions similar to yours above , are associated with octupoles , not " tripoles " ! more generally , $y_{\ell m}$ is the angular part of the $2^\ell$-pole . the powers of two are a natural way to describe the terms in the multipole expansions for reasons explained elsewhere , e.g. here : stackexchange-url in the multipole terminology , a " tripole " would correspond to a triplet ( e . g . vertices of a triangle ) of charges . if their total charge would be nonzero , there would be a leading " monopole " term . if the total charge cancelled , the system of 3 charges would still have a dipole moment . unless the three ( nonzero ) charges would lie on the same line , the dipole moment could not be canceled .
the potential in the schroedinger equation must be single-valued and almost everywhere defined , so to have your problem weel-defined you must first decide which of several solutions is to be taken . apart from that , one does not need an explicit potential . if one calculates solutions numerically it is enough that one can compute the potential numerically at any given $x$ .
i think the problem is a general tensor analysis one rather semiconductor physics ( gaas ) , since no particular values of any d-symbol is of any importance ? anyway , here is my effort to produce an answer . the piezoelectric coefficient $d_{ijk}=d^{ [ 110 ] }_{ijk}$ is a third rank cartesian tensor which transforms from the reference frame $ [ x_1 , x_2 , x_3 ] $ to $ [ \bar {x}_1 , \bar {x}_2 , \bar {x}_3 ] $ giving $\bar {d}_{pqr}= d^{ [ \bar110 ] }_{pqr}$ according to the general rule : ${d}^{110}_{ijk} =\sigma \bar {d}^{\bar {1}10}_{pqr}a_{pi}a_{qj}a_{prk} $ where $a_{\lambda\mu}$ are the cosines between coordinates in the two frames , and the summation is over repeated indices . in your particular coordinate transformation $ [ - x , y , z ] $ to $ [ x , y , z ] $ which transforms $\bar {d}_{pqr}$ to $d_{ijk}$ you need to bear in mind that $a_{pi}=\delta_{pi}$ etc so you get the general relationship with the $\delta$ symbols multiplying each other in each term of the summation . they are not exponents ( powers ) to base ( -1 ) . the expression you have written does not seem to give the correct relationship between $d^{ [ 110 ] }_{ijk}$ and $d_{pqr}=d^{ [ \bar {1}10 ] }_{pqr}$ . i hope this helps .
i believe that evaporative ionization is the opposite effect of what you are looking for , per the general discussion on ion processes in evaporated droplets . this constrains the range of charge-transfer in droplets to the triboelectric effect , as you mentioned . the kelvin generator , including discussion about charge formation that exceed models , may be more than triboelectric , due to ground effects - which is to say that charge may come from other sources that the system of droplet and atmosphere . the only way i could see a kelvin generator involve electrospray ionization is through corrosion or dissolving of the conductive cups that collect the water . the amsci group suggests that the charge arrives from slowing the water droplets ' fall . their model could align with the idea that ion-transfer from droplet to system occurs via coulomb fission .
information about double slit experiment : in double slit experiment , light behaves like a wave when it is not observed ( in simple words you do not know in which slit photon passed through ) , because it is a wave , when two waves meet each other they will interferece and on detector screen you will see interference pattern like this : but if you will put detectors on the slits , you will observe in which slit each photon passed though , and because of that interference pattern will disappear , because by observing light , it no longer behaves like a wave ( in other words it is wavefunction has collapsed ) instead it behaves like a particle ( photon ) so , on a detector screen you will no longer see interference pattern example of quantum eraser : easiest way to do that is to put polarizators on each slit which are opposite of each other ( in other words first polarizator will pass photons which have polarization $\rightarrow$ and second will pass photons which have polarization $\uparrow$ ) : because photons which have passed polarizators have different polarizations they can no longer interference ( see : interference of polarized light ) , so on detector screen you will no longer see interference pattern links for more info : for more information about quantum eraser see this page , this page and this page for a video about quantum eraser see this page for more info about interference see this page
here is the home page for the gufm model website . it also includes a link to a freely available pdf of the modern reference . also of interest is the noaa website . gufm model homepage noaa page note that this model is based upon catalogs of geomagnetic field measurements , these did exist prior to 1800 - although as you would expect their quality and quantity decreases the farther back in time you go . the model is trying to solve an inverse problem . a ' solution ' represents the model that provides the best fit to the available observations . it therefore may be considered our best empirical knowledge of the historic geomagnetic field based upon our limited observations . results such as the figure that you have posted would be more useful if , in addition to the most-likely values , they also displayed an objective measure of uncertainty ( such as confidence limits ) in the figure . to clarify these terms a bit - this study is based on historic records ( mostly ship 's logs ) . empirical information about the pre-historic geomagnetic field over geological time-scales is available to the science of paleomagnetism through the application of our understanding of rock magnetism . rock magnetism is the study of the magnetic properties of rocks , sediments and soils . the field of rock magnetism arose out of the need in paleomagnetism to understand how rocks record the earth 's magnetic field .
it is the result of a dependence of the pressure with growing depth , due to the gravitational field ( i.e. . the weight of the water ) . you may do an easy calculation with some simple geometrical form , e.g. a cylinder totally submerged in water , to quickly understand how it works . the force due to pressure in each surface element of the curved wall of the cylinder is proportional to the depth of that element , and has the normal direction to the wall , i.e. towards the axis of the cylinder . after an easy integration in polar coordinates , you can see that the resultant force points upwards . that is because the forces in the upper parts are smaller that the ones near the more deeply submerged part of the cylinder . a surprising conclusion is that a golf ball submerged in a tank of water in the space station , would not go upwards . . . or that the bubbles in a coke in the hands of an astronaut remain where they origin . . . i would love to see that .
yes , applying an electric field does create a ph gradient and in fact you can observe this simply by adding a suitable indicator to your system . for example see the section demonstration of ph gradient formation in this article .
dear rajesh , the goal as well as main achievement of string theory is not to " achieve spacetime quantization " whatever this phrase is supposed to mean but to allow calculations about spacetime that are compatible with the postulates of quantum mechanics . it is not the same thing . the statement that " spacetime is quantized " , whatever it means , is just a working hypothesis , not a holy grail that should be " achieved " . instead , what is needed is to have a theory that is a quantum theory as a whole , i.e. one that agrees with the uncertainty principle , probabilistic character of predictions , observables ' being expressed by linear operators on the hilbert space , and so on . on the other hand , the spacetime is an approximate concept in string theory . at distances much longer than the characteristic distance scale of string theory , the string scale ( or the related gravitational planck scale ) , classical general relativity is a good approximation - and some of its aspects remain exact at all distance scales . at distances comparable to the string scale ( or planck scale ) , entirely new set of physical phenomena take over . it is not true that the only difference of these new phenomena from classical general relativity is that " spacetime is quantized " . and in fact , it is not true in any sense and it cannot be true in a consistent theory of quantum gravity that spacetime becomes discrete . and in fact , geometric quantities - while remaining continuous whenever it makes sense to use them - may be shown in string theory to be invalid variables to describe physics at very short distances . to say the least , they are incomplete . so if your question is how string theory confirms the prejudices and beliefs that the spacetime quantities survive as good variables up to arbitrarily short distance scales and/or that they become discrete , the answer is that string theory is a way to prove that both of these prejudices are dead wrong . if your question is which quantum phenomena affecting spacetime are predicted by string theory , it is a valid question but it is too broad and a proper answer would require to review all of string theory - because all important insights of string theory , in some sense , show the consequences of the co-existence of quantum mechanics with a dynamical spacetime .
just write the transformation in its differential form and then rearrange . $$ d\eta = \frac{u_e}{\sqrt{\int_0^x\rho_e u_e \mu_e dx}}\rho dy $$ rearranging and integrating yields : $$ y = \frac{\sqrt{\int_0^x\rho_e u_e \mu_e dx}}{u_e}\int_0^\eta \frac{1}{\rho}d\eta $$
cute question ! for a neutrino with mass $m$ and energy $e\gg m$ , we have $v=1-\epsilon$ , where $\epsilon\approx ( 1/2 ) ( m/e ) ^2$ ( in units with $c=1$ ) . icecube has detected neutrinos with energies on the order of 1 pev , but that is exceptional . for neutrinos with mass 0.1 ev and an energy of 1 pev , we have $\epsilon\sim10^{-32}$ . the time of flight for high-energy photons has been proposed as a test of theories of quantum gravity . a decade ago , lee smolin was pushing the idea that loop quantum gravity predicted measurable vacuum dispersion for high-energy photons from supernovae . the actual results of measurements were negative : http://arxiv.org/abs/0908.1832 . photons with energies as high as 30 gev were found to be dispersed by no more than $\sim 10^{-17}$ relative to other photons . what this tells us is that interactions with the interstellar medium must cause $\epsilon \ll 10^{-17}$ , or else those interactions would have prohibited such an experiment as a test of lqg . according to wp , the density of the interstellar medium varies by many order of magnitude , but assuming that it is $\sim 10^{-22}$ times the density of ordinary matter , we could guess that it causes $\epsilon\sim 10^{-22}$ . this would be consistent with the fact that it was not considered important in the tests of vacuum dispersion . for a neutrino with a mass of 0.1 ev to have $\epsilon\sim 10^{-22}$ , it would have to have an energy of 10 gev . this seems to be within but on the high end of the energy scale for radiation emitted by supernovae . so i think the answer is that it really depends on the energy of the photon , the energy of the neutrino , and the density of the ( highly nonuniform ) interstellar medium that the particles pass through .
the principle of relativity says that we can analyze a physical situation from any reference frame , as long as it moves with some constant speed relative to a known inertial frame . thus , the ion drive does not find it more difficult to accelerate the ship when the ship is " going fast " because the ion drive cannot physically distinguish going fast from going slow . however , if the ion drive is going fast in the reference frame of earth , then when the ion drive burns , say 1 kg of fuel , it picks up less speed in the earth frame than it does in the rocket frame due to the relativistic velocity addition law . that velocity addition law is just the angle-addition law for the hyperbolic tangent . so , suppose the ship accelerates by shooting individual ions out the back . each time it does this , it accelerates the same amount from its own comoving frame . then from an earth frame , the $\textrm{arctanh}$ of the rocket 's speed increases by the same amount each time . if , as a function of the proper time $\tau$ experienced on the rocket , the acceleration of the rocket is $a ( \tau ) $ in a comoving frame , there is a quantity called the rapidity of the rocket which increases the way velocity does in newtonian mechanics . the rapidity $\theta$ will be $\theta ( \tau ) = \int_0^\tau a ( \tau ) d\tau$ , and the velocity is then $v ( \tau ) = \tanh\theta$ . specifically , if $a = g$ , the velocity is $$v ( \tau ) = \tanh ( g\tau ) $$ when one year of time has passed on the rocket , its velocity relative to earth will be $\tanh ( 1.05 ) = 0.78$ , or 78% the speed of light . the limit of the $\tanh$ function is one as $\tau \to \infty$ , so the rocket never gets to light speed . a more important limiting factor is the fuel . if the rocket carries all its fuel , then once it burns through it all , it can not go any more . fusion is not a way around this because by $e=mc^2$ there is a limited energy you can get from a given mass of fuel . if a fraction $f$ of the rocket is fuel , when the fuel is all burned , the momentum of the rocket will be $\gamma m ( 1-f ) \beta$ , with $m$ the original mass . the energy of the rocket is $\gamma m ( 1-f ) $ . similar relations hold for the fuel . the conservation of momentum and energy give $$m = \gamma m ( 1-f ) + e_{fuel}$$ $$0 = \gamma m \beta ( 1-f ) + p_{fuel}$$ $e_{fuel}$ and $p_{fuel}$ are the energy and momentum of the fuel after burning . solving for $\beta$ gives $$\beta = \frac{-p_{fuel}}{m - e_{fuel}}$$ the minus sign shows that the fuel and rocket go opposite directions . to maximize $\beta$ , we want to make $p_{fuel}$ as large as possible subject to a fixed $e_{fuel}$ . this means that we want the speed of the fuel as high as possible , so assume the fuel is massless with $\beta_{fuel} = 1$ and $p_{fuel} = -e_{fuel}$ . plugging this into the previous equations and doing some algebra , i got $$\beta = \frac{1 - ( 1-f ) ^2}{1 + ( 1-f ) ^2}$$ even if half the rocket 's original mass were fuel , it would only get to 3/5 the speed of light .
let $g$ be a group , e.g. a finite group or a lie group . then there exists the notion of a group action $g\times x\to x$ , where $x$ is a set . the set $x$ does not necessarily have to be a vector space . it could e.g. be a manifold . and even if $x$ has vector-space structure , the group action could be non-linearly realized , i.e. , a group element $g\in g$ is represented by a non-linear operator $t_g:x\to x$ . non-linear realizations pop up all over the place in modern physics . for instance , in nonlinear realization of supersymmetry , or in nonlinear realization of the conformal group . example : let the lie group $g=gl ( 2 , \mathbb{c} ) $ of invertible $2\times2$ matrices $$\tag{1} a~=~\begin{pmatrix}a and b\\c and d \end{pmatrix} , \qquad \det ( a ) \neq 0 , $$ act on the complex plane $\mathbb{c}$ ( which , by the way , is a vector space ) as $$\tag{2} a . z ~:=~\frac{az+b}{cz+d} , \qquad ( ab ) . z ~=~ a . ( b . z ) ~ . $$ in this way , matrices get non-linearly represented as meromorphic functions . the subgroup $sl ( 2 , c ) $ is the global conformal group in two space-time dimensions , which e.g. plays a fundamental role in the world-sheet description of string theory . finally , let us mention that in mathematics there exists a generalization of the notion of a $\mathbb{f}$-vector space , where the field $\mathbb{f}$ is replaced by a ring $r$ . it is known as an $r$- module .
nothing - that is the correct definition . one little caveat is that small systems are usually in contact with a larger system with a temperature that is more easily controlled or measured ( the " heat bath" ) , and $t$ usually stands for the temperature of the heat bath rather than the small system itself . however , this does not make a lot of difference in practice , because the small system will very rapidly attain the same temperature as the heat bath anyway .
the ball is deformed while bouncing off . in theory , this can be modelled as an entirely elastic process as a relatively good approximation , however , it actually is not , as some energy is lost in the process and radiated away as heat ( try deforming a ball a few hundred times , it will heat up ) . the process is therefore not entirely elastic , which reduces the kinetic energy of the ball . additionally , a number of other forces affect the ball , listing those mentioned above again for completenes and ordered roughly by the magnitude of the effect : energy lost due to inelasticity of the ball-earth interaction ( ball heats up ) friction of the ball with the air , causing it to slow down friction of the ball with the ground ( "stuck to the ground" ) roughness of ground causing the ball to start spinning or change direction forces stemming from the fact that the earth rotates , although this should mostly affect horizontal velocity momentum transferred to earth
a knot always requires the rope in the knot to be curved . this increases the stress on the outside of the curved bit of rope , and decreases the stress in the inside . this increase in the stress in a knot means the rope breaks at a lower overall stress than a straight rope would .
yes . let 's assume that the charge density is fixed relative to the surface of a sphere of radius $r$ , then spinning the sphere with angular velocity $\vec\omega = \omega \hat{z}$ would create a surface charge density $\vec k$ given by $$ \vec k ( \theta , \phi ) = \omega r\sin\theta\sigma ( \theta , \phi ) \hat\phi ( \theta , \phi ) $$ where $\theta$ and $\phi$ are the polar and azimuthal spherical coordinates .
generally both formulations ( largangian and hamiltonian ) are equivalent , but in your case , if $\theta$ is small , you have a simplified equation for $p$ and you can use a solution ansatz like $e^{i\omega t}$ for both $p$ and $\theta$ . to draw a path in the phase space , you have to solve the equations and/or manage to express $p ( \theta ) $ or $\theta ( p ) $ .
the amount of force generated by a stirling engine for a particular temperature gradient is different for each engine and depends on things like the particular engine configuration , materials used , volume of the pistons , stroke length , . . . as for the speed or frequency since its cyclic , this will also vary by just as many parameters . in general the larger the heat difference available the ' better ' an engine will run , by how much is something for the individual engine product manuals . for a given temperature gradient ( say between an underground volcanic heat source and the open air ) a trade off needs to be made between speed and force . an engine could be designed to be more powerful by making the pistons and cylinders really large to maximize the expanded gases pressing on the piston , or really fast by making pistons and cylinders much smaller so that the smaller amount of gas inside the chamber finishes expanding much faster for a new cycle to begin .
one perspective ( heh ) involves the following relation among position vectors : $$\vec{r}_{a\rightarrow c} = \vec{r}_{a\rightarrow b} + \vec{r}_{b\rightarrow c} . $$ these position vectors can be for anything ; object $a$ could be a house , object $b$ an ant , and object $c$ a leaf on the river . here 's a diagram to help : so if you want to know the position of object $c$ relative to object $a$ ( the bold dark arrow ) , you just have to know the position of some other object $b$ relative to those others . to answer your question , you can apply this same idea to the solar system : $$\vec{r}_{\mathrm{sun} \rightarrow x} = \vec{r}_{\mathrm{sun} \rightarrow \mathrm{earth}} + \vec{r}_{\mathrm{earth} \rightarrow x} . $$ or in pictures : the position of planet $x$ relative to the sun ( bold dark arrow , which is what we want ) can be found if we know earth 's position relative to the sun and planet $x$ 's position relative to earth . in this way , measurements of a planet 's position as measured from here on earth can be used to get a map of the solar system . . there is the added complication of knowing distance to planets and coming up with a convenient coordinate system in order to actually come up with values for these position vectors . others may have better information on that .
well at the most fundamental level , the index of refraction of a material is defined as $ n = \sqrt {\epsilon \mu} $ where $\epsilon $ is the electric permittivity and $\mu$ is the magnetic permeability of the material . this arises from the solution of maxwell 's equations in a medium . also arising from the wave equation , which can be derived from maxwell 's equations , is that the index of refraction is the speed of light in vacuum , $c$ divided by the speed of light in the material $c_m$ . $ n ={ c \over c_m} $ worth noting is that since $c_m$ is always less than $c$ , the index of refraction is always greater than 1 . now the phase velocity for an electromagnetic wave of angular frequency $\omega$ is given by $v_p = {\omega \over k}$ where $k= {2\pi \over\lambda_m}$ is the magnitude of the wave vector and $\lambda_m$ is the wavelength in the medium . so after a little algebra , we find that the wave vector and index of refraction are related by $k ={ n\omega \over c}$ where does all this come into play in refraction and snell 's law ? well , it is the wave vector that comes into play in satisfying the boundary conditions on the electric ( and magnetic ) fields at the interface between two media . to see this , let 's look at the simple case of a plane wave of monochromatic light incident on the interface between two media with indices of refraction $n_1$ and $n_2$: in this simple case , considering the boundary conditions on the electric field at the interface is sufficient to derive snell 's law . the boundary condition is given by equation ( 1 ) in the diagram , namely that the incident and reflected electric fields minus the transmitted electric field must be zero , or equivalently that the total electric field at the interface must be continuous . since we defined y=0 as the plane of the interface , this boundary condition must hold for any value of x . this leads after a little algebra to equation ( 2 ) in the diagram . this equation depends only on the angles of incidence $\phi_{inc}$ and refraction $\phi_{tr}$ , the wave vector magnitudes in both media $k_1$ and $k_2$ , and the transmission and reflection coefficients $t$ and $r$ ( the fraction of energy that is transmitted into the new medium and reflected into the old medium respectively ) . by symmetry , $\phi_{inc} = \phi_{refl}$ . because the left side of equation 2 is independent of angle , so must the right side be . this leads to the term in the exponential being zero , which leads directly to snell 's law , using the relation between wave vector and index of refraction shown above . the group velocity never comes into play in the boundary conditions of refraction . it does come into play in the propagation of energy in the media ( as opposed to the fields ) , but that is another question .
remember that the theta term appears in an exponential $e^{i\theta n}$ inside the path integral . if $\theta n$ shifts by $2\pi n$ , for any integer $n$ , the exponential is unchanged , and all path integrals have the same value . the integral $n = \frac{1}{32\pi^2} \int f \wedge f$ is not arbitrary either . it is a topological invariant , and it is normalized so that it will be an integer . indeed , on flat $\mathbb{r}^4$ with your normalization , it is equal to $2 \nu$ , where $\nu \in \mathbb{z}$ is the instanton number . ( you can find the argument in weinberg , vol ii , p 450-2 . ) but for now let 's just take it as granted that that $n$ is always integer-valued . it follows then that shifting $\theta \to \theta + 2\pi m$ for any integer $m$ sends $e^{i\theta n}$ to $e^{i\theta n + i 2\pi m n} = e^{i\theta n}$ . which means that only the value of $\theta$ mod $2\pi$ affects physics . so when someone says that $\theta$ is small , they mean that $\theta$ mod $2\pi$ is small , as you guessed .
the equation $w_\text{net , ext}=k_f-k_i$ is only correct when the only form of energy being transformed is kinetic . if you have other forms of energy that change value , this equation will not work . if you want to look at the system of the two colliding objects , you are correct that $w=0$ ( though for a slightly different reason than what you stated ; the net force is zero , but this does not mean the net work by external forces is zero . ) a more encompassing equation than the one you are using is $$w_\text{net , ext}=\delta e_\text{tot}=\delta k + \delta e_\text{thermal} + \delta u_\text{potential}+\cdots . $$ so , since $w=0$ , the decrease in kinetic energy is accompanied by an increase in other forms of energy , such as thermal energy , acoustic , etc . ( not sure what the " etc . " is actually ) . to more directly address your concern , internal forces absolutely can and do affect kinetic energy , even if the work done by external forces is zero . but just knowing that $w_\text{net , ext}=0$ does not tell you how the internal energies transform ; only that the total sum is constant . ( above i assumed that heat $q$ added to the system is zero . )
you are measuring the quantity $x$ and you got results $+1,0 , -1$ and perhaps $+1 , -1$ again . assuming that your systematic error is zero , these numbers are randomly generated around the right value you want to know . that is why you want to estimate the right value as the average of the results you obtained . that is $$ \overline{x}= \frac{ ( -1 ) +0+ ( +1 ) }{3} = \frac{ ( -1 ) +0+ ( +1 ) + ( -1 ) + ( +1 ) }{5} = 0$$ so there is no doubt about the mean value . it is zero in both cases . however , you also want to know the error of $\overline{x}$ . that is calculated as the square root of the expectation value of $ ( x-\overline{x} ) ^2$ . because $\overline{x}=0$ , we just have the expectation value of $x^2$ in this case which is $$ \frac{ ( -1 ) ^2+0^2+ ( +1 ) ^2}{9} = \frac{2}{9} $$ in the case of three measurements or $$ \frac{ ( -1 ) ^2+0^2+ ( +1 ) ^2+ ( -1 ) ^2+ ( +1 ) ^2}{25} = \frac{4}{25} $$ there are no mixed terms because the individual deviations are independent . so the errors are $\sqrt{2}/3$ and $2/5$ , respectively . note that you had an error in the numerator as well . your results $1/\sqrt{3}$ or $1/\sqrt{5}$ would occur if there were 3 or 5 terms in the numerator equal to 1 i.e. 3 or 5 measurements equal to $\pm 1$ , respectively . but one of the measurements was , in both cases , equal to zero which reduces the variance and reduces the error from your incorrect $\sqrt{3/9}$ or $\sqrt{5/25}$ to $\sqrt{2/9}$ and $\sqrt{4/25}$ , respectively . yes , if you repeat the measurement many times , the statistical error will go down as $1/\sqrt{n}$ . the proof is de facto contained in the simple calculation above . because we are computing the average which has $1/n$ in it , this produces $1/n^2$ when squared and is not quite compensated by the numerator which is the sum of $n$ terms so it goes just like $n$ . so the expectation value of $ ( \delta x ) ^2$ goes like $1/n$ and $\sigma$ therefore goes like $1/\sqrt{n}$ . again , i should emphasize that there can be errors that are fixed for your gadget – the gadget persistently produces a result that is off by some unknown but always the same deviation . such errors are known as the systematic errors and they can not be diminished by repeating the measurement many times . the total error of your measurement is composed both of statistical errors and systematic errors . you usually want to repeat the experiments a sufficient number of times so that the statistical error drops to the level of the systematic error or slightly below it ; it does not make much sense to repeat the experiment too many times again because it will not reduce the total error , now dominated by the systematic error .
there has indeed been some work on relating the geometry of the state space to the limitations of the theory . first , work relating the local state space to the non-locality present in a theory developed by janotta et al . they consider the local state space to be a regular polygon . for a large number of sides , the non-locality tends towards tsirelson 's bound . that is , in the infinite limit , the state space is quantum and so , self-dual . the issue of self-duality of state space has been explored and means that the space of effects is isomorphic to the state space . relating this to the optimality of quantum state space one can think about reversible computation . in order to have a model of quantum computing that encompasses the circuit model and more general state spaces , one can develop reversible computing for all possible gpts . firstly , reversible computations in box-world are trivial as shown by gross et al . this means that reversible dynamics for box-world is limited to permutations and relabelling of data . if we want reversible dynamics on a less trivial level , e.g. the power to map from one bit to another bit , then this leads to self-duality as shown by mueller and ududec . since the self-duality in the previous paragraph indicates a trade-off in non-locality , they also speculate that this reversible computation limits non-locality . so they connect a computational principle to the structure of the state space , very much in the spirit of barrett 's paper .
why do we gauge-fix the path integral in the first place ? if we were doing lattice gauge theory , we did not need to gauge-fix . but in the continuum case , ( the hessian of ) the action for a generalized$^1$ gauge theory has zero-directions that lead to infinite factors when performing the path integral over gauge orbits . in a brst formulation ( such as , e.g. , the batalin-vilkovisky formulation ) of a generalized gauge theory , the gauge-fixing conditions can in principle depend on gauge fields , matter fields , ghost fields , anti-ghost fields , lagrange multipliers , etc . perturbatively , a necessary condition for a good gauge-fixing procedure is that the gauge-fixed hessian is non-degenerate ( in the extended field-configuration space ) . generically , the number of gauge-fixing conditions should match the number of gauge symmetries . for yang-mills theory with lie group $g$ , one needs ${\rm dim} ( g ) $ gauge-fixing conditions . one may check that for various standard gauges that only involve the gauge fields , it is not necessary to gauge-fix matter fields to achieve a non-degenerate hessian . -- $^1$ by the word generalized gauge theories , we mean gauge theories that are not necessarily of yang-mills type .
$e$ = $mc^2$ a much better expression is $e^2 = ( mc^2 ) ^2 + ( pc ) ^2$ , where $m$ is the " mass " ( also known as " intrinsic mass " , also known " rest mass " , but most physicists nowadays just use " mass" ) of the particle and $p$ is the particle 's momentum . this reduces to $e=mc^2$ in the special case of a particle with zero momentum , but it also reduces to $e=pc$ in the case of a particle such as a photon with zero mass . using $e=mc^2$ as a general expression implies a rather different concept of mass , that of relativistic mass . many physicists did indeed use the concept of relativistic mass early on in the development of relativity theory . at least initially , even einstein was in that camp . most of those physicists , einstein included , abandoned that concept for the concept of " rest mass . " there are just too many problems with the concept of relativistic mass . the concept of " rest mass " ( or " intrinsic mass " or just " mass" ) makes much more sense than does the concept of relativistic mass . note that the term " rest mass " is a bit contradictory for massless particles such as photons . there is no frame in which a photon is at rest . this apparent contradiction vanishes if you use the phrase " intrinsic mass " ( or just " mass" ) in lieu of " rest mass . " there are a few hangers-on amongst professional physicists who still prefer the concept of relativistic mass over rest mass . these physicists are now few and far between . eventually they will die , and the concept of relativistic mass will eventually die with them .
water waves are rather complicated , and the differential equations which describe them are call boussenesq equations . a tsunami is not a transverse wave . it is a pressure wave with a longitudinal mode . it also travels very fast at about 700km/hr . what happens is that this travels as a pressure wave in the open ocean , but when it reaches a continental shelf the wave is reflected partially upwards . this has the effect of converting it into a transverse wave as water moving along is now pushed upwards . this is a very nonlinear process and nontrivial to model . this pushing up of the water does initially cause water at the shore to recede outwards . the wave which seconds later reaches shore is much more slow moving , and a lot of that wave energy is converted into the towering wave front that sweeps in .
when we measure a red shift , what we are actually measuring is the absorption spectra of elements in the stars and dust clouds in the target galaxy . these absorption spectra have well known patterns of lines , and when red shifted the entire pattern moves to lower energy/longer wavelength . so you are correct that you can not measure the red shift from just one line , because you do not know where that line was originally , but you can do it when you measure a known pattern of many lines .
the objects such as $\hat \phi ( x , y , z , t ) $ in a qft are strictly speaking " operator distributions " . they differ from " ordinary operators " in the same way how distributions differ from functions . only if you integrate such operator distributions over some region with some weight $\rho$ , $$\int d^3 x\ , \hat\phi ( x , y , z , t ) \rho ( x , y , z , t ) =\hat o , $$ you obtain something that is a genuine " operator " . in a free qft , the state vectors may be built as combinations of states in the fock space – an infinite-dimensional harmonic oscillator . but you may also represent them via " wave functional " . much like the wave function in non-relativistic quantum mechanics $\psi ( x , y , z ) $ depends on 3 spatial coordinates , a wave functional depends on a whole function , $\psi [ \phi ( x , y , z ) ] $ . for each allowed configuration of $\phi ( x , y , z ) $ , there is a complex number . yes , one may also integrate over all classical functions $\phi ( x , y , z ) $ . there also exists a dirac delta-like object , the dirac " delta-functional " , and it is usually denoted $\delta$ , $$\int {\mathcal d}\phi ( x , y , z ) f [ \phi ( x , y , z ) ] \delta [ \phi ( x , y , z ) ] = f [ 0 ( x , y , z ) ] $$ i wrote the zero as a function of $x , y , z$ to stress that the argument of $f$ is still a function . the functional integration is a sort of infinite-dimensional integration and the delta-functional is an infinite-dimensional delta-function . one must be careful about these objects , especially if we integrate amplitudes that may have amplitudes and especially if we integrate over curved infinite-dimensional objects such as infinite-dimensional gauge groups etc . – there may be subtleties such as anomalies . yes , the hilbert space of a free qft is still isomorphic to the usual hilbert space : there is a countable basis . but we are talking about the finite-energy excitations only . there are lots of " highly excited states " that are not elements of the fock space – one would need infinite occupation numbers for all one-particle states . physically , such states are inaccessible because the energy can not be infinite . however , when one is changing the energy from one hamiltonian to another ( e . g . by simple operations such as adding the interaction hamiltonian ) , finite-energy states of the former $h_1$ may be infinite-energy states of the latter $h_2$ and vice versa . so one must be careful : the physically relevant finite-energy hilbert space may be obtained from some infinite-occupation-number states in a different , e.g. approximate , hamiltonian . it is still true that the relevant hilbert space is as large as a fock space and it has a countable basis . the " totally inaccessible " states that are too strong deformations have an important example or name – they are " different superselection sectors " . rigor is a strong word . people tried to define a qft rigorously – by aqft , the algebraic/axiomatic quantum field theory . these attempts have largely failed . it does not mean that there is not any " totally set of rules " that qft obeys . instead , it means that it is not helpful to be a nitpicker when it comes to the new issues that arise in qft relatively to more ordinary models of quantum mechanics ; it is neither fully appropriate to think that a qft is " exactly just like a simpler qm model " but it is equally inappropriate to forget that it is formally an object of the same kind . formally , many things proceed exactly in the same way and there are also new issues ( unexpected surprises that contradict a " formal treatment" ) that have some physical explanation and one should understand this explanation . some of these new subtleties are " ir " , connected with long distances , some of them are " uv " , connected with ever shorter distances . the fact that a qft has infinitely many degrees of freedom is both an ir and uv issue . so even if you put a qft into a box , you will not change the fact that you need wave functionals , delta-functionals , and that there are superselection sectors and states inaccessible from the fock space . by the box , you only regulate the ir subtleties but there are still the uv subtleties ( momenta , even in a box , may be arbitrarily large ) . those may be regulated by putting the qft on a lattice . this has some advantages but some limitations , too .
when the person in the air pulls upward on the person on the ground he is essentially applying an upward force on said person on the ground . this causes a downward force upon the person in the air . with no normal force to counteract this , the person in the air will just pull himself back to the ground .
it is very common to abuse the notation here , so i will try to clarify a bit . the state of a physical system can be described by an abstract vector $\left|\psi\right\rangle$ , which is an element of a hilbert space . the wavefunction , $\psi ( x ) $ is the representation of that vector in the position basis , $\psi ( x ) \equiv\left\langle x | \psi\right \rangle \equiv \left\langle x , \psi\right \rangle$ . in this notation , $\left|x\right\rangle$ is a state which is located at the point $x$ and nowhere else . think of $\left|\psi\right\rangle$ as a column vector whose components are the values of $\psi ( x ) $ at each $x$ . $$\psi ( x ) = \begin{pmatrix}\vdots\\\psi ( x_1 ) \\\psi ( x_2 ) \\\psi ( x_3 ) \\\vdots\end{pmatrix}$$ this vector also has a dual , $\left\langle\psi\right| = \left|\psi\right\rangle^\dagger$ , which is the transpose conjugate . in a different basis , this vector would have different components . another common basis is the momentum basis , with components $\psi_p ( p ) =\left\langle p | \psi\right\rangle$ , typically written as simply $\psi ( p ) $ . just like we could write a 3d vector $\bf{v}$ in the $\bf{i} , \bf{j} , \bf{k}$ basis as ${\bf v} = {\bf v}_1{\bf i} + {\bf v}_2{\bf j} + {\bf v}_3{\bf k}$ , we can write $\left|\psi\right\rangle$ as a sum of $x$ components , $$\begin{align} \left|\psi\right\rangle and = \int^{\infty}_{-\infty}\left|x\right\rangle\ , \psi ( x ) \ , \text{d}x \\ and = \int^{\infty}_{-\infty}\left|x\right\rangle\ ! \left\langle x | \psi\right \rangle \ , \text{d}x \end{align}$$ from this it is clear that $\int^{\infty}_{-\infty}\left|x\right\rangle\ ! \left\langle x \right|\text{d}x$ is an identity , since when it acts on $|\psi\rangle$ it gives $|\psi\rangle$ back . eigenvectors are usually labeled by their eigenvalues , so that if $\hat p$ is an operator with an eigenvalue $p$ , we write $\hat{p}\left|p\right\rangle = p\left|p\right\rangle$ . the eigenvectors of a self-adjoint operator form a complete set of states , which is to say that the sum of all the projection operators is the identity , $\int \left|p\right\rangle\left\langle p\right| \ , \text{d}p $ . this means you could also write $$\begin{align} \langle p\rangle and = \left\langle\psi\right|\hat{p}\left|\psi\right\rangle \\ and =\left\langle\psi\right|\hat{p}\int\left|p\right\rangle\ ! \left\langle p\right|\ , \text{d}p\ , \left| \psi\right\rangle\\ and =\int{p}\ , \langle\psi\left|p\right\rangle\ ! \left\langle p\right| \psi\rangle\ , \text{d}p\\ and =\int p\ , \psi^*\ ! ( p ) \ , \psi ( p ) \text{d}p \end{align}$$ where $\psi ( p ) =\left\langle p | \psi\right\rangle$ . this might make more sense , since it is more obviously an average value of p . the reason that $\hat p = -i\hbar{\partial \over \partial x}$ in the x basis is that a momentum eigenstate ( a state with a single wavelength ) is a plane wave , $\langle x \left|p\right\rangle = {1 \over \sqrt{2\pi}} e^{ipx/\hbar}$ , and that means that in the x basis , $\left\langle x \right| \hat p\left|p\right\rangle = {p \over \sqrt{2\pi}} e^{ipx/\hbar} = {-i\hbar \over \sqrt{2\pi}}{\partial \over \partial x} e^{ipx/\hbar}$ .
huygens and barrow , newton and hooke by v . i arnold . i havent read it , but i am a fan of the author 's writing style . he is a celebrated russian mathematician and also one of the most highly cited russian scientists . road to reality ( which im currently reading ) would also have been a good suggestion but its pretty much all about recent theories , which you are not interested in .
the clockwise direction is normally defined by the right hand grip rule . when your thumb is pointing away from you , your fingers are curled clockwise . so when you look at a clock the axis of rotation is away from you through the clock . i would guess the downvotes are because people believe your question is not physics related , but in fact this rule is how you determine the direction of the angular momentum vector , so there is a connection with physics .
you can decompose a rank two tensor $x_{ab}$ into three parts : $$x_{ab} = x_{ [ ab ] } + ( 1/n ) \delta_{ab}\delta^{cd}x_{cd} + ( x_{ ( ab ) }-1/n \delta_{ab}\delta^{cd}x_{cd} ) $$ the first term is the antisymmetric part ( the square brackets denote antisymmetrization ) . the second term is the trace , and the last term is the trace free symmetric part ( the round brackets denote symmetrization ) . n is the dimension of the vector space . now under , say , a rotation $x_{ab}$ is mapped to $\hat{x}_{ab}=r_{a}^{c}r_{b}^{d}x_{cd}$ where $r$ is the rotation matrix . the important thing is that , acting on a generic $x_{ab}$ , this rotation will , for example , take symmetric trace free tensors to symmetric trace free tensors etc . so the rotations are not " mixing " up the whole space of rank 2 tensors , they are keeping certain subspaces intact . it is in this sense that rotations acting on rank 2 tensors are reducible . it is almost like separate group actions are taking place , the antisymmetric tensors are moving around between themselves , the traceless symmetrics are doing the same . but none of these guys are getting rotated into members " of the other team " . if , however , you look at what the rotations are doing to just , say the symmetric trace free tensors , they are churning them around amongst themselves , but they are not leaving any subspace of them intact . so in this sense , the action of the rotations on the symmetric traceless rank 2 tensors is " irreducible " . ditto for the other subspaces .
given that there has not been any acceleration to cause these velocities , [ . . . ] as a side issue , even in newtonian mechanics , accelerations do not cause velocities . accelerations are just a measure of how rapidly velocities are changing . what you are running into here is the fact that general relativity does not have any notion of how to measure the motion of object a relative to a distant object b . it is neither true nor false that a and b gain relative velocity due to cosmological expansion . it is neither true nor false that a and b have nonzero accelerations relative to one another . frames of reference in gr are local , not global . it is valid to say that distant galaxies are moving away from us at some velocity . it is also valid to say that everything is standing still , but the space between us and the distant galaxy is expanding . [ . . . ] are there still relativistic effects in play ? that is , is there time dilation between the two frames ? kinematic time dilation is well defined in sr , which means that in gr it is only defined locally . gravitational time dilation is only well defined in gr in the case of a static spacetime , but cosmological spacetimes are not static . so it is neither true nor false that there is time dilation between us and a distant galaxy . concretely , you could measure doppler shifts . if you feel like interpreting these shifts in purely kinematic terms , you can assign a velocity to the distant galaxy relative to us . but this is not mandatory and actually does not really work very well , in the sense that the velocity you get is usually several times smaller than the rate at which the proper distance between the galaxies is increasing . ( proper distance is defined as the distance you would measure with a chain of rulers , each at rest relative to the cmb , at a moment in time defined according to a notion of simultaneity defined by cosmological conditions such as the temperature of the cmb . ) in particular , there are galaxies that we can observe that are now and always have been receding from us at $v&gt ; c$ , if you define $v$ as the rate of change of proper distance . the fact that we can observe them tells us that their doppler shifts are finite and correspond to $v&lt ; c$ . here is a nice popular-level article that explains a lot of this kind of stuff : davis and lineweaver , " misconceptions about the big bang , " http://www.scientificamerican.com/article.cfm?id=misconceptions-about-the-2005-03 it is paywalled , but there are lots of copyright-violating copies floating around on the web . the following is a presentation of the same material at a higher level : davis and lineweaver , " expanding confusion : common misconceptions of cosmological horizons and the superluminal expansion of the universe , " http://arxiv.org/abs/astro-ph/0310808
there are theoretical arguments that a massless spin-2 particle has to be a graviton . the basic idea is that massless particles have to couple to conserved currents , and the only available one is the stress-energy tensor , which is the source for gravity . see this answer for more detail . however , the particle discovered at lhc this year has a mass of 125 gev , so none of these arguments apply . it would be a great surprise if this particle did not have spin 0 . but it is theoretically possible . one can get massive spin 2 particles as bound states , or in theories with infinite towers of higher spin particles .
you have to break up the domain to get rid of the absolute value . then , do the integral by integrating by parts .
there are plenty of quantities that do not obey the superposition principle . a simple pendulum , for example , will behave differently ( with a longer period ) if you double the initial amplitude . what griffiths means by that quote is that for the electromagnetic field there are no situations where the fields fail to add linearly . more specifically , the superposition principle is encoded in the linearity of maxwell 's equations , which states that if $ ( \mathbf{e}_1 ( \mathbf{r} , t ) , \mathbf{b}_1 ( \mathbf{r} , t ) ) $ and $ ( \mathbf{e}_2 ( \mathbf{r} , t ) , \mathbf{b}_2 ( \mathbf{r} , t ) ) $ are solutions of maxwell 's equations , then $$ ( \mathbf{e}_1 ( \mathbf{r} , t ) +\mathbf{e}_2 ( \mathbf{r} , t ) , \mathbf{b}_1 ( \mathbf{r} , t ) +\mathbf{b}_2 ( \mathbf{r} , t ) ) $$ is also a solution . this is indeed consistent with experiment , except for two situations : if the field strength inside a medium exceeds that of its linear response , then the material ( "macroscopic" ) maxwell equations are no longer a linear problem . this is the bread and butter of nonlinear optics , which describes a broad range of phenomena . however , this is not a failure of griffith 's claim , as the ' microscopic ' fields $\mathbf{e}$ and $\mathbf{b}$ are still a linear superpositions of those created by the free and bound charges . in certain , very careful experiments , it is possible to observe the scattering of light by light . this is explained by quantum electrodynamics as the temporary creation and annihilation of virtual particle-antiparticle pairs where the light beams meet , which transfer energy and information from one beam into the other . this does violate the superposition principle as stated above and as meant by griffiths in his textbook , and it has been observed experimentally . however , outside of very specific experiments specially designed to observe it , this effect is negligible and can be ignored as regards classical electrodynamics . in the quantum version , you have a whole host of such problems to deal with .
no , you cannot , since you have not specified the voltages of the two batteries . ignoring converter losses , the relevant quantity is not amp-hours but watt-hours . so let 's say the laptop battery is 18 volts and 2.5 amp-hours , while the ups is 12 volts and 7.5 amp-hours . the energy available from the laptop battery is 18 x 2.5 , or 45 watt-hours . the energy available from the ups battery is 12 x 7.5 , or 90 watt-hours . all else being equal , the ups will provide twice the duration of the laptop battery . all else , of course , is not equal . the laptop battery power goes through a set of dc-dc converters to provide the actual voltages used by the circuits , which have an efficiency less than one . the ups battery power goes through an inverter with its own inefficiency , and the resulting ac goes to the laptop where it is converted to the required internal voltages . consequently , you had expect the overall efficiency of the ups battery to be less than the efficiency of the laptop battery . exactly how much less this is , and its effect on the ratio of the two durations , is not something which can be figured out from first principles .
in principle of course you try something like that . but there are three issues that will kill you : $q$ . every resonance has a quality factor which represents how quickly the energy in the mode drains away by assorted dissipative processes . i do not know what it is for the schumann resonances , but i will give you long odds that it is not good : much of the energy you put into the field will just dribble away into space . power density . whatever energy you pump into these modes will spread out over the whole cavity , and you will only be able to draw as much as there is in the region covered by your antenna , which will be effective nothing even with gigawatts driven into the resonance . not only could not you power a iphone , you could not power the little shoplifting-prevention tag that retailers put onto bits of mobile merchandise . antenna dimensions . the naive way to design an antennas to use at frequency $f$ requires conductors of length on order of $c f$ . bit of a problem for frequencies of a few or few tens of hertz .
dear j.f. sebastian , for a few decades , physicists thought that the cosmic microwave background - photons created 350,000 years after the big bang that fill the space and currently correspond to thermal radiation at 2.7 kelvin degrees - carried most of the entropy of the universe . entropy , denoted $s$ , is the physical quantity representing the information that can be carried by the " arrangement of atoms " or other microscopic building blocks . it is the " useless information " that we can not measure in isolation and that is responsible for thermodynamic phenomena . most of the information cannot be decoded - the question what part of this entropy may be employed as memory in functioning memory chips is an engineering question but it is a small part . the entropy 's unit is " joule per kelvin " . however , if you divide it by $k$ , the boltzmann 's constant , you get a dimensionless number that measures the information in " nats " . one bit is equal to $\ln ( 2 ) \approx 0.69$ nats , so the dimensionless entropy is approximately the same thing as the number of ( useless ) bits that the atoms carry . microwave background so what is the entropy of the cosmic microwave background ? the approximate " radius " of the visible universe is $10^{60}$ planck lengths , so the volume is $10^{180}$ planck lengths . however , the cmb temperature is just $10^{-32}$ planck temperatures or so , which means that the volume has to be counted in units of volume that are $10^{96}$ times larger . we get about $10^{180-96}\approx 10^{85}$ photons in cmb and each of them carries one bit or so . so the cmb entropy is about $10^{85}$ bits . large black holes however , it was found that most galaxies store a huge black hole in their center and black holes actually maximize the entropy that can be squeezed into a fixed volume , or carried by a fixed amount of bound mass . at our galactic center , the sgr a* black hole has mass about 4 million solar masses or $10^{37}$ kg which is about $10^{45}$ planck masses , so the radius is also about $10^{45}$ planck lengths and the area is $10^{90}$ planck areas , producing $10^{90}$ bits of entropy just from the single black hole . because there are approximately $10^{11}$ galaxies in the universe , we get $10^{101}$ bits of entropy carried by the galactic black holes which is much higher than the cmb entropy . cosmic holographic bound on entropy it may be fundamental to mention that the ultimate entropy of the universe is bound by the area of the de sitter horizon in planck units . the radius of the horizon is about $10^{60}$ planck lengths so the area is $10^{120}$ planck areas . the largest entropy that our universe may carry is therefore about $10^{120}$ bits . in some sense , we may say that this huge entropy is already " out there " at the cosmic horizon today - but we may attribute it to " everything that is behind the horizon " and we do not see . however , there could be lots of matter inside the horizon and its entropy could approach those $10^{120}$ bits as well . in particular , a single black hole that would grow enough to almost touch the cosmic horizon ( which would shrink along the way ) would carry almost $10^{120}$ bits , too . however , such a black hole will never exist , of course . note that lawrence , who calculated the final bound , ended up with a figure is wrong by 40 orders of magnitude . his numbers have errors at each step . first of all , the radius of the cosmic horizon is $10^{28}$ centimeters rather than $10^{18}$ centimeters he wrote ; he apparently forgot to add the speed of light or to distinguish seconds and years . this gave him the first 20 orders of magnitude of mistake . he made two more errors that produced the remaining 20 orders of magnitude .
for good doping you need two things : ( 1 ) get enough dopant in to be useful in changing carrier concentrations , and ( 2 ) having an energy level close to a band edge to generate electrons ( holes ) in the band , rather than making a mid-level recombination center . the below is assuming you are trying to dope silicon . data is generally from sze 's excellent ' physics of semiconductor devices ' text . bismuth certainly has a donor level not much lower than as , satisfying ( 2 ) . however , the solid solubility of bi is roughly 3 orders of magnitude less than as , peaking at below $10^{18}/cm^3$ . this limits the utility of bi in current device technology - you just can not get enough in . nitrogen has very low solubility in silicon . i cannot find quickly any info on energy levels in the gap , but oxygen has several levels , all pretty much near gap . i would say nitrogen loses out on both ( 1 ) and ( 2 ) .
it depends how the charges are distributed in the material , and on the material 's conductance . if you have a metal , the charges of the plates would be mobile and result in a hard to compute distribution . i cannot help you with that . there are probably good approximations to tackle those kind of problems but i am no expert . if the charges are static and equally distributed among the surface , and the material has a relative permittivity ( $\varepsilon_r=1$ ) , you can use coulombs law with respect to infinitesimal parts of the charges and integrate over them . if you suppose that the rectangles have a width of $10cm$ , the force of the top plate on the middle plate could be calculated by $$ \hat{f}_{12} = \frac{1c^2}{4\pi\varepsilon_0}\int_{-25cm}^{25cm}\frac{dx_1}{50cm} \int_{20cm}^{30cm}\frac{dy_2}{10cm} \int_{-15cm}^{15cm} \frac{dx_2}{30cm} \int_{0cm}^{10cm}\frac{dy_2}{10cm} \frac{1}{ ( x_2-x_1 ) ^2+ ( y_2-y_1 ) ^2} \begin{pmatrix}x_2-x_1\\y_2-y_1\end{pmatrix} $$ as you can see , this is already quite complicated with the favourable assumptions we made . this should have an analytical solution but at the moment i am to lazy to do it . wolfram alpha could probably do the separate integrals and you would have to piece them together . you could then compare the result with what you would expect from point charges . ahh , and do not forget that there is the other plate as well . you would need to repeat the integration with opposite sign to obtain the second force and then take the difference for the total force .
the best reference for this is feigenbaum 's original article , reprinted in " universality in chaos " by cvitanovic . the point is that when you iterate a map , every time you period double , you fold up the function one more time . the behavior is dominated by the solution to the following equation : $$ \alpha g ( g ( x/\alpha ) ) = g ( x ) $$ which says that g iterated with itself and rescaled ( both in the domain and range ) looks just like g . the function $g$ is shifted relative to f , so that it is maximum is at 0 , not at some point between 0 and 1 , which means you do not have to follow the critical point under iteration . you can solve this condition more easily by imposing the symmetry $g ( x ) =g ( -x ) $ and using a taylor expansion , and this gives g and $\alpha$ , and $\alpha$ is the scale exponent . everything about the critical behavior is determined by g , and this is described best in the original article .
for an example , if i use iron instead of zinc in the example from wikipedia , i will get a different voltage ? yes . is determined only by the chemical correlation between the two metals . is this true ? no . the voltage of a galvanic cell is the sum of the voltages of the " half cells " , wich can ( not must ! ) be made from a metal and a solution around that " electrode " . both the metal ( or what else is there as " active " material ) and the solutions ( "electrolyte" ) quality and concentration determine the voltage of this " electrode " . there are cases , in which there is a single electrolyte for both " electrodes " and where the influence of the electrolyte on both electrodes cancel out . ( eg the oldfashioned acid zink/manganese dioxide " dry " cells and the more recent " alkaline " zink/mangese dioxide cells , both have about 1.5 volts ) . but in general , all components have an influence . ps be aware that the popular use of " electrodes " ( up to physics textbooks ! ) is wrong . the carbon rod in a zink/"carbon " cell is not an electrode at all . this is a contact material able to withstand the corrosive environment . the electrode is a mass of manganese dioxide . the latter then is " explained " as " depolarisator " , a theory obsolete ( and known to be wrong ) since nernsts law was published more than 100 years ago .
ok , i will assume you have the under-damped case . if you continue reading the wikipedia article in question you will find the solution for a underdamped oscillator writen as $$ x ( t ) = e^{- \zeta \omega_0 t} ( a \cos ( \omega_\mathrm{d}\ , t ) + b \sin ( \omega_\mathrm{d}\ , t ) ) $$ with $a$ and $b$ constant . so , take you data , and plot all the maxima ( or minima ) as a function of time , fit an exponential {*} to that and $\zeta \omega_0$ pops right out . if you also need to get $\omega_o$ from the data use $$ \omega_\mathrm{d} = \omega_0 \sqrt{1 - \zeta^2 } $$ where you get $\omega_\mathrm{d}$ by extracting the average period ( i.e. . time from peak to peak ) in the data and noting that the period is $t_d = \frac{2 \pi}{\omega_\mathrm{d}}$ . now you have two equations for two unknowns , so all you have left is a bit of algebra . {*} or plot amplitude versus time on semi-log paper if you are doing this the old-school way . or plot log ( amplitude ) versus time on linear--linear graph paper . then extract the slope .
a good source of spectroscopic data is the nist chemistry webbook . it compiles all possible information about polyatomic molecules including their uv/vis and ir spectra . you will find references to corresponding papers and , as i see , the new version includes a java applet that plots the uv spectrum . here is a direct link to tht spectrum .
you want the time . simply put , for that the minimum requirement is position ( and hence the distance ) and velocity . to know the position you need to detect it . once you detect it , you can calculate the trajectory and thus the time you have to settle your issues ( assuming it is on a collision course ) . i got this from cnn : the b612 foundation is building the sentinel space telescope , the world 's most powerful asteroid detection and tracking system , to see the millions of asteroids we can not see today and could pose threats to our planet . also , neossat , the near earth object surveillance satellite , is a micro-satellite launched in february 2013 by the canadian space agency ( csa ) that will hunt for neos in space . tracking systems are recording asteroids even as large as 140 meters . any asteroid with a radius more than 300 meters means an assured global catastrophe . check this out . the size that you are asking about is so big that it will create noticeable gravitational effects ( like perturbation in orbit ) and so we will know about it .
acuriousmind 's comment in answer form : $kt$ is something that appears all the time in equations in thermodynamics . you can always choose suitable units to ensure that the quantity $kt = 1$ ( think of it as some kind of normalisation if you like ! ) . to convert back to an answer that includes those $kt$ 's we simply perform some dimensional analysis and insert the appropriate $kt$ quantities back .
the mass of a small fluctuation is usually defined as $$ \pm m^2= \frac{d^2v}{d\phi^2}\biggr|_\text{vev}$$ the sign depends on your conventions . this makes sense in analogy with the canonical free field potential $$v_\text{free}=\pm \frac{1}{2}m^2\phi^2$$ for which the above formula is clearly right . more generally , we can expect any ( reasonably smooth , [ insert other obscure mathematical assumptions here ] ) potential to be well-approximated by a quadratic potential when it is close to an extremum , so we can define a mass in analogy with the harmonic oscillator - the free field is of course just that ! in your case , it yields , $$\pm m^2=\frac{\rho}{m^2} -\mu^2 +3\lambda \phi_\text{vev}$$
yes , you do need to add in the mass of dark matter if it is present , however on small scales the dark matter is almost uniformly distributed . to see this , consider formation of the solar system from the original dust cloud . if you take some test particle far from the sun and let it fall towards the sun it will accelerate towards the sun , then pass it and head on out again . if every particle in the original dust cloud behaved this way the solar system could never have formed since the dust cloud would simply oscillate about its centre of mass and stay the same overall size . the reason the sun formed is that electrostatic interactions between the dust particles allowed the cloud to dissipate energy as heat and settle towards the centre . now you see why the sun is not full of dark matter . dark matter only interacts by the weak and gravitational forces so the dark matter particles can not dissipate energy and can not settle into the sun . assuming there is dark matter in the solar system it will be oscillating about the sun . in principle weak interactions will eventually dissipate enough energy for the dark matter to become gravitationally bound within the sun , but it is going to take a long time . the average density of matter is astonishingly low . at present the total density is around 5 protons per cubic metre , so the dark matter density is only 1 proton per cubic meter ( and the average baryon density is 1 proton per five cubic metres ! ) . there will be variations in the dark matter density caused by quantum fluctuations during inflation , and indeed these are thought to have been critical in seeding formation of the first galaxies . however on sub-galactic scales the dark matter density fluctuations are so small we can ignore them .
unpredictability and special relativity can come from the fact that objects that are at a space like separation from us can influence our future like cone . for example , if alpha centauri exploded in a supernova right " now " in our reference frame , we would not know about it for 4 years since that star is 4 light years away from us . so we can not now predict that 4 years from now we will be hit with the supernova blast wave . as @arnoques succintcly put in his comment , to predict an event in our future light cone , the entire past light cone of that event would need to be known and that would include events that are outside our present light cone .
if this were computer science , we might say $\psi$ takes a $d$-tuple of reals ( $r$ ) and another real ( $t$ ) and returns a complex number with the attached unit of $l^{-d/2}$ in $d$ dimensions ( with $l$ being the unit of length ) . 1 if you want any more of an interpretation , well then you have already given it : $\psi ( r , t ) $ is the thing such that $\int_r\ \lvert \psi ( r , t ) \rvert^2\ \mathrm{d}v$ is the probability of the particle being observed in the region $r$ at time $t$ . you can loosely think of it as a " square root " of a probability distribution . the reason the " square root " interpretation is not quite right , and probably the reason you are not satisfied with the $\int_r\ \lvert \psi ( r , t ) \rvert^2\ \mathrm{d}v$ definition , is that any particular instance of $\psi ( r , t ) $ carries extraneous information beyond what is needed to fully specify the physics . in particular , if we have $\psi_1$ describing a situation , then the wavefunction defined by $\psi_2 ( r , t ) = \mathrm{e}^{i\phi} \psi_1 ( r , t ) $ gives identical physics for any real phase $\phi$ . so the return value of the wavefunction itself is not a physical observable -- one always takes a square magnitude or does some other such thing that projects many mathematically distinct functions onto the same physical state . even once you have taken the square magnitude , $\lvert \psi ( r , t ) \rvert^2$ arguably is not directly observable , as all we can measure is $\int_r\ \lvert \psi ( r , t ) \rvert^2\ \mathrm{d}v$ ( though admittedly for arbitrary regions $r$ ) . 1 you can check that $-d/2$ is necessarily the exponent . we need some unit such that squaring it and multiplying by the $d$-dimensional volume becomes a probability ( i.e. . is unitless ) . that is , we are solving $x^2 l^d = 1$ , from which we conclude $x = l^{-d/2}$ .
newton 's second law $f=ma$ does not depend on the point of application of force because this law is valid only for point particles . now to apply it to rigid bodies we must consider them as a system of particles . let a rigid body be made up of $n$ particles of mass $m_1 , m_2 , \cdots , m_n$ . now apply a force $f$ to some $i_{th}$ particle . all other particles will also exert internal forces on each other . therefore , the second law for all particles is \begin{align}f_1^{int} and =\frac{dp_1}{dt}\\ f_2^{int} and =\frac{dp_2}{dt}\\ \cdots\\ f+f_i^{int} and =\frac{dp_i}{dt}\\ \cdots\\ f_n^{int} and =\frac{dp_n}{dt}\end{align} adding all these $$\sum_j f_j^{int}+f=\sum_j\frac{dp_j}{dt}$$ by the third law $$\sum_j f_j^{int}=0$$ thus $$f=\frac{dp}{dt} \text{where } p=\sum_jp_j$$ now if you apply the same force $f$ to the center of mass of the body , you get the same equation for total momentum . $$f=\frac{dp}{dt}$$ therefore both situations will have identical solution for the total momentum and hence for the linear velocity of the center of mass of the rigid body .
the expectation value $$\tag{1} \langle \psi | v| \psi \rangle~=~0$$ of the potential energy operator $v$ is indeed zero , but the expectation value $$\tag{2} \langle \psi |k| \psi \rangle~=\frac{\hbar^2}{2m} \int_{\mathbb{r}}\ ! dx~ |\psi^{\prime} ( x ) |^2 ~=~+\infty$$ of the kinetic energy operator $k$ is actually infinite for the wave function $$\tag{3} \psi ( x ) ~=~ \sqrt{\frac{2}{l}}\left ( \theta ( x-\frac{l}{2} ) -\theta ( x-l ) \right ) , \qquad x\in \mathbb{r} . $$ here $\theta$ is the heaviside step function . the kinetic energy operator $k:=-\frac{\hbar^2}{2m}\frac{d^2}{dx^2}$ is an example of an unbounded operator , which only make sense on its domain ${\cal d}_k\subsetneq {\cal h}$ inside the hilbert space ${\cal h}:=l^{2} ( \mathbb{r} ) $ of square lebesgue integrable functions . in particular , it is a non-trivial mathematical problem how to apply the differential operator $k$ to the non-differentiable wave function ( 3 ) . the infinite result ( 2 ) can be seen ( at the physical level of rigor ) in at least three ways ( ordered with the computationally simplest calculation first ) : plug the heaviside step function into eq . ( 2 ) to get an integral over the square of a pair of dirac delta function situated at $x=\frac{l}{2}$ and $x=l$ . this is strictly speaking mathematically ill-defined . physically , it makes sense to assign the integral the value infinite , cf . this phys . se post . calculate the overlaps $c_n=\langle \phi_n | \psi \rangle$ , and show than the sum $$\tag{4} \langle \psi |h| \psi \rangle=\sum_{n=1}^{\infty}|c_n|^2 e_n ~=~+\infty $$ diverges . this infinite conclusion seems physically robust , since all terms in the series ( 4 ) are non-negative . by regularization , as emilio pisanty suggests in a comment . define a regularized wavefunction $\psi_{\varepsilon} \in c^1 ( \mathbb{r} ) $ in such a way that ( i ) it converges $\psi_{\varepsilon}\to \psi$ for $\varepsilon\to 0^{+}$ , ( ii ) the expectation value $\langle \psi_{\varepsilon} |k| \psi_{\varepsilon} \rangle$ is easy to compute and finite for $\varepsilon&gt ; 0$ . show that $\langle \psi_{\varepsilon} |k|\psi_{\varepsilon} \rangle\to +\infty$ diverges for $\varepsilon\to 0^{+}$ .
1 ) if there is an error $e_j$ , the new states $e_j|0\rangle_l$ and $e_j|1\rangle_l$ are eigenvectors , with eigenvalue $-1$ , of all the stabilizers $s_j$ belonging to some set subset $s_j$ of $s$ . ( the elements of $s_j$ anticommute with $e_j$ ) . this subset $s_j$ identifies uniquely the error $e_j$ . 2 ) $|0\rangle_l$ and $|1\rangle_l$ are eigenvectors , with eigenvalue $1$ , of all the stabilizers $s$ belonging to $s$ ( this is not true for the " components " of $|0\rangle_l$ and $|1\rangle_l$ like , for instance , $|1010101\rangle$ ) . for a stabilizer $s$ , you just calculate $s|0\rangle_l$ and $s|1\rangle_l$ , and you check that the result is $|0\rangle_l$ or $|1\rangle_l$ . for instance : $k^1\left|0\right\rangle_l = ( iiixxxx ) \\\frac{1}{\sqrt{8}} ( \left|0000000\right\rangle + \left|1010101\right\rangle + \left|0110011\right\rangle + \left|1100110\right\rangle + \left|0001111\right\rangle + \left|1011010\right\rangle + \left|0111100\right\rangle + \left|1101001\right\rangle ) = \\ \frac{1}{\sqrt{8}} ( \left|0001111\right\rangle + \left|1011010\right\rangle + \left|0111100\right\rangle + \left|1101001\right\rangle + \left|0000000\right\rangle + \left|1010101\right\rangle + \left|0110011\right\rangle + \left|1100110\right\rangle ) \\ =\left|0\right\rangle_l$ 3 ) $k^4 \left|1010101\right\rangle = iiizzzz |1010101\rangle$ . with $z |0\rangle = |0\rangle$ , and $z |1\rangle = -|1\rangle$ , you get : $k^4 \left|1010101\right\rangle = |1010101\rangle$
one has to have clear that the terminology " photon " describes an elementary particle . elementary particles are described concisely in the framework of quantum mechanics . in this framework an elementary particle , the photon in this case , can have two behaviors . either as a classical physics point particle with an x , y , z position , i.e. no extent in space , or as a probability wave , which means that a statistical accumulation of individual photons from the same starting conditions will display an interference pattern as a classical wave would . in the wave manifestation the extent of the locus where the photon may be found is bounded by the heisenberg uncertainty principle . this last tells us that the better we know the momentum of the photon , the less localized the locus of finding it is . in this sense the probable extent of a photon can be made as large as our knowledge of its momentum p=h/lamda , the planck constant over the wavelength . now nature and the physics models we have developed to describe and predict its behavior is continuous between the photon framework of quantum mechanics and the classical electromagnetic waves of maxwell 's equations . , the frequency of the photon is the frequency of the wave it will build up when present in large numbers . electromagnetic waves can have the extent one designs and the limits are the limits of the ability to produce them and direct them . radio waves are all over the place .
i am not going to provide a full answer here , because i do not know the answer , but i want to give some statements that illustrate quite nicely the kind of problems one would face when determining topology of anything : we know spacetime is a manifold . that means , locally , it looks just like $\mathbb{r}^4$ . that is already a bummer . we can not do jack at one place to find out anything about topology . but , as soon as we move , we get into all the complications of reference frames and whatnot . so , experimentally , whether or not we can principally detect topology , it is going to be one hell of a challenge . but it gets worse . you know how we always suppose that fields fall off at infinity ? that is one of the natural reasons principal bundles arise in gauge theories . if we want to make precise the notion of a field $a$ falling off at infinity , we say it has to be a smooth function and have a well-defined value $a ( \infty ) $ . and what is $\mathbb{r}^n$ together with $\infty$ ? the one-point compactification , also known as the sphere $s^n$ . but it is not quite feasible to find global solutions to the equations of motion of a gauge theory on $s^n$ , thanks to the hairy ball theorem and others . so we say : alright , let 's solve the e.o.m. locally on some open sets $u_\alpha , u_\beta$ homeomorphic to the disk ( think of the hemispheres overlapping a bit at the equator ) , and patch the solutions $a_\alpha , a_\beta$ together on the overlap by a gauge transformation on $u_\alpha \cap u_\beta$ . now , we have got our field living naturally on the sphere $s^n$ if we want a global solution . does this mean that we actually live on an $s^n$ , or just that we are inept to find a coherent description of the physics on $\mathbb{r}^n$ ? what would that even mean ? i can hear the people saying " we can always examine what the curvature is - $s^n$ has non-vanishing one , $\mathbb{r}^n$ has vanishing one . " . that is alright , but the above gauge argument forces us the either accept that there is no globally well-defined gauge potential $a$ on $\mathbb{r}^n$ or to think of some $s^n$ on which a patched-together solution lives . what is more real ? what would it even mean to say one of these views is more meaningful than the other ? so , you might be inclined to say : " screw these weird gauge potentials , we are living on a spacetime , and not some bundle ! " but there are topological effects of these bundles such as instantons or the aharonov-bohm effect . spacetime alone is not enough . and what would be a meaningful distinction between " these bundles are not where we live , they are ' above ' spacetime " and " we live on the bundles , and most often only experience the projection on spacetime " ? what i am trying to say is that it is not even clear what we should regard as the universe we live in . the ordinary , 4d spacetime is not enough to account for all the strange things that might happen . and as i said in the beginning , do not take this as an answer . i am biased from being immersed in gauge theories , and only having superficial knowledge of the intricacies of gr . but from what i see , all " non-trivial topology " can also be seen as arising from patching together local solutions to the physical laws that otherwise do not match well .
the hand-waving answer is nuclear pairing energy . protons and neutrons prefer to move in pairs in the nucleus , and they prefer to pair like-to-like rather than one to another . this means that , for the most part , nuclei with odd proton number and odd neutron number ( "odd-odd nuclei" ) are less tightly bound than " even-even nuclei , " and the odd-odd nuclei tend to turn into even-even nuclei by a weak decay . in fact , there are only eight naturally-occurring odd-odd isotopes : four light isotopes which are actually stable ( deuterium , lithium-6 , boron-10 , and nitrogen-14 ) and four with lifetimes comparable to the age of the earth ( potassium-40 , vanadium-50 , lanthanum-158 , and lutetium-176 ) . the pattern on the chart of nuclides is quite striking : for instance , tin ( $z=50$ ) has ten stable isotopes , while its odd-$z$ neighbors indium and antimony have only two each , both with even neutron numbers .
the problem with the phase space flow in hamiltonian mechanics is that the flow itself is non-dynamical , that is , the flow is immediately defined for a given hamiltonian , so there is no independent equation governing its evolution . thus , liouville equation is simply a transport of a scalar variable in a given flow . so , dimensional analysis of the flow would be simply subset of dimensional analysis of underlying hamiltonian structure . similarly , i do not think there is any sense of attempting to find turbulence in the phase space flows . sure , time dependence of hamiltonian can introduce changes in the phase space , including the type of changes associated with transitions to chaos : such as bifurcations , tori destruction . . . but again , the flow itself is not the fundamental object in such transitions . if we are talking about the phase space of kinetic equations , the same arguments apply . even though the flow is ' more dynamical ' especially if taken in the context of self-interacting system of equations such as vlasov-maxwell , in these equations the flow itself again is not a fundamental object , so rarely it is analyzed independently . however , most methods of ( numerical ) solutions for such equations like particle-in-cell method and its many variations do use approaches quite similar to that of hydrodynamics .
the expression for $\delta s_m$ that you are expecting holds provided the variation you are performing is the variation with respect to the inverse metric only ; there should be no $\delta\varphi$ terms . in other words ; set $\delta\varphi = 0$ , and you obtain the desired expression . see , for example , carroll spacetime and geometry p . 164 , he does the same computation and explicitly remarks " now vary this action with respect , not to $\phi$ , but to the inverse metric . . . " generally speaking , in fact , the stress tensor is defined to be proportional to the functional derivative of the action with respect to the inverse metric ; \begin{align} t_{\mu\nu} = -2\frac{1}{\sqrt{-g}}\frac{\delta s}{\delta g^{\mu\nu}} \end{align}
hawking thought - and could " rigorously " deduce from semiclassical gravity - that the information has to be lost because it can not get out of the black hole interior once it gets there . to see why , look at this " penrose causal diagram " that may be derived for a black hole solution . diagonal lines at 45 degrees are trajectories of light , more vertical lines are time-like ( trajectories of massive objects ) , more horizontal lines are space-like . if you look e.g. at the yellow surface of the star ( its world line ) , you see that it ultimately penetrates through the green event horizon into the purple black hole interior . once the object - and the information it carries - is inside , it can no longer escape outside ( do not forget : time is going up ) , into the light green region , because it would have to move along spacelike trajectories . so the information carried by the star that collapsed to the black hole inevitably ends at the violet horizontal singularity and it may never be seen in a completely different region outside the black hole . the information loss of course depends on the detailed geometry of the black hole that is not shared by a helium nucleus , so it should not be surprised that helium nuclei and black holes have different properties . as we know today , the information is allowed to " tunnel " along spacelike trajectories a little bit in quantum gravity ( i.e. . string/m-theory ) . this process is weak but this weak " non-local process " allows the information to be preserved .
let me see if i understand the question correctly : in general our solutions for a system will involve for every wavevector k a set of frequencies $\omega ( k ) $ . when these curves giving $\omega ( k ) $ do not cross , there is in obvious sense in which we can separate our solutions into different modes . but when they do cross how do we assign modes ? i believe the answer is in general we do not . ( or we do but we should not . ) unless the solutions have different symmetry properties , which is common enough , our label do not have meaning except convenience . i do not think this is not entirely about definitions . consider adiabatic motion : we think that if we perturb our solution slowly enough it will just change its frequency and wavenumber , but stay on the same mode . but in the vicinity of a crossing of two modes our solution will evolve to have components in both modes , barring symmetry . so is there is no sense to think of them as separate modes . ( the only case i can think where the labeling of modes actually matters is in topological insulators , a case which requires no [ bulk ] band touching . other than that it is just a convenience or it labels symmetry properties ) apologies if i misunderstood your question
this is a very-basic question . there are lot more things to digest than just that in em . . ! all because of maxwell equations . . . both electric and magnetic fields are inter-dependent ( i.e. . ) one field requires another ( or ) one field produces another . the phenomenon is called electromagnetism . for example , consider an electric charge at rest ( static ) . it produces an electric field . but when the charge is in motion ( current ) , a magnetic field is produced perpendicular to its direction of propagation . say , if you pass current through a straight wire , magnetic field is formed around the wire in the form of circular rings ( could affect compass or metal fillings nearby ) . on the other hand , you are passing current through a circular spring-like thing ( commonly , a coil ) called solenoid , magnetic field is produced along its axis . simply you could keep in mind that magnetic field is produced by moving charges ( current ) . this is an observed phenomena and it is explained by maxwell . your last question is " ok to ask " . . . yes , there are a lot of materials ( mostly metals ) that produce magnetic field when current flows through them . but , shape is not at all " a matter " . it is whether there is a change in the fields that matters . . .
do optical mode phonons interact differently than acoustic ones ? yes . look at the dispersion curve for any material and you can understand this better . let us take silicon for example . you can see that the gradient of the frequency as a function of wave vector ( aka , group velocity ) is what determines the amount of energy that can be carried by phonons . you can see that the optical branches have a much flatter profile than the acoustic branches . therefore , they do not participate heavily in energy transfer and storage ( thermal conductivity and specific heat capacity ) . you can also see that their frequency is much higher than acoustic branches . this means that they interact with em waves of similar frequcencis . this is why , for example , co2 is a greenhouse gas . also , is there a way i can quantify this ? yes . thier interaction is different from acoustic phonons and were first successfully described by einstein where he assumed a dispersion relations at a single frequency , $\omega_0$ , and independent single frequency oscillator at each atom . the density of states is given by $d ( \omega ) = n\delta ( \omega - \omega_0 ) $ the acoustic phonons are described by debye model and they have a liner dispersion relation $\omega= v_s k$ and are responsible for sound wave . the density of states is given by $$d ( \omega ) = \frac{v \omega^2}{2 \pi^2 v_s^3}$$
in an inertial reference frame , there most certainly is not zero net force on a planet . there is a force pulling the planet towards the sun , and that is it . there are no other forces on the planet . you are 100% correct : if there really were no net force on the planet , then the planet would be stationary or travel in a straight line at constant velocity . it would not travel in a circle or ellipse . this is newton 's first law . if your tutor disagrees with newton 's first law , i suggest you find a better tutor !
i will try to slightly elaborate on @vladimirkalitvianski answer . from maxwell 's equations , we can derive that the following combination of gauge transformations on $\mathbf{a}$ and $\phi$ leave both $\mathbf{b}$ and $\mathbf{e}$ invariant : \begin{align} and \mathbf{a}'=\mathbf{a}-\mathbf{\nabla} \alpha \\ and \phi'=\phi+\frac{\partial \alpha}{\partial t} \end{align} where $\alpha=\alpha ( \mathbf{x} , t ) $ . this means that all the field configurations of $\mathbf{b}$ and $\mathbf{e}$ related by a gauge transformation are physically equivalent . note that this has nothing to do with the hamiltonian operator in qm . now in qm , we know that a wave function can always be multiplied by a phase factor : $$ \psi'=e^{-iq\alpha}\psi , $$ where $\alpha \neq \alpha ( \mathbf{x} , t ) $ , because the probability of finding the particle at a particular position is unaffected by the above transformation , and also the schrodinger equation and the probability current are unaffected by the above transformation . if we now demand that the above also holds for when $\alpha = \alpha ( \mathbf{x} , t ) $ ( i.e. . a gauge transformation ) , then the schrodinger equation must be made gauge invariant : \begin{equation} i\frac{\partial\psi}{\partial t}=-\frac{1}{2m} ( \mathbf{\nabla}-iq\mathbf{a} ) ^2\psi+ ( v+q\phi ) \psi \end{equation} such that the schrodinger equation is invariant under the simultaneous gauge transformations : \begin{align} and \mathbf{a}'=\mathbf{a}-\mathbf{\nabla} \alpha \\ and \phi'=\phi+\frac{\partial \alpha}{\partial t} \\ and \psi'=e^{-iq\alpha}\psi \tag{1} \end{align} note that we can say that we have adjusted the " normal " hamiltonian by replacing the ordinary ( partial ) derivatives by : \begin{equation} \begin{array}{cc} \displaystyle \mathbf{\nabla} \rightarrow \mathbf{d}\equiv \mathbf{\nabla} - i q \mathbf{a} \ ; , and \displaystyle \frac{\partial}{\partial t} \rightarrow d^0 \equiv\frac{\partial}{\partial t} + iq \end{array} \end{equation} to sum up , by demanding that our theory is invariant under the gauge transformation expressed by equation $ ( 1 ) $ , we are forced to change the hamiltonian operator as we have done above . however , by doing this , the new hamiltonian describes a particle interacting with the potentials $\mathbf{a}$ and $\phi$ . if you are not convinced by this argument , i strongly recommend you to read up on the aharonov–bohm effect ( http://en.wikipedia.org/wiki/aharonov%e2%80%93bohm_effect ) . furthermore , note that we require that a gauge transformation does not affect any observables . this means that we must demand that the probability current is also unaffected . you can show ( although it is quite tedious ) that the current is made gauge invariant by making the replacement : $\displaystyle \mathbf{\nabla} \rightarrow \mathbf{d}$ .
degenerate usually means that the gas is in a quantum regime , that is the thermal de broglie wave length $\lambda_{\rm db}\propto t^{-1/2}$ is much larger that the interparticle distance $l=n^{-1/d}$ , where $n$ is the density and $d$ is the dimension of space . one then has $l\propto k_f^{-1}$ where $k_f$ is the fermi momentum . this regime is the opposite of that of the classical ( dilute ) gas . a degenerate fermi gas is thus such that $t\ll e_f=\frac{k_f^2}{2m}$ . this corresponds to the limit where the fermions form a well defined fermi sphere , etc described in text books ( usually in a chapter about the fermi gas ) . note that electrons in metals also form a degenerate fermi gas .
as you note , this is really a question about cosmology . there is a lot to say , but in the interest of brevity i will just address a part of the question . quantum chromodynamics is a strongly-coupled theory with a very complex vacuum state . the universe is not in the qcd vacuum today , but is pretty darn close to it , since the 2.7k background temperature is way below the mass gap of qcd . at high temperature qcd is a lot simpler -- the quarks and gluons are weakly interacting ( asymptotic freedom ) . the state of high temperature qcd is fairly simple ( though not trivial ) ; it is approximately an ideal gas of relativistic particles . as the universe expanded and cooled adiabatically it passed through a ( first order ) phase transition from the " easier " high-temp state to the " harder " low temp state . just below the phase transition the state was still pretty hot , but as the universe continued to expand adiabatically the state approached the qcd vacuum . of course , this would not have worked if preparing the qcd vacuum were qma hard ; the universe would have gotten stuck is some long-lived metastable state . that apparently did not happen , so we have cosmological evidence that preparing the qcd vacuum is easy , even if jlp have not proved it ( yet ) . you can ask further questions about how the hot initial state was prepared , why it was so homogeneous and isotropic , etc . which would lead us into a discussion about how cosmic inflation started and ended , but that is enough for now .
it depends on the knocking technique . do you knock with a finger , or do you knock with a full hand ? i would estimate the released energy with the kinetic energy of the finger/hand . if you approximate a hand with a sphere of 8 cm diameter and density of water , and the velocity with 0.1 m/s . you get $$ e \approx m v^2= 0.25 kg * ( 0.1 m/s ) ^2 = 3 mj$$ which is an absolute upper limit
if one calculates it to all orders , it should not . in particular , if the coupling constant is small , the $o ( g^k ) $ accuracy calculation of the cross section obtained in different schemes can differ at most by $o ( g^k ) $ , negligible amounts . however , when one truncates the cross section at some order , the results may really depend on the renormalization scheme by subleading , higher-order terms . physically , cross sections are measurable so a valid theory must predict unequivocal values for it . and a renormalization scheme does not really " change the theory " .
from skimming a few articles and patents on e-ink driver technology , my impression is that the primary reason is that each microcapsule acts as a capacitor . once voltage is applied , the particles move to one electrode or the other and remain there because there is no drain path for the charge . the ' gooiness ' of the fluid helps , as evidenced by the typical approach of applying a " shaking pulse " sequence after a certain number of image transitions . this pulse sequence helps ensure that all the microparticles are freed up to be driven to the appropriate state . this may or may not be helpful : http://patents.justia.com/patent/20060170648
the excesses have looked convincing to many people but they do not look convincing anymore . last october , lux in south dakota presented the results of their superior analysis http://motls.blogspot.com/2013/10/fiat-lux.html?m=1 http://motls.blogspot.com/2013/10/dark-matter-wars-are-over-lux-safely.html?m=1 which safely excluded the theories of wimp dark matter directly suggested by cdms ii si and other experiments . the complete and pure absence of a signal in lux shows almost certainly that the excesses in cdms ii si and other experiments were due to some overlooked background ( non-dark-matter-related ) processes . wimp is still plausible and attractive as a model of dark matter . but there is no evidence one way or another which is why physicists keep on studying it and other models as well , including e.g. those of the sterile neutrino dark matter or axions .
no , because the uncertainty principle operates between position and momentum rather than position and velocity . for speeds much less than $c$ , momentum is just proportional to velocity : $p = mv$ . but at relativistic speeds we have to use the relativistic version , $$ p = \gamma mv , $$ where $\gamma = 1/\sqrt{1-v^2/c^2}$ . substituting this in and squaring both sides we get $$ p^2 = \frac{m^2v^2}{1-{v^2}/{c^2}} , $$ which we can rearrange a little to get $$ v^2 = \frac{p^2}{ m^2 + p^2/c^2 } , $$ or $$ v = \frac{p}{\sqrt{ m^2 + p^2/c^2 }} . $$ now , the limit of this as $p \to \infty$ is just $$ v = \frac{p}{\sqrt{p^2/c^2 }} = c . $$ the momentum $p$ can fluctuate due to the uncertainty principle , but now you can see that now matter how big $p$ gets , $v$ will always be less than $c$ .
the basic problem of modelling a star is covered in a number of textbooks and lecture notes . try searching for " stellar structure and evolution " or something along those lines . the best readily available lecture notes , imo , are those of onno pols , available here . there was also a similar post on quora , which you can read too . in the mean time , here 's the basic run down . to construct a reasonable stellar model in a reasonable amount of time , we make several assumptions . we assume that a star is a spherically symmetric , dynamically stable , self-gravitating fluid in local thermodynamic equilibrium . here 's how we unpack all this . first , spherical symmetry means one spatial co-ordinate . in this case , for a fluid , we can write down the equation of mass conservation : $$\frac{dm}{dr}=4\pi r^2 \rho$$ this just means that an infinitesimal spherical shell of thickness $dr$ at radius $r$ contributes $dm$ to the total mass inside radius $r$ . ( i will probably end up calling $m$ the mass co-ordinate . ) now , if we regard our star as spherically symmetric and dynamically stable , we can kick out the velocity terms and time derivatives in euler 's equation . supposing gravity is the only external body force , we end up at the equation of hydrostatic equilibrium : $$\frac{dp}{dr}=-\frac{gm\rho}{r^2}=-\rho g$$ remember , this follows from the conservation of momentum . to conserve energy , like mass , we say that the contribution to the total luminosity at $r$ is the mass of the shell times the specific energy generation rate $\epsilon$ , so we write $$\frac{dl}{dr}=4\pi r^2\rho\epsilon$$ to keep it simple , i have neglected to specify where $\epsilon$ will come from . it generally includes the energy generated by nuclear reactions rates , less the losses due to neutrinos streaming out in some reactions , plus—in some phases—the energy released by contraction . ( it can be shown that when a star contracts it heats up but loses energy overall . see the virial theorem . ) the energy generation depends on the density , temperature and chemical composition of the material . it is not something we know from first principles . instead , we use tables of data taken either from detailed calculations or laboratory experiments . we now have to describe how energy is transported inside the star . the equations are a bit of a mouthful , so i will not write them here , but basically energy can either be transported by radiation or convection , depending on the temperature structure . in either case , you get an equation of the form $dt/dr=$ ( some right-hand side , see the notes ) . in the case of radiation , the transport coefficient depends on the opacity of the stellar material , denoted $\kappa$ , which itself depends again on the density , temperature and chemical composition . ( strictly speaking , opacity depends on frequency , but we use a specific average opacity : the rosseland mean opacity . ) like the energy generation rate , this is not known from first principles : we use tabulated lab data . finally , as is usually the case in fluid problems , we have to close the system with an equation of state , which relates the pressure , density , temperature and chemical composition . it is the third equation for which we generally use lab data , although here we do also have some approximate analytic forms . these four equations ( three given + temperature transport ) are almost entirely independent of time , so they are sometimes called the structure equations . the three tabulated inputs ( energy generation , opacities and equation of state ) are sometimes called the matter or microphysics equations . so , why does a star evolve ? the answer is because the composition changes . suppose there are $n$ chemical species ( ${}^1h$ , ${}^4he$ , etc . ) , each of whose fractional mass abundance is denoted $x_i$ . then the nuclear reactions convert species $i$ into $j$ at some rate $r_ij$ , and we can write a set of equations $$\frac{dx_i}{dt}=\sum_j r_{ij}$$ the rates also depend on the material properties ( density , temperature , etc . ) . also , in truth , we expect convection to mix material on a dynamical timescale , so we throw in a monstrous diffusion coefficient in those regions . but that is basically it . given a composition profile , the structure equations tell you what the star looks like . then , the reaction rates dictate how the composition changes , and the structure changes accordingly through the matter equations . i have not gone into details like boundary conditions and whatnot , but if you are still interested , i recommend the notes ! they are aimed at a reasonably high level ( i would say late undergrad although there is no reason a second-year could not make sense of them ) but if you are familiar with other areas of physics it should be a synch . if you want to build models , you can try using polytropes for very simple ( but still useful ) models . or , i would recommend the modules for experiments in stellar astrophysics ( mesa ) package for a fully-fledged , research-grade modelling tool .
a spatial fourier transform means a fourier transform in the spatial variable ( $x\rightarrow k$ ) , while a temporal fourier transform is the same transformation , but in terms of the time variable ( $t\rightarrow \omega$ ) . the equation you have written is the ( asymmetric ) temporal fourier transform of $f ( k , t ) $ . the spatial transform looks like some variation of \begin{equation} f ( k , t ) = \frac{1}{2\pi} \int g ( x , t ) e^{i k x} dx \end{equation} where $g ( x , t ) $ is the ( one-dimensional ) van hove function .
yes , the photons actually reach you , like rain falling on you , not like watching rain from a distance . when you see a star , photons from the star actually enter your eye . in for example rods of your eye , the photon causes a molecule of retinal to react by change from cis to trans isomer .
i would suggest ' single- and double-slit diffraction of neutrons " by zeilinger , gahler , shull , treimer , and mampe , reviews of modern physics 60 ( 4 ) , 1067-1073 ( 1988 ) . if i might quote the abstract : the authors report detailed experiments and comparison with first-principle theoretical calculation of the diffraction of cold neutrons ( $\lambda \approx$ 2 nm ) at single- and double-slit assemblies of dimensions in the 20—100 $\mu$m range . their experimental results show all predicted features of the diffraction patterns in great detail . particularly , their double-slit diffraction experiment is its most precise realization hitherto for matter waves . so , single and double slit experiments with neutrons have indeed been done , and indeed show just what would be expected . here 's a brief description of how the experiment worked . they used neutrons from a reactor , which were brought down to room-temperature thermal energies using heavy water . they let those out through a collimator to make a beam . to make the beam monochromatic , they bent it through a quartz prism and selected one wavelength using a a slit . they verified the energies using time of flight ( i think using a beam chopper ) . to produce diffraction , they used a tiny double slit with a spacing of 0.1 mm . at a distance of 5 m from the double slit , the spacing of the fringes was about 0.1 mm , and they had to make the graph by slowly moving the detector across the fringes and measuring a count rate at each position .
yes , you have the right idea . you will want to learn about fourier analysis , which lets you take a complicated-looking waveform like your third figure and analyze it to say " this is two sine waves , frequencies 1 and 5 , equal amplitudes , zero relative phase . " i like to think of a piano as an inverse fourier transform machine : you push the keys to tell the piano " please generate frequencies c , e , and g , with the c having larger amplitude than the others " and the piano makes the air vibrate for you . your auditory system then does the ordinary fourier transformation : with ear training , you can take those vibrations and say " oh , a major triad , with a strong root . "
the following reactions take place : $o_2 + 4 e^− + 2 h_2o → 4 oh^−$ $fe → fe^{2+} + 2 e^-$ $4 fe^{2+} + o_2 → 4 fe^{3+} + 2 o^{2−}$ $fe^{2+} + 2 h_2o ⇌ fe ( oh ) _2 + 2 h^+$ $fe^{3+} + 3 h_2o ⇌ fe ( oh ) _3 + 3 h^+$ $fe ( oh ) _2 ⇌ feo + h_2o$ $fe ( oh ) _3 ⇌ feo ( oh ) + h_2o$ $2 feo ( oh ) ⇌ fe_2o_3 + h_2o$ ( source : wikipedia ) now since initially only $fe$ was present and finally its oxides are present in the sample , there is definitely an increase in the mass of the sample .
( sorry i have to split this up because stackexchange will not let me post more than two links ) even helium atoms ( at reasonable temperatures ) do not tunnel through graphene , as the potential energy barrier is far too high . ( source : leenaerts et al . 2008 )
charging , as nibot said , happens because the " balance of charge " is altered . if you have a neutral iss , and you assume that no electrons are added or removed , the iss will stay neutral . this can of course change , and the iss can charge , if electrons are removed or added due to the exposure to ionizing radiations . however , you are right to say that if a metal moves in a magnetic field , you obtain a charge separation , and consequently a voltage . the same concept is used in any dynamo to produce electricity . there was indeed an experiment using this concept : http://en.wikipedia.org/wiki/electrodynamic_tether http://en.wikipedia.org/wiki/tether_satellite it was deployed twice with little success , but it did produce a ( small ) voltage , and therefore create a current .
i interpreted your question differently , more like a mathematics question . in quantum mechanics , we basically have an equation , the schrödinger equation , which is a differential equation on the space of square-integrable complex valued functions . this space is a hilbert space , which means that it is a vector space , and it also has a nice topological structure , basically all cauchy-sequences of vectors converge in that space . in newtonian mechanics , the equations are defined on phase space , which is basically a $6n$-dimensional space , $n$ is the total number of particles , on which coordinates for a point consist of the positions and momenta of each particle you want to describe . the solution of the equations induces a flow on this phase space . the structure of phase space is usually that of a symplectic manifold . in general relativity , the equations are einstein 's field equations . they link the riemann tensor to the energy-momentum tensor . they are difficult to solve in the sense that they are nonlinear and you have to specify an energy-momentum tensor , but this tensor will also depend on the geometry of space-time , thus the riemann tensor . so you have to solve in one go for the geometry and energy-matter distribution . in practice , many simplifying assumptions will be made . but the " space " of solutions is the space of geometries and energy-matter distributions compatible with the field equations .
"2 . where did they even get 2sa+sb=0 from ? " from the assumption that the length of the rope does not change . " why did they determine the change in distance this way ? my first assumption was that if block b moved up 2ft , then block a should move down 2ft ( the rope must " move " 2ft too right ? ) . " no , there is a difference between a movable pulley and a fixed pulley , so a moves down 1 ft . "3 . where did 2va=−vb come from ? " from the same assumption as above . however , the direction of va is shown incorrectly ( or , alternatively , the signs in the formula are wrong ) . "4 the fbd for block a is confusing , why is the friction force fa in the direction of the ropes ? i thought it was block b that is going down ? " it is writen in the statement of the problem that block b went up . " am i the only one who had trouble deducting that the pulley and block a are the same object ? " i do not quite understand what this phrase means exactly and how it is relevant . "5 . look at the final answer , how could vb be negative ? the problem says block b goes up . " see the answer to your item 3 .
dear leandro , it is because the pauli matrices , together with the $2\times 2$ identity matrix , form a full real basis of all the hermitian $2\times 2$ matrices ( note that $2\times 2$ hermitian matrices depend on four real parameters ) , and the identity matrix is irrelevant in a hamiltonian because it is just a conventional energy shift that acts on all vectors equally and does not create any subtleties such as line crossing so one may omit it in any discussion of interesting physical effects . so any $2\times 2$ hermitian matrix that " matters " - and yes , hamiltonians have to be hermitian operators - is a real linear combination of the three pauli matrices . they do not have to be interpreted as a spin of any kind . but they are still a more natural basis than any other basis of the hermitean matrices because the product of any pair of the matrices generates a multiple of another matrix in the basis , thus simplifying all the calculations . ( analogy with the conventional $so ( 3 ) $ generators is a more transparent way to see the power of this basis . ) but even if you chose a less clever basis than the pauli matrices , you could derive the same results . the equations could just become a bit more cumbersome . line crossing etc . is first observed at degeneracy 2 . however , one may also study $3\times 3$ or $n\times n$ matrices . conventional bases of the hermitian matrices are called the gell-mann matrices in the case of $su ( 3 ) $ - because gell-mann generalized the pauli matrices when he studied the strong force where $su ( 3 ) $ enters in two ways . the $n\times n$ hermitian matrices depend on $n^2$ real parameters . one usually takes the identity matrix to be one of the basis vectors ; the remaining $n^2-1$ vectors are " generators of $su ( n ) $" .
it is not the four-force that is conservative , but the einstein definition of force , $$ f= {dp\over dt}$$ this force for a particle in an electromagnetic or linearized gravitational field is conservative in the same way as in newton 's model : the force is $$ f = qe$$ and the integral of a static e around a closed loop is zero , still in relativity . the reason is explained in this answer : a priori validity of $w=\int fdx$ in relativity ? . the integral of the force over the distance as einstein defines it is still the work done in the relativistic system .
please do however remember the following line from the link you yourself have cited : in general , if the behaviour of a system of more than two objects cannot be described by the two-body interactions between all possible pairs , as a first approximation , the deviation is mainly due to a three-body force . hence , it can be seen that , initially , when people were thinking of many-body problems , they encountered terms in the mathematical formulation which they later termed as many body forces . incidentally , they were discovered in strong interactions and were a result of gluon mediation . in the celestial scale hence , you would need to have a similar mediating phenomena/theory to explain any physically valid three body force . also related : n-body forces in classical mechanics
i could be wrong , but in my understanding , you are describing and justifying steady state , not detailed balance . in thermal equilibrium , steady state is true always , and detailed balance is true sometimes . detailed balance means that the rate $x \rightarrow y$ is always the same as the rate $y \rightarrow x$ . if a system is both in thermal equilibrium and has time-reversal symmetry , you will have detailed balance . if time-reversal symmetry is broken , for example an electrolyte in an external magnetic field , you can have thermal equilibrium but you will not necessarily have detailed balance . the process $x \rightarrow y$ might be balanced by $y \rightarrow z \rightarrow x$ , instead of being balanced by $y \rightarrow x$ . as an explanation of steady-state , your " continuity equation " explanation is fine . but in my opinion you can say the same thing more clearly without using the words " continuity equation " or writing down any math . if you just say that the number of times per second that the system enters state y equals the number of times per second that the system leaves state y , i think that is intuitively sensible .
if a crystal has a discrete group of point symmetries then the electronic eigenfunctions will be suitably invariant under that group . formally , the symmetry requires that the eigenfunctions of a hamiltonian with symmetry group $g$ belong to the various representations of that group . in the abstract , a representation of a group $g$ is a vector space ( in this case a subspace of degenerate energy ) $v$ and a " recipe " for unitarily transforming the vectors in $v$ with the transformations from $g$ , in the form of a group homomorphism $r:g\rightarrow u ( v ) $ . if one knows the structure of a group ( in the form of its multiplication table ) then there is a lot that can be said about its possible representations , which are typically denoted by some standard notation ( e . g . $e$ , $a$ , $b$ , etc . ) . the wavefunctions are then labelled by the representation type of the subspace they belong to . to bring this back down to angular momentum , the subspaces with different $l$ are the different representation subspaces . the group in question is the rotation group $\text{so} ( 3 ) $ . it has an infinite family of representations of increasing finite dimension , and the index $l$ that labels them is precisely the angular momentum quantum number of those wavefunctions . in group theoretic terms , then , " having definite angular momentum " simply means " belonging to a suitable representation of $\text{so} ( 3 ) $" . thus a crystal with a point symmetry will have electronic eigenfunctions that do have " definite crystal angular momentum " , in the sense that they belong to a certain representation of the point group . added in response to comment : unfortunately , there is no physical quantity that corresponds to this symmetry . this is due to the general fact that discrete symmetries have no generators . while you can write rotations , for example , in the form $e^{i\mathbf{j}\cdot\hat{\mathbf{n}}\theta}$ , where $\mathbf{j}$ is the generator , this is not really meaningful for discrete symmetries . a good comparison for this is parity : if $\pi$ commutes with $h$ then we say parity is conserved , in the sense that the transformation itself is a constant of the motion . for a more general discrete group $g$ ( instead of $g=\{-1,1\}$ for parity ) then the labels $+$ and $-$ are replaced by the group representation . similarly the labels $l$ and $m$ correspond to the group representation and to the eigenvalues of some particular group transformation . both are conserved under $h$ , but there is no generator .
method 1 is universal , method 2 works only for dc . method 2 is for dc supply , whereas the method 1 is for ac supply . in ac circuit the inductor ( a coil ) and a capacitor pose resistance which is calculated as impedence of circuit . . whereas in dc , capacitor fully blocks the supply and thus capacitive reactance is infinite and the inductive reactance is zero . thus the current in a capacitive circuit is zero . z^2=r^2 + ( xl -xc ) ^2 ; z ( dc ) =r ; also the difference of xl and xc is small and thus answer is approximately the same by the method 2 in ac circuits too .
this is a notation from differential geometry that is unusual in physics ( as far as i am aware ) . the operators $\frac{\partial}{\partial y}$ and $\frac{\partial}{\partial z}$ are basis vectors for vector fields ; see e.g. this planetmath article for an introduction ( in particular where it says " in some sense " :- ) if you do not want to worry about differential geometry , you could write the field in good old-fashioned vector notation as $$ e=\pmatrix{0\\e_y ( t-x ) \\e_z ( t-x ) }\ ; , \quad b=\pmatrix{0\\b_y ( t-x ) \\b_z ( t-x ) }\ ; . $$
classical chaotic systems can be used to generate random numbers . specifically , if a system is chaotic then it will have a positive lyapunov exponent and so will be unpredictable . although classical mechanics is deterministic , it is not possible to know the initial conditions to infinite precision . therefore it is not possible to predict the future state of a chaotic system . if the future state cannot be predicted by any means , then it is random . you would have to make a study of the particular system in order to determine the rate at which randomness is produced , and the best way to extract it , but it can be done . chaotic systems are all around us ( the weather , turbulence , various electronic circuits , etc . ) however , in a practical sense , it is not possible to do anything without quantum mechanics since you live in a quantum world . in other words , anything you build in this physical world will , underneath it all , be quantum mechanical . now , with quantum mechanics there is something very nice that you can do . it is in fact possible to build a random number generator in which the numbers produced are certifiably random , even if you do not trust the hardware ( say , the hardware was built by your adversary ) . for more information on this , search for " certifiable quantum dice " by umesh vazirani ( which i have not read ) .
turbulence is indeed an unsolved problem both in physics and mathematics . whether it is the " greatest " might be argued but for lack of good metrics probably for a long time . why it is an unsolved problem from a mathematical point of view read terry tao ( fields medal ) here : http://terrytao.wordpress.com/2007/03/18/why-global-regularity-for-navier-stokes-is-hard/ why it is an unsolved problem from a physical point of view , read ruelle and takens here : http://www.ihes.fr/~ruelle/publications/%5b29%5d.pdf the difficulty is in the fact that if you take a dissipative fluid system and begin to perturb it for example by injecting energy , its states will qualitatively change . over some critical value the behaviour will begin to be more and more irregular and unpredictable . what is called turbulence are precisely those states where the flow is irregular . however as this transition to turbulence depends on the constituents and parameters of the system and leads to very different states , there exists sofar no general physical theory of turbulence . ruelle et takens attempt to establish a general theory but their proposal is not accepted by everybody . so in answer on exactly your questions : yes , solving numerically navier stokes leads to irregular solutions that look like turbulence no , it is not possible to solve numerically navier stokes by dns on a large enough scale with a high enough resolution to be sure that the computed numbers converge to a solution of n-s . a well known example of this inability is weather forecast - the scale is too large , the resolution is too low and the accuracy of the computed solution decays extremely fast . this does not prevent establishing empirical formulas valid for certain fluids in a certain range of parameters at low space scales ( e . g meters ) - typically air or water at very high reynolds numbers . these formulas allow f . ex to design water pumping systems but are far from explaining anything about navier stokes and chaotic regimes in general . while it is known that numerical solutions of turbulence will always become inaccurate beyond a certain time , it is unknown whether the future states of a turbulent system obey a computable probability distribution . this is certainly a mystery .
the $k$ notation is generally used to describe friedmann robertson walker cosmological models . these are built on the assumptions of homogeneity and isotropy . the spacetime can be described as being foliated by spatial slices of constant curvature . the k value is the sign of this spatial curvature if the {-1 , 0 , +1} convention is adopted . as the curvature is a constant , it makes sense to talk of its sign . further details here .
note that phase angle is not a real angle , it is just a convenient description of timing differences . one wavelength is $360^\circ$ by definition , so you simply find how large a fraction of a wavelength the waves are off and multiply that by $360^\circ$ . depending on the context you might also want to normalize the result to the range $ [ 0^\circ , 180^\circ ] $ , that is first normalizing to the range $ [ -180^\circ , 180^\circ ] $ by adding or subtracting an integer number of $360^\circ$ , then taking the absolute value .
the energy formula $$\tag{39.11} e~=~\frac{1}{2}mv^2 -\frac{1}{2} m ( {\bf \omega} \times {\bf r} ) ^2 + u $$ in ref . 1 ( of a point particle , as seen in a rotating reference frame $k$ ) consists of three terms : kinetic energy : $\frac{1}{2}mv^2$ . centrifugal potential energy : $-\frac{1}{2} m ( {\bf \omega} \times {\bf r} ) ^2$ . other potential energies $u$ . in particular , the minus sign in front of the second term is the correct one . it is a centrifugal potential , so it encourages the system to increase its radial coordinate $r$ . phrased equivalently , it costs work ( against the centrifugal potential ) to reduce the radial coordinate $r$ . references : l.d. landau and e.m. lifshitz , mechanics , vol . 1 , 1976 .
edit : i am not sure what specifically you are after . i will explain a little more why it is difficult to give you a number . it is also quite possible that the number you are seeking might not be what you think it is . . . . the decay would be similar to any plasma type reaction for a mercury-vapor gas . there is a delay between absorption and re-emission of light photons and a typical electrical system driving the energy is not going to stop instantly . in fact the dominate feature in the decay rate will probably be due to the slow discharge of the ballast system driving the gas . the common household circuit lighting up a tube usually has a large transformer with a big magnetic field to step up the voltage and resonate the gas to force the plates to conduct through the ionized gas and light up . it takes a considerable amount of time for this electrical system to discharge and while it does it is going to continue to drive energy to the lamp causing it to light and effecting your " decay " rate significantly . here is a 12v lamp driver just for you to see what is involved electrically : note : the large cap ( 0.047uf ) and the transformer are going to resonate the lamp energy much longer than the actual natural decay rate of the gas . in addition the 47uf cap is going to supply power to the circuit for a non-trivial amount of time after the 12v is removed . for comparison here is an 120v 60hz ballast design and you will see there are similar issues . if you want to look at just the gas decay rate then you might have to excite it with a laser for more precise measurements . the states that the energy moves through is usually measured and plotted on a jablonski diagram like this : in your case $\tau=\frac{1}{k}$ where k is equal to ( in the de-excitation case ) $k = k_f + k_i + k_x + k_{et} + …= k_f + k_{nr}$ where $k_f$ is the rate of fluorescence , $k_i$ the rate of internal conversion and vibrational relaxation , $k_x$ the rate of intersystem crossing , $k_{et}$ the rate of inter-molecular energy transfer and $k_{nr}$ is the sum of rates of radiationless de-excitation pathways . and a more detailed model can be measured : the method for measuring a gas properly looks like : here is an example of what the data looks like . much of this information was taken directly from the research documented here : www.jh-inst.cas.cz/~fluorescence/support/lectures/ufch_fluor03.pps‎
the result $$ a=\cos ( \frac{\beta}{2} ) e^{i\theta_a} $$ is simply a solution to the original set of two linear equations $$ ( \sin ( \beta ) \cos ( \alpha ) -i\sin ( \beta ) \sin ( \alpha ) ) b+\cos ( \beta ) a=a \\ ( \sin ( \beta ) \cos ( \alpha ) +i\sin ( \beta ) \sin ( \alpha ) ) a-\cos ( \beta ) b=b$$ those equations may be rewritten as $$ \sin\beta \exp ( -i\alpha ) b = ( 1-\cos\beta ) a\\ \sin\beta\exp ( +i\alpha ) a = ( 1+\cos\beta ) b $$ i have only accumulated terms by using $\exp ix = \cos x + i\sin x$ and rearranged the terms . now , divide the first equation by the second to get $$\exp ( -2i\alpha ) \frac{b}{a} = \frac{a}{b} \frac {1-\cos\beta}{1+\cos\beta} $$ it means , using $1+\cos\beta = 2\cos^2 ( \beta/2 ) $ etc . , that $$\frac{b^2}{a^2} =\exp ( 2i\alpha ) \frac{\sin^2\beta}{\cos^2\beta} $$ taking the square root , $$\frac{b}{a} = \pm \exp ( i\alpha ) \tan\beta $$ the absolute value of this equation is $$\left| \frac ba \right| = \tan\beta$$ because the ambiguous sign and the extra phase may be ignored . but because the ratio $b/a$ is $\sin\beta/\cos\beta$ and $|a|^2+|b|^2=1$ , which is also obeyed by the sine and cosine , it follows that $$|a|=\cos\beta , \quad |b|=\sin\beta$$ in fact , we were also able to derive that the relative phase of $b , a$ is $\pm\exp ( i\alpha ) $ ; it is plausible that only one of the signs is right and you may derive which one . the absolute phase is not determined : you may always find another solution by multiplying $a , b$ by the same phase $\exp ( i\gamma ) $ . at any rate , because we know the absolute values of $a , b$ , we may write them as the absolute values times some phases .
light has a dual nature , one of photons and the other of waves . but energy does not really travel in waves . so what do the wave represent ? let us be clear in our terminology and the domain to which we apply it . our everyday life is lived with classical mechanics and classical electricity and magnetism ( as long as we do not use the net and transistors and the other paraphernalia of modern life ) . classical theories are well developed mathematically and are applicable in the domain where hbar can be considered practically zero . particularly for light maxwell 's equations have been validated in this domain and describe light as a wave . this wave is a propagating changing electric and magnetic field and is a wave in the four dimensional space time , there are peaks and valleys . it displays the classical wave behavior of interference and dispersion . these carry energy , a universal example is simple sunlight . now you use the word photon . the photon is one of the elementary particles of the standard model . this means that we are no longer in the classical domain but in the quantum mechanical domain when we speak of light as a collection of photons . it means that it behaves sometimes as a particle , ( photoelectric effect ) i.e. it has a specific ( x , y , z , t ) value and sometimes as a probability wave , i.e. according to a quantum mechanical wave function the square of which gives the probability of finding a photon at a specific ( x , y , z , t ) which probability has a wave like variation in space because it is the solution of a wave type differential equation describing the dynamics of the situation . note , it is the probability which shows a wave nature , not the " particle " itself . now the wave nature of maxwell 's equation and the wave nature of the probability function are mathematically reconcilable , so that the frequency of the classical wave is the nu in the energy of the photon in e=h*nu . in addition lubos motl has an interestin article about the collective emergence of classical light from photons in his blog , though it needs a background in physics to understand it .
first of all , light waves and matter waves may be treated together , using the same maths , because the waves associated with light and the waves associated with matter are fundamentally the same thing . second , all the waves before they interfere and after they interfere may be written in terms of the probability current $j^\mu ( x , y , z , t ) $ , and its transformation from one frame to another is obtained by a simple lorentz transformation of the coordinates $ ( t , x , y , z ) $ . in the non-relativistic limit , the galilean transformation will work as a good approximation of the lorentz transformation . third , whenever a wave – locally or globally – has a well-defined energy-momentum vector of the corresponding particle ( whether it is a photon , electron , or something else ) , then the wave has a simple plane-wave form $$ \psi = \exp ( ip^\mu x_\mu ) $$ at least the frequencies and wavelengths are determined by this simple complex exponential . note that $x_\mu$ and $p^\mu$ transform according to the lorentz transformation , in the same way , and their inner product – the argument of the exponential – is therefore invariant under lorentz transformations which is why the phase above is invariant , too . one does not need to make " explicit " checks ; it is clear that the predictions for the locations of the interference minima and maxima will be the same in all reference frames . now , if the momentum in the direction between " slit and the photographic plate " which i will call the $z$-direction is $p_z$ and the total energy is $e$ for the interfering particle , then $ ( e , p_z ) $ is transformed by the standard lorentz transformation when boosted by the velocity $v$ . this transformation of $ ( e , p_z ) $ to $ ( e ' , p'_z ) $ is equivalent to the calculation of the velocity $v$ of the ( wave-represented ) particle $$ v = \frac{p_z c^2}{e} $$ ( the denominator is the total energy so in the non-relativistic limit , it has to be replaced by $m_0 c^2$ and not by $mv^2/2$ ) and composing this $v$ with the $v$ by the relativistic composition formula , $$v'= ( v+v ) / ( 1+vv/c^2 ) $$ for photons and other particles moving at the speed of light , the speed is $v=c$ and we get the " new " speed as $v'=c$ again . that is not too informative because for photons , the frequency or energy is not encoded in the speed ( the latter is always $c$ ) . the transformation of $e , p_z$ – that obey $e=|p_z c|$ for $z$-directed photons – under the lorentz transformation given by the motion at speed $v$ is given by the doppler shift – both the momentum and the frequency get rescaled by $$ \sqrt{ \frac{c+v}{c-v} }$$ the wavelength in the $z$-direction gets contracted by this factor . however , the distance between the slits and the plate gets contracted as well , and the time gets shortened because the plate is moving " against " the photons . these three modifications are consistent with each other because the number of wavelengths between the slit and the plate gets multiplied by $$ \frac{ {\sqrt{1-v^2/c^2}} }{ \sqrt{ \frac{c+v}{c-v} } } = 1-\frac vc$$ which is exactly the factor by which the time spent between the slit and the plate will shorten because the plate is moving against the photons . let me emphasize once again that if we only change the speed in the $z$ direction , the transverse momentum components $p_x , p_y$ and the corresponding components of the wave vector $k_x , k_y$ will remain unchanged . that is true for relativity and non-relativistic physics , massless as well as massive particles .
a square has its diagonals at right angles . so , find the forces along each diag . i.e. , m1 and m3 on m5 for one direction and m2 and m4 on m5 for other . you could actually simpy find difference between , " m1 and m3" and " m2 and m4" and the corresponding charges and use them as resultant mass and charge .
assuming $m_e$ means the mass of an electron , a photon with that much energy would be a gamma ray . the mass of an electron is about $ . 51 mev/c^2$ , and a photon with an energy of $ . 51mev$ is called a gamma ray .
i suppose that what you are thinking about is the principle of causality . we have two events : a cause and an effect , where the second event is a consequence of the first . that is how we perceive all events around us and what we intuitively accept as true . in physics , however , we sometimes obtain two different solutions : first with the cause before the effect , second with the effect before the cause ( such solutions come , for example , when deriving potentials dependent on time in electromagnetics ) . you could say that the second one reverse the arrow of time . indeed , there is no reason other than the principle of causality to discard this solution ( when time flow backwards ) , but this is what is usually done . interestingly , lack of that principle would suggest that we do not have free will and the universe is deterministic .
this class of problem is very simple or very complicated , depending on the ratio of the momentum of the bullet to the momentum lost by air drag . you have chosen the unfortunate latter case . basically , if the bullet 's velocity decreases significantly you will need some calculus for it . in the " simple " case where the velocity does not drop much , we will wind up reducing a lot out and using the mass-thickness of the atmosphere , $\mu \approx \rho h$ . $$ f_d = \frac{de}{dh} = \frac{1}{2 } \rho a c_d v^2$$ with a little magic . . . $$ \delta e = \frac{1}{2 } \mu a c_d v^2$$ to get the numbers : lets assume it has a pointy bullet shape ( best know shape ) with 5cm diameter and the orbit is the lower possible out of atmosferic resistance , lets assume the iss ( 150km ) . $$c_d = 0.42$$ $$ a = \pi \left ( 2.5 cm \right ) ^2 $$ $$ \mu = 10 \frac{ tons }{ m^2 } $$ $$ \frac{1}{2 } \mu a c_d = 3.75 kg $$ this is very bad for you , because your bullet weighs $100 g$ . what is the formula for kinetic energy of the bullet at launch ? why that is $\frac{1}{2} m v^2$ of course . let me use a qualitative picture . imagine that the atmosphere is a coherent ceiling with no significant depth . you bullet has to punch through it , but it makes a " perfect hole " when doing this . in short , your bullet 's momentum is decreased because it accelerates the mass in that hole to its same velocity . it picks up that hole and keeps moving with it . you should look at this as basic momentum balance . if the ceiling 's area within that hole is twice the mass of the bullet , the bullet 's velocity will be decreased by half . here , the mass of the hole it has to punch is 37.5 times the mass of the bullet . i hope you see where this is going . with this mental model , the answer is quite obvious . the velocity of the bullet must be 38.5 times orbital velocity . that is because the combined mass of the bullet and the air that it " picks up " in its trip is 38.5x the mass of the bullet . but this would only work if you shot the bullet straight up at orbital velocity . this would be quite useless . in actuality , you want to shoot it at a shallow angle - as shallow as possible so that the boosters that are used to circularize the orbit do not have to be as big . this correction is just dividing by a sin function . if you shoot 90 degrees straight up , then the above ratio holds . if you shoot at an angle lower than this , then divide by sin of the angle ( or multiply cosec ) , and that is the ratio for the extra mass of air you pick up . lets assume the iss orbit 150km above ground , we are at equator latitude this establishes that the apogee should be at the 150 km elevation , if you wanted to do this with only one rocket burn . that actually does not fully specify the problem . the perigee is unknown , but we know that it can not be higher than the surface of earth . still , that does not eliminate the possibility of shooting it straight up and doing the rocket burn for the full orbital velocity . even at a 45 degree shooting angle , the orbit would be extremely elliptical , and thus very far from full orbital velocity . nonetheless , even if you did shoot it straight up , you would require $1.2 km/s$ . then using the air resistance multiplier , that would be $46 km/s$ , which is just plain impossible to produce ( except with nuclear bombs ) . in short , you would send something heavier to get around this problem .
as several of us have argued beneath your other question here : special relativity version of feynman 's " space-time approach to non-relativistic quantum mechanics " quantum mechanics combined with special relativity needs one to use quantum field theory ( or a theory that is stronger than that ) . if one wants to study relativistic particle in the presence of slits , one may imitate it simply by putting various ( absorbing ) boundary conditions ( for the fields whose excitations are the interfering particles ) at the places where the boundaries of the material reside . the probability waves in space propagate according to the standard free equations , so interference measures the free particles ' propagators . in reality , the only relativistic particles whose interference may be observed in practice at present are photons . the mathematics of interference of individual photons is equivalent to the interference of classical electromagnetic waves - as they studied it since the early 19th century . the probability density is mapped to the energy density of an electromagnetic wave and the $e , b$ fields may be interpreted as the photon 's wave function . the results for the interference patterns are , up to a normalization that can be determined from the total number of particles , identical to the case of the classical electromagnetic waves . so papers for photons exist and the first ones were written in the early 19th century . papers for other particle species do not exist because they are simple sums of the well-known functions that govern the propagator of the corresponding probability waves in the space ; for example , one only needs the simple , free one-particle dirac equation to calculate the propagation of the electrons at any speeds the interference can only be measured for photons whose mathematical description is a very old story . for example , relativistic electrons must have a wavelength comparable to the compton wavelength of the electron , something like $10^{-12}$ meters , or even shorter ( ultrarelativistic electrons ) . the corresponding inteference pattern would probably be too tiny to be seen cheers lm
there is an approximate law that states that the frictional force , $f$ , is given by : $$ f = \mu w $$ where $w$ is the normal force and $\mu$ is the coefficient of friction ; $\mu$ is normal taken to lie in the range zero to one . for a car the weight $w$ is given by $mg$ , and the acceleration would be $f/m$ , so dividing through by $m$ we get th acceleration to be : $$ a = \mu g $$ if $\mu$ has the maximum value of one then the acceleration cannot exceed $g$ because trying to accelerate faster would just make the tyres spin . but . . . the equation we started with is an approximation , and friction is actually a far more complicated phenomenon that that simple law suggests . for example car tyres deform and can key into irregularities on the road to increase the friction . racing car tyres can have effective values of $\mu$ far greater than one , so they can accelerate at more than 1g . in fact a drag racer can accelerate at around 4g .
an element of the direct sum $h_1\oplus h_2\oplus . . . $ is a sequence $$\psi = \{\psi_1 , \psi_2 , . . . \}$$ consisting of an element from $h_1$ , and element from $h_2$ . . . etc ( countable ) . this has to have the special property that the sum $$\left\| \psi_1\right\|^2+\left\| \psi_2\right\|^2+ . . . $$converges . this convergence is part of the definition of direct sum . to show that the direct sum is a hilbert space , i need well defined addition operation . if i have a pair of such things : $$ \psi = \{\psi_1 , \psi_2 , . . . \}$$ $$ \phi = \{\phi_1 , \phi_2 , . . . \}$$ i can define their sum to be the sequence $$ \phi+\psi = \{\phi_1+\psi_1 , \phi_2+\psi_2 , . . . \}$$we need to check that $$\left\| \psi_1+\phi_1 \right\|^2+\left\| \psi_2+\phi_2 \right\|^2+ . . . $$converges . to do this , we use the fact that the individual terms satisfy $$\left\| \psi_i+\phi_i \right\|^2 $$ $$=\left\| \psi_i\right\|^2+\left\| \phi_i\right\|^2+ ( \psi_i , \phi_i ) + ( \phi_i , \psi_i ) $$ $$ \leq \left\| \psi_i\right\|^2+\left\| \phi_i\right\|^2+2\left\| \psi_i\right\|\left\| \phi_i\right\| $$ $$ \leq 2\left\| \psi_i\right\|^2+2\left\| \phi_i\right\|^2$$so convergence follows from the convergence properties of the individual spaces being summed . so this shows how we can add elements of the direct sum . scalar products follow in the same way . to define an inner product , we just add the inner products component wise , i.e. $$ ( \psi , \phi ) = ( \psi_1 , \phi_1 ) + ( \psi_2 , \phi_2 ) + . . . \ \ \ ( 1 ) $$to show the rhs converges , note $$| ( \psi_i , \phi_i ) | \leq \left\| \psi_i \right\| \left\| \phi_i \right\| $$but $$2\left\| \psi_i \right\| \left\| \phi_i \right\| \leq \left\| \psi_i \right\|^2+ \left\| \phi_i \right\|^2$$so the rhs of ( 1 ) converges absolutely and we are done - we have a well defined inner product space .
you can use a negative charge to test an electric field . you just have to remember that the electric field points antiparallel ( opposite ) to the force on the charge , rather than parallel to it ( in the same direction ) . that is just a convention , though ; we could have defined the electric field to point with the force on a negative charge , and physics would work the same , except for a couple of negative signs in some formulas .
why are you looking for a radial surface . . ? look it as an equipotential surface ( a surface where all points are at same constant electric potential ) as it comes with sphere . hence , you can assume the points a to b as radial to find the potential difference .
confine the motion to a plane , and for convenience make it the complex plane . describe the position of a particle in uniform circular motion by $z = re^{i\omega t}$ . then the jerk ( derivative of acceleration} is $z&#39 ; &#39 ; &#39 ; = -i\omega^3 z$ . compare that to velocity $z&#39 ; = i\omega z$ . the jerk points opposite the velocity . thus , to move in uniform circular motion with $f = ma&#39 ; $ , you must constantly feel a force pushing you backwards , opposite your current direction of motion . additionally , you must have the correct initial conditions for $z ( 0 ) $ , $z&#39 ; ( 0 ) $ , and $z&#39 ; &#39 ; ( 0 ) $ . presumably , a string could not provide this force , because the force would still act in the direction of the string . we would have $z&#39 ; &#39 ; &#39 ; \propto z$ , the force in the direction of the motion , if the string followed hooke 's law . the solutions are $z = e^{\alpha t}$ with $\alpha$ some constant times a cube root of unity . the interesting solutions have both periodic and exponential behavior , so the particle would spiral out or spiral in with this force law . free particles in this universe move the same way particles with constant force move in our universe - on parabolas .
a rapid change of volume means that there is little time for heat to escape ( adiabatic process ) . when you compress a gas , you are doing $p \delta v$ work on the gas , and the internal energy of the gas must change . there is nowhere for this energy to go and since the internal energy of the gas is proportional to temperature ( equipartition theorem / ideal gas properties ) the temperature must change . if you compress said gas and then it reach thermal equilibrium with its surroundings , then you can again extract some of this energy in the form of heat , so that it reaches room temperature . now , if you let it expand again , it is temperature will further drop . repeating this process you eventually reach a point in its phase diagram where the gas is a liquid .
the gravitational , massless , bosonic sector of the string effective action contains the metric tensor , and at least one more fundamental field φ , the dilaton . by comparing the einstein’s ( d+1 ) dimensional einstein-hilbert action with the effective tree level action mentioned before a relation between the effective string coupling ( also fixing the g constant ) and the dilaton field becomes manifest . the dilaton in string theory also can be rescaled to absorb trivial volume factors associated with compact spaces , but are also present in the non-compacted string models . the dilaton is a fundamental scalar field in closed string theory . the effective gravity equations in string theory includes the gravi-dilaton part that looks very similar to brans-dicke scalar-tensor theory of gravity ( this is valid only at tree level ) . the dilaton field , as mention before , controls the string coupling constant so the genus expansion in string theory is directly related to the dilaton field and to corrections to general relativity . there is also the possibility of a dilaton potential in noncritical dimensions . this creates the possibility that the dilaton field expectation value be fixed at a local minimum ( probably in a non-perturbative regime ) , fixing the coupling between strings . the dilaton field is then an essential component of all superstrings models , and thus of the cosmological scenarios based on effective string actions . chapter 9 in maurizios gasperini’s string cosmology book is a very nice introduction to dilaton phenomenology and its importance in cosmology .
the simple answer : satellites do feel this force , but obviously do not get ripped apart . the tidal forces are simply too small ( for the satellites ' materials ) to actually rip them apart . the why : tidal forces happen because one side of an object feels such a larger huge difference in force than the other side . the magnitude of the force not only has to deal with the size of things pulling on each other , but the distance . even massive things ( like the sun or jupiter ) have relatively little pull when very far away . if you get close , you feel the effects of them much more strongly and the effects increase more quickly ! due to the fact that io is a moon very close to other large bodies , this makes the difference of the force of gravity on one side is very different than that on the other . ( try plugging in correct values for io , jupiter , and the distance between them ; then try calculating the force of jupiter on io as seen from one side of io to the other . ) most man-made satellites around earth are much , much smaller than moons and they are surrounded by very distant or very small things . this makes the difference between the force of gravity on one side of the satellite is almost the same as the force of gravity on the other . this being said , if a man-made satellite were to be in the same situation as io , either being very large or being very close to other big things , it could get ripped to shreds . in short : it is all about what forces are being applied . distant objects apply small forces , close objects apply bigger forces . tidal forces happen when gravity changes wildly between your head and your feet .
does the stars in clusters rotate ? yes . all stars rotate to a greater or lesser extent whether they are in clusters , galaxies or whatever . broadly , move massive stars rotate more ( and are more often in binary systems ) and there is nothing special , as far as i know , about this trend continuing in clusters . does cluster 's stars have moons . if yes do they rotates/orbits or are they " frozen " in space ? this question does not make sense . moons do not orbit stars : they orbit planets . if you mean " do cluster 's stars have planets ? " i am not sure if we know . given that we are currently finding stacks of planets around stars , i there are probably planets around some cluster stars but they are too far away to see ( and will be for some time ) . and those planets will behave just like planets around other stars . they will rotate and follow an orbit . ( i am not sure what you mean by " frozen " in space ? ) what will happen if one of the stars blows up ? the main effect of supernovae in clusters is to expel interstellar gas from the cluster . that is , there are clouds of gas in the cluster at first and the fast-moving gas from a supernova blasts other gas out too . if other stars are very nearby they might be heated a bit but stars are quite hardy things when it comes to being irradiated by supernova . they would not be vapourized , for example . does that structure attracts or repels space objects ? clusters would attract " space objects " by exerting gravity on them . but actually , clusters repel objects in a way . when a star in a cluster approaches a pair of stars in a binary system , the tendency is for one of the three stars to be thrown out . the cluster effectively " radiates " stars over time . also , the above-mentioned expulsion of gas by supernova gradually depletes the cluster of mass and the stars move apart .
matter is held together by the electrical attraction between the electrons and the nuclei . within the bulk of a solid or liquid , an electron feels these attractions from all directions equally , and therefore the force it experiences equals zero on the average . but if an electron finds itself at the surface , this isotropy is broken . the electron feels attractive forces toward the interior , which are not canceled out by any forces from the outside . normally this causes any electron that impinges on the surface to be reflected back in like a pool ball hitting a cushion . to extract the electron out beyond the surface , you have to do a certain amount of work , called the work function $w$ . the work function for a metal is typically about 5 ev . why does a cathode have to be heated to emit electrons ? it is not actually true that it has to be heated -- cold-cathode devices do exist , and thermionic emission does occur at all temperatures . however , at room temperature , $kt\approx 0.03\ ev$ , which is much smaller than $w$ . that means that only a very tiny fraction of the electrons have more energy than $w$ . the probability of having an energy $w$ at temperature $kt$ goes like $e^{-w/kt}$ , and richardson found in 1901 that the current from a cathode , in the absence of an externally applied electric field , was proportional to $t^2e^{-w/kt}$ .
how do i stay alive to be killed by neutrinos ? you would not . the point is being made that even the beam of neutrinos with a supernova at one astronomical unit distance would be intense enough that enough of them would interact with the matter of your body to be lethal . so even the neutrinos would get you if all the other stuff - notably $\gamma$s did not . however , you had likely be plasma long before even a handful of neutrinos interacted with you in such a case . the article is meant to give a physicist , who knows how weakly neutrinos interact , some feeling for the unearthly intensity of all kinds of supernova radiation . in line with the article 's idea , a feather in low earth orbit orthogonal or head-on with an astronaut 's own orbit would be lethal , too . as for hiding behind a neutron star , from this source i glean a cross section of $10^{-45}{\rm m^2}$ for the neutrino-proton scattering cross-section : the case for neutrino is very representative in this case : neutrino-proton cross section for typical solar neutrinos of 1 mev is around $10^{-41} {\rm cm^2}$ , despite of a very larger proton “size” . let 's think of a neutron star 10km in diameter , and you sit at the opposite side of it is equator to the supernova . given a nuclear density of $6\times10^{17}{\rm kg\ , m^{-3}}$ , a cubic metre of neutron star holds about $6\times10^{17}\times 6\times 10^{23}\times 10^3 = 3.6\times 10^{44}$ neutrons . this means that the one square metre cross section cubic metre presents an effective collision cross-section , given the figure above , of about $0.36{\rm m^2}$ , or a probability of interaction with one neutrino of about 0.36 ( somewhat less , as i am simply adding probabilities linearly ) . so the eleven kilometer wide neutron star would begin by shielding you from neutrinos very well ( it would absorb almost all of them ) . however , absorption means energy dumping , and , given neutrinos account for only a small amount of the energy shed by a supernova , i am not sure how long even a neutron star at one au distance would last . i suspect that it too would be vaporized , and , if it were not it would swiftly heat to a level where its $\gamma$ radiation would be lethal , but i will have to leave that question to someone else .
the main point is that newtonian gravity fields are conservative . what that means is that it is impossible to have a configuration like the one you drew without there being gravitational fields pointing to the left and to the right in the regions where you want to do the ' horizontal ' transfer . for example , you might try to achieve this on earth by taking the usual uniform gravitational field and locating a very heavy mass just under the foot of the conveyor belt on the left . this will mean , though , that as you move your mass from the foot of that conveyor belt you will be fighting against the attraction of that very same mass , as shown with the red arrows : the net result is that doing both of those horizontal transfers takes work , and in fact it must take exactly the same amount of work as what you have gained from lifting the object in the weaker field . there are , of course , many possible ways to achieve the fields you want , apart from the one in my image , but because all gravitational fields are the sum of attractive forces to a bunch of point masses , and the field of each point mass is conservative , you will always , necessarily , have cross-pointing fields like the one i pictured that will do away with any perpetual motion engine .
you begin it the same way as for mass . $r$ is radost per unit mass , so you can say that : $$ dr = \rho r dv $$ where $r$ is the total radost . you then integrate to get : $$ r = \int_v \rho r dv $$ and apply the divergence theorem like you did for mass . note -- there are several steps between determining $r$ and applying the divergence theorem . but you stated you were unsure where to start and had an example from that point onwards so i left them out .
$$\delta y = v_0 \sin ( \theta ) t - \frac12 gt^2$$ this is a projectile , so it will hit its max at $t_{max}=\frac12t$ , where $t$ is the total time in which the projectile flies . the total time is when , as you know , $\delta y=0$ . hence we have got : $$0=v_0 \sin ( \theta ) t - \frac12 gt^2$$ $$\require{cancel}\frac12 gt^\cancel{2}=v_0 \sin ( \theta ) \cancel{t}$$ $$t=\frac{v_0 \sin ( \theta ) }{\frac12 g}=\frac{2v_0 \sin ( \theta ) }{g}$$ that is the total time of the projectile motion . take half of that to get the time of the maximum height : $$\frac12 t = t_{max} = \frac{v_0 \sin ( \theta ) }{g}$$ now plug it in back to $\delta y$ and you will get the maximum $y$ displacement : $$\delta y_{max} = v_0 \sin ( \theta ) t_{max} - \frac12 gt_{max}^2$$ $$\require{cancel}\delta y_{max}= v_0 \sin ( \theta ) \left ( \frac{v_0 \sin ( \theta ) }{g}\right ) - \frac12 g\left ( \frac{v_0 \sin ( \theta ) }{g}\right ) ^2=\frac{v_0^2 \sin^2 ( \theta ) }{g} - \frac12 \cancel{g}\left ( \frac{v_0^2 \sin^2 ( \theta ) }{g^\cancel{2}}\right ) =\frac{v_0^2 \sin^2 ( \theta ) }{2g}$$ just for bonus , here 's a mathematical proof as to why it is at maximum height halfway through : $$y ( t ) =v_0 \sin ( \theta ) t - \frac12 gt^2$$ $$\frac{dy}{dt}=y' ( t ) =v_0 \sin ( \theta ) - gt$$ $$0=v_0 \sin ( \theta ) - gt\rightarrow gt=v_0 \sin ( \theta ) \rightarrow t=\frac{v_0 \sin ( \theta ) }{g}$$ which is , again , halfway . of course it is a maximum because : $$\frac{d^2y}{dt^2}=y'' ( t ) =-g$$ cheers ! -shahar
if the ladder is slipping on the floor as well as the wall , then the point of rotation is where the two normal forces intersect . this comes from the fact that reaction forces must pass through the instant center of motion , or they would do work . in the diagram below forces are red and velocities blue . if the ladder rotated by any other point other than s then there would be a velocity component going through the wall , or the floor . s is the only point that keeps points a and b sliding . this leads to the acceleration vector of the center of mass c to be $$ \begin{aligned} \vec{a}_c and = \begin{pmatrix} \frac{\ell}{2} \omega^2 \sin\theta - \frac{\ell}{2} \dot{\omega} \cos\theta \\ -\frac{\ell}{2} \omega^2 \cos\theta -\frac{\ell}{2} \dot\theta \sin \theta \\ 0 \end{pmatrix} and \vec{\alpha} and = \begin{pmatrix} 0 \\ 0 \\ -\dot\omega \end{pmatrix} \end{aligned}$$ if only gravity is acting , then $$\dot\omega = \frac{m\ , g\ , \frac{\ell}{2}\sin\theta}{i_c+m \left ( \frac{\ell}{2}\right ) ^2} $$
the work is of course equal to the potential energy of the dipole $\vec m$ in the magnetic field , $$ u = -\vec m \cdot \vec b . $$ now , your wording indicates that the magnetic field $\vec b$ is macroscopic so it may be treated as a classical parameter . however , $\vec m$ from a spinning particle is quantum mechanical . for example , for an electron , $$ \vec m = -g_s \mu_b \frac{\vec s}{\hbar} $$ where the spin $g$-factor is $g_s\sim 2.0023$ , $\mu_b=e\hbar/ ( 2m_e ) $ is the bohr magneton , and $\vec s$ is the operator ( or triplet of operators ) of the electron 's angular momentum – that act as $\hbar/2$ times the pauli matrices on the spin-up and spin-down states . so up to a multiplicative constant $\gamma$ , $$ u = \gamma \vec \sigma\cdot \vec b . $$ the work done on the electron is a $q$-number , an operator , which acts on the spin-up and spin-down states . for the up and down states relatively to the direction of the external magnetic field $\vec b$ , the work done has a sharply defined value , an eigenvalue of $u$ . for general linear superpositions , the work done on the electron has a certain probability amplitude to be positive and a certain probability amplitude to be negative . the squared absolute values of these probability amplitudes determine the probabilities that the work will be seen to be one ( positive ) value or the other ( negative ) value , just like everywhere in quantum mechanics . it is however very interesting to consider superpositions of states in this context , especially for $j=1/2$ . for spin-1/2 particles , each superposition of up and down states is equivalent to " up " with respect to a certain axis in space . if we deal with general superpositions , this axis on which the spin is " up " will generally not be aligned with the direction of the magnetic field . if that is the case , the presence of the magnetic field will have the effect of causing " precession " of the axis defining the spin up . the axis at which the particle is polarized " up " will be " turning around " the direction of $\vec b$ like a ninny . this precession results from the time-dependent change of the phases of the wave function – which is opposite for the up and down states , so the relative phase is changing as well – and the fact that the relative phases of the wave functions always matter ( for something ) in quantum mechanics . the classic experiment measuring this spin-dependent magnetic work and exhibiting lots of the quantum properties is called the stern-gerlach experiment . with some probabilities , the particle behaves in one way or another .
the canonical metric on $cp^n$ is the fubini-study metric . the distance between two states $\left| x \right\rangle$ and $\left| y \right\rangle$ is $$\gamma ( x , y ) = \arccos \sqrt{\frac{\left| \left\langle x \middle | y \right\rangle \right|^2}{\left\langle x \middle | x \right\rangle \left\langle y \middle | y \right\rangle}} . $$ the infinitesmal metric is thus : $$ds = \frac{\langle dx | dx \rangle}{\langle x | x \rangle} - \frac{\left | \langle dx | x \rangle \right|^2}{\left | \langle x | x \rangle \right|^2} . $$ notice that for $cp^1$ this reduces to the natural metric on the bloch sphere .
if i am only allowed to use one single word to give an oversimplified intuitive reason for the discreteness in quantum mechanics , i would choose the word ' compactness ' . examples : the finite number of states in a compact region of phase space . see e.g. this phys . se post . the discrete spectrum for lie algebra generators of a compact lie group , e.g. angular momentum operators . see also this phys . se post . on the other hand , the position space $\mathbb{r}^3$ in elementary non-relativistic quantum mechanics is not compact , in agreement that we in principle can find the point particle in any continuous position $\vec{r}\in\mathbb{r}^3$ . see also this phys . se post .
read this link to get a framework of where the sm stands as far as interactions go . the sm is a mathematical shorthand of our data for the microcosm of quarks and leptons . look at table 1 and you will see that at the level of quarks and leptons the gravitational interaction is so weak that it is completely irrelevant and certainly its effect on the values used in the standard model cannot be measured with our present experimental accuracies .
yes , it does . we do not see it because our brain automagically ' correct it ' because it always see the same aberration from the childhood . our eye focuses on ' green ' wavelength as it is its peak sensitivity , so red and especially violet lines are usually slightly out-of-focus .
using $\sum f=m a$ and $\sum m = i \ddot{\theta}$ i arrive at $$ f = \left ( \frac{m+m}{m} \frac{i+m l^2}{l \cos\theta} - m l \cos \theta \right ) \ddot{\theta} + \left ( \frac{m+m}{m} \frac{\beta \dot\theta}{l \cos\theta} + m l \sin\theta \dot{\theta}^2 \right ) $$ now you can play with the values to get what you need .
a process where the energy is kept constant is called isoenergetic ( or , if you prefer , iso-energetic ) . it also seems from the literature that a flow where the energy is constant when following a fluid particle is usually called an isoenergetic flow . similarly , when the enthalpy is kept constant , the process ( or the flow ) is said to be isenthalpic ( or isoenthalpic ) . and so on . notice that if there is some subtlety and you keep a constant internal energy $u=\text{cte}$ but not a constant energy $e=u+e_{\text{m}}$ , by modifying the mechanical energy $e_{\text{m}}$ , you should refrain from using standard names like isoenergetic and explain precisely what happens .
observers in relative motion would not agree on the time order of all events . they may disagree on the order of events connected by a space-like interval , i.e. for events such that \begin{equation} -c^2\cdot \delta t^2 + \delta x^2 + \delta y^2 + \delta z^2 > 0 \end{equation} note that since the time interval between such events is too short for any communication between the events to occur there may be no causal relationship between them . all observers do agree however on the time order of events connected by a time-like interval , i.e. for events such that \begin{equation} -c^2\cdot \delta t^2 + \delta x^2 + \delta y^2 + \delta z^2 &lt ; 0 \end{equation} these events may have causal connection . to see how this happens consider three events which in certain inertial frame of reference o have the following (t, x, y, z) coordinates : \begin{equation} a = [ 0 , 0 , 0 , 0 ] \end{equation} \begin{equation} b = [ c , 0 , 0 , 0 ] \end{equation} \begin{equation} c = [ 0 , 1 , 0 , 0 ] \end{equation} events a and b are connected by a time-like interval ( in fact they correspond to the same place in o at different points in time ) while a and c are connected by a space-like interval ( in o they correspond to different places at the same moment in time ) . now , to obtain spacetime coordinates of a , b and c in a frame of reference o ' moving relative to o at velocity v> 0 along the x axis , we can use the following lorentz transformation \begin{equation} \lambda_x ( v ) = \left [ \begin{array}{cccc} \gamma and -\beta \gamma and 0 and 0 \newline -\beta \gamma and \gamma and 0 and 0 \newline 0 and 0 and 1 and 0 \newline 0 and 0 and 0 and 1 \end{array} \right ] \end{equation} where \begin{equation} \beta = \frac{v}{c}> 0 \end{equation} \begin{equation} \gamma = \frac{1}{\sqrt{1-\frac{v^2}{c^2}}}> 0 \end{equation} in the frame of reference o ' moving with velocity v along the x axis the coordinates of the three events are \begin{equation} a = [ 0 , 0 , 0 , 0 ] \end{equation} \begin{equation} b = [ \gamma c , -\beta \gamma c , 0 , 0 ] \end{equation} \begin{equation} c = [ -\beta \gamma , \gamma , 0 , 0 ] \end{equation} in the frame of reference o'' moving with velocity -v along the x axis ( i.e. . the same speed as o ' , but the opposite direction ) the coordinates of the three events are \begin{equation} a = [ 0 , 0 , 0 , 0 ] \end{equation} \begin{equation} b = [ \gamma c , \beta \gamma c , 0 , 0 ] \end{equation} \begin{equation} c = [ \beta \gamma , \gamma , 0 , 0 ] \end{equation} since only β changes sign . we see that from the perspective of an observer stationary in the o reference frame events a and c are simultaneous , from the perspective of an observer stationary in the o ' reference frame event c occurs before event a while from the perspective of an observer stationary in the o'' reference frame event a occurs before event c . in all frames a occurs before b . see also relativity of simultaneity .
waves on strings combine linearly . this means that you can split up a string 's motion into two ( or more ) superimposed waves . the two superimposed waves behave independently , as if the other one was not there . so if you have a standing wave set up on a string , and then you also introduce a travelling pulse , you get something like the following . ( the arrows represent the direction of movement , and the node is marked with a blue dot . ) now to answer your question . i wish i had a way to make the picture animated , but i think you can see it from still pictures . i am going to draw what happens after a short time , when the pulse reaches the node . the standing wave has also moved , and is now swinging back in the other direction . as you can see , the standing wave component still passes through zero at the node , as it always must , but the combined wave ( pulse + standing wave ) does not . because the pulse and the standing wave do not interact , the pulse just passes straight through the node as if it was not there , and the standing wave just keeps waving as if the pulse was not there . note that not interacting is not the same as not interfering . interference happens when two waves get added together and sum to zero , but neither of the two waves is affected by being added in this way , so even when waves interfere , they do not interact .
yep . yep , perfectly fine . this will work . the general method is that the expansion coefficients are given by the inner product $\int \psi^\star_{lm} \psi_0$ where $\psi_{lm}$ are the orthonormal basis functions of your expansion and the integral is over the full space with the natural measure . in your case the basis functions are the spherical harmonics and the measure is $\mathrm{d}\omega \equiv \sin\theta \mathrm{d}\theta \mathrm{d}\phi$ . you can plug in a general spherical harmonic and by doing the integrals get the answer , though in your simple case your method is easier and equivalent . by the way , mathematica knows about spherical harmonics already ( sphericalharmonicy , but make sure it uses the same conventions as you ) . the general method is like finding the fourier series by doing a bunch of integrals . the method you are using is like reading off the fourier series coefficients by rearranging the function itself to look like a fourier series . when you can do it you get the same answer as by doing the integrals , but it is not always easy to do the rearrangement . re edits to the question . q4 . yep , again . : ) q5 . it is as simple as noting that $l^2 y_{lm} = l ( l+1 ) y_{lm}$ and $l_z y_{lm}= m y_{lm}$ ( i am dropping factors $\hbar$ which you can put back yourself if need be ) . just plug your $\psi_0$ in $h\psi_0$ and use the fact that all the operators are linear . it is easy to do term by term and you will see that you get just the answer you have expected . that is because all of these operators $h , l^2 , l_z$ are simultaneously diagonalizable ( because they commute ) and we are working in the basis of their common eigenfunctions .
your approach is all right but the solution given by the textbook is wrong : ) , at least if no approximation is to be made . let 's go the other way around : start from $$p ( v , t ) = \frac{\gamma c_vt}{v} + \frac{\varepsilon}{2v_0}\left [ \left ( \frac{v_0}{v}\right ) ^5 - \left ( \frac{v_0}{v}\right ) ^3\right ] $$ and then derive the values of $\kappa_t$ and $\beta$ ( by the way , should not it rather be $\chi_t$ ( see here ) and $\alpha$ ( here ) ? ) . as you mentioned , to do this , we need to compute partial derivatives of $p$: $$ \left ( \frac{\partial p}{\partial t}\right ) _v = \frac{\gamma c_v}{v} $$ $$ \left ( \frac{\partial p}{\partial v}\right ) _t = -\left ( \frac{\gamma c_vt}{v^2} + \frac{\varepsilon}{2{v_0}^2}\left [ 5\left ( \frac{v_0}{v}\right ) ^6 - 3\left ( \frac{v_0}{v}\right ) ^4\right ] \right ) $$ which give $$ \frac{1}{\kappa_t} = -v \left ( \frac{\partial p}{\partial v}\right ) _t = \frac{\gamma c_vt}{v} + \frac{\varepsilon}{2{v_0}}\left [ 5\left ( \frac{v_0}{v}\right ) ^5 - 3\left ( \frac{v_0}{v}\right ) ^3\right ] $$ and $$ \beta = \kappa_t \left ( \frac{\partial p}{\partial t}\right ) _v = \frac1t \left ( 1 + \frac{\varepsilon }{2\gamma c_v t}\left [ 5\left ( \frac{v_0}{v}\right ) ^4 - 3\left ( \frac{v_0}{v}\right ) ^2\right ] \right ) ^{-1} $$ clearly , that is not what the textbook gives in the first place , but it is close enough to understand what they did : a taylor expansion at order 3 in $v_0/v$ in booth cases , which gives back the expressions $$ \frac{1}{\kappa_t} \approx \frac{\gamma c_vt}{v} + \frac{\varepsilon}{2{v_0}}\left [ - 3\left ( \frac{v_0}{v}\right ) ^3\right ] $$ and $$ \beta \approx \frac1t \left ( 1 + \frac{\varepsilon }{2\gamma c_v t}\left [ - 3\left ( \frac{v_0}{v}\right ) ^2\right ] \right ) ^{-1} \approx \frac1t \left ( 1 + \frac{\varepsilon }{2\gamma c_v t}\left [ 3\left ( \frac{v_0}{v}\right ) ^2\right ] \right ) $$ it really should have been clearer in the text that you could suppose $v\gg v_0$ ( and not only $v&gt ; v_0$ ) and i do not see how you could derive the term in $\left ( \frac{v_0}{v}\right ) ^5$ from this as it is completely neglected to find back $\kappa_t$ and $\beta$
your mistake is that in the expression $$ q=cv $$ the symbol $v$ here represents the magnitude of the potential difference between the spheres . thus , since $b&gt ; a$ here , you need to switch the order of $b$ and $a$ in the first expression you wrote down for $v$ if you want to plug it into the expression defining capacitance ( in other words , you need to take its absolute value ) .
although i do not know anything about this , using some rough estimates i think i can get the right order of magnitude : volume of graphite in a pencil : $10 cm$ cylinder of $1 mm$ thick = $0.314 mm^3$ ( error : ~factor 2 ) maximum surface a pencil can write : $50 km$ $\times$ $1$ mm = $10 m^2$ ( error : ~factor 5 ) thickness of the graphite layer : volume / surf . area = $31.4$ nanometers size of a ( carbon ) atom : $0.22 nm$ ( error : 10% ) thickness of the layer : $31.4 nm /0.22 nm$ = $142$ carbon atoms so i would say ' about a 100 atoms ' ( or at least more than 10 and less than 1000 ) . i might have been a bit conservative with my error estimates but this seems reasonable .
snell 's law still applies to the curved surface , but you have to measure the angles of incidence and refraction relative to the surface where the light hits . the image is my attempt to show parallel rays of light falling on a curved surface . even though the rays are parallel , the angle of incidence is different for the two rays because it has to be measured relative to the normal at the point the light strikes the surface . hence the angle $i$ is not the same as the angle $i'$ . response to comment : it has become clear from the comments that the problem is that the value of $n$ depends on whether the light is passing from the air to glass or from glass to air . to be precise the two refractive indices are reciprocals of each other i.e. $$ n_{air-glass} = \frac{1}{n_{glass-air}} $$ the refraction of the light ray happens because the speed of light , and therefore the wavelength , changes when the light enters and leaves the glass . the refractive index when a light ray passes from a medium 1 to a medium 2 is : $$ n_{1-2} = \frac{v_1}{v_2} $$ where $v_1$ is the speed of light in medium 1 and $v_2$ is the speed of light in medium 2 . so in our example the refractive index when passing from medium 2 to medium 1 is : $$ n_{2-1} = \frac{v_2}{v_1} $$ i.e. $$ n_{1-2} = \frac{1}{n_{2-1}} $$
( i will assume in my answer that people have read the discussion on the old question , linked to by the op . ) no , it is not like the aether . it is still true that locally , there is no preferred reference frame . you do not even really need to think about spacetime to see what is going on . consider a two-dimensional plane , parametrised by $ ( x , y ) $ , and roll it into a cylinder by identifying $ ( x , y ) \sim ( x + nl , y ) ~\forall~ n$ , where $l$ is some constant . locally , this space is still perfectly isotropic , but globally , the $x$ direction has been picked out by the identification . to see what this means , let 's imagine drawing two straight line segments , each beginning at $ ( x , y ) = ( 0,0 ) $ and ending at $ ( 0 , l ) $ . the first will just be $ ( 0 , t ) ~ , ~ 0\leq t\leq l$ , and the other will be $ ( t , t ) ~ , ~0\leq t\leq l$ ( which ends at a point equivalent to $ ( 0 , l ) $ under the identification , and therefore the same point on the cylinder ) . obviously the length of the first line is just $l$ , but the length of the second line is $\sqrt{2}l$ , by pythagoras . although any small patch of the cylinder is perfectly isotropic , we see here that the rotational symmetry is broken globally by the identification . in spacetime , a similar thing happens , replacing rotational symmetry by boost symmetry . short answer : generally speaking , there is never a twin paradox : in any spacetime , just write down the metric in any coordinate system you like , and calculate the proper times for the two trajectories of interest . this tells you unambiguously which twin is older and which younger .
you can add a gauss-bonnet term $r^2 - 4r^{\mu\nu}r_{\mu\nu} + r^{\mu\nu\rho\sigma}r_{\mu\nu\rho\sigma}$ . it is purely topological .
a hint $$ dxf ( x ) = f ( x ) +xdf ( x ) $$ $$ xdf ( x ) $$ take the diference and you get $ f ( x ) $ or $ 1 . f ( x ) $
i assume you used the formulae $f_o = fs\sqrt{\frac{1+v/c}{1-v/c}}$ for the clocks ahead of you and $f_o = fs\sqrt{\frac{1-v/c}{1+v/c}}$ for the clocks behind you . those formulae do imply a singularity for the clock that is closest to you . which equation to use ? the answer is neither . those expressions assume the travel is along the line of sight to the source . there is a singularity because collisions are singularities . your spaceship is plowing through the line of clocks . what you will see in front of you are a series of clocks ticking faster than yours . behind you , you will see a cloud of pulverized clocks . your spaceship had better have some very good forward shields . your spacecraft presumably is not doing that . instead , you are flying parallel to the line of clocks , with some constant , non-zero distance between the spacecraft and the line of clocks . you need to use the more generic expession $$f_o = fs \frac{\sqrt{1-\left ( \frac v c\right ) ^2}} {1-\frac v c \cos \theta_o}$$ where $\theta_o$ is the angle between the clock in question and your line of travel , as observed by you . the sign convention here is that $\theta_o$ is positive for clocks in front of you , negative for clocks behind you . the above expression reduces to the simpler expressions at the start of my answer for clocks very far in front of you and very far behind you . in between , you will get a nice continuous change from faster in front , slower behind . the clock right next to you ? it is a bit redshifted , and hence slower . here $\cos \theta_0=0$ , so in this case $fo=fs\sqrt{1- ( v/c ) ^2}$ . this is called the transverse doppler redshift . this means that there is a clock just slightly ahead of you that is ticking at your own clock 's rate .
the moment of a vectorfield $\vec{v}$ at a position $\vec{r}$ is equal to $$\vec{r}\times\vec{v} . $$ so torque is simply a special case where the vectorfield we look at is the force field , $\vec{v} = \vec{f}$ . another way of saying this is that torque is the moment of force .
the difference between fermi energy and fermi level is described in a previously asked question . to summarize , you are correct that the fermi level is the energy at which ( in a 0k system ) all of the lower energy states are occupied while all of the higher energy states are unoccupied . for a system not at 0k , the fermi energy is the energy state that has a 50% likelihood of occupation . with nonzero temperature we may only talk about the probability distribution of electrons in energy levels . the likelyhood of an electron to be in an energy state $\mathcal{e}$ is $f ( \mathcal{e} ) $ , where $f$ is the fermi function . the work function is defined as the energy necessary to remove an electron from the surface of a metal : $w = -e\phi - e_f$ . here , $e\phi$ is the energy of the electron just outside of the surface of the metal ( $\phi$ is the electric potential just outside the surface ) and $e_f$ is the fermi energy . recall that there are electrons with energies higher than $e_f$ and there are electrons with lower energy than $e_f$-- for these electrons it is easier ( harder ) to remove them from the metal . by only taking $e_f$ into account the work function is effectively averaging over all possible energy states weighted by the probability of the state being occupied . getting to your first question , the equation $w = |fe + fl|$ does not make sense to me , because in many systems the fermi level and energies are similar if not identical , i do not see what adding them would accomplish and why this would yield the work function . also , the work function is very heavily dependent on the surface properties of the metal , which are taken into account via the potential $\phi$ just outside the surface . i do not see how surface properties are taken into account in the equation your ta wrote . now , because for non-0k systems you can only talk about the probability of an energy level being occupied , your band diagram is a bit oversimplified for our discussion . see instead the band diagram in the " fermi function " section here . this band diagram is that of a semiconductor , which you can tell because the band gap is small enough for the fermi function ( read : probability of energy state occupation ) to be nonzero in the conduction band . however , the point at which the probability of occupation is 50% ( read : the fermi energy ) is the same as the fermi level for all temperatures . notice how the fermi function stretches out with increasing temperature , causing an increase in likelihood of electrons occupying the semiconducting band -- this is the picture of a semiconductor that i carry in my head ( and also clearly explains why heat increases conductivity ) . another minor correction to your picture is that the energy of an electron just outside of the surface of the metal is not necessarily zero ( it is $e\phi$ ) , the work function is the energy difference between being just outside of the metal and being at the fermi level , and is not the difference between being infinitely far away ( 0 energy ) and being at the fermi level .
the energy needed to remove an electron from a solid is called the work function . for most metals you would need uv photons ( 300 nm for aluminium ) that rarely reach the earth 's surface . visible light can eject electrons from alkali metals , but the quantum yield ( the probability of electron emission per incident photon ) for pure metals is low ( probably less than 1% ) . materials like cste that are used in photocathodes have efficiency up to 40% ( at certain wavelength ) but they are expensive and difficult to handle in open air . silicon solar cells also utilize photoelectric effect and compared to metals they are efficient and inexpensive .
there are two different questions at work here , that you have kind of mashed together . the first question is " what is the speed at which a change in the electric field propagates ? " the answer to that is the speed of light . in qed terms , the electromagnetic interaction that we see as the electric field is mediated by photons , so any change in an established field ( say , due to shifting the position of the charge creating the field ) will not be felt by a distant object until enough time has passed for a photon from the source to make it to the observation point . the second question is " what is the speed of propagation of electric current ? " this speed is slower than the speed of light , but still on about that order of magnitude-- the exact value depends a little on the arrangement of wires and so on , but you will not be far off if you assume that electrical signals propagate down a cable at the speed of light . this relates to electric field in that the charge moving through a circuit to light a light bulb has to be driven by some electric field , so you can reasonably ask how that field is established , and how much time it takes . qualitatively , the necessary field is established by excess charge on the surface of the wires , with the surface charge being generally positive near the positive terminal of a battery and generally negative near the negative terminal , and dropping off smoothly from one to the other so that the electric field is more or less piecewise constant ( that is , the field is the same everywhere inside a wire , and the field is the same everywhere inside a resistor , but the two field values are not the same ) . when the circuit is first connected , there is a rapid redistribution of the charge on the surface of the wires which establishes the surface charge gradients that drive the steady-state current that will eventually do whatever it is you want it to do . the time required to establish the gradients and settle in to the steady-state condition is very fast , most likely on the order of nanoseconds for a normal circuit . there is a good discussion of the business of how , exactly , charges get moved around to drive a current in the textbook that we use for our introductory classes , matter and interactions , by chabay and sherwood . it does not go into enough detail to let you calculate the relevant times directly , but it lays out the basic science pretty well . ( it is a textbook for a first-year introductory physics class , so it sweeps a lot of condensed matter physics under the metaphorical rug-- there is no discussion of band structure or surface modes , or any of that . it is fairly solid conceptually , though , at least according to colleagues who know more about those fields than i do . )
$p=iv$ where , $p$ is power , $v$ is voltage , and $i$ is current in amps . $e=pt$ where $e$ is energy and $t$ is time .
i think the issue is we need to separate the ' expected'/obvious quantum effects , from the unexpected ones . for instance , some of your questions refer to quantum mechanics in molecular structure . in the most trivial sense , we could not even have molecular structure or even stable atoms if $\hbar \rightarrow 0$ such that there are no quantum effects . so in a trivial sense , every protein structure is due to quantum mechanics . following up on this . . . i mean , if i do not neglect electron-electron interaction , then pretty much all electrons in a condensed matter system are entangled , are they not ? yes . in the simplest picture of matter , chemists refer to electrons occupying molecular orbitals . this is the hartee-fock approximation in quantum chemistry , and since the wavefunction is written as a slater determinant of the occupied molecular orbitals , in this approximation there is no electron-electron position correlation . obviously in the real world this is not the case . the wavefunction will be correlated to have two electrons further apart on average than in the simplified molecular orbital picture . this means the state is not a product state and there is indeed entanglement of the electron positions . so you are correct on this . ( and as an aside , more advanced multi-electron computational methods beyond hartree-fock can of course be used to account for most of this correlation energy in theoretical calculations . the molecular orbit picture works very well though , which is why this is how it is introduced to students and how chemists colloquially discuss and visualize the quantum mechanics of a molecule . ) however , again this is in some sense the ' trivial ' effects of quantum mechanics , as it is just " chemistry " . but as we get to larger structures , due to interaction with the environment and decoherence , we can increasingly well describe everything with phenomenological parameters and a classical theory . true the phenomenological parameters are due to the quantum mechanics , but this is not the exciting part . the exciting part is when we can not describe the protein interactions with phenomenological models due to the quantum coherence playing an important part at the macroscopic level . yes , it is somewhat arbitrary where we draw this line . but few really thought biology would end up using something so fragile as the coherence of entangled states to actually improve biological function . the better studied example i have seen regarding this is energy transfer in a particular photosynthesis step . whether something is amazing or not is not really a scientific question , but hopefully i clarified enough regarding your question of entanglement of electrons in molecules to see how the use of coherence to improve biological function is at the least unexpected , and hopefully exciting to you as well .
the $\theta$-term is also known as the axion term and it is simply the $f\wedge f$ term known to particle physicists . in a more condensed-matter-friendly language , $$\delta {\mathcal l} = \theta\left ( \frac{e^2}{2\pi h} \right ) \vec b \cdot \vec e $$ i do not know the optimum starting point but you may begin with http://arxiv.org/abs/1001.3179 and its followups and references . more generally , the $\theta$-term means the integral of the anomaly polynomial . note that the anomaly polynomial is a nice gauge-invariant expression - but in higher dimensions . the actual anomaly in the original spacetime is related to it by several operations .
like most questions related to aerodynamics , stanford university offers the best information needed to answer this question . the following instructional site ( presumably for students ) outlines the fundamentals of high-lift devices ( aa241 - high lift aerodynamics ) , and includes this highly informative plot of cl vs aoa for the dc-9-30: the image clearly shows that the slats barely modify the lift at a given angle of attack , but do allow the wing the operate at much higher angles before stalling . this is exactly what i was looking for ! ! !
if the eigienvalues form a continuous spectrum , like the eigenvalues of $x$ , then states must be normalized to a dirac delta , $$ \left\langle x \right| x ' \rangle = \delta ( x-x' ) $$ the trace of an operator is the sum of the diagonal elements , or if the basis is continuous , it becomes an integral \begin{eqnarray*} \mathrm{tr}\left ( \left|\phi\right\rangle\left\langle \psi \right|\right ) and = and \int_{-\infty}^{\infty}\mathrm{d}q\ , \left\langle q\right|\phi\rangle\left\langle \psi \right|q\rangle\\ and = and \int_{-\infty}^{\infty}\mathrm{d}q\ , \phi ( x ) \psi^* ( x' ) \left\langle q\left|x\left\rangle\right\langle x'\right|q\right\rangle\\ and = and \int_{-\infty}^{\infty}\mathrm{d}q\ , \phi ( x ) \psi^* ( x' ) \delta ( q-x ) \delta ( q-x' ) \end{eqnarray*} this last line is only nozero if $x=x'=q$ , so you can choose whichevr label you like for the integration variable . $$\mathrm{tr}\left ( \left|\phi\right\rangle\left\langle \psi \right|\right ) = \int_{-\infty}^{\infty}\mathrm{d}x\ , \phi ( x ) \psi^* ( x ) $$ as for as the difference between the kets and no kets , if the kets are there , it is the operator , ready to act on some vector . if the kets are not there , you have just the $x_1 , x_2$ matrix element . it is technically not the operator , but since the elements are just the values of some function of $x_1 , x_2$ , writing the element this way tells you all of the information you need . for example , you could write the 2x2 identity matrix as $$\mathbb{i} = \pmatrix{1 and 0\\0 and 0} + \pmatrix{0 and 0\\0 and 1}$$ you could just as easily say $$\mathbb{i}_{i , j} = \begin{cases}1 and \mathrm{if}\ ; i=j\\ 0 and \mathrm{otherwise}\end{cases}$$ and convey the same information .
let 's break the question down to its bare bones . let us define danger first . i think danger falls under following categories : any changes to the body 's ( internal or external ) structure . mental trauma --i would attribute it to the double derivative of the velocity ( i.e. . , jolt ) . but this is highly subjective . some people would attribute it to the impact while others would attribute it to the total energy transferred to the body . this is the part i will leave out from this discussion as it is highly subjective . so , let 's focus on 1 . what can cause changes to a [ human ] body 's structure ? one may think that structure changes with the application of force . intuitively , larger the force applied , more changes it is likely to produce . longer the force is applied , more changes it is likely to produce . but in concrete physical terms , structural changes happen when applied energy is sufficient to break the bonds between atoms/molecules making up the structure . thus , the bond strength and the amount of energy both matter . intuitively , a ball of putty or sponge is more likely to deform itself than a hard leather or cork ball . thus , when you are hit by a sponge ball you take less damage than when you are hit by a hard leather ball of the same mass thrown with the same velocity . how much energy will be transferred ? in a collision , energy is transferred between the objects that collide . some of that energy changes the velocity of the objects while rest of it is absorbed by the structure or released as heat . when no energy is absorbed by the objects colliding they are said to have collided elastically . but , as is more likely in a real world scenario , including the car collision case you described , when the final kinetic energies of the two objects do not sum up to the sum of their initial kinetic energies then it follows that some energy will be " lost " in the collision . that energy is not actually lost and is absorbed by the objects colliding . they are said to have collided inelastically in this case . it is the energy that was absorbed by the pedestrian that would contribute to the change in structure of the pedestrian 's body . now for your actual question --what difference does the mass of the car make ? so let us assume that all other factors ( mass of the pedestrian , angle of the collision , initial velocities , resilience of the car and the pedestrian , co-efficient of restitution etc . ) remain the same . let us say the mass of the pedestrian is $m$ and the mass of the car is $m$ . let us also say that they collide perfectly inelastically and the initial speed of the car is $v$ ( do not care about the direction , that is why i said speed ) . then the energy lost is : $$\frac{ ( mv ) ^2}{2 ( m + m ) }$$ roughly thus ( considering $m &lt ; &lt ; m$ ) , the energy lost is directly proportional to the mass of the car . in partially inelastic collisions , the formula becomes a little complicated but the essence of the discussion remains the same . so , everything same , more mass will mean more damage . you can use the formula above to calculate the energy for yourself but doubling the weight of the car will increase the energy lost substantially --and can mean life/death . doubling the speed will cause even more damage --hence the speed limits in congested areas . does the acceleration matter ? you question also asks if the acceleration matters . to answer it directly , yes but not in the same way as the energy transferred . acceleration by itself will not cause anything more sinister ( other than disturbing the fluids in the pedestrian 's body ) as long as it is uniform . if you experience space variant acceleration ( meaning different parts of the body are accelerated differently ) then it can cause tearing and that is harmful . the greater the difference , more will be the tearing . time dependent acceleration ( or jolts ) can cause a lot of discomfort as the body is stretched and fluids move here and there --that can cause vomiting or trauma or both . very large accelerations start with a significant jolt ( as the initial acceleration is very close to zero ) . for the example case you described , the cars are at the same initial speed . the " final " velocity of the pedestrian is $$\frac{mv}{m + m} , perfectly-inelastic$$ $$\frac{2mv}{m + m} , perfectly-elastic$$ therefore , once again , more massive the car , higher will be the acceleration experienced by the pedestrian assuming collision impact times remain same . but if you can make the impact softer , you can reduce this acceleration ( by increasing the time of impact ) --see next section . other ways to make cars safer to make cars safer for pedestrians , one should structure the car so that it absorbs most of the energy itself ( softer bumpers/hoods ) . on formula one race tracks you can see several layers of rubber tires that are there to absorb energy when a car collides with them . one can even structure the car in the fashion you described , allowing the pedestrian to slide over the car , thereby reducing the impact of the collision . but you definitely want to avoid an elastic collision where the pedestrian is reflected by the car into something more dangerous nearby ( e . g . , road , traffic sign pole , railings , a food stall with knives/boiling oil etc . ) .
your problem has two degrees of freedom , acceleration of the top block $a_1$ and acceleration of the bottom block $a_2$ . if the friction force between them is $f$ then the equations of motion are $$ m a_1 = f \\ m a_2 = p - f $$ if the blocks are stuck then $a=a_1=a_2$ and $f$ are unknown . when they slip then $f=\mu m g$ is known , but $a_1$ , $a_2$ are different unknowns . in each case there are two unkowns and two equations . the tansition occurs when friction tries to be $f \ge \mu m g$ which you need to find first .
the reason for having two prongs is that they oscillate in antiphase . that is , instead of both moving to the left , then both moving to the right , and so on , they oscillate " in and out " - they move towards each other then move away from each other , then towards , etc . that means that the bit you hold does not vibrate at all , even though the prongs do . you might ask why it is that they do that , instead of oscillating in the same direction as one another . the answer is that at first they oscillate in both ways at the same time , but the side-to-side oscillations are rapidly damped by your hand , so they die out quickly , whereas the in-and-out ones are not damped this way , so they ring on long enough to hear them . an excellent illustration of this can be seen in this video of a fem model of a two-pronged fork , which shows you all the vibrational modes separately . ( hat tip to ghoppe , who posted this video in a comment . ) having a third prong would not help very much with reducing damping . there are ( at least ) three different ways a three-pronged fork could vibrate : one with all three vibrating side-to-side in phase with one another , and two where one of the prongs stays still and the other two vibrate in and out . ( at first i thought there would be three of this latter type of mode , but the third can be formed from a linear combination of the other two : $ ( 1,0 , -1 ) - ( 1 , -1,0 ) = ( 0,1 , -1 ) $ . ) the vibrational mode in which everything moves in the same direction would be damped by your hand , and some combination of the other two would continue to sound for a while . as ilmari karonen pointed out in a comment , there would also be a " transverse " $ ( 1 , -2,1 ) $ mode , where prongs vibrate out of the plane of the fork . this mode would not necessarily have the same frequency as the primary in-and-out mode , so it is probably something we had want to avoid . but ultimately there is little reason why , in a three-pronged fork , the vibrations would ring on longer than just two prongs - there would be the same amount of vibrational energy as in a two-pronged fork , but just shared between two or three modes of vibration instead of one .
if you equate the torque about the center of the cylinder you will find that only two force can produce torque . therefore:- $$f_t*r=f_f*r$$ , and the tension is equal to the friction . then you write the equation in x direction along the slope of the incline :- $$f_t*cos ( \theta ) +f_f-m*g*sin ( \theta ) =o $$ here you replace friction with tension and you will have the answer in required form .
hawking radiation is a very robust prediction . it comes simply from applying quantum field theory in the curved space-time near the event horizon . it is also part of the synthesis called " black hole thermodynamics " , for which string theory provides an explanation in terms of the statistical mechanics of microstates . in the s-matrix of quantum gravity , if black holes did not evaporate , they had show up as asymptotic states , but they do not . ( there are eternal black holes in anti de sitter space , but they still evaporate , they just do not get to evaporate completely ; the particles produced by the evaporation can not escape to infinity because of the peculiarities of ads geometry , and fall in again . ) so denying the existence of hawking radiation would screw up many other things . you could say that hawking radiation is real but that it falls back in , like in ads space , but there is no reason for it to do so . the paper featured at arxivblog is a " what if " paper which ignores all these problems and proceeds to calculate some of the consequences . you could compare it to an engineering study of one of m.c. escher 's impossible structures : if you ignore the contradictions in its design , maybe you can calculate some of its properties , but it only has recreational value to do so . we do not quite know that a nonevaporating quantum black hole is logically impossible , in the way that we know the impossible staircase is impossible , but in the future a genuine proof may be available . but empirical confirmation of black hole evaporation is rather unlikely . if we could produce mini black holes in colliders , then we had see it , but those models are not especially favored ; they are a " what if " of a different sort , one in which there is at least a consistent fundamental picture behind the hypothesis ( particular braneworld models ) , but it is just one of many possibilities about what happens at the next frontier of physics and those models are not significantly favored . ( these models are also the ones which predict a detectable signature in grb data . ) if we could send a probe to the edge of an astrophysical black hole , maybe the radiation could be detected , but that is a job for interstellar civilizations , if they exist . maybe you could find indirect evidence for hawking evaporation of primordial black holes in the cosmic microwave background . but i do not know how likely that is - again , it would be highly model-dependent .
that is a hard experiment . the remnant nucleus generally has a very low ( non-relativistic ) velocity{*} , making it difficult to detect and characterize . should one only consider individual events where all decay products have been detected and their individual momenta obtained ? this is the formal definition of an " exclusive " event , but experimentally we ( that is the nuclear and particle physics " we" ) almost always relax it with respect to the remnant nucleus . how do you make sure that a set of decay product events are associated to the same event ? time correlation ? does this mean that the sample size needs to be small enough so overlapping decay events are percentually few and can be filtered out ? where possible you set your event rate slow compared to the daq latch and report rate . so almost all reported daq events are associated with only one physics event . there is a bit of art and science in this . if i was really going to do it , i would consider measuring the transverse direction only at a radioactive beam accelerator{**} . that will not make it easy , mind you , just less horribly difficult . that simplifies your life in the sense that the remnant is now a ionizing particle with a macroscopic track length , so you can id it , but of course you no longer have good information on exactly where the decay occurs and may not be able to extract a good value for the longitudinal component of the momentum due to the decay {+} . {*} momentum of a few mev , and mass of multiple to hundreds of gev . you might ask the amo people about this , they are more used to working with just barely ionizing energies than particle physicists and may have an answer . perhaps it is a good application for a multi-channel plate . {**} they do exist . {+} lost in the noise of the much larger component due to the beam momentum .
the main problem about a rigorous solution to such a scattering proplem is that computations are extremely demanding . just imagine you have a wavelength $\lambda$ of some $400$nm to $700$nm for visible light ( from here ) : now , to do physically meaningful simulations , you will need a sub-wavelength lattice which makes any computational cell above , say $10\ , \mu m^3$ not accessible since you have in the order of one million grid points . approximative approaches but of course there can be ways out of it if you are willing to make some approximations which will largely depend on the characteristics of the particles you are looking at . it is best to assume that we only have spherical particles since we can apply mie theory in this case . large particles first of all , let us consider particles which are much larger than the wavelength . then , the radius $r$ times the wave vector $k=2\pi/\lambda$ is much bigger than one , $$kr\gg1$$ which basically means that one observes reflection at a plane interface . you can implement these particles using geometrical optics ( mixed with fresnel reflection if you like ) since nothing really wave-like will happen as in this image ( taken from here ) : small particles second , the particles should be much smaller than the wavelength , $$kr\ll1\ , . $$ then , everything what is observed is a sum of dipolar responses of the particles in the so-called rayleigh-scattering . then , the intensity of light scattered by a single small particle from a beam of unpolarized light of wavelength $\lambda$ and intensity $i_0$ is given by : $$i=i_0 ( 1+\cos^2\theta ) \frac{ ( kr ) ^6}{2 ( kr ) ^2}\left ( \frac{n_p^2-1}{n_p^2+2}\right ) $$ where i have chosen the variables to be consistent with the used terminology and $r$ is the distance to the object , $\theta$ is the scattering angle and $n_p$ is the sphere 's refractive index . here is an image of such a situation with some metal particles also having quadrupolar excitation ( from here ) : a mean field approach - effective permittivity if you have a lot of these small objects , you may use the clausius-mossotti relation which gives you an effective permittivity $\epsilon_p=n_p^2$ depending on the concentration of the particle in some volume : $$\epsilon_{eff} = \epsilon_p + \frac{n\alpha}{1-\frac{n\alpha}{3\epsilon_p}}$$ where $\alpha$ is the polarizability of the sphere , for details see e.g. electromagnetic mixing formulas and applications by sihvola . this would be something like a mean-field approach . you can make some very neat effects using this effective approach since it allows you to calculate a continuous refraction around some particle streams under water . however , if the particles size is in the order of the wavelength , $$kr\approx 1$$ then you may have to take higher multipole moments into account which may be a very demanding task . for much more on the subject i would recommend bohren and huffmanns classic absorption and scattering of light by small particles . sincerely
from the article to which you linked : the cloud shines brightly in gamma rays due to a reaction governed by einstein’s famous equation $e=mc^2$ . negatively charged subatomic particles known as electrons collide with their antimatter counterparts , positively charged positrons . so you see , the very reason that we know it is a cloud of antimatter is precisely because it is already annihilating with normal matter that is floating in interstellar space . this will not create a chain reaction because there is not that much more antimatter in our galaxy . ( if there was , it would have annihilated a long time ago . ) and , as others have commented , the products of electron-positron annihilation ( gamma rays ) cannot go on to trigger further annihilations , so the idea of a chain reaction does not apply in this case .
no . particles with spin will feel the torsion not only through their spin precession , since the equations of motion for them ( mathisson-papapetrou equations ) will contain the asymmetric part of the connection . one source for the question is the review hehl , f . w . , von der heyde , p . , kerlick , g . d . , and nester , j . m . ( 1976 ) . general relativity with spin and torsion : foundations and prospects . rev . mod . phys . , 48 ( 3 ) , 393 . ( it has an online version ) . from there we learn : we have already indicated that photon and spinless test particles sense no torsion . a test particle in $u_4$ theory , one which could sense torsion , is a particle with dynamical spin like the electron . its equation of motion can be obtained by integrating the conservation law of energy-momentum ( 3.12 ) . in so doing , we obtain directly the mathisson-papapetrou type equation${}^{20}$ for the motion of a spinning test particle ( hehl , 1971 ; trautman , 1972c ) ${}^{21}$ adamowicz and trautman ( 1975 ) have studied the precession of such a test particle in a torsion background . all these considerations seem to be of only academic interest , however , since torsion only arises inside matter . there , the very notion of a spinning test particle becomes obscure ( h . gollisch , 1974 , unpublished ) . only neutrinos , whose spin self interaction vanishes , seem to be possible candidates for $u_4$ test particles . ( hehl , 1971 ) reference here is apparently original result on the motion of test particle with spin : hehl , f . w . ( 1971 ) . how does one measure torsion of space-time ? . phys . lett . a , 36 ( 3 ) , 225-226 . ( http://dx.doi.org/10.1016/0375-9601(71)90433-6 ) for a more modern notation ( tetrad formalism ) for the mentioned mathisson-papapetrou equations you can use the thesis : laskoś-grabowski , p . ( 2009 ) . the einstein–cartan theory : the meaning and consequences of torsion . master 's thesis pp . 17-19 the references there should provide all further information .
atoms in a solid are usually stuck to each other in some sort of a rigid lattice , which gives the solid its structure and shape . this is not the only possible state of affairs : they can also slide past each other , if the temperature is high enough , but then they are likely to do so endlessly , all of them , and the material becomes a liquid . and , indeed , in a liquid you can have both negative and positive charge carriers , where the positive ones are entire atoms with one electron stripped off .
the solid angle is defined as the area on the unit sphere subtended by the angle divided by one unit area . it is a ratio so it is a single dimensionless number . i see why you think it should be a 2d quantity , because the surface of a sphere , and any patch on it , is a 2d manifold and you need two quantities ( traditionally $\theta$ and $\phi$ ) to map it . when you calculate an area on the sphere you are basically calculating a definite integral over $\theta$ and $\phi$ , and the result is of course just a single number . you do lose information in the process - for example you just know the total area not the shape of the patch on the sphere . the solid angle that covers the whole sphere is of course $4\pi$/1 or $4\pi$ .
the momentum flux tensor comes from the momentum equation of navier-stokes equations : $$ \frac{\partial\left ( \rho\mathbf{u}\right ) }{\partial t}+\nabla\cdot\mathbf{p}=0\tag{1} $$ or , using indices ( where it is easier to see that $\mathbf{p}$ is a rank-2 tensor ) : $$ \frac{\partial\left ( \rho u_i\right ) }{\partial t}+\frac{\partial\pi_{ij}}{\partial x_j}=0\tag{2} $$ we can split this tensor into three components : advection of $i$-momentum in the $j$-direction : $\left ( \rho u_i\right ) u_j$ pressure : $p\delta_{ij}$ stress tensor : $\sigma_{ij}$ the first two are rather straight-forward ( but can be elaborated on a little bit more if you need it ) , but the third is a little more complicated . we generally regard the stress tensor as traceless and symmetric : $$ \sigma_{ij}=\mu\left ( \frac{\partial u_j}{\partial x_i}+\frac{\partial u_i}{\partial x_j}-\delta_{ij}\frac{2}{3}\ , \frac{\partial u_i}{\partial x_i}\right ) $$ where $\mu$ is a ( fluid-dependent ) constant , referred to as viscosity . this term describes the deformation of the fluid . edit if we absorb bullets 2 and 3 into the single quantity $\tau_{ij}$ , then equation ( 1 ) becomes $$ \frac{\partial\left ( \rho \mathbf{u}\right ) }{\partial t}+\mathbf{u}\cdot\nabla\rho\mathbf{u}+\nabla\cdot\tau=0\tag{3} $$ the reason we do this is because of the subscript $j$ ( see equation ( 2 ) ) on one of the $u$ 's is identical to the gradient operator , $\nabla$ . we can further describe the left two terms as a single entity most-often called the material derivative : $$ \frac{d}{dt}=\frac{\partial}{\partial t}+\mathbf{u}\cdot\nabla\equiv d_t $$ from the leibniz rule , $$ d_t\rho\mathbf{u}=\mathbf{u}d_t\rho+\rho d_t\mathbf{u} $$ and if we assume that the flow is incompressible , then $d_t\rho=0$ and equation ( 3 ) becomes $$ \rho d_t \mathbf{u}+\nabla\cdot\tau=0\tag{4} $$
for a linear system , the superposition principle holds since , be definition , a linear system has the following property : ( 1 ) if $y_1$ is the output for input $x_1$ and ( 2 ) if $y_2$ is the output for input $x_2$ then ( 3 ) the output is $ay_1 + by_2$ for input $ax_1 + bx_2$ in other words , the output for a superposition of inputs is the superposition of the associated outputs . so , if the differential equation for your system is linear , e.g. , the harmonic oscillator , the superposition principle holds . what , then , are you trying to prove ? prove the superposition principle for inhomogenous linear equations of motion used in deriving the motion of a driven oscillator . will it still apply if the force on an oscillator was −kx2 instead of −kx ? this is , i think , misworded . for example , for the mass on a ( linear ) spring system , the force on the mass , due to the spring is , by hooke 's law , $-kx$ . a driving force , on the other hand , would be given as a function of time : $f_d = f ( t ) $ . then , the net force on the mass is the sum of the driving force and the spring force , $f = f ( t ) - kx$ , which leads to a linear differential equation : $$m \ddot x +kx = f ( t ) $$ and thus , the superposition principle holds by definition . this is easy to show by assuming $f ( t ) = f_1 ( t ) + f_2 ( t ) $ and $x ( t ) = x_1 ( t ) + x_2 ( t ) $ and inserting into the differential equation . however , the way i read the problem as stated in your edit , it is the restoring force , not the driving force , that is $-kx^2$ . if that is in fact the case , the resulting differential equation is non-linear $$m \ddot x +kx^2 = f ( t ) $$ and thus , the superposition principle will not hold since $$ ( x_1 + x_2 ) ^2 = x_1^2 + x_2^2 + 2x_1x_2 \ne x_1^2 + x_2^2$$
you do not need to use that . you can simply do the cross-contractions by hand . let 's do that . note that i only care about the the $\frac{1}{z^4}$ term to evaluate the central charge . we have \begin{equation} \begin{split} t ( z ) t ( w ) and =\left ( : \partial_z b c ( z ) : - \lambda \partial_z : b c ( z ) : \right ) \left ( : \partial_w b c ( w ) : - \lambda \partial_w : b c ( w ) : \right ) \\ and = : ( \partial_z b ) c ( z ) : : ( \partial_w b ) c ( w ) : - \lambda \partial_z : b c ( z ) : : ( \partial_w b ) c ( w ) : \\ and ~~~~~~~~~~~~~~~~~~~~~~- \lambda : ( \partial_z b ) c ( z ) :\partial_w : b c ( w ) : + \lambda^2 \partial_z : b c ( z ) :\partial_w : b c ( w ) : \end{split} \end{equation} now at each step , we only keep the full contractions to extract the central charge . we then find \begin{equation} \begin{split} t ( z ) t ( w ) and \sim \partial_z \frac{1}{z-w}\partial_w \frac{1}{z-w} - \lambda \partial_z \left ( \frac{1}{z-w} \partial_w \frac{1}{z-w} \right ) \\ and ~~~~~ - \lambda \partial_w \left ( \frac{1}{z-w} \partial_z \frac{1}{z-w} \right ) + \lambda^2 \partial_z \partial_w \frac{1}{ ( z-w ) ^2} \\ and = \frac{-6\lambda^2 + 6 \lambda - 1 }{ ( z-w ) ^4} + \cdots \end{split} \end{equation} we can then read off $$ c = 2 \left ( -6\lambda^2 + 6 \lambda - 1 \right ) = - 3 ( 2 \lambda - 1 ) ^2 + 1 $$
i am not sure about a convention , but i would normally write it as follows $$k=k ( x ) =k ( l \hat{x} ) = k_0f ( \hat{x} ) , $$ where $k_0$ is the pre-factor carrying the dimension and $f ( \hat{x} ) $ the function determining the spatial variation , which can easily be derived from $k ( x ) $ . this will give you $$\frac{d\hat{t}}{d\hat{t}}=f ( \hat{x} ) \frac{d\hat{u}^2}{d\hat{x}^2} . $$ in practical problems , the terms $x/l$ and $k_0$ naturally appear , e.g. $k ( x ) =k_0 ( 1+b\frac{x}{l} ) $ would give you $f ( x ) =1+b\hat{x}$ .
when you solve the schrodinger equation for a free particle you get a family of solutions of the form $\psi ( x , t ) = a e^{i ( kx - \omega t ) }$ and all superpositions of these functions . so just solving the schrodinger equation does not give you a solution for a specific particle . for that you need to specify the initial conditions . if you take the solution to be $\psi ( x , t ) = a e^{i ( kx - \omega t ) }$ then you are ( without realising it ) specifying the initial condition to be a completely delocalised particle i.e. one for which we have precise knowledge of the momentum but no knowledge of the position . that is why when you attempt to calculate the position you get a silly answer . if you specify the initial conditions as $\psi ( x , 0 ) $ then you have effectively created a wavepacket describing your particle , so it does have a finite uncertainty in position , and of course now a finite uncertainty in momentum . you can now calculate the expectation value of position as a function of time . your $\psi ( x , 0 ) $ will probably be expressed as a linear superposition of the plane wave solutions . to calculate the superposition just fourier transform your $\psi ( x , 0 ) $ . response to comment : in your comment you ask : i would have to plug in some constraints first and get a value for a or how would i go about doing that ? but it is not just a matter of choosing the value of $a$ in $a e^{i ( kx - \omega t ) }$ because this does not describe a localised particle whatever value you choose for $a$ . suppose at time $t = 0$ we know the position of the particle precisely , $x = 0$ . this means the initial wavefunction is a delta function : $$ \psi ( x , 0 ) = \delta ( x ) e^{-i\omega t} $$ i.e. $\psi ( x , 0 ) $ is zero everywhere except at $x = 0$ . the position of this particle is obviously $x = 0$ . the trouble is that it is not obvious how this wavefunction evolves in time . we know how the plane waves evolve in time , so we can easily calculate the time evolution if we could express the $\delta ( x ) $ function as a sum of plane waves : $$ \delta ( x ) = \sum\limits_i a_i e^{i ( kx - \omega t ) } $$ the problem is working out how to do this sum , i.e. what are the values of the coefficients $a_i$ and how many terms we need in the sum . we can work this out by fourier transforming our $\delta$ function , because this is exactly what a fourier transform does . it expresses any function as an integral of plane waves . the wikipedia article i have linked goes into more detail on this . the answer is that : $$ \delta ( x ) = \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{i ( kx - \omega t ) } dk $$ in fact choosing a $\delta$ function as the initial conditions is not helpful because if we have an exact position we have infinite uncertainty in momentum , and if the momentum is infinitely uncertain we can not calculate the future position . if you are trying to describe a real system you would choose something like a gaussian : $$ \psi ( x , 0 ) = k \space e^{- ( x^2/\delta x ) ^2} e^{-i\omega t} $$ this describes a particle with the expectation value of $x = 0$ and the uncertainty in $x = \delta x$ . you can now fourier transform your $\psi ( x , 0 ) $ to express it as an integral of plane waves , and you can now calculate the expectation value of $x$ as a function of time .
the small changes in sunrise and sunset are caused by the tilt of the world , and the changes in light the earth gets causes winter and summer . it is been going on for millions of years , so there is no real harm there . one might also note things like the effect of these on thunderstorms , because the thunderstorms come ultimately from the earth 's magnetic field , and this can be influenced by the solar wind and the moon . nasa keeps an eye out for ' solar flares ' which , while not part of weather , can cause a nasty shock to society chance one comes our way .
all the work was done during the time your foot was in contact with the ball--those few milliseconds of impulse--identical to momentum--was transferred . without gravity that ball would not travel a certain distance but travel forever . the work done is equal to the ke the ball now possesses . if your foot remained in contact with the ball it would continue to accelerate , and even more work would have been done .
the book was published in 1988 . flipping through it now , i do not notice anything that has actually turned out to be wrong . it predates the modern era of high-precision cosmology and the discovery of the acceleration of cosmological expansion . the chapter on string theory dates from the era when string theory was not yet looking like a failed research program .
$i_{13}$ , $i_{32}$ and $i_{21}$ in eq . ( 3.56 ) are positive , as shown in the sentence below ( 3.57 ) and also in the caption of fig . 6 .
it seems they are using the law of cosine http://en.wikipedia.org/wiki/law_of_cosines and $\theta$ is really an angle between $r$ and $r'$ . in this problem you can always choose the coordinate system in the way that the $r$ is directed along the axis z in cartesian coordinates , because only relative distance between points designed by $r$ and $r'$ matters
how does this connect in any way to my equations ? you know the distance travelled and you know the proper time , $t ' = \tau = 10y$ . using the timelike invariant interval equation , solve for t : $t^2 = \tau^2 + ( \frac{r}{c} ) ^2 = ( 10y ) ^2 + ( 4.4y ) ^2 = 119.36y^2 \rightarrow t = 10.93y$ $u = \frac{r}{t} = 0.403c$ since you insist on doing it the hard way , here 's one approach : from your first transformation equation : $\dfrac{\delta x}{\delta t'} = \gamma u = \dfrac{4.4ly}{10y} = 0.44c$ from your 4th transformation equation : $\gamma \delta t = \delta t ' + \delta x \frac{\gamma u}{c^2} = 10y + 4.4y ( 0.44 ) = 11.94y$ combining these results : $ ( \gamma u ) ( \gamma \delta t ) = 5.25ly = \gamma^2 u \delta t = \gamma^2 \delta x = \gamma^2 4.4ly$ from which we get : $\gamma = 1.093$ $\delta t = \gamma \delta t ' = 1.093 \cdot 10yr = 10.93y$ $u = \gamma u / \gamma = \dfrac{0.44c}{1.093} = 0.403c$ as i attempted to point out in comments , the invariance of the interval is fundamental and useful here . the lorentz transformations guarantee that : $ ( c\delta t ) ^2 - ( \delta x ) ^2 = ( c\delta t' ) ^2 - ( \delta x' ) ^2$ you know that $\delta x = 4.4ly , \delta x ' = 0$ , and $\delta t ' = 10y$ so you just put in what you know and you get $\delta t$ immediately and the speed $u = \dfrac{\delta x}{\delta t}$ immediately follows . there is no need to find $\gamma$ in this problem
this is very much a quick and dirty answer , i havn't though too much about it . i might update my answer if i find time in the weekend , otherwise i hope others will give more precise answers . $v_{\alpha\beta\gamma\delta}$ transforms reducibly under $su ( 2 ) $ , as tensor products of four spin $\frac 12$ representations . using $\bf\frac 12\otimes\frac 12 = 0 \oplus 1$ and $\bf 1\otimes 1 = 0 \oplus 1 \oplus 2$ , we find that $v_{\alpha\beta\gamma\delta}$ decomposes into these irreducible representations $$\mathbf{\frac 12\otimes\frac 12\otimes\frac 12\otimes\frac 12} = ( 2\times\mathbf{0} ) \oplus ( 3\times \mathbf{1} ) \oplus \mathbf{2} , $$ two singlets , three triplets and a spin 2 ( 5-dimensional representation ) . there is an action of the permutation group $s_4$ on the indices of $v_{\alpha\beta\gamma\delta}$ , for $su ( n ) $ it turns out that decomposing this tensor into irreducible representations of the permutation group also corresponds to irreducible representations of $su ( n ) $ . this can be done rather quickly using young tableau , you can find the details in most representation theory books for physicists ( i do not have a relevant book here and do not remember the details . i might add the answer in the weekend ) . in the case of a tensor product $\bf 1\otimes 1 = 0\oplus 1\oplus 2$ , we get the following decomposition $$ t_{ij} = \delta_{ij}\frac{tr ( t ) }3 + \left ( \frac{t_{ij}-t_{ji}}2\right ) + \left ( \frac{t_{ij}+t_{ji}}2 - \delta_{ij}\frac{tr ( t ) }3\right ) . $$ these three terms transform irreducibly as spin $0$ , spin $1$ and spin $2$ representations of $su ( 2 ) $ , respectively . in terms for the permutation group , these are the trivial , anti-symmetric and trace less symmetric representations , respectively . something similar can be done for $v_{\alpha\beta\gamma\delta}$ , by using young tableau or just by playing around with it .
yes . g2 shows up often , starting with atomic physics ( perhaps racah is the first ; see r . e . behrends , j . dreitlein , c . fronsdal , and b . w . lee , “simple groups and strong interaction symmetries , ” rev . mod . phys . 34 , 1 ( 1962 ) . ) . you will find some refences in my 1976 phys rev paper on cns . physics . gatech . edu/grouptheory/refs . i have whole folder of physics g2 papers , but now i see i did not bother to enter g2 history into www.birdtracks.eu. nobody 's perfect . sorry predrag ( for responses , email to dasgroup [ snail ] gatech . edu , i sometimes look at those . pure accident i saw this question . . . )
yes , the $r$-argument really is $r_{ik}:=|r_i-r_k|$ , as he writes two pages earlier at the beginning of " systeme von endlich vielen teilchen " . but then you do not need the force to show the relation , it is just the chain rule , which makes derivatives of $u$ into a two term expression and notice that $u ( r_{ik} ) =u ( |r_i-r_k| ) =u ( |r_k-r_i| ) =u ( r_{ki} ) $ . also , it is not so good , that you write "$\frac{d}{dt}r_{a}\nabla_{a}u_{ab}$" for "$\frac{dr_{a}}{dt}\nabla_{a}u_{ab}$" , because it suggests that you mean "$\frac{d}{dt} ( r_{a}\nabla_{a}u_{ab} ) $" . moreover , the books name is not the autors name . and i would change the title to something readable , and by that i do not mean the problem with the total derivative , but a title which is a sentence , not a formula . e.g. " a problem deriving the energy conservation for radial two particle potentials " .
no . boiling itself does not mean that the water will cook anything . if you have boiling water at 30°c you could touch it ( if we forget that it is at really low pressure ) and nothing would happen . boiling is not what cooks , but temperature . in fact , if you want to purify water at high altitudes , you need to boil water for a longer time because it will be at a lower temperature .
the other answers given are correct , but i want to elaborate - we can use a simplifying assumption of " gravity acts as if all mass is at the center of the object " only if the object is a sphere or uniform mass density . because really , portions of star #1 are closer than others ( and thus exert more force ) , yet we consider all the mass of star #1 to be concentrated at its center . we can not apply this assumption to systems of multiple spheres .
pure states are a convenient abstraction for studying tiny , specially-prepared quantum systems . states of complicated systems are never pure . one makes ( very small ) systems pure by careful preparation . for example , a simple spin by means of a stern-gerlach magnet and a screen where only the up particles can pass . more complex systems need more complex preparation to make them ( at least approximately ) pure . note also that pure states can be entangled ; indeed , entanglement is usually defined only for pure states . thus there is no conflict between entanglement and pureness .
the ions and the electrons do not necessarily have the same temperature ( non-thermal plasma ) , but if you leave them for a while , they will undergo equilibration . i would not overestimate the value in kelvins of different degrees of freedom of subsystems . the temperature is associated with a mean kinetic energy . if you tackle an electron , you can accelerate it easily because of its low mass . conversely , even a fast electron will not give raise to the same momentum transfer as a heavy particle . so a fast electron is " not as powerful " as an equally fast ion . if you have an application like the ball , there the effect is mainly generated by accelerating of electrons in the electric field . if you go away from the electrodes , the field gets weaker and there the electrons lose their kinetic energy due to collisions with heavier particles . this is why a too high particle density ( or pressure ) is not the friend of open corona discharges - the glow effect can not extend too far away without an opposing charge somewhere else , such that there is a relevant electric field in between . of course , if the temperature is generally high ( thermal plasma ) as in the sun , then you will have charges flying around in any case . but for the earthly applications you have in mind , the area containing free electrons/ions does not extend forever and the temperature will not kill you unless the electric field that produced it is super strong . then as shaktyai pointed out , plasmas are not always totally ionized , usually the opposite is the case . for some cases the saha equation holds and there you get an idea about the functional dependence of the ionization degree with temperature . for high $t$ , the factor goes against 1 ( graph exp ( -1/x ) in wolfram alpha or so ) .
the sun is not " made of fire " . it is made mostly of hydrogen and helium . its heat and light come from nuclear fusion , a very different process that does not require oxygen . ordinary fire is a chemical reaction ; fusion merges hydrogen nuclei into helium , and produces much more energy . ( other nuclear reactions are possible . ) as for rockets , they carry both fuel and oxygen ( or another oxidizer ) with them ( at least chemical rockets do ; there are other kinds ) . that is the difference between a rocket engine and a jet engine ; jets carry fuel , but get oxygen from the air .
there is a sense in which this is right . if we model the laser as a stream of photons , hitting some surface , then it is the change in the momentum of the photons due to their interaction with the surface that causes the pressure . for example , if the laser shines on a mirror , then the photons will bounce back after hitting the mirror , and there momenta will change by an amount equal to twice the magnitude of their original momentum . however , it is also possible for the photons to be absorbed by the material . in this case , the photons do not bounce back , but their momentum change by an amount equal to the initial magnitude of the their momenta , and this momentum change due to interaction with the surface is what causes the pressure . on the other hand , if you are referring to the object that emits the laser , then it will certainly be the case that this object will feel a force in a direction opposite that of the beam travel direction . by conservation of momentum , if the laser emits a photon of momentum $p$ , then its momentum must increase in the opposite direction by that same amount $p$ as well . the change in momentum of the object then leads to propulsion . see also the wiki and especially the second paragraph in the quantum theory argument section .
a very general discussion-not specific to a system : the internal energy , $u$ , of a system is a function of state , which means that its value only depends on the thermodynamic variables ( $p , v , t ) $ for example , at a given state ( this means for a given set of values of these variables ) . let us make this more concrete : imagine the system is in a thermodynamic state where the thermodynamic variables have the values ( $p_i , v_i , t_i$ ) ( $i$ stands for initial ) . at these values of the thermodynamic variables the internal energy has a value : internal energy at the initial state $i$: $u ( p_i , t_i , v_i ) $ . you can think of a gas at pressure , volume and temperature condition ( $p_i , v_i , t_i$ ) . now imagine you change the thermodynamic variables to these ones ( $p_f , v_f , t_f$ ) ( $f$ stands for final ) . the internal energy now has a new value internal energy at the initial state $f$: $u ( p_f , t_f , v_f ) $ . in this process you have changed the internal energy of the system by an amount : change in u : $\delta u= u ( p_f , t_f , v_f ) - u ( p_i , t_i , v_i ) $ i hope it is clear to observe that the system could have followed an infinitely large set of $ ( p , v , t ) $-points , along an infinitely large number of different paths in order to go from state $i$ to state $f$ . however , these are not , in any way , influencing by how much $u$ will change , you can take which ever path you please to go from state $i$ to state $f$ . so the system has no memory of the intermediate states . in mathematical terminology , this means that the differential change , $du$ , is a perfect differential and this is stated by the simple mathematical expression $\oint_c du=0$ it is very similar to the gravitational potential of the earth , for example , which tells us that the amount of energy we need to spend to lift an object by 3m , does not depend whether we bring it straight vertically up or we follow some other path .
the answer to your question is , sometimes , but it depends on the source and your hypothesised relationship approximately holds in some cases . in three dimensions , your relationship does not hold . if the sound source is small , then its pressure field outside the source 's hardware will be a general multipole scalar field , i.e. a general superposition of spherical waves each fulfilling helmholtz 's equation $ ( \nabla^2 + k^2 ) \psi = 0$ where $k = 2\pi/\lambda$ is the wavenumber at the frequency in question in the medium in question : $$\psi ( r , \theta , \phi ) = \sum\limits_{\ell=0}^\infty \sum\limits_{\nu= -\ell}^\ell \psi_{\ell , \nu}\ , j_\ell ( k r ) p^{|\nu|}_\ell ( \cos ( \theta ) ) e^{i\ , \nu\ , \phi}$$ where $ ( r , \theta , \phi ) $ are the spherical co-ordinates of the point in question , $p^{|\nu|}_\ell$ are the associated legendre functions and $j_\ell$ are the spherical bessel functions of the first kind and order $\ell$ . in the farfield , i.e. when $k r \gg \ell$ we have $j_\ell ( k\ , r ) \approx \sin ( k\ , r - \ell \pi/2 ) / r$ so that the wave 's intensity varies like $1/r^2$ in the farfield . therefore , a factor of 10 increase in the distance from the source leads to a factor of 100 decrease in the intensity ( radiated power per unit area ) , which in decibel terms is a loss of $10\log_{10}100 = 20{\rm db}$ . therefore , to match the intensity of a 50db source at 1m distance , you are going to need a 70db source at 10m and a 90db source at 100m . you can get this result imagining the source is like an isotropic radiator with some , say dipole , radiation pattern . there will be an inverse square intensity dependence on dustance . however , suppose your source is somehow cylindrical . maybe it is like a paper cone loudspeaker but the cone is replaced by a very long paper cylinder radiating in and out . if you are near enough to the cylinder that it can be approximated as being infinitely long , then you have essentially gotten yourself a two dimensional problem , the waves are now cylindrical waves : $$\psi ( r , \theta ) = \sum\limits_{\ell=0}^\infty \sum\limits_{\nu=-\infty}^\infty \psi_{\nu , \ell}\ , j_\nu ( k_\ell r ) e^{i\ , \nu\ , \phi}$$ where now $j_\nu$ is the bessel function of the first kind and order $\nu$ . now in the farfield , $j_\nu ( k_\ell r ) \approx\sqrt{2/\pi}\cos ( k_\ell r -\nu\pi/2 -\pi/4 ) /\sqrt{r}$ so that the wave 's intensity varies like $1/r$ in the farfield and now a factor of ten increase in distance corresponds to a loss of 10db . therefore , to match the intensity of a 50db source at 1m , you would need a 60db source at 10m way or a 70db source at 100m away .
the waning half moon is 3 weeks old , it rises at about midnight . all celestial objects appear to rise in the east and set in the west because of the rotation of the earth . therefore , from your choices , the answer would be 1 , on the eastern horizon . i guess that does not fully answer your question , which is different from the question you reference . the moon rises about 48 minutes later each day . when the moon is full it rises as the sun sets because it is . . . full . 3 weeks later you have 21 days * 48 minutes = 1008 minutes or about 17 hours . if the sun sets at 6pm then the moon will rise about 17 hours later , and about an hour later , at midnight , it will be just above the eastern horizon . hope that makes more sense : )
i do not think there will yield any new physics if you use your order of eigenvectors to form pauli matrices . the reason we are using the basis eigenvectors from $m_s=+s$ to $m_s=-s$ ( left-upper corner to right-lower corner ) is by convention , i guess . now we can figure out what happens if we use your suggestion . suppose we adopt your suggestion and use the basis eigenvectors from $m_s=-s$ to $m_s=+s$ ( left-upper corner to right-lower corner ) , what will the pauli matrices be under the basis in this order ? we should see that the $s_x$ , $s_y$ , $s_z$ operators do not change , they are still : $s_x=\frac{\hbar}{2} ( |+\rangle \langle -|+ |-\rangle \langle +| ) $ , $s_y=\frac{\hbar}{2} ( -i|+\rangle \langle -|+ i|-\rangle \langle +| ) $ , $s_z=\frac{\hbar}{2} ( |+\rangle \langle +|- |-\rangle \langle -| ) $ . here $|+\rangle$ and $|-\rangle$ are eigenvectors of $s_z$ . then we can write the pauli matrices as following : $s_x=\begin{pmatrix}\left \langle -|s_x | - \right\rangle and \left\langle -|s_x | + \right\rangle\\ [ 0.1in ] \left \langle + |s_x| - \right \rangle and \left \langle + |s_x | + \right \rangle \end{pmatrix}=\frac{\hbar}{2} \begin{pmatrix} 0 and 1 \\ [ 0.1in ] 1 and 0\end{pmatrix}=\frac{\hbar}{2}\sigma_x$ . similarly , we have $s_y=\begin{pmatrix}\left \langle -|s_y | - \right\rangle and \left\langle -|s_y | + \right\rangle\\ [ 0.1in ] \left \langle + |s_y| - \right \rangle and \left \langle + |s_y | + \right \rangle \end{pmatrix}=\frac{\hbar}{2} \begin{pmatrix} 0 and i \\ [ 0.1in ] -i and 0\end{pmatrix}=\frac{\hbar}{2}\sigma_y$ . $s_z=\begin{pmatrix}\left \langle -|s_z | - \right\rangle and \left\langle -|s_z | + \right\rangle\\ [ 0.1in ] \left \langle + |s_z| - \right \rangle and \left \langle + |s_z | + \right \rangle \end{pmatrix}=\frac{\hbar}{2} \begin{pmatrix} -1 and 0 \\ [ 0.1in ] 0 and 1\end{pmatrix}=\frac{\hbar}{2}\sigma_z$ . we can check that the commutation relation $ [ s_i , s_j ] =\epsilon_{ijk}i \hbar s_k$ still holds for the new pauli matrices . besides , other properties of pauli matrices hold as well . therefore , it does not matter which order of eigenvectors you use . we use the commonly-accepted order by convention .
a wavefront ( your signal ) has a fixed amount of energy given to it by the transmitter . whatever happens to the wave once it leaves the transmitter is independent of the transmitter , thus receiving a signal does not drain any additional energy from the transmitter ( though it can drain energy from the wavefront itself ) . edit : as pointed out by @alfred centauri in the comments , the transmitter would be affected if the receiver was in the near field . for amateur radio purposes , the near field ceases to exist well within 200 meters of the transmitter ( for the vast majority of cases ) , thus it is unlikely that anyone " tuning in " to your broadcast would be directly affecting your transmitter .
strictly speaking it is a unit of energy . but using $m=\frac{e}{c^2}$ , you can convert energy into mass . operating , we get $1{\rm\ , ev}/c^2 =1.78\cdot 10^{-36}\rm{\ , kg}$ . ( the $c^2$ is usually ommited . )
there are a few differences between luna and titan . one of the primary mechanisms for atmospheric loss is thermal escape . titan is much colder . the particles which escape are essentially the tail of the maxwell-boltzmann distribution , the portion with velocity higher than the escape velocity . this end of the distribution is dominated by an $e^{-e/kt}$ contribution , so as you had expect , lower temperature means fewer particles with enough kinetic energy to escape . note also that since we care about escape velocity , while the distribution is really about energy , more massive particles will not escape as easily . titan 's atmosphere is mostly nitrogen , while the moon is mostly helium and argon . the helium in the moon 's atmosphere is easily lost , since it is so much lighter . another big cause of atmospheric loss is the solar wind . titan itself does not have a magnetic field to protect it from the solar wind , but it does happen to orbit an enormous planet with a magnetic field . titan is protected from the solar wind by saturn 's magnetosphere . titan orbits at about 20 $r_s$ , while the magnetopause is somewhere between 16 and 27 $r_s$ , so titan is inside the magnetosphere a substantial amount of the time . there are a lot of complications due to passing through the magnetopause , but from what i understand , the net effect is definitely protective . and of course , titan is farther from the sun , so the solar wind is weaker .
( 1 ) since $u ( \textbf{r} ) = u ( \textbf{r}+\textbf{r} ) $ , we can expand this part in terms of reciprocal lattice vectors , $u_k ( \textbf{r} ) = \sum_\textbf{g}{e^{i\textbf{g}\cdot \textbf{r}}u_\textbf{k-g}}$ . we can therefore write : \begin{equation} \psi_{\textbf k+\textbf k} = e^{i ( \textbf k + \textbf k ) \cdot \textbf r}\sum_\textbf{g'}{e^{i\textbf{g'}\cdot \textbf{r}}u_{\textbf k-\textbf k- \textbf g'}} = e^{i\textbf k \cdot \textbf r}\sum_\textbf{g'}{e^{i ( \textbf{g'}+\textbf k ) \cdot \textbf{r}}u_{\textbf k-\textbf k- \textbf g'}}=e^{i\textbf k \cdot \textbf r}\sum_\textbf{g}{e^{i\textbf{g}\cdot \textbf{r}}u_{\textbf k-\textbf g}} = \psi_\textbf k \end{equation} where $\textbf g = \textbf k+\textbf g'$ . ( 2 ) you can interpret $\textbf p'$ as being equal to $\textbf p$ . this is true because the real space lattice is periodic ; $\textbf k$ is always equal to $\textbf k + \textbf k$ . ( 3 ) the conserved quantity is $\textbf k$ $\textit mod$ $\textbf k$ . you can see that i used this fact in the answer to ( 2 ) . you can read just about any solid state physics textbooks for complete justification though my personal favorite is ziman 's theory of solids .
since the magnetic field lines have to close themselves , when it transverses the superconductor it has to do it in a continuous fashion . this means , since the superconductor expels the field when in the sc state , the field gets trapped because it has no way of transverse the cylinder ring without opening the magnetic field lines ( some geometric imagination is useful here ; ) . hope it is clear enough .
the correct answer is that surface tension prevents the formation of small water droplets . you see , as the surface energy is proportional to the droplet area ( $r^2$ ) and the bulk energy - to its volume ( $r^3$ ) , their ratio ( $1/r$ ) will rise to infinity as the droplet radius $r$ goes to zero . therefore , the droplet cannot grow steadily from $r=0$ . even below 0°c there is a certain critical droplet radius $r_c ( t ) $ , below which the gibbs free energy of droplet is higher than that of vapor so the droplet evaporates . condensation nuclei are a shortcut to the droplet formation : if they are hygroscopic and have a radius above $r_c$ , water surface energy will be too small to prevent condensation and the droplet will grow quickly ( assuming that vapor is supersaturated ) .
i have a strong reason to believe i have found the correct answer to my own question , you may correct me if i am wrong . but this image seems to explain everything about my question in one single hit : these are results from bowmaker and dartnall ( 1980 ) . relevant reference : bowmaker , j.k. , and dartnall , h.j.a. visual pigments of rods and cones in a human retina . journal of physiology , 298 , 1980 , 501-511 . it seems that the l-receptor is actually more active at the very shortest end of wavelengths than it is for just longer than what we can see as visible light . you can see curve of red going up towards the short end of the wavelength axis . the l-receptor ( associated with red ) activation is not a bell-curve over the linear wavelength axis ( as one would expect ) . that would explain the little bit purple-ish blue we see at 400 nm ! so luckily the brain is not freaking out , but the receptors are just a bit strange , probably with the goal to distinguish blue from more blue ( from a functional view of ' evolution' ) . note that it is logical that this is not the case on the right ( longer wavelength ) side of the graph , because there red is accompanied by green closely . thus we can distinguish red from redder by the mixture of green .
in standard newtonian mechanics , acceleration is indeed considered to be an absolute quantity , in that it is not determined relative to any inertial frame of reference ( constant velocity ) . this fact follows directly from the principal that forces are the same everywhere , independent of observer . of course , if you are doing classical mechanics in an accelerating reference frame , then you introduce a fictitious force , and accelerations are not absolute with respect to an " inertial frame " or other accelerating reference frames - though this is less often considered , perhaps . note also that the same statement applies to einstein 's special relativity . ( i do not really understand enough general relativity to comment , but i suspect it says no , and instead considers other more fundamental things , such as space-time geodesics . )
it is about 0.003 gev , see just to be sure , the measured width of the bumps is due to experimental errors and other things that depend on the situation , not because of the higgs ' intrinsic width . the width dramatically increases with the mass . as one approaches a tev , the width would be almost exactly equal to the mass itself . for the sake of completeness , this is the graph of the branching ratios ( proportions of the decays ending with a given final state ) : the 125 higgs decays to ( virtual or real ) $b\bar b$ ( messy final state , a bit hard to isolate from the background ) in 65% of cases , $ww$ ( neutrinos from the decayed $w$ are missing energy ) in 20% , $gg$ ( messy ) in 7% of cases , $\tau\tau$ in 6% of cases , $zz$ in 3% of cases , $c\bar c$ in 2% of cases , $\gamma\gamma$ in 0.2% , $\gamma z$ in 0.15% of cases . the numbers were estimated by looking at the graph above so they do not quite add to 100 percent , sorry .
the reason for the asseveration if time $t$ , does not appear in lagrangian $\mathcal{l}$ , then the hamiltonian $\mathcal{h}$ is conserved . this is the energy conservation unless the potential energy depends on velocity . is that , from the definition of the hamiltonian as the legendre transformation , $$\mathcal{h}\equiv\sum_i\dot{q}_i\frac{\partial\mathcal{l}}{\partial\dot{q}_i}-\mathcal{l}\hspace{1in} ( \dagger ) $$ and knowing that for any function in phase space , $f=f ( q_i , p_i , t ) $ , $$\frac{df}{dt}=\frac{\partial{f}}{\partial{q}_1}\frac{d{q_1}}{d{t}}+\ldots+\frac{\partial{f}}{\partial{q}_n}\frac{dq_n}{dt}+\frac{\partial{f}}{\partial{p}_1}\frac{d{p_1}}{d{t}}+\ldots+\frac{\partial{f}}{\partial{p}_n}\frac{dp_n}{dt}+\frac{\partial{f}}{\partial{t}}\\=\sum_{j=1}^n\left ( \frac{\partial{f}}{\partial{q}_j}\dot{q}_j+\frac{\partial{f}}{\partial{p}_j}\dot{p}_j\right ) +\frac{\partial{f}}{\partial{t}}\\=\left\{f , \mathcal{h}\right\}+\frac{\partial{f}}{\partial{t}}$$ where $\left\{f , \mathcal{h}\right\}$ is the poisson bracket of $f$ and $\mathcal{h}$ , defined as $$\left\{f , \mathcal{h}\right\}\equiv\sum_{j=1}^n\left ( \frac{\partial{f}}{\partial{q}_j}\dot{q}_j+\frac{\partial{f}}{\partial{p}_j}\dot{p}_j\right ) =\sum_{j=1}^n\left ( \frac{\partial{f}}{\partial{q}_j}\frac{\partial{\mathcal{h}}}{\partial{p}_j}-\frac{\partial{f}}{\partial{p}_j}\frac{\partial{\mathcal{h}}}{\partial{q}_j}\right ) $$ if $\mathcal{l}=\mathcal{l} ( q_i , p_i ) $ , i.e. the lagrangian does not depend explicitly on time , which in turn means , from the definition $\mathcal{l}\equiv{t}-v$ , that kinetic energy $t$ and potential $v$ does not depend explicitly on time , then $\frac{d\mathcal{h}}{dt}=\left\{\mathcal{h} , \mathcal{h}\right\}=0$ . now , a constant of motion is precisely some function $f$ of phase space that is independent of time , i.e. such that $\frac{df}{dt}=0$ , so in this case the hamiltonian would be conserved . now , from the definition $ ( \dagger ) $ , you may verify that the hamiltonian equals the energy , $$\mathcal{h}\equiv{t}+v$$ only if $v=v ( q_i ) $ alone . so if that is the case , then energy would be conserved . so identify that in your lagrangian and get your conclusions , anyway you can always verify it this way for your particular case .
the fundamental representation of a lie group $g$ , as commonly used in this context , is the smallest faithful ( i.e. . injective ) representation of the group . we do not require fermions to belong to the fundamental rep , it is just the case that , in the standard model , they always either belong to the fundamental or the trivial representation ( as that is thoroughly indicated by experimental data ) , so there is rarely a need to look at other representations . to belong to a certain representation $v_\rho$ of $g$ means that the field is a section of the associated vector bundle $p \times_g v_\rho$ , where $p$ is the principal bundle belonging to our gauge theory . equivalently , the field is a $g$-equivariant function $p \to v_\rho$ fulfilling $f ( pg ) = \rho ( g^{-1} ) f ( g ) $ for all $p \in p$ and $g \in g$ . if principal bundles are not talked about , the field is often just taken to be a function $\sigma \to v_\rho$ , though this is , strictly speaking , not the way to do it . every field must belong to a representation of the gauge group ( even if it is the trivial one ) since the gauge transformations must have a defined action upon everything in our theory . it is not required that fields belong to irreducible representations ( again , it is simply often just the case ) , but since every reducible representation can be split up into irreducible ones , it is enough to look at the behaviour of the irreducible representations . ( note though that there are fields transforming in reducible representations - the usual $\frac{1}{2}$-spinors ( not weyl spinors ! ) transform as members of the $ ( \frac{1}{2} , 0 ) \oplus ( 0 , \frac{1}{2} ) $-representation of the lorentz group )
well , $\widehat{p^2} = \hat{p}^2= \hat{p} \hat{p}$ . so , in the position basis it is $-\hbar^2 \frac{d^2}{dx^2}$ , and $\langle p^2 \rangle = \int_{-\infty}^\infty \bar{\psi}\left ( -\hbar^2 \frac{d^2}{dx^2} \right ) \psi dx$ . note : $\hat{p}$ is technically not equal to $-i\hbar d/dx$ , but rather in the position basis $\langle x | \hat{p}| x ' \rangle = -i\hbar d/dx \delta ( x-x' ) $ .
what the definition needs to capture is that a black hole is not ( 1 ) a naked singularity , or ( 2 ) a big bang ( or big crunch ) singularity . we also want the definition to be convenient to work with so that , for example , it is possible to prove no-hair theorems . since we want to exclude naked singularities , it is natural that we require an event horizon . event horizons are by their nature observer-dependent things . for example , if we have a naked singularity , we can always hide its nakedness by picking an observer who is far away from it and accelerating continuously away from it . such an accelerated observer always has an event horizon , even in minkowski space . this example shows that it makes a difference what observer we pick . actually , we can not have a material observer at null infinity , since timelike infinity , not null infinity , is the elephants ' graveyard for material observers . however , the choice of null infinity is the appropriate one because a black hole is supposed to be something that light can not escape from . of course the actual universe is not asymptotically flat , but that does not matter . in practice , all we care about is that the black hole is surrounded by enough empty space so that the notion of light escaping from it is well defined for all practical purposes . there are other possible ways of defining a black hole , e.g. , http://arxiv.org/abs/gr-qc/0508107 .
http://en.wikipedia.org/wiki/containment_building by " nuclear reactor " i take it you mean the containment building that is visible looking at a nuclear reactor site . the pressure vessel which houses the nuclear reactor itself is not quite bell shape , and the containment building itself is not always bell shape either . here is an image from that article . you can browse through wikipedia for actual pictures of examples . i typically refer to these containment buildings being " can " shape or " sphere " shape . the best way to explain the reason for these two extreme shapes is to consider the two extremes from a design perspective . that is : limiting factor is the internal steam pressure in an accident limiting factor is material strength to hold up the containment itself if #1 is the case , the objective is to keep the containment from leaking or blowing apart , and you will make the containment out of steel . steel has good tensile strength and you will also build it spherical . this should probably go without saying , but the strongest structure to hold a pressure inside is a sphere . but that is not always the restricting engineering limit . consider the case that a massive massive volume is needed . large volume , lower maximum pressure . in that case , the ideal material selection will change , and you will objectively use a material that is cheaper and still has good specific compressive strength . trying to build a large volume , lower maximum pressure , containment will lead to a more " can " like shape , where you basically build a cylinder . this lets you build tall , keeping the forces compressive , and still not sacrifice too much in terms of strength against an internal pressure . of course , you still need a roof . generally , containment buildings exhibit a compromise between these two cases , which is what i was intending to demonstrate in that sketch . really , it is incorrect to say they are always dome/bell shape . the shape varies , and varies strongly country to country , and generation to generation , reflecting different engineering design choices .
the higher the water level in the tank , the faster the flow will exit from the orifice in the bottom . how fast will it exit ? viscous effects are of course present in the flow , but they can be neglected for the streamtube which exits the container , so we can employ bernoulli 's equation . mathematically , the flow variables will conform to the following relation : $p+\frac{1}{2}\rho v^2+\rho gz=p_0=const . $ where $p$ is the fluid static pressure , $\rho$ is the fluid density ( $\rho=1000kg/m^3$ in this case ) , $v$ is the local velocity , $g$ is the acceleration of gravity $ ( g=9.81m/s^2 ) $ , $z$ is the height above the reference plane , and $p_0$ is the stagnation pressure . now , the key insights for this problem are twofold . the first is to realize that the flow pressure is equal to atmospheric both at the top of the body of water and at the bottom $ ( p_{top}=p_{bottom}=p_{atm} ) $ . additionally , the velocity of the flow at the top is all but negligible in comparison to that of the jet flowing out of the orifice $ ( v_{top}\cong0 ) $ . bernoulli 's equation , with these assumptions applied , reduces to : $\rho gh=\frac{1}{2}\rho v_{jet}^2$ ( $z=0$ is the bottom of the container ) . because the density of water is essentially constant , we can simply divide it out and rearrange our equation to leave us with : $\boxed{v_{jet}=\sqrt{2gh}}$ . this result could have been derived purely from the principle of conservation of energy , but it is instructive to rigorously derive it in fluid dynamic terms . thus we can see that the higher the water level in the tank , the faster will be the velocity of the water jet , and ( for a given cross-sectional area ) , the flowrate will increase proportionately $ ( q=va ) $ . there is only one height for which the flowrate entering the container will be equal to the flowrate exiting , i trust that you can solve the problem from here .
set $x = r \cos \theta$ , $y = r \sin \theta$ . taking the total differentials , $$\mbox{d}x = \mbox{d}r \cos \theta - r \sin \theta \mbox{d} \theta , $$ $$\mbox{d}y = \mbox{d}r \sin \theta + r \cos \theta \mbox{d} \theta . $$ squaring and simplifyng $$\mbox{d}s^2 = {\mbox{d}x}^2 + {\mbox{d}y}^2 = {\mbox{d}r}^2 + r^2 {\mbox{d}\theta}^2 . $$ hence $$\frac{\mbox{d}s}{\mbox{d} t} \mbox{d} t = \sqrt{ \left ( \frac{\mbox{d}r}{\mbox{d}t}\right ) ^2 + r^2 \left ( \frac{\mbox{d}\theta}{\mbox{d}t}\right ) ^2 }\ , \mbox{d}t . $$ now , the property of being extremal is a characteristic of the curve , not of the coordinate system , so it is independent on the local chart you choose . in particular , the euler-lagrange equation retains the same form in both systems ( obviously , one changes the labels : $ ( x , y ) \to ( r , \theta ) $ ) . this remarks answer to point $ ( a ) $ and $ ( b ) $ . point $ ( c ) $ is a simple verification you can do eventually after have inverted the previous relations between $ ( x , y ) $ and $ ( r , \theta ) $ . for a brilliant discussion of this and more subtle points , let 's see arnold , mathematical methods of classical mechanics , paragraph 12 . c , 12 . d .
ds , as a segment of the wire carrying current , produces a magnetic field at " o " from all points that the wire is curved . when the wire is in the aa ' section or the cc ' section ds ( as an arbitrarily small section of the wire ) must now point at the origin and therefore it will not produce a magnetic field at the origin . in your question you probably mistakingly omited the $\hat{r}$ part which is why i asked for clarification .
what level are you at ? when you totally internal reflect light on a surface there is an electric field which extends a very small distance out of the surface - this is called the evanescent wave and excites states on the surface called surface plasmons . the interesting thing is that if you very slightly change the electrical characteristics of the surface you can make a big change in the internally reflected light . so a clever way of making extremely sensitive sensors is to bind some metal nanoparticles to the surface and coat them with some chemical that reacts with what you are looking for . the chemical you are sensing sticks to the nanoparticles , changes the electrical field and changes the reflected light = very sensitive chemical/biochemical detector .
$v_f - v_i = \int_i^f \vec{e} \cdot d \vec{r}$ . the dot product has a sign depending on the relative orientation between the electric field $\vec{e}$ and infinitesimal displacement $d \vec{r}$ . also note that as you move radially inwards from infinity to some point , the displacement $d \vec{r} = \vec{r_f} - \vec{r_i}$ points radially inward , whereas the electric field of a positive point charge points radially outward . like @darenw says in the comment , make sure you have accounted for that relative sign . that should take care of your sign mistake .
as others have said , in young 's double slit experiment , $d &lt ; &lt ; l$ . this means , that that $\mathbf{ep} \approx \mathbf{r_1} \approx \mathbf{r_2}$ . if you draw the picture in scale , you will see , that it is really the case . $\implies$the angle $p$ - $b$ - ''point at the right side wall at same height as $b$'' is equal to $\theta$ also , because it is just the same angle as $pec$ . ( and because $pe$ || $r_1$ ) $\implies dba = 90^\circ - \theta \implies bad = 90^\circ - ( 90^\circ - \theta ) = \theta \text{ }_\box$
it seems most experts think the abiotic theory is nonsense see http://www.theoildrum.com/story/2005/11/4/15537/8056 for example dr . jon clarke : . the fact remains that the abiotic theory of petroleum genesis has zero credibility for economically interesting accumulations . 99.9999% of the world 's liquid hydrocarbons are produced by maturation of organic matter derived from organisms . to deny this means you have to come up with good explanations for the following observations . the almost universal association of petroleum with sedimentary rocks . the close link between petroleum reservoirs and source rocks as shown by biomarkers ( the source rocks contain the same organic markers as the petroleum , essentially chemically fingerprinting the two ) . the consistent variation of biomarkers in petroleum in accordance with the history of life on earth ( biomarkers indicative of land plants are found only in devonian and younger rocks , that formed by marine plankton only in neoproterozoic and younger rocks , the oldest oils containing only biomarkers of bacteria ) . the close link between the biomarkers in source rock and depositional environment ( source rocks containing biomarkers of land plants are found only in terrestrial and shallow marine sediments , those indicating marine conditions only in marine sediments , those from hypersaline lakes containing only bacterial biomarkers ) . progressive destruction of oil when heated to over 100 degrees ( precluding formation and/or migration at high temperatures as implied by the abiogenic postulate ) . the generation of petroleum from kerogen on heating in the laboratory ( complete with biomarkers ) , as suggested by the biogenic theory . the strong enrichment in c12 of petroleum indicative of biological fractionation ( no inorganic process can cause anything like the fractionation of light carbon that is seen in petroleum ) . the location of petroleum reservoirs down the hydraulic gradient from the source rocks in many cases ( those which are not are in areas where there is clear evidence of post migration tectonism ) . the almost complete absence of significant petroleum occurrences in igneous and metamorphic rocks ( the rare exceptions discussed below ) . the evidence usually cited in favour of abiogenic petroleum can all be better explained by the biogenic hypothesis e.g. : rare traces of cooked pyrobitumens in igneous rocks ( better explained by reaction with organic rich country rocks , with which the pyrobitumens can usually be tied ) . rare traces of cooked pyrobitumens in metamorphic rocks ( better explained by metamorphism of residual hydrocarbons in the protolith ) . the very rare occurrence of small hydrocarbon accumulations in igneous or metamorphic rocks ( in every case these are adjacent to organic rich sedimentary rocks to which the hydrocarbons can be tied via biomarkers ) . the presence of undoubted mantle derived gases ( such as he and some co2 ) in some natural gas ( there is no reason why gas accumulations must be all from one source , given that some petroleum fields are of mixed provenance it is inevitable that some mantle gas contamination of biogenic hydrocarbons will occur under some circumstances ) . the presence of traces of hydrocarbons in deep wells in crystalline rock ( these can be formed by a range of processes , including metamorphic synthesis by the fischer-tropsch reaction , or from residual organic matter as in 10 ) . traces of hydrocarbon gases in magma volatiles ( in most cases magmas ascend through sedimentary succession , any organic matter present will be thermally cracked and some will be incorporated into the volatile phase , some fischer-tropsch synthesis can also occur ) . traces of hydrocarbon gases at mid ocean ridges ( such traces are not surprising given that the upper mantle has been contaminated with biogenic organic matter through several billion years of subduction , the answer to 14 may be applicable also ) . the geological evidence is utterly against the abiogenic postulate . also abiogenic origin of hydrocarbons : an historical overview by dr . geoffrey lasby abstract : the two theories of abiogenic formation of hydrocarbons , the russian-ukrainian theory of deep , abiotic petroleum origins and thomas gold 's deep gas theory , have been considered in some detail . whilst the russian-ukrainian theorywas portrayed as being scientifically rigorous in contrast to the biogenic theory which was thought to be littered with invalid assumptions , this applies only to the formation of the higher hydrocarbons from methane in the upper mantle . in most other aspects , in particular the influence of the oxidation state of the mantle on the abundance of methane , this rigour is lacking especially when judged against modern criteria as opposed to the level of understanding in the 1950s to 1980s when this theory was at its peak . thomas gold 's theory involves degassing of methane from the mantle and the formation of higher hydrocarbons from methane in the upper layers of the earth 's crust . however , formation of higher hydrocarbons in the upper layers of the earth 's crust occurs only as a result of fischer-tropsch-type reactions in the presence of hydrogen gas but is otherwise not possible on thermodynamic grounds . this theory is therefore invalid . both theories have been overtaken by the increasingly sophisticated understanding of the modes of formation of hydrocarbon deposits in nature .
for compactified minkowski space see , e.g. , conformal infinity . this is sometimes useful to prove mathematical statements about general relativity and its relatives . but to reach infinity or to get information from there takes infinitely long , given the finite speed of light . this is why we can never find out whether or not spacetime is compactified .
conservation of particle current is nothing but the statement that a theory has to be unitary . in other words the scattering matrix $s$ has to obey $ss^\dagger=1$ defining $s=1+it$ i.e. rewriting the scattering matrix as a trivial part plus interactions ( encoded in $t$ which corresponds to your $f$ ) one finds from the unitarity condition : $itt^\dagger=t-t^\dagger=2im ( t ) $ $tt^\dagger$ is nothing but the crosssection ( i suppressed some integral signs here for brevity ) the optical theorem is right there . hence one finds $\sigma\sim im ( t ) $
i do not know the problem specifically , but i can easily imagine how 1-y came up . just try to draw a prism using the three points ( 0,0 ) , ( 0,1 ) and ( 1,0 ) on the xy-plane . now try to figure out the linear equation that governs the line between ( 0,1 ) and ( 1,0 ) , and you will see that the line 's equation is $x=1-y$ . by doing a surface integral on surface that is limited by the points i mentioned , and the integral will be ( in the simplest case for calculating the area ) : $$\iint_s ds = \int_0^1dy\int_0^{1-y}dx$$ i hope this helps .
if the symmetry axis of the cone lies along the direction of travel , and if you are using the drag equation \begin{align} f_\mathrm{drag} = \frac{1}{2}\rho v^2a \end{align} to compute the drag force , then you should take $a$ to be the area of the base , namely the full area that would be obstructing your vision if you were looking at the object coming toward you . this assertion is confirmed by wikipedia when it defines the drag coefficient which uses the nice term " projected frontal area ; " the reference area depends on what type of drag coefficient is being measured . for automobiles and many other objects , the reference area is the projected frontal area of the vehicle . this may not necessarily be the cross sectional area of the vehicle , depending on where the cross section is taken . for example , for a sphere $a=\pi r^2$ .
the anti-particle for any particle is obtained by charge c and parity p conjugation . c is the operation that interchanges positive and negative charges and p is the operation that reflects in a mirror . the combined operation of cp must produce a particle of the same mass . this is a theorem of relativistic quantum field theory due to cpt symmetry . this other particle is either the same particle or an antiparticle with opposite charge and/or chirality . some particles such a photons , gluons , z bosons , pions , higgs , graviton etc , do not have anti-particles because they are invariant under the cp transformation . you can say that they are their own anti-particle . this can only happen for particles without electric charge and with no chirality . in principle the qcd color charge is also reversed for an anti-particle . this suggests that a gluon should not be regarded as its own anti-particle , but since colourless states are never seen the distinction cannot really be made in any operational sense . all known particles which are their own anti-particle are bosons , but it is also possible for a fermion to be its own anti-particle if it is a majorana spinor rather than a dirac spinor . no known fermions are of this type ( unless neutrinos are majorana ) but they exist in susy models . observed elementary particles that do have anti-particles include all the quarks and leptons ( except possibly the neutrinos ) and the charged w bosons . any composite particle also has an antiparticle made of the anti-particles of its constituents . this can only be its own anti-particle if all its constituents are ( e . g . a glueball ) , or if it is made of particle/anti-particle combinations as is the case for pions .
if you poured just one liquid its level would be equal on both sides , now when you pour the liquid on one side , the liquid already present in the u tube rises by lets say h1 height . lets assume the height of liquid column of the different liquid on other side is of height h2 . we can find out that h2-h1 would be equal to 5 cm ( difference in level of two liquids ) now to equate pressure on both sides you can use h1 × d1 × g = h2 × d2 × g here d1 and d2 are respective densities , remember the liquid having column of height h2 will be less dense as it stays above other liquid . on solving the equation you can find that h1 = 5 cm and h2 = 10 cm . the separation line would be at the bottom of the lighter liquid column ( the one of height h2 )
the zeroth law posits the existence of temperature by stating that if a is in equilibrium with b and a is in equilibrium with c , then b is in equilibrium with c . we can then assign an intensive property to a , b and c that we call " temperature " . they are in equilibrium == they have the same temperature . as soon as they are not in equilibrium , the zeroth law is silent . thus , as you observed in your question , we cannot derive an ordering of temperatures based on the zeroth law alone . here , the second law comes to the rescue . the formulation i am familiar with states the entropy of a closed system never decreases if we have two objects that are not in thermal equilibrium , then when we bring them into contact we expect heat to flow between them . now according to the second law , if we move heat $\delta q$ from $a$ to $b$ ( at temperatures $t_b$ and $t_b$ respectively ) , the change in entropy is $$\delta s = t_a \delta q - t_b \delta q\\ = \delta q ( t_a - t_b ) $$ now if the entropy of the system cannot decrease , then if $\delta q$ is positive we know that $t_a - t_b$ must be positive . this is where we find the ordering of temperature : heat travels from hotter to cooler until thermal equilibrium is reached . thus when we have two objects in unequal states we can tell which is hotter by looking at the direction in which heat flows between them . that direction is always from hotter to colder - and to prove this you need the second law . there is an amusing ( although somewhat dated - 50 years old this year ) song by the duo of flanders and swann that touches on this topic . see http://youtu.be/vnbivw_1fns
the probability current is just that - the rate and direction that probability flows past a point . it is analogous to electric current or to a fluid current , and the continuity equation is the same as for those concepts . for example , if the the probability current is high on the left-hand side of a region and low on the right hand side , more probability is flowing in from the left than out from the right , and the total probability for the particle to be found in that region is increasing . to calculate this , the probability that a particle is found in a region is $$\int_{region} \phi^* \phi \ , \ , \mathrm{d}x$$ the time derivative of this is the rate that the probability for the particle to be in that region changes . $$\int_{region} \left ( \frac{\partial \phi^*}{\partial t} \phi + \phi^*\frac{\partial \phi}{\partial t}\right ) \ , \ , \mathrm{d}x$$ we know what the time derivative of $\phi$ is , though , from the schrodinger equation . if you plug that in and assume the potential is real , this simplifies to $$\frac{i \hbar}{2m} \int_{region} \left ( ( \nabla^2\phi^* ) \phi - \phi^* ( \nabla^2 \phi ) \ , \ , \right ) \mathrm{d}x$$ if you integrate this by parts , you see it is the same as the integral of the flux of the probability current over the surface . thus the probability current is a flow of probability the same way the electric current is a flow of charge . the continuity equation is just the differential form of this same relation . since we had to use the schrodinger equation to find $\frac{\partial \phi}{\partial t}$ , we have shown that the continuity equation follows from schrodinger 's equation .
what exactly is the difference between internal resistance and resistance ? an ideal voltage source can provide unlimited current to an external circuit such that the source voltage is maintained . but , there are no ideal voltage sources , i.e. , all real voltage sources have some maximum current delivered into a short circuit . this is modelled by placing a resistor in series with an ideal voltage source and this resistance is the " internal resistance " or " source resistance " . clearly , the maximum current from such a source is : $i_{max} = v_{oc} / r_s$ where $v_{oc}$ is the " open circuit voltage " , i.e. , the voltage across the source when the source provides no current . this is related to the concepts of thevenin and norton equivalent circuits .
i actually solved the problem . the key idea is to use the fact that the metric depends on the internal manifold ( the flag ) only through the maurer-cartan forms and hence the scalar curvature cannot depend on the position in the internal manifold . one can then expand the elements of $su ( n ) $ near the origin . keep the metric exact in terms of the lambdas and to second order in the flag directions . one can then perform explicit calculations of the scalar curvature . in case someone needs this in the future , this is the result for the scalar curvature : $r=-4 ( p-1 ) \sum\limits_{i\neq j}\frac{1}{ ( \vec\lambda_i-\vec\lambda_j ) ^2}-3\sum\limits_{i\neq j\neq k}\frac{ ( \vec\lambda_i-\vec\lambda_j ) . ( \vec\lambda_i-\vec\lambda_k ) }{ ( \vec\lambda_i-\vec\lambda_j ) ^2 ( \vec\lambda_i-\vec\lambda_k ) ^2}$ , where $p$ is the number of commuting matrices and $\vec\lambda_i$ are the eigenvalues .
there are several different levels of advanced in quantum mechanics . i will try to answer using these levels of quantum mechanics : basic : single particle or single particles interacting with a single atom/nucleus , or classical field picture--- anything einstein would have been comfortable with . advanced : highly entangled many-body quantum mechanics , involving many-body effects that cannot be understood from single-body or single-field picture . inscrutable : quantum computing--- actual exponential computation as compared to classical behavior . i will give an off-the-cuff list of things that were predicted theoretically at each of the three levels , that was hard to understand from just seat-of-the-pants non-quantum intuition . at level 1 , there are essentially as many examples as you care to list : electron diffraction : the diffraction of electrons from crystals was one of the early predictions of quantum mechanics which was confirmed experimentally . knowing that electrons diffract is important for the construction of electron microscopes , and you need to know the relation between wavelength and momentum . lasers come from spontaneous emission theory , and einstein 's prediction that a coherent collection of bosons will make other bosons be created with the same momentum preferentially was very surprising . this is the basic idea behind lasers , and bec 's , and these were only found because the theoretical principles were known in advance . slow neutrons are dangerous : if you classically estimate the neutron scattering cross section off a nucleus , at low momenta , you are completely wrong , because of resonance effects . these effects can blow up the nucleus to look ( to the neutron ) as it it were the size of a barn , when it is normally the size of a kernel of corn . you can look up the unit " barn " for a more accurate etymology . qualitative chemistry : if you use simple quantum mechanical orbitals , and the notion of superposition ( which is called resonance in chemistry ) , you can get an idea of which molecules will make dyes , what shapes will be preferred , and so on . these types of things were worked out by linus pauling , and led to the discovery of the alpha-helix , and later , to the structure of dna . there are too many class-1 examples to list , so consider class 2 . here , one is looking for a theoretical insight in a many-body system , with a highly entangled wavefunction , which leads to practical predictions . the easiest example that comes to mind is bcs theory . bcs theory : this predicts that any very cold fermi system with the weakest of attractive interactions will produce a strange vacuum state , where it is like a bose einstein condensate of paired-fermions , even when the force is too weak to bind two individual fermions into actual pairs . the presence of other fermions in the sea is essential , it makes a condensate of particles which do not exist really . one of the most striking prediction of bcs theory was the prediction that he3 should become superfluid at ultra-cold temperatures . there is no reason you would suspect this from experiments on superconductors , without the detailed theory of cooper pairing . this was spectacularly confirmed by difficult experimental work of lee , osheroff , and richardson , work which was awarded the 1996 nobel prize . the theory of renormalization is quantum , class 2--- many body . but it is equally applicable to statistical systems , where simple models allow one to predict all sorts of phenomena that were not suspected experimentally . here is an example : anderson localization in 1d and 2d : any sufficiently long wire is insulating . any sufficiently large sheet of conductor is also insulating . you would never guess even the 1d business from experiment , but it is true , and needs to be considered when you make very thin wires . anderson localization itself is in class 1 , but the renormalization analysis which allows you to say things like this is class 2 . quantum field theory has made contact with experiment , most elegantly through 2d conformal field theory : rational 2d critical exponents : this was predicted from sophisticated quantum field theory considerations , relying on the conformal algebra from string theory , relying on the 2d conformal field theory of belavin , polyakov zamolodchikov . that in itself was an extension of 1960s work on operator product expansion , by zimmermann , wilson , kadanoff and polyakov , which defined the correct algebra for renormalized fields . the rational critical exponents are confirmed experimentally using systems as divers as polymers , 2d fluids , but also using exact solutions , and conputer simulations . you would have a hard time guessing an exponent is rational from an experiment . but by far the most spectacular type 2 quantum theoretical application is : semiconductor physics : the qualitative ideas of semiconductor physics , including the existence of " p-type " charge carriers , were understood theoretically in tandem with the experimental production of these materials . the theory of doping is not so sophisticated--- you need to know which are donors and which are acceptors , but the theory of p-type semiconductors relies crucially on many-body effects , so that you have particle hole symmetry . this is the central technological advance of the late 20th century , and made possible the computer revolution . in class 3 , there are several potential applications : simulating quantum systems : as feynman noted , a quantum computer will be able to simulate other quantum systems efficiently . this is impossible on a classical computer . factoring : given a quantum computer , peter shor showed how to factor numbers , which will make current cryptographic systems insecure . grover 's database search : this allows you to search a database with n items in $\sqrt{n}$ steps . guaranteed secure communications : you can make a channel in which you can ensure that you and your communication partner are not eavesdropped on . for the other applications in this class , i defer to neilson and chuang . the problem with class 3 applications ( at least the full blown computational ones ) is that we are not going to be 100% sure they will work until we build them . the other option is that quantum mechanics will fail for these .
colin 's comment is spot on , but to expand a bit on the " lots of details " he mentioned , heat radiated from the earth 's surface is partially absorbed by greenhouse gases in the troposphere , and because the troposphere is turbulent this heat gets redistributed throughout the troposphere instead of escaping into space . if you e.g. double the co$_2$ content of the troposphere it will intercept and redistribute more of the heat radiated from the earth , so the earth will overall radiate less heat into space . because the earth is now radiating less heat than it receives , it gets hotter . but as it gets hotter more heat is radiated from the surface and more escapes into space . eventually the temperature rises until the heat radiated once again matches the heat received , and the temperature stabilises .
they are the same , because integration is linear : $$\int _{t_1}^{t_2} \left ( f ( t ) - g ( t ) \right ) \ , dt = \int_{t_1}^{t_2}f ( t ) \ , dt - \int_{t_1}^{t_2}g ( t ) \ , dt$$ addendum : consider two paths . let the system trajectory in the first path be denoted $x_1 ( t ) $ and the trajectory for the second path be denoted $x_2 ( t ) $ . let the lagrangian be denoted $l$ . then the action for the first trajectory is $$s_1 = \int_{t_1}^{t_2}l ( x_1 ( t ) ) \ , dt$$ and the action for the second trajectory is $$s_2 = \int_{t_1}^{t_2}l ( x_2 ( t ) ) \ , dt . $$ the difference in the action between the two paths is \begin{eqnarray} \delta s \equiv s_1 - s_2 and = and \int_{t_1}^{t_2}l ( x_1 ( t ) ) \ , dt - \int_{t_1}^{t_2}l ( x_2 ( t ) ) \ , dt \\ and = and \int_{t_1}^{t_2} \left [ l ( x_1 ( t ) ) -l ( x_2 ( t ) ) \right ] \ , dt \\ and = and \int_{t_1}^{t_2}\delta l ( t ) \ , dt \end{eqnarray} in the last equation we wrote $\delta l$ to denote $l ( x_1 ) - l ( x_2 ) $ . this is not quite what we are going for : we want to get $\delta l$ inside the integral , where $\delta l$ is the variation of the lagrangian . the variation is the derivative of a function with respect to something . when you think about the definition of a derivative , you have something like $$\frac{df}{dx} ( x_0 ) \equiv \lim_{h\rightarrow 0}\frac{f ( x_0+h ) -f ( x_0 ) }{h} . $$ so really , taking the $\delta$ inside the integral requires the linearity property we already demonstrated and the ability to move the limit inside the integral . this is ok in almost every case you will ever encounter . to prove why you can move the limit inside the integral you have to do some analysis which i really do not want to recall and type here . you can find that sort of thing in analysis books .
the illustration does not show the underlined physical reality . a proton is made up of 3 quarks , namely $uud$ , but it is also constituted , as jinawee pointed out , of virtual quarks and antiquarks who are constantly being created and annihilated via strong force which is mediated by gluons , described by quantum chromodynamics ( qcd ) . the grey sphere in wikipedia 's site , shows the region where quarks make the proton , in other terms , if the wave-function shows the probability of finding a particle in a region of space , then this sphere shows the probability where you can find the essential quarks making up a proton .
you are seeing the wave-like nature of matter . the atoms are not completely isolated to a specific location and they exhibit wave properties . the rings around the atoms are the result of electron scattering off of the probability wave of the atom . the details of a scanning tunneling microscope ( stm ) may help . the wave effect can be reinforced via constructive interference to create standing waves . see the ring in the upper right : these are quantum coralls and they can create a " quantum mirage " like the apparent atom at the center . the effect can be quite dramatic : there is not an atom in the center but the wave-nature of matter is quite obvious .
1 ) gravitational field strength you have to consider the analogy with a uniform accelerated move , in special relativity , that is : $z ' = \frac{1}{a} ch ( a \tau' ) , t ' = \frac{1}{a} sh ( a \tau' ) $ here $a$ is the acceleration and $\tau'$ is the proper time . you get : $dz'^2 - dt'^2 = -d\tau'^2$ , as wished . now , make the coordinate change $\tau = a \tau'$ , we get : $z ' = \frac{1}{a} ch ( \tau ) , t ' = \frac{1}{a} sh ( \tau ) $ $dz'^2 - dt'^2 = - \frac{1}{a^2} d\tau^2$ . but this is the same result that the rindler metric with $d\rho=d \tilde x =d \tilde y = 0$ , so we make the identification between $\rho$: and $\frac{1}{a}$ , so the gravity strengh ( acceleration ) is inversely proprotionnal to the space-like quantity $\rho$ 2 ) horizon i am not sure to understand all your questions , but you have to be careful about several points : the minkowski metric of the form : $-dt^2 + dz^2 + d \tilde x^2 + d \tilde y^2$ , linked to the rindler metric , by the transformations $z= \rho \cosh{\tau} , t= \rho \sinh{\tau}$ , is only correct : near the horizon for a small angular region , that could be considered centered at $\theta = 0$ the horizon itself is not a 3 dimensional-surface in space-time , because the horizon corresponds to $\rho=0$ , that is $t=z=0$ . this is because the metrics $g_{00} = 0$ at the horizon , so the horizon has no extension in the time coordinates . so , considering that the horizon is $z^2 - t^2 = 0$ is not correct , the horizon is $t=z=0$ . a correct global representation is then using kruskal–szekeres coordinates , which gives a correct global point of view .
apparently that is possible . from http://en.wikipedia.org/wiki/formula_one_car: indycars , for example , produce downforce equal to their weight ( that is , a downforce:weight ratio of 1:1 ) at 190 km/h ( 118 mph ) , while an f1 car achieves the same at 125 to 130 km/h ( 78 to 81 mph ) , and at 190 km/h ( 118 mph ) the ratio is roughly 2:1 . from http://www.formula1.com/inside_f1/understanding_the_sport/5281.html: a modern formula one car is capable of developing 3.5 g lateral cornering force ( three and a half times its own weight ) thanks to aerodynamic downforce . that means that , theoretically , at high speeds they could drive upside down .
if you see it staying in one spot you can infer that it is moving directly along your line of sight , as you say . but in the more likely event that you see it move across the sky you can determine that it is moving in a given plane only . unless you have some independent way of judging its size or distance you can not tell any more . this actually happens all the time when a bug flies in front of someone 's camera and they think they have seen an interstellar visitor .
the best single source is at nist . the data is located at the chemistry webbook . once you are at that site click on formula or name under " general searches " . you can not only get all the data you want in a downloadable format , it will even graph it for you enjoy yourself . there is no better way to learn about data than to work with it .
redgrittybrick 's comment is correct . in order to make a roller turn , there has to be force whose vector does not go through the roller 's axis . it needs to avoid the axis in order to exert a turning moment . the only such forces are the frictional ones from the plank and the ground .
the stefan-boltzmann law governs the irradiance ( radiant power per unit area ) . the total energy is not sigma*t^4 , but rather the total power $p$ of a body with surface area $a$ and temperature $t$ is given by \begin{equation} p=a\sigma t^4 \end{equation} this result may be surprising , but it is correct . the reason irradiance rises so quickly is because the wavelengths of light decrease with increasing temperature , carrying away more energy with each photon ( on average ) . at the same time , higher temperatures cause the photon emission rate from the surface to increase . it is the increase in the energy of the photons coupled with the increase in photon production rate that gives the $t^4$ dependence .
firstly , definition of torque is $\vec{r}\times \vec{f}$ and angular momentum $\vec{r}\times \vec{p}$ . and now w.r.t. your frame $\vec{f}$ and $\vec{p}$ and $\vec{r}$ are all relative . but newton 's second law of rotation holds for all frames . . because all points are just frames and to maintain the distances in frame , you have to move with that frame , and as force and momentum both are relative to your frame , so will be torque and angular momentum , but the thing is they will all give the correct angular accelerations and angular velocities and linear velocities relative to them as newton 's laws can be made valid in all frames ( by applying pseudo force in some ) . and answer in your book must be given in absolute terms , you can find correct answer by then applying gallilean relativity to your frame .
the momentum eigenstate is not normalizable . suppose $x$ is a periodic variable with period $2\pi r$ -- the periodicity is important as momentum is then a `good ' operator . then one has the allowed values of momenta are quantized i.e. , $p_n = \frac{n \hbar}{ ( r ) }$ with $n\in \mathbb{z}$ . for the values of $p$ mentioned above , the momentum eigenstates are normalizable with $c=1/\sqrt{2\pi r}$ . let us call this normalized state $\langle x|n\rangle$ in the coordinate basis . explicitly , one has $$\langle x|n\rangle = \tfrac1{\sqrt{2\pi r}}\ e^{ip_n x/\hbar}\ . $$ ( you should treat the remark in the ucsd page about one particle to mean that your normalize your state to one . ) the eigenstates are orthogonal to each other as one can check explicitly . $$ \langle n | m\rangle =\delta_{n , m}\ . $$ the next step is to go from the box normalizable states to the delta function normalizable states . one needs to take $r\rightarrow\infty$ and convert kronecker delta into the dirac delta function . one uses the following identification which follows from the properties of the dirac delta function and standard integration : $$ \sum_m = \frac{r}{\hbar}\int dp \quad \mathrm{and}\quad \delta_{m , n} = \frac{\hbar}{r} \delta ( p_n-p_m ) \ . $$ with these identifications , we define $$ |p_n\rangle =\sqrt{\tfrac{r}{\hbar}}\ |n\rangle\ . $$ it is now easy to see that the dirac delta normalization implies that in the $x$-basis , one has $$u_p ( x ) :=\langle x|p_n\rangle = \tfrac1{\sqrt{2\pi \hbar}}\ e^{ip_n x/\hbar}\ . $$ this leads to a simple mnemonic : replace $r$ by $\hbar$ to go from box normalizable states to delta function normalization . of course , momentum which was discrete in a box now takes values in the continuum .
for questions about resonances and particles the particle data group is the best reference . one can find the whole delta resonance family and remind oneself what each number is standing for , and thus know how to pronounce the symbol . the number in parenthesis is the mass in mev . the superscript is the charge of the particular resonance displayed on the plot , presumably . s , p , d , . . . are by convention the labels of the angular momentum quantum number " l " , and the two numbers are the numerators of the isospin and j quantum number ( j is the total angular momentum quantum number ) . so the first one is read as : delta zero seventeen fifty ( pee three one ) or ( pee three halves one half ) . etc . ( the superscript of parity is missing in your information . ) the bar over a symbol denotes an antiparticle , antidelta ( 1910 ) zero in the second line . i would try and put the charge next to the main symbol , your first option but the other way is clear also . for similar questions the naming scheme for hadrons would be a help in comprehension as well as pronunciation .
okay , so i did some poking around and the 66th-75th editions of the crc handbook of chemistry and physics all have the incorrect atomic mass of cu-63 [ 62.939598 ] , and from 76th edition on they seem to have figured it out . those isotope mass tables are put together from a number of sources , so it is hard ( time consuming ) to tell exactly where the error came from . i did notice however that starting in the 76th edition of the crc , where they get it right , they start citing g . audi and a.h. wapstra , " the 1993 atomic mass evaluation " , nuc . phys . a 565 ( 1993 ) 1-65 . in editions 66-75 , they were citing audi and wapstra 's " the 1983 atomic mass table " which appeared in nuc . phys 432 , 1 ( 1985 ) . now , i looked at the 1993 version , and it has the correct 62.929 . . mass , but i have not been able to find wapstra and audi 's 1983 version of the same table , so i do not know if it was one error by the crc which got carried over year after year , or if 62.939 . . is in fact the value given in that paper . i did find at least one piece of evidence which points to an error by the editors of the crc in the 2nd edition of the encyclopedia of physics [ lerner and trigg , 1991 ] . their table of isotopes lists the correct 62.929 . . . value and also cites wapstra and audi 's 1983 paper . i hope that satisfies everyone 's curiosity , because i do not think i can do any better then that . ; - )
i would like to add to the answers already given . indeed , the atmosphere is transparent to shortwave radiation from the sun , but absorbs a lot of the longwave radiation from the earth ; that is why we have the greenhouse effect , that is why the earths has a liveable climate and that is part of the reason why we have the lapse rate we observe . but why is it colder at the tibetan plateau , which is a large , flat area at roughly 4 km elevation ? are not we equally close to the local surface there as when we are at sea level ? let 's assume the tibetan plateau receives the same intensity of solar radiation as lower areas at the same latitude . in reality , it probably receives more due to the dry climate . then it should heat up more , should not it ? but it does not . the system earth-atmosphere can be considered to be in a local radiative-convective equilibrium ( see the diagram from kevin trenberth below ) . this means that the energy flows " in " and " out " cancel out by energy transport due to radiation and convection . in other words : what goes in , must go out ( this is not really true locally , because there are large-scale flow patterns known as wind ) . now , the earth surface emits radiation according to its temperature with $p = \epsilon \sigma t^4$ . some of this radiation is aborbed by greenhouse gases ( or clouds ) in the atmosphere : water vapour , carbon dioxide , methane , and others . then the atmosphere heats up , and again radiates according to $p = \epsilon \sigma t^4$ ; part of this radiation goes into space , and part goes back to the surface . the greenhouse gases keep the surface of the earth warm like a blanket . now at the tibetan plateau , the atmosphere is much less dense , because the elevation is so high . therefore , radiation emitted by the surface is not absorbed much , but mostly exits straight into space . this means that the surface cools down . to return to the blanket analogy : tibet has a much thinner blanket than lower elevations do . now i have made a number of severe simplifications , because in reality it depensd on day/night , on clouds , on atmospheric flow such as wind , on humidity , and on other factors . but whereas the explanation given by others explains why it gets colder higher up in the free atmosphere , i think it does not really explain why it is colder at the tibetan plateau .
i can not really answer your question because calculating a comet 's orbit is not something i can describe in a few lines ( actually it is not something i can describe at all , but as always google is your friend ! ) . as martin says , finding your comet is easy in principle . you photograph the same bit of sky on ( at least ) three occasions a few days apart , then you look for anything that has moved i.e. that is in different positions in the three pictures . this sounds easy , but there is a lot of sky and only a few comets so it is incredibly painstaking work . professionals have automated systems for comparing images and looking for moving objects , but amateurs just have to get on with it . anyhow , once you have found your moving object you can use the position in the sky and the change in position to calculate the orbit . martin casually describes this as " a bit of algebra " but actually it is rather a lot of algebra . i found this book online if you really want the gory details . i imagine most amateur astronomers would just feed their measurements into some software rather than do all the sums themselves . i confess i know little about this area but a quick google found several calculators e.g. this one . incidentally , in a previous question you asked about how to tell a hyperbolic from a parabolic ( and elliptical ? ) orbit . there is no easy way to tell except by calculating the orbit to see what shape it is .
i will try to give an answer in purely classical thermodynamics . summary heat is a way of accounting for energy transfer between thermodynamic systems . whatever energy is not transferred as work is transferred as heat . if you observe a thermodynamic process and calculate that system a lost $q$ calories of heat , this means that if the environment around system a were replaced with $q$ grams of water at 14c and the process were repeated , the temperature of that water would rise to 15c . energy energy is a number associated with the state of a system . it can be calculated if you give the state variables - things like mass , temperature , chemical composition , pressure , and volume . ( these state variables are not all independent , so you only need to give some combination of them . ) sometimes the energy can be accounted very simply . for an ideal gas , the energy is simply proportional to the temperature , number of molecules , and number of dimensions . for a system with interesting chemistry , internal stresses and deformation , gravitational potential , etc . the energy may be more complicated . essentially , we get to invent the formulas for energy that are most useful to us . there is a nice overview of energy , summarizing richard feynman , here . for a more theoretical point of view on where these energy formulas come free , see lubos motl 's answer here . energy conservation as long as we make the right definitions of energy , it turns out that energy is conserved . suppose we have an isolated system . if it is not in equilibrium , its state may change . energy conservation means that at the end of the change , the new state will have the same energy . ( for this reason , energy is often treated as a constraint . for example , an isolated system will maximize its entropy subject to the constraint that energy is conserved . ) this leaves the question of what an isolated system is . if we take another system ( the environment ) and keep it around the isolated system , we find no observable changes in the environment as the state of the isolated system changes . for example , changes in an isolated system cannot change the temperature , pressure , or volume of the environment . practically , an isolated system should have no physical mechanisms for interacting with the rest of the universe . matter and radiation cannot leave or enter , and there can be no heat conduction ( i am jumping the gun on that last one , of course , but take " heat conduction " as a rough term for now ) . a perfectly isolated system is an idealization only . next we observe systems a and b interacting . before the interaction , a has 100 joules of energy . after interacting , a has 90 joules of energy , so it has lost 10 joules . energy conservation says that if we measure the energy in system b before and after the interaction , we will always find that system b has gained 10 joules of energy . in general , system b will always gain exactly however much system a loses , so the total amount is constant . there are nuances and caveats to energy conservation . see this question , for example . work work is defined by $$\textrm{d}w = p\textrm{d}v$$ $p$ is pressure ; $v$ is volume , and it is fairly easy to give operational definitions of both . using this equation , we must ensure that $p$ is the pressure the environment exerts on the system . for example , if we took a balloon into outer space , it would begin expanding . however , it would do no work because the pressure on the balloon is zero . however , if the balloon expands on earth , it does work given by the product of its volume change and the atmospheric pressure . that example treats the entire balloon as the system . instead , we might think of only the air inside the balloon as a system . its environment is the rubber of the balloon . then , as the balloon expands in outer space , the air inside does work against the pressure from the elastic balloon . i wrote more about work in this answer . adiabatic processes work and energy , as described so far , are independent ideas . it turns out that in certain circumstances , they are intimately related . for some systems , we find that the decrease in energy of the system is exactly the same as the work it does . for example , if we took that balloon in space and watched it expand , the air in the balloon would wind up losing energy as it expanded . we had know because we measure the temperature , pressure , and volume of the air before and after the expansion and calculate the energy change from a formula . meanwhile , the air would have done work on the balloon . we can calculate this work by measuring the pressure the balloon exerts on the air and multiplying by the volume change ( or integrating if the pressure is not constant ) . remarkably , we could find that these two numbers , the work and the energy change , always turned out to be exactly the same except for a minus sign . such a process is called adiabatic . in reality , adiabatic processes are approximations . they work best with systems that are almost isolated , but have a limited way of interacting with the environment , or else occur too quickly for interactions beside pressure-volume ones to be important . in our balloon , the expansion might fail to be adiabatic due to radiation or conduction between the balloon and the air . if the balloon were a perfect insulator and perfectly white , we had expect the process to be adiabatic . sound waves propagate essentially adiabatically , not because there are no mechanisms for one little mass of air to interact with nearby ones , but because those mechanisms ( diffusion , convection , etc . ) are too slow to operate on the time scale of the period of a sound wave ( about a thousandth of a second ) . this leads us to thinking of work in a new way . in adiabatic processes , work is the exchange of energy from one system to another . work is still calculated from $p\textrm{d}v$ , but once we calculate the work , we know the energy change . heat real processes are not adiabatic . some are close , but others are not close at all . for example , if i put a pot of water on the stove and turn on the burner , the water 's volume hardly changes at all , so the work done as the water heats is nearly zero , and what work is done by the water is positive , meaning the water should lose energy . the water actually gains a great deal of energy , though , which we can discover by observing the temperature change and using a formula for energy that involves temperature . energy got into the pot , but not by work . this means that work is not a sufficient concept for describing energy transfer . we invent a new , blanket term for energy transfer that is not done by work . that term is " heat " . heat is simply any energy transferred between two systems by means aside from work . the energy entering the boiling pot is entering by heat . this leads to the thermodynamic equation $$\textrm{d}e = -\textrm{d}w + \textrm{d}q$$ $e$ is energy , $w$ work , and $q$ heat . the minus sign is a convention . it says the if a system does work , it loses energy , but if it receives heat , it gains energy . interpreting heat i used to be very confused about heat because it felt like something of a deus ex machina to say , " all the leftover energy must be heat " . what does it mean to say something has " lost 30 calories through heat " ? how can you look at it and tell ? pressure , temperature , volume are all defined in terms of very definite , concrete things , and work is defined in terms of pressure and volume . heat seems too abstract by comparison . one way to get a handle on heat , as well as review everything so far , is to look at the experiments of james joule . joule put a paddle wheel in a tub of water , connected the wheel to a weight so that the weight would drive the wheel around , and let the weight fall . here 's the wikipedia picture of the set up : as the weight fell , it did work on the water ; at any given moment , there was some pressure on the paddles , and they were sweeping out a volume proportional to their area and speed . joule assumed that all the energy transferred to the water was transferred by work . the weights lost energy as they fell because their gravitational potential energy went down . assuming energy is conserved , joule could then find how much energy went into the water . he also measured the temperature of the water . this allowed him to find how the energy of water changes as its temperature changes . next suppose joule starting heating the water with a fire . this time the energy is transferred as heat , but if he raises the temperature of the water over exactly the same range as in the work experiment , then the heat transfer in this trial must be the same as the work done in the previous one . so we now have an idea of what heat does in terms of work . joule found that it takes 4.2 joules of work to raise the temperature of one gram of water from 14c to 15c . if you have more water than that , it takes more work proportionally . 4.2 joules is called one calorie . at last we can give a physical interpretation to heat . think of some generic thermodynamic process . imagine it happening in a piston so that we can easily track the pressure and volume . we measure the energy change and the work during the process . then we attribute any missing energy transfer to heat , and say " the system gave up 1000 joules ( or 239 calories ) of heat " . this means that if we took the piston and surrounded it with 239 grams of water at 14c , then did exactly the same process , the water temperature would rise to 15c . misconceptions what i discussed in this post is the first law of thermodynamics - energy conservation . students frequently get confused about what heat is because they mix up its definition with the role it plays in the second law of thermodynamics , which i did not discuss here . this section is intended to point out that some commonly-said things about heat are either loose use of language ( which is okay as long as everyone understands what is being said ) , or correct use of heat , but not directly a discussion of what heat is . things do not have a certain amount of heat sitting inside them . imagine a house with a front door and a back door . people can come and go through either door . if you are watching the house , you might say " the house lost 3 back-door people today " . of course , the people in the house are just people . the door only describes how they left . similarly , energy is just energy . " work " and " heat " describe what mechanism it used to leave or enter the system . ( note that energy itself is not a thing like people , only a number calculated from the state , so the analogy only stretches so far . ) we frequently say that energy is " lost to heat " . for example , if you hit the brakes on your car , all the kinetic energy seems to disappear . we notice that the brake pads , the rubber in the tires , and the road all get a little hotter , and we say " the kinetic energy of the car was turned into heat . " this is imprecise . it is a colloquialism for saying , " the kinetic energy of the car was transferred as heat into the brake pads , rubber , and road , where it now exists as thermal energy . " heat is not the same as temperature . temperature is what you measure with a thermometer . when heat is transferred into a system , its temperature will increase , but its temperature can also increase because you do work on it . the relationship between heat and temperature involves a new state variable , entropy , and is described by the second law of thermodynamics . statements such as " heat flows spontaneously from hot bodies to cold bodies " are describing this second law of thermodynamics , and are really statements about how to use heat along with certain state variables to decide whether or not a given process is spontaneous ; they are not directly statements about what heat is . heat is not " low quality energy " because it is not energy . such statements are , again , discussion of the second law of thermodynamics . reference this post is based on what i remember from the first couple of chapters in enrico fermi 's thermodynamics .
in classical mechanics , you can make up a complicated system with many different natural frequencies . in general , these frequencies are completely independent of each other . due to non-linearities in the coupling forces , it may happen that when two modes are vibrating simultaneously , you get a new frequency appearing in the spectrum as the sum or difference or the two primary modes . but in quantum mechanics , you never see the primary modes at all . . . you only see the sum or difference frequencies . furthermore , if you try to explain them by non-linear forces , you should also expect to see multiples of the fundamental frequencies . these are absent in , for example , the spectra of atoms . it is hard to explain by a classical model involving things like masses and springs . it manifests itself in qm , of course , because the " fundamental " frequencies , the natural modes , evolve in time without any oscillating charges associated with them . the oscillating charges only appear when you have the superposition of two fundamental modes . this is how quantum mechanics is very different from classical .
with a delta function potential , the particle is free on either side of the barrier : $$ \psi ( x ) =\begin{cases}\psi_l ( x ) =a_re^{ikx}+a_le^{-ikx} \\ \psi_r ( x ) =b_re^{ikx}+b_le^{-ikx}\end{cases} $$ where $a_i , \ , b_i$ are constants such that $a_r+a_l=b_r+b_l$ ( i.e. . , $\psi ( x ) $ satisfies the continuous function condition ) . but at the barrier we have the issue that $v ( 0 ) =\infty$ . so to resolve this issue , we use schroedinger 's equation and integrate it over some small region $\left [ -\epsilon , \ , \epsilon\right ] $ and then let $\epsilon\to0$: $$ -\frac{\hbar^2}{2m}\int_{-\epsilon}^\epsilon\psi''\ , dx+\int_{-\epsilon}^\epsilon v\psi\ , dx=e\int_{-\epsilon}^\epsilon\psi\ , dx $$ the first term is clearly $d\psi/dx$ evaluated at two points . the last term goes to zero in the limit $\epsilon\to0$ ( recall that $e$ is constant and finite , so that as $\epsilon\to0$ , the width goes to 0 and so does the whole value ) . for the potential term , the delta function has the great property that $$ \int\delta ( x-a ) f ( x ) \ , dx=f ( a ) $$ thus , that middle term becomes $\left . \psi ( x ) \right|_{-\epsilon}^\epsilon$ . we then combine these two to get $$ -\frac{\hbar^2}{2m}\left [ \psi'\left ( +\epsilon\right ) -\psi' ( -\epsilon ) \right ] +\left . \lambda\psi ( x ) \right|_{-\epsilon}^{\epsilon}=0 $$ as $\epsilon\to0$ , we can get the relation you are confused over : $$ \psi'_r ( 0 ) -\psi'_l ( 0 ) =+k\psi ( 0 ) $$
the fresnel equations describes the portion of the electromagnetic field that is reflected at a surface . for any indefinite and non-engineered material , in order to have refraction you must have an index greater than 1 , and thus you must have reflection at the surface . there are two significant exceptions to this . first , if you create a slab of material such that the reflected wave off of the second boundary is both perfectly out of phase with the wave reflected off of the first surface and of equal amplitude , the reflected waves will destructively interfere causing no reflection , but transmission will still occur . this will lead to refraction but no perceived reflection . secondly , if you were to engineer an isotropic metamaterial such that it had an index of -1 , the surface will have no index mismatch so there will be no reflected wave , but " refraction " will still occur . this is only a theoretical condition , as no isotropic negative index metamaterial has been fabricated . for a brief overview of metamaterials see here .
if you have a copy of griffiths , he has a nice discussion of this in the delta function potential section . in summary , if the energy is less than the potential at $-\infty$ and $+\infty$ , then it is a bound state , and the spectrum will be discrete : $$ \psi\left ( x , t\right ) = \sum_n c_n \psi_n\left ( x , t\right ) . $$ otherwise ( if the energy is greater than the potential at $-\infty$ or $+\infty$ ) , it is a scattering state , and the spectrum will be continuous : $$ \psi\left ( x , t\right ) = \int dk \ c\left ( k\right ) \psi_k\left ( x , t\right ) . $$ for a potential like the infinite square well or harmonic oscillator , the potential goes to $+\infty$ at $\pm \infty$ , so there are only bound states . for a free particle ( $v=0$ ) , the energy can never be less than the potential anywhere , so there are only scattering states . for the hydrogen atom , $v\left ( r\right ) = - a / r$ with $a &gt ; 0$ , so there are bound states for $e &lt ; 0$ and scattering states for $e&gt ; 0$ .
energy is associated with work not with force as work energy theorem states . in other words , force can be exerted without generating any work , in such a case whatever exerted the force does not lose or gain any energy because no work was done . in your example , the rubber slope exerted force ( friction force ) but no work was done ( the metal block did not move ) . so there is no exchange of energy although force is exerted . think of the force in this case as energy-free action . the energy of the slope has nothing to do with the gravitational potential energy of the metal block
i suggest to see the scenario physically first , and proceed from there . here , the gravitational force does not change the distance from the center , but it provides the centripetal force for the circular motion ( or any conic section for that matter , based on initial values ) . assuming it is a circular motion for now , we can just get right into the mathematical description of the motion . for such systems , masses move in a circle with their com at the center . because there is no other force in system , the com does not accelerate . it is free to move at a constant velocity though , but this is ( usually ) irrelevant when analyzing the system . because they move in a circle about the com , just get the distances from their com , and equate the gravitational force to the centripetal force . this way you have fully described the motion . example for $m_2:$ $$ \dfrac{gm_1}{\underbrace{r}_{\text{distance between bodies}}} = \dfrac{{v_2^2}}{\underbrace{r_{2_{cm}}}_{\text{distance of 2 from com}}} $$ similar for $m_1$ .
after much investigation , simulation and a deep literature search , i have figured out the true answer . you perceive a chirp because you are being hit with the echos of the sharp noise that generated the sound . the times between the arrival of those echos is decreasing inversely with time , so it sounds as if it were a tone with a fundamental frequency increasing linearly in time , hence the chirp . to get a feel for the phenomenon , consider a simulation : above you see a slowed down version of the simulated pressure wave inside a 2d racquetball court . i threw up the generated sound on soundcloud . if you watch the simulation , pick a particular point and watch the reflected sounds go by , you will notice the different instances of the multiple echos arrive faster and faster as time goes on . you can clearly hear the chirps in the generated sound , and if you listen closely you can hear secondary chirps as well . these are also visible in the spectrogram : this phenomenon was studied and published recently by kenji kiyohara , ken'ichi furuya , and yutaka kaneda : " sweeping echoes perceived in a regularly shaped reverberation room , " 　j . acoust . soc . am . vol . 111 , no . 2 , 925-930 ( 2002 ) . more info in particular , they explain not only the main sweep , but the appearance of the secondary sweeps using some number theory . worth reading in full . this suggests that for the best sweep one should both stand and listen in the center of the room , though they should be generic at any location . simple geometric argument following the paper , we can give a simple geometric argument . if you imagine standing in the middle of a standard racquetball court , which is twice as long as it is tall or wide , and clap , your clap will start propagating and reflecting off the walls . a simple way to study the arrival times is with the method of images , so you imagine other claps generated by reflecting your clap across the walls , and then reflections of those claps and so on . this will generate a whole set of " image " claps , located at positions $$ ( m , l , 2k ) l $$ where $m , l , k$ are integers and $l$ is 20 feet for a racquetball court , the time for any particular clap to reach you is $t = d/c$ and so we have $$ t = \sqrt{m^2 + l^2 + 4k^2} \frac{l}{c} $$ for our arrival times . if we look at how these distribute in time : it becomes clear why we perceive a chirp . the various sets of missing bars , which themselves are spaced like a chirp , give rise to our perceived subchirps . details of the 2d simulation for the simulation , i numerically solved the wave equation : $$ \frac{\partial^2 p}{dt^2} = c^2 \nabla^2 p $$ and used impedance boundary conditions on the walls $$ \nabla p \cdot \hat n = -c \eta \frac{\partial p}{\partial t} $$ i used a collocation method spatially , with a chebyshev basis of order 64 in the short axis and 128 on the long axis . and used rk4 for the time integration . i modeled the room as 20 feet by 40 feet and started it of with a gaussian pressure pulse in one corner of the room . i listened near the back wall towards the top corner . i put up an ipython notebook of my code , with the embedded audio and video . i recommend playing with it yourself . on my desktop it takes about minute to do a full simulation of the sound . effect of listening location i have updated the code to generate sound at multiple locations , and generate their sounds . i can not seem to embed audio on stackexchange , but if you click through to the ipython notebook view , you can listen to all of the generated sounds . but what i can do here is show the spectrograms : these are laid out in roughly their locations inside of the room . here the noise was generated in the lower left , but the chirps should be generic for any listening and generation location .
on point i . ) : johhnymo1 's comment touches the essential point , though the result he quoted holds assuming that the manifold is hausdorff . i emphasize this because the definition of paracompactness in the literature is not uniform - sometimes it is assumed that a paracompact topological space is hausdorff , sometimes not ( m . w . hirsch 's book " differential topology " , for instance , does not - neither does he assume that a manifold must be hausdorff , by the way ) . more precisely , the result is a direct consequence of the smirnov metrization theorem : a topological space is metrizable if and only if it is hausdorff , paracompact and locally metrizable ( i.e. . any point has an open neighborhood whose relative topology is metrizable ) . any manifold clearly satisfies the latter condition . ( edit : i have just got acquainted with the smirnov metrization theorem , which allows one to do away with the connectedness hypothesis . moreover , the counterexample i previously wrote is incorrect ) one should also add that paracompactness is equivalent to the existence of partitions of unity , which allow us to glue together locally defined objects in the manifold - for instance , this is how you prove existence of riemannian metrics . on point ii ) : if by " compact " you mean " compact without boundary " ( like $s^n$ ) , compact space-times indeed necessarily have vanishing euler characteristic - conversely , any compact manifold with vanishing euler characteristic admits a time oriented lorentzian metric . however , such space-times are not physically interesting because they necessarily have closed timelike curves . the argument is simple : since any space-time $ ( \mathscr{m} , g ) $ may be covered by the chronological futures of all its points ( which are open sets ) , using compactness one can pass to a finite subcover , say $\mathscr{m}=i^+ ( p_1 ) \cup\cdots\cup i^+ ( p_n ) $ . therefore , $p_1$ must belong to $i^+ ( p_{j_1} ) $ for some $j_1=1 , \ldots , n$ , $p_{j_1}$ must belong to $i^+ ( p_{j_2} ) $ for some $j_2=1 , \ldots , n$ , and so on . since we are dealing with a finite number of points , eventually one must have $p_{j_k}=p_1$ for some $k$ between $1$ and $n$ , thus producing a closed timelike curve . since such space-times are not globally hyperbolic , they are also unsuitable for the analysis of hyperbolic ( i.e. . wave-like ) pde 's . noncompact ( hausdorff , connected and paracompact , as in point ( i ) ) manifolds , on the other hand , always admit a time oriented lorentzian metric . a reference that discusses which topological hypotheses on space-time manifolds are natural is the classic book by s . w . hawking and g . f . r . ellis , " the large scale structure of space-time " ( cambridge university press , 1973 ) .
i take this question to mean : why does the laguerre-gaussian ( lg ) modes have an $e^{i\ell\phi}$ dependance on the azimuthal coordinate $\phi$ ? why is $\ell$ required to be an integer ? question 1 the lg modes are solutions to the paraxial wave equation in cylindrical coordinates . this means that we get solutions that reflect this symmetry . in particular the solutions should only trivially change if you make the change $$\phi\to\phi+\delta\phi . $$ if we define a rotation operator $r_{\delta\phi}$ such that this operator acting on any function $f ( \phi ) $ gives $$r_{\delta\phi}f ( \phi ) \equiv f ( \phi+\delta\phi ) $$ cylindrical symmetric solutions will be the eigenfunctions of $r_{\delta\phi}$ , i.e. $$r_{\delta\phi}f ( \phi ) =\lambda f ( \phi ) , $$ where $\lambda$ is a constant . the solution to this equation is of the form $$f_\ell ( \phi ) \sim e^{i\ell\phi} , $$ i.e. $$r_{\delta\phi}f_\ell ( \phi ) =e^{i\ell\delta\phi}e^{i\ell\phi}=\lambda_\ell e^{i\ell\phi} . $$ therefore cylindrically symmetric solutions such as the lg modes will be of the from $$lg ( r , \phi ) = f ( r ) e^{i\ell\phi} . $$ question 2 the reason $\ell$ has to be an integer ( i.e. . quantized ) is because $\phi$ is periodic . what this means is that $\phi$ and $\phi+2\pi$ are the exact same point , therefore all functions of $\phi$ must meet the requirement $$f ( \phi+2\pi ) =f ( \phi ) . $$ if our function is $e^{i\ell\phi}$ , as we saw in part 1 , then this means $$e^{i\ell ( \phi+2\pi ) }=e^{i\ell\phi}\to e^{i\ell 2\pi} =1 , $$ which is only true if $\ell$ is an integer .
i am not a professional , but i will try to answer anyway . meteor showers occur when the earth passes through the orbit of a comet ( or , in at least one case , an asteroid ) . over time , the debris spreads over the entire orbit of the comet . a shower can last for several days , which is an indication of how wide the debris stream is . assuming a duration of 1 day , and assuming the earth 's orbit is roughly at right angles to the debris stream , that gives a width of very roughly 2.5 million kilometers ( and a length of several hundred million kilometers ) . the earth is only about 12,735 kilometers in diameter . say the comet 's orbit is 1 billion kilometers long ( that is probably shorter than average ) . then multiplying the length of the orbit by the area of a circle 2.5 million kilometers across gives the volume of the stream , and multiplying 2.5 million kilometers by the area of a circle 12,735 kilometers in diameter gives the volume of the stream through which the earth passes ( the hole it punches in the stream ) . the ratio is about 15 million . other factors : earth 's gravity will pull in some debris that would not otherwise have hit it , making its effective diameter a bit bigger ( thanks to ghoppe 's comment ) . the density of the stream is not uniform . there are bound to be clumps of greater density . there is probably also a systematic change of density with distance from the sun . the width of the stream probably varies as well . i have no idea of the details the moon ( and its gravity well ) will also sweep up some debris -- but the moon 's effective area is a small fraction of earth 's . but the blatant errors in my assumptions undoubtedly swamp any such effects , and i am only looking for a rough estimate . so yes , the earth 's passage through a meteor stream will effectively punch a hole in it , but it is a very small hole relative to the size of the entire stream . it could have a significant effect over millions of years . i am making a lot of simplifying assumptions here , but the conclusion seems about right if i have gotten the result within one or two orders of magnitude . reference : http://www.amsmeteors.org/meteor-showers/meteor-faq/#5, plus some of my own extremely rough back-of-the-envelope calculations .
to be honest , i just learned about all this myself in the last months , so i am not sure whether this is actually correct . since you have this spherical symmetry , i think that you need spherical harmonics . they are orthogonal functions , think of them as a fourier series on the surface of a sphere . your charge density $\sigma$ does not depend on $\phi$ , therefore we can use the simpler legendre-polynomials , where $m = 0$ . fist , we want to express the potential like so : $$ \varphi ( \vec x ) = \frac1{\varepsilon_0} \sum_{l=0}^\infty \frac{1}{2l+1} \varrho_{l} \frac{1}{r^{l+1}} y_{l , 0} ( \theta , \phi ) $$ the coefficients $\varrho_l$ are given by : $$ \varrho_l = \int \mathrm d^3 x'\ , r'^l \varrho ( \vec x ) y^*_{l , 0} ( \theta ' , \phi' ) $$ now with $$ y^*_{l , 0} ( \theta ' , \phi' ) = \sqrt{\frac{2l+1}{4\pi}} p_l ( \cos \theta' ) $$ and $$ p_0 ( x ) = 1 , \quad p_1 ( x ) = x $$ we can calculate the coefficitents . but first , we need to convert the surface charge density $\sigma$ into a volume charge density . for that , we use the $\delta$-distribution : $$ \varrho ( \vec x ) = \delta ( r-r ) \sigma_0 \cos ( \theta ) $$ if you plug those into the $\varrho_l$ , you will get $\varrho_0 = 0$ and $\varrho_1 = \sqrt{\frac{3}{4 \pi}} \frac{4}{3} \pi r^3 \sigma_0$ . i hope this is correct . then we can put this into the first formula and get $\varphi$: $$ \varphi ( \vec x ) = \frac{1}{\varepsilon_0} \sqrt{\frac{3}{4 \pi}} \frac{4}{3} \pi r^3 \frac{1}{r^2} \sigma_0 \cos \theta $$ since this is a pure dipol potential , the $1/r^2$ seems about right . and if you look at the dimensions , the $r^3 \sigma_0 / r^2$ have just the needed charge/length . spherical harmonics might be overkill , maybe there is a simpler method to do this .
i assume you are asking how a black hole can evaporate due to hawking radiation . the answer is that the hawking radiation does not come from the event horizon , but instead comes from a region just outside the event horizon so time has not stopped at its position . if you were to watch a black hole form then evaporate , you would never see an event horizon form . that is because in your co-ordinates the event horizon would take an infinite time to form . you would see the infalling matter slow and red shift , then be re-emitted as hawking radiation without the event horizon ever having formed .
the roche limit applies when the astronomical body in question is held together by gravity rather than electromagnetic forces . this is the case for bodies with a diameter larger than around 500km . obviously for smaller bodies , like humans , we can get arbitrarily close to the surface , but i suspect this is not what you are asking about . for moons much smaller than the planet they are orbiting , and assuming the moon and the planet have roughly equal densities , the roche limit is about 2.44$r_p$ , where $r_p$ is the radius of the planet . the angle subtended by the planet from the moon is 2 arctan ( 1/2.44 ) or about 45º . so assuming you take the sky to cover 180º , at the roche limit the planet will cover a quarter of the sky ( by width , rather less by area ! ) . if the density of the moon is much greater than the planet the roche limit will be reduced and the planet can look bigger , and likewise if the density of the moon is lower the maximum size of the planet would smaller . however the roche limit varies as the cube root of the density ratio , so you need a big density difference to make much difference to the roche limit . response to comment : if you include the densities the expression of the roche limit is : $$ d = 2.44 r_p \left ( \frac{\rho_p}{\rho_m} \right ) ^{1/3} $$ where $\rho_p$ is the density of the planet and $\rho_m$ is the density of the moon . the average density of jupiter is 1.33kg/m$^3$ and the average density of the moon is 3.35kg/m$^3$ , and substititing these values gives the roche limit as 1.79$r_p$ . using the formula for the angle gives about 58º . you can use the formula to calculate what density ratio is required for the roche limit to fall to $r_p$ , i.e. for the moon to touch the planet 's surface . the required density ratio is about 15 . this could be attained for jupiter if the moon was made of pure osmium ( the densest element ) but this is , to say the least , unlikel ; y to occur in nature .
friction is always in the opposite direction of the movement of the object . if a ball rolls north on a floor , friction points south . for the ball to experience friction from the wall , it has to move along the wall . if the ball is thrown perpendicular to the wall it will not be moving along the wall , so it can not slip . however , if the ball is spinning , it will slip because the surface is moving along the wall .
first , let me restore the complex conjugation that was omitted without a good reason : $$ \rho_{\rm pure}=|\psi \rangle \langle\psi |= \big ( \begin{matrix} |\alpha|^2 and \alpha^* \beta \\ \alpha \beta^* and |\beta|^2 \end{matrix} \big ) $$ now , let us use a prettier ( inverse ) ordering of the tensor factors for the bra vectors . you meant : $$ \rho=|\psi \rangle \langle\psi | \rightarrow |\psi \rangle |r\rangle \langle r | \langle \psi| $$ $$ =|\alpha|^2 |0 \rangle |r_0\rangle \langle r_0 | \langle 0|+\alpha\beta^* |0 \rangle |r_0\rangle \langle r_1 | \langle 1|+\\ +\alpha^*\beta|1 \rangle |r_1\rangle \langle r_0 | \langle 0| +|\beta|^2 |1 \rangle |r_1\rangle \langle r_1 | \langle 1| $$ i think that the only extra step , key step of decoherence , you want to be shown is the partial trace of $\rho$ over the $r_0/r_1$ degree of freedom . we have $$\mathop{\rm tr}^{r_0 , r_1} \rho = |\alpha|^2 |0\rangle \langle 0| +|\beta|^2 |1\rangle \langle 1 | = \pmatrix{ |\alpha|^2 and 0\\ 0 and |\beta|^2} $$ that is it . note that the mixed terms did not contribute anything to the partial trace because $\langle r_0|r_1\rangle=0$ and similarly for its complex conjugate . the main result is that this reduced density matrix , a partial trace , after decoherence has vanishing off-diagonal entries , unlike the density matrix for the pure state $\rho_{\rm pure}$ that we started with . in more detailed calculations of decoherence , we usually take into account the fact that $|r_0\rangle$ and $|r_1\rangle$ that the environmental degrees of freedom evolve into just a moment later are not exactly orthogonal to one another . that means that the off-diagonal elements get reduced but they do not quite vanish instantly . however , the inner product is decreasing faster than exponentially as the same information is being imprinted and copied into additional degrees of freedom of the environment . by an exponentially growing avalanche , $\exp ( ct ) $ of qubits get modified according to the initial decohering bits . each of these environmental degrees of freedom or qubits contributes a factor of order $i\ll 1$ from the inner product to the off-diagonal entries so the off-diagonal entries go like $$ \rho_{12}\sim i^{\exp ( ct ) }=\exp ( -b\exp ( ct ) ) $$ where $b=\ln ( 1/i ) $ . to make the off-diagonal entries of the density matrix vanish or almost vanish after some time , the entanglement with the environmental degrees of freedom is essential . if the two subsystems were not entangled , in other words , if the total pure state were a tensor product $|a\rangle\otimes |r\rangle$ , the tracing over the $r$ degrees of freedom would give you the pure density matrix $|a\rangle\langle a|$ back : the system $r$ would have no effect on the system $a$ because of the lack of entanglement ( lack of correlation ) . you may read about decoherence e.g. at pages 9-16 here : http://www.karlin.mff.cuni.cz/~motl/entan-interpret.pdf
you are right , fourier transform spectrometer is just a scanning michelson interferometer . in spectroscopic applications these are just synonims . spectral information is a fourier transform of intensity dependence on path length , thus the name . usually a term wavemeter is used , meaning some interferometric device to measure wavelength , including fts as well as fabry–pérot interferometers .
this is analogous to the definition of an empty product in mathematics . for a finite non-empty set $s=\{s_1 , \ldots , s_n\}$ , the product over $s$ can be defined as $$\prod_{s\in s}s=s_1\times \cdots\times s_n . $$ for such a product you had want disjoint unions to map into products : if $r\cap s=\emptyset$ , then you want $\prod_{x\in r\cup s}x=\left ( \prod_{s\in s}s\right ) \times \left ( \prod_{r\in r}r\right ) $ , but for this to make sense you want to be able to handle the empty set , and the only way to make the rules consistent is to set $$\prod_{s\in\emptyset}s=1 . $$ this essentially says : if there is nothing to multiply , the result is one . ( similarly , empty sums are defined to be zero , for the same reason . ) in the case in hand , you could simply say if there are no units to multiply , then you get one . as luboš points out , this is the harmless choice as multiplying by one does not change the quantity . ultimately , though , this is a convention , and it is just something to get used to ; it is neither ' right ' nor ' wrong ' . i do agree , though , that it would probably be more consistent to say that dimensionless quantities like mach numbers should be said to have " dimension 1 . "
the charges are still the same . only the dielectric medium is varied . so , it is easier than you think . . . both the forces differ only by the relative permittivity $\epsilon_r$ . hence the relation , $$\frac{f}{f_m}=\frac{\epsilon}{\epsilon_0}=\epsilon_r$$
to perform such a calculation , we will use the flat minkowski space-time of special relativity . assuming that the traveller does not come sufficiently close to any massive body during the trip . now , in order to perform this calculation relativistically ( assuming you want to include the effects of a changing lorentz factor and associated acceleration ) we must first obtain/derive an expression for how the lorentz factor of a moving object transforms . this will allow us to derive the required relativistic expression for the objects proper-acceleration . so , let us consider two inertial frames s ( the ' stay-at-home observer ) and s ' ( the traveller ) in ' standard configuration ' ( that is , assuming that s ' is moving in the positive x-direction with speed $v$ ) . let $\mathbf{u} = ( u_{1} , u_{2} , u_{3} ) ^{\mathsf{t}}$ be the instantaneous vector of velocity in s of the traveller . we now wish to find the velocity and $\mathbf{u}' = ( u'_{1} , u'_{2} , u'_{3} ) ^{\mathsf{t}}$ of the traveler in frame s ' . we can define $$\mathbf{u} = ( \mathrm{d}x/\mathrm{d}t , \mathrm{d}y/\mathrm{d}t , \mathrm{d}z/\mathrm{d}t ) ) ^{\mathsf{t}} , $$ $$\mathbf{u}' = ( \mathrm{d}x'/\mathrm{d}t ' , \mathrm{d}y'/\mathrm{d}t ' , \mathrm{d}z'/\mathrm{d}t' ) ) ^{\mathsf{t}} . $$ from this definition and the fact that the two frames are in the ' standard configuration ' , we can immediately write the well know velocity transformation formulae ( without derivation ) : $$u'_{1} = \frac{u_{1} - v}{1 - u_{1}v/c^{2}} , \ ; u'_{2} = \frac{u_{2}}{\gamma ( 1 - u_{1}v/c^{2} ) } , \ ; u'_{3} = \frac{u_{3}}{\gamma ( 1 - u_{1}v/c^{2} ) } . $$ no assumptions about uniformity were made here and these formulae apply equally to the instantaneous velocity in a non-uniform motion . let us now write $u = ( u_{1}^{2} + u_{2}^{2} + u_{3}^{2} ) ^{\frac{1}{2}}$ and $u ' = ( {u'}_{1}^{2} + {u'}_{2}^{2} + {u'}_{3}^{2} ) ^{\frac{1}{2}}$ for the magnitudes of the corresponding velocities in s and s ' . now let us choose the signature of our metric tensor $g_{\mu \nu}$ of our minkowski space-time so that for our two inertial frames we can write $$c^{2}\mathrm{d}t'^{2} - \mathrm{d}x'^{2} - \mathrm{d}y'^{2} - \mathrm{d}z'^{2} = c^{2}\mathrm{d}t^{2} - \mathrm{d}x^{2} - \mathrm{d}y^{2} - \mathrm{d}z^{2} . \ ; ( \mathrm{a} ) $$ in our ' standard configuration ' the lorentz transformations for our coordinates are given by $$\mathrm{d}x ' = \gamma ( \mathrm{d}x - v\mathrm{d}t ) , \ ; \mathrm{d}y ' = \mathrm{d}y , \ ; \mathrm{d}z ' = \mathrm{d}z , \ ; \mathrm{d}t ' = \gamma ( \mathrm{d}t - v\mathrm{d}x/c^{2} ) . \ ; ( \mathrm{b} ) $$ now , factoring out $\mathrm{d}t'^{2}$ and $\mathrm{d}t^{2}$ form the lhs and rhs of ( a ) , respectivley and using ( b ) we can write $$\mathrm{d}t^{2} ( c^{2} - u^{2} ) = \mathrm{d}t'^{2} ( c^{2} - u'^{2} ) = \mathrm{d}t^{2}\gamma^{2} ( v ) ( 1 - u_{1}v/c^{2} ) ^{2} ( c^{2} - u'^{2} ) . \ ; ( \mathrm{c} ) $$ now , cancelling $\mathrm{d}t^{2}$ from the above we can now obtain the following transformation for $u^{2}$ , the squared magnitude of our traveller 's velocity : $$c^{2} - u'^{2} = \frac{c^{2} ( c^{2} - u^{2} ) ( c^{2} - v^{2} ) }{ ( c^{2} - u_{1}v ) ^{2}} . $$ note here $u_{1}v = \mathbf{u} . \mathbf{v}$ so that the rhs is actually symmetric in $\mathbf{u}$ and $\mathbf{v}$ - meaning that this holds for any two subliminal 3-velocities . now , rewriting the above interms of $\gamma ( u ) $ and $\gamma ( u' ) $ , with some work we get the following useful relations $$\frac{\gamma ( u' ) }{\gamma ( u ) } = \gamma ( v ) ( 1 - \frac{u_{1}v}{c^{2}} ) $$ this expression shows how the lorentz factor of a moving object transform ( for +ive $v$ ) . we can now use this to get an expression for our proper-acceleration . now using the rapidity function we can simplify the following derivation ( in a big way ! ) . the rapidity function $\phi ( u ) $ can be written as $$\phi ( u ) = \tanh^{-1} ( \frac{u}{c} ) , $$ which allows the velocity addition formula to be rewritten in the remarkably simple form $$\phi ( u ) = \phi ( v ) + \phi ( u' ) , $$ now differentiating with respect to ( wrt ) $t$ yields $$\frac{\mathrm{d}}{\mathrm{d}t}\phi ( u ) = \frac{\mathrm{d}}{\mathrm{d}t'}\phi ( u' ) \frac{\mathrm{d}t'}{\mathrm{d}t} . \ ; ( \mathrm{d} ) $$ which can be written as $$\frac{\mathrm{d}}{\mathrm{d}t}\phi ( u ) = \frac{1}{c}\gamma^{2} ( u ) \frac{\mathrm{d}u}{\mathrm{d}t} , $$ and from ( c ) above we can write $\mathrm{d}t'/\mathrm{d}t = \gamma ( u' ) /\gamma ( u ) $ . substituting this and the equation above ( and its primed version ) into the above expression for $\phi ( u ) $ we can write the desired acceleration transformation formula ( hoping i have made no mistakes ! : ] ) $$\gamma^{3} ( u' ) \frac{\mathrm{d}u'}{\mathrm{d}t'} = \gamma^{3} ( u ) \frac{\mathrm{d}u}{\mathrm{d}t} . $$ now if we define the proper acceleration $\alpha$ ( say ) , as that which is measured in our travellers rest-frame s ' , we find on setting $u ' = 0$ and $\mathrm{d}u'/\mathrm{d}t ' = \alpha$ , using our acceleration transformation equation we get $$\alpha = \gamma^{3} ( u ) \frac{\mathrm{d}u}{\mathrm{d}t} = \frac{\mathrm{d}}{\mathrm{d}t} [ \gamma ( u ) u ] . $$ this proper acceleration $\alpha$ is exactly the push we feel in an accelerating rocket . now finally , in our case of interest , that of rectilinear motion with constant proper acceleration $\alpha$ . we can integrate the above equation once , choosing $t = 0$ when $u = 0$ $$\alpha t = \gamma ( u ) u . \ ; ( \mathrm{e} ) $$ square this , solve for $u$ and integrate again with the same initial conditions give us the following equation of motion $$x^{2} - c^{2}t^{2} = c^{4}/\alpha^{2} . $$ hence why motion with constant proper accelleration is called hyperbolic ! we can now solve your question . it is likely that at 20g for half the distance we will be well beyond the speed of light . let us take the version of the derived relativistic equation of motion above for the frame s . now , setting the distance as 2.125 light years with an acceleration of 20g , we can work out the time taken to reach the halfway point ( using the relativistic equation of motion above ) , from the home observers reference frame which turns out to be 1531 days ( or 4.19 years ) . this motion will be symmetric so the time taken for the entire trip in frame s ( taking into account full relativistic motion ! ) will be 3062 days ( or 8.39 years ) . now for the time taken as measured in frame s ' . . . i will let you work that out ! it is not as simple as using a lorentz transform on the total time taken in this case ; as we have seen that the lorentz factor will change for and accelerating body . as for the maximum speed i will also leave this as an exercise - i have purposely missed out the step where we derive equation for $u$ . you can get $u$ from ( e ) , and work out the max speed accordingly . you will also notice that in the newtonian calculation , the time taken to get to the half-way point is 166 days . this is because the speed of light is reached in 17.69 days at a distance of 212 light minutes ; giving a speed at the half-way mark of a whopping 9.38c ! the relativist calculation reflects the limit of c in the calculation . i hope you enjoy reading this as much as i did going through it . i am off work ! i just hope it is right ! all the best .
in linear electrodynamics ( i.e. . low intensities ) , the dielectric constant and refractive index remain unchanged . a different thing is nonlinear optics ( applying to ed in general as well ) , for more theory see e.g. here . but this happens only for materials with strong non-linear parameters ( usually $\chi^{ ( 2 ) }$ or $\chi^{ ( 3 ) }$ ) and for rather high field intensities . this usually yields effect such as the second harmonic generation , but there is also the effect of self-focusing among others , which stems from the dependence of the refractive index on the field amplitude .
there is a list on wikipedia . radar guns use an optical doppler effect to measure speed . their acoustic equivalent is used in medicine , where it is called doppler ultrasound and used to measure blood flow or other sorts of motion in the body . animals that use echolocation can use the doppler shift to gain information about the motion of their surroundings . a sonic boom occurs when the doppler shift shifts a frequency to infinity . i guess one can continue to concoct scenarios . i wonder whether , when you drop a cat off a cliff , you can hear the pitch of its screaming drop as it accelerates . ( it is not all that cruel - cats can usually survive a fall at terminal velocity . ) you could use it to determine which way a whale is swimming if you have two boats , both listening to the same whalesong . you could even use the doppler shift to gain information about the position of the whale because both the whale 's position and its velocity contribute to the observed doppler shift at any given place . ( i do not have any information about this actually being done , but it might be an interesting problem to work out the locus of possible whale locations for an observed doppler shift . ) i was also curious about whether the doppler effect gives us information on the motion of the crust that moves during an earthquake . i found this reference which suggests it does .
nothing would really happen that may be of any significant consideration at least for normal sized passenger planes from private jets to boeing . lets see two major steps , i have considered landing and take off similar , and flight as separate . first lets consider take off/landing the blue lines are supposed to be magnetic field lines of earth , they are not normally so straight , but this would be a good approximation . lets assume a $30^o$ angle between the field and plane 's body , the earth 's magnetic field being $0.65g$ at maximum that is a mere $6.5 \times 10^{-5} t$ . while the largest wingspan of an aeroplane till now is $97.51m$ we will consider is $100m$ just for our ease . so even if an aeroplane had been a square of such a large side , its area would be $10^4m^2$ , and a decent $30^o$ angle with magnetic field would give you a feeble $0.56weber$ of flux . even if it reaches this much change of flux in just one second which it does not , then we would have a potential difference of about $0.5v$ across the wings , now with the resistance the current would just become too negligible and already these numbers are too large because we considered the plane a square and a constant angle of $30^o$ that the area vector had with the horizontal while its not like that and is much over $45^o$ thus concluding that at least while take-off and landing we do not need to care about any induced currents and voltages now lets consider the flight this is a very typical route for a flight , but still at this point you can rotate the plane in all directions and still get a near $90^o$ angle between the magnetic field and area vector in nearly all places where aeroplanes can land and/or take off ! same calculations with just a difference of angles when applied in this case give even smaller and much less noticeable currents . now you might think of antarctica where the lines are pretty much perpendicular , but even for that if you calculate for $90^o$ you will get negligible results only . now i am going to take an impossible scenario just to show that any real life flux changes are easily negligible . the minimum that a flux can be is $0weber$ and the maximum it can be even for the largest square sheet aeroplane is $0.65weber$ , even if any plane makes this amount of flux change in a mere second the potential difference developed would only be $0.65v$ this would give negligible currents and hence can be neglected over all . now to the case of thunderstorms , the maximum trouble that any aeroplane would receive is well known to arise from air turbulence , during thunderstorms these would create a danger far greater than any lightning bolts etc can produce , lets take a look . during a typical thunderstorm the potential difference between the clouds and earth is more than $1\times 10^8$ , the total resistance of the plane would be negligible as compared to that of the air , the major current that runs between the cloud and earth to establish a channel for the discharge is of the order of $100a$ , now lets calculate the energy density that will arise in the plane due to joules heating , the energy generated would be for the square sheet plane $u = i^2 \frac{\rho l}{a}$ and energy per unit area would be $$u = \frac{i^2 \rho l}{a^3}$$ for our square plane , where sides are equal it would result in $$u = \frac{i^2 \rho}{s^5}$$ lets assume an impossible current of $1000a$ for our square plane of $100m$ side , and check again$$u=\rho \times 10^{-4}$$ clearly negligible energy density for a small plane , like the drones we have nowadays or even any thing with wingspan less than $10m$ the energy density increases rapidly , but i do not think those planes are operational under such harsh weather conditions ! ps : i am not 100% sure about the thunderstorm analysis , but i am sure about the analysis for magnetic flux addendum : the " discharge wicks " you show in your picture are 100% for static discharge with as simple a reason as their other name is " static discharger " . i will not increase the length of the answer explaining their working etc but you can easily check it out here or simply do a google search of " discharge wick " .
for the academic source - someone directly involved with physics would just not be interested in this kind of text . but look for introductions for academic fields that are only remotely/lightly involved with nuclear physics , maybe medicine or biology . and : not the right place for this i think .
energy and momentum are both conserved . working in the centre of momentum frame the momenta of the incoming photons are equal and opposite so the total momentum is $p=p+ ( -p ) =0$ . the energies of the photons are also equal and equal to $pc$ . the total energy $e=2pc$ . now let the photons scatter into two photons of energy/momentum $e_1 , \ p_1$ and $e_2 , \ p_2$ respectively . since the total momentum is conserved we must have $p_1 + p_2 = 0$ , so $p_2 = - p_1$ . the momenta remain equal in magnitude . further , the energies are equal : $e_1 = |p_1| c$ and $e_2 = |p_2| c = |p_1| c = e_1$ . since the energies are equal and the total energy is still $e=2pc$ , we have that the energy of the final photons must be $pc$ . so nothing can change except for the direction of the outgoing photons . you can get the answer in any other frame by doing a lorentz boost of this result .
straight from the horse 's mouth : source : bureau international des poids et mesures ( search for " dimensionless " for all guidelines . ) the international bureau of weights and measures ( french : bureau international des poids et mesures ) , is an international standards organisation , one of three such organisations established to maintain the international system of units ( si ) under the terms of the metre convention ( convention du mètre ) . the organisation is usually referred to by its french initialism , bipm . wikipedia
synchrotron radiation can be coherent and incoherent . coherent sr arises when electrons are grouped into short bunches so that the entire bunch emits sr as a whole . quantum mechanically , in coherent sr the photon emission from different electrons in a bunch sum up at the amplitudes level and constructively interfere . in the incoherent sr they sum up at the level of intensity , and there is no interference . incoherent sr does not care how electrons are distributed along the ring , while the coherent sr is obviously boosted up in the presence of strong bunching . so , the more homogeneously you distribute the electrons , the less the effect of coherent sr will be and the less overall sr you will have . now let 's look at the incoherent sr . theoretically , you are right : if we managed to create the absolute homogeneous charge distribution along the ring , we would ( classically ) have no sr at all because charge distribution does not change in time . the point is that this is not feasible experimentally , at least for the accelerators and the beams we have . that would require putting electrons in a well-defined quantum state of the radial motion and a well-defined angular quantum number m for the azimuthal dependence , and the accelerator technology is very far from that . however , there is another thing which mimics that situation closely . people have managed recently to put freely propagating electrons in states with well-defined orbital angular momentum ( m as high as 75 ) , see this paper in science for details , and they really see the annular distribution for the electron density . for such a state there exists a reference frame where the electron does not move along the z axis but just rotated as a whole in the transverse plane ( with some radial distribution ) around the symmetry axis . this rotation is not driven by any force , it is just the peculiar superposition of plane waves that creates this steady pattern . so in this case you can say that the electron indeed circulates but does not emit any sr .
the waves are not necessarily sinusoidal , but any description of a function which is integrable ( i.e. . , has a finite area or " energy" ) can be decomposed into superpositions ( sums ) of sines and cosines , or alternatively ( and equivalently ) complex exponentials . this is why they are shown as sine or cosine waves , because that is the simplest object to think about . in reality , they are a ( possible infinite ) sum of the sine and/or cosine waves . also , you can build an antenna , and if you modulate it very carefully with a sine wave electrically , it will radiate a sine wave . . .
as far as i know , gravity is very very weak that your applied stress can easily overcome it . the shear force you apply through your knife does not just cut the onion or meat or whatever food , but it is also transferred to the board . if you ask someone to hold the board , while you cut those onions - he can easily experience this force . moreover , bamboo sticks have pores on them through which the formaldehyde adhesive can leak out ( what that link suggests can be true , because i have not observed it ) . . .
in the equation $$ c_v = at+ bt^3\ln t$$ the logarithmic term may be explained by paramagnons , i.e. fluctuations in which the adjacent atoms are demanded to be aligned . such fluctuations are long-lived . this explanation of the non-analytic term was found by doniach and engelsberg as well as berk and schrieffer . both articles are in prl 1966 . http://prl.aps.org/abstract/prl/v17/i14/p750_1 http://prl.aps.org/abstract/prl/v17/i8/p433_1 a full text of berk and schrieffer : http://books.google.cz/books?hl=enlr=id=yqu2bjfykrgcoi=fndpg=pa90dq=berk+schriefferots=vcx7dizdxgsig=hcfjbki-54txrpywjj9of7ovuqeredir_esc=y#v=onepageq=berk%20schriefferf=false brinkman and engelsberg discuss some limitations of the applicability of the log term in 1968: http://prola.aps.org/abstract/pr/v169/i2/p417_1 pethic and carneiro with their fermi liquid explanation came later . 1966 was before the renormalization group but it is not really needed for the calculations . the logarithmic corrections do arise from one-loop processes and similar corrections have been known in condensed matter physics and particle physics long before we knew about the right philosophical words linked to the renormalization group from the 1970s .
in physics " nothing " is generally taken to be the lowest energy state of a theory . we would not normally use the word " nothing " but instead describe the lowest energy state as the " vacuum " . i can not think of an intuitive way to describe the qm vacuum because all the obvious analogies have " something " instead of nothing " nothing " , so i will do my best but you may still find the idea hard to grasp . that is not just you - everybody finds it hard to grasp . start with the classical description of an electric field ( maxwell 's equations ) . it is not too hard to image an electric field as a field filling space . you can even feel the field : for example if you put your hand near an old style tv screen you can feel the static electricity . you can imagine turning down the electric field until it disappears completely , in which case you are left with the vacuum i.e. nothing . now imagine the same field , but this time we are using the quantum description of the field ( quantum electrodynamics instead of maxell 's equations ) . at the classical level the field is approximately the same as the description maxwell 's equations give , but now we have fluctuations in the field due to the energy-time uncertainty principle . just as before , imagine turning down the electric field until it disappears . unlike the classical description , the ( average ) electric field may disappear but the fluctuations do not . this means the quantum vacuum is different from the classical vacuum because it contains the fluctuations even after you have turned the field down to zero . the key point is that when i say " turn the field down " i mean reduce the energy to the lowest it will go i.e. you can not make the energy of the electric field any lower . by definition this is what we call the " vacuum " even though it is not empty ( i.e. . it contains the fluctuations ) . it is not possible to make the vacuum any emptier because the fluctuations are always present and you can not remove them .
yes , he did . there is a press release at ucsb that acknowledges it as his second win : http://www.ia.ucsb.edu/pa/display.aspx?pkey=3161 the reason for this is , presumably , that the committee is considering giving him the bigger "2014 fundamental physics prize . " in essence , the physics frontiers prize is a nomination for that . ( this is based on the description on the press release , and past reading . )
( add my comment as an answer ) a short answer would be that although electric and thermal conductivity have ( movement of free ) electrons as their primary carriers , they operate on different ranges/frequencies/wavelengths , and as such the structure and energy zones of the material ( or material compound ) can have quite different factors for each type of conductunce . from wikipedia article on thermal conductivity : in metals , thermal conductivity approximately tracks electrical conductivity according to the wiedemann–franz law , as freely moving valence electrons transfer not only electric current but also heat energy . however , the general correlation between electrical and thermal conductance does not hold for other materials , due to the increased importance of phonon carriers for heat in non-metals . highly electrically conductive silver is less thermally conductive than diamond , which is an electrical insulator , but due to its orderly array of atoms it is conductive of heat via phonons
i see the problem with your equation now . when differentiating $\vec r=r\cos\theta\vec i+y\vec j$ , you have considered $r$ to be constant , which is wrong . $r$ is given by $$r=\sqrt{l^2+y^2}$$ where $l$ is the side-length of the square . so $r$ will change with $y$ , and you will have to differentiate $r$ too . this is where the math gets pretty ugly and dissuading ! to avoid that , what we can do is we can observe that in the rotation frame , the bead will experience an outward centrifugal force . this force will have a component along $bc$ . that component can be written as ( i will be borrowing your variables ) $$f_{bc}=m\omega^2r\sin\theta$$ $$f_{bc}=m\omega^2 \sqrt{l^2+y^2} \frac{y}{\sqrt{l^2+y^2}}$$ thus by dividing by $m$ on both sides you get $$\ddot y=\omega^2y$$ note that this is the same as applying $\frac{d^2\vec r}{dt^2}= ( \frac{d^2\vec r}{dt^2} ) '+2\vec\omega\times ( \frac{d\vec r}{dt} ) '+\vec\omega\times ( \vec\omega\times\vec r ) $ . it is just that this approach is more problem specific ( and a lot easier too ! ) .
after some search , i found a video on youtube which was about the rainbow . it says that the set of all points which have a fixed angle between the sunlight , the raindrop and the observer creates the circular arc of the rainbow . this picture helped me . for the second question about the reason that we just see a special color at each angle , i watched lewin 's lecture at delft university of technology . he mentioned the reason and said at the maximum values of $\varphi$ , there is a peak in the intensity of that special light . because of that , although all colors exist at e.g. $40.7^\circ$ but just one color is seen because the peak intensity of blue occurs at $40.7^\circ$ .
i do not believe the coriolis force has much effect on a tsunami because it does only affect moving masses . coriolis force in fact is not a force but a movement pattern looking as though a force were involved . it is a result of inertia " driving " the moving masses towards a constant direction in space and at the same time the earth 's rotation taking place . however , while a tsunami travels across the globe there is little water moving , instead what actually is moving is its energy . by contrast , in hurricanes there is actually a huge amount of air moving which is affected by coriolis force . update : raskolnikov supplied us with sources that suggest coriolis force has an effect which i do not question . however , i think it is negligible , although the wave recordings on the images presented to us show a curved trajectory . deepak vaid suggests this is due to ocean currents which i find not very convincing as they move at negligible speeds compared to that one the tsunami moved at . i think the curvature of the trajectory we see on the images is an illusion due to the map projection just as in the image below showing the geodesic ( straight line ) between japan and chile : this is the aforementioned image mapped onto a sphere . http://www.youtube.com/watch?v=yodfmhn4alq the noaa map in gnomonic projection
1 . x-ray diffraction takes pictures of fourier space as briefly described on pg . 34 of " introduction to solid state physics " 7th edition by kittel , the scattering amplitude for an arbitrarily-shaped object is $$f ( \mathbf{k}_i , \mathbf{k}_o ) =\widehat{n} ( \mathbf{k}_i-\mathbf{k}_o ) $$ where $\mathbf{k}_i$ is the incident wavevector , $\mathbf{k}_o$ is the outgoing ( or scattered ) wavevector , and $\widehat{n}$ is the fourier transform of the material 's scattering density $n$ . since it is often electrons which scatter light , $n$ is often assumed to be the electron density as a function of location in the material . this is a surprisingly simple equation ; to illustrate it is visual meaning , imagine an object is placed inside a hollow sphere whose walls are lined with photographic paper , and through a small hole the object is bombarded with x-rays of fixed wavevector $\mathbf{k}_i$ , which are scattered by the object and strike the interior of the imaging sphere , forming an image . what is this image ? imagine a bubble of radius $|\mathbf{k}_i|$ centered at $\mathbf{k}_i$ in fourier space . the image formed on the photographic paper is the image of $\widehat{n}$ on the surface of that bubble , a fact which is simple to deduce by examining the set of points of the form $\mathbf{k}_i-\mathbf{k}_o$ for a fixed value of $\mathbf{k}_i$ , and noting that $|\mathbf{k}_i|=|\mathbf{k}_o|$ . that is what x-ray diffraction examines ; it literally takes a bubble-shaped slice of an object in fourier space . this bubble is sometimes call the " ewald bubble " , and it is what diffraction takes a picture of . 2 . fourier space of a crystal now , let 's insert the fact that we are looking at a crystal . a very simple approximation of a crystal is a 3d dirac comb ( ie , a dirac delta placed at each unit cell ) , whereupon $$n ( \mathbf{r} ) \approx \mbox{diraccomb} ( \mathbf{ar} ) $$ where $\mathbf{a}$ is the inverse of the $3\times3$ matrix whose columns are the 3 lattice basis vectors for the crystal . one then fourier transforms to obtain $$\widehat{n} ( \mathbf{k} ) \approx\mbox{det} ( a ) ^{-1}\mbox{diraccomb}\left ( \frac{\mathbf{a}^{-1}\mathbf{k}}{2\pi}\right ) $$ which essentially tells you that the fourier transform of the lattice $n$ is also another lattice . this lattice , residing in fourier space , is often referred to as the " reciprocal lattice " of a crystal . 3 . crystal diffraction now let 's combine the previous two results . if a reciprocal lattice point of $\widehat{n}$ happens to reside on the surface of the ewald bubble , then light will be diffracted in that direction in real space . but how do we image the rest of reciprocal space , not just the points that happen to lie on that one bubble-shaped slice ? suppose we rotate the crystal through euler angles $\alpha , \beta , \gamma$ . then $n ( \mathbf{r} ) $ will become $n ( \mathbf{r} ( \alpha , \beta , \gamma ) \cdot\mathbf{r} ) $ where $\mathbf{r} ( \alpha , \beta , \gamma ) $ is the rotation matrix for the crystal rotation . since $$\mathbf{r} ( \alpha , \beta , \gamma ) ^{-1}=\mathbf{r} ( -\gamma , -\beta , -\alpha ) , $$ we see that $\widehat{n} ( \mathbf{r} ) $ becomes $\widehat{n} ( \mathbf{r} ( -\gamma , -\beta , -\alpha ) \cdot\mathbf{r} ) $ ( rotations are unitary so the determinant is 1 ) , and hence rotation of a crystal in real space correspond to a reversed rotation of the crystal 's reciprocal space . thus , by rotating the crystal and imaging it after each rotation , we can sweep out a spherical region of radius $2|\mathbf{k}|$ in reciprocal space . visually , we are rotating the ewald bubble , whose surface is attached to the origin , to sweep out a spherical region whose radius is the diameter of the imaging bubble ( which is $2|\mathbf{k}|$ ) . 4 . powder diffraction with those preliminaries out of the way , powder diffraction is quite simple . a powder is a large number of very small crystals oriented in random directions . if there are a large number of crystals all oriented randomly ( isotropic fine powder ) then we can average over all orientations to get $$\widehat{n}_{avg} ( \mathbf{k} ) \propto\int_0^{2\pi}d\alpha\int_0^{\pi}d\beta\mbox{sin} ( \beta ) \int_0^{2\pi}d\gamma\widehat{n} ( \mathbf{r} ( -\gamma , -\beta , -\alpha ) \cdot\mathbf{k} ) $$ but you do not need to worry about integrating it , because it is visually obvious that each delta function located at a reciprocal lattice point $\mathbf{g}$ will be " rotationally smeared " out to form a series of concentric bubbles of radius $|\mathbf{g}|$ centered at the origin . where these concentric bubbles intersect the ewald bubble , diffraction will occur in that direction . and it is a simple fact of geometry that when one bubble intersects another , the intersection region is a circular ring common to both . as a result , there will be ring-shaped regions on the ewald bubble where constructive interference can occur , and thus , the sample will emit cone-shaped beams of light . and that is how the cone-shaped beams of light in the picture above come to fruition .
choose $\alpha$ as the generalized coordinate , so $y_\text{com}=\frac{1}{2}l \sin\alpha$ , and $x_b=l\cos\alpha$ . then $\delta y_\text{com}=\frac{1}{2}l \cos\alpha \ , \delta\alpha$ , and $\delta x_b=-l\sin\alpha \ , \delta\alpha$ . substitute into the equation .
" feeling hotter " is a matter of heat transfer from an object to your hand . the crust of your pie does not transfer heat very well , whereas the somewhat liquidy innards of the pie are very good at transferring heat . it is the same reason that aluminum foil coming out of a hot oven will not burn you .
well , if i read the problem correctly then your kinetic energy is wrong . your $\dot{x}_m$ is the $x$-component of the velocity of the ball , but you are missing the $y$-component and also the velocity of the cart , not to mention you are multiplying $\dot{x}_m^2$ by $m$ when you should be multiplying it by $m$ . the $\cos^2\phi$ goes away because the $y$-component of the ball 's velocity has $\sin \phi$ in it , and as we all know $\cos^2 \phi + \sin ^2 \phi = 1$ .
" you will agree that if you shoot in an open field , the bullet will stay at the same altitude for several seconds no , we would not . in fact we do a demo ( often called ' shoot the monkey' ) in introductory classes that shows unambiguously that it falls very much as if you had held it at arms length and dropped it . as noted in the comments the mythbusters guys actually ran the experiment with a bullet .
the total voltage difference across the resistors ( $v_3$ ) is , by design , a constant 5.4 volts 1 ( because it is being supplied by a power supply that pretty well approximates a constant voltage source ) . this total voltage difference must be dropped across the two resistors $r_1$ and $r_2$ in series : that is , $$v_1 + v_2 = v_3 . $$ as the resistors are in series , and there are no other paths the current might take , 2 the same current must flow across both resistors : $$i_1 = i_2 . $$ by ohm 's law , the current across a resistor equals the voltage divided by the resistance : $$i = \frac{v}{r} . $$ combining these equations , we see that $$\frac{v_1}{r_1} = i_1 = i_2 = \frac{v_2}{r_2} , $$ which we can rearrange to get $$\frac{v_1}{v_2} = \frac{r_1}{r_2} . $$ that is to say , the ratio $v_1 / v_2$ of the voltage drops across the resistors is equal to the ratio $r_1 / r_2$ of their resistance . since the total voltage $v_1 + v_2$ dropped across the resistors is fixed , this means that , when $v_1$ goes up , $v_2$ must go down , and vice versa . in particular , this means that , when you keep $r_1$ fixed and decrease $r_2$ , you are increasing the ratio $r_1 / r_2$ , and thus $v_1 / v_2$ . since the sum of $v_1$ and $v_2$ is constant , increasing the ratio means that $v_1$ must increase and $v_2$ must decrease by the same amount . for example , if , as in your first experiment , $r_1$ equals $r_2$ , then $v_1$ also equals $v_2$ , and thus both must be half of the total voltage drop $v_3 = v_1 + v_2$ . 3 similarly , if $r_1$ is twice $r_2$ , as in your second experiment , then $v_1$ will also have to be twice $v_2$ . thus , $v_1$ will be two thirds of the total voltage drop $v_3$ , while $v_2$ will be one third of $v_3$ . note that the individual values of $r_1$ and $r_2$ do not matter , only their ratio does : you had get the same results with $r_1 = 100\ ; \omega$ and $r_2 = 50\ ; \omega$ as with $r_1 = 10\ ; \omega$ and $r_2 = 5\ ; \omega$ . of course , the current passing through the resistors ( and the heat dissipated by them ) would be ten times greater for the second case as for the first , but this would not affect the voltages in any way . 4 1 ) you say in the text that it is supposed to be 6 volts , but the measurements say 5.4 , so i am going with that . see note 3 below . 2 ) the voltage sensors do leak some tiny bit of current , but they are designed to make this leakage current as small as possible , so we can normally safely neglect it . 3 ) the difference between the calculated values and your experimental results is presumably due to experimental inaccuracies , e.g. in the readings of your voltmeters or in the actual resistance values of your resistors . 4 ) that is , it would not affect the voltages as long as the experimental assumptions remained valid . if you reduced the resistances too far , either the power supply voltage would start to drop , or the increased current would cause your components to overheat . conversely , if you increased the resistor values too far , at some point the minuscule amounts of current leaking though the voltage meters would start to affect the results measurably .
following this reference formulae ( 8 ) , ( 9 ) , ( 10 ) , ( document ) page 295 , your twist 1-from is zero . this can be done by looking at the square of $\omega_\mu v^\mu$ for an arbitrary vector $v$ . this brings different contractions for the levi-civita tensors , and they are all zero , due to the properties of the null killing field . reference : null-killing vector dimensional reduction and galilean geometrodynamics b . julia h . nicolai nuclear physics b
an addendum to the answers of daniel grumiller and sb1: the major difference of the gravitational field and other fields is that according to general relativity the gravitational field defines space and time and therefore defines the relation of events . it is true that it is possible to do an " arbitrary " split of a certain linear approximation of the gravitational field into a " flat background " and " waves " propagating on this background . in principle this kind of reasoning is a violation of the very idea that the gravitational field defines the background of spacetime in a holistic way , and it was the subject of a lot of discussions if this approximation is of any use . this is considered to be settled by the observational evidence that bistar systems loose energy in exact the way that the " graviational wave approximation " predicts , as cited by daniel grumiller . the existence of gravitons is a conjecture based on the assumption that gravitational waves exhibit the same quantum nature as classical waves , e.g. waves in classical electromagnetism . at the basis of this conjecture is the idea that it should be possible to split the gravitational field in a part defining the background , and then having gravitational waves propagating on this background and exhibiting the same wave-particle duality as other waves . it would then be possible to treat quantum gravitational effects in a semi-classical approximation . since there is no observational evidence for this , this conjecture is still the subject of controversy . are these different communities of physicists ? some have a strong believe in the existence of gravitons , some think that quantum gravity needs a bigger conceptual step than only gravitions , and some do both , so , yes , there are different communities believing different things . does relativity explain only part of the story of masses acting under gravity ? it explains everything in a classical setting with not too strong forces alright , but does not exlpain quantum effects or what happens when forces get so strong that singularities occur . is gravity a force or not ? is it only an apparent force or not ? it is a force , it is an apparent force in the sense that classical gr says that you feel it because you live in an accelerating reference frame . both statements are valid in the classical setting and are independent of the quantum nature of gravity , and in particular of the existence of gravitons . can such an apparent force ' generate ' exchange particles ? are the exchange particle and geometric models both different views of the same underlying truth ? yes , see above ( geometric model = classical setting , exchange particle = semi-classical approximation ) . why can not the other forces be explained away similarly ? or is that what is happening with all this talk of small extra dimensions ? the gravitational field is fundemantally different from other fields ( see above ) , and this has no connection to extra dimensions . i would appreciate any illumination on this matter , or suggested reading ( preferably at the ' popular science ' or undergraduate level ) . the problem is that if you are able to ask this question , you are already beyond the popular science level . i would really like to recommend to you an introducory class on qft and one on gr , there you had get the best answer to your question :- )
space_cadet mentioned already work about deriving spacetime as a smooth lorentzian manifold from more " fundamental " concepts , there are a lot of others -like causal sets , but the motivation for the question was : the reason for my interest in this regards one of the mysteries of quantum mechanics , that of quantum entanglement and action at distance . i wondered whether , if space is imagined as having a topology that arises from a notion of neighbourhood at a fine level , then quantum entanglement might be a result of a ' short circuit ' in the connection lattice . i am not convinced that such an explanation is possible or warranted , the reason for this is the reeh-schlieder theorem from quantum field theory ( i write " not convinced " because there is some subjectivity allowed , because the following paragraph describes an aspect of axiomatic quantum field theory which may become obsolete in the future with the development of a more complete theory ) : it describes " action at a distance " in a mathematically precise way . according to the reeh-schlieder theorem there are correlations in the vacuum state between measurements at an arbitrary distance . the point is : the proof of the reeh-schlieder theorem is independent of any axiom describing causality , showing that quantum entanglement effects do not violate einstein causality , and do not depend on the precise notion of causality . therefore a change in spacetime topology in order to explain quantum entanglement effects will not work . discussions of the notion of quantum entanglement often conflate the notion of entanglement as " an action at a distance " and einstein causality - these are two different things , and the first does not violate the second .
whether entropy was zero at the big bang or not is very much an open question of physics , in big part due to the fact that we do not yet have a good enough understanding of physics at high energies and high gravitational fields . but for the zero entropy state this is a bit easier to answer and the answer does depend on laws of physics . zero entropy state basically depends on how many completely distinguishable states the laws of physics allow . the universe is in a zero entropy state precisely when it is in a single state and it can be known which state it is in . in many situations there are infinitely many different zero entropy states . so the zero entropy state at the beginning of the universe is unique if and only if the laws of physics at that time require that there is a single state in which the universe can be found . whether they do require that or not is a very big question in physics which everybody would like to know the answer to .
well for the human body , to calculate the impact force is not as simple as you think , because we are soft objects and have a large cross section area ( of course depends on which part of your body you land ) . the involved factors are : velocity upon impact : with initial speed $v_0=0$ and s the distance of the fall , then $$v=2gs$$ rate of deceleration : with $d$: deceleration distance : $$a=\frac{v^2}{2d}$$ g-force : with g : gravitational acceleration : $$g = \frac{a}{g} $$ force of impact : w : weight $$f=wg$$ duration of fall ( redundant here ) : $$t=\sqrt{\frac{2s}{g}}$$ impact pressure : a : cross-section of impact $$p = \frac{f}{a} $$ force withstand based on stress absorption ( specific to object denoted $\sigma$ ) :$$f_{abs}=\sigma a$$ numerical estimate for your example : simplification : to simplify the deal with force withstand of your body , we can consider landing on a surface that deflects $0.3 m$ upon impact , so : $$v=\sqrt{2*9.8*1}\approx 4.43 m/s$$ $$a=\frac{4.43^2}{2*0.3}\approx 32.6 m/s^2$$ $$g=\frac{32.6}{9.8}\approx 3.33$$ $$f=10*9.8*3.33=326.34 n$$ much lower than the impact force you had in mind ! now if you fall on your hip , the impact pressure is ( considering a hip of $a=0.025 m^2$ ) : $$p=\frac{326.34}{0.025}=13053 pa$$ note we were working with $m=10 kg$
the $3/2kt$ is the expectation value of kinetic energy of non-interacting particles in thermodynamic equilibrium , i.e. , particles with random velocities in an equilibrium probability distribution . it does not apply to convective " bulk flow " of electrons in an electric field , where in the ideal case all electrons are at the same velocity . now there is such a thing as equilibrium electronic temperature . this is temperature of an equilibrium distribution electrons . for examples , the conduction bad electrons in a solid under zero potential . that is an equilibrium condition not a non-equilibrium bulk flow situation . ideally bulk flows due to conservative fields have no entropy , and their exergy equals energy . the best way to think about it , is consider you flow to be made of classical particles that all get accelerated the same way when imposed to a macroscopic potential energy gradient . the entropy comes when these particles collide with each other and develop different velocities ( i.e. . , scramble ) . this is what happens in a real system , e.g. , wind flowing due to some pressure gradient . entropy is generated due to viscous dissipation of kinetic energy while flowing . but exergy is defined as the maximum -reversible work you can get and that is the same as the kinetic energy for a bulk flow , i.e. , 100% .
this impressive effect has several stages and reason : once she throws the water into the air , she hugely increases the surface area because the body of the water becomes lots of small droplets . because the water was close to getting vaporized , the large surface of these droplets will be quickly turned to vapor . a low humidity of the air makes this process even more efficient . the remaining much smaller droplets are much more quickly turned into ice crystals because they have a much higher surface/volume ratio . the fog that is falling down is made of small pieces of ice - it is a kind of snow . if you like unusual effects with water phases , you may also try supercooling . when you open a bottle of coke taken from the fridge at the right sub-freezing temperature , which was however still liquid , it may fully crystallize within 20 seconds . that is because the suddenly lowered pressure increases the freezing point , so you are suddenly beneath it .
the wikipedia article on color can clear the confusion on color perception . the electromagnetic spectrum in the visible range corresponds to colors as the human eye observes them , but color perception is more general and depends on the phsyiology of the eye and brain . one can make color photographs using for illumination only two frequencies , for example , as polaroid inventor e . land demonstrated . a hot iron rod is red because the upper end of the infrared spectrum , which is the heat you are feeling/measuring , not seeing , goes into our perception of red . the overwhelming majority of the energy is in the infrared for which we have no retina sensors , only skin ones . a book may be red because it is reflecting the red frequency , or we may perceive it as red because of the cones in our retina ( read the article of color in wiki ) . the cover absorbs the non reflected part of the spectrum but that is a very small amount of energy to be converted to heat from room light , so we do not perceive the difference in temperature .
in minkowski spacetime the one way light travel time to a galaxy at proper distance $\chi$ is just : $$ t = \frac{\chi}{c} $$ so : $$ \chi = ct $$ as you say . however in an frw universe the travel time is given by a different equation so the proper distance is not simply $ct$ . let 's assume all motion is in the $x$ direction , so the metric simplifies to : $$ c^2ds^2 = -c^2dt^2 + a^2 ( t ) dx^2 \tag{1} $$ we will take our position to be $ ( 0 , 0 ) $ and the galaxy to be at $ ( 0 , \chi ) $ , and we will adopt the usual convention that $a = 1$ at the current time . to get the proper distance we integrate $ds$ , and since $dt = 0$ and $a = 1$ the proper distance is just : $$ \delta s = \int_0^\chi dx = \chi $$ now let 's calculate the time it takes the light beam to get from the galaxy back to us ( i.e. . one half of the journey ) . light travels on a null geodesic so $ds = 0$ and putting this into the metric ( 1 ) and rearranging we get : $$ \frac{dx}{dt} = \frac{c}{a ( t ) } $$ if the universe is static $a ( t ) = 1$ for all $t$ , and we get $x = ct$ so you would be correct that the proper distance is equal to half the total travel time times $c$ . but then with $a = 1$ we just have minkowski spacetime so that is hardly surprising . to calculate the trajectory of the light we need to assume a form for $a ( t ) $ so let 's make the approximation : $$ a ( t ) = 1 + ht $$ where $h$ is the current value of the hubble constant . then we get : $$ \frac{dx}{dt} = \frac{c}{1 + ht} $$ and this integrates to give us : $$ \chi = \frac{c}{h} \log ( h\tau + 1 ) $$ or rearranging this to get the travel time : $$ \tau = \frac{exp ( \frac{\chi h}{c} ) -1}{h} \tag{2} $$ so the proper distance $\chi$ is not simply the travel time times $c$ . just to reassure ourselves that we get the correct result in the limit of $h \rightarrow 0$ , i.e. minkowski spacetime , note that for small $h$: $$ exp ( \frac{\chi h}{c} ) \approx 1 + \frac{\chi h}{c} $$ put this back into equation ( 2 ) and we get : $$ \tau \approx \frac{\chi}{c} $$ which is where we came in .
i agree with jwenting but in some sense , i feel that he is not answering the question : why there is no " combined $\alpha$ plus $\beta$ decay in which a nucleus emits e.g. a helium atom ? well , let me start with the $\beta$-decay . nuclei randomly - after some typical time , but unpredictably - may emit an electron because a neutron inside the nuclei may decay via $$ n\to p+e^- +\bar\nu$$ which may be reduced to a more microscopic decay of a down-quark , $$ d\to u + e^- +\bar\nu . $$ this interaction , mediated by a virtual w-boson , is why a nucleus - with neutrons - may sometimes randomly emit an electron . so the $\beta$-decay is due to the weak nuclear force . on the other hand , the $\alpha$-decay is due to the strong nuclear force : the nucleus literally breaks into pieces , with a very stable combination of 2 protons and 2 neutrons appearing as one of the pieces ( helium nucleus ) . the two processes above are independent , and each of them can kind of be reduced to a single elementary interaction whose origin is different . this independence and different origin is why the " combined " decay , with an emission of both electron ( or two electrons ) and a helium nucleus , is extremely unlikely . such an emission of a whole atom ( which is electrically neutral but it is surely not " nothing " ! ) could only occur if several of the elementary decay interactions would occur at almost the same time which is extremely unlikely .
let me first please correct your expression after the integration $p_0$ component , the result should be $g_f ( \mathbf{x} ) = \int \frac{d^3p}{2 ( 2\pi ) ^3e}e^{i\mathbf{p} . \mathbf{x}} ( e^{iet}-e^{-iet} ) $ this is because there are two poles corresponding to positive and negative energies . now please observe that the propagator can be written as : $g_f ( \mathbf{x} ) = \int \frac{d^3p}{2i ( 2\pi ) ^3}\int_{-\infty}^{\infty}d\tau e^{i\mathbf{p} . \mathbf{x}} ( \theta ( t-\tau ) e^{ie\tau}+\theta ( \tau-t ) e^{-ie\tau} ) $ in this representation the ( relativistic ) causality is manifest as only " earlier " sources effect the point for positive energies and " later " sources for negative energies . on the other hand in the original representation the propagator is analytic in the $p_0$ plane except for the two poles . this exercise therefore shows that the causality in the time domain is a consequence of the analyticity in the energy domain , and only one of them is manifest in a given form .
there is not any precise analogy between electromagnetism and gravity . in newton 's theory , only the gravitational acceleration ( the gradient of the gravitational potential ) exists at each point of space as an independent field ; there is no independent extra " gravimagnetic " field . in gr , the gravitational acceleration is given by $\gamma^a_{bc}$ , the christoffel symbol , but it is not antisymmetric in the same sense as $f_{\mu\nu}$ so one can not really hodge-dualize it to get the dual magnetic field . one may speculate about the torsion , extra fields added to gr , but the observations exclude their existence at long distances at any significant couplings . " gravimagnetic " ( literally ) is not an adjective used anywhere in serious physics literature . instead , " gravitomagnetic " is sometimes used . but it refers to any effects - in gr or whatever right theory we consider - in which masses are moving and their impact is proportional to the velocity , much like lorentz 's force $v\times b$ in magnetism . see e.g. http://arxiv.org/abs/gr-qc/0207065 for a review . no full analogy with electromagnetism , of course . kaluza-klein monopoles the most interesting insight about " magnetic monopoles constructed purely from gravitational degrees of freedom " that one may discuss in serious physics are the so-called kaluza-klein monopoles . they appear in the kaluza-klein theory where the electromagnetic $u ( 1 ) $ gauge symmetry is geometrized as the group of rotations of an extra , circular coordinate of spacetime . in this setup , one may find the solutions of higher-dimensional einstein 's equations that looks like the dirac magnetic monopole if the $g_{\mu 5}$ components of the metric are related to the electromagnetic potential $a_\mu$ according to the usual kaluza-klein dictionary . in 4+1d gravity compactified on circle , to imitate 3+1d gravity coupled to electromagnetism , kaluza-klein monopoles are point-like objects in 3+1d . the kk monopoles play an important role in string/m-theory . in particular , d6-branes in type iia string theory become kk monopoles ( with 6 extra spatial dimensions in which the solution is extended/constant ) if the coupling of type iia string theory is sent to infinity to get m-theory in 11 dimensions . the m-theory ( or 11d supergravity ) solution for the kk monopole that becomes a d6-brane is actually completely non-singular and smooth .
no , a shared atmosphere between body and moon is not possible . for a natural satellite to remain , the orbit must be very stable , because those satellites exist for billions of years . even the tiniest bit of atmosphere ( a few molecules ) would cause a tiny drag . however , drag adds up , so over a long time period , even a heavy object ( such as the moon ) would be dragged down due to drag and ultimately collide with the body it is rotating around . a balloon needs a quite significant atmosphere to be used . present balloons can reach up to 30–35 km altitude . since atmospheric density ( in the heterosphere ) drops off exponentially with elevation , balloons would get gigantic even to reach a little bit higher . reaching an elevation where the atmosphere has negligible density is , in a balloon , impossible . one can however , in theory , try to go as high as possible with a balloon , and then use other methods ( such as rockets ) from there , thus bypassing the densest part of the atmosphere and save a lot of fuel . edit : one more way to look at it : if a satellite would have enough gravitational pull to pull up an observer in a balloon , it would certainly pull up the atmosphere ; therefore the satellite would be in the atmosphere , which is impossible . therefore , a a satellite can never have enough gravitational pull to pull up an observer inside the atmosphere .
this is an old notational trick , with absolutely no significance . when you have a two-component object , you can write it as a pair of real numbers x , y , or as one complex number $$ z = x + i y $$ and it is conjugate $$ \bar{z} = x - iy $$ if you imagine for a moment that x and y are complex numbers , then the transformation from x , y to $z , \bar{z}$ makes two independent variables . if you have any function of x , y , you can pretend that you have transformed to z and \bar{z} , but the condition that x and y are real becomes the condition that z is equal to the conjugate of $\bar{z}$ . the differentiation with respect to z and $\bar{z}$ is found by changing variables from complex x , y to complex independent $z , \bar{z}$ , and using the appropriate differential operators for this change of variables $$ \partial_z = {1\over 2} ( \partial_x - i\partial_y ) $$ $$ \partial_\bar{z} = {1\over 2} ( \partial_x + i \partial_y ) $$ note that the derivative with respect to z of $\bar{z}$ is zero , etc , etc , all the obvious properties are ok . the integral over several complex variables is , when you are looking at holomorphic stuff , the integral over half the dimensions . for one complex variable , you integrate over a contour , and the contour does not matter . for two complex variables , you integrate over a 2d surface which is locally compatible with the complex structure ( locally , it is a product of contours in some complex coordinate pair ) . one such surface is the surface where x is real and y is real , so this is the integral over the surface z-bar equals the complex conjugate of z . the notation is not hard to unravel , it can always be translated to real variable language , and then it is obvious--- you integrate over all the real fields .
the mixing matrix tells you exactly the correspondence between the mass states and the flavor states . this is true in the quark sector , too , but unlike the quarks where the mass--flavor identification is pretty strong , it is very weak in the neutrino sector . the elements of the mixing matrix are exactly the flavor content of each mass state $$ \nu_\alpha = \sum_{i=1}^3 v_{\alpha , i} \nu_i \ , , $$ for $v$ the mixing matrix ( in the notation used in 2 ) . this image ( linked rather than imported because i can not find any license information ) shows the flavor makeup of each mass state graphically for both sectors . i believe that there is an assumption of normal hierarchy in that figure as the results do depend a little on the hierarchy . ( image from here . ) the strongest identification in the neutrino sector is between $\nu_1$ and $\nu_e$ , and even that is very rough : flavor changes could occur almost immediately , as evidenced by the recent success of the $\theta_{13}$ experiments ( daya bay , reno and double chooz ) in observing electron-anti-neutrino oscillation to more than five sigma at ranges on order of 1 kilometer .
the only way is to figure out some tricky atom structure which will work at higher temperatures . check out existing examples : http://en.wikipedia.org/wiki/bscco http://en.wikipedia.org/wiki/high-temperature_superconductivity#examples you ( probably ) can not make a superconductor by passing current through non-superconductor , but i do not say it is impossible . from the it side , if one would be able to create precise model for superconductivity in any atom structure and then bruteforce different structures , then it might be possible to invent something new . but this is insanely large piece of work .
see this better introduction to the history : http://en.wikipedia.org/wiki/principle_of_least_action#origins.2c_statement.2c_and_the_controversy of course , the notion of " action " only becomes meaningful when one actually knows what the purpose of the action is - to be minimized . around 1744 and 1746 , pierre louis maupertuis figured out that this could be a way to formulate laws of physics when he generalized fermat 's 17th century " principle of least time " ( for the trajectory taken by light in any environment with a variable index of refraction ) by this proverb : nature is thrifty in all its actions . obviously , some word had to be constructed or borrowed to describe the new quantity whose importance was previously unknown to the humans ( and remains to be unknown to most humans even today ) . note that the word " actions " appeared as the only noun of the quote in the context of these minimization problems , so it became known as wirkung ( $w$ ) in german and action ( $s$ ) in english . i am actually not sure why the letter $s$ was chosen . there have been claims that leibniz had found the principle as early as in 1707 . maupertuis also wrote : the laws of movement and of rest deduced from this principle being precisely the same as those observed in nature , we can admire the application of it to all phenomena . the movement of animals , the vegetative growth of plants . . . are only its consequences ; and the spectacle of the universe becomes so much the grander , so much more beautiful , the worthier of its author , when one knows that a small number of laws , most wisely established , suffice for all movements .
a color ccd is made of a monochrom ccd sensor and an array of filters . the common bayer pattern is a pattern of 2 green , a blue and a red filter . the filters transmits light on your broad band ccd sensor . a color ccd already does some spectral analysis . the four physical pixels are read into rgb channels of one color pixel . if you want to see the spectrum of color than see a histogram of rgb channels in your favorite graphics tool . e.g. the green bandpassfilter allows transmittance of several light frequencies $\nu = \frac{c}{\lambda}$ in the green and possibly overlapping with blue and red filters . usually the wavelength $\lambda$ is used to define the color of the light . this relation for visible light is visualized in the spectrum of light . imagine the intensity on the green pixel is composed of all light transmitted through the green filter . the ccd just delivers electrons and firmware translates it to a digital value . a pure ccd can not decompose the measured intensity . the information is lost .
the decay of a neutral $\pi^0$ to three photons would indeed violate charge conjugation . the charge conjugation argument goes as follows : the reaction $$\pi^0 \to 3 \gamma$$ is mediated by electromagnetism . qed has a charge conjugation symmetry , so you should be able to apply a charge conjugation to both sides of the equation . under charge conjugation , $\pi^0 \to \pi^0$ while $\gamma \to - \gamma$ ( this follows from the gauge principle ) . therefore , the initial state has an even c transformation while the final state has an uneven c transformation . in other words : applying these transformation properties to both sides of the above equation gives you a minus sign ( this may seem like a bogus calculation , but you can work it out with the actual fields if you like ) . the angular momentum thing is more difficult . you have to take into account that the system of three photons does not only have spin , but can also have an non-zero ' regular ' angular momentum ( e . g . the photons are emitted in a p-wave and not an s-wave ) .
[ quote ] if i understood correctly what i have been taught so far , in qft one must find some way to quantize the fields obeying the field equation in question . [ \quote ] this is correct . in this particular case you start with the lagrangian that has schrodinger 's equation as its eom . the following turns out to be the correct one : $$\mathcal{l} = i\psi^*\partial_{t}\psi - \frac{1}{2m} ( \partial_{j}\psi^* ) ( \partial_j \psi ) . $$ ( the $j$ 's are summed over ) . now to quantize , we can proceed in the way you suggested , i.e. impose ccr on the field $\psi$ and $\psi^*$ and then express them in terms of the creation and annihilation operators . here it turns out that you can define the creation and annihilation operators to be the fourier transform of the fields $\psi$ and $\psi^{\dagger}$ , since you can show that the fourier transforms obey the same commutation relations as the creation and annihilation operators need to , i.e. $$ [ \hat{\psi} ( \mathbf{k} ) , \hat{\psi}^{\dagger} ( \mathbf{k'} ) ] = ( 2\pi ) ^3\delta ( \mathbf{k}-\mathbf{k'} ) . $$ though one thing i am unsure why your professor did was summation instead of integration , since creation and annihilation operators usually depend on a continuous degree of freedom $k$ and thus need to be integrated over . that is , if i was to take that approach , i could postulate that $$\psi ( \mathbf{x} ) = \int \frac{d^3\mathbf{k}}{ ( 2\pi ) ^3} e^{i\mathbf{k . x}}a_{\mathbf{k}} , $$ and from there show that $\psi$ and $\psi^{\dagger}$ obey the correct commutation relations if i impose the commutation relations for the $a$ 's .
how the tap work ? and how we can apply equation of continuity to the water flow when we turn the knob and when we cover the tap with thumb the tap works by changing the minimum cross-sectional area of the flow . for a given pressure difference ( upstream pressure minus downstream pressure ) flow rate is a function of minimum cross-sectional area . using your thumb would do the same thing . you can stop the flow with your thumb if you are strong relative to the force of the flow . http://people.uncw.edu/lugo/mcp/diff_eq/deproj/orifice/orifice.htm where am getting wrong with my understanding of the hydraulic analogy . probably you are misunderstanding the equation of continuity . the equation of continuity only means that the mass flow rate in equals the mass flow rate out . it does not mean that the flow in and the flow out never change . flow rate in and flow rate out can change simultanteously . your statement " on the other hand removing pipe 2 will not change water flow [ rate ] " is incorrect . removing pipe 2 will make a big difference in total flow if it is large in cross section compare to pipe 1 . it will make a small difference in total flow if it is small in cross section compare to pipe 1 . your statement " when we decrease the area of the mouth of tap by our thumb the amount of water flowing out remains same " is also incorrect . instead , the flow rate approaches zero as you make the cross-sectional area of the unblocked portion of the mouth small .
to understand this explanation , you need to understand fourier decomposition of the electromagnetic field . in any homogeneous medium , any electromagnetic field can be thought of as a linear superposition of plane waves , all in different directions . because they run in different directions , the phase delays they undergo in propagating from , say , your aperture to another , parallel plane are all different . therefore the wavefront gets " scrambled " owing to these direction-dependent phase delays . this interference between the different plane wave components of the electromagnetic field is what we commonly call " diffraction " . i further explain this idea , as well as draw some diagrams in this answer here as well as this one here . so with this introduction in mind , let 's look at your paragraph . for simplicity , assume only one transverse direction and one axial ( in the direction of propagation ) direction . let 's also assume scalar optics , i.e. that the electromagnetic field is well represented by the behaviour of one of its cartesian components , so that we can do fourier optics on scalar field . so we have a uniformly lit aperture of width $a$ . its transverse profile is therefore the function ${\rm rect} ( 2 x/a ) $ where ${\rm rect} ( x ) = 1 ; \ , |x| \leq 1$ and ${\rm rect} ( x ) = 0 ; \ , |x| &gt ; 1$ . we take a fourier transform to find the superposition weights of each plane wave component , because each such component has a transverse variation $\exp ( i\ , k_x\ , x ) $ where $k_x$ is the fourier transform variable with units of reciprocal length . the fresnel distance is , as the paragraph says , simply the axial distance needed for this spread to double the beam width . so it is a rough measure of how quickly the light spreads . so this is how the " divergence " arises from diffraction , i.e. the interference between an optical field 's plane wave components as they propagate . also $\sin\theta = k_x/k$ where $k = 2\pi/lambda$ defines the angle that this plane wave component makes with the axial direction . we take the fourier transform , we find that there is a spread of $k_x$ values such that the plane wave components most skewed to the axial direction make an angle without direction of roughly $\lambda/a$ . so , owing to these skewed components , the field 's energy spreads out . the beam width diverges slowly at first and then , after an axial distance of several fresnel distances , the divergence speeds up so that the propagation becomes well modelled by the cone of rays diverging from the centre of the aperture . indeed if you plot contours of constant intensity , they are hyperbolas which begin at right angles to the aperture but bend so that their asymptotes are the cone defined by ray theory . the fresnel distance defines how far the " knee " of the hyperbol is from the aperture . for your question : is it not that the validity holds when all objects are comfortably larger , and not smaller , than the wavelength of light ? this is in general right , but it breaks down near focuses and in situations like this where we are near and aperture and if the aperture is comparable to the light wavelength . in this case you should be able to understand from the fourier analysis the reciprocal relationship between the aperture width and the angular spread .
well yes , for the ideal gas model , $pv=nk_bt$ , you find $$u=\frac{3}{2}nk_bt\propto t , $$ and $$c_v=\left ( \frac{∂u}{∂t}\right ) _v=\frac{3}{2}nk_b=\text{const} . $$ this itself is a violation of the third law . what does it say to us ? the lengthy discussion in the comments of this question might help you understand the problem .
this is because of the symmetry of the problem . using coulomb 's law for each point of the charge distribution ( summing over each point ) $$ e ( \vec{r} ) = \frac{1}{4\pi\varepsilon_0}\int \frac{\rho ( \vec{s} ) ( \vec{r}-\vec{s} ) }{|\vec{r}-\vec{s}|^3} d\vec{s}$$ since the charge only depends on $|x|$ , you can view this as a infinite sheet of charge place at any value of $x$ . thus , if you consider a test charge at $r_1 = ( x_1 , y_1 , z_1 ) $ , the contribution from $y&gt ; y_1$ is opposite to that from $y&lt ; y_1$ , and therefore cancels . the same holds in the $z$ direction . thus , the components of the field in $y$ and $z$ are zero . the antisymmetry of $e$ , i.e. $e ( x ) = - e ( -x ) $ , comes from the fact that the charge distribution depends on $|x|$ . indeed , considering 1d , $\rho ( -x ) \cdot ( -x ) = -\rho ( x ) \cdot x = - [ \rho ( x ) \cdot x ] $ because of $\rho = f ( |x| ) $ .
if you close both switches 1 and 2 and redraw the circuit , it will look like this so bulbs a , b and c are connected in parallel . the current will flow in all the bulbs , all of them will be working
if you continued to read on , it goes on to say : actually , all bodies are electrified , but may appear not to be so by the relative similar charge of neighboring objects in the environment . an object further electrified + or – creates an equivalent or opposite charge by default in neighboring objects , until those charges can equalize . therefore , since all bodies are electrified or can be electrified , your statement is correct .
this page says : however , the surface madelung constant computed in the ( 100 ) plane is 96 % of the bulk value for crystals with a sodium chloride structure . in the ( 110 ) planes in crystals with sodium chloride and cesium chloride structures , the surface madelung constant is 86% and 90% respectively , of the bulk value . seems high , but i do not have access to the cited reference ( p . h . citrin and t . d . thomas , j . chem . phys . 57 , 4446 ( 1972 ) ) to check .
the comment on this page http://chemistry.about.com/od/photogalleries/ig/nuclear-tests-photo-gallery/operation-teapot-test.htm http://chemistry.about.com/b/2011/04/19/nuclear-explosion-lines-spikes.htm says : sounding rockets or smoke flares may be launched just before a device explodes so that their vapor trails may be used to record the passage of the otherwise invisible shock wave . to learn about every detail of these tests , contact your nearest fbi agent .
there is a number of interesting points to this . the passage from 1 . to 2 . is not trivial . if you do the calculation , you will see that the laplacian $\nabla^2\vec{e}$ from the wave equation gives rise to the term in $\left ( \nabla\chi\right ) ^2$ you mention as well as a term in $\nabla^2\chi$ . this second term only goes away in the small $\lambda$ limit and it is the essence of the eikonal approximation . it is not a calculation you should wave away : work it out in full and implement the approximation , noticing that locally $\chi ( \vec{x} ) =\vec{k}\cdot\vec{x}+\text{slow factors}$ , where $\vec{k}$ is large . ( you will of course need to quantify " slow " . ) ( the calculation that $\vec{s}=\frac{c^2}{n^2\omega}\nabla\chi$ , on the other hand , is trivial . ) as kdn mentioned , the integral curves of $\vec{s}$ and its unit vector $\vec{s}$ are the same . this follows from the definition of integral curves : they are curves such that the vector field is tangent to them throughout . this is independent of the length of the vector . ( in terms of the curve it corresponds to a reparametrization of the " time": it changes the speed but not the direction of the velocity . ) using a unit vector means that light rays will be parametrised by path length . one can simply define light rays to be the integral curves of $\vec{s}$ and be happy about it , though of course that is simply missing the physics . the key fact about the light rays , so defined , is that they are everywhere normal to the surfaces of constant $\chi$ , i.e. the surfaces of constant phase , i.e. the wavefronts . plane waves propagate in straight lines normally to the wavefronts in free space , and so do light rays ( so defined ) . it is the normal to the wavefronts that matters when working out fresnel equations , and therefore the ( so defined ) light rays will obey snell 's law . ultimately , proving 3 . is a matter of definition : what are light rays ? write down any defining property and you will be able to prove the integral curves of $\vec{s}$ obey it . it is important to note that in isotropic media $\vec{s}$ is not only the local unit poynting vector , but it is also the local unit wave vector . ( essentially , this is the same point as above . ) intuitively , light rays ought to follow wave vectors because it is wave vectors that tell light waves where to go . in a birefringent ( not isotropic ) medium the phase propagation direction ( wave vector ) and the energy propagation direction ( poynting vector ) are not necessarily the same ( and the snell law does not apply ) . proving 4 . is an interesting exercise ( i.e. . do it ! ) but it is essentially trivial . it relies on the identity $\frac{d\vec{x}}{d\tau}=\vec{s}$ , which defines light ray curves $\vec{x} ( \tau ) $ , on judicious use of the total derivative $\frac{d}{d\tau}= ( \frac{d\vec{x}}{d\tau}\cdot\nabla ) $ , and some interesting vector calculus manipulations . ( hint : prove $ ( \nabla\chi\cdot\nabla ) \nabla\chi=\frac12\nabla\left ( \nabla\chi\right ) ^2$ . ) presumably you know by now that what you get is called the ray equation , what it means , and how to use it , or you would not have stopped there ; ) . this looks like enough to get you going but if you have more questions , do ask .
i think the answers in the duplicate have covered most of the key points . i will just add to them the feeling of weight , strain , stress etc . is due to the differential force acting on different points of the body . even while floating in curved space-time you may feel strain , weight etc . because your body is not point size , the particles of your body will be accelerated in different directions relative to each other . to maintain the rigidity of your form , the body exerts a force to hold itself together . this force is weight , strain , stress etc . so irrespective of your state of motion from any frame of reference , if the particles of body are being forced in different directions relative to each other , your nervous system will register this as weight , stress , strain in the respective parts of the body . the cause of relative acceleration between parts of your body is twofold- tidal acceleration due to the structure of space time , the action of non gravitational forces on the different particles in your body . so even in non-inertial frames without the effects of curved-space you might feel this weight , strain etc . the only time you will feel absolutely weightless , is if the relative acceleration between all the particles in your body is equal to 0 . due to the fact that the earth is curved and that you are not a point particle , there is a slight inward lateral strain even when you free fall in vacuum on earth .
i decided to look at whether the estimate you arrive at gives a reasonable-seeming result . 0.000145 parsecs ( 30 au , about the radius of neptune 's orbit ) is a close encounter indeed . this closeness made me think at first that 50 million years seemed to often . we do not have evidence of giant planets passing that nearby that often . so then i looked at the mass distribution of these ' nomad planets ' . in the article it says that they are " ranging from the size of pluto to larger than jupiter " , or $0.002 m_e$ to $&gt ; 300 m_e$ . we can assume that there are many more dwarf-planet sized bodies than gas giants . now we ask , does this seem right ? about every 50 million years a pluto-sized body passes as close as neptune 's orbit ? and i have to say yes , this does seem reasonable . a body the size of pluto would not produce much perturbation at that distance , almost certainly not enough to significanly effect the planets ' orbits and only sometimes enough to disturb a few oort cloud and kuiper belt objects . ( a jupiter-sized body might pass that close far less frequently - on time scales of a billion years or so ; you could imagine that this could help explain some anomalies like the distribution of the giant planet 's orbits , but that is just speculation . ) this is similar to how asteroids fairly commonly pass closer than the distance of the moon to earth . it is interesting and notable , but not catastrophic , which makes me want to say based on intuition that your estimate is reasonable .
there are in fact two field lines that depart each charge headed towards the other . these lines meet at the origin ( the mid-point of the two charges ) , where the field is zero , and vanish there . there are also two other lines , which are born at the origin and depart along the vertical axis . thus , formally , two lines go in and two lines go out , so no lines actually die in empty space . these lines are actually a limiting case of lines that leave the point charges at a small angle $\epsilon$ from the intercharge axis ; these lines make increasingly close approaches to the origin as $\epsilon\rightarrow0$ , and then they shoot off to infinity , increasingly close to the vertical axis . ( if you are sharp , you will notice there is actually an infinity of such lines , since there is also lines that go off perpendicularly to the screen and at any angle in between . thus my " two-for-two " argument is not actually quite right . can you see the limiting behaviour that makes it right ? ) pictures of this were relatively hard to find , but you can see them in this wolfram web app : you also have to consider one key point : at the origin , the field is zero , so actually there should be no field lines through it . or , more formally , the density of field lines should be zero . this comes about in that the angle $\epsilon$ should be really small for the lines to actually approach the origin . you should then plaster the diagram with lines leaving equiangularly at angle $\epsilon$ from each charge , and that will mean a lot of lines on the " outside " of the charges . ultimately , though , the lesson is that individual field lines are not that important , and it is the set of lines , equiangularly leaving the charges ( in 3d ! ) , that makes a physically relevant diagram . and even then , field line diagrams are only of limited utility in understanding electric fields , mostly because they only incorporate with the utmost difficulty the superposition principle , which is at the real heart of classical electromagnetism .
in gilmore 's book : http://www.amazon.com/single-particle-detection-measurement-gilmore/dp/0850667550 chapter 5 is on solid state ionization and it is use in detectors . that was hanging around the lab when i was working in particle physics . i would give that one a try . it is a bit old these days but the principles are there .
actually in electrostatics energy density of e-field is not a physical observable . as you say , only when charges move will there be any work done . while the two ways of calculating total energy end the same , you cannot distinguish whether energy is stored on the charges or in the field . even e-field itself is more of an abstract mathematical entity , without which everything can be calculated in terms of coulomb law . the physical reality of e and b fields ( and the energy density associated ) becomes apparent only in non-static cases . for example , in electromagnetic radiation , fields can propagate in free space without associating with charges and currents , and the radiation may do work on non-charges ( for example , light pressure ) . because from maxwell equations we can derive a general formula of energy density $$\rho = \frac{\epsilon_0}{2} |\vec e|^2 + \frac{1}{2\mu_0} |\vec b|^2$$ which coincides with the electrostatic case , we deduce that even in electrostatics energy is indeed stored in the fields .
there are three main reasons . 1 ) while venus is orbiting the sun at 35.02 km/s , the earth is also orbiting the sun in the same direction at 29.78 km/s . this factor will decrease the relative transit velocity of venus as seen from earth . 2 ) venus is travelling at 35.02 km/s an elliptical orbit . hence the actual distance traveled by venus during the transit will be slightly more than its diameter because it is travelling on a curved path and not a straight line . this factor will increase the actual transit distance covered by venus . however the contribution of this is negligible and can be ignored except of high precision calculations . 3 ) there will be a small but measurable impact because of the surface velocity of earth 's rotation at 0.434 km/s ( at the equator ) about its axis . notice that the tangential velocity of an observer on earth due to the rotation of the earth about its axis will be in opposite direction to the tangential velocity of both the venus and earth around the sun . this factor will increase the relative transit velocity of venus as seen from earth . my calculation , using kepler 's law differ slightly from that of nathaniel but it is essentially same in spirit . we obtain the transit time of 19 mins 56 seconds which is accurate enough . $$ t \approx \frac{d_v}{v_v\{1 - ( t_v/t_e ) ^{2/3}\} + v_e} = 19 \min 56 \sec $$ where $d_v$ = diameter of venus , $v_v$ = orbital velocity of venus , $v_e$ = orbital velocity of earth , $t_v$ = orbital period of venus , $t_e$ = orbital period of earth , $v_e$ = rotation velocity of earth .
first , this question has nothing to do with matlab . besides , if you think there is an error in your code implementation , this is most certainly the wrong stackexchange . that said , the error is a physical one . you are confusing the distribution of velocity vectors $\vec{v}$ , which is a 3d gaussian ( by that i mean a function from $\mathbb{r}^3$ to $\mathbb{r}$ that is the product of gaussians in three cartesian coordinates ) with the 1d distribution of those vectors ' magnitudes . the former had better be symmetric about 0 - do you expect that gas in thermal equilibrium is moving in one particular direction ? this is in fact exactly what you plot ( well , actually you plot a 1d slice of the 3d domain , essentially the distribution of $x$-velocities ) . you need a $v^2$ factor in the distribution ( and a few modified constants ) to get the asymmetric thing you are looking for . this is the same $v^2$ ( or $r^2$ ) that arises , for instance , when converting a spherically symmetric integrand from cartesian to spherical coordinates . when you do that , by the way , do not try to plug in negative values - they do not make physical sense , since again it is a distribution of magnitudes . in terms of the notation in that wikipedia article , the distribution of vectors is $f_\mathbf{v}$ , which is $f_v^3$ , and you are plotting $f_v$ . what you want to plot is what the article calls unsubscripted $f$ ( well , one of the many distinct things it calls $f$ ) : $$ f ( v ) = \sqrt{\left ( \frac{m}{2\pi kt}\right ) ^3} 4\pi v^2 \exp\left ( \frac{-mv^2}{2kt}\right ) . $$
i do not understand how to compute a finite resistance for an arc that would come out as infinite in some other cases . arc formation is a sufficiently non-stationary and nonlinear process . so , one has to use dynamic circuit theory , where the resistivity in ohm 's law is a complex number and contains both active and reactive components depending on the applied voltage . that is in general . in practice , modeling the arc 's resistivity in both static and dynamic ( transient ) regimes is very hard problem which was attempted to be solved by many groups . searching in google you can find several approaches based on the equivalent circuit method , where conducting chanel is approximated by a set of resistors , capacitors and inductors . understanding of this phenomena is strongly related to microscopic nature of electron transport in conducting channel .
you can not integrate the right hand side because $f=f ( x_i ) $ and you have got a differential on $t$ . as for a ) , if you rearrange terms , you can verify that $$m\frac{d\dot{x_i}}{f ( x_i ) }=g ( t ) \ , dt$$ so that now you can not integrate the left hand side because $f$ depends un $x_i$ and you have got a differential on $\dot{x}_i$ .
one weber is a unit of magnetic flux – it is $$\int\vec b\cdot d\vec s . $$ so it depends on the area $s$ one integrates over . that is why it is misleading to associate a certain number of webers with the adjectives " strong " and " weak " . the magnetic flux is not really an intensive quantity : it is an extensive quantity of a sort . the larger area you integrate over , the more webers you get . one tesla is one weber per squared meter . so you may imagine $x$ webers as $x$ teslas integrated over a squared meter and use the table for $x$ teslas from wikipedia . of course , you may choose a different area than one squared meter ; the numbers must be changed appropriately . you may get rid of the dependence on the area if you consider the magnetic flux around a magnetic monopole – which has not been observed yet . the dirac quantization considerations tell you that $e\int_d\vec b\cdot d\vec s$ must be a multiple of $2\pi\hbar$ which means that the elementary magnetic monopole has the magnetic flux ( over any spherical surface that surrounds it ! ) $2\pi\hbar/e$ , about $10^{-15}$ weber , calculate the exact number if you want . note that dimensionally , one weber is one joule per one ampere ( and many equivalent ways to write it ) so $e$ times the magnetic flux is one ampere-second ( coulomb ) times one joule over one ampere . the amperes cancel and you are left with joule-seconds , the unit of $\hbar$ .
time zones moving on from the calendar to time , we recommend the abolition of all time zones , as well as of daylight savings time , and the adoption of atomic time—in particular , greenwich mean time , or universal time , as it is called today . like the adoption of a modern calendar , the embrace of universal time would be beneficial . for example , the adoption of universal time would give new flexibility to economic management in the vast east-west expanse of russia : everyone would know exactly what time it is everywhere , at every moment . opening and closing times of businesses could be specified for every class of business and activity . if thought desirable , banks and financial institutions throughout the country could be required to open and to close each day at the same hour by the world time . this would mean that bank employees in the far east of russia would start work with the sun well up in the sky , while bank employees in the far west of russia would be at their desks before the sun has risen . but , across the country , they could conduct business with one another , all the working day . ( this would have a second benefit : at least in the far east and far west , the banks would be open either early , or late , convenient for those who are working “sunlight hours , ” such as farmers . ) with universal time , agricultural workers , critically dependent on the position of the sun , could rise with the sun , without producing any impact on other aspects of cultural and economic life . the readings on the clocks , and the date on the calendar , would be the same for all . but , times of work would be attuned with precision to russia’s local and national needs . china already has adopted a single time zone for the same purposes . and all aircraft pilots , worldwide , use universal time exclusively , for exactly the same reason that we are advocating its broad adoption—plus avoiding collisions . moscow could introduce both a simplified calendar , identical each year ( harmonized with the seasons by rare full-week adjustments at year’s end ) , and universal time , which would abolish the international date line , making the date and the time identical everywhere , including alaska and the farthest eastern regions of russia . there , and also in the center of the pacific ocean , the date would change at 00:00:00 , just as the sun passed overhead . source and context . also see this and this for discussion . dst one of the biggest reasons we change our clocks to daylight saving time ( dst ) is that it reportedly saves electricity . newer studies , however , are challenging long-held reason . a report was released in may 2001 by the california energy commission to see if creating an early dst or going to a year-round dst will help with the electricity problems the state faced in 2000-2002 . you can download an acrobat pdf copy of the staff report , effects of daylight saving time on california electricity use , publication # 400-01-13 , ( pdf file , pages , 5.2 megabytes ) . the study concluded that both winter daylight saving time and summer-season double daylight saving time ( ddst ) would probably save marginal amounts of electricity - around 3,400 megawatt-hours ( mwh ) a day in winter ( one-half of one percent of winter electricity use - 0.5% ) and around 1,500 mwh a day during the summer season ( one-fifth of one percent of summer-season use - 0.20% ) . winter dst would cut winter peak electricity use by around 1,100 megawatts on average , or 3.4 percent . summer double dst would cause a smaller ( 220 mw ) and more uncertain drop in the peak , but it could still save hundreds of millions of dollars because it would shift electricity use to low demand ( cheaper ) morning hours and decrease electricity use during higher demand hours . the energy commission has also published a new report titled the effect of early daylight saving time on california electricity consumption : a statistical analysis . publication # cec-200-2007-004 , may 27 , 2007 . ( pdf file , 592 kilobytes ) a more recent study concludes that daylight saving time in indiana actually increases residential electricity demand . that study titled " does daylight saving time save energy ? evidence from a natural experiment in indiana " . ( pdf file ) looked at the electricity use when portions of the state finally started to observe dst . before the new extended dst , portions of indiana did not observe dst . some have wondered whether this study would be true for the entire united states . initial analysis by staff of the california energy commission says a similar study may not yield the same results for california because : the use of residential air conditioning is relatively low in indiana , and the saturations are low . where as california has high usage of air conditioning in the summer . heating use is relatively high in indiana , while it is relatively low in california . the diurnal variation in temperature is low while california is very high . indiana is located in western edge of the same time zone as maine and florida , but the sun actually comes up at an earlier time than those other two states . indiana 's north-south location will affect how long the days are in the summer and might very well lead to different results in different areas . so , while the analysis is of interest to indiana , it is conclusions may not be totally correct for california or the rest of the country . the first national study since the 1970s , was mandated by congress and was done by the u.s. department of energy . the doe study can be downloaded at : http://www1.eere.energy.gov/ba/pba/pdfs/epact_sec_110_edst_report_to_congress_2008.pdf ( pdf file , 285 kb ) [ actually : here ] the key findings in the report to congress are : the total electricity savings of extended daylight saving time were about 1.3 tera watt-hour ( twh ) . this corresponds to 0.5 percent per each day of extended daylight saving time , or 0.03 percent of electricity consumption over the year . in reference , the total 2007 electricity consumption in the united states was 3,900 twh . in terms of national primary energy consumption , the electricity savings translate to a reduction of 17 trillion btu ( tbtu ) over the spring and fall extended daylight saving time periods , or roughly 0.02 percent of total u.s. energy consumption during 2007 of 101,000 tbtu . during extended daylight saving time , electricity savings generally occurred over a three- to five-hour period in the evening with small increases in usage during the early- morning hours . on a daily percentage basis , electricity savings were slightly greater during the march ( spring ) extension of extended daylight saving time than the november ( fall ) extension . on a regional basis , some southern portions of the united states exhibited slightly smaller impacts of extended daylight saving time on energy savings compared to the northern regions , a result possibly due to a small , offsetting increase in household air conditioning usage . changes in national traffic volume and motor gasoline consumption for passenger vehicles in 2007 were determined to be statistically insignificant and therefore , could not be attributed to extended daylight saving time . source and further references and context
here 's part of my answer to the derivvation of the em tensor for the ghost action . it does not match the expression you gave , but i may have made a mistake . can you check my work ? we start with the action \begin{equation} \begin{split} s_{gh} and = - \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} \nabla_\mu c^\beta \\ \end{split} \end{equation} let us now vary the action w.r.t. metric . we get \begin{equation} \begin{split} \delta s_{gh} and = - \frac{i}{2\pi} \int d^2 \sigma \left ( \delta \sqrt{g} \right ) g^{\alpha\mu} b_{\alpha\beta} \nabla_\mu c^\beta \\ and ~~~~~~~~~~~~~~~~~- \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} \left ( \delta g^{\alpha\mu} \right ) b_{\alpha\beta} \nabla_\mu c^\beta \\ and ~~~~~~~~~~~~~~~~~- \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} \delta \left ( \nabla_\mu c^\beta \right ) \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} \left [ b_{\alpha\mu} \nabla_\beta c^\mu + b_{\beta\mu} \nabla_\alpha c^\mu - g_{\alpha\beta} b_{\rho\sigma} \nabla^\rho c^\sigma \right ] \delta g^{\alpha\beta} \\ and ~~~~~~~~~~~~~~~~~ - \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} c^\lambda \delta \gamma^\beta_{\mu\lambda} \\ \end{split} \end{equation} we now use \begin{equation} \begin{split} \delta \gamma^\beta_{\mu\lambda} = \frac{1}{2} g^{\beta\rho} \left [ \nabla_\lambda \delta g_{\rho \mu} + \nabla_\mu \delta g_{\rho \lambda} - \nabla_\rho \delta g_{\mu\lambda}\right ] \end{split} \end{equation} note that in particular , it is a tensor . the last term then becomes \begin{equation} \begin{split} i and = - \frac{i}{2\pi} \int d^2 \sigma \sqrt{g} g^{\alpha\mu} b_{\alpha\beta} c^\lambda \delta \gamma^\beta_{\mu\lambda} \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} b^{\mu\rho} c^\lambda \left [ \nabla_\lambda \delta g_{\rho \mu} + \nabla_\mu \delta g_{\rho \lambda} - \nabla_\rho \delta g_{\mu\lambda}\right ] \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} b^{\mu\rho} c^\lambda \nabla_\lambda \delta g_{\rho \mu} \\ and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} \nabla_\lambda \left ( b_{\alpha\beta} c^\lambda \right ) \delta g^{\alpha\beta} \end{split} \end{equation} we then have \begin{equation} \begin{split} \delta s_{gh} and = - \frac{i}{4\pi} \int d^2 \sigma \sqrt{g} \left [ b_{\alpha\mu} \nabla_\beta c^\mu + b_{\beta\mu} \nabla_\alpha c^\mu - g_{\alpha\beta} b_{\rho\sigma} \nabla^\rho c^\sigma + \nabla_\lambda \left ( b_{\alpha\beta} c^\lambda \right ) \right ] \delta g^{\alpha\beta} \end{split} \end{equation}
here is a link that allows you to simulate the field lines and equipotentials of both point charges and charged plates : http://www.falstad.com/emstatic/ perhaps you will find what you are looking for here .
mushroom clouds are formed in explosions ( not necessarily nuclear - see picture ) as a result of the rayleigh-taylor instability . for given density contrast $\frac{\rho_{cold}-\rho_{hot}}{\rho_{cold}+\rho_{hot}}$ between the hot cloud and the cold atmosphere , the timescale $t_{rt}$ for this instability scales with the length scale $l_{rt}$ according to : $$t_{rt} \approx \sqrt{\frac{l_{rt}}{g}\frac{\rho_{cold}+\rho_{hot}}{\rho_{cold}-\rho_{hot}}}$$ with $g$ the gravitational acceleration . for $\frac{\rho_{cold}-\rho_{hot}}{\rho_{cold}+\rho_{hot}}\approx 0.1$ and $l_{rt} = 1 km$ we find $t_{rt} \approx 30 s$ . for a cup sized ( $l_{rt} = 0.05 m$ ) explosion with the same density contrast we find $t_{rt} \approx 0.2 s$ . the ' mushroom ' has disappeared before it really takes shape . bottom line is that you need fairly large explosions to observe a mushroom .
provided its a general situation , the heat will circulate through the pipe ( and the water of course ) but most of it will be radiated out . at no point will a measurable amount of water reach a $1000 ^\circ c $ . even if some of the water reaches that temperature or close to it , it will lose it to the colder surroundings . in the end at some point this heat leak out to the surroundings . assuming that somehow there is no heat loss , eventually the whole pipe will approach $1000^\circ c$ as there is an inflow of energy but no place for it to escape to .
in the early days of radio , the resonance of the antenna in combination with its associated inductive and capacitive properties was indeed the item which " dialed in " the frequency you wanted to listen to . you did not actually change the length of the antenna , but by changing the inductor ( a coil ) or capacitor connected to the antenna you tuned the resonance . the output signal is an alternating voltage , and by rectifying it with a diode ( called a " crystal " then . . ) you could extract a signal modulated as a varying amplitude of the carrier wave . all this without any battery ! : ) but actually the antenna in a normal modern radio is not the component that " dials in " the selected broadcast frequency . the antenna circuit should indeed have a resonance within the band of frequencies you are interested in but this wide-band signal is then mixed with an internally generated sinusodial signal in the radio in an analog component , this subtracts the frequencies and lets the rest of the radio operate on a much easily handled frequency band ( called the intermediate frequency ) . it is in the mixer you tune the reception in a modern superheterodyne radio receiver . it is much easier to synthesize an exact mixing frequency to tune with than to change the resonance of the antenna circuit . the rest is not really physics , but the difference between an analog and a digital radio comes in the circuits after this and basically an analog radio extracts a modulation from the intermediate frequency which is amplified and sent to the speakers or radio output . in a digital radio , the signal represents a digital version of the audio , just like a wav or mp3-file on a computer is a digital representation which can be turned back into an analog signal you can send to a speaker . the benefit of this is that the digital signal requires ( potentially ) less bandwidth in the air so you can fit more signals in the same " airspace " and that the digital signal can be less susceptible to noise . i write " can " , because unfortunately many commercial digital radio/tv stations do not do this to improve the viewing or listening quality but just to fit in more content . let me reiterate that in a " digital " radio , the component that selects the reception frequency is still analog but the mixing ( tuning ) frequency is digitally controlled and selected . there is also a very interesting thing called software defined radio , sdr , which is the principle where the intermediate frequency ( or in some cases the antenna frequency directly ) is turned into a digital signal and demodulated by a signal processor which is completely software-upgradeable . since it is much easier to program new software than to solder electronic components around , this created large interest in the radio hobby community where you can completely change the properties of a radio receiver just by downloading someone else 's software from the net or write a new one yourself . if you include sdr , and apply it without any intermediate frequency ( take the antenna directly to an analog/digital converter and into a signal processor ) , you do indeed have a purely software-way of tuning your source like you ask for , although this is not how the most common digital radios work currently .
look up the catenary curve properties and notice that the weight of the cable causes a reaction in both $x$ and $y$ axis as you noted . the vertical components of the reactions sum up to the weight of the cable , and the horizontal components are such as the tension being tangent to the shape of the cable . now what you " feel " when you move the support is the inertial force of moving the cable which i suppose it too small for you to feel . if you try with a heavy steel cable , and you accelerate in the same order of magnitude as gravity then you will feel inertial resistance . notice that you also feel an effective stiffness as you pull on the cable as the sag decreases and the incident angle creates higher horizontal reactions . in addition you might have elastic deformation also decreasing the above stiffness by $1/k_{eff} = 1/k_{elastic} + 1/k_{geometric}$ .
if you assume that your body is a uniform , thin rigid rod . one end of the rod is pivoted ( aka your feet ) during the fall . then one simply recalls that the angular velocity $\omega$ of rotation of your body is related to the tangential velocity $v$ of a point a distance $r$ from the pivot by \begin{align} v = \omega r \end{align} now , if you have height $h$ , then your center of mass will be a distance $h/2$ from your feet , and your head will be a distance $h$ from your feet , so the ratio $v_\mathrm{head}/v_\mathrm{com}$ of the tangential velocity of your head to that of your center of mass is \begin{align} \frac{v_{\mathrm{head}}}{v_{\mathrm{com}}} = \frac{\omega h}{\omega ( h/2 ) } = 2 . \end{align} in other words , the tangential velocity of your head will be twice as large as that of your center of mass . the intuition behind this is simply that for a given change in the angular position of your body , your head moves twice as far as your center of mass . note that in the real world , the assumptions above are not really correct , but they do give you an idea of the basic physics in a rough approximation .
to change the past you require a closed timelike curve . stephen hawking proved that closed timelike curves cannot be created in a finite system without using exotic matter . i think the proof was in his paper on the chronology protection conjecture but i do not have access to the paper at the moment . this far we have a reliable grasp on whether causality can be violated , but from here things get speculative . it seems likely that no infinite structures exist , if only because the universe has not existed for an infinite time ( though if there was a big bounce we could be wrong about this ) . the big question is whether exotic matter exists . the trouble is that there is no proof that exotic matter either exists or does not exist . it is like negative mass - there is nothing to stop you plugging a negative value for mass into the gr equations , but that does not mean it is a physically meaningful thing to do . we have never observed exotic matter , but that does not necessarily mean it does not exist . the chronology protection conjecture that i mentioned above is the closest we have to a mathematical approach to constraining causality violations , but it is just a conjecture and has not been proven - though it has not been disproven either . at the moment we simply do not understand the physics well enough to give a definitive answer .
we start with a collision between two particles . particle 1 has mass $m_1$ , momentum $\mathbf{p}_{\rm lab}$ , and total energy $e_{\rm lab}$ while particle 2 ( mass $m_2$ ) is stationary . particle 1 then scatters off at an angle $\psi$ while particle 2 scatters off in angle $\zeta$ . by using a transformation of coordinates , we can jump into a frame in which it would appear that the two particles are coming towards each other at the same speed ( center of mass ( c . m . ) frame ) ( source ) it is often convenient to work in the c.m. frame , so we will use that . it can be shown that the total energy in the c.m. frame is given by $$ w^2=m_1^2+m_2^2+2m_2e_{\rm lab}\tag{1} $$ the change in energy for particle 2 is then given by $$ \delta e=\frac{m_2}{w^2}p^2_{\rm lab}\left ( 1-\cos\theta\right ) =\frac{m_2}{m_1^2+m_2^2+2m_2e_{\rm lab}}p^2_{\rm lab}\left ( 1-\cos\theta\right ) \tag{2} $$ in order to find the maximum energy transfer , we need to do a little hokey business . first is an easy one : $$\left ( 1-\cos\theta\right ) _{max}=2\quad ( {\rm at}\ , \ , \theta=\pi ) $$ next , we use the relation $p=\gamma\beta m$ and assume that $m_1\gg m_2$ so that $w\simeq m_1$ . equation ( 2 ) then becomes $$ \delta e_{max}\simeq\frac{m_2}{m_1^2}\cdot\gamma^2\beta^2m_1^2\cdot2 $$ the $m_1^2$ terms cancel leaving the expected relation : $$ \delta e_{max}\simeq2m_2\gamma^2\beta^2 $$
let 's see two charges $q_1$ and $q_2$ , we have to find electric field at some point between them . let 's assume $d$ to be the distance between the charges , which is constant and $x$ ( from the center of the line joining the two charges , $\frac{d}{2}$ from the chagres ) to be the distance where we want to measure electric field at . now , $\large e_{q_1x} = \frac{1}{4 \pi \epsilon} \frac{q_1}{\left ( \frac{d}{2}+x\right ) ^2}$ and $\large e_{q_2x} = \frac{1}{4 \pi\epsilon} \frac{q_2}{\left ( \frac{d}{2}-x\right ) ^2}$ total electric field at $x$ , $$e_{x} = \frac{1}{4 \pi \epsilon} \frac{q_1}{\left ( \frac{d}{2}+x\right ) ^2} + \frac{1}{4 \pi\epsilon} \frac{q_2}{\left ( \frac{d}{2}-x\right ) ^2}$$ i tried to make it as generalized as possible so you do not have to face problems with such type of problems in future . i hope it helps you and good luck for your exam ! i tried to attach a picture but because of low reputation i could not , sorry .
because you were also in orbit around the sun with the earth and still have that velocity . you may be imagining this in terms of stepping off of a slow moving vehicle on the earth : you jump off , you come to a stop relative the ground and watch the trolley car go it is merry way . but that is a feature of friction between you and the ground . there is no such thing as a absolute reference frame in the universe and when you " leave the earth " you do not come to stop relative anything so that you can watch the earth fly away . newton 's laws apply here : " a body in motion ( that is the you or the planet ) will continue in motion unless acted on by an external force " . you just keep going except for changed induced by your drive .
you might want to calculate poynting vector ( which corresponds to intensity [ w/m$^2$ ] of electromagnetic waves ) : $$\vec{s} = \frac{1}{\mu_0} \vec{e} \times \vec{b} , $$ which for plane waves amounts to $$\langle s \rangle = \frac{c b_0^2}{2 \mu_0} . $$
you conjecture is correct . one can relate the 2d ising model with the bond correlated percolation model . the details are in the paper percolation , clusters , and phase transitions in spin models . the basic idea is to consider interacting ( nearest neighbor ) spins as forming a bond with a certain probability . one can then show that the partition function of the ising model is related to the generating function of the bond-correlated percolation model . the above paper demonstrates that the bond-correlated percolation model has the same critical temperature and critical exponents as the 2d ising model . however , the values of $t_c$ and the critical exponents seem to be dependent on exactly how one defines a bond . see section iii . a . 1 in universality classes in nonequilibrium lattice systems ( or arxiv version ) . nonetheless your intuitive picture that there would be spanning clusters below $t_c$ and no such clusters above $t_c$ remains valid . edit 21 may 2012 i found a pedagogical paper that discusses this issue .
it is true that all even-even nuclei ( hundreds of such isotopes have been measured ) have spin-0 in the ground state . this is due to what is often called the pairing effect . protons and neutrons are spin-½ particles , and they have a tendency to respectively pair up in proton-proton and neutron-neutron pairs so that their spin ( and orbital ) angular momentum adds to zero . this is a pillar of the nuclear shell model which says that we can predict many properties of a nucleus by examining the " unpaired " nuclei . the spin and parity of odd-even and even-odd nuclei are generally determined by the " valence nucleon " ( cf . the valence electron in the atomic shell model ) that is left when all pairing has occurred . odd-odd nuclei are not commonly found in nature , which we can describe to its tendency to convert the odd proton into a neutron ( or vice versa ) via $\beta$ decay to gain binding energy through the pairing force . as to " why ? " , that is a larger question . this effect is an empirical observation in nuclear physics , and it is seen to be helpful in predicting how nuclei behave over a large range of isotopes . the " pairing term " is a part of the semi-empirical mass formula , which does a fairly good job of predicting nuclear properties , at least among heavier and stable nuclei . the links to the nuclear shell model and the semi-empirical mass formula are probably good further reads .
this a bit of a sketch ; the $s$-matrix acts on shift the state or momentum state of a particle . a state with two particle states $|p , p’\rangle$ is acted upon by the $s$ matrix through the $t$ matrix $$ s~=~1~–~i ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) t $$ so that $t|p , p’\rangle~\ne~= 0$ . for zero mass plane waves scatter at almost all energy . the hilbert space is then an infinite product of n-particle subspaces $h~=~\otimes_nh^n$ . as with all hilbert spaces there exists a unitary operator $u$ , often $u~=~exp ( iht ) $ , which transforms the states s acts upon . $u$ transforms n-particle states into n-particle states as tensor products . the unitary operator commutes with the $s$ matrix $$ sus^{-1}~=~ [ 1 – i ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) t ] u [ 1~+~i ( 2π ) ^4 \delta^4 ( p~–~p’ ) t^\dagger ] $$$$ = u~+~i ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) [ tu~–~ut^\dagger ] ~+~ [ ( 2\pi ) ^4 \delta^4 ( p~–~p’ ) ] ^2 ( tut^\dagger ) . $$ by hermitian properties and unitarity it is not difficult to show the last two terms are zero and that the s-matrix commutes with the unitary matrix . the lorentz group then defines operator $p_\mu$ and $m_{\mu\nu}$ for momentum boosts and rotations . the $s$-matrix defines changes in momentum eigenstates , while the unitary operator is generated by a internal symmetries $a_a$ , where the index a is within some internal space ( the circle in the complex plane for example , and we then have with some $$ [ a_a , ~p_\mu ] ~=~ [ a_a , ~m_{\mu\nu} ] ~=~0 . $$ this is a sketch of the infamous “no-go” theorem of coleman and mundula . this is what prevents one from being able to place internal and external generators or symmetries on the same footing . the way around this problem is supersymmetry . the generators of the supergroup , or a graded lie algebra , have 1/2 commutator group elements $ [ a_a , ~a_b ] ~=~c_{ab}^ca_c$ ( $c_{ab}^c$ = structure constant of some lie algebra ) , plus another set of graded operators which obey $$ \{{\bar q}_a , q_b\}~=~\gamma^\mu_{ab}p_\mu , $$ which if one develops the susy algebra you find this is a loophole which allows for the intertwining of internal symmetries and spacetime generators . one might think of the above anti-commutator as saying the momentum operator , as a boundary operator $p_\mu~= -i\hbar\partial_\mu$ which has a cohomology , where it results from the application of a fermi-dirac operator $q_a$ . fermi-dirac states are such that only one particle can occupy a state , which has the topological content of $d^2~=~0$ . this cohomology is the basis for brst quantization .
a magnetic dipole transition can be modelled as a time-dependent perturbation $v_{\text{md}} ( t ) = {e\over 2 m} ( \vec{l} + 2\vec{s} ) \cdot \vec{b}e^{-i \omega t}$ . fermi 's golden rule tells us that the transition rate for $b-x , 1$ is proportional to the matrix element of the perturbation between the initial and final states , $$w \propto \langle \psi_b|{e\over 2 m} ( \vec{l} + 2\vec{s} ) \cdot \vec{b}|\psi_{x}\rangle , $$ where $|\psi_b\rangle$ is the excited state and $|\psi_{x}\rangle$ is the ground state ( with three possible $m_s$ values . ) the effect of $\vec l$ and $\vec s$ will be to turn the final state into some combination of the triplet states , but it will not change $j$ . therefore we might expect the transition to be ' spin-forbidden': $$w \propto \langle b^1\sigma_g^+ |x^3\sigma_{g , m_s=0 , \pm1}^-\rangle = \langle j=0 | j=1 \rangle = 0 . $$ this is where the spin-orbit coupling comes into play . spin-orbit coupling is the reason why the singlet $ ( b ) $ state has a higher energy than the triplet $ ( x ) $ states . it is a perturbation of the form $v_\text{so}={\mu\over\hbar}\vec{l}\cdot\vec{s}$ , which can be rewritten as $v_\text{so}={\mu \over 2\hbar} ( j^2-l^2-s^2 ) $ . in a spherically symmetric system like the helium atom , this perturbation commutes with the hamiltonian , so all you get is a shift in the energy of the triplet ( l=1 ) and singlet ( l=0 ) states . however , in a linear molecule like $o_2$ you lose the spherical symmetry , so $ [ l^2 , h ] \neq0$ and in addition to an energy shift , you also get some mixing of the unperturbed eigenstates , so that the excited state is not exactly $|b^1\sigma_g^+\rangle$ , but rather $|\psi_b\rangle = c_1|b^1\sigma_g^+\rangle + c_2|x^3\sigma_{g , m_s=0}^-\rangle$ . this mixing of j=0 and j=1 states is what allows $w$ to have a nonzero value . since we can write $s_x = s_+ + s_-$ , there will be a term in the transition rate like $$w\propto c_2^*\langle x^3\sigma_{g , m_s=0}^-|s_{\pm}|x^3\sigma_{g , m_s=\mp1}^-\rangle+\cdots \neq 0 . $$ does this help ? i know this is a bit hand-wavy so let me know if i can clarify anything .
as guillermo angeris correctly pointed out , this is essentially a numerical roundoff problem , not a physical situation . as a physical example , there are sungrazing comets that get very close to to sun , yet they maintain their original elliptical ( or hyperbolic ) orbit , without the orbit precessing a full third of a circle as you seem to be seeing . computationally , there are a few interesting issues . as kyle pointed out in a comment , many integration schemes are indeed unreliable in that roundoff error ( which is always present in floating-point computations ) can accumulate in a runaway feedback . indeed i often advise using leapfrog methods over euler ( used by box2d ) or even runge-kutta ( see for instance what is the correct way of integrating in astronomy simulations ? over at the computational science stackexchange ) . however , i suspect your problem is even simpler , in the sense that even an unstable numerical scheme should work for one or two orbits . given that everything is going wrong in just one pass , it seems that your timesteps are simply too large . a brief glimpse at the box2d documentation suggests you do not change the timestep mid-simulation , so i presume you are just using a good value to simulate the whole process in reasonable time . the problem is that when gravitating bodies get close in their orbit , they move quickly , sometimes very quickly . the way the code works is it updates each object 's position and velocity at each timestep , where the new velocity is determined by the force . as far as i can tell , this is done in line 206 of b2Island.cpp ( v . 2.2.1 ) : v += h * (b-&gt;m_gravityScale * gravity + b-&gt;m_invMass * b-&gt;m_force);  without looking at your code , i am guessing you simply calculate the gravitational force the body should feel at that moment , and have the simulation chug away . the problem is this moves the orbiting object in a straight line for the next timestep , and that straight line takes it too far away from the gravitating mass for that mass to properly curve its orbit into a closed ellipse . the quick schematic below shows the blue object moving to the tip of the red arrow , rather than staying on the path . physically , your timestep should be smaller than any timescale you encounter in the problem . now for an orbit conserving angular momentum , the product of the orbiting body 's mass , tangential velocity $v$ , and distance from the other object $r$ should be constant : $v \sim 1/r$ . at the same time , the acceleration $a$ it feels is given by newton 's law of gravity : $a \sim 1/r^2$ . so one natural timescale in this problem is $$ t \sim \frac{v}{a} \sim \frac{1/r}{1/r^2} \sim r $$ ( omitting dimensional constants ) , which goes to show that if your timescale is just barely small enough and then you tweak the orbit so as to half the periapsis distance ( distance of closest approach ) , then you would expect to need timesteps at least twice as small in order to preserve the integrity of the simulation .
the resonances are due mainly to plasma oscillations . but these metals are not simple plasmas ; there are interband transitions as well as collective plasma oscillations , and the frequencies of these various excitations can overlap causing interactions . in silver , there is no interband transitions at the plasm frequency , so the plasma resonance is not damped , whereas in other metal , that is not the case . even in silver interband transitions play a role in shifting and sharpening the resonance .
the state from $1\text{ atm}$ to $2\text{ atm}$ is normally called decompression or contraction . an equation you can use going from one state to the next is : $$\frac{p_1v_1}{t_1}=\frac{p_2v_2}{t_2}$$ where $p$ is pressure , $v$ is volume and $t$ is temperature . now if you want to calculate the force you have to know the surface area of what you are ( de ) compressing . the equation relating force and pressure is : $p=f/a$ , where $a$ is surface area . finally , keep in mind that you probably have to take into account outside pressure as well , because this has a substantial influence .
yes . i feel like there should be more of an explanation , but it is pretty straightforward . a blackbody absorbing energy will increase in mass . the absolute amount of increase is pretty miniscule , but it is not zero . since you ask about an object that does not also radiate energy , a blackhole might be a decent analogy . so , does a blackhole increase in mass when photons fall into it ? sure . if it helps , you can imagine that a photon of sufficiently high energy can produce pairs of electrons ( or other particles ) that could subsequently fall into the blackhole . . . well , except the antiparticle that would be annihilated shortly after creation . either way , it seems easier to imagine the scenario with particles that have a rest mass because it more closely corresponds to our quotidian experience .
yes , but one first has to generalize the classical 2-point brachistochrone problem $a \to b$ where the initial speed $v_a$ traditionally is zero , to the case where the initial speed $v_a$ may be non-zero but fixed . the solution to this initial speed brachistochrone problem ( assuming no friction ) is still a cycloid . now consider the 3-points brachistochrone problem $a \to b\to c$ with initial speed $v_a$ . the speed $v_b$ is given by energy conservation alone . thus the two segments $a \to b$ and $b \to c$ are completely decoupled , and they can be optimized as two independent 2-point brachistochrone problems with initial speeds $v_a$ and $v_b$ , respectively , leading to two corresponding cycloids $a \to b$ and $b \to c$ .
in the specific case of the hamiltonian the non-locality arises because the time evolution depends on values of the field which are arbitrary far away . in the one dimensional case we have \begin{equation} i \frac{\partial}{\partial t}\ , \psi ~~~=~~~ \tilde{h}\ , \psi ~~=\ \sqrt{~m^2+\mathbf{\tilde{p}}^2_x~}\ \psi\ =\ \nonumber \end{equation} \begin{equation} \sqrt{ ~m^2-\partial_x^2~}~~ \psi ~~=~~ \frac{m}{x} k_1\left ( mx\right ) ~*~ \psi ~~~~~~~ \end{equation} ( we used $\hbar=c=1$ ) . in the last term $*$ denotes a convolution , in this case with a bessel k function . it is clear that this instantaneous dependency violates the speed of light restriction . see also my stackexchange answer here : $\nabla$ and non-locality in simple relativistic model of quantum mechanics now in the general case the value of $\psi ( x ) $ will depend on $\psi ( y ) $ at other locations in the past an it will depend on other fields such as $a^\mu ( y ) $ at other locations in the past . mathematically these dependencies stem from " taylor-expanded series " of differential operators but as long as you do not violate the speed of light restriction then this is perfectly fine . hans
sounds like you are getting at the " coefficient of elasticity , " which is a value in [ 0,1 ] which represents what percent of the pre-collision kinetic energy is found after the collision . in homogeneous materials , the remainder of the energy is typically lost to deformation or heat ( phonons ) as you suggest . you could imagine , for the sake of argument , a steel ball hitting an object which has a spring with a retention device ( a locking lever of some sort ) . in this specialized case , the steel ball does in fact transfer a decent amount of recoverable potential energy into compressing the spring . it takes some other action , i.e. releasing the spring , to return that portion of the potential energy to kinetic . to be clear : suppose the steel ball has n joules of kinetic energy but the spring bottoms out at m
the problem in yours is that you are taking the net force acting downward to be $ ( m_2+m_3 ) g$ is incorrect and that led you to take the total mass to be $m_1+m_2+m_3$ which is again incorrect because $m_2\neq m_3$ . if $m_2=m_3$ then the center of mass of $m_2$ and $m_3$ will lie on the straight vertical line through the center of the pulley b and the force would act exactly at the center of the pulley b but $m_2\neq m_3$ so the center of mass will shift so at the center of the pulley b the effective mass , $m$ , due to which the net force is acting downward is to be found out . the net force acting is the two tension in the string where the masses $m_2$ and $m_3$ are suspended . from free body diagram of $m_2$ and $m_3$ tension $t$ can be found out and net force acting on pulley b will be $2t$ . $$f_{net}=2t=4m_2m_3g/ ( m_2+m_3 ) $$ $f_{net}/g=m$ where $m=4m_2m_3/ ( m_2+m_3 ) $ is the effective mass of $m_2$ and $m_3$ with pulley b so the new problem consists of two masses $m_1$ and $m$ with pulley a , $m$ replacing $m_2$ and $m_3$ . $f_{net}=mg$ and the total mass now is $m=m_1+m$ and $$a_0=f_{net}/m$$
$$ ( \psi^\dagger \gamma^0 \psi ) ^* = \psi^\dagger \gamma^0 \psi$$ because $\gamma^0$ is hermitian . also , $$ \begin{align} ( \psi^\dagger i \gamma^0 \gamma^\mu \partial_\mu \psi ) ^* and = -i \partial_\mu\psi^\dagger \gamma^{\mu\dagger} \gamma^0 \psi\\ and = -i \partial_\mu\psi^\dagger ( \gamma^0 \gamma^\mu \gamma^0 ) \gamma^0 \psi\\ and = -i \partial_\mu\psi^\dagger \gamma^0 \gamma^\mu \psi\\ and = i \psi^\dagger \gamma^0 \gamma^\mu \partial_\mu\psi + \mathrm{surface\ , \ , term}\\ \end{align} $$ for the second line i used $\gamma^{\mu\dagger} = \gamma^0 \gamma^\mu \gamma^0$ and for the last line i integrated by parts . i think your question hinges on this part , because the last " index " we sum over is the spacetime index $x^\mu$ , i.e. , integration . it is the same reason why the quantum mechanical momentum operator $p = i \tfrac{\partial}{\partial x}$ is hermitian . edit : something i glossed over is that the spinors are also grassmann numbers , so care has to be taken . in particular , this means that the components of the spinors satisfy $$ ( \psi_i \phi_j ) ^* = \phi_j^* \psi_i^*$$ ( more about that here ) . one already interchanges the objects when taking the hermitian conjugate by the rules of matrix algebra , and there is a temptation to want to introduce a minus sign because they are grassmann numbers , but this would be redundant . borrowing from the linked math . se answer : $$ \begin{align} ( \eta\xi ) ^* and = [ ( a+ib ) ( c+id ) ] ^*\\ and = ( ac-bd+ibc+iad ) ^*\\ and =ca-db-icb-ida\\ and = ( c-id ) ( a-ib ) =\xi^*\eta^* \end{align} $$
the fact that the radiation will fall off at $\frac{1}{r}$ will break the set of conditions required for the enveloping metric to stay asymptotically flat i am not sure this is right . there are various definitions of asymptotic flatness . older definitions were written in terms of coordinates , newer ones in terms of conformal transformations . the original motivation , as described in ch . 11 of wald , was to accomplish for gr what had already been done for e and m . in e and m in sr , the coordinate-based requirements given by wald are that the fields fall off like $1/r^2$ at $i^0$ , but only like $1/r$ at $\mathscr{i}^+$ . this is clearly designed to allow radiation . the definition of asymptotic flatness in wald is actually framed in a pretty restrictive context . he first gives a definition that is purely for a vacuum spacetime ( not electrovac ) , and then remarks that the definition carries over automatically to a spacetime in which there is a vacuum in some open neighborhood of the boundary . obviously it should be possible to extend this to a case in which the matter fields fall off fast enough , but it looks like he just wants to avoid making the already technical discussion even more technical . but the definition of asymptotic flatness for vacuum spacetimes definitely allows for spacetimes with gravitational radiation , since the adm energy , which is only defined in asymptotically flat spacetimes , includes the energy of gravitational radiation at null infinity . ( this could probably be checked explicitly by power-counting . for an asymptotically flat spacetime , the metric differs from minkowski by $o ( 1/v ) $ , where $v$ is an affine parameter defined in the lightlike direction . ) as further confirmation that these spacetimes with hawking radiation are asymptotically flat , you can find penrose diagrams for them . for example , there is one in figure 2.41 in penrose , cycles of time .
i will not get into theoretical details -- luboš ad marek did that better than i am able to . let me give an example instead : suppose that we need to calculate this integral : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3}$ here $y_{lm}$ -- are spherical harmonics and we integrate over the sphere $d\omega=\sin\theta d\theta d\phi$ . this kind of integrals appear over and over in , say , spectroscopy problems . let us calculate it for $m_1=m_2=m_3=0$: $\int d\omega ( y_{30} ) ^*y_{20}y_{10} = \frac{\sqrt{105}}{32\sqrt{\pi^3}}\int d\omega \cos\theta\ , ( 1-3\cos^2\theta ) ( 3\cos\theta-5\cos^3\theta ) =$ $ = \frac{\sqrt{105}}{32\sqrt{\pi^3}}\cdot 2\pi \int d\theta\ , \left ( 3\cos^2\theta\sin\theta-14\cos^4\theta\sin\theta+15\cos^6\theta\sin\theta\right ) =\frac{3}{2}\sqrt{\frac{3}{35\pi}}$ hard work , huh ? the problem is that we usually need to evaluate this for all values of $m_i$ . that is 7*5*3 = 105 integrals . so instead of doing all of them we got to exploit their symmetry . and that is exactly where the wigner-eckart theorem is useful : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3} = \langle l=3 , m_1| y_{2m_2} | l=1 , m_3\rangle = c_{m_1m_2m_3}^{3\ , 2\ , 1} ( 3||y_2||1 ) $ $c_{m_1m_2m_3}^{j_1j_2j_3}$ -- are the clebsch-gordan coefficients $ ( 3||y_2||1 ) $ -- is the reduced matrix element which we can derive from our expression for $m_1=m_2=m_3=0$: $\frac{3}{2}\sqrt{\frac{3}{35\pi}} = c_{0\ , 0\ , 0}^{3\ , 2\ , 1} ( 3||y_2||1 ) \quad \rightarrow \quad ( 3||y_2||1 ) =\frac{1}{2}\sqrt{\frac{3}{\pi}}$ so the final answer for our integral is : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3}=\sqrt{\frac{3}{4\pi}}c_{m_1m_2m_3}^{3\ , 2\ , 1}$ it is reduced to calculation of the clebsch-gordan coefficient and there are a lot of , tables , programs , reduction and summation formulae to work with them .
aqwis , it would help in the future if you mentioned something about your background because it helps to know what level to aim at in the answer . i will assume you know e and m at an undergraduate level . if you do not then some of this explanation probably will not make much sense . part one goes back to dirac . in e and m we need to specify a vector potential $a_\mu$ . classically the electric and magnetic fields suffice , but when quantum mechanics is included you need $a_\mu$ . the vector potential is only defined up to gauge transformations $a_\mu \rightarrow g ( x ) ( a_\mu + \frac{i}{e} \partial_\mu ) g^{-1} ( x ) $ where $g ( x ) =\exp ( i \alpha ( x ) ) $ . the group involved in these gauge transformations is the real line ( that is the space of possible values of $\alpha$ ) if electric charge is not quantized , but if charge is quantized , as all evidence points to experimentally , then the group is compact , that is it is topologically a circle , $s^1$ . so to specify a gauge field we specify an element of $s^1$ at every point in spacetime . now suppose we do not know for sure what goes on inside some region ( because we do not know physics at short distances ) . surround this region with a sphere . we can define our gauge transformation at every point outside this region , but now we have to specify it on two-spheres which cannot be contracted to a point . at a fixed radial distance the total space of angles plus the gauge transformation can be a simple product , $s^2 \times s^1$ but it turns out there are other possibilities . in particular you can make what is called a principal fibre bundle where the $s^1$ twists in a certain way as you move around the $s^2$ . these are characterized by an integer $n$ , and a short calculation which you can find various places in the literature shows that the integer $n$ is nothing but the magnetic monopole charge of the configuration you have defined . so charge quantization leads to the ability to define configurations which are magnetic monopoles . so far there is no guarantee that there are finite energy objects which correspond to these fields . to figure out if they are finite energy we need to know what goes on all the way down to the origin inside our region . part two is that in essentially all models that try to unify the standard model you find that there are in fact magnetic monopoles of finite energy . in grand unified theories this goes back to work of ' t hooft and polyakov . it also turns out to be true in kaluza-klein theory and in string theory . so there are three compelling reasons to expect that magnetic monopoles exist . the first is the beauty of a deep symmetry of maxwell 's equations called electric-magnetic duality , the second is that electric charge appears to be quantized experimentally and this allows you to define configurations with quantized magnetic monopole charge , and the third is that when you look into the interior of these objects in essentially all unified theories you find that the monopoles have finite energy .
you may always add the numbers in front of the units , and if the units are the same , one could argue that the addition satisfies the rules of dimensional analysis . however , it still does not imply that it is meaningful to sum the temperatures . in other words , it does not mean that these sums of numbers have natural physical interpretations . if one adds them , he should add the absolute temperatures ( in kelvins ) because in that case , one is basically adding " energies per degree of freedom " , and it makes sense to add energies . adding numbers in front of " celsius degrees " , i.e. non-absolute temperatures , is physically meaningless , unless one is computing an average of a sort . this is a point that famously drove richard feynman up the wall . read judging books by their covers and search for " temperature " . he was really mad about a textbook that wanted to force children to add numbers by asking them to calculate the " total temperature " , a physically meaningless concept . it only makes sense to add figures with the units of " celsius degrees " if these quantities are inteprreted as temperature differences , not temperatures . as a unit of temperature different , one celsius degree is exactly the same thing as one kelvin . if you interpolate or extrapolate a function of the temperature , $f ( t ) $ , you do it as you would do it for any other function , ignoring the information that the independent variable is the temperature . results of simplest extrapolation/interpolation techniques will not depend on the units of temperatures you used .
water at 20c and atmospheric pressure has a higher symmetry than ice vii at 100c and 10gpa pressure . you may well point out this is cheating because the pressure is different in the two cases . however this makes the point that temperature is not the only variable . if you are looking at a phase transition between a disordered and ordered phase then you need to consider the gibbs free energies of the two phases . the phase with the lower gibbs free energy is the one that will form . the gibbs free energy is defined as : $$ g = h - ts $$ and a negative gibbs free energy change means the phase change occurs ( give or take a few kinetic barriers ) . generally speaking , for a change from ordered to disordered the entropy increases . i.e. $\delta s$ is positive . if you consider an isothermal change you get a negative $-t\delta s$ contribution that gets more negative as the temperature gets higher , so you would expect that in general changes from ordered to disordered will occur at increased temperature . this does not mean it is impossible to get a disorder to order transition with increasing temperature , but the enthalpy , $h$ , would have to have an odd temperature dependance to outweight the entropic effect .
first of all , the vev of $\phi$ scales like a positive power of $\mu$ which has the units of mass . so all the effects of the symmetry breaking scale like a positive power of the mass scale $\mu$ . at energies $e$ satisfying $e\gg \mu$ , i.e. much higher than $\mu$ , the value of $\mu$ itself as well as vev and other things may simply be neglected relatively to $e$: all the corrections from the symmetry breaking go to zero , relatively speaking . in the uv , i.e. at short distances , the dimensionless couplings ( and interactions ) such as $\lambda$ are much more important than the positive-mass-dimension dimensionful couplings such as $\mu$ . it is still true that the high-energy theory was not exactly symmetric in the treatment above ; it was just approximately symmetric . however , in fact , sometimes it is exactly symmetric under the sign flip of $\phi$ . that is because the value of $\mu$ runs , as in the renormalization group , and in many interesting theories , $\mu$ actually switches the sign at energies exceeding some critical energy scale $e_0$ . so the high energy theory may have a single symmetry-preserving minimum of the potential and the symmetry breaking may be a result of the flow to low energies . it is legitimate you want to know the higher-loop corrections to the higgs potential but the subtlety you should appreciate is that all these calculations proceed relatively to some renormalization ( mass ) scale . if the scale is low , the behavior of the theory at much higher energies may only be deduced by the ( inverted ) rg flows . however , the deduction of the short-distance theory from its long-distance effective field theory approximation is never exact ; it is the high-energy theory that is the legitimate starting point and the low-energy physics is its consequence .
key is to notice that your steps provide you with a unit length as well as a unit time . so , let 's measure distance in $steps$ and time in $ticks$ , with your speed being $1 \ step/tick$ . the length of the train is $x$ steps , and its speed is $v \ steps/tick$ ( $v&lt ; 1$ ) . it follows that $$x \ + \ 18 \ v \ = \ 18 $$ $$x \ - \ 11 \ v \ = \ 11 $$ adding 11x the first equation to 18x the second yields $29 x = 396$ . the train is $396/29 \ steps$ long . you also need to check if indeed $v &lt ; 1 \ step/tick$ . leave that to you to demonstrate .
an orbit is stable because of conservation of angular momentum . suppose we start with an object in an exactly circular orbit and slow it down slightly . that means it is moving at less than orbital velocity so it starts to fall inwards . however as its distance to the sun decreases the tangential component of its velocity has to increase to conserve angular momentum . so as the object nears the sun it moves faster and faster , and at its closest approach to the sun it is moving at well above orbital velocity so it starts to move outwards again . you end up with an elliptical orbit : ( this diagram shamelessly cribbed from google images ) it is actually very difficult to get something orbiting a star to fall into it , because you have to reduce the tangential velocity to zero . at the distance of the earth from the sun the orbital velocity is 108,000 km/h . you would have to slow the earth by this amount to make it fall into the sun , and fortunately no meteorite is likely to do that . on a side note , nasa recently sent the messenger spaceship to study mercury , and getting the ship to mercury was hard because of the need to shed all that orbital velocity . even though mercury is a lot closer to the sun than the earth is you can not just fall there . messenger had to use several gravity assists to shed enough speed to allow it to orbit mercury .
what you want to do is keep the angle between your direction of motion and the line of sight to police car the same as the angle between the truck 's direction of motion and the truck 's line of sight to the police car . in other words , we want to keep $a1=a2$ in the picture above . this is a problem in similar triangles . the answer will be that the ratio of your motion to your distance to the cop will have to be the same as the ratio of the truck 's motion to the truck 's distance to the cop . this can be seen from the following : we note that $$\cot{ ( a1 ) }=\frac{\textrm{truck&#39 ; s speed}}{d1}$$ $$\cot{ ( a2 ) }=\frac{\textrm{car&#39 ; s speed}}{d2}$$ set $a1=a2$ so we are always hidden behind the truck and solve for $\textrm{car&#39 ; s speed}$: $$a1=a2$$ $$\cot{ ( a1 ) }=\cot{ ( a2 ) }$$ $$\frac{\textrm{truck&#39 ; s speed}}{d1}=\frac{\textrm{car&#39 ; s speed}}{d2}$$ $$\textrm{car&#39 ; s speed}=\frac{\textrm{truck&#39 ; s speed}\times d2}{d1}$$ so , suppose the lanes are the same size ( 10 feet wide , say ) , and the cop is 5 feet off of the highway . then , $d1=5\textrm{ feet}$ , $d2=15\textrm{ feet}$ . the speed you need if the truck is going 65 miles per hour is $$\textrm{car&#39 ; s speed}=\frac{65 \textrm{mph}\times 15}{5}=195\textrm{ mph}$$ edit : some concerns were raised in the comments that this treats the truck as a point . this turns out not to matter . here 's a second picture like the first , but now we have a zone ( colored in green ) which the truck covers . the green triangle gives you a little bit of wiggle room , since you can be covered by the front of the truck or the back or anything in between . however , the total size of your wiggle room does not change while you move ( in other words , it does not depend on a4 ) . as a result , it should be pretty clear that this does not change things much at all - we can think of it as two point-size trucks going at the same speed , and we have to stay between them . of course , this will give exactly the same answer as the first case - it is really just like hiding behind one point-sized truck . there is actually one small change , as david notes : if you start out covered by the front of the truck , you can go a little slower than the 195 mph cited above , because you can slowly slide back until covered by the back of the truck . however , if the length of the truck is $l_{tr}=40\textrm{ feet}$ ( say ) , then this change in the velocity is quite small . for example , suppose that we slide back 40 feet from the front of the truck to the back over the course of a mile . we are going 195 miles per hour , so it takes us 18.5 seconds to go one mile . in those 18.5 seconds , we move 40 feet relative to the truck ; this is a speed of about 1.5 mph . so , we can go 1.5 mph slower if we start at the front and go to the back over a mile ; taking this into consideration , we get that the speed needed is actually 193.5 mph .
if the airy disk is smaller than a pixel ( rather common ) , then you want to defocus . star trackers on satellites do this in order to get sub-pixel pointing accuracy . if the airy disk is much larger than a pixel , then you probably do not want to defocus . in the latter case the situation is complicated by aberrations and the problem of modeling the shape of the spot on the focal plane , which in general is no longer circular . that modeling problem might be more accurate for the in-focus case . i suppose as a practical matter one might do the best one can in the design stage , but then actually measure point spread functions for various angles . that is , calibrate the actual device . ( that is speculation . ) but if you calculate the size of the airy disk for typical devices you will find that it is generally smaller than a pixel , so defocussing usually wins .
update to address new questions . the answer to this question is no . at least if you take the question purely formally . only theories such as classical field theory , quantum field theory and continuum mechanics are field theories ( you generally recognize them by having continuous degrees of freedom ; also they usually have the word field in the title :- ) ) . but physically , lots of different theories may be equivalent , or may be approximations of some other theory , so there are many connections among them ( this is the point i was trying to illustrate , but maybe i overemphasized it ) . difference between qm and qft is essentially the same as between classical mechanics and classical field theory . in the mechanics you have just a few particles ( or more generally , small number of degrees of freedom ) , while fields have an infinite number of degrees of freedom . naturally , field theories are a lot harder than the corresponding mechanics . but there is a connection i already mentioned : you can see what happens when you let the number of particles grow arbitrarily large . this system will then essentially behave as a field theory . so in a sense , we can say that field theory is a large $n$ ( number of degrees of freedom ) limit of the corresponding mechanical theory . of course , this view is very simplified , but i do not want to get too technical here . field theory is a theory that studies fields . now what is a field ? i suppose everyone should be familiar with at least some of them , e.g. gravitational or electromagnetic ( em ) field . now , how do you recognize that object is a field ? well , essentially , you look at how complicated the object is . to make this more precise : main objects of study of classical mechanics are point particles . all you need to keep track of them is just few parameters ( position , velocity ) . on the other hand , consider the em field : you need to keep track of the data ( electric and magnetic field vector ) in every point of the universe , so there is infinitely many parameters of this system ! this is what i meant by system being large : you need a lot of data to describe it . now , it might seem that something is amiss . you do need a lot of data to describe real objects ( just think of how many atoms there are in the grain of sand ) . so are ordinary objects fields ? yes and no , both answers are correct depending on your point of view . if you consider a massive object as essentially being described by few parameters ( like center of mass velocity and moment of inertia ) and completely ignore all information about atoms then it is clearly not a field . nevertheless , at the microscopic level , atoms wiggle around and even the grain of sand really is as complicated object as any em field ( not to mention that atoms themselves produce em field ) , so it is certainly correct to call them that . now let us see where our definition of field takes us . let 's talk about quantum mechanics for a while . what about two quantum particles ? is it a field ? well , clearly not . what about three ? still not . and what if we keep adding particles so that there will be a huge number of them ? well , it turns out that we will get a quantum field ! this is precisely the correspondence between e.g. photons and quantum em field . you can either look at em field as being described by vector of electric and magnetic field at every point as in the classical case , or you can instead reorganize your data so that you keep track of what kind of photons you have . it is useful to carry both pictures in head and use the more appropriate one . there is also a subject of continuum mechanics . there you can also start with particles ( describing atoms in some real object , e.g. water ) and because there are so many of them , you can again reorganize your data , consider the object as being essentially continuous ( which real objects surely are at least unless you look at them with a microscope ) , and instead describe them by parameters such as pressure and temperature at every point . to summarize : the field theory is essentially about dealing with large objects . however , when we are looking at the problem with particle hat on , we usually do not say it is a field . for instance , when describing real objects as consisting of atoms , we are usually talking about statistical mechanics , or condensed matter physics . only when we move to the realm of continuum mechanics , we say that there are fields . there is much more to be said on the topic but this post got already too long so i will stop here . if you have any questions , ask away !
i suppose that " periodic movement " need not be oscillatory . for example , consider an object that moves right for one minute , stops for one minute and repeats . this would be periodic movement in the sense of occuring at regular intervals . and , while oscillation implies back and forth movement ( if we are still talking about movement ) , it need not be periodic .
to formalize dushya 's comment as an answer : since the kilogram is an arbitrary , man-made unit , the actual numerical value of the proton mass in kilograms is meaningless ( i.e. . it is as good as its value in pounds , ounces , stones , solar masses , $\textrm{mt}/c^2$ , etc . ) . the true fundamental constants of nature are dimensionless : they have the same value in every unit system . thus dimensional constants like $c$ , $\hbar$ , $g$ , and indeed $m_p$ and $m_e$ , are not very meaningful and can be set to $1$ with a judicious choice of units ( which is done quite often ) . true fundamental constants are often ratios of dimensional quantities such as the fine structure constant , $$\alpha=\frac{e^2/4\pi\epsilon_0}{\hbar c} , $$ which quantifies how strong , on a quantum scale , the electromagnetic interaction is . in terms of mass , the constants you had like to predict are things like the ratio $m_p/m_e\approx 1800$ , and so on . given that , the formula you have found is just a fluke : a consequence of the fact that we chose as our basic unit of mass the mass of a cube of water whose sides measure one hundred-millionth of a quarter of a meridian . edit , given the long comment thread : @fred , let me try and rephrase this a bit to see if i can bring out the arbitrariness we are talking about well up to the surface . the real number you have discovered is the inverse of the one you posted : $$\frac{10^{26}}{\sum_{m=1}^\infty \frac{1}{ ( m^2+1 ) _{2m}}}\approx 5.978638 \times 10^{26} , $$ which appears to approximate within experimental error the number of protons and neutrons that will fit - at sea level and at " room " temperature - a cubical box about yea big in side containing that particular common chemical that you find in drinking fountains , kitchen sinks , lakes , and even falling out of the sky ( on earth ) rather often .   since the proton really is quite fundamental , any stabilizing influence of the fractalness needs to account for the size of the earth , its predominant climate a hundred years ago , the abundance of water in it , and the detailed chemical state of the brains of a number of mainly french gentlemen that sat down a while ago to try and make unit systems ( which are always arbitrary ) at least simple to work with .
your equation is the right solution to schrödinger 's equation in the momentum-energy representation . however , it is only that simple for schrödinger 's equation with no potential , $v ( x , y , z ) =0$ . if it is zero , the solution ( or , similarly , the reformulation of the equation ) is as easy as the algebraic relationship you wrote - but it is also uninteresting for the same reason . the interesting cases have e.g. the coulomb potential $k/r$ or the harmonic oscillator potential $kx^2/2$ and they can not be " solved " in the simple way you sketched . for a nonzero potential , the problem is genuinely equivalent to a partial differential equation . however , that does not mean that it is the only way in which the problem may be formulated or solved . both the harmonic oscillator and the hydrogen atom may be solved ( i.e. . their spectrum may be found ) algebraically , by the creation and annihilation operators in the harmonic oscillator case , or by a hidden $so ( 4 ) $ symmetry in the hydrogen atom case . a general schrödinger 's equation in quantum mechanics is really an ordinary differential equation for the state vector ; the " spatial derivatives " only appear as the action of particular operators on the hilbert space . some of these operators - e.g. the momenta in the position representations - are conveniently represented as partial derivatives with respect to spatial coordinates but that is only the case if we use a " continuous basis " for the hilbert space .
the non-relativistic expression for the wave operators $$ ω±=lim{_{t→∓∞}}u ( t ) {_{full}}u{_{0}} ( t ) , $$ needs revision in field-theoretic situations since usually the free and interacting fields act in different hilbert spaces . an early example is given in " asymptotic conditions and infrared divergences in quantum electrodynamics " by p . p . kulish and l . d . faddeev . theoretical and mathematical physics 1970 , volume 4 , issue 2 , pp 745-757 . thus i expect that there is no simple answer to this question .
physics should not depend on the system of units you choose to solve any problem , otherwise physics would in europe will be different than physics in the us , which is stupid ! the answer to your question depends on the initial definition of units you use in your problem . you start with si units , you end with joules . you start with cgs , you end with ergs . you start with natural units , you end with ev .
but the magnetic field of the electron cannot couple to its own spin ? or can it ? how do i explain the energy spit in this reference frame ? in classical em theory , the common explanation of the ls term from the frame of the electron is not very convincing , because this frame is non-inertial and there are potentially all kinds of non-inertial forces which were not discussed . explaining this in the frame of nucleus seems as an easier task . if we imagine charged rotating and orbiting ball in a central electric field of the nucleus , this central electric field affects the orbital motion of the ball ; the ball accelerates , which changes its own electric and magnetic field . these fields of the ball then could influence the rotation of the ball . i do not know if it gives something close to the ls term , but it seems possible .
the radius of a non-rotating black hole is $$r_s = \frac{2gm}{c^2} \tag{1}$$ where $m$ is the mass , $g$ is newton 's constant , and $c$ is the speed of light . this is the distance from the center of the black hole to the event horizon . the event horizon is the surface that traps light and objects , it separates inside and outside the black hole . anything that passes inside the event horizon can never escape , even light . this is why it is said that the escape velocity at the surface of a black hole is $c$ . it is also the case that any object with $r &lt ; r_s$ is a black hole . but since the mass should be roughly proportional to the volume , and the volume is proportional to $r^3$ , anything heavy enough will form a black hole . therefore from ( 1 ) the proper answer to your question is : because they are very massive , not because they are small .
hard disks use magnetic storage , http://en.wikipedia.org/wiki/magnetic_storage i.e. the information is stored in the south/north orientation of small pieces of the magnetic medium ( also the case for tapes in the old tape recorders ; and credit cards ; these applications differ by the " geometry " how the bits are arranged but not by the essence how a bit is stored ) . a stronger magnet in the head is able to remagnetize the pieces of the surface ; a weaker magnet is able to " feel " in what way the pieces are magnetized . usd flash disks and ssds use flash memory , http://en.wikipedia.org/wiki/flash_memory which evolved from eeprom memory chips . flash memory is composed of flash memory cells which primary contain the floating-gate transistors ; such cells have a control gate ( cg ) and de facto isolated floating gate ( fg ) . fg can keep electric charge for years and the electric field from that affects the threshold voltage of the cell ( voltage at which the cell becomes conductive ) . some extra operations are needed to erase and rewrite the chips which can not be done indefinitely ; after less than a million of erasures , the cell breaks down . there are many other physical mechanisms in which the information is being stored . all the " relatively easily " rewritable technologies use some kind of variable electric or magnetic fields that may be produced by matter . compact disks ( cd/dvd ) use optical information and so on .
it is best to always stick to the rigorous formulation of the second law , which comes in two parts . quote from the book " fundamentals of statistical and thermal physics " by f . reif : 1 ) in any process in which a thermally isolated system goes from one macrostate to another , the entropy tends to increase , i.e. , $$\delta s\geq 0$$ 2 ) if the system is not isolated and undergoes a quasi-static infinitesimal process in which it absorbs heat $dq$ , then $$ds = \frac{dq}{t}$$ combining 1 ) and 2 ) in the form of statements such as $ds\geq\frac{dq}{t}$ are weaker than the second law as formulated above , as you need to make additional assumptions that would make this valid . it is best to never use such statements directly and always stick to the modern rigorous formulation of the second law that does not assume that temperature is definable during any non quasi-static change . the paradox you consider is resolved when you consider that the system is not in thermal equilibrium . note that you are in principle free to define the thermodynamic description of a given physical system . so , the physical system is described exactly by specifying its microstate , and the thermodynamic description is a course grained description of it where you keep only a few macroscopic parameters . now , what then can happen is that a slightly less course grained description would have allowed equilibrium thermodynamics to be applicable to the system . this is the case here . so , you have two choice . you if you describe the system as one isolated system , then temperature is not defined and you can only use the first part of the second law which doesn ; t invoke a notion of temperature . however , if you take a sligly less course graiend look at the system , you see that you have two subsystems , the changes in entropy of the system can be described by the second part of the second law , provided the changes are quasi-static .
dear lurscher , the quote is the kind of c-physics described by the c-word which is a favorite word of mine but is discouraged on this server , so i will not use it - but you have used it . you do not misunderstand anything - quite on the contrary , you are right on the money . these comments about a non-existent test of parity in the equivalence principle are due to " uncle al " schwartz , see e.g. the first comment under http://www.iac.es/galeria/masc/outreach_files/dark-energy_particle_spotted_naturenews290609.pdf to see that even extremely microscopic details about the " idea " coincide with the paragraph you quoted . " uncle al " is known to most physics bloggers - as well as many other physics forums . from a physics viewpoint , the claims are completely preposterous . first of all , one needs pretty special objects to get a parity-violating physics in 4 spacetime dimensions : either chiral ( weyl ) spinors or self-dual 2-forms ( and their interactions ) . none of those things is included in gr ; einstein-cartan theory ; conventional affine , teleparallel , and noncommutative gravitational theories - which is why all these gravitational theories automatically preserve parity . moreover , self-dual 2-forms do not exist in 3+1 dimensions ( although they do exist in 4+0 dimensions ) because $*^2=-1$ in 3+1 dimensions . as you correctly say , only the weak interactions as expressed by the electroweak theory violate parity which is because the gauge field interacts with chiral ( weyl ) spinors . the gauge fields $a_\mu$ , when coupled to spinors , get naturally multiplied by $\gamma^\mu$ which switches the chirality ( because it anticommutes with $\gamma_5$ ) ; on the other hand , $g_{\mu\nu}$ with an even number of indices preserves the chirality , so the spinors interactions with the metric have to be parity-preserving . moreover , the existing tests of the equivalence principle de facto eliminate the possibility of substantially different interactions of left-handed and right-handed helixes , too . for example , newton 's apple contains vitamin c ( l-ascorbic acid ) which is parity-asymmetric , and it would accelerate differently if the laws of gravity cared about the mirror images . there is really no way compatible with an effective field theory how to make the two mirror objects accelerate differently . for some semi-serious paper by authors from well-known places who talk about gravitational parity violation , see e.g. http://arxiv.org/abs/arxiv:1005.3310 and the papers mentioned by it ( by stephon alexander and others ) . it would still be impossible for me to say that any scenario in those papers is defensible but at least those papers do not sound as a self-evident nonsense of the kind that " uncle al " schwartz has managed to incorporate into the wikipedia page .
the centre of mass is the point at which our collection of objects will balance if we put a pivot there . let 's call this point $\bf r$ . the vector joining the point $i$ to $\bf r$ is simply $\bf r_i - \bf r$ . the force acting at this point is $m_i \bf g$ , so the torque at the point $i$ is : $$ \bf t_i = m_i \bf g \times ( \bf r_i - \bf r ) $$ the total torque must sum to zero , because that is how we define the centre of mass , so : $$ \sum m_i \bf g \times ( \bf r_i - \bf r ) = 0$$ and the cross product is distributive over addition so we can take it outside the sum : $$ \bf g \times \sum m_i \bf ( \bf r_i - \bf r ) = 0$$ and this can ony be satisfied if : $$ \sum m_i \bf ( \bf r_i - \bf r ) = 0$$
an oscillator is usually characterized by its quality factor q . this is a dimensionless parameter which measures how " good " of an oscillator it is . it also relates to the quantity you are interested in - a linear , damped oscillator will exhibit a lorentzian peaked response in the frequency domain . the bandwidth of the resonance ( points where the response is decreased to 50% ) is given by $\delta f = \frac{f_0}{q}$ . the quality factor can also be related to the damping coefficient - for more info check out wikipedia : http://en.wikipedia.org/wiki/q_factor
actually , no , something different is going on . in materials , the reason light travels slower than $c$ is because the photons occasionally hit atoms and are absorbed , then re-emitted . so the average velocity with which they propagate is less than the speed of light . however , between interactions with the atoms , the photon does travel at speed $c$ . ( n . b . this is a simplification of the real quantum-mechanical picture , though it yields the same results ) there is no evidence to suggest that the photon has a mass . the exception would be in superconductors , where the photon does acquire some sort of effective mass , but i am not completely familiar with the details of that process .
i ) concerning an action principle $s=\int\ ! dt~ l$ , let us assume that the lagrangian is of the form $$\tag{1} l~=~t-u , $$ where $t$ is the kinetic term , and $u ( {\bf r} , \dot{\bf r} , \ddot{\bf r} , \dddot{\bf r} , \ldots ; t ) $ is a generalized potential , which we would like to find . the generalized potential $u$ should satisfy $$\tag{2} {\bf f}~=~-\frac{\partial u}{\partial {\bf r}} +\frac{d}{dt}\frac{\partial u}{\partial \dot{\bf r}} -\frac{d^2}{dt^2}\frac{\partial u}{\partial \ddot{\bf r}} +\frac{d^3}{dt^3}\frac{\partial u}{\partial \dddot{\bf r}} - \ldots , $$ where ${\bf f} ( {\bf r} , \dot{\bf r} , \ddot{\bf r} , \dddot{\bf r} , \ldots ; t ) $ is a given total force on the point particle . ii ) let us for fun consider a force proportional to the $n$'th time-derivative of the position $$\tag{3} {\bf f}~=~-k \frac{d^n{\bf r}}{dt^n} $$ for any non-negative integer $n\in\mathbb{n}_0$ . for an even integer $n$ , we can use the generalized potential $$\tag{4} u~=~ ( -1 ) ^{\frac{n}{2}}\frac{k}{2} \left ( \frac{d^{\frac{n}{2}}{\bf r}}{dt^{\frac{n}{2}}} \right ) ^2 . $$ the case $n=0$ of a force proportional to the position $$\tag{5} {\bf f}~=~-k {\bf r} , \qquad u ~=~\frac{k}{2}{\bf r}^2 , \qquad k~&gt ; ~0 , $$ is the well-known hooke 's law/harmonic oscillator . the case $n=2$ of an applied force proportional to the acceleration $$\tag{6} {\bf f}~=~-k \ddot{\bf r} , \qquad u ~=~-\frac{k}{2}\dot{\bf r}^2 , \qquad $$ behaves like a ( non-relativistic ) kinetic term . the case $n=1$ of a friction force proportional to the velocity $$\tag{7} {\bf f}~=~-k \dot{\bf r} , \qquad k~&gt ; ~0 , $$ is discussed in e.g. this phys . se post and this mathoverflow post . more generally , using very similar methods as in these two posts , one may show that it is impossible to assign a generalized potential $u$ to the force ( 3 ) for any odd positive integer $n$ . so in particular , the case $n=3$ , the abraham-lorentz force $^1$ proportional to the jerk $$\tag{8} {\bf f}~=~-k \dddot{\bf r}\qquad k~&lt ; ~0 , $$ does not have a generalized potential $u$ . -- $^1$ however , see also this related phys . se post .
since you are specifically asking for references , the only reference on this subject that i am aware of is an old artlcle by wu-ki tung : " relativistic wave equations and field theory for arbitrary spin " phys rev vol 156 #5 pp1385-1398 apr 1967 it may be available online somewhere . it talks about the extra restrictions you might want to impose in order to obtain reasonable field equations and talks a little about what is special about the cases of spin 0 , 1/2 and 1 .
the individual streamline with velocity zero may not make much sense on its own , but it often does when you consider the bulk of the fluid as a whole . in the case of the surface of a body immersed in a fluid , you could trace a streamline starting at a point infinitesimally close to the surface , where the velocity would be infinitesimally small , but non-zero . the streamline on the surface would be the limit of the streamlines as your starting point moves towards the boundary . such analysis cannot be performed on stagnant fluid , i.e. it makes no sense talking of streamlines in the bulk of a stationary fluid .
if you use a plate glass window instead of a wall you will find that the rubber and iron balls bounce by a similar amount ( though be careful throwing iron balls at windows :- ) . it is a basic principle in physics that energy cannot be lost . the rubber ball starts off with kinetic energy , hits the wall , and rebounds moving with about the same kinetic energy . so no energy is lost . if the iron ball does not bounce it must mean that the energy it originally had has been transferred to the wall . rubber balls are soft , so they decelerate relatively slowly and they deform and spread out as they hit the wall . this means that the pressure they exert on the wall while they are bouncing is relatively low . by contrast an iron ball is very hard so it stops very suddenly and all the force it exerts on the wall is concentrated on a small area . that means the pressure is high enough to damage the wall . it might cause a visible dent , or it might just cause cracks within the wall that you can not see . in both cases energy is used in damaging the wall , and this energy comes from the motion of the ball . that means little energy is left for the iron ball to bounce back . i started by saying the iron ball would bounce off plate glass . this is because plate glass is very rigid and provided you do not shatter it the glass is not damaged by the iron ball . since no energy is absorbed by the glass , the iron ball bounces back just as the rubber ball does .
you can apply the lorentz force equation $f=qv\times b$ at the microscopic level , since the magnet is made out of charged particles . however , it is not practical to do this for a ferromagnetic object . the electrons in the ferromagnetic material also have intrinsic spin 1/2 and an intrinsic dipole moment , and they therefore experience an additional force in a field gradient . this force is not described by the lorentz equation . a complete , realistic calculation is going to be extremely difficult . you could solve maxwell 's equations numerically , putting in the correct permeability . note that you are going to have hysteresis effects .
the area under the curve in the pv-diagram is the integral $$ \int p \ ; \mathrm dv = \int \frac fa a\ ; \mathrm ds=\int f \ ; \mathrm ds \equiv w $$ by definition of pressure as force per area and ( infinitesimal ) volume as area times distance . this is the mechanical work done by the system on the environment in case of expansion or by the environment on the system in case of compression , which differ by sign . it is called external to emphasize the interaction with the environment .
you had freeze to death faster in the atlantic ocean . space has essentially no thermal conductivity . all the heat you lose will be radiated away . according to the stefan-boltzman law , $w = \sigma t^4$ , you would lose at most 500 watts per square meter of body surface area . by contrast , the convective heat transfer coefficient in water is about 12,500 watts/square meter / degree kelvin temperature difference . so , i think freezing would be the least of your concerns .
this is a heavy question , that contains many topics in it that are worthy of their own questions , so i am not going to give a complete answer . i am relying mainly on this excellent review paper by nayak , simon , stern , freedman and das sarma . the first part can be skipped by anyone already familiar with anyons . abelian and non-abelian anyons anyons are emergent quasiparticles in two dimensional systems that have exchange statistics which are neither fermionic nor bosonic . a system that contains anyonic quasiparticles has a ground state that is separated by a gap from the rest of the spectrum . we can move the quasiparticles around adiabatically , and as long as the energy we put in the system is lower than the gap we will not excite it and it will remain in the ground state . this is partly why we say the system is topologically protected by the gap . the simpler case is when the system contains abelian anyons , in which case the ground state is non-degenerate ( i.e. . one dimensional ) . when two quasiparticles are adiabatically exchanged we know the system cannot leave the ground state , so the only thing that can happen is that the ground state wavefunction is multiplied by a phase $e^{i \theta}$ . if these were just fermions or bosons than we would have $\theta=\pi$ or $\theta=0$ respectively , but for anyons $\theta$ can have other values . the more interesting case is non-abelian anyons where the ground state is degenerate ( so it is in fact a ground space ) . in this case the exchange of quasiparticles can have a more complicated effect on the ground space than just a phase , most generally such an exchange applies a unitary matrix $u$ on the ground space ( the name ' non-abelian ' comes from the fact that these matrices do not in general commute with each other ) . the quantum dimension so we know that the ground space of a system with non-abelian anyons is degenerate , but what can we say about its dimension ? we expect that the more quasiparticles we have in the system , the larger the dimension will be . indeed it turns out that for $m$ quasiparticles , the dimension of the ground space for large $m$ is roughly $\sim d_a^{m-2}$ where $d_a$ is a number that depends on $a$ - the type of the quasiparticles in the system . this scaling law is reminiscent of the scaling of the dimension of a tensor product of multiple hilbert spaces of dimension $d_a$ , and for this reason $d_a$ is called the quantum dimension of a quasiparticle of type $a$ . you can think of it as the asymptotic degeneracy per particle . for abelian anyons we have a one-dimensional ground space no matter how many quasiparticles are in the system , so for them $d_a=1$ . although we used the analogy to a tensor product of hilbert spaces , note that in that case the dimension of each hilbert space is an integer , while the quantum dimension is in general not an integer . this is an important property of non-abelian anyons that differentiates them from just a set of particles with local hilbert spaces - the ground space of non-abelian anyons is highly nonlocal . more details on anyons and the quantum dimension can be found in the review paper cited above . the quantum dimension can be generalized to other systems with topological properties , maintaining the same intuitive meaning of asymptotic degeneracy per particle . it is in general very hard to calculate the quantum dimension , and there is only a handful of papers that do ( most of them cited in the paper by kitaev and preskill that inspired this question ) . relation to entanglement i can also try and give a handwaving argument for why the quantum dimension would be related to entanglement . first of all , the fact that the entanglement entropy of a bounded region depends only on the length of the boundary $l$ and not on the area of the region is very clearly explained in this paper by srednicki , which is also cited by kitaev and preskill . basically it says that the entanglement entropy can be calculated by tracing out the bounded region , or by tracing out everything outside the bounded region , and the two approaches will yield the same result . this means the entanglement has to depend only on features that both regions have in common , and this rules out the area of the regions and leaves only the boundary between them . now for a system with no topological order the entanglement would go to zero when the size of bounded region goes to zero . however for a topological system there is intrinsic entanglement in the ground space which yields the constant term $-\gamma$ in the entanglement . the maximal entanglement entropy a system with dimension $d$ has with its environment is $\log d$ , so in an analogous manner the topological entanglement is $\gamma=\log d$ where $d$ is the quantum dimension . again this last argument relies heavily on handwaving so if anyone can improve it please do . i hope this answers at least the main concerns in the question , and i welcome any criticism .
as the other answers ( and dmckee 's comments ) note , yes , if you take the square root of a dimensional quantity then you need to take the square root of the units too : $$ \sqrt{4\ ; {\rm kg}} = 2\ ; {\rm kg}^{\frac12} $$ and no , i can not think of any meaningful physical interpretation for the unit ${\rm kg}^{\frac12}$ either . however , in the comments you say that you were " told to plot a graph of distance against square root of mass . " what that means is simply that you should scale the mass axis non-linearly , presumably in order to more clearly show the relationship between the two quantities . for labeling the mass axis , you basically have two choices : label the axis $\sqrt m$ , with equally spaced ticks at , say , $1\ ; {\rm kg}^{\frac12} , 2\ ; {\rm kg}^{\frac12} , 3\ ; {\rm kg}^{\frac12} , 4\ ; {\rm kg}^{\frac12} , \dotsc$ , or label the axis $m$ , with equally spaced ticks at $1\ ; {\rm kg} , 4\ ; {\rm kg} , 9\ ; {\rm kg} , 16\ ; {\rm kg} , \dotsc$ . while , technically , both of these are valid , i would strongly recommend the latter option . just compare these two plots and see which one you find easier to read : $\hspace{60px}$ alas , not all plotting software necessarily supports such axis labeling , or at least does not make it easy , which is why you sometimes see plots with funny units like ${\rm kg}^{\frac12}$ .
in a vacuum , the dc resistance to the flow of current is infinite . there is nothing contained within the vacuum that can conduct electricity .
when physicists say that a quantum field $\phi ( x ) $ is real-valued , they are usually referring to feynman 's path integral formulation of quantum field theory , which is equivalent to schwinger 's operator formulation . the values of a field $\phi ( x ) $ in the path integral formulations are numbers . e.g. : if the numbers are real , we say that the field $\phi ( x ) $ is real-valued . ( such a field $\phi ( x ) $ typically corresponds to a hermitian field operator $\hat{\phi} ( x ) $ in the operator formalism . ) if the numbers are complex , we say that the field $\phi ( x ) $ is complex-valued . if the numbers are grassmann-odd , we say that the field $\phi ( x ) $ is grassmann-odd . ( the numbers in this case are so-called supernumbers . see also this phys . se post . )
1 . ) as the box moves to the left , the photon moves to the right , and their momenta is conserved . since the masses are moving proportionally and opposite to one another , the center of mass of that system remains fixed . 2 . ) it is the same as the center of mass of a system consisting of a large gymnasium and a tennis ball inside the gymnasium , if that helps make it clearer . it is just that photons are very , very , very , ( very ) " small " - but the idea behind it is the same . 3 . ) yes , it does mean that . the box has moved , but so has the photon , so the center of mass of the box-photon system has not moved , they have just shifted relative to one another . 4 . ) it means that the mass must be non-negligible , so that it is accounted for in calculating the center of mass of the system , so that 1 . ) is true . i hope this helps answer your questions , but please follow up if anything is unclear .
according to wikipedia the magnetic field is indeed the result of feedback . actually the wikipedia article is very good so i am not sure how much there is left to say . the convection currents from the inner core outwards get bent onto spirals by the coriolis effect of earth 's rotation , and this gives a geometry where the magnetic field and electric currents sustain each other . re luboš' comment , i would have a google around the nasa web site as they have loads of data about pretty much everything to do with the earth e.g. http://science.nasa.gov/science-news/science-at-nasa/2003/29dec_magneticfield/ is an article aimed at the general public . there is bound to be raw data on the site somewhere . the wikipedia article mentions how hard it is to numerically model the magnetic field generation in the core . there have been a couple of really quite alarming experiments in the last decade trying to model the core . for example see http://www.nature.com/news/dynamo-maker-ready-to-roll-1.9582 - if 13 tons of liquid sodium is not alarming i do not know what is :- ) see http://physicsworld.com/cws/article/news/2007/mar/09/molten-sodium-mimics-earths-magnetic-field-flipping for an earlier experiment that claims to have modelled the field reversals . for an excellent popular introduction to this see the bbc horizon programme called " the core " . this is on youtube , though i am not sure that is an official upload so how long the programme will stay there i do not know .
yes the density of water changes with temperature in a non-linear way ( which is important if you want life on your planet ) . it has a maximum density at 4deg c and is unusual in that it expands ( lower density ) as a solid - see http://en.wikipedia.org/wiki/water_(properties)#density_of_water_and_ice as a gas it is density varies with temperature and pressure like any other gas .
the canonical momenta do not change if you add a total derivative to the lagrangian . the particular total derivative you wanted to add to the lagrangian as well as the lagrangian itself has free $i , j$ indices . you surely meant something else because the lagrangian should have no free indices like that . let me assume that you meant both expressions to be summed with the sum and prefactor $\sum_{ij} c_{ij}$ . of maybe you really meant the lagrangian to be a monomial for fixed values of $i , j$ . but that is not the issue here . the error relevant for your question is that you considered a phase space that has coordinates $\psi_j$ , $\bar\psi_i$ , $\pi_{\psi_i}$ , and $\pi_{\bar\psi_j}$ , and you think they are independent coordinates on the phase space . that would be too many phase space coordinates for such a limited system . well , they are not independent . the right derivation , using any form of the lagrangian you want , will give you $\pi_{\psi_i}=-\bar \psi_i$ ( without one-half ; and equations that may be obtained by simple conjugations from this one ! ) so it means that the " same " non-differentiated $\psi$ 's are their own momenta , too . if you rewrite the lagrangian in such a way that the redundant notation is eliminated , i.e. you do not think that coordinates that are dependent are actually independent ( this is the error that made you end up with the canonical momenta being 1/2 of their right value ; for example , you incorrectly used $\partial\dot{\bar\psi_i} / \partial \psi_j = 0$ , which is not true , in the first momentum you mentioned ) , you will see that $$\frac{\partial l}{\partial \dot\psi_j }=-\bar\psi_i$$ if i use your confusing non-summation over $i , j$ . there is no factor of 1/2 . indeed , to derive this thing without problems , it is helpful to first rewrite the lagrangian as $\bar\psi_i\dot\psi_j$ by adding the appropriate total derivative . this form is unique because it contains no $\dot{\bar\psi_i}$ and no $\psi_j$ , so it is only expressed as a function of the independent 1/2 of the degrees of freedom . needless to say , the hamiltonian is zero if the fermionic lagrangian only contains the kinetic term with the time derivative .
dear dbrane , $\lambda_{\rm qcd}$ is the only dimensionful parameter of pure qcd ( pure means without extra matter ) . it is dimensionful and replaces the dimensionless parameter $g_{\rm qcd}$ , the qcd coupling constant . the process in which a dimensionless constant such as $g$ is replaced by a dimensionful one such as $\lambda$ is called the dimensional transmutation : http://en.wikipedia.org/wiki/dimensional_transmutation the constant $g$ is not quite constant but it depends on the characteristic energy scale of the processes - essentially logarithmically . morally speaking , $$ \frac{1}{g^2 ( e ) } = \frac{1}{g^2 ( e_0 ) } + k \cdot \ln ( e/e_0 ) , $$ at least in the leading approximation . because $g$ depends on the scale , it is pretty much true that every value of $g$ is realized for some value of the energy scale $e$ . instead of talking about the values of $g$ for many specific values of $e$ , one may talk about the value of $e$ where $g$ gets as big as one or so , and this value of $e$ is known as $\lambda_{\rm qcd}$ although one must be a bit more careful to define it so that it is 150 mev and not twice as much , for example . yes , it is the characteristic scale of confinement and all other typical processes of pure qcd - those that do not depend on the current quark masses etc . in most sentences about the qcd scale , including your quote , the detailed numerical constant is not too important and the sentences are valid as order-of-magnitude estimates . however , given a proper definition , the exact value of $\lambda_{\rm qcd}$ may be experimentally determined . with this knowledge and given the known lagrangian of qcd - and the methods to calculate its quantum effects - one may reconstruct the full function $g ( e ) $ .
the lhc was envisioned as a " discovery " machine , a multipurpose one . the higgs gets the press but the expectations is that new physics will become accessible with the higher energy available for center of mass collisions . the z was discovered in the sps the proton antiproton previous generation collider . the previous machine in the same tunnel as the lhc , lep , an electron positron collider was needed to establish with great accuracy the parameters of z itself and the standard model . in general leptonic collisions probe elementary interactions with many less assumptions than proton proton or proton antiproton machines . this is because one is throwing balls of three quarks with their gluons at each other and measures the debris , in order to study the interactions . new physics , because of the high energy , will appear , but will be in a complexity unprecedented up to now . hopefully the next generation will be a lepton machine that will allow to establish the appropriate models unequivocally . now on the question of one off detectors : cern is practically using all the accelerators built up to now as increasing energy stages to feed the end machine , the lhc . nothing is wasted . in addition a lot of experiments are approved and running in beam lines that are not in the mainstream but may prove valuable or have unexpected theoretical repercussions . thus one expects that the lhc will open the window to the new physics that is tantalizing us , strings and unification of all forces at the moment , and the next generation machines will be leptonic ones to allow accurate measurements of parameters and decide between models .
i will start this with right hand grip rule for solenoids . . . " the coil ( solenoid ) is held in the right hand so that the fingers point the direction of current through the windings . then , the extended thumb points the direction of magnetic field " . ( which would be along the axis of the coil ) the higher the current , the more the magnetic field would be produced . . . for your example , let us assume the aluminium ring as a circular coil . when the uniform magnetic field is produced , there is a change in magnetic flux ( such as this increase in magnetic field ) along the axis of the ring , according to faraday 's law , induced current flows through the ring whose direction is given by lenz 's law . this induced current in the ring flows in a direction such that it opposes the magnetic field in the solenoid ( the one which actually produces it ) . ( but , the magnitude of induced magnetic field is always lesser than the field in the solenoid ) . anyways , there is a repulsion . with the maximum repulsive force produced , the ring is thrown off from the solenoid . this force always depends on the magnitude of $b$ in the solenoid .
you need to know how big one sigma is , for your gaussian distribution . it could be narrow , or it could be wide . you get that by calculating the variance , and taking its square root . then just divide the difference between the two values by sigma . more info here .
approximate the person with a brick with a width of 0.9 m and a height of 1.2 meter . the torque around the tipping point caused by gravity is $mg\cdot l$ , where $l$ is the horizontal distance from the tipping point to the center of mass of the brick , i.e. half the brick width assuming it has a uniform density . you need to counteract this torque by exerting a horizontal force at the top . the torque caused by your force is $f\cdot h$ . to sum up : $$ mgl=fh \rightarrow f=\frac{mgl}{h}\approx\frac{70\cdot9.8\cdot0.45}{1.2}\approx257 \mathrm{n} $$ however , many of the assumptions made here may not be realistic in a real world person-toppling event . edit : the solution above assumes that the toppling force is horizontal . if you can apply the force at an angle you can get a slightly longer lever , maximally the length of the brick diagonal : $$ d=\sqrt{1.2^2+0.9^2}=1.5 $$ this gives the smallest possible toppling force : $$ f=\frac{mgl}{d}\approx\frac{70\cdot9.8\cdot0.45}{1.5}\approx206 \mathrm{n} $$
the discrepancy results from the fact that the first approach implicitly assume distinguish-ability while the second does not . let 's take a simple example : $n=1$ , $n=2$ , and there are two cells ( $v=\frac{1}{2}v$ ) , what is the probability to have 1 particle in the first cell ? now label the particles by $1$ and $2$ ( and remove the label in the end ) and exam your two approaches . there are four configurations , $ [ ( 1,2 ) , ( ) ] $ , $ [ ( 1 ) , ( 2 ) ] $ , $ [ ( 2 ) , ( 1 ) ] $ , $ [ ( ) , ( 1,2 ) ] $ . you treat them with equal probability $\frac{1}{4}$ . there are two configurations with only 1 particle in the first cell . so the probability is $\frac{1}{2}$ . there are only three configurations , because $ [ ( 1 ) , ( 2 ) ] $ and $ [ ( 2 ) , ( 1 ) ] $ are the same physical states . the probability is therefore $\frac{1}{3}$ . the second approach is correct , because only different physical states are equally likely . edit feb . 7 , 2014: my conclusion was wrong , although particles should be indistinguishable . op and i implicitly assume there is only 1 microstate in each cell , which is not realistic . suppose there are $m$ sub-cells in the unit cell with volume $v$ ; equivalently there are $m$ different states in each cell $v$ . let $\frac{v}{v} = c$ , then there are $ ( c-1 ) m$ sub-cells outside $v$ as shown in the figure . there are $\binom{m+n-1}{m-1}$ ways to distribute $n$ indistinguishable particles into $m$ sub-cells in $v$ . similarly there are $\binom{ ( c-1 ) m+n-n+1}{c ( m-1 ) -1}$ ways to distribute the rest $n-n$ particles into $ ( c-1 ) m$ sub-cells outside $v$ . so the total number of configurations for exactly $n$ particles in $v$ is \begin{equation} \gamma ( n ) =\binom{m+n-1}{m-1} \binom{ ( c-1 ) m+n-n-1}{ ( c-1 ) m -1} \end{equation} while the total number of configurations is the ways to distribute $n$ particles into $cm$ sub-cells , \begin{equation} \sum_{n=0}^{n}\gamma ( n ) = \binom{cm+n-1}{cm-1} \end{equation} the probability in question is then \begin{equation} p_n = \frac{\gamma ( n ) }{\sum_{i=0}^{n}\gamma ( i ) } = \binom{n}{n} \frac{ ( m+n-1 ) ! }{ ( m-1 ) ! } \frac{ [ ( c-1 ) m+n-n-1 ] ! }{ [ ( c-1 ) m-1 ] ! } \frac{ ( cm-1 ) ! }{ ( cm+n-1 ) ! } \end{equation} then we take the dilute limit , which means the number of sub-cells $m$ are large compared even with $n$ . these factorials can be simplified , \begin{equation} p_n \approx \binom{n}{n} ( \frac{1}{c} ) ^n ( \frac{1-c}{c} ) ^{n-n} = \binom{n}{n} ( \frac{v}{v} ) ^n ( 1- \frac{v}{v} ) ^{n-n} \end{equation} oh god , i got the same answer as the first approach . so there is no inconstancy for these two approaches after we take $m\rightarrow \infty$ ; that means particles even in $v$ can only occupy a small fraction of sub-cells , which somehow makes them effectively distinguishable .
from what i have read in " american prometheus : the triumph and tragedy of j . robert oppenheimer " teller was the first one to express this concern before the trinity test . also quoting from : http://www.sciencemusings.com/2005/10/what-didnt-happen.html physicist edward teller considered another possibility . the huge temperature of a fission explosion -- tens of millions of degrees -- could fuse together nuclei of light elements , such as hydrogen , a process that also releases energy ( later , this insight would be the basis for hydrogen bombs ) . if the temperature of a detonation was high enough , nitrogen atoms in the atmosphere would fuse , releasing energy . ignition of atmospheric nitrogen might cause hydrogen in the oceans to fuse . the trinity experiment might inadvertently turn the entire planet into a chain-reaction fusion bomb . robert oppenheimer , chief of the american atomic scientists , took teller 's suggestion seriously . he discussed it with arthur compton , another leading physicist . " this would be the ultimate catastrophe , " wrote compton . " better to accept the slavery of the nazis than run a chance of drawing the final curtain on mankind ! " oppenheimer asked hans bethe and other physicists to check their calculations of the ignition temperature of nitrogen and the cooling effects expected in the fireball of a nuclear bomb . the new calculations indicated that an atmospheric conflagration was impossible . " bethe apparently then convincingly showed that the atmosphere would not be set on fire by a nuclear bomb .
for the $\theta : 2\theta$ goniometer , the x-ray tube is stationary , the sample moves by the angle $\theta$ and the detector simultaneously moves by the angle $2\theta$ . at high values of $\theta$ , small or loosely packed samples may have a tendency to fall off the sample holder .
it depends also on the shape of the object . if you assume the trampoline is circular , and the object is much smaller ( like a point mass ) then you can start developing the equations . you have to know the initial tension of the trampoline , and also assume the material non-elastic but supsended by perfect strings in a radial direction ( with known stiffness ) . after some math the static deflection ( with pre-tension ) obeys the following : $$ \frac{w}{k\ , r}=\tan\theta+\left ( \frac{f_{0}}{k\ , r}-1\right ) \sin\theta $$ if $r$ is the radius of the trampoline then the dip is $h=r\ , \tan\theta$ and so $\theta$ is the angle from horizontal that the cone makes . for any given angle $\theta$ the trampoline supports weight $w$ ( given above ) given total stiffness of $k$ and pre-tension of $f_{0}$ . so the above will give you the weight $w$ it will support given a dip $h$ . it is the reciprocal of what you want , but it is solvable . if the trampoline has $n$ linear springs each with stiffness rate of $k_i$ then the total stiffness ( springs in parallel ) is $k=k_i\ , n$ . to define the pre-tension $f_{0}$ assume that the free radius of the trampoline surface is $r$ , but the springs are located at $r_0$ then the pre-tension is $f_{0}=k\ , ( r_0-r ) $ . example : a trampoline of 12 feet in diameter needs $f_{0}=100$ lbs total of pulling to string into a 12.5 foot ring . the stiffness is $k=\frac{100}{6}$ in pounds per inch . to dip the trampoline by 5 feet $\tan ( \theta ) =\frac{h}{r}=\frac{5\times12}{12\times12}$ . plug these into the above and you should get $w=115.4$ lbs . i know i am going to confuse some people because i am treating radial quantities such as stiffness and loads as linear , but it works out ( just use cylindrical coordinates ) . approximation small angle approximation ( weight &lt ; 10% pre-tension , cone angle &lt ; 6° ) $$w = f_{0}\ , \frac{h}{r}$$ example : using the same numbers as above a $w=10$ lbs weight will dip $h = ( 12\times12 ) \frac{w}{100} = 14.4$ inches . the full solution above gives $13.1$ inches theory the deformed shape of the trampoline is a perfect cone . the distance from the center to where the springs start in the deformed state is always equal to $r$ . the extension of the springs tension is then $f=f_{0}+k\ , \left ( \frac{r}{\cos\theta}-r\right ) $ which needs to be balanced by the weight as $w = f\ , \sin\theta$ . the pieces come together to make the equation shown above . update solution is corrected for the fact the radial distance is constant , not the surface area of the trampoline . as the trampoline deforms its perimeter crumples and folds on itself like a napkin when lifted from the center .
there is no restriction . the simplest choice is $p = \kappa \rho$ , where $\kappa$ is a real constant , and that is often used to get simple results . $\kappa=0$ is dust , $\kappa=-1$ is cosmological constant , and $\kappa=\frac{1}{3}$ is a gas of photons . a proper analysis also needs an equation describing the flow of internal energy and its relationship to temperature . as a concrete example , note that $pv=nkt$ can be rewritten as $p = \frac{n}{v}kt = \frac{\rho}{m_{p}}kt$ , where $m_{p}$ is the mass of the particle making up the ideal gas . thus , if the fluid is held at a constant temperature , we get $\kappa = \frac{kt}{m_{p}}$ . if the fluid is not at a constant temperature , we need to use the first law of thermodynamics and knowledge of the process ( isoentropic , adiabatic , etc ) the system is undergoing to get a relationship between the density and pressure .
lubos hit the key point : you need to calculate the polarization of an atom/molecule so that is the starting point . as you may already know , when a dielectric is subjected to the impinging e-field of an em wave , there are dipoles generated that contribute to the total internal field . the resultant field for most materials is given by $ ( \epsilon−\epsilon_0 ) e=p$ . however , classically , the polarization will depend on the relative displacement between the electron cloud and the nucleus and this displacement can be calculated by thinking of the electron as a harmonic oscillator . that is , the electron cloud will oscillate about the nucleus . there are three terms that must come into play to describe the displacement of the electron . the electron cloud bound to the nucleus must have some sort of restoring force : $−mω_0^2x$ where $ω_0$ is the resonate frequency and m is the mass of the electron . the impinging em wave will exert a time varying force , say $cosωt$ , on the electrons : $ee ( t ) = eecosωt$ where $ω$ is the driving frequency . for a gas , atoms are far enough apart that we can “ignore” interactions between them . however , for atoms and molecules in close proximity , one cannot ignore these interactions which behave as “frictional” type forces . that is , the electron oscillators will dissipation some of their energy as heat . therefore , there must be some type of velocity term : $mβ\frac{dx}{dt}$ where $β$ is a damping constant . if we now stuff all of this into newton’s second law , we have an equation for the electron displacement : $$m\frac{d^2 x}{dt^2} = −mω_0^2x - mβ\frac{dx}{dt} + ee_0cosωt$$ physically , we expect that the electron will oscillate at the same frequency as the impinging em wave , so that the equation above has the solution $x ( t ) = acosωt$ . substituting this assumed solution and solving for the amplitude , we get $$x ( t ) = \frac{ee ( t ) }{m ( ω_0^2-ω^2+iβ ω ) }$$ the electric polarization is the density of dipole moments : $p ( t ) = ex ( t ) n $ where $n =$ number of dipoles . solving for the electric permittivity using $ ( ϵ−ϵ_0 ) e=p$ , $$ϵ=ϵ_0 + \frac{p ( t ) }{e ( t ) } = ϵ_0 + \frac{ne^2}{m ( ω_0^2-ω^2+iβω ) }$$ the way the electric permittivity is related to the index of refraction is as follows : most materials are nonmagnetic at optical frequencies ( the relative permeability is very close to one ) . so to a good approximation , the index of refraction depends only on the relative permittivity $ϵ_r : n^2 ( ω ) = ϵ_r = \frac{ϵ}{ϵ_0}$ . therefore , the dispersion relationship from a damped harmonic oscillator view point looks like $$n^2 ( ω ) = ϵ_r = \frac{ϵ}{ϵ_0} = 1 + \frac{ne^2}{m ϵ_0} \frac{1}{ω_0^2-ω^2+iβω}$$ you can see that the index of refraction is frequency dependent . note that this is valid for only a single resonate frequency ; a given substance has several such resonate frequencies and this equation will need to be modified but the quantum mechanical solution looks very similar to the one above . i will not explain this since i think that i have answered your question .
i would like to add a little to lubos 's answer : first a historical note : this is what einstein proposed as a way of understanding quantum mechanics in 1919 or thereabouts , in the paper " do gravitational fields play a role in the composition of the elementary particles ? " einstein was of the opinion that a complicated enough classical theory , like general relativity , would lead continuous waves to collapse into standard-size soliton-like particles and these particles he felt might then bang around along the wave in such a way to reproduce quantum mechanics . this idea reappears several times in the literature , but it demonstrably does not work . a field theory , like gr , is a classical theory , and it therefore is local hidden variables ( the variables are not even hidden in this case ) . this is ruled out by bell 's theorem--- the correlations in quantum mechanics do not allow local fields to carry the data that determines the experimental outcome , not without conspiracy ( superdeterminism ) or nonlocal equations ( faster than light changes in the variables ) . neither works in a straightforward field theory like gr . secondly , gr is not as badly understood as all that , although it is not as well understood as one would like , mostly because numerical methods are in their infancy , and one 's intuition must come laboriously from analyzing exact solutions when these are available . the example i gave of particles oscillating into and out of an extremal black hole is not really new ( do not give me too much credit ) , the new thing there is the holographic interpretation , namely that the coming-out is an ordinary coming out event in this universe . the oscillations of particles into and out of a near-extremal black hole were appreciated in the 1960s , but each oscillation takes you to a disconnected branch classically , because crossing a horizon takes an infinite amount of t-time . this is not possible quantum mechanically , since this disconnected maximally extended thing is not compatible with unitarity . the nice thing about the in-out solution for geodesics in the extremal reissner nordstrom is that if you replace the test particle with a little charged black hole , you can make nonrelativistic oscillations if both black holes are near extremal . the external field of the two black holes does not have a full merger , the little black hole , now not considered as a test particle , but as a solution to gr proper , just smears out on the horizon , then bounces back . i did not calculate this in detail yet , but it can be solved completely with an analysis along the lines of atiyah and hitchin in their famous paper on slow soliton scattering ( the atiyah hitchin space ) , except here , unlike the other case , i am not optimistic there will be a simple geometrical solution , rather one has to bite the bullet and trace the bouncing behavior in the solution either by numerical integration or solving for the near-static phase-space geometry of the two extremal black holes . causalities and ctc 's the basic idea you are giving is that perhaps hidden variables plus closed time-like curves can reproduce bell inequality violations . i will give some sentences about why this is extremely unlikely . quantum mechanics has entangled wavefunctions . what this means is that the wavefunction for k particles is in 3k dimensional space , not in 3 dimensional space . the growth in dimensions means that quantum mechanics packs a stronger computational punch than classical mechanics , and you can not simulate quantum mechanics of k-particles with less than exponentially much classical information . this is why quantum computation works in pure quantum mechanics . so the structure of quantum mechanics is exponentially big and has the entanglements that violate bell 's inequality . if you wish to reproduce this from something like gr , you need gross nonlocality and some way of reproducing nonlocality . so if you have a pair of electrons that bind to an atom ( so that their spins anti-align ) , and then you knock out the nucleus , and do bell measurements on the two outgoing electrons , you need to reproduce the nonlocal correlations from ctc 's in gr . this means that the electron needs to have ctc 's " inside " which go back in time and magically alter the attributes of the other electron . this only became required once you put them together in an atom , and let the photons radiate , and during this process the two point electrons did not necessarily come close to each other ( assuming they are classical and described in space ) . how do ctc 's help correlate them ? to make this work , you would have to go all the way back in time to where the two electrons were created from the inflaton field , and correlate them back then . this type of back-and-forth in time description is utterly conspiratorial , and very unconvincing . there is also no shred of a hint that this will reproduce anything like qm , it is just not ruled out , because you are postulating little tiny internal back-in-time paths on all electrons , something we have no evidence for . there are no real ctc 's in physical exterior solutions of gr . the ctc 's in the intepretation i gave of oscillations into and out of extremal black holes are unphysical--- they are only closed in time because of the wrongness of the classical picture of the horizon . the ctc 's in the interior of a kerr solution can only occur when you wind around the ring singularity , and then it should be possible to unwrap the interior so that it has a pure-causal description , simply by including the winding number of your path around the ring . i do not know the interior kerr well enough to see how to do this , and this must work in any number of dimensions , not just 4 , so i hesitate to say it is what happens , but there must be a reconciliation of causality and kerr interior , because you can set up fields at the horizon of kerr , and let them traverse the interior , and the evolution equation should not have additional constraints , as come from ctcs . all in all , the form of the two theories , gr and qm , is completely different , the descriptions are of a different computational complexity , and the causality notion is totally different in the two schemes , so it is implausible in the highest degree that gr can explain qm . what is more , today we have a good quantum version of gr , string theory , which subsumes and extends the classical theory , so that it is a mistake to pretend that this progress does not exist , and to work as if we were living in 1926 . within string theory , you give a full accounting of all gr effects on flat and ads backgrounds in principle , from an ordinary unitary quantum theory . this quantum gr means that we know how gr and qm are reconciled ( in perturbations to flat and ads backgrounds ) , and the classical limit where gr is reproduced is just not quantum , it is an ordinary classical field theory .
there are various ways to decide which of the assumptions are primary and which of them are their consequences but $e=vq$ may be most naturally interpreted as the definition of the potential . the potential energy is a form of energy and the potential ( and therefore voltage , when differences are taken ) is defined as the potential energy ( or potential energy difference ) per unit charge , $v = e/q$ . that is equivalent to your equation . the potential energy is proportional to the charge essentially because of the linearity of maxwell 's equations ( the superposition principle ) . once we know about the proportionality , we must just give a name to the proportionality factor between $e$ and $q$ and we simply call it potential ( or voltage ) .
the clebsch-gordan coefficients appear in the representation theory of the [ lie ] group of rotations $so ( 3 ) $ [ and its fundamental cover $su ( 2 ) $ ] . when expressing the tensor product of two irreducible representations of this group [ itself being a reducible representation ] as a direct sum of irreducible representations , the normalized coefficients of the expansion are the clebsch-gordan coefficients . they express the multiplicity of each irreducible representation in the decomposition . the clebsch-gordan coefficients are themselves orthonormal , with orthonormality relation $\sum_{|m_1|\leq j_1 , |m_2| \leq j_2} c ( j_1 , j_2 , m_1 , m_2|j , m ) c ( j_1 , j_2 , m_1 , m_2|j&#39 ; , m&#39 ; ) = \delta_{j , j&#39 ; } \delta_{m , m&#39 ; }$ $\sum_{j=|j_1-j_2|}^{j_1+j_2} \sum_{|m|\leq j} c ( j_1 , j_2 , m_1 , m_2|j , m ) c ( j_1 , j_2 , m_1&#39 ; , m_2&#39 ; |j , m ) = \delta_{m_1 , m_1&#39 ; } \delta_{m_2 , m_2&#39 ; }$ and as exposed above appear when decomposing reducible representations into sums of irreducible representation . in terms of angular momentum states $|j_1 , m_1 \rangle \bigotimes|j_2 , m_2\rangle=\sum_{j , m} c ( j_1 , j_2 , m_1 , m_2|j , m ) |j , m\rangle$ where $c ( j_1 , j_2 , m_1 , m_2|j , m ) =\langle j , m|j_1 , j_2 , m_1 , m_2\rangle$ the clebsch-gordan coefficients appears also , in the expansion of the product of two spherical harmonics in terms of spherical harmonics themselves . the derivation of the formula is a bit cumbersome and the result looks like this $y_{l_1}^{m_1} ( \theta , \varphi ) y_{l_2}^{m_2} ( \theta , \varphi ) =\sum_{l , m} \ \sqrt{\dfrac{ ( 2l_1+1 ) ( 2l_2+1 ) }{4 \pi ( 2l+1 ) }} \\ \times c ( l_1 , l_2 , m_1 , m_2|l , m ) c ( l_1 , l_2,0,0|l , m ) y_{l}^m ( \theta , \varphi ) $ they are also related to other more complicated structures like the wigner 3-j symbols or the racah coefficients . in addition , i may add that there exists a closed formula for them in $3$ dimensions ( derived by racah ) and that this formula is not known for arbitrary dimensions .
i have written an answer to mathoverflow in which explicit formulas for the classical and quantum hamiltonians of a spin system ( generators of $su ( 2 ) ) $ were written explicitely . the classical hamiltonians are given by means of functions on the two sphere and the quantum hamiltonians by means of holomorphic differential operators ( which act on the sections of the quantum line bundle ) . for many spin system with a linear hamiltonian in each spin , one just has a distinct one particle hamiltonian per spin . sorry for referring to my own work , but it is by no means original .
there are at least two mechanism of thermal conductivity - free electrons and thermal phonons . the first mechanism can be prevalent in metals , the second one is important in dielectrics . i did not look up thermal conductivity of glass , but such excellent dielectric as diamond has higher thermal conductivity than any metal , as far as i know .
let me try to answer . for your first question the statement is that you can work with either ${\mathbb p}^2$ or ${\mathbb p}^1\times {\mathbb p}^1$ - the moduli space is the same . more generally , if $s$ is any surface which contains ${\mathbb a}^2$ as an open subset and $d_{\infty}$ is the divisor at $\infty$ then $bun_g ( s , d_{\infty} ) $ is independent of $s$ . for the second question : it is true that ${\mathfrak q}={\mathcal m}_{g , p}$ ( for $p$ being the borel subgroup and $g=sl ( n ) $ ) but it is not true that $q={\mathcal qm}_{g , p}$ . the point is that the quasi-maps ' space ${\mathcal qm}_{g , p}$ is defined for any $g$ and it is singular ; for $g=sl ( n ) $ ( and only in that case ) it has a nice resolution of singularities which is given by the laumon space . if you are interested to know more , you can read my 2006 icm talk ( "spaces of quasi-maps into the flag varieties and their applications" ) - the above questions are discussed there .
you can indeed test two quantum states for being equal , but the results are not 100% guaranteed accurate : you measure the eigenvalue of the swap operator ( which swaps the two quantum states ) . if they are equal , then you have a 100% chance of getting the +1 eigenvalue . if they are orthogonal , then you have a 50% chance of getting either the +1 and -1 eigenvalue . this test ( a ) destroys the quantum state if you test it against a state that it is not equal to and ( b ) only yields the correct answer half the time if the answer is " no " . these two drawbacks mean that you cannot use it to clone . however , this is still a very useful test as a subroutine in designing some quantum algorithms . i do not know whether anybody has proved a theorem saying that you cannot test equality better than the swap test , but it is definitely true , as the op speculates , that there is no perfect test for equality of quantum states .
i will assume in this answer that " drag " means tension . you are asked to find the tension in the chain as it is rotating . this is independent of the link size , so long as the links are not a significant fraction of the circumference . if you have a hoop of mass density per unit length $\rho$ and circumference c ( so that $\rho c = m$ where m is the total mass ) , rotating with rotational velocity $\omega$ , the centripetal force on a segment of length l is the mass times the rotational velocity squared times the radius , or $$ f_c = \rho l w^2 {c\over 2\pi} $$ if the chain is at tension t , the two endpoints of the segment pull in with a total force of $$ {tl\over c} $$ setting the two forces equal , the l drops out ( as it must ) and gives the tension : $$ t = ( \rho c ) \omega^2 {c\over 2\pi} = m \omega^2 {c\over 2\pi} $$ or $\omega= 30 {1\over s}$ , $m= . 4 \mathrm{kg}$ , $c = 1.2 m$ , this is about 68n .
mass is only conserved in the low-energy limit of relativistic systems . in relativistic systems , mass can be converted into energy , and you can have processes like massive electron-positron pairs annhillating to form massless photons . what is conserved ( in theories obeying special relativity , at least ) is mass energy--this conservation is enforced by the time and space translation invariance of the theory . since the amount of energy in the mass dominates the amount of energy in kinetic energy ( $mc^{2}$ means a lot of energy is stored even in a small mass ) for nonrelativistic motion , you get a very good approximation of mass conservation . out of the energy conservation .
the short answer ( and likely one you are not going to like ) to your question is that you are going to need as many zernike terms as it is experimentally found are needed to model the aberration accurately . zernike polynomials are normalized so that they contribute equally to the mean square phase error , and this latter , to the first order , is what sets the strehl ratio , which is effectively what you are after here . the simplest analysis of the relation between aberration and phase is explored in sections 9.2 and 9.3 of born and wolf " principles of optics " ( mine is the sixth edition ) ; in 9.3 the maréchal criterion is discussed , which is limit on the total mean square aberration that is maréchal 's definition of " diffraction limited performance " . you may be able to narrow your description down to a few key zernike polynomials , but there is no fundamental physics or mathematics to tell you which ones : either you must determine this experimentally or sometimes you can hazard a good guess if you can find any details of the production process that builds your mirrors out and understand the production process 's " symmetries " . as for the ellipticity of your problem ( wrought by the 45 degree beam folding in your " periscope " arrangement ) , i believe it can be handled by simple co-ordinate transformations as detailed below that make the mirror system nominally axisymmetric . so your zernike analysis will be the " wonted " or " normal " one , but you transform your elliptical domain into a circular one with the $x$-direction shrink embodied in equations ( 2 ) and ( 3 ) below . thus your situation will have the following special symmetry considerations : spherical aberration terms become astigmatic for example , astigmatic terms will tend to become spherical - likewise , the symmetry class of all aberrations on the mirror surfaces will be changed by the shrink embodied by ( 2 ) and ( 3 ) ; your mirrors will themselves are ellipsoidal rather than spherical ( which i am sure you understand ) to yield the focusing power when 45 degree tilted rather than orthogonal to the optical axis . i am pretty sure ( without further analysis ) the nominal contours on the ellipses are going to be of the form $\frac{1}{2} x_m^2 + y_m^2 = r^2$ ( see ( 2 ) and ( 3 ) - so astigmatic zernike terms are going to be especially important . there could be " azimuthal " misalignment between the two principal axes of the mirrors : i.e. the major and minor axes of one mirror may not be quite parallel to one another . this could lead to zernike terms of high azimuthal symmetry classes : tetrafoil , heaxfoil and octofoil ( $\cos$ or $\sin ( 4 \theta ) $ , $\cos$ or $\sin ( 6 \theta ) $ , $\cos$ or $\sin ( 8 \theta ) $ ; are you diamond turning these mirrors , btw ? diamond turning is extremely accurate , even in non axisymmetric components - the biggest errors are likely to be spherical ( or spherical in the transformed co-ordinates in your case ) so i am guessing the major aberrations are going to arise from misalignements of the principal mirror axes . now we get onto how the analysis looks . given $d \gg \lambda$ , fraunhofer diffraction applies , so the input field undergoes the equivalent of the following three steps : diffract to mirror system so that the wave has a diverging spherical wavefront of curvature radius $d$ ; mirror system , through weak but intended ellipsoidal shape , imparts phase to the diffracting spherical wavefront so that now the wavefront has the conjugate phase ( intended function , so that it becomes a converging rather than diverging wave ) together with an unintended phasing $\exp ( i\ , \phi ( x , y ) ) $ , where $\phi ( x , y ) $ is the unintended aberration as a function of the rectangular mirror surface co-ordinates $x$ and $y$ ( to be defined so as to take account of the " folding " owing to the two 45 degree slanted mirrors ) ; diffraction " back " to the image plane . i am thus thinking of this problem as a more complicated version of a diffracting field bouncing off a spherical mirror aligned to the spherical wavefront so that the field focusses back at its initial positing in the ideal case . the transformation undergone by a field $\psi ( x , y ) $ in the object plane to reach the image plane is thus : $$\psi \mapsto \mathfrak{f}^{-1}\ , \exp ( i\ , \phi ( x , y ) ) \ , \mathfrak{f}\ , \psi\quad\quad ( 1 ) $$ where $\mathfrak{f}$ is the unitary two dimensional fourier transform . the inverse transform comes from the fact that my equivalent system imparts the nominal conjugate phase to the wavefront and sends it " backwards " to the image plane . as long as we deal with measurements in the image plane alone , we cannot tell which of the pair any aberration comes from , so we may as well represent the combined effect of the " periscope " pair by the one aberration function $\phi ( x , y ) $ . this aberration is the thing whose effect you wish to quantify . now for the definitions of $x$ and $y$ , which is where the 45 degree tilting is accounted for : $$x = \frac{k\ , x_m}{\sqrt{2}\ , d}\quad\quad ( 2 ) $$ $$y = \frac{k\ , y_m}{d}\quad\quad ( 3 ) $$ here $x_m$ and $y_m$ are the physical distances measured along the surface of the effective mirror , $i . e . $ imagine the mirrors flattened out taking away their by their nominal ellipsoidal curvature designed to offset the field curvature arising from the first diffraction . the residual deviations from this design goal are then added together to get the aberration function $\phi$ . here the $x$ direction is along the plane containing the system chief ray 's nominal folded path : i.e. if the tube ( line joining the two mirror centres ) of the " periscope " is horizontal , then the $x$-axis is horizontal and likewise if the periscope tube is in any other direction . the square root of two factor accounts for the mirror tilt : the beam spreads out further on the mirror in the $x$ direction than it does in the $y$ owing to the effective mirror 's intersecting the beam at the slant . so now we are left with the effect of $\phi ( x , y ) $ . let the input field be : $$\psi_i\left ( x , y\right ) = \frac{1}{\sqrt{\pi}\ , \sigma} \exp\left ( -\frac{x^2+y^2}{2\ , \sigma^2}\right ) \quad\quad ( 4 ) $$ where $\sigma$ is the input field 's spotsize . the first diffraction sends $\psi$ into the $x-y$ space by : $$\psi_i\left ( x , y\right ) \mapsto \psi ( x , y ) =\frac{1}{2\ , \pi\ , \sqrt{\pi}\ , \sigma} \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty \exp\left ( -i \left ( x\ , x +y\ , y\right ) \right ) \ , \exp\left ( -\frac{x^2+y^2}{2\ , \sigma^2}\right ) \ , \mathrm{d}x\ , \mathrm{d}y = \frac{\sigma}{\sqrt{\pi}} \exp\left ( -\frac{1}{2}\ , \sigma^2\ , \left ( x^2+y^2\right ) \right ) \quad\quad ( 5 ) $$ and now the output field is : $$\psi_o\left ( x , y\right ) =\frac{\sigma}{2\ , \pi\ , \sqrt{\pi}} \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty \exp\left ( -i \left ( x\ , x +y\ , y\right ) \right ) \ , \exp\left ( -\frac{1}{2} \sigma^2 \left ( x^2+y^2\right ) \right ) \ , \exp ( i\ , \phi ( x , y ) ) \ , \mathrm{d}x\ , \mathrm{d}y\quad\quad ( 6 ) $$ now we can generalize the above result by including the effects of a defocus by noting that the diffraction through axial distance $\delta z$ of a unidirectionally propagating scalar field fulfilling the helmholtz equation undergoes the transformation defined by : $$\psi \mapsto \mathfrak{f}^{-1} \ , \exp\left ( i\ , \delta z \sqrt{k^2 - x^2 - y^2}\right ) \ , \mathfrak{f} \ , \psi \approx e^{i\ , k\ , \delta z}\ , \mathfrak{f}^{-1}\ , \exp\left ( -i\ , \delta z \frac{x^2+y^2}{2\ , k}\right ) \ , \mathfrak{f}\ , \psi\quad\quad ( 7 ) $$ where $\psi$ is the field at the input plane and is transformed to the field on a parallel plane a distance $\delta z$ in the direction of the field 's propagation . the first fourier transform $\mathfrak{f}$ splits the field into plane wave components with $x$-wavenumber $x$ and $y$-wavenumber $y$ , then the phasing term sandwiched between the two fourier transforms imparts the right phase delay for each plane wave component ( if a plane wave has $x$ and $y$ wavevector components $x$ and $y$ , then the $z$ component of the wavevector must be $\sqrt{k^2 - x^2 - y^2}$ where $k$ is the wavenumber ) , then the last inverse fourier transform assembles all these delayed plane wave components into the diffracted field . the approximation assumes a low numerical aperture field , so that $x$ and $y$ are small compared to the wavenumber $k$ and the plane wave components all make small angles with the axial direction . so now we can combine ( 1 ) and ( 7 ) to get a version of ( 6 ) generalized to where the light field is transformed by the whole system , followed by a defocus of $\delta z$: $$\psi_o\left ( x , y\right ) =\frac{\sigma}{2\ , \pi\ , \sqrt{\pi}} \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty \exp\left ( i \left ( \phi ( x , y ) - x\ , x -y\ , y - \delta z \frac{x^2+y^2}{2\ , k}\right ) \right ) \ , \exp\left ( -\frac{1}{2} \sigma^2 \left ( x^2+y^2\right ) \right ) \ , \mathrm{d}x\ , \mathrm{d}y\quad\quad ( 8 ) $$ the strehl ratio is the intensity ratio of the above quantity to its aberration free value ( which is $\frac{1}{\sqrt{\pi}\ , \sigma}$ ) , assuming the output field 's peak amplitude is at position $ ( x , y ) $ . so now we look carefully at the total aberration term that lowers the peak amplitude : $$\phi ( x , y ) - x\ , x -y\ , y - \delta z \frac{x^2+y^2}{2\ , k}\quad\quad ( 9 ) $$ and rewrite that in polar co-ordinates $ ( \rho , \theta ) $ . to begin applying zernikes to this aberration , we must define the " outer radius " in the normalized mirror co-ordinates $ ( x , y ) $ ; the gaussian $\exp\left ( -\frac{1}{2} \sigma^2 \left ( x^2+y^2\right ) \right ) $ is well " contained " within a radius of , say , $r = 4 / \sigma$ ; you can rework the following for any value of $r$ , although i am suggesting a working value is going to be very like what i have just suggested . with polar co-ordinates normalized so that $x^2+y^2 = r^2$ corresponds to the whole aperture and therefore to $\rho = 1$ ( i.e. $\rho^2 r^2 = x^2+y^2$ ) , our total aberration is : $$\phi ( \rho , \theta ) - x\ , r\ , \rho\ , \cos\theta -y\ , r\ , \rho\ , \sin\theta - \frac{\delta z\ , r^2}{4\ , k} ( 2\rho^2 - 1 ) + \frac{\delta z\ , r^2}{4\ , k}\quad\quad ( 10 ) $$ here you see the two tilt and defocus zernike functions $\rho\ , \cos\theta$ , $\rho\ , \sin\theta$ and $2\rho^2 - 1$ and they correspond to sideways system misalignments ( corresponding to tilts on your mirror , which i assume you have the freedom to impart for the sake of alignment ) and small errors in the axial position of the output plane : again , i assume you have the freedom to impart small axial displacements to the mirror system in your alignment controls . the net aberration , after all these adjustments have been optimized , will be your mirror aberration $\phi ( \rho , \theta ) $ with the defocus and tilt zernikes removed . let $\tilde{\phi} ( \rho , \theta ) $ be the net aberration after tilt and defocus zernikes have been removed . then the strehl ratio is : $$\begin{array}{lcl}\mathcal{s} and = and \left ( \frac{\sigma^2\ , r^2}{2\ , \pi} \int\limits_0^\infty\int\limits_0^{2\pi} \rho\ , \exp\left ( i \tilde{\phi} ( \rho , \theta ) \right ) \ , \exp\left ( -\frac{r^2}{2} \sigma^2 \rho^2\right ) \ , \mathrm{d}\theta\ , \mathrm{d}\rho\right ) ^2 \\ and \approx and \left ( 1 - \frac{\sigma^2\ , r^2}{2\ , \pi} \int\limits_0^r\int\limits_0^{2\pi} \rho\ , \frac{\left ( \tilde{\phi} ( \rho , \theta ) \right ) ^2}{2} \ , \exp\left ( -\frac{r^2}{2} \sigma^2 \rho^2\right ) \ , \mathrm{d}\theta\ , \mathrm{d}\rho\right ) ^2\\ and \approx and 1 - \frac{\sigma^2\ , r^2}{2\ , \pi} \int\limits_0^r\int\limits_0^{2\pi} \left ( \tilde{\phi} ( \rho , \theta ) \right ) ^2 \ , \rho\ , \exp\left ( -\frac{r^2}{2} \sigma^2 \rho^2\right ) \ , \mathrm{d}\theta\ , \mathrm{d}\rho\end{array}\quad\quad ( 10 ) $$ the last expression is the one i believe you are looking for to describe the degradation of the mode peak . the last two steps came from assuming small aberration so that we expand $\exp ( i\tilde{\phi} ( \rho , \theta ) ) $ as a taylor series to quadratic terms , and then to note that the integral of the linear taylor term vanishes because we have stripped away the mean value of $\tilde{\phi} ( \rho , \theta ) $ when we removed the tilt and defocus zernike terms . recall that the constant $r$ is the domain radius in your normalized co-ordinates defined by ( 2 ) and ( 3 ) and so has the dimensions of inverse length . this analysis is not too different from that in section 9.3 of born and wolf . the main difference here is the co-ordinate transformations ( 2 ) and ( 3 ) which map the physical mirror surface co-ordinates into the normalized co-ordinates suitable for zernike analysis , and the " lopsided " imparting of the $\sqrt{2}$ factor in ( 2 ) accounts for the tilted beam path in the " periscope " , so you will need to transform your datasets by ( 2 ) and ( 3 ) before doing your zernike analysis . mathematica encodes zernike radial functions by zerniker [ n , m , r ] ( radial class $n$ , azimuthal symmetry $m$ , $r$ the independent variable ) . most wavefront analysis softwares that come with interferometers can do the kind of linear transformation on datasets you want - the one i use is 4d technology "4sight " . the software " durango " has a free evaluation version but i have not used it . failing all that , i have my own c++ library for doing zernike analysis so if you need to use this , contact me by my email on my user page . lastly , for the sake of theoretical completeness , one strictly should not use zernike functions for situations where beams are gaussian apodised like yours . one should use functions that are orthogonal with respect to the weight function $\rho \ , \exp\left ( -\frac{r^2}{2} \sigma^2 \rho^2\right ) $ ; the zernikes are orthogonal with respect to the weight function $\rho$ . but this should not make a great deal of difference as long as you choose your " cutoff " radius $r$ well .
the earth is not a perfect sphere ( or even a perfect oblate spheroid ) so its gravitational field is not axially symmetric . you have probably seen the geoid measured by the goce and grace satellites . as the earth rotates the asymmetries in its gravitational field rotate with it , and any satellite whose orbital period is a ratio of one day can build up a resonance with the daily variations in the earth 's gravity . this is essentially the same physics as the resonances seen in , for example , the moons of jupiter . i had a quick google and found this paper that gives a fairly detailed analysis of the phenomenon . see in particular section 1.4 . the galileo satellites orbit 17 times every 10 days . this is sufficiently far from a simple ratio that resonances do not build up .
i think it is expected that you have a bit of common sense about this . let 's take the operator $o ( t ) $ which is the position operator when $t$ is between 9:00 and 10:00 in the morning , and the momentum operator the rest of the day . now take a system in a stationary state , and ask " what is the expectation value of $o ( t ) $ at each time $t$" ? whoa , the expectation value changes dramatically each day at 9:00 , then changes again at 10:00 ! does that mean the state is not " stationary " at 9:00 or 10:00 ? no , of course it does not mean that ! when an operator has explicit time dependence , as $o ( t ) $ does , it means that you have a different -- possibly totally unrelated -- operator at each time $t$ . wikipedia says " the system remains in the same state as time elapses , in every observable way " . that is correct . i do not think a reasonable person reading that sentence would infer that if you calculate the expected position at 9:30 , and then you calculate the expected momentum at 10:30 , the two calculations should have the same answer for a stationary state .
this might be more of a math question . this is a peculiar thing about three-dimensional space . note that in three dimensions , an area such as a plane is a two dimensional subspace . on a sheet of paper you only need two numbers to unambiguously denote a point . now imagine standing on the sheet of paper , the direction your head points to will always be a way to know how this plane is oriented in space . this is called the " normal " vector to this plane , it is at a right angle to the plane . if you now choos the convention to have the length of this vector ( "the norm" ) equal to the area of this surface , you get a complete description of the two dimensional plane , its orientation in three dimensional space ( the vector part ) and how big this plane is ( the length of this vector ) . mathematically , you can express this by the " cross product " $$\vec c=\vec a\times\vec b$$ whose magnitude is defined as $|c| = |a||b|sin\theta$ which is equal to the area of the parallelogram those to vectors ( which really define a plane ) span . to steal this picture from wikipedia 's article on the cross product : as i said in the beginning this is a very special thing for three dimensions , in higher dimensions , it does not work as neatly for various reasons . if you want to learn more about this topic a keyword would be " exterior algebra " update : as for the physical significance of this concept , prominent examples are vector fields flowing through surfaces . take a circular wire . this circle can be oriented in various ways in 3d . if you have an external magnetic field , you might know that this can induce an electric current , proportional to the rate of change of the amount flowing through the circle ( think of this as how much the arrows perforate the area ) . if the magnetic field vectors are parallel to the circle ( and thus orthogonal to its normal vector ) they do not " perforate " the area at all , so the flow through this area is zero . on the other hand , if the field vectors are orthogonal to the plane ( i.e. . parallel to the normal ) , the maximally " perforate " this area and the flow is maximal . if you change the orientation of between those two states you can get electrical current .
unlike neutrinos , light will be slowed down by gas in the tunnel , which would have to go through the earth . it is much cheaper and easier to mathematically analyze the opera results to find their error , were they ever to release their detailed protocol , which is unlikely , because they do not seem to want the error discovered .
i am not entirely sure what you are trying to ask , but i think it is this : when is the schrodinger equation ( or a similar differential equation ) separable ? what conditions must the potential function satisfy ? the short answer is that the schrodinger equation is separable when the potential is independent of time ( though there maybe time independent potentials that also work ) . a differential equation of two independent variables is separable if the equation can be algebraically manipulated such that only one type of variable appears on each side of the equation . in the case of a partial differential equation ( i.e. . the schrodinger equation ) dependent variable can be written as a product of functions of the two independent variables ; that is $$ \psi ( x , t ) = \rho ( x ) \phi ( t ) $$ if we apply the schrodinger equation to this " guess " and assume $v$ is independent of time we find ( after a few steps ) : $$ -\frac{\hbar^2}{2m} \frac{d^2\rho}{dx^2} = ( e-v ) \rho $$ for and $e=$ constant . note that this equation is an ordinary differential equation though we started with a partial differential equation . more importantly , since $\rho ( x ) $ is independent of time , and therefore so is this entire equation . hence , it is called the time independent schrodinger equation . this is essentially a shortened version of the derivation provided in griffith 's book .
you can define quantum mechanics on a cantor set , but in order for it to be nontrivial , it needs to be a levy quantum mechanics , not a gaussian quantum mechanics , in that it will be the quantum analog of a levy process , not a brownian motion , as the ordinary schrodinger equation is . to define schrodinger quantum mechanics , you take the continuum limit of a nearest neighbor amplitude random walk . to do this , i will first remind you of the standard imaginary time map between random walks and quantum mechanical systems . when you have a stochastic process on a discrete space in discrete time , you have a transition operator : $$ \rho_j ( t+1 ) = \sum_i \rho_i ( t ) k_{i\rightarrow j} $$ where $k_{i\rightarrow j} = k_{ij} $ is a stochastic matrix : $$ \sum_j k_{ij} = 1 $$ these stochastic matrices generically have a stationary distribution , which i will call $\rho^0$ . i will assume that this stationary distribution obeys detailed balance , or in mathemtical jargon , that it is the " reversing measure " for k : $$ \rho^0_i k_{ij} = \rho^0_j k_{ji}$$ this says that the transitions between states i and j balance in equilibrium separately from any other transitions . the stationary distribution for random walk on a graph obeys detailed balance and it is ${1\over d ( i ) }$ where d is the degree of the vertex . when you take the continuous time limit , you make k equal to the identity plus an infinitesimal transition rate , and the stochastic equation becomes : $$ {d\over dt} \rho_j = \sum_i \rho_i r_{ij} $$ and you still have a stationary distribution $\rho0$ for the continuous time case . now you can define a symmetric h from the continuous time random process : $$ h_{ij} = {1\over \sqrt{\rho^0_i}} r_{ij} \sqrt{\rho^0_j} $$ and the detailed balance condition gives you symmetry of h . you then can define the imaginary time continuation as a standard quantum mechanical unitary time evolution , generated by this hamiltonian . this is the most abstract form of wick continuation . if you do this process on a random walk whose limit is a brownian motion , you get ordinary schrodinger quantum mechanics . if you do the same process on a random walk which takes steps of size s according to a distribution : $$ p ( s ) \propto {1\over s^{1+\alpha}}$$ where $0&lt ; \alpha&lt ; 2$ , you get levy quantum mechanics . so to define quantum mechanics on a cantor set , all you need is an appropriate stochastic motion . the ordinary brownian motion fails to have a limit , it just stays still on the cantor set--- it ends up fully localized . but the levy process generalizes just fine . the cantor set can be defined as all base 3 numbers with digits which are all 0 or 2 . a discrete approximation is truncating this at n digits . define a random walk on this graph by toggling a digit between 0 to 2 at digit position k with a rate which goes as : $$ e^{-ak} $$ where $a&gt ; 0$ . if you take the limit of continuous time , timesteps of size $\epsilon$ , and $a= {a\over \epsilon}$ , you get a hop which is a power law in size ( since it is an exponetial distribution on exponentially shrinking sizes , and this is a powerlaw in the size ) , and the continuum limit is levy quantum mechanics restricted to the cantor set . this is related to the question of localizing dirac fermions , since the |k| dispersion relation is levy . you do not localize levy particles with a local potential , unlike normal schrodinger particles . this was the subject of this question : how to localize the massless fermions in dirac materials ? .
at the start of the launch , the rocket has the largest mass of its entire flight . any rocket that can make it to orbit necessarily is fairly big , making its fully loaded mass enormous . the combination of large size and large mass makes its relative air drag smaller than compared to a smaller and less massive rocket . the rocket 's speed is also a consideration . at maximum acceleration , the rocket becomes supersonic only after it has reached the very upper limits of the troposphere . this means it only start moving really fast after it has climbed above the most dense parts of the atmosphere . since air resistance depends quadratically on speed , but the air density drops roughly exponentially with altitude , air resistance is hardly a consideration at all ( for large rockets ) . naturally , if there is no air at all , there is no air resistance , so less propellant would be required in all . but with regard to fuel efficiency : for any rocket that can make it to orbit , removing the whole atmosphere would be less effective than launching that rocket from the top of mt . everest : )
strictly according to double slit experiment calculations , yes infinite fringes are possible if ( 1 ) the slits are infinitesimal and placed infinitely apart , but actually , the fringes will be very close to each other so your eyes will not be able to differentiate and secondly each of the fringe will have almost $0$ intensity . so for obtaining infinite fringes you also need to have an infinite screen . the above results come from the fact that for a fringe : $\frac{dsin\theta}{\lambda}=n$ now for $n$ to be grow without bound , $d$ should be grow without bound . also replying on your suggestion that a screen placed infinitely far from the sources , you seem to assume that the distance between the fringes will get small , but this is not the case as the distance between two consecutive fringes is $\frac{\lambda d}{d}$ so if you get $d \rightarrow \infty$ the fringes also get infinitely apart , thus even if you will have an infinite screen length , you will only be able to say a finite number of fringes on the screen as $\frac{\infty}{\infty}$ will be finite in this case as both are dependent on exponent $1$ of length hence are of same order , here the ratio of infinities is taken to denote number of fringes on the infinite screen . intuitively , you may think that the $sin\theta$ of the maxima gets smaller , as the screen is getting farther away but what also is happening is that $n$ and $\lambda$ are finite so to get to second maxima , you have to travel an infinite distance on the screen to change$\theta$ by an appreciable amount . edit : it actually if you see carefully is a case of similar triangles . if the distance from the source gets infinity , then the fringe width gets infinity , if you draw out the case of two screens such that one is behind other , you will see because for fixed $n , d , \lambda$ $\rightarrow$ $\sin\theta$ is constant .
first , one inevitably gets the same solutions if he solves the problem in the slab 's rest frame , and then lorentz-transforms the result to the frame where the slab is moving ; or if one solves the problem directly in the frame where the slab is moving . the reason is that maxwell 's equations are covariant under the lorentz transformations . so if they are satisfied in one frame , they will be satisfied in any frame related by boosts , too . however , we must properly transform all the magnetizations and material relations etc . and add the corresponding moving sources which will be the main subtlety in the text below . in your particular problem , one may say some generic statements about the magnetic ( and electric ) fields without much thinking . for example , if $\vec m$ is in the $x$-direction , it means that the electrons may be thought of to spin in the $yz$-plane . take a surface of the slab parallel to the $yz$-plane - i.e. one face that belongs to a $x=x_0$ plane . it is pretty clear that there will inevitably be a component of the magnetic field $\vec b$ in the $x$-direction near the external side of the surface . if one boosts the $b_x$ magnetic field in the $z$-direction , there will inevitably be a nonzero electric field in the $y$-direction , $e_y$ . in the frame where the slab is moving , we seem to have no electric sources $\rho$ of the $\mbox{div}\ , \vec d=\rho$ gauss 's law and no right-hand side of the maxwell-faraday equation , $\nabla\times \vec e = -\partial \vec b / \partial t$ . so because there are no electric sources , you would think that the electric field should vanish . however , this is a flawed argument because the form of maxwell 's equations we are using here are only " maxwell 's equations for materials at rest " . in particular , the gauss 's law is optimized for $\vec d$ which we are imagining to be given by $\epsilon \vec e$ , and is " purely electric " . however , for a moving material , there should be an extra term of the type $\vec v\times \vec m$ included in $\vec d$ . because the latter has a nonzero $y$-component in the moving frame , there will be a nonzero $e_y$ in this frame , too . the precise form of maxwell 's equations in a moving medium may be confusing and unfamiliar so i think it may be a good idea to try to transform the local physics to the rest frame of any material , whenever needed , and perhaps lorentz-transform back . whenever subtleties would occur , one would have to revisit the derivation of the " macroscopic maxwell 's equations " ( for materials ) and redo it with the possibility of moving materials . microscopic maxwell 's equations alternatively , you could always try to use the microscopic maxwell 's equations which include the gauss 's law in the form $\vec \nabla\cdot \vec e = \rho / \epsilon_0$ . but in this form , $\rho$ includes not only free charges but also the " microscopic charges " related to the material . because the slab has nonzero values of $j_y$ and $j_z$ ( currents inside the material ) - recall that the electrons are kind of rotating in the $yz$-plane ( to produce the magnetic $x$-field ) , it is also true that when we boost the system in the $z$ direction , the corresponding multiple of $j_z$ will produce a nonzero value of $\rho$ ( microscopic charge density ) . this will be the source of the $e_y$ field discussed above . in particular , $j_z$ will be proportional to $ [ \delta ( y-y_1 ) -\delta ( y-y_2 ) ] $ in the slab 's frame which means that there will be $\rho \sim [ \delta ( y-y_1 ) -\delta ( y-y_2 ) ] $ in the frame where the slab is moving . it is this $\rho$ that will induce a nonzero value of $e_y$ right outside the material ( in the frame where the slab is moving ) .
the mass of the ring is wrong . the ring ends up at an angle , so its total width is not $dx$ but $\frac{dx}{sin\theta}$ you made what i believe was a typo when you wrote $$\text{d}m = \frac{m}{4\pi r^2}\cdot 2\pi \left ( r^2 - x^2 \right ) \text{d}x$$ because based on what you wrote further down , you intended to write $$\text{d}m = \frac{m}{4\pi r^2}\cdot 2\pi \sqrt{\left ( r^2 - x^2 \right ) }\text{d}x$$ this problem is much better done in polar coordinates - instead of $x$ , use $\theta$ . but the above is the basic reason why you went wrong . in essence , $sin\theta=\frac{r}{r}$ so you could write $$\text{d}m = \frac{m}{4\pi r^2}\cdot 2\pi \frac{r}{sin\theta} \ \text{d}x \\ = \frac{m}{4\pi r^2}\cdot 2\pi \frac{r}{\frac{r}{r}} \ \text{d}x\\ = \frac{m}{4\pi r^2}\cdot 2\pi r \ \text{d}x\\ = \frac{m}{2 r} \ \text{d}x$$ now we can substitute this into the integral : $$i = \int_{-r}^{r} \frac{m}{2 r} \cdot \left ( r^2 - x^2\right ) \ \text{d}x \\ = \frac{m}{2r}\left [ {2r^3-\frac23 r^3}\right ] \\ = \frac23 m r^2$$
if you are thinking about light the ' conservative answer ' would be : for this experment photon behaves as a wave , it doesnt have actual size , and so the only thing that will happen is that the wave will stretch its lenght , but the photon itself will not stretch as it has no diameter .
depends on the type of engine . if its a space engine , watch out for the exhaust . the rocket speed can be computed solely by its mass lost , but if the torus absorbs that then uhoh . also , the rocket engine speeds up the torus whilst accelerating , so finding when the two are equal is kind of tough . it is a fair assumption that the angular veloctiy added by the car to the torus is negligible though . the answer is yes , just imagine a frictionless car attached to the rail as you said , where the rocket is stationary compared to the earth or perhaps rather co-rotating with it . the rocket can not help being pulled down . i am a little worried with what you mean by 1g , since at the equator objects are both farther away from the center of the earth ( it is an ellipsoid ) and spinning more quickly . orient your torus parallel to the equator and perhaps mean corotating as in rotating at the same speed ( but not frequency ) as the equator . realize that equal frequencies will result in greater tangential speed for larger circles . wish i could bust out some equations but i am pretty bad at more than conceptual methods of viewing things . finally , about absolute rotation , realize that that is a thing . there is a difference between a spinning bucket and a non-spinning one , as newton showed ( just look at the water bulging ) . just because i am spinning with the bucket does not mean it will be the same reality , in fact it will be different . special relativity states that physics are the same in inertial reference frames . the answer is yes though , if you do not worry so much about the inverse square law of gravity , which i do not think you were . good question . thinking about these frames of reference will help your understanding of special relativity . remember-accelerating frames are not equal to inertial ones .
michael dine 's response , quoted with permission : i now have to think back , but the argument in qed is based on the spectral representation ( "kallen-lehman representation" ) . the argument purports to show that the wave function renormalization for the photon is less than one ( this you can find , for example , in the old textbook of bjorken and drell , second volume ; it also can be inferred from the discussion of the spectral function in peskin and schroder ) . this is enough , in gauge theories , to show that the coupling gets stronger at short distances . the problem is that the spectral function argument assumes unitarity , which is not manifest in a covariant treatment of the gauge theory ( and not meaningful for off-shell quantities ) . in non-covariant gauges , unitarity is manifest , but not lorentz invariance , so the photon ( gluon ) renormalization is more complicated . in particular , the coulomb part of the gluon ( $a^0$ ) is not a normal propagating field .
electricity ( and electromagnetic fields in general ) will invariably follow the " easiest " path . the entire point of a faraday cage is to provide the electricity from whatever discharge you are facing with an " easy " path that does not involve the person inside the cage . holes will not change a thing . why would the electricity even try to get through the holes if there is so much conducting chicken wire for it to go through ? electricity loves chicken wire , it hates air . so if there is chicken wire , it just goes through it . nonetheless it is perfectly possible to get electric shock inside a faraday cage . just install an electric socket inside the cage , linked to the electric grid outside , and then do something stupid like cram your fingers into the socket . . . ( mark the the frequency of any external discharge will not change a thing . electricity prefers conducting material to isolators like air and people , so the faraday cage will shield you from all external discharges . )
solutions to the klein-gordon equation can be interpreted as wave functions or operator fields . interpreting solutions as wave functions leads to relativistic quantum mechanics ( rqm ) . this has all the nasty negative probabilities you have heard of . rqm is rarely taught in classes ; people who need it learn it . interpreting solutions as an operator field , however , leads to one of the useful quantum field theories ( qft ) , namely spin-zero field theory or scalar field theory , or whatever else it is called . this theory has positive probabilities and energies . yay . a particularly straightforward way to derive spin-zero qft is to start with the rqm solutions to the klein-gordon equation . then , you just simply re-interpret the solutions as an operator-valued field , and impose particular commutation relations between the operators . bam , qft . source : chapter 3 of student friendly quantum field theory by r . klauber , some of which is available free online here .
the raw data from modern particle physics experiments are many terabytes ( even petabytes ) in size , and quite complicated . for collider experiments the detectors are compound , layered devices with three or more different technologies used by five or more distinct subsystems , plus ancillary monitoring of detector performance , temperature and humidity conditions in the experimental hall , data provided by the accelerator operating crew on the state of the beam , and on and on and on . there are ten of thousands of individual detector channels and hundreds of " slow " devices ( like the thermoments , magnet currents , beam current monitors , etc . . . ) . all of this has been pre-filterd by the trigger hardware ( and exactly what filtering was applied changes over time ) . for neutrino experiments the data are detailed information about the charge detected from photo-tubes ( some combination of total charge in a window , peak voltage , peak time , onset time , and/or digitized wave forms ) for hundreds or thousands of pmts . plus environmental monitoring like that done by the collider people . in both cases there scads of calibration data , changes in operating conditions throughout the data taking period , and sometimes replacement of re-tuning of sub-systems part way through . there is typically many tens of thousands of lines of custom computer code for opening and processing the data files . code written by physicists . now , particle physicists are a little more professional about coding then some of their peers , but that does not mean state of the art process and beautiful code . it generally takes many thousands of grad-student and post-doc hours to reduce this to something physics can be extracted from . there is a reason we call this " big science " . that said , you generally can get the data . eventually . ( each collaboration will hold theirs for a while to insure they get to publish first . ) how do you get it ? just ask . but you will have to provide your own storage ( and possibly copying hardware ) ; come to where the data are kept ; understand that the documentation will be scattered over hundreds or thousands of internal ( to the collaboration ) documents written as they went along by diverse authors some of whom have english as a second or third language ( and may evidence some idiosyncrasies ) ; and that help interpreting all this will be terse as these people have moved on and have other projects keeping them busy . and you may have to convince the people with the data that you have the capacity to manage it . the availability of partly processed data sets is not something i am as sure about , but you could try asking for that too . the worst that can happen is you get told " no " . but even if you can get this , do not image that it is easy to work with . if i have not dissuaded you , let me suggest a practical method for getting started . go to the nearest university that has a nuclear or particle physics group , and ask to help out . really . there is always a need for lab monkeys , and you will learn as you go along because you can not do the work if they do not teach you stuff . in the process you will learn how some of the sub-systems work . get a feel for what kind of raw data they return and how it is processed into less raw data . if you ask people will tell you how the less raw data can be transformed into still more physics-like information and eventually reconstructed into particles . make some contacts in the business . begin able to say " i work with prof . smith and podunk u . " is much better than " i am interested . " when it comes to getting access to data .
for any nonzero gravitational acceleration we know : ( 1 ) horizontal kinetic energy at impact = . 16 time vertical ke at impact 1.6/10 ( 2 ) thus horizontal velocity at impact is . 4 times vertical velocity at impact . ( 3 ) the time averaged vertical velocity is half of the end velocity . so the average horizontal vel is . 8 times the average vertical velocity , it should travel . 8 times as far horizontally as vertically . also $\sqrt{2}\times\sqrt{32} =\sqrt{64}=8$ , so you need not use your calculator . it is a good idea to have a reasonable guess as to what the result is , before trusting in calculator results ( its so easy to type it in wrong ) .
as others have pointed out in the comments , it is not really trivial to apply any model to reality , especially since we don’t know much about reality . however , we can make a few educated guesses and estimations and see how well everything fits together . first , assuming perfect weight distribution , we see that five screws would probably be sufficient to support your wall . and while the assumption is probably wrong , we can be relatively sure that a safety margin of 500% is a good start . second , we can look at how much a single screw has to support according to your diagram . an upper boundary for the area of the wall supported by a single screw seems to be four ‘rectangles’ , corresponding to about $$\frac{4}{44} \approx 0.09 \hat{=} 45.5\textrm{ lb }\hat{=} \frac{1}{3}\textrm{ screwweight}_{\textrm{max}}\quad . $$ that still looks rather good , doesn’t it ? third , we could check if there are any screws that , if they are removed , leave another screw with many more ‘neighbouring’ tiles . as far as i can see , the maximum would still be about six ( corresponding to approx . $\frac{1}{2}\textrm{ screwweight}_{\textrm{max}}$ ) . i would hence tend to say that you are fine , but there are many , many problems that could possibly arise ( not to mention that i am not a construction engineer and roughly followed http://xkcd.com/793/ ) . it appears that you are building some sort of sound-proofing . not to take into account possible issues with vibrations ( and hence faster wear of the screws ) seems silly . depending on how and where you fix things to the wall , you might have to deal with ugly resonances , both between the two walls and within the drywall . without knowing the speed of sound in drywalls , it is difficult to make any estimates here . everything else i didn’t think of .
some obscure thermal conductivity effect might heat the air inside to double the temperature , thereby doubling its pressure , but if the air expands and the pita does not let new gas in , it will not be able to expand more than a factor of 2 before the pressure is the same as outside . the expansion observed is a factor of a ten or more , so it is gas production . i think that the explanation is production of steam , and the critical temperature is close to the boiling point of water , give or take the effects of solutes in water trapped in the pita . steam production gives as much expansion as you want , and also , when i cut the pita open , i notice that it is full of steam . ( i do not think its such a terrible question either )
" equinox of date " means that in accounting for general precession , the equinox ( or roughly speaking , the origin of the equatorial and ecliptic coordinate systems ) is that for the date for which a computation is performed . alternatively , some standard equinox ( e . g . j2000.0 ) is used . catalogs are always referred to a standard equinox whereas ephemerides are ( usually ) referred to the equinox of date . the difference is significant when pointing a telescope .
some definitions might be useful : potential : the potential energy per unit charge , $v = \frac{u}{q}$ . potential depends only on the environment and the location , not on what is placed at that location . voltage : a difference in potential , $\delta v$ , between two points in the same environment . you can think of this as the change in potential energy per unit charge for a test charge moving between the two points . so the first answer i would give you is that potential energy depends on the test charge ( $+q$ in your example ) , but voltage does not , because it is per unit charge . but i think what you really mean to ask is , why is not the potential energy of a capacitor , $\frac{1}{2}cv^2$ , the same as the potential energy of a charge moving across the capacitor , $qv$ ? that is because the potential energy of a capacitor represents the energy that had to be put in to move all the charges that are already in the capacitor . the first charge $q$ to be moved from one plate to the other did not need any energy to do it , because at the beginning , the plates were uncharged , and thus at the same potential . after one charge had been moved , there was a potential difference $v_1 = q/c$ . in order to overcome that potential difference , the next charge needed energy $qv_1 = q^2/c$ . after the second charge had been moved , there was a potential difference $v_2 = 2q/c$ . so the third charge needed energy $qv_2 = 2q^2/c$ . . . . and so on . adding all these up gives $$\frac{q^2}{c} + \frac{2q^2}{c} + \frac{3q^2}{c} + \cdots + \frac{nq^2}{c} = \frac{ ( n^2 + n ) q^2}{2c} \approx \frac{ ( nq ) ^2}{2c}$$ where $nq$ is the total charge on the capacitor . ( $n$ particles , each of charge $q$ . ) of course in practice , we consider infinitesimal elements of charge , and do an integral instead : $$\int_0^{q_\text{total}} \frac{q}{c}\mathrm{d}q = \frac{q_\text{total}^2}{2c}$$
since this is homework , we are not supposed to give you the answer . but one mistake you made is in your formula for the magnitude of $r$ - the inner square root needed to be squared . so the length of $r$ is simply the square root of the sum of the squares of the $i$ , $j$ and $k$ lengths . good luck . . .
a simple google-book search of " solar declination " lead me to this google-book preview of solar energy engineering : processes and systems by soteris a . kalogirou . this book gives the spencer formula as equation ( 2.6 ) , on page 55 .
the reason you are not finding scale invariance in your equation is that you have inserted a factor of $\lambda^2$ in the first two terms , which as far as i can see has no reason to be there . i am guessing you put it in because of your uncertainty as to how to dilate derivatives . transforming a derivative is nothing complicated , though . you simply make the transformation on the variable that is being differentiated with respect to , treat it as a differential in the denominator , and simplify . for example : $$\frac{\partial}{\partial x_i} \to \frac{\partial}{\partial ( \lambda x_i ) } = \frac{\partial}{\lambda\partial x_i} = \lambda^{-1}\frac{\partial}{\partial x_i}$$ because $\mathrm{d} ( \lambda x_i ) = \lambda\mathrm{d}x_i$ . when you do this , in each of the first two terms you get $\lambda^{-1}$ from the transformation of the field and $\lambda^{-2}$ from the transformation of the derivative , for an overall scaling factor of $\lambda^{-3}$ . and in the last term , you get $\lambda^{-3}$ from the fields alone . all those factors of $\lambda^{-3}$ then cancel out . the reason dimensionless couplings are preferred ( in certain applications ) is that they do not set any scale for the theory . if your coupling $g$ had a dimension of $l^{-1}$ , for example , then you could establish a characteristic length scale for that theory as $g^{-1}$ . but being scale invariant means that there should be no preferred length scale ; you can change any scale $x \to \lambda x$ without altering the theory . if you have a dimensionful coupling , that is obviously not the case , because you can distinguish between $x$ and $\lambda x$ by measuring them in units of $g^{-1}$ . you can get around that by having $g$ change under a scale transformation as well , but then it is no longer a coupling constant , which makes things more complicated .
you have used the right formula for the centripetal force , but the wrong radius . you have used $l$ as radius but it should be $\frac{l}{2}$ . redo the same calculation and you should get the right answer .
to modify the distribution of magnetic field ( irrespectively of the source - pms or coils ) , you need some material with non-unit magnetic permeability , such as steel . these materials ' concentrate ' the field lines , pulling them in away from the surrounding air ( which has a permeability of very nearly 1 ) . so to shield the outside world , you had need to create a ' circuit ' of permeable material ( let 's say steel ) , which channels the magnetic flux from the north pole of your magnet to its south pole . if you leave a little gap in one part of your circuit , the field will be enhanced in the gap .
these are basically ways of describing which leading diagram contributes the largest term to the cross-section ; they are named after which of the mandelstam variables characterizes the 4-momentum of the virtual particle . in t- and u-channel processes the 4-momentum of the exchange particle is space-like ( has a negative norm ) . in the s-channel the 4-momentum of the exchange particle is time-like ( has a positive norm ) . this necessitates the annihilation of the incident particles .
as an orbit goes around the earth , your challenge if you launch vertically is that once you reach the desired height you will then need to accelerate sideways to orbital velocity . for an l2 orbit , this is around 1km/s with respect to the earth . so you need to carry all that fuel up with you , to then burn it . that is a vast amount of mass wasted in the initial launch , so realistically it is not going to happen . in reality , the launch profiles used give you orbital velocity and height as efficiently as possible , to maximise payload .
first question : na for fast reactors sodium is better for faster reactors because it has a lower total cross section than water . fast reactors still has some moderation and obviously all types have some neutron loss due to absorption from the moderator in addition to whatever other materials may be in the core . for numbers , i am going to reference nist : h http://www.ncnr.nist.gov/resources/n-lengths/elements/h.html na http://www.ncnr.nist.gov/resources/n-lengths/elements/na.html sum up all of the cross sections for all types of reactions , which is the total scattering + absorption from that link . you find $82 \text{b}$ for hydrogen and $3.8 \text{b}$ for sodium . for water you add oxygen . combine that with density and atomic weight information to get the macroscopic cross sections . for water , we have $3.45 cm^{-1}$ and for na we have $0.115 cm^{-1}$ . these numbers are from my reference . a fast reactor primarily wants the moderator to do nothing . indeed , without a coolant or moderator the reaction works just fine . . . aside from getting cooled . even if a moderator would not absorb any neutrons , it would muck with your intent , which is to get the fuel atoms ( u , th , pu ) to absorb fast neutrons from fission . the reason you want fast neutrons to hit the fuel atoms includes : to breed new fuel , for which the isotope chain is only neurotically favorable with fast neutrons some isotopes are only fissile at fast energies , and at lower energies will have small fraction fission , meaning you could not sustain the chain reaction ( $k&lt ; 1$ ) with a thermal spectrum if you slow down the neutrons , these objectives will be thwarted . second question : d20 for thermal reactors heavy water is used in some designs because it is a superior moderator . for a single metric to give a figure of merit for moderation , my reference proposes : $$\text{moderating ratio} = \frac{\xi \sigma_s}{\sigma_a}$$ to break this down , $\xi$ represents the lethargy gain per collision , which is the best metric for " how much relevant energy is lost per collision " . then the cross sections are the macroscopic scattering on top and macroscopic absorption on the bottom . if you imagine neutrons in a vacuum with only the moderator to hit , then the higher the moderating ratio is , the more will make it to thermal energies . according to my reference , the moderating ratio for $h_20$ is $71$ and for $d_20$ is $5670$ . the main reason for this is that deuterium has a very low absorption cross section . funny enough , deuterium is actually vastly inferior to a simple proton on the basis of scattering cross-section as well as energy lost per collision , so the entire benefit comes from the lower absorption cross section . the following is my own calculation for the moderating ratio of $na$ . $$\frac{\xi \sigma_s}{\sigma_a} = \frac{ ( 0.0845 ) ( 3.28 ) }{ ( 0.53 ) } = 0.52$$ sodium is not a good moderator . it is 100 to 200 times worse than all the other common options , which include h , he , be , and c . reference : nuclear reactor analysis . james j . duderstadt and louis j . hamilton .
some good things to remember for basic circuits are that parallel pathways have the effect of increasing the area available to current flow , and because of this always lower resistance -- at least in the basic circuits i know about . another thing i remember is adding inverses always amounts to the product over the sum : ( 1/r1 + 1/r2 + 1/r3 + . . . ) ^-1 = r1r2r3 . . . /r1+r2+r3 . . . } this is a relationship seen in many areas of physics , most notably ( imo ) for the reduced mass of a system , which can make many calculations much simpler . since other people have basically already answered your question ( the angle reveals the length ) , i thought i would give my two cents , i hope it moves your studying along a bit faster . also , unless i missed something too , i think the lower length should be 5 pi/3 . if the path from a to b is a circle , then depending on how you write your fractions , the sum should be 6 pi / 3 or 2 pi . does this also suggest how , simple as it may be , one of my suggestions might speed things up ?
i will take a swing at this , but bear in mind that you probably will not get definitive answers because you are asking about two active and difficult areas of research ( pop iii star formation and re-ionization ) . i will answer the particular questions , but i am hoping you get a feel for the fact that we do not have clear-cut answers yet . during what range of years after the big bang did the stars form ? the consensus is somewhere in the range $20&lt ; z&lt ; 50$ , which corresponds to 50 myr $&lt ; t&lt ; $ 200 myr . the spread is quite large because star formation depends broadly on gas density and temperature . because the big bang produces some big density perturbations and some small ones , they will form stars at different times . what is the expected range of masses of these stars . . . this is really , really hard to answer . until about a year ago , consensus was settled on a very heavy mass distribution , with many ( if not most ? ) stars in a range 100-1000 $m_\odot$ ( see e.g. the 2004 review by bromm and larson ) . this is argued on the grounds that the smallest gravitationally unstable mass in a homogeneous isothermal gas , the jeans mass , runs like $t^{3/2}$ and primordial gas cannot cool as much as metal-polluted gas . the difference in temperature is about a factor of 30 , so pop-iii stars would naively be about 100 times heavier than modern stars , whose mass distribution seems to peak around 0.5 $m_\odot$ . however , recently ( as in , articles in science in the last few weeks ) have reported very detailed simulations of early star formation where the stars stop their own growth as they start to radiate . the result is stars that are a few times 10 $m_\odot$ . still very big by modern standards , but not as big as previously thought . so the jury is out , imo . . . . and what is the expected lifetime before they supernova ? well , it depends on how large they are , but broadly stars of about 40 solar masses last about a million years or less . how they evolve is unclear because it is difficult to compute the rate at which they might lose mass from their surfaces . i assume these stars resulted in the re-ionization of ism ( interstellar medium ) , so what is the evidence and estimates of the age of the re-ionization era ? the role of pop iii stars in reionization is not at all clear . this is slightly outside of my own work , but from my own knowledge i think it is quite well known that ionization was complete by about redshift $z\approx6$ , which is about a billion years after the big bang . remember that many pop iii stars could have already been born , evolved , and died , producing enough metals to produce pop ii stars in the next generation . working out just how much radiation had been poured out is theoretically very difficult . however , as far as i know , observations of a $z=7.085$ quasar have given us some idea that the intergalactic medium around it was not re-ionized . so its more likely that stars after pop iii ( and maybe agn/quasars ? ) had more to do with re-ionization because it happened broadly later than they were born . that is just wild speculation on my part , though .
boiling point of water changes with altitude because atmospheric pressure changes with altitude . so , how/why boiling point changes with pressure . there is good explanation of this at hyperphysics ( with diagrams ) . now , why does pressure change with altitude ? imagine you are swimming in water . deeper you go more pressure you feel , because there is more water above you . $p = p_0 + dh$ $p$ - pressure , $p_0$- pressure at the surface , $d$ - density of fluid , $h$- height/depth to free surface there is huge column of air above our heads . $\rightarrow h$ at sea level > $h$ at hill station . $\rightarrow p$ at sea level > $p$ at hill station . $\rightarrow$ boiling point at seal level > boiling point at hill station .
you should be able to avoid the non-continuous derivative at l/2 by splitting the integral into two parts 1 ) from 0 to $l/2 - \epsilon$ and 2 ) from $l/2 + \epsilon$ to l . then take the limit as $\epsilon-&gt ; 0$ of your expectation value for p . this will give you $&lt ; p&gt ; $ , which is nonzero . you will also need $&lt ; p^2&gt ; $ ( which you should find to be zero ) . the uncertainty in momentum can then be calculated from $&lt ; p&gt ; $ and $&lt ; p^2&gt ; $ .
from the lagrangian one can obtain the equations of motion , called the euler-lagrange equations . these equations are , in general and also in this case , differential equations . as far as i understand your level of knowledge , you do not know anything about this subject ? in differential equations you are looking for a whole function as a solution , not only a variable ( in this case the solution is the function you are looking for : plug in the time and get out the position of each mass ) . the functions not only occur in an algebraic equation , there are also derivatives of this function in this equations . so in your case the two functions you are looking for are $\theta_1 ( t ) $ and $\theta_2 ( t ) $ . unfortunately you can not analytically solve these equations stated in the wiki article : " it is not possible to go further and integrate these equations analytically , to get formulae for θ1 and θ2 as functions of time . it is however possible to perform this integration numerically using the runge kutta method or similar techniques . " this means : without numerics you have no chance , at least as far as i know .
it is not necessarily true . for a zero potential $v_2$ you have $p_2=0$ , whereas if $v_1$ is a rectangular pit , in general , $p_1&gt ; 0$ .
the opposite sign of the shifts is due to the conservation of the location of the center-of-mass or , equivalently , momentum conservation . at least for the simple system of 2 bodies , the animation on the page is being observed from the inertial frame of the center of mass . one may check this fact by seeing that the trajectories are periodic ( ellipses ) even if the masses are comparable and the initial velocities and locations are generic ( but allowing a bound state ) . in the center-of-mass frame , the total momentum of both bodies is zero . so if one of them moves in one direction , the other is moving in the opposite direction . their trajectories are really ellipses that are similar to each other ( one obtains one from another by scaling , multiplication by a negative number ) . it follows that if the apsis of one body is on one side from the center of mass , the apsis of the other body must be on the other side .
given a volume $\delta x$ ( i am assuming linear density here ) , the number of particles that get absorbed is going to be the number of particles in a given volume times the probability that an absorption occurs . this is just the density times the cross section , $n_0 \sigma$ . thus , the change in the number of particles crossing through that volume is the product of the probability that a neutron will got absorbed with the total number of neutrons in the volume \begin{equation} n_f = n_i - n_i ~n_0 \sigma ~\delta x \end{equation} for a one-dimensional problem . in the $\delta x \rightarrow 0$ limit , this becomes the differential equation \begin{equation} \frac{d n}{d x} = - n_0 \sigma ~ n \end{equation} whose solution is just the exponential decay .
the brownian motion $x ( t ) $ is non-differentiable , so a particular trajectory $x ( t ) $ can not extremize an action $s$ which would be a functional of $x ( t ) $ and its derivative , $\dot x ( t ) $ , because the derivative is not even well-defined and any expression of the type $\int [ \dot x ( t ) ] ^2 dt$ , the usual kinetic term in the action , diverges . ( see e.g. middle of page 2 of this paper to see the statement that there is no lagrangian , too . the paper does its best to construct something that is " as close as possible " to the normal lagrangian formulation . ) however , when you mention field theory , it is interesting to point out that the typical trajectories $x ( t ) $ that contribute to feynman 's path integral computation of ordinary quantum mechanics do resemble the brownian trajectories very closely . but the amount of zigzag motion is determined by the uncertainty principle and planck 's constant , not by adjustable collisions with the molecules of a liquid etc . there are many other differences in the physical interpretation , too .
an hbti works in a very similar manner to a fizeau / michelson stellar interferometer . but in an hbti , a correlation is made between fluctuations of amplitude ( intensity ) at points across a surface , unlike a fizeau/michelson which correlates fluctuations in phase . the timing of these fluctuations is much longer and this leads to a much larger tolerance in path length differences than with phase interferometers . it can be shown [ 1 ] [ 2 ] that the visibility $v$ at a baseline $d$ is equal to : $$v^{2}=\gamma^{2}= \frac{\langle i_1 * i_2 \rangle}{\langle i_1 \rangle \langle i_2 \rangle}$$ where $i_1$ and $i_2$ are the measurements at two separated detectors , and the angle brackets indicate time averages . [ 1 ] the intensity interferometer , hanbury brown . [ 2 ] optical stellar interferometry , labeyrie .
( 1 ) the first law is written in form of differentials themselves , so i think there may be no escape from using differential equations . ( 2 ) the way most commonly the first law is written is , $du=dq-dw_{\text{work done by the system}}$ . here dw is work done by the system . however , in subjects other than physics , more important quantity is the work done by the experimenter . ( this is quite common in chemistry ) as the process in thermodynamics are most " quasistatic" ( http://en.wikipedia.org/wiki/quasistatic_process ) , the container/piston is always in equilibrium . so , $\vec{f_{ext}}=-\vec{f_{int}}$ ( they are equal and opposite ) , then we have , $dw_{\text{by the system}}=-dw_{\text{on the system}}$ . and so the first law can be written as : $du=dq+dw_{\text{work done on the system}}$ . here dw is work done on the system . as the two $dw$s have different meanings we will not a different answer . ( 3 ) the internal energy of a gas is state variable/state function ( http://en.wikipedia.org/wiki/functions_of_state ) . $u$ depends only on the final and initial states of the system and not on what process was used to get from initial to the final state . so we can use a constant volume process to get $du$ which then can be used in any process without any modification .
there does not need to be an magnetic field in the inductor for there to be " back emf " ( i would prefer " induced emf" ) . the induced emf is the consequence of a changing magnetic field and not of a magnetic field itself and hence there can be a changing magnetic field even at zero magnetic field ( something like a positive acceleration downwards for a ball thrown upwards , momentarily at rest . velocity is zero but the rate of change is not ) . the induced emf is given by $e=-\frac{d\phi}{dt}=-l\frac{di}{dt}$ , where $\phi$ is the magnetic flux through the circuit ( inductor ) . as a matter of fact , in a simple ac generator , which works on the principal of electromagnetic induction , the value of the induced emf is maximum when the magnetic flux through the loop of the generator is zero . now , to derive an equation of the current as a function of time , at any time t:- $$e-ir=l\frac{di}{dt}=-e_i$$ where $e$ is the emf of the ideal battery and $e_i$ is the induced emf . rearranging the equation and integrating:- $$\int_0^t dt=\int_0^{i_s}\frac{l}{e-ir}$$ where $i_s$ is the current at infinite time , i.e. at steady state where there is no longer changing magnetic fields and hence no induced emf . this is given by $i_s=e/r$ since the inductor has no effect at steady state . solving the above equation gives us:- $$i ( t ) =i_s ( 1-e^{-\frac{t}{\tau}} ) $$ where $\tau=l/r$ is called the time constant . at time $t=0$ , the current is zero but the rate of change of magnetic field is non zero and hence the induced emf is equal to the battery emf ( the maximum value ) . as time passes , the induced emf reduces slightly , and the current starts slowly and rises steadily till it reaches the steady state at $t\rightarrow \infty$ . you can get the expression for the induced emf as $$e_i=-l\frac{di}{dt}=i_sre^{-\frac{t}{\tau}}$$ the back emf acts as an opposing emf ( principally like a battery of varying emf fixed in an opposing direction to the original battery ) , and its value is maximum at the beginning ( equal to $e$ ) and hence there is zero current , and its value starts dropping as the rate of change of magnetic field starts dropping exponentially , and becomes zero at steady state ( $t\rightarrow \infty$ ) where the rate of change of magnetic field is zero .
mass is not always first . for example we write newton 's law for the force between two objects as : $$ f = \frac{gm_1m_2}{r^2} $$ i do not think there are hard and fast rules . i suspect conventions have arisen over the years and we have all got used to what we learned at school , which was taught by teachers who are used to what they learned at school and so on . we tend to put constants first , as in the case above where newton 's constant $g$ is first , and in many cases the mass is a constant . for example when we write : $$ f = ma $$ in the vast majority of cases $m$ is constant and that is probably why we put it first .
just in view of the double universal covering provided by $su ( 2 ) $ , $so ( 3 ) $ must a quotient of $su ( 2 ) $ with respect to a central discrete normal subgroup with two elements . this is consequence of a general property of universal covering lie groups : if $\pi : \tilde{g} \to g$ is the universal covering lie-group homomorphism , the kernel $h$ of $\pi$ is a discrete normal central subgroup of the universal covering $\tilde{g}$ of $g= \tilde{g}/h$ , and $h$ is isomorphic to the fundamental group of $g$ , i.e. $\pi_1 ( g ) $ ( wich , for lie groups , is abelian ) . one element of that subgroup must be $i$ ( since a group includes the neutral element ) . the other , $j$ , must verify $jj=i$ and thus $j=j^{-1}= j^\dagger$ . by direct inspection one sees that in $su ( 2 ) $ it is only possible for $j= -i$ . so $so ( 3 ) = su ( 2 ) /\{i , -i\}$ . notice that $\{i , -i\} = \{e^{i4\pi \vec{n}\cdot \vec{\sigma}/2 } , e^{i2\pi \vec{n}\cdot \vec{\sigma}/2 }\}$ stays in the center of $su ( 2 ) $ , namely the elements of this subgroup commute with all of the elements of $su ( 2 ) $ . moreover $\{i , -i\}=: \mathbb z_2$ is just the first homotopy group of $so ( 3 ) $ as it must be in view of the general statement i quoted above . a unitary representations of $so ( 3 ) $ is also a representation of $su ( 2 ) $ through the projection lie group homomorphism $\pi : su ( 2 ) \to su ( 2 ) /\{i , -i\} = so ( 3 ) $ . so , studying unitary reps of $su ( 2 ) $ covers the whole class of unitary reps of $so ( 3 ) $ . let us study those reps . consider a unitary representation $u$ of $su ( 2 ) $ in the hilbert space $h$ . the central subgroup $\{i , -i\}$ must be represented by $u ( i ) = i_h$ and $u ( -i ) = j_h$ , but $j_hj_h= i_h$ so , as before , $j_h= j_h^{-1}= j_h^\dagger$ . as $j_h$ is unitary and self-adjoint simultaneously , its spectrum has to be included in $\mathbb r \cap \{\lambda \in \mathbb c \:|\: |\lambda|=1\}$ . so ( a ) it is made of $\pm 1$ at most and ( b ) the spectrum is a pure point spectrum and so only proper eigenspeces arise in its spectral decomposition . if $-1$ is not present in the spectrum , the only eigenvalue is $1$ and thus $u ( -i ) = i_h$ . if only the eigenvalue $-1$ is present , instead , $u ( -i ) = -i_h$ . if the representation is irreducible $\pm 1$ cannot be simultaneously eigenvalues . otherwise $h$ would be split into the orthogonal direct sum of eigenspaces $h_{+1}\oplus h_{-1}$ . as $u ( -1 ) =j_h$ commutes with all $u ( g ) $ ( because $-i$ is in the center of $su ( 2 ) $ and $u$ is a representation ) , $h_{+1}$ and $h_{-1}$ would be invariant subspaces for all the representation and it is forbidden as $u$ is irreducible . we conclude that , if $u$ is an irreducible unitary representation of $su ( 2 ) $ , the discrete normal subgroup $\{i , -i\}$ can only be represented by either $\{i_h\}$ or $\{i_h , -i_h\}$ . moreover : since $so ( 3 ) = su ( 2 ) /\{i , -i\}$ , in the former case $u$ is also a representation of $so ( 3 ) $ . it means that $i = e^{i 4\pi \vec{n}\cdot \vec{\sigma} }$ and $e^{i 2\pi \vec{n}\cdot \vec{\sigma}/2 } = -i$ are both transformed into $i_h$ by $u$ . in the latter case , instead , $u$ is not a true representation of $so ( 3 ) $ , just in view of a sign appearing after $2\pi$ , because $e^{i 2\pi \vec{n}\cdot \vec{\sigma}/2 } = -i$ is transformed into $-i_h$ and only $i = e^{i 4\pi \vec{n}\cdot \vec{\sigma}/2 }$ is transformed into $i$ by $u$ .
old ways used schrodinger 's equation 's solutions for the atoms and mapped the square of the wave function . . since the solution fitted the spectrum of the atom it was accepted that the orbital was also correct . recently there has been an experiment that measured the orbitals of the hydrogen atom the abstract from the link : to describe the microscopic properties of matter , quantum mechanics uses wave functions , whose structure and time dependence is governed by the schrödinger equation . in atoms the charge distributions described by the wave function are rarely observed . the hydrogen atom is unique , since it only has one electron and , in a dc electric field , the stark hamiltonian is exactly separable in terms of parabolic coordinates ( η , ξ , φ ) . as a result , the microscopic wave function along the ξ coordinate that exists in the vicinity of the atom , and the projection of the continuum wave function measured at a macroscopic distance , share the same nodal structure . in this letter , we report photoionization microscopy experiments where this nodal structure is directly observed . the experiments provide a validation of theoretical predictions that have been made over the last three decades . . a popularization is here . after zapping the atom with laser pulses , ionized electrons escaped and followed a particular trajectory to a 2d detector ( a dual microchannel plate [ mcp ] detector placed perpendicular to the field itself ) . there are many trajectories that can be taken by the electrons to reach the same point on the detector , thus providing the researchers with a set of interference patterns — patterns that reflected the nodal structure of the wave function . and the researchers managed to do so by using an electrostatic lens that magnified the outgoing electron wave more than 20,000 times . please note that the orbitals are a probability distribution for finding an electron in a specific ( x , y , z ) around the nucleus , not a matter density in the classical sense . this experiment is for one electron and from the description it does not seem it would work for higher atomic numbers , at least not as simply .
i would rephrase your question as : what is the experimental signature of a black hole . if there exists a definite experimental signature of a celestial body that defines a black hole , your question is answered . i found the following paper that clarifies the issue : classical black holes are solutions of the field equations of general relativity . many astronomical observations suggest that black holes really exist in nature . however , an unambiguous proof for their existence is still lacking . neither event horizon nor intrinsic curvature singularity have been observed by means of astronomical techniques . this paper introduces to particular features of black holes . then , we give a synopsis on current astronomical techniques to detect black holes . further methods are outlined that will become important in the near future . for the first time , the zoo of black hole detection techniques is completely presented and classified into kinematical , spectro-relativistic , accretive , eruptive , obscurative , aberrative , temporal , and gravitational-wave induced verification methods . principal and technical obstacles avoid undoubtfully proving black hole existence . we critically discuss alternatives to the black hole . however , classical rotating kerr black holes are still the best theoretical model to explain astronomical observations . from this it is evident that in contrast to most physics subjects where first there is experimental evidence needing explanation and then theory arrives , black holes are an " artefact " of general relativity theory , i.e. the concept carries all the baggage of gr . nevertheless , what is called a " black hole " in gr has some experimental signatures which any other competing gravitational theory would have to account for . it might not be in the enticing format of a " black hole " , but some data are there . these at the moment are consistent with the theoretical black hole expected from gr . so in a sense your question has as answer : " yes the definition of a black hole is within the terminology introduced by general relativity " and may not be defined outside it ; but also " no the experimental signatures do not depend on general relativity in order to exist , just their interpretation and attribution as a black hole " may be in question , if an alternative theory to gr succeeds to describe reality .
the degrees of freedom of a diatomic gas are as follows : 3 translational : the molecule can move in x , y and z-direction . 2 rotational : the molecule can in principle rotate around each axis . but consider rotations around the molecular axis ( connecting the h atoms ) : in this case , the physics does not change . another way of thinking about it is that the axial rotation mode only can store a vanishing amount of energy , compared to the others . the rotational modes are only available at higher temeratures , since the molecule has a moment of inertia that has to be overcome to start rotating . 2 vibrational : the atoms can wiggle together and apart , which is one degree of freedom . but there is also another one , which is harder to see : think of the molecule as an harmonic oszillator with kinetic and potential energy . if both atoms are displaced towards the middle , the molecule has a higher potential energy . the equations of the harmonic oscillator would normally fix the kinetic energy of the atoms in this case . but in a gas with its random kinematics it is totally possible that they have the " wrong " kinetic energies for their relative displacement . so , for the purposes of statistical mechanics , these are 2 further dofs , making seven .
as you stated , the degree of green is directly dependent on the thickness of glass you stare at ( beer-lambert law ) . it actually comes from the absorption of the other wavelengths by the glass . due to refraction , even when you look at the glass from a grazing angle in the air , the light rays bend to a higher angle in the glass which makes the light path through the glass shorter ( figure 2 ) . on the contrary , when you stare at the glass from the edge , total internal reflection makes the light rays travel through the whole length of the glass to your eye ( figure 3 ) .
as in physics in general , a suitable choice of coordinates makes our life so much better . time dilation in this problem is somewhat a more trivial effect , and the transformation of gravitational field is somewhat a more complicated phenomenon . with this in mind , let me reformulate slightly the two situations : case 2 . pendulum is at rest with respect to the earth ( and some observer moves with respect to them , observes time dilation etc etc ) case 1 . pendulum is set above the earth , which moves relativistically below it ( and some observer moves with the earth , observes time dilation etc ) so , let us settle the physics first , and the observer effects last . case 2: classical physics problem , nothing to settle . case 1: from the pendulum 's point of view , the gravitational field is generated by a moving body ( => the field is unknown ) . from the earth frame , a relativistic body moves in a gravity field ( => the equations of motion are unknown ) . one might transform the energy-momentum tensor of the earth from the earth rest frame to the pendulum frame , but special care should be taken about the fact that the earth ceases to be spherical in the new frame ( though its density does increase as $\gamma^2$ ) . additionally , it is not clear appriori that the motion of the earth does not cause any additional forces . i propose to use a straightforward yet more secure method of transforming the metric tensor from the earth frame to the pendulum frame , and hence obtain the gravity , acting on the pendulum . in the earth rest frame the metric tensor is known to be $$g_{\mu\nu}=\left ( \begin{array}{cccc} and 1-2u and 0 and 0 and 0 and \\ and 0 and 1-2u and 0 and 0 and \\ and 0 and 0 and 1-2u and 0 and \\ and 0 and 0 and 0 and -1-2u and \\ \end{array} \right ) , $$ where $u$ is the newtonian potential of the earth . this expression corresponds to the so called weak field limit , when the metric tensor is nearly flat . we use the standard notation of mtw ( $c=1$ , signature $ ( +++ - ) $ , einstein 's summation rule etc ) and refer to this book for further details on linearized gravity . transformation of the field to the pendulum frame : lorentz tranformation matrix is given by : $$ \lambda_{\mu&#39 ; }^{~\mu}=\left ( \begin{array}{cccc} and \gamma and 0 and 0 and \beta \gamma and \\ and 0 and 1 and 0 and 0 and \\ and 0 and 0 and 1 and 0 and \\ and \beta\gamma and 0 and 0 and \gamma and \\ \end{array} \right ) , $$ with $\beta=\dfrac{v}{c} , \gamma= ( 1-\beta^2 ) ^{-1/2}$ and $v$ being the relative velocity of the pendulum with respect to the earth rest frame . the transformed metric tensor is obtained by : $$g_{\mu&#39 ; \nu&#39 ; }=\lambda_{\mu&#39 ; }^{~\mu}\lambda_{\nu&#39 ; }^{~\nu} g_{\mu\nu}=\left ( \begin{array}{cccc} and 1-2u\dfrac{1+\beta^2}{1-\beta^2} and 0 and 0 and -\dfrac{4 u \beta}{1-\beta^2} and \\ and 0 and 1-2u and 0 and 0 and \\ and 0 and 0 and 1-2u and 0 and \\ and -\dfrac{4 u \beta}{1-\beta^2} and 0 and 0 and -1-2u\dfrac{1+\beta^2}{1-\beta^2} and \\ \end{array} \right ) $$ in the pendulum frame ( further primes in the indices are omitted ! ) : it is known that only the term $g_{44}$ determines the newtonian potential . one can see that by writing out the lagrangian for the pendulum : $$ \mathcal{l}=\dfrac{1}{2}g_{\mu\nu} u^\mu u^\nu=\\ =\dfrac{1}{2} ( ( u^1 ) ^2+ ( u^2 ) ^2+ ( u^3 ) ^2- ( u^4 ) ^2 ) -\\ -u ( ( u^2 ) ^2+ ( u^3 ) ^2+4 u^1 u^4 \beta \gamma^2+ ( ( u^1 ) ^2+ ( u^4 ) ^2 ) \dfrac{1+\beta^2}{1-\beta^2} ) $$ here $u^\mu$ is the 4-velocity of the pendulum . as the latter moves non-relativistically ( in its own frame ) , we may consider $u^4\gg u^1 , u^2 , u^3$ and $u^4\approx \mathrm{const}$ , which leaves : $$ \mathcal{l}=\dfrac{1}{2} ( ( u^1 ) ^2+ ( u^2 ) ^2+ ( u^3 ) ^2 ) -u ( u^4 ) ^2\dfrac{1+\beta^2}{1-\beta^2} $$ if the pendulum as a whole did not move with respect to the earth , we would have $\beta = 0$ and $$ \mathcal{l}_0=\dfrac{1}{2} ( ( u^1 ) ^2+ ( u^2 ) ^2+ ( u^3 ) ^2 ) -u ( u^4 ) ^2 $$ effectively , therefore , the pendulum in its rest frame experiences the gravitational field magnified by the factor of $\dfrac{1+\beta^2}{1-\beta^2}$ . the pendulum frequency is thus magnified by $\dfrac{ ( 1+\beta^2 ) ^{1/2}}{ ( 1-\beta^2 ) ^{1/2}}$ . remarks : the neglected terms in the lagrangian are either $\dfrac{v}{c}$ or $ ( \dfrac{v}{c} ) ^2$ smaller than the kept leading terms . hence , up to $\dfrac{v}{c}$ accuracy the direction of motion does not affect the pendulum frequency . finally , lets add time dilations to get the final answers . let the period of the pendulum in the case when observer , the earth , and the pendulum do not move with respect to each other be $t_0$ . then : case 1: in the pendulum frame , as we have seen it has the period of $\dfrac{ ( 1-\beta^2 ) ^{1/2}}{ ( 1+\beta^2 ) ^{1/2}} t_0$ . then in the observer frame , due to time dilation , the period is $\dfrac{1}{ ( 1+\beta^2 ) ^{1/2}}t_0$ . case 2: in the pendulum frame the period is $t_0$ . in the observer frame the period is $\dfrac{t_0}{ ( 1-\beta^2 ) ^{1/2}}$ . to conclude , the two cases are quite different due to the different physics happening . in one case the observed period changes due to the change of the reference frame , whereas in the other there is an additional factor due to the fact that the gravity of a moving source is not the same as that of a still source .
as commenters have pointed out , it is german strecke . note that $s$ is for displacement , whereas $d$ is for distance . distance is the distance along the path traveled by a body , whereas displacement is the birds-eye distance traveled . displacement can also be negative in 1-d , depending upon your reference positive direction . for some reason , strecke actually means distance , not displacement , but its symbol is used for displacement . you might want to check out this paper , it is got an analysis of the naming , mainly for electrodynamic units . a few symbols from the table at the end of the paper : $c$ ( speed of light ) comes from latin celeritas ; $i$ ( current ) comes from " intensity of current " in french ( intensite du courant ) . the $\mathbf{a}$-potential , $\mathbf{b}$-field , $\mathbf{h}$-field got their symbols from the alphabetic order of the others .
it comes from the normalization of the polyakov action , \begin{align*} s=\frac{t}{2}\int d^2\sigma\ , ( \dot{x}^\mu\dot{x}_\mu-{x'}^\mu{x'}_\mu ) . \end{align*} the canonical momentum is \begin{align*} \frac{\partial \mathcal{l}}{\partial \dot{x}^\mu}=t\dot{x}_\mu , \end{align*} and this gives the equal time commutator ( or poisson bracket ) that you wrote down , \begin{align*} [ x^\mu ( \tau , \sigma ) , \dot{x}^\nu ( \tau , \sigma' ) ] =\frac{i}{t}\eta^{\mu\nu}\delta ( \sigma-\sigma' ) . \end{align*} the way i think about this is that as the tension of the string goes to zero , the string becomes less and less classical ( alternatively , $t$ plays the role of $1/\hbar$ on the worldsheet ) .
in some sense , yes . but one can only measure a difference of phase . for example , interferences between two bose-einstein condensate can be interpreted as interferences due to the relative phase between the wave function of the two gases ( in a good approximation , all the atoms of each gas are described by only one wave function ) . another closely related example is the superfluid velocity ( of a superfluid system ) , which is related to the gradient of the phase of the superfluid . yet another example , a josephson junction allows to measure the relative phase of the order parameter of two superconducting piece of metal .
galaxies interact with each other due to their mutual gravitation . if matter had been exactly evenly distributed after the big bang then galaxies would not collide , but there would not be any galaxies anyway . the inhomogeneities in the matter distribution are believed to originate from quantum fluctuations that occurred when inflation ended . some regions ended up with higher than average matter density and some with lower than average matter density . in the overdense regions the mutual gravity of the matter overcame the expansion of spacetime and those regions collapsed to form galaxy superclusters , then clusters then galaxies . within a galaxy cluster the galaxies have essentially random velocities relative to each other , which is why there are sometimes collisions .
a supersymmetric extension for ${\mathrm{ads}}_4$ background was found by konstein and vasiliev in nucl . phys . b331:475-499,1990 , and later generalised by vasiliev in hep-th/0404124 to higher dimensions . in 4d , there are three classes of infinite-dimensional extended higher spin superalgebras which generate symmetries of the higher spin equations of motion on ${\mathrm{ads}}_4$ . in each case , the bosonic part contains a subalgebra of the form ${\mathfrak{so}} ( 3,2 ) \oplus {\mathfrak{g}} ( m ) \oplus {\mathfrak{g}} ( n ) $ , comprising the ${\mathrm{ads}}_4$ isometries and ${\mathfrak{g}}$ being either ${\mathfrak{u}}$ , ${\mathfrak{o}}$ or ${\mathfrak{usp}}$ . the corresponding higher spin superalgebras are denoted ${\mathfrak{hg}} ( m , n|4 ) $ . they contain the usual $n$-extended lie superalgebra ${\mathfrak{osp}} ( n|4 ) $ as a subalgebra only when $m=n$ . indeed , for $m\neq n$ , massless unitary irreps of ${\mathfrak{hg}} ( m , n|4 ) $ contain a different number of bosons and fermions . in the simplest class with ${\mathfrak{g}}={\mathfrak{u}}$ , bosons have all integer spins $\gt$ 1 and are in the adjoint of ${\mathfrak{u}} ( m ) \oplus {\mathfrak{u}} ( n ) $ while fermions have all half-integer spins $\gt$ 3/2 and are in the bifundamental of ${\mathfrak{u}} ( m ) \oplus {\mathfrak{u}} ( n ) $ . ( the standard spin 2 graviton is contained in a diagonal ${\mathfrak{u}} ( 1 ) $ factor . ) the amount of extended higher spin supersymmetry in this sense is therefore unconstrained .
thanks to @user12262 for pointing me in the direction of the kww function . after perusing that link and searching scifinder for stretched and compressed exponential functions in relation to nmr , i ran across this paper ( subscription required , sorry ) . to ( briefly ) summarize the paper , the compressed exponential function , $e^{-kt^q}$ , with $1 &lt ; q &lt ; 2$ can be represented as a distribution of gaussian functions with different relaxation rates , $$r_c ( t ) = \frac{1}{\pi} \int^∞_0 p_c ( s ; q ) \ , e^{− ( sr^*t ) ^2} d\textrm{s} , $$ where $r_c ( t ) $ is the observed decay curve , $p_c ( s ; q ) $ is the probability distribution of gaussian decays , $r^*$ represents some average value of the rate , and $s = r/r^*$ . as the value of $q$ approaches 2 , the distribution function approaches a delta spike ( as one would expect ) . in the case of nmr $t_2$ decays , this most likely represents a distribution of relaxation couplings ( e . g . interactions with 1 , 2 , 3 , etc . other nearby spins ) .
no , the distribution does change . if the ' central ' person measures both the qubits in the $ |0\rangle$ , $ |1\rangle$ basis , say , then the observers do not see the same distribution in general . only their measurements along the z-axis still have the same distribution , but by tilting their measurement axis , they can conclude whether the qubit was measured or not . more concretely , let the state being prepared be $$ |\psi \rangle = \frac{|01\rangle + |10 \rangle}{\sqrt{2}} $$ then the density matrix corresponding to this ( pure ) state is $$ \rho = \frac{1}{2} \left ( \begin{array}{ccc} 0 and 0 and 0 and 0 \\ 0 and 1 and 1 and 0 \\ 0 and 1 and 1 and 0 \\ 0 and 0 and 0 and 0 \\ \end{array} \right ) , $$ whereas the state of the system after measurement by the central person would become $$ \rho_m = \frac{1}{2} \left ( \begin{array}{ccc} 0 and 0 and 0 and 0 \\ 0 and 1 and 0 and 0 \\ 0 and 0 and 1 and 0 \\ 0 and 0 and 0 and 0 \\ \end{array} \right ) , $$ which is not a pure state and is distinguishable from the previous state .
i have seen bubbles made with hydrogen . this is a popular trick with the various lecturers who do fireworks related lectures because the bubbles make a satisfying pop if you ignite them . a bubble is mainly stabilised by layers of surfactant adsorbed at the gas/water interface . as the bubble wall thins , the adsorbed surfactant layers at the opposite gas/water surfaces come into contact and prevent further thinning . this is a purely kinetic barrier as the gas/water surface tension is still greater than zero ( i.e. . you had reduce the overall energy by reducing the surface area ) but the rate of desorption of surfactant from the surface is slow . in principle the gas will affect the adsoption of the surfactant at the gas/water interface and possibly affect the stability , but in practice all common gases are so different from water that the relatively minor differences between gases makes little difference . bubbles can even be blown with steam as long as you keep the gas phase temperature above 100c . however , over medium timespans the gas inside the bubble will diffuse out through the water film and cause the bubble to shrink . the rate at which this happens will depend on the solubility of the gas and its diffusion rate in water . i am sure there will be differences between hydrogen and air , though i do not know of anyone who has actually measured it . i found papers reporting diffusion rates here and here , though both are behind paywalls . i had better luck with solubility figures . both hydrogen and helium are about a factor of ten less soluble in water than nitrogen , which would make their bubbles more stable than air bubbles though their greater diffusion rates will counteract this to some extent .
i think the problem here is that green , red and yellow/orange do not intuitively suggest different levels of conservation . in addition to that yellow/orange and red have a very poor contrast with each other . here 's what i suggest : in the full conserved region--> take blue in the partially conserved region--> take a much lighter shade the same kind of blue in non-conserved region --> take white the key to good making pretty presentations and related stuff is minimalism and clarity .
ok . i found the answer : $$ \partial v/\partial \theta = n_x \cos \psi - n_y \sin \psi $$ $$ \partial v/\partial \phi = n_x \sin \theta \sin \psi + n_y \sin \theta \cos \psi + n_z \cos \theta$$ $$ \partial v/\partial \psi = n_z $$ where $\theta , \psi , \phi$ are euler angles and $n_x , n_y , n_z$ are torque components .
two objects that are initially at rest with respect to each other have initially parallel world lines . however , the curvature of spacetime means that world lines that are initially parallel do not remain so . this is called geodesic deviation . ( image credit ) in the above image , the geodesic segments are parallel at the equator but , nonetheless , converge at the pole . imagine the time direction ( and remember , every object is relentlessly " moving " forward through the time direction even when " at rest " in space ) is along lines of longitude and the spatial direction is along lines of latitude . if the surface were , instead , a plane , the two geodesics would remain parallel and the objects associated with those world lines would not move towards or away from each other .
a conducting body can have a potential , and it need not be zero . potential can be arbitrarily set , depending upon your reference potential . the only difference in the tratement of conducting bodies is that they must be equipotential , i.e. , they must have constant potential at all points inside them ( but not necessarily points inside cavities ) . the potential of a metal shell due to its own charge $q_1$ is $\frac{kq_1}{r_{shell}}$ if you add a point charge $q_2$ at the center , then the potential becomes $\frac{kq_1}{r_{shell}} + \frac{kq_2}{r_{shell}}$ . remember , potential at a point is can be defined with respect to the work required to get a test charge there from infinity . if the field at a point is zero , that does not imply that the field is zero all the way to infinity . it just means that you can jiggle a test charge in the neighborhood of that point without doing work .
you are right about the equation of motion for an object in free fall with air resistance ( well , almost right : your $c$ is not the usual definition of the drag coefficient ) , but when you integrate it , you do not go from $v^2$ to $s^3/3$ . that only works when the thing being squared is actually the variable of integration : $\int t^2\mathrm{d}t = t^3/3$ , but $\int f ( t ) ^2\mathrm{d}t \neq f ( t ) ^3/3$ . to properly solve the equation , you will need to start by finding speed as a function of time . you can write the equation as $$\frac{\mathrm{d}v}{\mathrm{d}t} = g - \frac{c}{m}v^2$$ this is a separable differential equation , so you can put everything involving the independent variable $t$ on one side and everything involving the dependent variable $v$ on the other side , $$\frac{\mathrm{d}v}{g - \frac{c}{m}v^2} = \mathrm{d}t$$ this can be integrated over $t$ , giving $$t = \int_{v ( 0 ) }^{v ( t ) }\frac{\mathrm{d}v}{g - \frac{c}{m}v^2} = \sqrt{\frac{m}{cg}}\tanh^{-1}\biggl ( \sqrt{\frac{c}{mg}}v\biggr ) $$ ( assuming $v ( 0 ) = 0$ ) . then you can solve this for velocity , $$\frac{\mathrm{d}s}{\mathrm{d}t} = v = \sqrt{\frac{mg}{c}}\tanh\biggl ( \sqrt{\frac{cg}{m}}t\biggr ) $$ which is another separable equation , $$\int_{s ( 0 ) }^{s ( t ) }\mathrm{d}s = \int_0^t\sqrt{\frac{mg}{c}}\tanh\biggl ( \sqrt{\frac{cg}{m}}t\biggr ) \mathrm{d}t$$ the result of that integration is $$s ( t ) = s ( 0 ) + \frac{m}{c}\log\cosh\biggl ( \sqrt{\frac{cg}{m}}t\biggr ) $$ i have written a blog post about a ( possibly ) interesting " application " of this calculation . the math above is basically a summary of part of that post .
that is a really good question . you are right that measuring the tranverse velocity is a very difficult measurement , mostly due to andromeda 's distance from the sun . the problem can be tackled in two ways : directly , and indirectly . direct measurements mean actually tracking a positional change between andromeda and even more distant objects assumed to be essentially at rest , like quasars . the recent discovery of water masers mentioned above should make this possible ; a transverse velocity of ~100 km/s is an angular shift on the order of 10 microarcseconds per year . this is much smaller than is possible with optical telescopes ; the extreme baselines of radio telescopes like the very long baseline array , however , do make direct measurements feasible . these observations are currently taking place , and we should have a published measurement within a couple of years . indirect measurements of andromeda 's transverse velocity use a few different techniques . the loeb et al . ( 2005 ) paper made their estimate based on the fact that m33 , a neighboring galaxy to andromeda , shows no sign that its stellar population has been disturbed by passing nearby andromeda . this constrains the possible range of directions and speeds of andromeda 's velocity . they combine this with data on m33 's orbit , plus simulations of how close the galaxies would have to be to show an effect , and estimate both a direction ( mostly eastward ) and speed ( $100 \pm 20$ km/s ) of andromeda 's proper motion . a second indirect method was published by van der marel and guhathakurta in 2008 ; they used information on the orbits of satellite galaxies orbiting m31 to estimate the center of mass ( or barycentre ) of our local group . since the position and velocity of the local group barycentre depend partially on m31 's orbit , they also estimated a transverse velocity . their result is -78 km/s w , -38 km/s n . the upcoming direct measurement of m31 's proper motion should answer which ( if either ) of these other estimates are correct . in addition , we are looking forward to answering several interesting questions regarding both the past and future of our local group of galaxies . stay tuned !
given the rather large volume of the universe , i suppose it is possible . not as an initial condition as far as i can tell though because of the conservation of angular momentum . however , given the right circumstances of impact events on a rogue planet ( with no other bodies to perturb its non-rotation ) , i suppose it is possible . highly unlikely , but theoretically possible . as to why planets rotate , cornell ( the home of carl sagan ) has a great explanation . what i am saying is that there will be no planets if there was no initial angular momentum in the primordial solar nebula . if a nebula with absolutely no rotation collapses , then there will only be a central non-rotating star and there will not be any planets . planets form out of a protostellar disk , which itself forms only because of the initial angular momentum of the cloud . the dynamics of a rotating body is of course controlled by forces like gravity . kepler 's laws are a direct consequence of gravity .
i find it is not a problem if one simply omits the $o ( \epsilon ) $ terms since we are taking the $\epsilon \rightarrow 0$ limit . the author just did not state it clearly .
i assume it is referring to specific orbital energy $$ \epsilon=\frac{v^2}{2}-\frac{\mu}{r} $$ where $v$ , $\mu$ and $r$ are the velocity , gravitational parameter and distance to the sun . in that case it would be the other way around . so if negative then it comes from the solar system , if positive it is extrasolar . but our solar system does not only consist of the sun , other celestial bodies are also part of it . these bodies , especially the gas giants , can also have a big influence on the trajectory of other bodies when close enough ( see gravity slingshots ) . so this means that the specific orbital energy does not guarantee that something came from inside or outside the solar system . good examples are the two voyager probes . but you mention spaceships , which would suggest that there is some control over it and is not just surrendered to the external forces ( mainly gravity ) . edit the path of an object in a gravitational well is described by $$ r ( \theta ) =\frac{a ( 1-e^2 ) }{1+e\cos{\theta}} , $$ where $\theta$ is the true anomaly , $a$ is the semi-major axis , which can be expressed as a function of $\epsilon$ $$ a=\frac{-\mu}{2\epsilon} $$ and $e$ is the eccentricity , which can also be described as a function of $\epsilon$ $$ e=\sqrt{1+\frac{2\epsilon h^2}{\mu^2}} $$ where $h$ is the specific angular momentum . an object will be able to escape the solar system if its trajectory extend infinitely far away ( $r=\infty$ ) . and this will be the case when $1+e\cos{\theta}=0$ . since $\theta$ is a variable and the rest a constants , from this follows that $e\geq1$ , so $e^2=1+\frac{2\epsilon h^2}{\mu^2}\geq1$ . and therefore $\frac{\epsilon h^2}{\mu^2}\geq0$ , but because $h^2$ and $\mu^2$ are always positive this will only be true when $\epsilon\geq0$ ( so when positive ) .
the term " rosenberg-coleman effect " originates from the article heliographic latitude dependence of the dominant polarity of the interplanetary magnetic field . it is also referred to as the " dominant polarity effect " . as the earth orbits the sun , the earth travels above and below the equator of the sun . according to rosenberg and coleman , the polarity of the interplanetary magnetic field ( imf ) at a given location in the solar system , such as earth , depends upon the corresponding latitude of the sun . according to this proposal , the imf at earth should be dominated by the southern solar pole from december 7th to june 6 , and the northern pole the other half of the year .
in the case of quantum field theory : first of all for a massless gauge field the most general form of the effective action will contain the renormalizable term $ \mathcal{l} = -\frac{1}{4 g^2} f^{\mu \nu} f_{\mu \nu} $ . this follows simply from lorentz invariance ( indices must be contracted properly essentially ) and gauge invariance . i do not want to turn this into a post concerning gauge invariance simply because you did not ask about that but i just want to say that while gauge invariance is a ' fake ' symmetry it is a symmetry of the action none-the-less for our purposes so our effective lagrangian should respect it . i also do not want to make this a post about renormalization so i just say that in general there will be higher order terms in the field strength ( $ ( f^{\mu \nu} f_{\mu \nu} ) ^2$ and so on ) in the effective action but these are irrelevant both in the formal sense of the word in field theory and in the sense that we do not care about them here since you were really only asking about the quadratic part of the action . if you just want the term quadratic in the gauge field $a^\mu$ it will take the form : $ a_{\mu} \pi^{\mu \nu} a_{\nu} $ which just follows from lorentz invariance . in momentum space the only thing we have around that has indices on it is the 4-momentum , $p^\mu$ and the metric $\eta^{\mu \nu}$ so $\pi^{\mu \nu}$ must take the form $\pi^{\mu \nu} ( q^2 ) = \pi ( q^2 ) \left ( \alpha \eta^{\mu \nu} + \beta q^\mu q^\nu \right ) $ . now from gauge invariance we know that $a^\mu \rightarrow a^\mu + q^\mu$ must be a symmetry or $\alpha q^\nu + \beta q^2 q^\nu = 0 $ hence we can take $\alpha = q^2 $ and $\beta = -1$ and the remaining overall constant can be absorbed into $\pi ( q^2 ) $ , giving $\pi^{\mu \nu} ( q^2 ) = \pi ( q^2 ) \left ( \eta^{\mu \nu} q^2 - q^\mu q^\nu \right ) $ . for a $u ( 1 ) $ gauge field the $ a_{\mu} \pi^{\mu \nu} a_{\nu} $ is the same things as $- \frac{1}{4 g_p^2} f^{\mu \nu} f_{\mu \nu} $ where $g_p $ is physical coupling constant . for non-abelian gauge fields there are 3 and 4 pt interactions as well in $ f^{\mu \nu} f_{\mu \nu} $ as well . so writing something like $\mathcal{l} =- \frac{1}{4 g_p^2} f^{\mu\nu} f_{\mu \nu} + a_\mu \pi^{\mu \nu} a_\nu $ is redundant , because $a_\mu \pi^{\mu \nu} a_\nu$ is all already there in the $f^{\mu\nu} f_{\mu \nu}$ . for a massive vector field there is no gauge invariance and so my $\alpha$ and $\beta$ above are not constrained . in this case the effective action takes the form $\mathcal{l}= - \frac{1}{4 g_p^2} f^{\mu\nu} f_{\mu \nu} - \frac{1}{2} m_p^2 a_\mu a^\mu $ and so part of the polarization tensor is not accounted for in the $f^{\mu\nu} f_{\mu \nu}$ term ( where $m_p$ is the physical mass ) . for a massless gauge field the polarization tensor takes the form $\pi_{ij} ( q^2 ) = \pi ( q^2 ) \left ( \delta_{ij} - \frac{q_i q_j}{q^2} \right ) $ if one makes the gauge choice $a_0= 0 $ and $\ q_i a_i = 0 $ .
here is a visualization : momentum is mass times velocity , so draw it as the area of a rectangle : if we change the mass and velocity a little , we change the momentum : the total change in the momentum is the sum of green , blue , and purple rectangles . their sizes are just length times width , so overall we have $\delta p = m\delta v + v\delta m + \delta v \delta m$ this looks like the answer you are seeking except for the extra term at the end . suppose we cut $\delta m$ and $\delta v$ down to one tenth their current size . then the first two terms become one tenth as large , but $\delta m \delta v$ becomes one hundredth as large . the purple box shrinks away much faster than the blue and green ones . therefore , for very small changes , we can ignore the purple box and write $\delta p = \delta ( mv ) = m\delta v + v\delta m$ we usually indicate this limiting procedure by changing the $\delta$ to $\mathrm{d}$ , so $\mathrm{d} p = \mathrm{d} ( mv ) = m\mathrm{d} v + v\mathrm{d} m$
there are tons of papers on the connection between quantum processes and probability theory ( though i do not understand why you single out coherent states - they do not play a special role in this connection ) . the theory of stochastic processes and the theory of quantum processes are the commutative and noncommutative side of the same coin , with many similarities . see , e.g. , the books by gardiner ( handbook of stochastic processes ) or barndorff-nielsen ( quantum independent increment processes : structure of quantum lévy processes , classical probability , and physics ) online is the following article by barndorff-nielsen http://www.jstor.org/stable/10.2307/3647584
@brandon is correct . you can compute the average kinetic for any free particle using the equipartition theorem , which gives $\langle e \rangle = \frac{1}{2}k_b t$ per quadratic degree of freedom , where $t$ is the temperature and $k_b$ is boltzmann 's constant . for free particles in 3d this gives $\langle e \rangle= \frac{3}{2} k_b t$ ; equating $\frac{1}{2} m_e v^2 = \frac{3}{2} k_b t$ ( in a classical approximation ) shows that the root-mean-square velocity $v$ of free electrons is temperature dependent . $^1$ in the above , the velocities are measured with respect to the environment at temperature $t$ -- we do not even need to consider the fact that the electrons you are talking about are presumably on earth , being accelerated around the sun in centripetal motion . even light will change its speed $c=c_0/n$ in an explosion , because the index of refraction $n$ of the surrounding medium will become inhomogeneous and fluctuate : it is only the speed of light $c_0$ in vacuum that is constant . $^1$note , as also pointed out by brandon , electrons often move at relativistic speeds , so $\frac{1}{2}m_e v^2$ is a poor approximation . $v$ in this formula can exceed the speed of light $c$ , for example . to quantitatively calculate the velocity you need the correction from special relativity . this does not qualitatively change the answer though .
in general physics course we assume maxwell 's equations as the result of many experiments . after that , in field theory we build the lagrangian which satisfies the maxwell equations . we also can build non-linear field theories of em interactions , but there is a requirement to getting of the maxwell equations in the limit of weak fields . so these methods are not connected with the derivation . but we can derive the equations by using some postulates , which generalize experimental facts . the number of postulates should be reduced to a minimum . maxwell 's equations can be earned from the coulomb 's law , special relativity theory and superposition principle ( more details you can see in my answer on this question ) . the other question is why do we need derivation of equations instead of postulating them . they satisfy the experiments , so that is enough for using them in practical cases . the postulating them is not much worse then deriving in this situation .
the question seems a bit odd because " time of maximum spring compression " is an odd concept . the spring compression is a function of time and the time of maximum spring compression is zero because it is an instant not a time interval . maybe the question means the time interval from the time the car first touches the spring to the time of greatest compression . assuming this is the case , and bearing in mind that because this is a homework question we are only allowed to give hints , the trick to doing this question is to realise that the spring behaves as a simple harmonic oscillator i.e. the compression of the spring from the moment the car touches it will be : $$d = a sin ( \alpha t ) $$ where $a$ and $\alpha$ are some constants that you need to calculate . the problem simplifies a lot if you think about the relation between the period of a harmonic oscillator and the amplitude of oscillation .
they will equalize pressure at the entrance to the tube between them . that pressure is the density of the fluid times the height from the bottom to the lowest point in the vortex , because the fluid at the lowest point has zero velocity and so is equivalent to standing fluid .
suppose the radius of the sphere is $r$ . if you will permit me , let 's calculate the electrostatic potential $\phi$ inside of the sphere , and then let 's use the definition $$ \mathbf e = -\nabla\phi $$ to determine the electric field . if you want , i can directly do the integral for $\mathbf{e}$ , but it is just a bit messier . in any case , we have $$ \phi ( \mathbf x ) = \frac{1}{4\pi\epsilon_0}\left ( \int_{|\mathbf x'|&lt ; |\mathbf x|}d^3x'\ , \frac{\rho}{|\mathbf x - \mathbf x'|}+\int_{r&gt ; |\mathbf x'|&gt ; |\mathbf x|}d^3x'\ , \frac{\rho}{|\mathbf x - \mathbf x'|} \right ) $$ let 's choose $\mathbf x = r\mathbf e_z$ as you did , for simplicity , then in spherical coordinates we have $$ d^3x ' = dr'd\theta'd\phi'\ , r'^2\sin\theta ' , \qquad |\mathbf x - \mathbf x'|=r^2+r'^2-2rr'\cos\theta ' $$ so we have $$ \phi ( \mathbf x ) = \frac{2\pi\rho}{4\pi\epsilon_0}\int_0^r dr'r'^2\int_0^\pi d\theta'\sin\theta' ( r^2+r'^2-2rr'\cos\theta' ) ^{-1/2}+ \big ( \int_r^r\cdots\big ) $$ where the $2\pi$ came from the integration in $\phi'$ . now , we make the substitution $$ u = r^2+r'^2-2rr'\cos\theta ' , \qquad \frac{du}{2rr'} = \sin\theta ' d\theta ' $$ so the integral becomes $$ \phi ( \mathbf x ) = \frac{\rho}{4\epsilon_0r}\int_0^r dr ' r'\int_{ ( r-r' ) ^2}^{ ( r+r' ) ^2}du\ , u^{-1/2} + \big ( \int_r^r\cdots\big ) $$ performing the integral in $u$ gives \begin{align} \phi ( \mathbf x ) and = \frac{\rho}{2\epsilon_0r}\int_0^r dr ' r' ( \sqrt{ ( r+r' ) ^2}-\sqrt{ ( r-r' ) ^2} ) + \big ( \int_r^r\cdots\big ) \\ and = \frac{\rho}{\epsilon_0r}\int_0^r dr ' r'^2 +\frac{\rho}{\epsilon_0}\int_r^r dr'r'\\ and = \frac{\rho}{\epsilon_0}\left [ \frac{r^2}{3} - \frac{r^2}{2} + \frac{r^2}{2}\right ] \end{align} where we have used the fact that $\sqrt{ ( r^2-r'^2 ) }$ equals $r-r'$ for $r&gt ; r'$ and $r'-r$ for $r&lt ; r'$ . finally , taking the negative gradient of the potential in spherical coordinates gives $$ \mathbf e ( \mathbf x ) = \frac{\rho }{3\epsilon_0}r\ , \mathbf e_r $$ which is correct as you can check via gauss 's law .
first of all i try to restate your question into a more clear form . consider $\mathbb r$ equipped with the equivalence relation : $x \sim y$ if and only if $x-y= 2k\pi$ with $k \in \mathbb z$ . the space ${\mathbb r}/ \sim$ of equivalence classes $ [ x ] $ is $\mathbb s^1$ also as a topological space using the quotient topology . next consider the standard actions of the lie group of translations $\mathbb r$ on the real line $\mathbb r$: $$t ( a ) x:=x+a\quad \forall x , a \in \mathbb r\: , $$ and define the representation of the translation group on $\mathbb s^1$ as $$t′ ( a ) [ x ] := [ t ( a ) x ] \:\forall x , a \in \mathbb r\: . \quad ( 1 ) $$ the map $\mathbb r\ni a \mapsto t′ ( a ) $ is in fact a representation of the translation group on $\mathbb s^1$ in terms of isometries of the circle ( when equipped with the standard metric ) . in particular , one has $t' ( 0 ) = id$ and $t' ( a ) t' ( b ) = t' ( a+b ) $ . however all that has nothing to do with compactness ( false ! ) of the translation group , even if the outlined procedure gives rise to a representation of that ( non-compact ) lie group on a compact manifold , in terms of isometries of that manifold . let us eventually come to the relation with the rotations group of $\mathbb r^2$: $so ( 2 ) \equiv u ( 1 ) $ . as $\mathbb r$ is the universal covering of $u ( 1 ) $ , with covering ( surjective lie group ) homomorphism : $$\pi : \mathbb r^1 \ni a \mapsto e^{ia} \in u ( 1 ) \: , \qquad ( 2 ) $$ every representation of the group of $\mathbb r^2$ rotations $u ( 1 ) $ is also a representation of the group of translations $\mathbb r$ . identifying $\mathbb s^1$ with $u ( 1 ) $ in the standard way , the natural action ( representation ) of $u ( 1 ) $ on the circle is trivially $$r ( e^{ia} ) e^{ix} = e^{i ( a+x ) } \qquad ( 3 ) $$ where the first $e^{ia}$ is viewed as an element of the group $u ( 1 ) \equiv so ( 2 ) $ and the other two are viewed as elements of the circle $u ( 1 ) \equiv \mathbb s^1$ . the interplay of $t ' , r$ and $\pi$ , as one easily proves is : $$r ( \pi ( a ) ) = t' ( a ) \quad \forall a \in \mathbb r\: . \qquad ( 4 ) $$ this is in agreement with the remark above that reps of $so ( 2 ) $ are also reps of $\mathbb r$ . thus , as a matter of fact , it is not possible to distinguish between the action of $\mathbb r$ and that of $so ( 2 ) $ on the circle $\mathbb s^1$ , though they are different groups and only the latter is compact ( and in a certain way related with the component of angular momentum orthogonal to $\mathbb r^2$ . )
physics is independent of our choice of units and for something like a length plus a time , there is no way to uniquely specify a result that does not depend on the units you choose for the length or for the time . any measurable quantity belongs to some set $\mathcal{m}$ . often , this measurable quantity comes with some notion of " addition " or " concatenation " . for example , the length of a rod is $l \in \mathcal{l}$ a measurable quantity . you can define an addition operation $+$ on $\mathcal{l}$ by saying that $l_1 + l_2$ is the length of a the rod formed by sticking rods 1 and 2 end-to-end . the fact that we attach a real number to it means that we have an isomorphism $$ u_{\mathcal{m}} \colon \mathcal{m} \to \mathbb{r} , $$ in which $$ u_{\mathcal{m}} ( l_1 + l_2 ) = u_{\mathcal{m}} ( l_1 ) + u_{\mathcal{m}} ( l_2 ) . $$ a choice of units is essentially a choice of this isomorphism . recall that an isomorphism is invertible , so for any real number $x$ you have a possible measurement $u_{\mathcal{m}}^{-1} ( x ) $ . i am being fuzzy about whether $\mathbb{r}$ is the set of real numbers or just the positive numbers ; i.e. whether these are groups , monoids , or something else . i do not think it matters a lot for this post and , more importantly , i have not figured it all out . now , since physics should be independent of our choice of units , it should be independent of the particular isomorphisms $u_q$ , $u_r$ , $u_s$ , etc . that we use for our measurables $q$ , $r$ , $s$ , etc . a change of units is an automorphism of the real numbers ; given two units $u_q$ and $u'_q$ , the change of units is $$ \omega_{u , u'} \equiv u'_q \circ u_q^{-1}$$ or , equivalently , $$ \omega_{u , u'} \colon \mathbb{r} \to \mathbb{r} \ni \omega ( x ) = u'_q ( u_q^{-1} ( x ) ) . $$ therefore , $$ \omega ( x+y ) = u'_q ( u_q^{-1} ( x+y ) ) \\ = u'_q ( u_q^{-1} ( x ) +u_q^{-1} ( y ) ) \\ = u'_q ( u_q^{-1} ( x ) ) + u'_q ( u_q^{-1} ( y ) ) \\ = \omega ( x ) + \omega ( y ) . $$ so , since $\omega$ is an automorphism of the reals , it must be a rescaling $\omega ( x ) = \lambda x$ with some relative scale $\lambda$ . consider a typical physical formula , e.g. , $$ f \colon q \times r \to s \ni f ( q , r ) = s , $$ where $q$ , $r$ , and $s$ are all additive measurable in the sense defined above . give all three of these measurables units . then there is a function $$ f \colon \mathbb{r} \times \mathbb{r} \to \mathbb{r} $$ defined by $$ f ( x , y ) = u_s ( f ( u_q^{-1} ( x ) , u_r^{-1} ( y ) ) . $$ the requirement that physics must be independent of units means that if the units for $q$ and $r$ are scaled by some amounts $\lambda_q$ and $\lambda_r$ , then there must be a rescaling of $s$ , $\lambda_s$ , such that $$ f ( \lambda_q x , \lambda_r y ) = \lambda_s f ( x , y ) . $$ for example , imagine the momentum function taking a mass $m \in m$ and a velocity $v \in v$ to give a momentum $p \in p$ . choosing $\text{kg}$ for mass , $\text{m/s}$ for velocity , and $\text{kg}\ , \text{m/s}$ for momentum , this equation is $$ p ( m , v ) = m*v . $$ now , if the mass unit is changed to $\text{g}$ , it is scaled by $1000$ , and if the velocity is changed to $\text{cm/s}$ , it is scaled by $100$ . unit dependence requires that there be a rescaling of momentum such that $$ p ( 1000m , 100v ) = \lambda p ( m , v ) . $$ this is simple -- $10^5 mv = \lambda mv$ and so $\lambda = 10^5$ . in other words , $$ p [ \text{g} \ , \text{cm/s} ] = 10^5 p [ \text{kg} \ , \text{m/s} ] . $$ now , let 's consider a hypothetical situation where we have a quantity called " length plus time " , defined that when length is measured in meters and time in seconds , and " length plus time " in some hypothetical unit called " meter+second " , the equation for " length plus time " is $$ f ( l , t ) = l + t . $$ this is what you have said - $10 \text{ m} + 5 \text{ s} = 15 \text{ " m+s"}$ . now , is this equation invariant under a change of units ? change the length scale by $\lambda_l$ and the time scale by $\lambda_t$ . is there a number $\lambda$ such that $$ f ( \lambda_l l , \lambda_t t ) = \lambda_l l + \lambda_t t $$ is equal to $$ \lambda f ( l , t ) = \lambda ( l+t ) $$ for all lengths and times $l$ and $t$ ? no ! therefore , this equation $f = l + t$ cannot be a valid representation in real numbers of a physical formula .
the left parentheses are equal to zero due to $u_{\rho}u^{\rho}=-c^2$ . this is true for timelike vectors in the ( -1,1,1,1 ) signature .
define " best " . as always , there is no one-size-fits-all answer . are you just a casual observer , looking mostly for naked-eye objects ? or are you looking through a telescope for deep-space stuff ? is your scope a go-to that can be interfaced with and controlled from the phone ? here are some examples , look at the features and decide what is best for you : skysafari 3 , either plain , or plus , or pro . my favorite . a lot of folks hauling big dobs and whatnot use it . star walk stellarium - good for casual naked-eye gazing but not much else . starmap 3d , either plain or plus pocket universe
in statics , you can still have a force without acceleration so $f$ is independent of $a$ . $f$ is the cause of the change in the position of an object initially at rest in some frame . to give it physical meaning , you have to define how it is to be measured and one way would be to define 1 unit of f causing one unit of compression in some standard spring . now if $f$ causes a body at rest to change its position , then over a time dt the postion has changed by dx . your job as a physicist is to construct an equation relating f to the change in velocity of the body . so with all this in mind , what would happen if $f=m*d^3x/dt^3$ ? it would mean that even though $f$ is the cause behind the change in velocity of a body , there are some changes in the velocity possible where $f = 0$ such as for $a = const$ . you would end up with particles accelerating in arbitary directions for $f = 0$ .
the ordinary twistor space is parameterized by $ ( \lambda^\alpha , \mu_{\dot\alpha} ) $ . here , the $\alpha$ is a 2-valued $sl ( 2 , c ) $ spinor index of one chirality and the dotted index is its complex conjugate , the index of the opposite chirality . at the level of spinors , vectors are equivalent to " spintensors " with one undotted and one dotted index . $$ v_\mu = \sigma_\mu^{\alpha \dot\alpha} v_{\alpha \dot \alpha} . $$ this is a basic fact about the lie algebras . $so ( 3,1 ) $ is locally isomorphic to $sl ( 2 , c ) $ - they are the same 6-dimensional lie groups - and the 4-vector is the tensor product of ${\bf 2}$ and ${\bf \overline{2}}$ . if you are unfamiliar with this equivalence of vectors of " spintensors " with two indices , notice that the components of a 4-vector may be organized as $$v^{\alpha\dot\alpha} = \left ( \begin{array}{cc} v^0+v^3 and v^1-i v^2\\ v^1+i v^2 and v^0-v^3 \end{array} \right ) $$ note that the determinant of this matrix - a natural function of the matrix elements - is simply $v^\mu v_\mu$ . the whole matrix may be understood as the appropriate combination of the three pauli matrices - plus the " time-like pauli " ( identity ) matrix multiplied by the time component of the vector . sorry if my signs deviate from the prevailing convention . so far , it has only been a story about the vectors or spinors . what about the twistors ? well , it is a simple one-line formula . if i have a point $x^\mu$ which is equivalent to the $x^{\alpha \dot\alpha}$ matrix - as any 4-vector - i may simply write an equation $$ \lambda^\alpha = x^{\alpha \dot\alpha} \mu_{\dot\alpha} $$ note that it is a set of two complex linear equations - for $\alpha=0,1$ - so it defines a linear object . for a different value of $x^\mu$ , i get different equations . moreover , the equations link the $\lambda$ and $\mu$ objects which are coordinates on the twistor space . now , i have to explain why the equations above define a line . first , the $\lambda$ and $\mu$ objects are pairs of complex numbers , so in total , we have four complex coordinates . however , the twistor space is a projective space . note that if the equations above are satisfied for some $\lambda$ and $\mu$ - four complex numbers - they will also be satisfied if you multiply both $\lambda$ and $\mu$ by an arbitrary complex number ( the same one for both ) . this projectivity holds universally : $\mu$ is naturally scaled in the same way as $\lambda$ because it may be understood as a dimensionally inverse object to $\lambda^{\dot\alpha}$ ( an object that appeared for the first time in this answer ) that scales in the inverse way relatively to $\lambda^\alpha$ in order to keep e.g. the vector $\lambda^\alpha \lambda^{\dot\alpha}$ constant . so the twistor space is really a complex projective space , $cp^3$ , and the equations above are two complex conditions , so we are left with a one ( -complex ) -dimensional object , a complex line . spacetime points are in one-to-one correspondence to complex lines in the twistor space . many more things may be translated . for example , if two spacetime points are separated by a null interval , the corresponding two lines in the twistor space intersect . the intersection - a point in the twistor space - may be identified with a null line in the minkowski space , and i could derive many other things of this kind .
it is a matter of flux . two factors enter the ability to detect gammas . the flux of the gammas , i.e. . how many per meter square per second , and the crossection of interaction . the crossection is dependent on the energy , and for a given energy is the same for extraterrestrial and terrestrial gamma rays . the flux is not . gamma rays no matter how high was the flux when they were created reach us from large to enormous distances , the flux spreading like 1/r^2 from the distant point ( for us ) source . terrestrial gamma rays are close to the satellites in comparison and the flux much higher than the extra galactic one . the probability of finding terrestrial gammas is measurable , a substantial number survives the trip through the atmosphere . extra terrestrial ones are few and the probability of surviving to the surface very small to zero .
air pressure is the intrinsic pressure in a quantity of air . it can come from any number of sources . perhaps there is a closed cylinder of air with a piston compressing it isothermally . or perhaps the air in the tire in your car is under pressure due to the weight of the car and the surface tension of the tire . atmospheric pressure is air pressure due solely to the weight of the air above you in the atmosphere . it is typically around $10^5\ \mathrm{n/m^2}$ on earth , but can vary with altitude and weather . it is just a " natural , " ubiquitous special case of air pressure in physical situations taking place inside an atmosphere . note that this is a very subtle distinction , and interchanging the terms " air pressure " and " atmospheric pressure " should cause absolutely no confusion in practice .
it is only for a spherically symmetric shape that you can treat an extended body as if it were a point mass at the com . the ringworld is stable against axial displacements after which it will gently bob back and forth around the star . unstable against transverse ones because the gravitational attraction of the near-side is large than that of the far-side .
he ignored the radius of the earth as negligible . his estimates for the angle were from the shape of the shadow the sun casts on the moon , and the difference between this and a straight line when the moon is halfway between full and new is too small to percieve precisely . he fooled himself into thinking he measured a different angle , so his estimate was really only giving a lower bound on the distance to the sun . as a lower bound , it was enough to establish that the sun is larger than the earth , and this was important , in that it lent strong support to heliocentric models . but it was not an accurate method .
consider a tiny part of th conductor 's surface . then the field at this part is approximately uniform so this is like an infinite parallel plane : $e = \sigma/2\epsilon_0$ . whence , the surface charge density is $\sigma = 2\epsilon_0 e$ . since it is a conductor , there is no volumetric charges : everything is concentrated in the surface .
i personally think the text is misleading . it is blindly applying gauss ' law while not considering its subtleties . here 's a more cause-and-effect way to look at it . after this , we will get to gauss ' law . let 's take a look at the positively charged plate . yes , the surface charge density on one side doubles . but the surface charge density on the other side goes to zero . a known result about infinite sheets of charge is that distance from the sheet does not affect the electric field . so , in effect , the region to the right of the positive plate is not affected by the redistribution of charge on this plate . the total surface charge density ( if you were to flatten the plate and look at it as truly two dimensional ) did not change . so , the positive plate 's effect on the electric field is not changed when the negatively charged plate is brought near . instead , it is the presence of the second negatively charged plate that doubles the electric field in the region between the plates , not the doubling of the surface charge density on one of the plates . there is only one doubling going on . in fact , if you were to somehow take two infinite sheets each with $\sigma$ , and separated them by some small distance , the electric field outside of the region between the plates would be identical to a single infinite sheet with $2\sigma$ . this is a crucial idea . i hope it is clear . why , then , is the text saying it has to do with the surface charge density ? they are applying gauss ' law to a gaussian pillbox with the left face in the middle of the positive conductor and the right face in the vacuum between the plates . with this choice of gaussian surface , there is only a flux through the right face . this is convenient for calculations . however , the electric field that gives rise to this flux is not due only to the enclosed charge , even though we mathematically calculate it like that . rather , the electric field one should use in gauss ' law is the total electric field due to all charge distributions , including charges outside the gaussian surface . this is one of the subtle but amazing facts of gauss ' law : to calculate the flux through a surface , you only need to mentally worry about the enclosed charge , but the result you get for the electric field ( if you can indeed extract the field from the flux , usually in a highly symmetric geometry ) is due to all of the charge , not just the enclosed charge . so , for your particular problem from h and r , the flux through the right face of the gaussian surface is caused by the superposed electric field from both plates . neither of these fields alone changed , but their effects superpose , causing a doubling of the net electric field . but when applying gauss ' law for this problem , we usually do not worry about what actually causes this electric field . the doubling of the electric field is " accounted for " mathematically by the doubling of the surface charge density on the positive plate . however , viewing this doubling of the charge as the cause of the now-doubled electric field is not correct in my opinion .
if we assume a circular orbit , the equation relevant to your question is given by the equality of gravitational to centripetal force : $$g\frac{mm}{r^2}=\frac{mv^2}{r} , $$ where $m$ is the mass of the satellite , $m$ the mass of the planet , $g$ the gravitational constant , $r$ the distance between the centers of mass of both bodies and $v$ the tangential velocity . you can solve this equation for $v$ and end up at $$v=\sqrt{g\frac{m}{r}} . $$ as you can see , there is one solution to this equation . it is determined by two variables : the mass of the planet and the radius .
it is not true that \begin{align} e_n = h+\hbar\omega_0 . \end{align} why ? well , $h$ is a linear operator while $e_n$ and $\hbar\omega_0$ are real numbers ; an operator cannot equal a real number . you might try to fix this by multiplying each of the real numbers by the identity operator and then claim that \begin{align} e_n i = h+\hbar\omega_0 i \end{align} this is still not correct . you can immediately tell that it can not be correct because the left hand side depends on $n$ , a non-negative integer , by the right hand side does not . so what is going on ? as essentially mentioned in the comments , the following statement is true : if $\psi$ is an eigenvector of the harmonic oscillator , then there exists a non-negative integer $n$ for which \begin{align} h\psi = \left ( n+\frac{1}{2}\right ) \hbar\omega\psi \end{align} where here we are using notation in which the harmonic oscillator hamiltonian is given by \begin{align} h = \frac{1}{2m} p^2 + \frac{1}{2}m\omega^2x^2 \end{align} where $p$ and $x$ are the position and momentum operators respectively . in other words , if you act the hamiltonian on an eigenvector , then it acts simply by multiplying that eigenvector by a real number , the corresponding eigenvalue . but when the hamiltonian acts on a vector that is not an eigenvector , this does not happen .
i assume with energy you mean electricity , and your generators sound like rechargeable batteries to me . in that case you have a series connection , which will simply double the voltage available . no , this would not generate infinite energy . whatever you do , each generator needs fuel that contains chemical energy . in the best ( =impossible ) case it would be entirely converted into electrical energy . whether you temporarily store a part of g1 's energy in g2 or not does not change anything about the fact that the total energy cannot exceed the chemical energy of your fuel ( unless of course there are other influences , e.g. a solar panel and the very energetic sun which will not run out of fuel for some billion years . . . )
to get a list of all physics and math articles on wikipedia , you could run catscan on category:wikiproject physics articles and category:wikiproject mathematics articles , like this . note that a lot of the pages returned by catscan will be article talk pages , since that is where the wikiproject templates normally go . depending on the output format you choose , you may need to postprocess the list to remove the Talk: prefix from page titles . you can then use special:export to download the actual articles .
first of all , velocity has a sign . after rebounding the velocity is in the opposite direction so $\delta v = ( 25 - ( -22 ) ) = 47 m/s$ $47 m/s / 0.0035 s = 1,342.9 m/s^2$ [ correction $13,429 m/s^2$ ]
at the particle level the verb " charge " has no definition . charge cannot be added to a particle . a particle has charge ( noun ) ; it is a quantum number that characterizes the particle , and its charge may be 0 , +/-1/3 , +/-2/3 , +/-1 ( and some resonances +/-2 ) . a photon has charge 0 , spin 1 and mass 0 . that is why it is called a photon and not an electron . if it is possible to bend it than why not charge that it can change direction ( bend ) is a kinematic effect and controlled by the equations of motion . the quantum numbers are intrinsic and unchangeable in the definition of each particle .
y is changing in time , but in your second solution you integrated the x equation as if it were constant , which is illegitimate . $${dy_p\over dt} = 2 \implies y=2t+y_0$$ $${dx_p\over dt} = y^2 = ( 2t+y_0 ) ^2 \implies x_p = {4\over 3}t^3 + 2y_0 t^2 + y_0^2 t + x_0$$ which is consistent with the first solution .
any reshaping of the droplet will require flow of water inside the droplet and there will be viscous losses . presumably the energy would come from an increased torque on whatever motor was moving the droplet and substrate .
the determinant is fairly easy to calculate . you know already , essentially , the eigenvalues of the stiffness matrix ; more accurately , you know the eigenvalues of the matrix $\mathbf{m}^{-1}\mathbf{k}$ , because the $\omega_i$ are zeros of the equation $$0=\det ( \mathbf{m}^{-1}\mathbf{k}-\omega^2 ) . $$ ( the more aesthetically minded would replace $\mathbf{m}^{-1}\mathbf{k}$ with $\mathbf{m}^{-1/2}\mathbf{k}\ , \mathbf{m}^{-1/2}$ to get a hermitian matrix , but no matter . ) if you express the second determinant in the corresponding eigenbasis , you get $$ \det ( \mathbf{k}-\mathbf{m}\ , \omega^2 ) =\det ( \mathbf{m} ) \det ( \mathbf{m}^{-1}\mathbf{k}-\omega^2 ) =\frac{m^3}2\det\begin{pmatrix} \omega_1^2-\omega^2 and 0 and 0 \\ 0 and \omega_2^2-\omega^2 and 0 \\ 0 and 0 and \omega_3^2-\omega^2 \end{pmatrix} , $$ which gives your textbook 's expression . more generally , this is an expression of the principle that a matrix 's determinant is the product of its eigenvalues . the adjunct , on the other hand , does not ( to my knowledge ) satisfy any such nice relation ; in any case it is a nasty beast to deal with and i think few people judiciously substituting in the definition $k=m\omega_2^2/2$ of $\omega_2$ instead of $k$ .
the physics of a gliding airplane are simple . there is potential energy , proportional to height above the ground . there is also kinetic energy , proportional to speed squared . first , understand the speed . if the plane is not slightly nose-heavy , it will fly a scalloped up-down cycle . if it does that , add a little weight to the nose , or distribute the wing area more toward the rear . assuming you have done that , you control the speed by turning up the trailing edges . the more they are turned up , increasing the angle of attack , the slower it flies . ( up to a maximum angle of attack , at which the wings stop working , or " stall " . ) back to energy . if there were no drag , the plane would never come down . since there is drag , the drag tends to slow the plane down , decreasing its kinetic energy . countering that is the plane 's tendency to maintain constant speed and kinetic energy , so it descends , turning potential energy into kinetic energy , just like a ball rolling down a slope . so the more drag , the more quickly it descends , the less drag , the more slowly it descends . a way to minimize drag is to minimize speed , because drag force is proportional to speed squared . ( therefore the sink rate is roughly proportional to speed squared . ) so the speed you trim it for depends on what you want to maximize : to maximize gliding range , you trim for a speed which is slow enough to have low drag , but not so slow that you do not cover much ground . to maximize time aloft , you trim for an even slower speed which has even lower drag , thus minimizing the sink rate . this speed is roughly half way between the speed for maximum range and the even slower stall speed $v_s$ . check these links : v-speeds , and gliding flight .
the easiest ( and roughest ) way to to do it would be to convert your running " work " into a vo2 score . the american college of sports med 's equation is vo2= resting component + horizontal component + vertical component or vo2= 3.5 + ( 0.2 x speed ) + ( 0.9 x speed x elevation gain ) so , using your example of 8.67 mph ( speed in the equation is in meters per min ) 3.5 + ( . 2*232.67 ) + ( 0.9*232.67* . 045 ) = 59.5 thus running on flat ground should give you a speed of 280 m/min or 10.44 mph 5 min 44 sec per mile ( i am an exercise scientist , not a physicist )
he did not mean ' forget ' it . he just meant that it is not really relevant . it is easy to understand the various relationships in dc . when you go to ac they are all the same at any given moment on the wave . it all boils down to v=ir and vi=w , ri^2=w , v^2/r=w . with dc it is just a constant . with ac they change with time . if you pick any moment in that time and apply the above , you will have your answer .
there are numerous excellent reviews out , two that come to mind are : r . hoffmann in angew chem int ed engl , 26 , 846 r . hoffmann in rev mod phys , 60 , 601
i would recommend steering clear of schwarzschild coordinates for these kind of questions . all the classical ( i.e. . firewall paradox aside ) infinities having to do with the event horizon are due to poor coordinate choices . you want to use a coordinate system that is regular at the horizon , like kruskal-szekeres . indeed , have a look at the kruskal-szekeres diagram : ( source : wikipedia ) this is the maximally extended schwarschild geometry , not a physical black hole forming from stellar collapse , but the differences should not bother us for this question . region i and iii are asymptotically flat regions , ii is the interior of the black hole and iv is a white hole . the bold hyperbolae in regions ii and iv are the singularities . the diagonals through the origin are the event horizons . the origin ( really a 2-sphere with angular coordinates suppressed ) is the throat of a non-traversable wormhole joining the separate " universes " i and iii . radial light rays remain 45 degree diagonal lines on the kruskal-szekeres diagram . the dashed hyperbolae are lines of constant schwarzschild $r$ coordinate , and the dashed radial rays are lines of constant $t$ . you can see how the event horizon becomes a coordinate singularity where $r$ and $t$ switch roles . now if you draw a worldline from region i going into region ii it becomes obvious that it crosses the horizon in finite proper time and , more importantly , the past light-cone of the event where it hits the singularity cannot possibly contain the whole spacetime . so the short answer to your question is no , someone falling into a black hole does not see the end of the universe . i do not know the formula you ask for for $t$ , but in principle you can read it off from light rays on the diagram and just convert to whatever coordinate/proper time you want to use .
$x_{max}$ is the amplitude of the oscillations , and yes , ${\omega}t - \varphi$ is the phase . we know that the period $t$ , is the reciprocal of the frequency $f$ , or $$t = 1/f$$ we also know that $\omega$ , the angular frequency , is equal to $2\pi$ times the frequency , or $$\omega = 2{\pi}f$$ from here , we can use the initial conditions to find the amplitude . $x ( 0 ) = x_{max}cos ( \varphi ) $ $\dot{x} ( 0 ) = {\omega}x_{max}sin ( \varphi ) $ from here it should be a simple matter to find $\varphi$ .
let me begin with the second question where you do not change the dimensionality , just the volume . the entropy never decreases when you actually compress gas . the compression means that the walls are mostly moving against the colliding molecules which means that they are recoiled backwards at higher velocities . the molecules ' kinetic energy increases so they occupy a larger volume in the momentum space ( in macroscopic language , a gas heats up while being compressed ) which at least compensates the decrease of the volume in the position space . the other answer is incorrect . the second laws says not only that systems exhibit some activity indicating that they do not like a decreasing entropy ; instead , it says that whatever activity physical systems display , they will never achieve a macroscopic decrease of the entropy . it is just impossible . to compress gas by 70% is possible , to decrease the entropy by a macroscopic amount is not . now , the interesting first question . if you could change the effective dimensionality , it would still be true in any consistent theory that the entropy can not decrease . so if your theory were just able to add dimensions like that while keeping a molecule in a sphere of the increasing dimension , the second law of thermodynamics would imply that such an addition of dimensions is not physically possible – it would be another , more sophisticated example of the perpetual motion machine of the second kind . in some sense , it is true that the second law encourages physical systems to lose the dimensions ( a way to increase the entropy , given your formula for the higher-dimensional spherical volumes ) . when the energy dissipates , the energy per degree of freedom effectively goes down which allows us to use a lower-dimensional " effective " description . for example , a gas full of kaluza-klein particles probing ( moving in ) extra dimensions will tend dissipate its energy and decay to many lower-energy quanta which are effectively living just in 3+1 dimensions .
a part of your question sounds like the covariant treatment of membranes ( and higher-dimensional branes ) in string/m-theory . of course , to do actual quantitative calculation , one has to choose a particular embedding and world volume coordinates along the membrane , and impose the coordinate redefinition symmetry ( the same thing is done for strings ) . however , the embedding in some actual spacetime still exists . if it is guaranteed or required to exist , it makes no sense to pretend that it does not exist : the system will effectively be all about wave equations for the transverse coordinates ( to the brane ) . if you wanted to encode the curvature of the brane in its metric tensor only , assuming that the metric tensor behaves just like an induced metric from a higher-dimensional space , you would get different equations of motion . the simplest equations for the metric are einstein-like equations which are second-order in the metric ; however , the induced metric is proportional to the derivatives of spatial coordinates , so the einstein-like equations would be third-order in the spacetime coordinates , and moreover nonlinear to contract the odd number of indices . but i probably do not understand what exactly you have in mind - and apologies , it is probably because what you have in mind is impossible mathematically .
you should not think of the schrödinger equation as a true wave equation . in electricity and magnetism , the wave equation is typically written as $$\frac{1}{c^2} \frac{\partial^2 u}{\partial t^2} = \frac{\partial^2 u}{\partial x^2}$$ with two temporal and two spatial derivatives . in particular , it puts time and space on ' equal footing ' , in other words , the equation is invariant under the lorentz transformations of special relativity . the one-dimensional time-dependent schrödinger equation for a free particle is $$ \mathrm{i} \hbar \frac{\partial \psi}{\partial t} = -\frac{\hbar^2}{2m} \frac{\partial^2 \psi}{\partial x^2}$$ which has one temporal derivative but two spatial derivatives , and so it is not lorentz invariant ( but it is galilean invariant ) . for a conservative potential , we usually add $v ( x ) \psi$ to the right hand side . now , you can solve the schrödinger equation is various situations , with potentials and boundary conditions , just like any other differential equation . you in general will solve for a complex ( analytic ) solution $\psi ( \vec r ) $: quantum mechanics demands complex functions , whereas in the ( classical , e and m ) wave equation complex solutions are simply shorthand for real ones . moreover , due to the probabilistic interpretation of $\psi ( \vec r ) $ , we make the demand that all solutions must be normalized such that $\int |\psi ( \vec r ) |^2 dr = 1$ . we are allowed to do that because it is linear ( think ' linear ' as in linear algebra ) , it just restricts the number of solutions you can have . this requirements , plus linearity , gives you the following properties : you can put any $\psi ( \vec r ) $ into schrödinger 's equation ( as long as it is normalized and ' nice' ) , and the time-dependence in the equation will predict how that state evolves . if $\psi$ is a solution to a linear equation , $a \psi$ is also a solution for some ( complex ) $a$ . however , we say all such states are ' the same ' , and anyway we only accept normalized solutions ( $\int |a\psi ( \vec r ) |^2 dr = 1$ ) . we say that solutions like $-\psi$ , and more generally $e^{i\theta}\psi$ , represent the same physical state . some special solutions $\psi_e$ are eigenstates of the right-hand-side of the time-dependent schrödinger equation , and therefore they can be written as $$-\frac{\hbar^2}{2m} \frac{\partial^2 \psi_e}{\partial x^2} = e \psi_e$$ and it can be shown that these solutions have the particular time dependence $\psi_e ( \vec r , t ) = \psi_e ( \vec r ) e^{-i e t/\hbar}$ . as you may know from linear algebra , the eigenstates decomposition is very useful . physically , these solutions are ' energy eigenstates ' and represent states of constant energy . if $\psi$ and $\phi$ are solutions , so is $a \psi + b \phi$ , as long as $|a|^2 + |b|^2 = 1$ to keep the solution normalized . this is what we call a ' superposition ' . a very important component here is that there are many ways to ' add ' two solutions with equal weights : $\frac{1}{\sqrt 2} ( \psi + e^{i \theta} \phi ) $ are solutions for all angles $\theta$ , hence we can combine states with plus or minus signs . this turns out to be critical in many quantum phenomena , especially interference phenomena such as rabi and ramsey oscillations that you will surely learn about in a quantum computing class . now , the connection to physics . if $\psi ( \vec r , t ) $ is a solution to the schrödinger 's equation at position $\vec r$ and time $t$ , then the probability of finding the particle in a specific region can be found by integrating $|\psi^2|$ around that region . for that reason , we identify $|\psi|^2$ as the probability solution for the particle . we expect the probability of finding a particle somewhere at any particular time $t$ . the schrödinger equation has the ( essential ) property that if $\int |\psi ( \vec r , t ) |^2 dr = 1$ at a given time , then the property holds at all times . in other words , the schrödinger equation conserves probability . this implies that there exists a continuity equation . if you want to know the mean value of an observable $a$ at a given time just integrate $$ &lt ; a&gt ; = \int \psi ( \vec r , t ) ^* \hat a \psi ( \vec r , t ) d\vec r$$ where $\hat a$ is the linear operator associated to the observable . in the position representation , the position operator is $\hat a = x$ , and the momentum operator , $\hat p = - i\hbar \partial / \partial x$ , which is a differential operator . the connection to de broglie is best thought of as historical . it is related to how schrödinger figured out the equation , but do not look for a rigorous connection . as for the hamiltonian , that is a very useful concept from classical mechanics . in this case , the hamiltonian is a measure of the total energy of the system and is defined classically as $h = \frac{p^2}{2m} + v ( \vec r ) $ . in many classical systems it is a conserved quantity . $h$ also lets you calculate classical equations of motion in terms of position and momentum . one big jump to quantum mechanics is that position and momentum are linked , so knowing ' everything ' about the position ( the wavefunction $\psi ( \vec r ) ) $ at one point in time tells you ' everything ' about momentum and evolution . in classical mechanics , that is not enough information , you must know both a particle 's position and momentum to predict its future motion .
why laundry dry up also in cold/frost ? probably because , initially , the clothes and the liquid water trapped in the clothes fibres , are both at a temperature well above 0 c . when you have frost , water in the clothes should freeze , and it does , when the temperature of the garment and water trapped within it have eventually reduced to below 0 c but if clothes are dry , then it should be possible that steam in the clothes if clothes are already dry , the situation is not relevant to your question of how ( wet ) clothes become dry ( or dryer ) when air temperatures are below 0 c . it should be possible that steam in the clothes does not have time to freeze . if you move warm wet clothes into a cold environment for a sufficiently short time , the water will indeed not have time to freeze . the temperature of the majority of water will not be sufficiently reduced . left longer in air at a temperature below 0 c the liquid water in the clothes will freeze and any water vapour in the clothes will form frost .
i have never seen a paper where the calculation is performed in a manifestly covariant manner . however , i have posted a set of reference notes on my website ( http://jacobi.luc.edu/notes.html ) that contains the variations needed to carry out the calculation . let me summarize the calculation here . the action for gravity on a compact region $m$ with boundary $\partial m$ is $$i_{eh} + i_{ghy} = \frac{1}{2 \kappa^2} \int_{m}d^{d+1}x \sqrt{-g} r + \frac{1}{\kappa^2} \int_{\partial m} d^{d}x \sqrt{-h} k ~ . $$ the metric on $m$ is $g_{\mu\nu}$ , and $r = g^{\mu\nu} r_{\mu\nu}$ is the ricci scalar . the induced metric on the boundary $\partial m$ is $h_{\mu\nu} = g_{\mu\nu} - n_{\mu} n_{\nu}$ , where $n^{\mu}$ is the ( spacelike ) unit vector normal to $\partial m \subset m$ . now consider a small variation in the metric : $g_{\mu\nu} \to g_{\mu\nu} + \delta g_{\mu\nu}$ . the quantities appearing in the einstein-hilbert part of the action change in the following manner : $$ \delta \sqrt{-g} = \frac{1}{2} \sqrt{-g} g^{\mu\nu} \delta g_{\mu\nu}$$ $$ \delta r = -r^{\mu\nu} \delta g_{\mu\nu} + \nabla^{\mu}\left ( \nabla^{\nu} \delta g_{\mu\nu} - g^{\nu\lambda} \nabla_{\mu} \delta g_{\nu\lambda} \right ) $$ thus , the change in $i_{eh}$ is $$\begin{aligned}\delta i_{eh} = and \frac{1}{2\kappa^{2}}\int_{m} d^{d+1}x \sqrt{-g} \left ( \frac{1}{2} g^{\mu\nu} r - r^{\mu\nu} \right ) \delta g_{\mu\nu}\\ and + \frac{1}{\kappa^2} \int_{\partial m} d^{d}x \sqrt{-h} \frac{1}{2} n^{\mu} \left ( \nabla^{\nu} \delta g_{\mu\nu} - g^{\nu\lambda} \nabla_{\mu} \delta g_{\nu\lambda}\right ) ~ , \end{aligned}$$ with the boundary term coming from the volume integral of the total derivative in $\delta r$ . the variations of the quantities in the ghy term are a bit more complicated to work out , but they all basically follow from standard definitions and this result for the variation of the normal vector : $$\delta n_{\mu} = \frac{1}{2} n_{\mu} n^{\nu} n^{\lambda} \delta g_{\nu\lambda} = \frac{1}{2} \delta g_{\mu\nu} n^{\nu} + c_{\mu}~ . $$ in the second equality i have introduced a vector $c_{\mu}$ that is orthogonal to $n^{\mu}$ ; it is given by $$c_{\mu} = - \frac{1}{2} h_{\mu}{}^{\lambda} \delta g_{\nu\lambda} n^{\nu} ~ . $$ the reason i have introduced this vector is that the variation in the trace of the extrinsic curvature can be written as $$\delta k= - \frac{1}{2} k^{\mu\nu} \delta g_{\mu\nu} - \frac{1}{2} n^{\mu}\left ( \nabla^{\nu} \delta g_{\mu\nu} - g^{\nu\lambda} \nabla_{\mu} \delta g_{\nu\lambda} \right ) + d_{\mu} c^{\mu}$$ where $d_{\mu}$ is the covariant derivative along $\partial m$ that is compatible with the induced metric $h_{\mu\nu}$ . so , the change in the ghy part of the action is $$\delta i_{ghy} = \frac{1}{\kappa^2} \int_{\partial m} d^{d}x \sqrt{-h}\left ( \frac{1}{2}h^{\mu\nu} \delta g_{\mu\nu} k + \delta k \right ) ~ . $$ combining this with $\delta i_{eh}$ we see that the several terms cancel , leaving $$\begin{aligned} \delta i = and \frac{1}{2\kappa^2}\int_{m} d^{d+1}x \sqrt{-g}\left ( \frac{1}{2} g^{\mu\nu} r - r^{\mu\nu} \right ) \delta g_{\mu\nu}\\ and + \frac{1}{\kappa^2}\int_{\partial m} d^{d}x \sqrt{-h}\left ( \frac{1}{2} ( h^{\mu\nu} k - k^{\mu\nu} ) \delta g_{\mu\nu} + d_{\mu} c^{\mu} \right ) ~ . \end{aligned}$$ we discard the term $d_{\mu} c^{\mu}$ , which is a total boundary derivative .
the absolute velocity of the electrons actually does not matter for joule heating . think about it this way , if there is no current flowing there would not be any joule heating . so , even if electrons are moving quickly and randomly when no current is flowing , we know no joule heating would occur and that joule heating is really about the net change in effect caused by the current . that is , the base electron velocity does not have an effect . all that matters is the $\delta v$ over the base electron velocity which is given by the drift velocity . joule heating is really about electrical energy lost to heat due to resistance . even if the average drift velocity of an electron is tiny , there are so many electrons moving that the tiny energy loss to heat for each electron adds up . as you know , current is the result of a huge number of moving electrons . it is a numbers game . the more electrons losing a tiny bit of energy there are , the more total heat is generated . via ohm 's law you can see that $p_{ower} = i^2 r$ so it is no wonder that heat generation is proportional to $i^2 r$ . also , in your question you mentioned an electron bumping into a nucleus . that is not what is happening . electrons are colliding with the electron cloud of atoms and via electromagnetic repulsion are pushing the whole atom a bit , increasing its kinetic energy . it is just free electrons interacting with bound electrons .
the problem is azimuthally-symmetric , so the tangential direction is the one with no azimuthal component , i.e. the one " straight up the side " if you were a small mountain climber climbing from the surface to the top of the drop . all three surface tensions are required because all three exert forces . for example , if the solid-air surface tension were extremely high , the system would try to minimize the area of contact between the surface and the air , which corresponds to spreading the drop out flat . if the solid-air surface tension were very low , the opposite would occur and the drop would be a sphere touching the surface in a very small area .
well , the answer is yes and no . the band inversion between the $s$-like ( conduction ) band $\gamma_6$ and $p$-like ( valence ) band $\gamma_8$ in hgte is primarily responsible for its topologically nontrivial band structure . the bulk band structure of hgte with ( right ) and without ( left ) spin-orbit coupling is shown in the figure below . there are a total of eight bands ( including spin ) shown in both figures . since we’re interested in the physics close to the $\gamma$ point , we can approximately ignore bulk inversion asymmetry . under this assumption the spin up and down bands are degenerate as clearly seen from the figure . from this point on i will not consider spin explicitly when talking about bulk band structure ; i.e. there are a total of four bands ( ignoring spin ) in the figures below . note : please don’t focus on the quantitative details of the left figure . it is a hypothetical scenario introduced purely for pedagogical purposes . you can notice that , in the figure on the left ( without spin-orbit coupling ) , the heavy hole ( hh ) and light hole ( lh ) bands are degenerate . when you turn on spin-orbit , the $\gamma_6$ and $\gamma_8$ bands reverse their order , the $\gamma_8$ band splits its degeneracy , and the lh band gets inverted . the fermi energy sits at the intersection point of the lh and hh bands . but notice that , despite lh and hh acting as the conduction and valence bands ( right figure ) respectively , there is no gap between them ! you cannot get a topological insulator without a bulk gap . if you could somehow induce a gap between the lh and hh bands ( say ) by straining hgte then it could , in fact , be turned into a 3d topological insulator ! now , there were several ( experimental ) advantages in creating a cdte/hgte/cdte quantum well . first of all , since it’s a quantum well you would have sub-bands ( not bands unlike bulk materials ) due to quantum confinement in the out-of-plane ( say $z$ ) direction . as a result , a single band in the bulk will split up into several sub-bands , each corresponding to a different quantized $k_z$ , as you shrink the thickness of the material in the $z$-direction . now , you can notice ( in the figure below ) , unlike the bulk , the electron ( conduction ) and hole ( valence ) sub-bands do have an energy gap . this plot obviously shows the minima ( electron ) or maxima ( hole ) of these sub-bands ; they still disperse in k-space . and as you may know the inversion of the sub-bands will occur when you cross the critical thickness ( as shown in the figure below ) . another very important advantage of using a quantum well structure in doing your experiments is that , unlike a bulk sample , you can electrically tune your fermi energy using a gate . you could both tune your fermi energy to intersect the electron ( or hole ) sub-band or keep it in the gap , and observe the change in conductance . when you are in the quantum spin hall regime you will never stop conducting as your fermi energy goes from the electron ( or hole ) sub-band to the gap ; this is due to the topologically protected ( due to time-reversal symmetry ) edge states inside the bulk gap ( here bulk means not on the edge of the well ) . in a bulk sample ( bulk meaning not quantum confined ) you would probably have performed some sort of controlled doping ( assuming the gap has already been induced somehow ) to control your fermi energy . in that case you would probably have to fabricate different samples for different values of fermi energy ; that’s certainly very inconvenient . in summary , you need to somehow induce a gap in hgte , by either quantum confinement or induced strain to turn it into a 2d or 3d topological insulator . cdte is not responsible for the key physics , i.e. band inversion , which gives rise to a topologically nontrivial band structure in hgte . it is interesting to note that the hgte quantum well was not the first proposal by bernevig , hughes , and zhang . the experimental difficulty of working with strained hgte led them to revise their proposal and predict a topological insulator in the quantum well instead ! this was back in 2006 ; people have now managed to experimentally create 3d topological insulators out of strained hgte .
idea #2: build an electronic circuit that separates a periodic input signal ( e . g . a square wave ) into its component frequencies ( using an array of band-pass filters ) and then adds these signals back together to get an approximation of the original signal .
what you say is correct in principle , but ignores the important fact that practical car engines are horribly inefficient , and their effeciency changes quite a bit over the range of speed and power required to move the car . note that this is the point of transmissions . at best they do not loose any power , but they make the overall process more efficient by allowing the gasoline engine to operate at a more efficient point . in one way , you can look at a hybrid as having a wide-ranging finely adjustable transmission , but there is more to it than that . the efficiency of a gasoline engine is in part related to what fraction of peak power it must put out . if the gas engine is the only mechanical output in the car , then it must be sized to supply peak power . however , most of the time much less than peak power is needed , so the engine often runs at a inefficient point . with a electric motor available to fill in the when peak power is demanded , the gas engine can be sized smaller and it is easier to make it more efficient over most of the normal operating range . it also allows for the option of not using the gas engine at all at very low power levels where it would be very inefficient . instead it can effectively be run in bursts of more efficient operation . for example , if the gas engine is 3% efficient at 500 w , but 6% efficient at 1 kw , then you are better off running it at 1 kw half the time instead of at 500 w all the time . with a hybrid , you have this option . with just a gas engine , it is stuck having to produce whatever power is demanded at the moment , regardless of how efficient that is . i have a honda civic hybrid , and i can tell you this stuff really works . i routinely get 50 miles/gallon minimum on the highway , often substantially more . the engine is physically small for the size car , and it has been specially designed to be easily shut down and restarted . going down a hill , even at highway speeds , the engine often turns off . if the hill is steep enough , the motor is run as a generator and charges the battery . when i get to the bottom of the hill , i can see that for a little while the control system uses the electric motor to keep the car going at the set speed ( this is all with cruise control engaged ) , then eventually gives up and switches on the gas engine . i can feel a slight klunk when that happens , and the charge indicator goes abruptly from discharge to charge .
the short answer is no : no , there is not a way to calculate $l_x$ . let me expand . you may be aware of the uncertainty relation between position and momentum , which can be written like this $$ \langle [ \hat{x} , \hat{p}_x ] \rangle \geq i \hbar $$ what this means is that it is impossible to know precisely both the position and the momentum of a particle at a given point in time . if your particle is in some state for which the position is known very precisely , then the momentum of the particle must be very uncertain . that is to say , the particle does not have a precise , definite momentum --- the best we can do is talk about its average momentum and uncertainties in its momentum . it turns out that there is a very similar relation for angular momentum , which looks like this $$ \langle [ \hat{l}_x , \hat{l}_y ] \rangle \geq i \hbar \langle \hat{l}_z \rangle $$ here ( as before ) the angle brackets denote ' expectation value ' and the square brackets denote the commutator . what this is saying , essentially , is that for any system which has a non-zero z-component of angular momentum ( for example , our hydrogen atom in the state $m = 1$ ) , the quantities $l_x$ and $l_y$ are intrinsically uncertain . the system simply does not have a definite x- or y-component of angular momentum . note that the quantum number $m$ means the z-component of angular momentum ( in units of $\hbar$ ) . so the ' formula ' for getting from the quantum number $m$ to the z-component of angular momentum --- namely , $l_z = m\hbar$ --- is not really a formula at all , it is basically just a definition . as such , there is simply no way that we could write $l_x$ or $l_y$ in terms of $m$ , in the same way that if you gave me a randomly shaped box and told me its height $h$ , there is simply no way i would be able to write the width in terms of it . the height of the box does not tell me anything about the width . however , because of what i have argued above , we can actually make a stronger statement than just ' you can not write $l_x$ in terms of $m$' . the stronger statement is that we could not even find a number that characterised the x-component of angular momentum , because for so long as we know the z-component , the x- and y- components are uncertain . hope this helps !
the most fundamental definition of temperature is derived from the zeroth law of thermodynamics . the zeroth law declares thermal equilibrium an equivalence relationship , and thus we can tag each equivalence class with a number that we call temperature . or in less mathematical term , temperature is a physical quantity tagged to each thermodynamic system such that any two systems with the same temperature would stay in thermal equilibrium when they contact . the exact way of assigning temperature to a system is called a temperature scale . there were multiple scales before , most based on thermal properties of a particular substance . then kelvin devised a scale based solely on thermodynamic principles , which we call " absolute scale " .
the hinge , which connects the " system " ( bullet and beam ) to the outside world does not exert any torque on the system , but it can and does exert both a vertical and horizontal force on the system at the moment of impact . consider for a moment if the bullet hit close to the hinge ; the hinge could fail and allow the beam and bullet to move in the direction of the bullet 's travel . i think that a consideration of " center of percussion " , http://en.wikipedia.org/wiki/center_of_percussion , could clarify the situation . . . not that in this article , the hinge is replaced with a sliding u-bolt , which cannot exert horizontal forces . . .
it is not a matter of opinions , it may be calculated . the hawking radiation inevitably shrinks primarily the black hole mass ( it may also change the angular momentum and charges but they are being reduced " proportionally " to the mass in the limit of many hawking quanta ) . lighter black holes carry a smaller entropy . the reduction of the black hole entropy does not contradict the second law of thermodynamics because the hawking radiation which is newly created carries some entropy , too . in fact , the total entropy of ( the black hole + the hawking radiation ) is continuing to increase .
you might want to have a look at the work of gavin crook ( http://threeplusone.com/gec/ ) , especially the first two chapters of his phd thesis ( to be found on his website ) are quite revealing . i will quickly summarize his main result : assume a system is what he calls microscopically reversible , that is , the probability of a trajectory through phase space is related to the probability of the system taking the reverse trajectory by a simple function of the heat ( eq . 1.10 in his thesis ) . it is initially in equilibrium . then you drive it out of equilibrium by some ( time-reversible ) protocol . now for an arbitrary function $f$ depending on the path of the system through phase space , it holds that \begin{equation} \langle f \rangle_{\mathrm{f}} = \langle \hat f \exp ( -\beta w_\mathrm{d} ) \rangle_{\mathrm r} \end{equation} where $\langle \ldots \rangle$ denotes an average over all possible paths the system can take through phase space and f / r denotes the forward / reverse non-equilibrium process . $w_{\mathrm{d}}=w_{\mathrm{tot}} - w_{\mathrm{r}}$ is what he calls dissipative work ; it is just the total work minus the minimum amount of work required ( reversible work , that is , the free energy difference ) . $\hat f$ is the time reversal of $f$ . and now it comes : this holds regardless of the strength of the perturbation ! by choosing $f=1$ ( or any other constant ) , one obtains the jarzynski equality \begin{equation} \langle \exp ( -\beta w ) \rangle = \langle \exp ( -\beta \delta f ) \rangle \end{equation} ( $\langle \ldots \rangle$ again denotes an average over all possible realizations of the non-equilibrium process ) which relates the work performed during a non-equilibrium process to the free energy difference by an equality ( ! ) instead of the inequality resulting from the second law . with more sophisticated $f$ 's one also obtains other relations like the transient fluctuation theorem , the kawasaki response ( which gives you a probability distribution for a non-equilibrium ensemble ) . there is a lot more literature ; i also recommend the 1997 papers of christopher jarzynski ( sadly no free access ) . at the moment , i am learning about all these things myself , so the above might not be 100% waterproof explained , but i hope , one gets the idea .
i think the answer to this depends a lot on your definition of " directly . " relativity of simultaneity is built into the lorentz transformation , and lorentz invariance is one of the most precisely tested physical theories in all of history . essentially you are asking for an experiment that verifies one element of the matrix involved in the lorentz transformation , but every element of the matrix is present in all cases . i would consider the sagnac effect to be a fairly direct test , and the sagnac effect was one of the effects observed in the hafele-keating experiment , as well as many other , earlier tests of relativity . every time you fly on a commercial jet , you are benefiting from a ring laser gyro , which works based on the sagnac effect .
that prof müller said it : shock waves in addition of the usual things . the picture in your link is rather different from the thing shown in the video , for the time being , i do not understand really what is the new thing . from thermodynamics it is clear , that they ( hope ? ) to have a higher effective deltat , obviously without having higher temperatures at machiney parts ( which makes them expensive and/or short-lived ) i assume that those shock waves can be transformed into working pressure without the high temperatures of the combustion shock wave touchng machine parts . i hope we will hear more from prof müller in near future . edit : this link is somewhat more detailed and less press-release-silly . in general it says what i surmised ( by application of thermodynamics basics ) http://www.zdnet.com/blog/emergingtech/wave-disk-engines-to-make-hybrid-vehicles-cheaper-more-efficient/1887
i can not explain qm here . it takes a lot of reading and working things out for yourself . for this particular question , however , an analogy might help ( this may be far below your level , in which case apologies ) . qm is very often about " simple harmonic oscillators " ( shos ) , for which the oldest prototype is the pendulum ( approximately , if the amplitude is small ) . for a pendulum , if we want to know how far it will go from the vertical , we can wait to see how far it goes on each cycle . an alternative way is to measure how fast the pendulum goes when it passes through the vertical . for any given speed , there is a corresponding farthest distance from the vertical . we can equate these two , in a notional sort of way , by choosing units just so , $s_0=d_1$ , the speed at its maximum is the same as its farthest distance from vertical . [ if you do not want to choose such helpful units , write $s_0=kd_1$ . ] now , suppose that we measure the speed and the distance at some intermediate point , for which we obtain $s_t , d_t$ . for a simple harmonic oscillator , and approximately for a pendulum if its oscillations are small , we obtain $\sqrt{s_t^2+d_t^2}=s_0=d_1$ . the square root $\sqrt{s_t^2+d_t^2}$ is an invariant quantity of the coordinates $ ( s_0,0 ) $ and $ ( 0 , d_1 ) $ , which in general are $ ( s_t , d_t ) $ . anything that is a function of the square root $\sqrt{s_t^2+d_t^2}$ is also a function of $s_t^2+d_t^2$ , so we can work with whichever is more convenient . the effects of a given sho on other systems ---or of a system that contains many shos on other systems--- are determined both the phases and by the amplitudes , but the amplitude often determines the more obvious properties , with differences of phase causing important but often more subtle effects , which we typically might call interference ( but there are many other words , such as " caustics " , or even , in a new age sort of way , " sympathetic vibrations " ! ) . the effects of a given quantum mechanical system are , at an elementary mathematical level , sui generis with a classical sho or system of shos , but quantum mechanics describes the ways that the probabilities of discrete events evolve over time , instead of describing the evolution of a trajectory . the introduction of probability as an essential property makes qm a discussion of a higher order mathematical object . especially different is the fact that we can no longer talk about velocities , because individual events do not have velocities ( if we are determined to talk in terms of particles we cannot in general be sure which individual events go with which particle ) , however it is useful to introduce a notional object that we call momentum , which allows us to model patterns that we observe in the evolution of the probabilities as interference effects ( whether that is what they are not , we can model the patterns in the probabilities using patterns of varying phases and amplitudes ) . the mathematical quantity that we call momentum is , however , sufficiently different from the classical momentum that is associated with a particle trajectory that the analogy breaks down in various mathematically significant ways . i can not see how to address the final aspect of this that occurs to me , for now , at least not well . the much touted linearity of quantum mechanics is a consequence of the fact that qm describes the evolution of probabilities of individual measurement events . the object we call momentum is closely related to the mathematics of fourier transforms of probability distributions , which is essentially associated with a squared modulus like $s_t^2+d_t^2$ . one consequence of that is noncommutativity of the algebra of observables . this is a quick and very vague writing down of a lot of experience , without much editing , so take it with a pinch of salt and with a lot of other reading of what other people have to say about the hard questions that quantum mechanics poses for us . i hope you find it more useful than confusing , but hey , i can take a few downvotes , and it is been oddly useful to me to write this down in this somewhat wild way . in fact , if you can see the ways in which this answer is related to your question , you understand qm pretty well already .
starting at ${position}_z$ = $z$ = 0 and $v ( z ) = 0$ and by tracking multiple acceleration values either with a time interval or at fixed intervals , $t$ , then you can get the position . . . . somewhat . it will drift over time . also , your device cannot rotate whatsoever , or else you need a gyroscope to track that and then use trigonometry to properly orient the x y and z values from the accelerometer . assuming it is always oriented such that the $a ( z ) $ is always perfect vertical acceleration ( if you are in a vehicle that is always flat , in which case z does not matter , or you are on a vertical guide rail ) , $$p ( z ) = \int_0^t v ( z ) ~dt = \iint_0^t a ( z ) ~dt $$ also , from here : short answer : forget about it . longer answer : unless you are on a perfectly straight rail , you will not achieve what you want to do without ( a ) a set of gyros ; and ( b ) far more accurate sensors than what you have . accelerometers measure acceleration in the body fixed reference frame , whereas you need some displacement in an earth-fixed frame . therefore , you need not only to integrate the accelerometers , but rotate them into the earth-fixed frame before doing the integration . this is assuming perfect sensors . mems sensors are far from perfect - i have written up a post on some of the errors here . consider two errors : 1 . a bias on the accelerometer . 2 . an initial attitude ( tilt ) error . in addition to whatever acceleration signal there is , integrate a bias and you get a ramp error with time . integrate the ramp and you get a quadratically increasing error with time . this will add up really , really quickly . consider a tilt error . you will now be measuring some of the gravity vector in the forward ( or whatever ) direction . integrate this error twice and you will have the same problem as the bias . so , my advice again is do not ! find another method . also , check this book out for more detailed designs , or use whatever sensors and algorithm these guys are on : http://www.youtube.com/watch?v=6ijarke8vku if you still want to give this a shot , use the trapezoidal method in excel , it is pretty easy . there is an explanation page here with a sample , but here 's a more complete way :
you are slightly misinterpreting some words by prof matt strassler . he says that the force mediated by the exchange of the higgs bosons – the " higgs force " – is attractive , much like gravity between two ordinary positive-mass objects . but that does not mean that " everything " in the presence of a higgs field is attractive . indeed , the higgs potential contributes a term to the stress-energy tensor that is proportional to $g_{\mu\nu}$ , and it therefore includes a negative pressure whose magnitude is equal to the energy density . at this qualitative level , the higgs field behaves just like the inflaton . the negative pressure acts in a " repulsive " way and may cause the exponentially accelerating expansion of the universe . however , the detailed shape of the potential and the energy scale associated with the higgs field and the higgs boson is generally different than what one needs for inflation , so the general expectation is that the ordinary higgs field known from its 125 gev higgs bosons is not capable of producing inflation . this viewpoint is ( or would be ) strengthened by the discovery of the primordial gravitational waves by bicep2 which would indicate that the energy density during inflation was huge , near the gut scale , and therefore much higher than the energy densities expected from the relatively light 125 gev higgs boson 's field . on the other hand , there are semirealistic models that use the ordinary higgs field as an inflaton , too . they typically add some new coupling of the field to the curvature . see e.g. http://arxiv.org/abs/0710.3755 and its references and followups . such models could be very economical when it comes to the field content but this attractive feature is compensated by the awkward interactions one has to add . moreover , there are other reasons to think that new physics does occur at the gut scale so particle physicists generally do not view gut as a liability at all . grand unification helps ( or would help ) to explain and unify many other things in physics .
the classical version of this problem was solved by henri poincaré way back in 1896 . this is also problem 5.43 in electrodynamics by griffiths . the classical trajectories are geodesics on the surface of a cone . a recent treatment of the classical version of this problem is here . the quantum mechanical version was also solved long back by igor tamm in 1931 . this is discussed in section 2.3 of the book magnetic monopoles by y m shnir , who follows the treatment in charge quantization and nonintegrable lie algebras by hurst . the quantum mechanical version of the problem turns out to be separable in spherical polar coordinates . the angular part has the generalized spherical harmonics as its eigenvalues , while the radial solution is the same as the radial wave function of the standard schroedinger equation . the centrifugal potential in the schroedinger equation turns out to be always repulsive which implies that there are no bound states for this system of an electron in a magnetic monopole field . however a dyon field does have bound state solutions .
the idea of the existence of galaxies is certainly not new , and quite a bit older than the field of modern astrophysics . in 1750 , thomas wright , an english astronomer correctly speculated that the milky way was a flattened disk of stars and that some of the nebulae astronomers viewed in their telescopes were separate " milky ways " . in 1755 immanuel kant introduced the term " island universe " for these distant nebulae . in 1912 , vesto slipher made spectrographic studies of the brightest spiral nebulae to determine if they were made of chemicals found in a planetary system . he discovered they had high red shifts , indicating they were moving away at a rate higher than the milky way 's escape velocity . the matter was conclusively settled in 1922 when ernst öpik gave a distance determination which supported the theory that the andromeda nebula is an extra-galactic object .
the name of the property is itself a clue here : enthalpy of vaporization . by nature , enthalpy does take into account the work required to push the atmosphere . you can see the impact of increasing the pressure on the enthalpy of vaporization on a mollier diagram . increasing the pressure has the overall the effect of reducing the enthalpy of vaporization , until it becomes zero at the critical point . at this stage , there is no longer a phase change associated with vaporization .
well , let 's see if i understood correctly the question . you are talking of this : you have some kind of pendulum attached to a point r . in fact , the total kinetic energy is $$t=t_{p}+t_{rot}$$ where $t_{p}$ is the translational energy of the point p and $t_{rot}$ the rotational energy around that point . realize that the expression of the two energies is different , depending of your axis . when you consider your axis system at a fixed point , you only have rotational energy , because the fixed point has not a transalation . the other common option is to take the center of mass , and , in that case , you also have to consider its translational energy . this is useful if you do not have any fixed point . however , $t$ is indepedent of the axis , so when you calculate rotational kinetic energy on p , you obtain the total energy ; in the other hand , the center of mass of the system is translating and rotating ( if the rod has mass ) , so translational energy of the ball will be lower than total energy . if the rod has no mass , then the center of mass is only translating and yes , in that case the rotational energy on p would be the same has the ball 's translational energy . to solve problems with fixed point , the best option is to take that point , because you do not have to deal with translational energy . in this particular case , p is a fixed point , so you can calculate the moment of inertia of the system on p ( using steiner 's theorem ) , write the position and the angular velocity as function of $\beta$ , ( angular velocity would be $\dot{\beta}$ ) , and apply energy conservation . if the rod has no mass , you also can try to calculate center of mass position in terms of $\beta$ , derive it to obtain the velocity and calculate the modulus . if the rod has mass , this second method becomes a difficult problem : you have to calculate center of mass of the two bodies , the inertia tensor at that point , and apply both translational and rotational energies . i hope this will be useful . if you need some aclaration , say it .
no , because even though the force that you exert on the earth is equal and opposite to the force it exerts back on you , you are not doing the same amount of work on the earth as the earth on you . your kinetic energy increases due to the work done by the earth on you . remember that $w = f \cdot d$ ; your bicycle moves a lot due to this force , but the earth does not really move much at all . another way to think about this is in terms of kinetic energy . $\mathrm{ke} = \frac{1}{2} mv^2$ , so if your velocity is high , so is your kinetic energy . the earth 's velocity is low , and so is its kinetic energy . so the forces are equal and opposite , and the impulse , or change in momentum , is too , but the kinetic energy stays mostly with you .
it is a good question ! physics is all about linking your intuition to science , so it is good that you are thinking about this . the statement that momentum should be proportional to mass and velocity is intuition . like elfmotat says , you can choose your constant as long as it is consistent with units , i guess . if you want another reason , consider the time-derivative of the equation $p=mv$ . it is newton 's second law ! $f=ma$ . in other words , $f=\frac{dp}{dt}$ , which is a great reason for k to be 1 .
basically you have a train with an observer a inside who emits a beam of light to the left which is reflected off a wall . . . let 's call that wall w , for reference below . . . at distance $d$ from a . . . . let 's call that distance $d := aw$ ( anticipating that distances between certain other participants will have to be considered , and distinctly named , below ) . the time it takes for the beam to get back to the observer is $t_0 = \frac{2 \ , d}{c}$ which is the proper time . . . . a.k.a. " ping duration " $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c}$ . so far , so good . now consider an observer b outside the train . the train is moving at a velocity $v$ to the right relative to b . ( also : b as well as everyone at rest wrt . b are moving at velocity $v$ relative to a and w ; from a towards w . ) thus the time it takes for the light to hit the wall is $\frac{d}{c + v}$ that is eventually incorrect . let 's try to be more precise : b and a were passing each other ( which is supposed to be visible by everyone else ; a.k.a. " emitting a light signal " ) , there exists some participant ( let 's call it p ) who was and remained at rest wrt . b , who therefore of course was passed by w " sometime " , and specificly : whose indication of being passed by w was simultaneous to b 's indication of being passed by a , and there exists some participant ( let 's call it q ) who was and remained at rest wrt . b ( as well as p ) and who was passed w just as q and w observed ( together , at their meeting ) that b and a had passed each other . therefore $\frac{pq}{bq} = \frac{v}{c}$ , and $\frac{bp}{bq} = \frac{pq}{bq} + 1 = 1 + \frac{v}{c} = \frac{c + v}{c}$ . of interest is then b 's duration from the indication of being passed by a until the indication simultaneous to q 's indication of being passed by w ( and observing that b and a had passed each other ) . this is of course half of the ping duration $\mathop{\delta}\limits_{\text{ping}} \tau_b [ q ] $ , i.e. half of $\frac{2 \ , bq}{c}$ ; thus $\frac{bq}{c}$ which is in turn equal to $\frac{bp}{c + v}$ . and the time it take for it to return to a is $\frac{d}{c - v}$ . arguing similarly to the above , this corrsponding duration of b is more precisely equal to $\frac{bp}{c - v}$ . now , in order to compare a 's ping duration $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c}$ with the sum of the corresponding durations of b , i.e. with $\frac{bp}{c + v} + \frac{bp}{c - v} = \frac{2 \ , c \ , bp}{c^2 - v^2} = \frac{2 \ , bp}{c} \frac{1}{1 - \beta^2}$ we still need to establish value of the distance ratio $\frac{aw}{bp}$ . that means we are now left with having to derive " length contraction " ! but that is not difficult , given all of the explicit setup and named participants that were introduced above already : we should consider one more participant , j , who was and remained at rest wrt . a and w , and whose indication of being passed by b was simultaneous to w 's indication of being passed by q ( and observing that b and a had passed each other ) . therefore $\frac{aj}{aw} = \frac{v}{c}$ , and $\frac{jw}{aw} = 1 - \frac{aj}{aw} = 1 - \frac{v}{c} = \frac{c - v}{c}$ . considering the two explicit requirements of simultaneity above , the corresponding ratios of distances should be equal : $\frac{bp}{aw} = \frac{jw}{bq}$ . inserting expressions from above : $\frac{bp}{aw} = \frac{jw}{aw} \frac{aw}{bp} \frac{bp}{bq} = \frac{c - v}{c} \frac{aw}{bp} \frac{c + v}{c} = \frac{aw}{bp} \frac{c^2 - v^2}{c^2} = \frac{aw}{bp} ( 1 - \beta^2 ) = \sqrt{ 1 - \beta^2 }$ . consequently : $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \frac{2 \ , aw}{c} = \frac{2 \ , bp}{c} / \sqrt{ 1 - \beta^2 }$ . calling b 's corresponding duration $\frac{2 \ , bp}{c} \frac{1}{1 - \beta^2} = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_w}$ therefore $\mathop{\delta}\limits_{\text{ping}} \tau_a [ w ] = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_w} \sqrt{ 1 - \beta^2 }$ , as may have been expected . last week in class we derived the formula for time dilation using light clocks well , should not that have been pretty much the derivation i just sketched ? . edit for completeness , and to emphasize a particular point in the following , here 's also the derivitation involving light clocks " perpendicular to the direction of motion " ( which seems to have been mentioned in passing in the op 's question ) : expanding on the setup described above , with the principal protagonists a and b and suitable auxiliary participants ( w and j at rest wrt . a ; p and q at rest wrt . b ) , and all of them " sitting or moving in one line " , we now also consider participant f ( at rest wrt . a , j , w ) with distance ratios $\left ( \frac{af}{fj} \right ) ^2 + \left ( \frac{aj}{fj} \right ) ^2 = 1$ , and ( without loss of generality , but just to re-use setup relations from above ) with $\frac{aw}{fj} = 1$ , therefore $\frac{af}{fj} = \sqrt{ 1 - \left ( \frac{aj}{fj} \right ) ^2 } = \sqrt{ 1 - \left ( \frac{aj}{aw} \right ) ^2 } = \sqrt{ 1 - \beta^2 }$ ; and participant g ( at rest wrt . b , p , q ) with distance ratios $\left ( \frac{bg}{gp} \right ) ^2 + \left ( \frac{bp}{gp} \right ) ^2 = 1$ , and such that g and f met each other in passing . importantly , the entire region containing the setup is of course supposed to be flat . therefore it can be demonstrated ( what otherwise may be glanced over for seeming " too obvious to even point out" ) , that f 's indication of having been passed by g was simultaneous to a 's indication of having been passed by b ; and vice versa that g 's indication of having been passed by f was simultaneous to b 's indication of having been passed by a . then , by the same argument that was used above for comparison of distance ratios between pairs of participants who were not at rest to each other , we set : $\frac{af}{bg} = \frac{bg}{af}$ , and therefore $\frac{af}{bg} = 1 . $ with $\mathop{\delta , \tau_a}\limits_{\text{ping trip } b_g} = \frac{2 \ , fj}{c} = \frac{2 \ , af}{c} / \sqrt{ 1 - \beta^2 }$ and $\mathop{\delta}\limits_{\text{ping}} \tau_b [ g ] = \frac{2 \ , bg}{c}$ follows $\mathop{\delta}\limits_{\text{ping}} \tau_b [ g ] = \mathop{\delta \ , \tau_a}\limits_{\text{ping trip } b_g} \sqrt{ 1 - \beta^2 }$ . finally , as can be shown explicitly , it holds symmetrically that $\mathop{\delta}\limits_{\text{ping}} \tau_a [ f ] = \mathop{\delta \ , \tau_b}\limits_{\text{ping trip } a_f} \sqrt{ 1 - \beta^2 }$ .
tidal forces are residual forces , they are the consequence of gravitational forces acting more strongly on one part of an extended body than another . remember that gravity is proportional to $1/r^2$ . so one side of the earth ( the " nearby " side from the sun 's perspective ) feels a gravitational force $$f_g^{near} \propto 1/ ( d_{s-e}-r_e ) ^2$$ where $d_{s-e}$ is the distance between the center of the sun and the center of the earth and $r_e$ is the earth 's radius . contrarily , the " far " side of the earth feels a force $$f_g^{far} \propto 1/ ( d_{s-e}+r_e ) ^2 &lt ; f_g^{near}$$ so the far side of the earth is accelerated towards the sun less than the near side . this residual force ( the difference between the force on the near side and that on the far side ) is what we call a tidal force . notice that this discussion is incomplete , since the gravitational force is not only influenced by distance , it is also proportional to mass ( or mass density ) . because water and ( basically ) rocks do not have the same mass density , the effect will be different for the oceans and the ' solid ' chunk of ' rock ' they ' envelope ' . ( lots of quotes there ) this is not vital to our discussion here , though , since the tidal forces are not strong enough to significantly distort the shape of the rigid earth . but it is worth keeping in mind . furthermore , note that the tides on earth are mainly due to the gravitational effect of the moon on the earth , not the sun . the sun may be a lot heavier than the moon , but the moon is a lot closer . and gravitational forces scale only linearly with mass while they scale quadratically with inverse distance . as a consequence , the solar tides are about half as large as the lunar tides . ( see also this illustrative app and this link for some more information ) richard feynman actually briefly addressed the tides in his lectures on physics and it is fun and interesting to watch , so here 's a link . his discussion of the tides starts at around 25:00 . explicit calculation and comparison between solar tides and lunar tides let 's quickly crunch some rough numbers , shall we ? of course , we will need values for a few quantities . ( i will be using si units since you will probably be most familiar with those ) $$\begin{align} m_s and \approx 2\times10^{30}\ , \text{kg} \\ m_m and \approx 7.3\times10^{22}\ , \text{kg} \\ d_{se} and \approx 1.5\times10^{11}\ , \text{m} \\ d_{me} and \approx 3.8\times10^{8}\ , \text{m} \\ r_e and \approx 6.4\times10^{6}\ , \text{m} \\ g and \approx 6.7\times10^{-11}\ , \text{m}^3\ , \text{kg}^{-1}\ , \text{s}^{-2} \end{align}$$ with these numbers we can calculate the gravitational acceleration $a=f_g/m$ experienced by the near and far side of the earth . ( we do not calculate the force because the affected mass $m$ will in general be different ) the difference between the gravitational acceleration for the near and far side is a measure for the strength of the tides . for the sun we find $$a_s = gm_s\left ( \frac{1}{ ( d_{se}-r_e ) ^2} - \frac{1}{ ( d_{se}+r_e ) ^2}\right ) \approx 1\times10^{-6}\ , \text{m}\ , \text{s}^{-2}$$ and for the moon $$a_m = gm_m\left ( \frac{1}{ ( d_{me}-r_e ) ^2} - \frac{1}{ ( d_{me}+r_e ) ^2}\right ) \approx 2.3\times10^{-6}\ , \text{m}\ , \text{s}^{-2} . $$ so we see from this crude estimate that indeed the solar tides are about half as large as the lunar tides .
a good way to understand this is to delve a little more deeply into the meaning of timelike coordinates in flat space , then move up to schwarzschild space . in flat space , we have the metric $$ds^2 = -dt^2 + dx^2+dy^2+dz^2 . $$ looking at the metric , we can see that only one of the spacetime coordinate differentials gets a negative sign ( $dt^2$ ) . looking closer , you can see that this term is the only one that will contribute negatively to the metric line element , no matter the values of $dx , dy , dz$ , or $dt$ . in general , you can think of the timelike coordinate as the one that will contribute negatively to the metric line element . what makes a coordinate timelike ? a good way to think about this question is to think about how time differs from the other spacetime coordinates we are used to in flat space . for us , time can only move forward ( as @elfmotat says , you can not avoid getting older ) , while we can move freely in the other spacetime coordinate directions . so , for particles , if a coordinate contributes negatively to the line element , then you can only move forward in the direction of that coordinate . to motivate this , i am just going to appeal to your intuition for lightcones . in minkowski space : particles are forced to follow timelike geodesics $ ( ds/d\tau ) ^2 =-1$ , all of which lie within the future lightcone in the diagram . so the statement that the coordinate that contributes negatively to the metric line element is timelike is kind of a geometrical one . basically , it confines you to a certain region of spacetime and in so doing only allows forward motion . now let 's look at the $r , t$ lightcone around a black hole : once you have crossed the event horizon , all worldlines move backward in $r$ . so we say that $r$ becomes timelike while $t$ becomes spacelike . in the schwarzschild metric , $$ds^2 = -\left ( 1-\frac{2gm}{r}\right ) dt^2 + \left ( 1-\frac{2gm}{r}\right ) ^{-1}dr^2+r^2d\omega ^2 , $$ we can see that for $r &gt ; 2gm$ , the terms in the parenthesis are positive , and therefore the timelike coordinate is $dt$ , since it already has a negative sign outside it . but for $r &lt ; 2gm$ , the terms in the parenthesis are negative , so the time-spacelikeness of $dt , dr$ flip , and $dr$ becomes the only coordinate that contributes negatively to the metric line element . what this means physically is that you can only move forward in $r$ , so in that sense $r$ becomes a timelike coordinate . a bit more pondering should reveal that this mathematical treatment of the problem reproduces the well-known black hole behavior exactly . ( can not go back when you have crossed the event horizon ) .
for your dose calculation you will note that photons and electrons have a weighting factor of 1 and then go the the particle data book chapter on " passage of particles through matter " and read the sections on electrons and photons . now the table of the isotopes tells me that u-238 is mostly a alpha emitter , but that is a non-starter for both your geiger tube and a person . there does not seem to be a single beta decay listed , and the double-beta branching ratio is or order $10^{-10}$ . ouch . so we look at the alpha-linked gamma and x-ray lines . only two of those exceed 1% branching ratio and they are both below 20 kev . double ouch . none-the-less those are presumably most of what you are detecting . the betas would be much more energetic , but they will be one for every hundred million of the gammas . in the pdb you will see that at those energies the photons dump essentially all there energy in any person close enough that the intervening air does not shield them .
i am not an aerodynamics specialist , so the following is almost certainly a huge oversimplification ( or maybe downright wrong ) , but i think it might help with intuition . suppose you have an amount of energy $e$ available to spend , and you are trying to accelerate an object of mass $m$ . suppose you can impart the energy in the form of kinetic energy to an air mass of mass $m$ . what speed will this accelerate the mass $m$ to ? by energy and momentum conservation , you have $$e=\frac{1}{2}m v_m^2+\frac{1}{2}m v_m^2$$ and $$mv_m+mv_m=0$$ giving $$v_m=\sqrt{\frac{2 e m}{m ( m+m ) }}$$ which as a function of air mass $m$ looks like this : Plot[Sqrt[(2 e m)/(M (m + M))] /. {e -&gt; 1, M -&gt; 100}, {m, 0, 500}, PlotRange -&gt; All]  which means that according to this simple model , it is more energy efficient to use large values of $m$ , ie , to eject large volumes of slow air rather than a small volume of fast air . aerodynamic efficiency is not necessarily a measure of how much energy you impart to the ejected air per fuel consumption , but rather is how much momentum you impart to the aircraft per fuel consumption .
general relativity attributes gravitational effects to the curvature of spacetime . free objects tend to move along paths which minimize the energy required . the presence of massive objects alters the curvature of spacetime and so affects observed motion . when an object is subjected to a force which counteracts it is ability to " fall freely " it feels a fictitious force that we call gravity . for example we feel the fictitious force of gravity , because electromagnetism is preventing us from freely falling towards the earth 's core . now from a classical perspective this fictitious force must have an associated acceleration due to newton 's second law . we call this the " acceleration due to gravity " . but from the point of view of general relativity this acceleration is just a frame dependent quantity . so to answer your questions : yes - quite right . acceleration due to gravity is an effect we observe from a non-freely-falling frame like earth 's surface . it is just an artifact of our choice of reference frame . acceleration due to other forces ( principally electromagnetism ) is frame-independent , and is therefore a genuine physical effect . no - the classical concept of " acceleration due to gravity " is necessarily tied up with newtonian ideas in our reference frame on earth 's surface . in general relativity we generalize our notion of acceleration to 4-acceleration . one then calculates the motion of particles through spacetime by setting this equal to zero . so heuristically the equation of motion under gravity $mg=f=ma$ becomes $ma = 0$ in general relativity . yes - ish . time dilation and length contraction are effects that one experiences when comparing different frames of reference . and we have seen above that " acceleration due to gravity " is also just a frame dependent effect . but it is not really true to say that one is a " result " of the other . it is better to realize that time , length and " gravitational force " are all frame dependent quantities .
the simplest way to explain the christoffel symbol is to look at them in flat space . normally , the laplacian of a scalar in three flat dimensions is : $$\nabla^{a}\nabla_{a}\phi = \frac{\partial^{2}\phi}{\partial x^{2}}+\frac{\partial^{2}\phi}{\partial y^{2}}+\frac{\partial^{2}\phi}{\partial z^{2}}$$ but , that is not the case if i switch from the $ ( x , y , z ) $ coordinate system to cylindrical coordinates $ ( r , \theta , z ) $ . now , the laplacian becomes : $$\nabla^{a}\nabla_{a}\phi=\frac{\partial^{2}\phi}{\partial r^{2}}+\frac{1}{r^{2}}\left ( \frac{\partial^{2}\phi}{\partial \theta^{2}}\right ) +\frac{\partial^{2}\phi}{\partial z^{2}}-\frac{1}{r}\left ( \frac{\partial\phi}{\partial r}\right ) $$ the most important thing to note is the last term above--you now have not only second derivatives of $\phi$ , but you also now have a term involving a first derivative of $\phi$ . this is precisely what a christoffel symbol does . in general , the laplacian operator is : $$\nabla_{a}\nabla^{a}\phi = g^{ab}\partial_{a}\partial_{b}\phi - g^{ab}\gamma_{ab}{}^{c}\partial_{c}\phi$$ in the case of cylindrical coordinates , what the extra term does is encode the fact that the coordinate system is not homogenous into the derivative operator--surfaces at constant $r$ are much larger far from the origin than they are close to the origin . in the case of a curved space ( time ) , what the christoffel symbols do is explain the inhomogenities/curvature/whatever of the space ( time ) itself . as far as the curvature tensors--they are contractions of each other . the riemann tensor is simply an anticommutator of derivative operators--$r_{abc}{}^{d}\omega_{d} \equiv \nabla_{a}\nabla_{b}\omega_{c} - \nabla_{b}\nabla_{a} \omega_{c}$ . it measures how parallel translation of a vector/one-form differs if you go in direction 1 and then direction 2 or in the opposite order . the riemann tensor is an unwieldy thing to work with , however , having four indices . it turns out that it is antisymmetric on the first two and last two indices , however , so there is in fact only a single contraction ( contraction=multiply by the metric tensor and sum over all indices ) one can make on it , $g^{ab}r_{acbd}=r_{cd}$ , and this defines the ricci tensor . the ricci scalar is just a further contraction of this , $r=g^{ab}r_{ab}$ . now , due to special relativity , einstein already knew that matter had to be represented by a two-index tensor that combined the pressures , currents , and densities of the matter distribution . this matter distribution , if physically meaningful , should also satisfy a continuity equation : $\nabla_{a}t^{ab}=0$ , which basically says that matter is neither created nor destroyed in the distribution , and that the time rate of change in a current is the gradient of pressure . when einstein was writing his field equations down , he wanted some quantity created from the metric tensor that also satisfied this ( call it $g^{ab}$ ) to set equal to $t^{ab}$ . but this means that $\nabla_{a}g^{ab} =0$ . it turns out that there is only one such combination of terms involving first and second derivatives of the metric tensor : $r_{ab} - \frac{1}{2}rg_{ab} + \lambda g_{ab}$ , where $\lambda$ is an arbitrary constant . so , this is what einstein picked for his field equation . now , $r_{ab}$ has the same number of indicies as the stress-energy tensor . so , a hand-wavey way of looking at what $r_{ab}$ means is to say that it tells you the " part of the curvature " that derives from the presence of matter . where does this leave the remaining components of $r_{abc}{}^{d}$ on which $r_{ab}$ does not depend ? well , the simplest way ( not completely correct , but simplest ) is to call these the parts of the curvature derived from the dynamics of the gravitational field itself--an empty spacetime containing only gravitational radiation , for example , will satisfy $r_{ab}=0$ but will also have $r_{abc}{}^{d}\neq 0$ . same for a spacetime containing only a black hole . these extra components of $r_{abc}{}^{d}$ give you the information about the gravitational dynamics of the spacetime , independent of what matter the spacetime contains . this is getting long , so i will leave this at that .
if you believe the fault-tolerant threshold theorem for quantum computers , you do not require hundreds of digits of accuracy . levin does not believe this theorem . more precisely , he believes that the hypotheses required for the theorem to work do not apply to the actual universe . i believe his mental model of quantum mechanics resembles the idea that the physics of the universe is being simulated on a classical machine which has floating point errors . i do not believe this is true .
if dark matter interacts only gravitationally , then the cross section for producing it in the e+e- machines is inherently too low to be detected . i am discussing e+e- machines because those are the ones that can give a closed enough system to be able to detect missing mass and energy cleanly . the cross section at the y ( about 10gev in mass ) is something like 10^-2 millibarn . now the coupling constant in front of the calculations ( squared ) is the electromagnetic one , which is orders of magnitude larger than the gravitational one . this will affect to practically zero both the magnitude and the width of any reaction producing the hypothetical 7 gev particle , either in some pair production , or associated production . there was some talk of finding more positrons than electrons associated with the measurements reported . in that case there exists a coupling between electromagnetic fields and these proposed particles , but a specific model would be needed to say at what level the production of these would be excluded by the existing world data from e+e- machines . there are limits given assuming super symmetry is the valid theory . see this aleph thesis which gives limits over 40 gev .
$$a=\pi r^2$$ $$\frac{da}{dr}=\pi\cdot2r$$ $$da=2\pi rdr$$ alternatively , you can write : $\lim_{\delta r\to 0}\frac{\delta a}{\delta r}=\lim_{\delta r\to 0}\frac{\pi\{ ( r+\delta r ) ^2-r^2\}}{\delta r}=\lim_{\delta r\to 0}\frac{2\pi r\delta r+\delta r^2}{\delta r}=2\pi r+0$ you have to ignore $ ( dr ) ^2$ as it is very small . why ? because you took the limit while taking infinitesimal rings .
at the galactic center , there is an object called sagittarius a* which seems to be a black hole with 4 million solar masses . in 1998 , a wise instructor at rutgers made me make a presentation of this paper http://arxiv.org/abs/astro-ph/9706112 by narayan et al . that presented a successful 2-temperature plasma model for the region surrounding the object . the paper has over 300 citations today . the convincing agreement of the model with the x-ray observations is a strong piece of evidence that sgr a* is a black hole with an event horizon . in particular , even if you neglect the predictions for the x-rays , the object has an enormously low luminosity for its tremendously high accretion rate . the advecting energy is pretty " visibly " disappearing from sight . if the object had a surface , the surface would heat up and emit a thermal radiation - at a radiative efficiency of 10 percent or so which is pretty canonical . of course , you may be dissatisfied by their observation of the event horizon as a " deficit of something " . you may prefer an " excess " . however , the very point of the black hole is that it eats a lot but gives up very little , so it is sensible to expect that the observations of black holes will be via deficits . ; - )
since there are no specialists in depletion mass spectrometry , i will try to answer in a more general way . with depletion spectroscopy you look at a small variation of a large signal so what you need is not high sensitivity but signal stability and high dynamic range of the detector . i assume that you have a continuous stream of ions , otherwise pulse to pulse fluctuations of ion concentrations will be the major limiting factor . probably , you will not need superb mass resolution so you would want to buy a quadrupole mass spectrometer - these are cheap , compact and there are plenty of companies that make them . dynamic range depends on the ion detector and since you would be looking for a wide dynamic range and stability with time , the very best choice is a simple faraday cup . you can be sure that , whatever your experiment is , the sensitivity will be limited not by the detector but by fluctuations of your signal - most likely , by how stable your depleting factor is .
you can see accurate visualizations of ( simulated ) eddy currents in multiple papers . one example is : efficient solvers for nonlinear time-periodic eddy current problems
you get a rise in a capillary tube because it reduces the energy stored in the surface tension at the air-water and air-glass interface . the water rises until the reduction in the surface tension energy is balanced by the increase in the gravitational potential energy of the water . but it is not at all obvious how you could extract energy from this . if you evaporate water from the top of the tube then you will certainly pull up more water to replace the water lost by evaporation . i suppose this is analogous to a tree pulling up water , though my limited memory of biology i think the sap is driven up the tree by osmotic pressure in the roots as well as by capillary action . i suppose you could put a microturbine at the bottom of the capillary tube then heat the top and extract energy as the water rises up the tube to replace the water that is evaporated . however i doubt this would be as efficient as just using the same amount of heat in steam engine . were you wondering if there was a way to make the water rise up the tube , then fall back , then rise up again , generating energy with each cycle ? the only way you could do this was if there was some way to change the air-water or air-glass surface tension in some reversible way . you can easily reduce the air-water surface tension by adding surfactant , and this will make the water drop , but you had need to get the surfactant back out to make the water rise again .
you have mentioned a number of pretty intense examples of symmetry breaking , but if i am reading your question rightly , all you are really looking for is " what does symmetry breaking mean when translated to everyday ( classical ) physics ? " that is actually a pretty easy question if that really is your intent : symmetry breaking just means being forced to making a choice . for example , a pencil balanced on its flat eraser end is perfectly symmetric with respect to every possible orientation on the flat surface on which it rests . but if you tip the pencil over , that perfect symmetry is lost , and the pencil must " choose " a specific orientation into which to fall . once that fall has taken place , the pencil has lost all of its original beautiful symmetry with respect to the plane , and will not be able to regain it unless you can " heat it up " ( energy was lost during the fall ) and return it to its original upright position . any form of crystallization is an other example . water is statistically isotropic in three dimensions in its liquid form , but as soon as ice begins to form , the molecules must give up their carefree ways and " choose " some very specific orientation . that too is a symmetry break , and if you think about it , it is not that different from the pencil example . one of my personal favorites is topological , and involves changing the number of available dimensions of an embedding space . imagine molding some clay in into a smooth , symmetric band . the two edges of the band are fully symmetric in 3d in the sense that they can always be rotated to replace each other . now paint one edge red and the other edge blue . next , transform the band into a 2d space ( reduce its embedding space0 by flattening it onto a table surface , trying as best you can to preserve its internal connectivity in the new version . you will find that a washer-like form is the best you can do , and that means you must make a choice : red edge on the inside , or blue edge on the inside ? the fully symmetric 3d form of the band thus breaks down into two non-exchangeable forms when the dimensionality of its embedding space is reduced to two . notice that while the pencil and ice both have an infinite number of choices when their symmetries are broken , in this case only two choices are available . that kind of twofold symmetry breaking is akin to the one between matter and antimatter . that symmetry can similarly be interpreted as the result of " locking down " the time vector of mass-energy in a 4d space , so that in 3d the local time vector must point in either in the same or the opposite direction as classical time . no matter how exotic the topic sounds in advanced physics , one way or another the same sort of " make a choice " process . making the choice lowers the energy of the system , but also destroys the lovely symmetry of the higher energy version . this in a nutshell is also why particle physics has been firmly devoted for many decades to building larger and larger particle accelerators . the higher energies they provide make it possible to search for those lost symmetries found at higher energies .
can any solid material with a low heat capacity exist that feels closer to human body temperature than another solid material with a higher heat capacity ; where both materials were previously kept in either a mundane oven or freezer for a sustained period ? let me rephrase to : is there any solid which disobeys the inverse proportionality of thermal conductivity and specific heat capacity ? consider $1000kg$ of wood and $1000kg$ of aluminium , both at $320k$ ( very warm ) . at the instant you place a finger on such large thermal masses , your perception of temperature comparison is dependent on heat conductivity of the materials , not their heat capacity ( their masses are so large compared to your finger , their temperature is almost constant depsite losing heat to your finger ) . using such large masses and ( equal masses for that matter ) is necessary since otherwise i can instantly answer yes to your question by giving you 100g of wood and 1g of gold ( beaten to the same surface area of the wood ) just taken from the freezer and you would perceive gold being closer to body temperature than the wood after a second . so lets define the question by specific heat capacity , and instantaneous perception of heat transfer . to answer it though , there is in fact no metal which disobeys this relation due to the electron sea being the majority carrier of kinetic energy in the bulk metal . their having large mean free paths and low masses allow them to attain very high velocities ( which is a property of high temperature ) and therefore are able to transfer energy quickly in the bulk material . in other words , if metals used anything heavier to transmit heat , like their nuclei , it would not only take much more heat to accelerate them to the same velocities the electrons could attain ( resulting in higher heat capacity ) , but the rate at which that kinetic energy is transmitted across the material is accordingly slower ( lower thermal conductivity ) . in fact the lattice of metal nuclei do in fact contribute to both properties via phonons not translational kinetic energy like in gases , but phonons are still greatly superseded by the effect from electrons . therefore the inverse relation between thermal conductivity and heat capacity is valid for metals . what you are looking for is a non conductor with both higher heat capacity and thermal conductivity than a conductor . for that i give you diamond ( figuratively . . . i can not afford one ) , which has a specific heat capacity of $0.5 j/gk$ , higher than that of any metal denser than vanadium ( which is almost all of them ) , but has a thermal conductivity of $&gt ; 900w/mk$ , trumping silver 's $421w/mk$ which is tops for all pure metals . indeed , $1kg$ of silver would feel much closer to body temperature than $1kg$ of diamond ( that is alot of diamond ! ) despite diamond having a higher heat capacity .
clouds do not absorb light ( much ) , they reflect and refract it , and this applies to uv light in the same way as visible light . so a 100% cloudy sky will block uv light in the same way it blocks visible light . a quick google suggests that heavy cloud cover will remove 80-90% of uv light . anyone disputing this should try sunbathing on a cloudy day :- ) i have heard occasional claims that broken cloud cover can actually enhance uv levels in the unshaded areas by reflecting uv light into the breaks . however i have never come across any studies that prove this happens . while it seems vaguely plausible i would be surprised if the effect was very big .
some more misconceptions : the chemguide website quoted above might be a useful reference for " uk-based exam purposes " as stated there , but it certainly does not help in solving the question . the arguments given above that followed the comments on chemguide are inaccurate . a simple quantum chemistry calculation of gold in its ground state will give you that the electron in the s orbital ( a1g ) is the most energetic in this atom . hence , ionization will most easily be accomplished by removal of this electron , and not of d electrons , and this is easily proved by another computation for ionized gold , which will show you that the 5d orbitals will remain filled while the 6s orbital is no longer occupied . actually , it is known that if the most external d shell is filled , the energies of these orbitals will be effectively lowered , and there is a very high probability that the ionized electron will not come from it , but from more energetic s or p orbitals . ( i have just done a few of these calculations in order to make sure this point is right ) in order to analyze why some metals are more inert than others , various effects come into play . relativistic effects , such as the contraction of s orbitals , for example , are a major factor in making gold less reactive than silver , and in lowering the oxidation potential of gold . so , in addition to looking at chemical potentials when discussing the inertness of metals in different environments , it is better not to reduce the arguments to simple electron configuration trends which usually work quite well for main group elements , since , though they might generate insights for the understanding of the behavior of metals , these insights might be either right or wrong .
because the " theory " you write down does not exist . it is just a logically incoherent mixture of apples and oranges , using a well-known metaphor . one can not construct a theory by simply throwing random pieces of lagrangians taken from different theories as if we were throwing different things to the trash bin . for numerous reasons , loop quantum gravity has problems with consistency ( and ability to produce any large , nearly smooth space at all ) , but even if it implied the semi-realistic picture of gravity we hear in the most favorable appraisals by its champions , it has many properties that make it incompatible with the standard model , for example its lorentz symmetry violation . this is a serious problem because the terms of the standard model are those terms that are renormalizable , lorentz-invariant , and gauge-invariant . the lorentz breaking imposed upon us by loop quantum gravity would force us to relax the requirement of the lorentz invariance for the standard model terms as well , so we would have to deal with a much broader theory containing many other terms , not just the lorentz-invariant ones , and it would simply not be the standard model anymore ( and if would be infinitely underdetermined , too ) . and even if these incompatible properties were not there , adding up several disconnected lagrangians just is not a unified theory of anything . two paragraphs above , the incompatibility was presented from the standard model 's viewpoint – the addition of the dynamical geometry described by loop quantum gravity destroys some important properties of the quantum field theory which prevents us from constructing it . but we may also describe the incompatibility from the – far less reliable – viewpoint of loop quantum gravity . in loop quantum gravity , one describes the spacetime geometry in terms of some other variables you wrote down and one may derive that the areas etc . are effectively quantized so the space – geometrical quantities describing it – are " localized " in some regions of the space ( the spin network , spin foam , etc . ) . this really means that the metric tensor that is needed to write the kinetic and other terms in the standard model is singular almost everywhere and can not be differentiated . the standard model does depend on the continuous character of the spacetime which loop quantum gravity claims to be violated in nature . so even if we are neutral about the question whether the space is continuous to allow us to talk about all the derivatives etc . , it is true that the two frameworks require contradictory answers to this question .
here 's what happens when you apply $h$ to $|\mathbf k\rangle = a^\dagger_\mathbf k|0\rangle$ to find the energy of a single particle state : \begin{align} h|\mathbf k'\rangle and = \int \frac{d^3\mathbf l}{2 ( 2\pi ) ^3}a^\dagger_\mathbf la_\mathbf la^\dagger_\mathbf k|0\rangle \\ and = \int \frac{d^3\mathbf l}{2 ( 2\pi ) ^3}a^\dagger_\mathbf l ( [ a_\mathbf l , a^\dagger_\mathbf k ] -a^\dagger_\mathbf ka_\mathbf l ) |0\rangle \\ and = \int \frac{d^3\mathbf l}{2 ( 2\pi ) ^3}a^\dagger_\mathbf l ( ( 2\pi ) ^3 2\omega_\mathbf k\delta^{ ( 3 ) } ( \mathbf l - \mathbf k ) -a^\dagger_\mathbf ka_\mathbf l ) |0\rangle \\ and = \omega_\mathbf k a^\dagger_\mathbf k|0\rangle \\ and = \omega_\mathbf k|\mathbf k\rangle \end{align} the computation for a multi-particle state is simiar .
as witten explains in his notices of the ams article ( please see also his more recent lecture ) , the fully quantum string theory is characterized by two coupling constants ( or in the language of deformation quantization : two deformation parameters ) . the string coupling $g_s$ and the string tension $\alpha^{'}$ . in perturbation theory , one gets dependence of the string amplitudes on powers of $g_s$ ( or equivalently in $\hbar$ ) through the genus expansion . the dependence of the amplitudes $\alpha^{'}$ is obtained once one takes into account that in the presence of background fields , the string lagrangian is not free , it is described by a sigma model . if we compute the quantum correction to this sigma model we get terms with more and more derivatives multiplied by more powers of the of the string tension ( as in chiral perturbation theory ) . when the quantum corrections to the trace of the energy momentum tensors are calculated then here also terms depending on powers of $\alpha^{'}$ will appear and the condition of vanishing of the beta function will results einstein 's equations with correction terms proportional to $\alpha^{'}$ . please see equations 3.7.14 . in polchinsky 's first volume , where the beta functions are given to the first power of $\alpha^{'}$ . witten explains that for a while , the work on string theory concentrated on finding candidates of $\alpha^{'}$ deformed theories ( as conformal field theories ) , then $\hbar$-quantize them as in ordinary quantum theory . but , as witten explains , after the discovery of the full set of string dualities and the role of membranes , it was realized that in order to fully quantize string theory , the two quantizations or two deformations ( $\hbar$ , $\alpha^{'}$ ) must be perfomed together . this route has profound consequences , for example , it leads to the conclusion that the string full quantum theory should be in the realm of noncommutative geometry , because in the presence of a $b$-field and brane boundary conditions , the position-position commutation relations will obtain $\alpha^{'}$ deformation and become noncommutative . as a consequence , the ordinary uncertainty relation will get $\alpha^{'}$ deformation and turns into a generalized ( minimal scale ) uncertainty , in which the position uncertainty has a nonvanishing minimum : $\delta x \geqslant \frac{\hbar}{\delta p}+ \alpha^{'}\frac{\delta p}{\hbar}$
as you noted , the ising model has spins that are $\pm 1$ whereas in a full quantum model such as the heisenberg model , the spins are represented by pauli matrices . this means that they are not interchangeable . the biggest difference is that at zero temperature , there are no spin fluctuations in an ising model , whereas there are fluctuations in the heisenberg model . the ising hamiltonian can be written as $$h = j\sum_{\langle i , j\rangle} \sigma_i \sigma_j$$ and all the $\sigma_i \in \{-1,1\}$ . whereas for quantum spins , we had have $$h = j\sum_{\langle i , j\rangle} \vec{s_i} \cdot \vec{s_j}$$ and the $\vec{s_i}$ are vectors with elements determined by the pauli matrices . this product can then be expanded as $$\vec{s_i} \cdot \vec{s_j} = s_i^z s_j^z + \frac{1}{2}\left ( s_i^+ s_j^- + s_i^- s_j^+\right ) $$ where $s_i^+$ and $s_i^-$ are the spin raising and lowering operators . thus , we get one term that looks just like the ising term , because $s_i^z$ can be either $-1$ or $1$ , but we also get terms that describe how the two spins can flip : they start with opposite spin and then both of them flip . for a ferromagnet in the ground state , this is not important , because there the spins are all in parallel and thus the flip terms give zero contribution , but in an antiferromagnet , it makes the ground state highly complicated , whereas in an ising model the ground state would just have neel order .
your statement is true . proof : let $\rho$ be the mass density of the rigid body . remember that the tensor of inertia $i$ is given by : $$ \vec{v}^t i \vec{w} = \int d^3b\ , \rho ( \vec{b} ) ( \vec{v} \cdot \vec{w} - ( \vec{v}\cdot \vec{b} ) ( \vec{w}\cdot \vec{b} ) ) $$ for all $\vec{v} , \vec{w} \in \mathbb{r}^3$ . now take an orthogonal matrix $o$ which represents a rotation around an axis $\mathsf{a}$ with direction vector $\vec{n}$ , i.e. $o\vec{n} = \vec{n}$ . your invariance means that $\rho$ is invariant under $o$ , i.e. $\rho ( o \vec{x} ) = \rho ( \vec{x} ) $ for all $\vec{x}$ . next show that the inertia tensor commutes with $o$: $io = oi$: $$ \begin{align*} \vec{v}^t i o \vec{w} and = \int d^3b\ , \rho ( \vec{b} ) ( \vec{v} \cdot o \vec{w} - ( \vec{v}\cdot \vec{b} ) ( o \vec{w}\cdot \vec{b} ) ) \\ and = \int d^3b\ , \rho ( \vec{b} ) ( o^t \vec{v} \cdot \vec{w} - ( \vec{v}\cdot \vec{b} ) ( \vec{w}\cdot o^t \vec{b} ) ) \\ and = \int d^3b\ , \rho ( o^t \vec{b} ) ( o^t \vec{v}\cdot \vec{w} - ( o^t \vec{v}\cdot o^t \vec{b} ) ( \vec{w}\cdot o^t \vec{b} ) ) \\ and = \int d^3b\ , \rho ( \vec{b} ) ( o^t \vec{v}\cdot \vec{w} - ( o^t \vec{v}\cdot \vec{b} ) ( \vec{w}\cdot \vec{b} ) ) \\ and = \int d^3b\ , \rho ( \vec{b} ) ( ( \vec{v}^t o ) ^t\cdot \vec{w} - ( ( \vec{v}^t o ) ^t \cdot \vec{b} ) ( \vec{w}\cdot \vec{b} ) ) \\ and = \vec{v}^t o i \vec{w} \end{align*} $$ for all $\vec{v} , \vec{w} \in \mathbb{r}^3$ then one sees that $i\vec{n}$ is again an eigenvector of $o$ because $o ( i\vec{n} ) = io\vec{n} = i\vec{n}$ . now $i$ has only one real eigenvalue $1$ with eigenspace $\mathbb{r}\vec{n}$ . this implies that there is a unique $\lambda$ with $i\vec{n} = \lambda \vec{n}$ . thus $\vec{n}$ is an eigenvector of $i$ i.e. $\vec{n}$ points along a principal axis of $i$ .
an interesting question indeed :- ) yes , you can flip the overall sign of the minkowski metric , and in fact a lot of physicists do this ! the sign choice $\operatorname{diag} ( -1 , 1 , 1 , 1 ) $ is conventional in fundamental quantum field theory and in quantum gravity , if i remember correctly , whereas $\operatorname{diag} ( 1 , -1 , -1 , -1 ) $ is conventional in particle physics . this does not affect the lorentz transform , though . if you apply the lorentz transform to a metric tensor , it computes as $g'_{\alpha\beta} = \lambda_\alpha^\mu \lambda_\beta^\nu g_{\mu\nu}$ , and so you will automatically come out with the same sign convention that you put in .
the darwin term can be obtained from the low energy approximation ( $|p|^2/m^2&lt ; &lt ; 1$ ) of the dirac equation of the electron in a central field . an elegant way to perform this task is by means of the foldy-wouthuysen transformation . the same approximation leads also to the other terms detected in the hydrogen atom fine structure including the spin-orbit term . however , the relation to the zitterbewegung is also correct , at least in the heuristic level . please see a derivation in the following work by klaus capelle ( equation 4 ) . the explanation is that due to the zitterbewegung high frequency motion , the central potential gets smeared on the average and acquires the extra darwin term . charles darwin together gordon , were the first to compute the exact energy levels of the hydrogen atom using the dirac equation in 1928 , just two years after dirac discovered his equation . darwin also performed the low energy approximation to understand the differences from the nonrelativistic treatment . a little piece of history about the darwin term can be found in footnote 7 of the following lecture notes which is quoted in the following : " sir charles galton darwin ( 1987-1962 ) , a british physicist , was a grandson of the charles darwin of evolution fame . sir charles was the first , with gordon , to work out the exact energy levels of hydrogen according to the dirac equation and thereby discovered the eponymous term in the levels . he worked out the lagrangian and hamiltonian for classical motion of several interacting charges correct to o ( $v^2/c^2$ ) . he also worked on statistical mechanics ( darwin-fowler method ) . later in life he took part in the manhattan project . " darwin 's work was referred to in the original foldy-wouthuysen article : phys rev . 78 no . 1 , 29-36 , 1950 .
the essence of a glide is that the aircraft is descending . just like a car rolling down a moderate grade , it is trading potential energy to replenish the kinetic energy lost to drag . whether the nose points up or down only relates to the angle of attack , which only relates to speed . an aircraft traveling at slow speed has a higher angle of attack , so its nose will point up , compared to when it is traveling at high speed . one of the things you learn in flight training is how to handle a loss of power . there is a mnemonic for that : abc a : trim for the airspeed ( 65 kts in a c172 ) that gives you the best glide range . this is fairly slow and nose-high . ( there is even a somewhat slower speed that gives you less range but more time aloft . ) b : look for the best landing site , be it a field , road , or if you are lucky , an airport . c : look in the cockpit for what you can do , like trying to restart the engine , and calling on the radio . so , under a , you can see that a slow glide is relatively nose-up , even while the aircraft is descending .
such operators are ill-defined in an interacting theory because whatever counterterms we try to subtract , their expectation value in any finite-energy state will diverge . the closest operators that are well-defined are densities of charge – number operators with signs labeling antiparticles – because the divergent contributions naturally cancel for them . in free quantum field theories , you may define the number operator and write it as an integral but the integrand will not really be commuting with itself at other points so the attribution of the particles into different points will be misleading . in the non-relativistic limit of quantum field theory , all these problems go away under some extra assumptions .
goldstein 's " closed " means the orbiting body will eventually return to some point with the same velocity it had there previously ; i.e. that the path will repeat itself exactly . note that this can occur even in the case of precession : if the ratio between radial period and angular period is rational , the orbit will be closed . there is no precession only if this ratio is unity . what goldstein is saying is that in this point in the text , all he has shown is that the objects ' separation $r$ will satisfy $r_1 &lt ; r &lt ; r_2$ for all time , and that this does not necessarily imply that the orbit will ever repeat itself . for example , we have not ruled out the possibility that $r$ could be periodic with period $\pi$ , while $\theta$ could repeat every 3 units of time . other authors use " closed " or " elliptical " to mean goldstein 's " bounded , " and " open " or " hyperbolic " to mean " unbounded . " part of the degeneracy in terms probably comes from the fact that for two ideal point masses in newtonian mechanics , bounded orbits will be closed in goldstein 's sense . see the application of bertrand 's theorem to the inverse square law .
nowadays there exists a more fundamental geometrical interpretation of anomalies which i think can resolve some of your questions . the basic source of anomalies is that classically and quantum-mechanically we are working with realizations and representations of the symmetry group , i.e. , given a group of symmetries through a standard realization on some space we need to lift the action to the adequate geometrical objects we work with in classical and quantum theory and sometimes , this action cannot be lifted . mathematically , this is called an obstruction to the action lifting , which is the origin of anomalies . the obstructions often lead to the possibility to the realization not of the group of symmetries itself but some extension of it by another group acting naturally on the geometrical objects defining the theory . there are three levels of realization of a group of symmetries : the abstract level : for example the action of the lorentz ( galilean ) group on a minkowski ( eucledian ) space . this representation , for example is not unitary , and it is not the representation we work with in quantum mechanics . the classical level : when the group action is realized in terms of functions belonging to the poisson algebra of some phase space . for example , the realization of the galilean or the lorentz groups on the phase space of a classical free particle . the quantum level when the group action realized in terms of a linear representations of operators on some hilbert space ( or just operators belonging to some $c^*$ algebra . for example , the realization of the galilean or the lorentz groups on a quantum hilbert space of a free particle . now , passing from the abstract level to either the classical or the quantum level may be accompanied with an obstruction . these obstructions exist in already quantum and classical mechanics with finite number of degrees of freedom , and not only in quantum field theories . two very known examples are the galilean group which cannot be realized on the poisson algebra of the phase space of the free particle , rather , a central extension of which with a modified commutation relation : $ [ k_i , p_j ] =-i \delta_{ij}m$ , is realized . ( $k_i$ are boosts and $p_i$ are translations $m$ is the mass ) . this extension was discovered by bargmann , and sometimes its is called the bargmann group . a second example , is the realization of spin systems in terms of sections of homogeneous line bundles over the two sphere $s^2$ . now , the action of the isometry group $so ( 3 ) $ cannot be lifted to line bundles corresponding to half integer spins , rather a $\mathbb{z}_2$ extension of which , namely $su ( 2 ) $ can be lifted . in this case the extended group is semisimple and the issue that $su ( 2 ) $ being a group extension of $so ( 3 ) $ and not just a universal cover is not usually emphasized in physics texts . the group extensions realized as a consequence of these obstructions may require : 1 ) ray representations of the original group which are true representations of the extended group . this is the case of $so ( 3 ) $ , where the half integer spins can be realized through ray epresentations of so ( 3 ) , which are true representations of $su ( 2 ) $ . in this the lie algebras of both groups are isomorphic . 2 ) group extensions corresponding lie algebra extensions . this is the more general case corresponding for example to the galilean case . now , in the quantum level , one can easier understand , why the obstructions lead to group extensions . this is because , we are looking for representations satisfying two additional conditions : 1 ) unitarity 2 ) positive energy sometimes ( up to $1+1$ dimensions ) , we can satisfy these conditions merely by normal ordering , which results central extensions of the symmetry groups . this method apply to the case of the virasoro and the kac-moody algebras which are central extensions to the witt and loop algebras respectively , and can be obtained in the quantum level after normal ordering . the relation between normal ordering and anomalies can be explained in that the quantization operators are needed to be toeplitz operators . a very known example is the realization of the harmonic oscillator on the bargmann space of analytic functions , then the toeplitz operators are exacly those operators where all derivatives are moved to the right . this is called the wick quantization and it exactly corresponds to normal ordering in the algebraic representation . the main property of toeplitz operators is that their composition is performed through star products , and star products of toeplitz operators are are also toeplitz operators thus the algebra of quantum operators is closed , but it is not closed to the original group but rather to a central extension of which . this important interpretation hasn’t been extended to field theories yet . it is worthwhile to mention that central extensions are not the most general extensions one can obtain when a symmetry is realized in terms of operators in quantum theory , there are abelian and even non-abelian extensions . one of the more known extensions of this type is the mickelsson-faddeev extension of the algebra of chiral fermion non-abelian charge densities when coupled to an external yang-mills field in $3+1$ dimensions : $ [ t_{a} ( x ) , t_{b} ( y ) ] = if_{ab}^c t_c ( x ) \delta^{ ( 3 ) }x-y ) +id_{ab}^c\epsilon_{ijk} \partial_i\delta^{ ( 3 ) } ( x-y ) \partial_j a_{ck}$ this extension is an abelian noncentral extension . the explanation of the existence " anomalies " in the classical case , i.e. , on the poisson algebra can be understood already in the case of the simplest symplectic manifold $\mathbb{r}^2$ , the poisson algebra is not isomorphic the translations algebra . a deeper analysis for example given in:marsden and ratiu page 408 for the case of the galilean group . they showed that on the free particle hilbert space , the galilean group lifts to a central extension ( the bargmann group ) which acts unitarily on the free particle hilbert space : $\mathcal{h} = l^2 ( \mathbb{r}^3 ) $ . now , the projective hilbert space $\mathcal{ph}$ is a symplectic manifold ( as any complex projective space ) in which the particle 's phase space is embedded . the restriction of the representation to the projective hilbert space and then to the particle 's phase space retains the central extension i.e. , is isomorphic to the extended group , thus the extended group acts on the poisson algebra . as a matter of fact one should expect always that the anomaly should be realized classically on the phase space . the case of fermionic chiral anomalies seems singular , because it is customary to say that the anomaly is existent only at the quantum level . the reason is that the space of grassmann variables is not really a phase space , and even in the case of fermions , the anomaly exists in the classical level when one represents them in terms of " bosonic coordinates " . these anomalies are given as wess-zumino-witten terms . ( of course these representations are not useful in perturbation theory ) . another reasoning why anomalies exist always on the classical ( phase space ) level is that in geometric quantization , anomalies can be obtained on the level of prequantization . now , prequantization does not require any more data than the phase space ( not like the quantization itself which requires a polarization ) . now , trying to respond on your specific questions . it is true that chiral anomalies were discovered in quantum field theories when no ultraviolet regulators respecting the chiral symmetry could be found . but anomaly is actually an infrared property of the theory . the signs for that is the adler-bardeen theorem that no higher loop ( than one ) correction to the axial anomaly is present and more importantly only massless particles contribute to the anomaly . in the operator approach that i tried to adopt in this answer the anomaly is a consequence of a deformation that should be performed on the symmetry generators in order to be well defined on the physical hilbert space and not a direct consequence of regularization . secondly , the anomaly exists in equally on both levels quantum and classical ( on the phase space ) . the case of fermions and regularization was addressed separately . update - elaboration of the spin case : here is the elaboration of the $so ( 3 ) $ , $su ( 2 ) $ case which contains all the ingerdients regarding the obstruction to lifting and group extensions , except that it does not have a corresponding lie algebra extension . we work on $s^2$ using the stereographic projection coordinate given in terms of the polar coordinates by : $z = tan \frac{\theta}{2} e^{i \phi}$ an element of the group $su ( 2 ) $ $g=\begin{pmatrix} \alpha and \beta\\ -\bar{\beta} and \bar{\alpha } \end{pmatrix}$ acts on $s^2$ according to the mobius transformation : $ z \rightarrow z^g = \frac{\alpha z + \beta}{-\bar{\beta} z + \bar{\alpha } }$ however , one observes that the action of the special element : $g_0=\begin{pmatrix} -1 and 0\\ 0 and -1 \end{pmatrix}$ is identical to the action of the identity . this element is an su ( 2 ) element that projects to the unity of so ( 3 ) ( this can be seen from its three dimensional representation which is the unit matrix ) . thus the group which acts nontrivially on $s^2$ is $so ( 3 ) $ now quantum mechanically spin systems can be realized on the sphere in hilbert spaces of analytic functions : $ ( \psi , \xi ) = \int_{s^2} \overline{\xi ( z ) } \psi ( z ) \frac{dzd\bar{z}}{ ( 1+\bar{z}z ) ^2}$ transforming under $su ( 2 ) $ according to : $ \psi ( z ) \rightarrow \psi^g ( z ) = ( -\bar{\beta} z + \bar{\alpha } ) ^{2j} \psi ( z^{g^{-1}} ) $ this is a ray representation of $so ( 3 ) $ as $so ( 3 ) $ does not have half integer representatioons . now , the first observation ( the quantum level ) is that the special element does not act on the wave functions as the unit operator , for half integer spins it adds a phase of $\pi$ . this is what is meant that the $so ( 3 ) $ action cannot be lifted to the quantum hilbert space . now turning to the the classical level . the symplectic form on $s^2$ is proportional to its area element . the proportionality constant has to be an integer in a prequantizable theory ( dirac quantization condition ) $\omega = 2j \frac{dz \wedge d\bar{z}}{ ( 1+\bar{z}z ) ^2}$ the corresponding poisson bracket between two functions on the sphere : $\{f , h\} =\frac{1}{2j} ( 1+\bar{z}z ) ( \partial_z f \partial_{\bar{z}} h - \partial_z h \partial_{\bar{z}} f ) $ the function generating the group action in the poisson algebra is given by : $f_g= ( \frac{\alpha \bar{z}z + \beta \bar{z} - \bar{\beta}z + \bar{\alpha}}{1+\bar{z}z} ) ^{2j}$ now , the function representing the unity of su ( 2 ) in the function $f=1$ , while the function representing the special element is $f=-1$ for half integer spins , which is a different function ( it has to be a constant because it belongs to the center of $su ( 2 ) $ , thus it has to poisson commute with all functions . thus even at the classical level , the action of $so ( 3 ) $ does not lift to the poisson algebra . now , regarding the question of classically distinguishing $su ( 2 ) $ of $so ( 3 ) $ . if you compute the classical partition function of a spin $\frac{1}{2}$ gas interacting with a magnetic field , it will be different than say spin $1$ , but spin $\frac{1}{2}$ exists in the first place only if $su ( 2 ) $ acts because $so ( 3 ) $ allows only integer spins .
let 's start with the schwarzschild metric $$ \text{d}s^2 = \left ( 1 - \frac{r_\text{s}}{r}\right ) c^2\text{d}t^2- \left ( 1 - \frac{r_\text{s}}{r}\right ) ^{-1}\text{d}r^2 - r^2\text{d}\omega^2 , $$ with $$ \begin{align} r_\text{s} and = \frac{2gm}{c^2} , \\ \text{d}\omega^2 and = \text{d}\theta^2 +\sin^2\theta\ , \text{d}\varphi^2 . \end{align} $$ now let 's introduce a new radial coordinate $\bar{r}$ , defined as $$ r = \bar{r}\left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^2 . $$ after some algebra , we find that $$ \begin{align} \text{d}r^2 and = \left ( 1 - \frac{r_\text{s}}{4\bar{r}}\right ) ^2\left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^2\text{d}\bar{r}^2 , \\ \left ( 1 - \frac{r_\text{s}}{r}\right ) and = \left ( 1 - \frac{r_\text{s}}{4\bar{r}}\right ) ^2\left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^{-2} , \end{align} $$ so that the schwarzschild metric can be written in the form $$ \text{d}s^2 = \left ( 1 - \frac{r_\text{s}}{4\bar{r}}\right ) ^{2}\left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^{-2}c^2\text{d}t^2 - \left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^{4}\left ( \text{d}\bar{r}^2 + \bar{r}^2\text{d}\omega^2\right ) . $$ now , in a local frame around a coordinate $ ( t , r , \theta , \varphi ) $ , we can treat $r$ in the coefficients as constant , so that $$ \begin{align} \left ( 1 - \frac{r_\text{s}}{4\bar{r}}\right ) ^{2}\left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^{-2} and \approx \alpha^2 , \\ \left ( 1 + \frac{r_\text{s}}{4\bar{r}}\right ) ^{4} and \approx \beta^2 , \end{align} $$ with $\alpha$ and $\beta$ constants , thus $$ \text{d}s^2 \approx c^2\alpha^2 \text{d}t^2 - \beta^2\left ( \text{d}\bar{r}^2 + \bar{r}^2\text{d}\omega^2\right ) . $$ finally , with the coordinate transformation $$ \begin{align} \tilde{t} and =\alpha\ , t , \\ \tilde{r} and =\beta\ , \bar{r} , \end{align} $$ we obtain the familiar minkowski metric $$ \text{d}s^2 \approx c^2\text{d}\tilde{t}{}^2 - \left ( \text{d}\tilde{r}^2 + \tilde{r}^2\text{d}\omega^2\right ) . $$ this transformation can be performed at any coordinate $ ( t , r , \theta , \varphi ) $ along the path , so the local metric is always of minkowski form .
thermal conductivity has dimensions of $\mathrm{power / ( length * temperature ) }$ . power is the rate of heat flow , ( i.e. . ) energy flow in a given time . length represents the thickness of the material the heat is flowing through , and temperature is the difference in temperature through which the heat is flowing . in si units , it is commonly expressed as $\mathrm{watts / ( meter * kelvin ) }$ , and in us units , it is commonly given in $\mathrm{btu/hr/ ( feet\ *\ ^of ) }$ . it expresses the rate at which heat is conducted through a unit thickness of a particular medium . that rate will vary linearly based on the temperature difference across the material , so it is expressed as a value per degree of temperature difference , thus heat rate per unit thickness per degree of temperature difference .
monopoles are still created in inflationary models . they are just created before ( or during ) inflation , so that the rapid expansion thereafter dilutes their density to unobservably low levels . at the time when the monopoles are created , they are created at a density of order 1 per hubble volume -- that is , there is one in each " observable universe " at that time . in general , when a symmetry breaks , topological defects form that are separated on a length scale of order ( speed of propagation of the field ) ( time scale over which the symmetry breaks ) . the first is of order $c$ , and the second is of order the hubble time , so monopoles are separated by a distance of order the hubble length . you should take " of order " here very liberally -- i do not actually care if i am off by factors of $10^5$ or $10^{10}$ or anything measly like that ! after all , inflation blows up lengths by something like $10^{20}$ or more . so one monopole per horizon volume becomes one per $10^{60}$ horizon volumes . ( also , the horizon volume continues to change after inflation is over , but not by anything like this sort of factor . ) with densities like that , we certainly would not expect to see any monopoles . problem solved .
i think it is a parallax effect/optical illusion , and i am not confident of explaining this clearly but here goes ! the normal vector to the illuminated portion of the moon is pointing generally away from the earth/moon system towards a point over our horizon . at low altitudes ( evenings ) the sun will be close to the horizon and this can lead to the brain interpreting it as closer than it is and messing up the geometry . this is similar to the enlarged moon illusion when close to the horizon . basically the normal vector appears to overshoot the sun as we interpret the sun as closer than it is .
no , the friedmann equations assume the cosmological principle : the universe at large scales is homogeneous and isotropic . the metric that describes earth may be approximately isotropic ( the same in every direction ) , but it is not homogeneous ( the same in every place ) . a better approximation to the space-time around earth is actually the schwarzschild solution , the one that also describes static black holes and any other spherically symmetric mass .
you assume a magnetic field as static ? magnetic fields may represent and even present at level 1x as standing wave functions , which has led classical physical sciences to regard them as static . however , basic researches have shown magnetic fields to be " consistently dynamic " and " coherently interactive " ( distributable both algebraically and geometrically like fluids ) ; magnetic fields " work " either with external impingement or without .
there is a difference between temperature and energy . plasma is , as you said , very hot - but there is not very much of it . the density of plasma in the tube is very low . so when it does hit the walls of the tube it transfers very little energy . so the mass of the glass tube increases in temperature only very slightly . it is like a firework sparkler , the sparks are at 2000degc but they are very small , have very little mass and contain very little energy - so when one lands on you it transfer much less energy than a hot cup of coffee at 80deg c .
every ordinary star we are able to individually observe is a part of the milky way . well , except for stars in a small number of very nearby galaxies but even galaxies such as andromeda look like a " continuum " so we are not observing the stars individually although we see that the galaxy is not just a point . only if a star goes nova ( a lethal nuclear explosion of a white dwarf star ) of supernova ( a similar explosion but stronger ) , it may be observed outside the milky way . in all such cases we have experienced , one may always identify a galaxy at the same location that was known before the nova/supernova explosion . so the star going nova/supernova clearly belongs to that galaxy . please note that distant galaxies look like dots – pretty much visually indistinguishable from stars in the milky way . a star going nova has 50,000-100,000 times higher luminosity than the sun ; the number is even higher for a supernova . that is a sufficient increase of the luminosity for an exploding star in a distant galaxy to become " almost as bright " as the whole galaxy , well , not quite .
if your argument were fine , no energy would have been required for anything in this universe and there would not have been an energy crisis ! ! ! your intuition is fine when there is no resisting force acting on your body . say , you want to move a block of mass $m$ applying a horizontal force with constant velocity $v$ on a floor with friction coefficient $\mu$ . then we know that frictional force $f = \mu mg$ . hence you need to keep applying a force $f$ equal in magnitude but opposite in direction to the frictional force to keep the body moving at $v$ . and hence the work = $f . v$ which is definitely non zero .
the much larger mass of the rotating cylinder than that of the bullet , together with the friction between the cylinder and chamber position mechanism , would make the downward bias negligible .
this answer is specifically about guitars because i have guitar building and repair experience . the strings and the truss rod are under tension so the neck itself is mainly under compression . there is some tension on the back side of the neck due to neck relief ( forward bow of neck ) but not much . necks are made from wood from the trunk , a material ' built ' to handle the compressive stress from the weight of the tree . the safety factor ( failure load/service load ) of guitar necks are very large . a very basic calc . to demonstrate this : compressive yield strength parallel to grain for the most common guitar neck wood ( maple ) = 21.5 m pa approximate cross sectional area = 0.0007 m ^2 f = pa = ( 21.5 m pa ) * ( 0.0007 m^2 ) = 15 kilo newtons . sf = ( failure load/service load ) = ( 15 kn ) / ( 800 n ) = 18.75 basically , guitar necks are super strong and only fail due to impact ( see gibsons broken headstock syndrome ) or warp due to poorly seasoned wood or extreme humidity or temp . changes .
my statistical physics professor call those ones laplace integrals $i ( h ) $ . $$i ( h ) =\int_{0}^{\infty}x^{h}e^{-a^2x^2}dx$$ note that $$\int_{-\infty}^{\infty}x^{h}e^{-a^2x^2}dx=2i ( h ) $$ some values $$i ( 0 ) =\frac{\sqrt{\pi}}{2a} , i ( 1 ) =\frac{1}{2a^2} , i ( 2 ) =\frac{\sqrt{\pi}}{4a^3} , i ( 3 ) =\frac{1}{2a^4} , i ( 4 ) =\frac{3\sqrt{\pi}}{8a^5} $$ you may brute force by integrating by parts to get rid of $x^{h}$ and use $i ( 0 ) $ a classical result , or you may use induction over $h$ or some other method .
the usual way linear polarisation is measured is by shining polarised light onto a polarising filter , rotating that filter and then using malus ' law to fit the data to a $i_0 cos^2 ( \theta_{beam} - \theta_{polariser} ) $ shape . by finding the angular position of the intensity peak we can infer the angle of polarisation of the incoming beam . now , assume we shine a beam of a certain spread in the transverse plane , having different polarisations everywhere . if we shine this beam onto a polariser , we will get a pattern of intensities $i ( x , y ) = i_0 cos^2 ( \theta_{beam} ( x , y ) - \theta_{polariser} ) $ . we can find the values of $\theta_{beam} ( x , y ) $ as follows : calibration -- rotate $\theta_{polariser}$ and calculate $\max_{x , y} ( \theta_{beam} ( x , y ) ) $ , the peak intensity . the maximum of the peak intensity over all values of $\theta_{polariser}$ will give you $i_0$ . fix $\theta_{polariser}$ to the value that gives the maximum peak intensity . now you know that you have aligned your polariser with one of the polarisations present in the beam . then , the ratio $i ( x , y ) / i_0$ will give you $cos^2 ( \theta_{beam} ( x , y ) - \theta_{polariser} ) $ , from which $\theta_{beam} ( x , y ) $ can be inferred . this method works for polarisation that is constant in time . for other types of polarisation , you can always take a snapshot over a short period of time that has roughly constant polarisation . or , if the polarisation is circular , you can use a half waveplate to convert it to linear .
for geometrical optics we can introduce eikonal $\psi$ by relation $$ f = ae^{-ik_{\mu}r^{\mu} + i\varphi} = ae^{i\psi} . \qquad ( 1 ) $$ for small time interval and space lengths eikonal can be expanded in a form $$ \psi = \psi_{0} + \mathbf r \frac{\partial \psi}{\partial \mathbf r} + t \frac{\partial \psi}{\partial t} , $$ so , by compairing with left side $ ( 1 ) $ it can be directly show that $$ \mathbf k = \frac{\partial \psi}{\partial \mathbf r} , \quad \omega = -\frac{\partial \psi}{\partial t} . \qquad ( 2 ) $$ so by comparing $ ( 2 ) $ with hamilton-jacobi equations we have clear analogy : wave vector acts rule of classical momentum and frequency acts rule of hamiltonian in geometrical optics . so it is possible to introduce the analogy of principle of the least action for rays . for light it can be done by maupertuis principle : $$ \delta s = \delta \int \mathbf p d \mathbf l = 0 \to \delta \psi = \delta \int \mathbf k d \mathbf l = 0 . $$ for example , for optically isotropic and homogeneous space $\mathbf k = const * \mathbf n $ , and $$ \delta \int dl = 0 , $$ which leads to fermat 's least time principle .
no , it is not a boolean property . entanglement between two quantum systems ( could be particles , or anything else ) could be partial , and can be quantified using different measures . in the specific example of bell states , the two systems ( each of them with 2 states $|0\rangle$ and $|1\rangle$ ) are said to be maximally entangled with the entanglement entropy being 1 qubit .
bosonic fields are " more significant " than fermionic fields because they may get large vacuum expectation values – from a condensate of many bosons in the same state . consequently , there may exist a meaningful classical field theory limit . massless fields are " more significant " than massive fields because the massive fields in string/m-theory because the massive ones have masses of order the string scale or the planck scale which is huge and at these short distances or high energies , the classical reasoning breaks down , anyway . so we apply the classical effective equations of motion only at distances much longer than the string or planck scale and at these low energies , only the massless fields are visible ( the massive fields can not be excited so they are " integrated out " and do not appear in the action ) . becker-becker-schwarz try to jump to the truly consistent full theory , which is the supersymmetric one , as quickly as they can so the general bosonic string theory 's effective action may be absent in the book . but the corresponding action for the superstring theory is on page 311 etc . – type ii supergravity . borrow another textbook such as polchinski if your primary interest is bosonic string theory . there are kinetic and potential terms for the tachyon , some kinetic terms for the dilaton , the einstein-hilbert action for the metric tensor , and some natural " squared field strength " from the $b$-field . strictly speaking , the tachyon terms should be removed if we talk about " massless fields " because the tachyon field is not massless . but because of its negative squared mass , it is even " less massive " than the ordinary massive states as well as the massless states – it is " below " the massless level – so we usually do include it , too .
i believe that the answer to this question involves multiple parts . i will try and hit all of them . since you mentioned the ray model , i will assume you are relatively familiar with geometric optics . first , we do see different images of the same object at times ! or rather , we see a blurry image rather than a sharp one . if you bring a pencil so close to your eyes that you cannot focus on it using your iris ( more on that later ) , then you will see a blurry image . this happens for exactly the reason you mention . rays from the same point of the object take different paths to your retina . if the path taken to your lens makes a large angle compared to the path that passes straight through the lens , then in general the rays will not recombine at a single point , but will have some spread . as the object moves closer to your eye , the angles increase , so the spread becomes larger and you see a blurry image . your iris is very important for creating sharp images . generally , the distance between your lens and your retina is fixed . the distance between the object and your lens is not fixed , but we would like to be able to resolve detail for some range of object distances , rather than just a single one . so , the only parameter your body can control is the focal length . by constricting and relaxing , the iris changes the curvature of your lens . the change in curvature leads to a change in focal length . so , whatever object you are attempting to focus on , your iris constricts so that the object is beyond the focal length of your lens . this ensures that the rays will converge toward the retina and produce an image . however , even with the object beyond the focal length you still get a blurry image if the rays make a large angle with the axis that is perpendicular to your eye/lens/retina ( as discussed before ) and this is one reason why you have a pupil . the pupil only allows rays that are approximately all parallel to each other to fall on your lens . it effectively acts as a collimator . so now , you have an object at or beyond the focal length and a set of approximately parallel rays that fall on your lens and are focused . this leads to a relatively sharp image on your retina ( assuming that the object is far enough away that the pupil can do its job ) . the final piece of the puzzle is your brain . even though your iris , lens , and pupil do what they can to create a sharp image for your retina , it is still imperfect . there are still a number of aberrations in the image that falls on your retina . your visual cortex and related support areas in the brain do all of the processing and reconstructing that leads to you perceiving a sharp image .
okay , so i gather from the link that $g^{ ( k ) }$ in your notation refers to the correlation between field values at $2k$ points , with $\varepsilon^+$ inserted at half of them and $\varepsilon^-$ inserted at the other half . this concept of an $n$-point correlation function is very similar to the $n$th moment of a random variable or statistical distribution . for simplicity , consider an example with 1 dimension : a field over 1 time dimension ( at a single point in space ) , described by a random variable . now , we can consider the probability distribution of this random variable and talk of it is moments . the mean value would be called the 1st moment and the variance would be related to the second moment ( it is in fact called the second central moment ) . similarly , you can generalize to higher order moments which help characerize the distribution . the moments help you characterize the distribution and also give an intuitive feel for the function . generalize this concept to random variables which are fields over many-dimensional spacetime . that is what your correlation functions are . btw , for a gaussian distribution ( non-interacting fields i.e. quadratic action ) , all odd moments vanish . ( that might be the motivation for $g^{ ( k ) }$ to be defined as the correlation between field values at $2k$ points . . . even though the actual physical theory you are considering will probably be interacting , else all correlation functions are fairly trivial ) . also , all even moments beyond the 2nd-moment are completely specified by the 1st and the 2nd moment . ref1 and ref2 if you had an interaction term in the hamiltonian/lagrangian involving 4 fields , then the 4-point correlation function would have 2 kinds of contributions : 2 sets of 2-point correlation functions between pairs of points among those 4 points a nontrivial contribution from the interaction term with one of it is field insertions at each of the 4 points . so you can see that higher order correlations functions give you very important ( an unique ) information in an interacting physical theory . update : the ( many ) answers to this se question might also shed some light on the discussion .
the anomalies in four dimensions are calculated from a triangular feynman diagram with a chiral ( left-right-asymmetric , when it comes to the couplings with the gauge bosons or gravitons ) fermion running in the loop and three gauge bosons ( and/or graviton [ s ] ) attached at the vertices . for the standard model , all the gauge anomalies cancel ( both leptons and quarks must be included , otherwise it would not work : it is somewhat nontrivial although the cancellation may be reduced to one simple observation in gut theories , among other fast methods to see why it works ) . they must also cancel in the supersymmetric models and in order to see what these anomalies are , we may look at the difference between the anomalies in the non-supersymmetric and supersymmetric models . the minimal supersymmetric standard model has almost the same spectrum of chiral spin-1/2 particles , the quarks and leptons : their anomalies cancel . their new superpartners are scalars which do not contribute to the anomaly . the new superpartners of gauge bosons are majorana fermions which are left-right-symmetric and contribute zero to the anomalies , too . the only new particles in the supersymmetric theory that may be running in the loop that contribute to the anomaly are higgsinos , the superpartners of the higgs boson ( the whole doublet ) . the anomaly ( for various combinations of gauge bosons ) from one higgsino , one new weyl fermion , is nonzero . it must be cancelled because gauge anomalies are inconsistencies ( preventing us from decoupling negative-probability time-like polarizations of gauge bosons ) . so the mssm deals with that by adding two opposite higgsinos whose charges are opposite to each other so all the anomalies cancel in between them . there is one more supersymmetric reason why we need two higgs doublets : the yukawa couplings must be holomorphic , arising from a superpotential $w=y\cdot h \bar q_l q_r$ , and when one distinguishes chiral superfields and antichiral superfields ( their complex conjugate ) , he finds out that only the up-type quarks ( or only the down-quark quarks ) could get masses from one higgs doublet ( the charges would not add up to zero if you added the opposite quarks ) . so the opposite , complex conjugate higgs doublet superfield ( whose higgsinos have the opposite handedness for the same sign of the supercharge and the weak isospin ) is needed to give masses to the remaining one-half of the quarks .
topology is of fundamental importance even to systems in classical mechanics . the configuration space ( or phase space ) of a generic classical mechanical system is a manifold and manifolds are topological spaces with some extra structure ( e . g . a smooth structure in the case of smooth manifolds ) . at the very start of any classical mechanics problem , you need to specify topological information . for example , when we consider a single particle moving in one-dimension , we could consider a particle constrained to move either on a compact manifold ( like a circle ) , or a non-compact manifold ( like the entire real line ) . in each of these cases , the global topology drastically changes the nature of the solutions . if , for example , the particle were otherwise free in each of these cases , then in the compact case ( the circle ) , the particle will always return to the same point after some finite time , while in the non-compact case ( the real line ) , this cannot happen . addendum . beyond it is fundamental importance to mechanics as described above , topological properties of classical mechanical systems are important for proving high-powered theorems about dynamical systems . if you , for example , open foundations of mechanics by abraham and marsden ( which is really more a math than physics ) , then you will find a chapter on so-called " topological dynamics " where you will find results like corollary 6.1.9 ; let $m$ be a compact , connected , two-dimensional manifold , $x\in\mathscr x ( m ) $ and $a$ a minimal set of $x$ . then either ( i ) a is a critical point , ( ii ) a is a closed orbit , ( iii ) $a=m$ and $m=s^1\times s^2$ . notice that the statement of this corollary depends on the topological assumptions that $m$ is compact and connected . there are all sorts of theorems like this in dynamical systems . see the poincare recurrence theorem as another example . update . ( 2014 - july - 10 ) more interesting information and discussion in the following physics . se post : what kind of manifold can be the phase space of a hamiltonian system ? see also the links in the comments to that post , especially to mathoverflow . update . ( 2014 - july - 16 ) even more interesting related information and discussion in the following post : is there a physical system whose phase space is the torus ?
you should use $u=q\epsilon_1$ . with the total number of particles $n$ being constant , we have : $$\frac{\partial s}{\partial q}=\epsilon_1 \frac{\partial s}{\partial e}=\frac{1}{t}\epsilon_1\tag{1}$$ as you said : $$\frac{\partial s}{\partial q}=k_b\ln ( n/q - 1 ) =k_b\ln ( n\epsilon_1/q\epsilon_1 - 1 ) =k_b\ln ( n\epsilon_1/u - 1 ) \tag{2}$$ $$\to \ , \ , \ , \ , \ , u ( t ) =n\epsilon_1\frac{e^{-\epsilon_1/ ( kt ) }}{1+e^{-\epsilon_1/ ( kt ) }}$$
basically , in atomic physics , you would have two electrons , each with an angular momentum $l_1$ and $l_2$ and spin $s_1$ and $s_2$ , and you want to couple all those to get the best approximation for the resulting spectrum . so you have two options : 1- you couple $l_1$ and $l_2$ to $l$ , and $s_1$ and $s_2$ to $s$ , and then you couple $l$ with $s$ to get $ls$ . 2- you couple $l_1$ and $s_1$ to $j_1$ , and $l_2$ and $s_2$ to $j_2$ , and then couple $j_1$ and $j_2$ to to get $jj$ . so you have two ways to couple those , and the choice depends on how far the electrons are from each other where the specific angular momentum coupling is more pronounced . so if the electrons are close to each other , then you use ls coupling . while if you have them far apart , you use jj coupling . i hope this helps .
1 ) spline curves are designed for just this sort of thing . a spline is basically a set of cubic ( typically ) polynomials at each section of your data that go exactly through your points . but since they are just cubic polynomials , they are pretty smooth . assuming the path is a reasonably small portion of the earth , and the points are close together on a global scale , you can just apply a spline to the latitude as a function of time , then to longitude as a function of time , then to elevation as a function of time . as an added bonus , most spline implementations will also let you take the derivative at any point along the spline . this gives you the speed -- up to appropriate factors because you are dealing with lat/long/elevation . i do not know what sort of computer packages you have access to , but python 's implementation can definitely do the job . ( for specifics about that , it is probably better to ask on stackoverflow . ) 2 ) i am guessing you mean to use this smoother path as the source for those intermediate points ? then , you can just ask the spline to evaluate at a time that is halfway between two times that you have in your original data . how you do this depends on your computer package . ( and again , is an issue better suited to stackoverflow . ) 3 ) in terms of prettiness , this can certainly make things look much better , and it is generally a reasonable thing to do . but you have to remember that the points you do not have in your data will just be made up , regardless of what you do . a spline is good at giving you something that looks reasonable . but whether or not the gps actually traveled that path is anyone 's guess .
there were historically several systems of units ( ancestors to modern si , cgs electric , cgs magnetic , cgs gaussian , cgs by heaviside ) , and the ultimate choice in favour of gaussian cgs was made when special relativity has united electric and magnetic fields into one electromagnetic field tensor . only in gaussian ( and heavisidian ) versions , these fields take no additional factors and make components of the field tensor immediately . any other choice just looks ugly . for the reference , the electromagnetic field tensor in cgs has a form $$f_{\mu\nu}=\left ( \begin{array}{cccc}\hphantom{-}0 and \hphantom{-}e_x and \hphantom{-}e_y and \hphantom{-}e_z\\-e_x and \hphantom{-}0 and -b_z and \hphantom{-}b_y\\-e_y and \hphantom{-}b_z and \hphantom{-}0 and -b_x\\-e_z and -b_y and \hphantom{-}b_x and \hphantom{-}0\end{array}\right ) $$
the very claim that there are " four fources " is an approximation . we know that the electromagnetic and the weak force have to be unified to an electroweak theory . so counting the electroweak theory as one force , there are just three known elementary forces . the electroweak theory is based on the $su ( 2 ) \times u ( 1 ) $ group which has two factors , but these two factors are not in one-to-one correspondence with the electromagnetism and the weak force , respectively . the strong force with its $su ( 3 ) $ group is another seemingly independent factors , except that there is evidence that all three non-gravitational forces get unified into a grand unified force of a gut theory at high energies . string theory unifies the non-gravitational forces with gravity , too . every vacuum of string theory predicts gravity described by gr plus extra non-gravitational forces . the number of factors and their higgs-like breaking patterns are essentially random properties of the string vacua . according to the anthropic picture of the world , the number of low-energy forces is an accidental property of our world that could be different in different parts of the multiverse . according to non-anthropic reasoning , the precise selection of our vacuum - including the fact that it has 4 low-energy forces - could be derivable from some more unique theoretical principles . however , this research program remains a wishful thinking as of 2011 .
if you go to this link you will see that the lifetime of the pi0 is orders of magnitude shorter than of the charged pions . 8.4 ± 0.6 × 10^−17 seconds , a time characteristic of electromagnetic reactions . it decays to two photons , which can be measured in the laboratory . if it is produced with some energy in the laboratory system , its speed can be estimated by measuring the four momenta of the photons and equating the sum to the four momentum of the pi0 . its speed then can be found for that individual measurement . there is no general " speed " of the pi0 , as there is no general speed of any elementary particle , their four momenta being dependent of the interaction that produced them and very variable . to have a speed a fraction of the speed of light any pion or other elementary particle should have an energy given by the relativistic formulae . have a look here where they calculate the energy necessary for a velocity 1% of the velocity of light for various particles .
is mr . koks wrong or am i missing something conceptually ? you are simply misunderstanding him . here is the full context of article in question : these equations are valid in any consistent system of units such as seconds for time , metres for distance , metres per second for speeds and metres per second squared for accelerations . in these units c = 3 × 108 m/s ( approx ) . to do some example calculations it is easier to use units of years for time and light years for distance . then c = 1 lyr/yr and g = 1.03 lyr/yr2 . here are some typical answers for a = 1g . so in theory you can travel across the galaxy in just 12 years of your own time . if you want to arrive at your destination and stop then you will have to turn your rocket around half way and decelerate at 1g . in that case it will take nearly twice as long in terms of proper time t for the longer journeys ; the earth time t will be only a little longer , since in both cases the rocket is spending most of its time at a speed near that of light . ( we can still use the above equations to work this out , since although the acceleration is now negative , we can " run the film backwards " to reason that they still must apply . ) mr . koks is saying that the total proper time , for a journey that constantly accelerates half the distance and then constantly decelerates ( at the same rate ) the remaining half , will be nearly twice as long as a journey that accelerates at a constant rate the entire distance . once the rocket reaches ultra-relativistic speed , the proper time nearly stops . so , it is reasonable that the proper is nearly twice as long for a journey that decelerates to sub-relativistic speed and then to a stop at its destination instead of speeding by it at near light speed . also , it is reasonable that the total coordinate time does not change that much between the two scenarios for the reason give by the author . do you see why ?
here we will only consider the first half of the question ( v2 ) . the birkhoff 's theorem is e.g. proven ( at a physics level of rigor ) in ref . 1 and ref . 2 . imagine that we have managed to argue that the metric is of the form of eq . ( 5.38 ) in ref . 1 or eq . ( 7.13 ) in ref . 2: $$ds^2~=~-e^{2\alpha ( r , t ) }dt^2 + e^{2\beta ( r , t ) }dr^2 +r^2 d\omega^2 . \qquad\qquad ( a ) $$ it is a straightforward exercise to calculate the corresponding ricci tensor $r_{\mu\nu}$ , see eq . ( 5.41 ) in ref . 1 or eq . ( 7.16 ) in ref . 2 . the notation is here $x^0\equiv t$ , $x^1\equiv r$ , $x^2\equiv\theta$ , and $x^3\equiv\phi$ . assuming a vanishing cosmological constant $\lambda=0$ , the einstein 's equations in vacuum read $$r_{\mu\nu}~=~0~ . $$ the argument is now as follows . from $r_{tr}=0$ follows that $\beta$ is independent of $t$ . from $e^{2 ( \beta-\alpha ) } r_{tt}+r_{rr}=0$ follows that $\partial_r ( \alpha+\beta ) =0$ . in other words , the function $f ( t ) :=\alpha+\beta $ is independent of $r$ . define a new coordinate variable $t:=\int^t dt&#39 ; ~e^{f ( t&#39 ; ) }$ . then the metric $ ( a ) $ becomes $$ds^2~=~-e^{-2\beta}dt^2 + e^{2\beta}dr^2 +r^2 d\omega^2 . \qquad\qquad ( b ) $$ rename the new coordinate variable $t\to t$ . then eq . $ ( b ) $ corresponds to setting $\alpha=-\beta$ in eq . $ ( a ) $ . from $r_{\theta\theta}=0$ follows that $$1=e^{-2\beta} ( 1-2r\partial_r\beta ) \equiv\partial_r ( re^{-2\beta} ) , $$ so that $re^{-2\beta}=r-r$ for some real integration constant $r$ . in other words , we have derived the schwarzschild solution , $$e^{2\alpha}~=~e^{-2\beta}~=~1-\frac{r}{r} . $$ finally , if we switch back to the original $t$ coordinate variable , the metric $ ( a ) $ becomes $$ds^2~=~-\left ( 1-\frac{r}{r}\right ) e^{2f ( t ) }dt^2 + \left ( 1-\frac{r}{r}\right ) ^{-1}dr^2 +r^2 d\omega^2 . \qquad\qquad ( c ) $$ it is interesting that the metric $ ( c ) $ is the most general metric of the form $ ( a ) $ that satisfies einstein 's vacuum equations with $\lambda=0$ . the only freedom is the function $f=f ( t ) $ , which reflects the freedom to reparametrize the $t$ coordinate variable . references : sean carroll , spacetime and geometry : an introduction to general relativity , 2003 . sean carroll , lecture notes on general relativity , chapter 7 . the pdf file is available here .
escape velocity depends on what you are trying to escape from and how far away from it you are . that wikipedia reference makes that very clear . as sachin shekhar pointed out , if you are in the vicinity of the sun , and you are trying to escape the galazy , you need 525 km/s . if you are trying to escape the solar system , and you are at neptune 's distance from the sun , you only need 7.7 km/s . when did voyager 2 achieve escape velocity from the solar system ? according to this diagram from wikipedia , it occurred during its gravity assist from jupiter . voyager 1 was probably not too much different .
by the word classical we will mean $\hbar=0$ , and we will use the conventions of ref . 1 . the lagrangian density for maxwell theory with various matter content is $$\tag{1} {\cal l} ~=~{\cal l}_{\rm maxwell} + {\cal l}_{\rm matter} , $$ $$\tag{2} {\cal l}_{\rm maxwell}~=~ -\frac{1}{4}f_{\mu\nu}f^{\mu\nu} , $$ $$\tag{3} {\cal l}_{\rm matter}~=~{\cal l}_{\rm matter}^{\rm qed}+{\cal l}_{\rm matter}^{\rm scalar qed} + \ldots , $$ $$\tag{4} {\cal l}_{\rm matter}^{\rm qed} ~:=~ \overline{\psi} ( i\gamma^{\mu} d_{\mu}-m ) \psi , $$ $$\tag{5} {\cal l}_{\rm matter}^{\rm scalar qed}~:=~ - ( d_{\mu}\phi ) ^{\dagger} d^{\mu}\phi -m^2\phi^{\dagger}\phi -\frac{\lambda}{4} ( \phi^{\dagger}\phi ) ^2 , $$ with covariant derivative $$ \tag{6} d_{\mu}~=~d_{\mu}-iea_{\mu} . $$ ( here we are too lazy to denote various matter masses $m$ and charges $e$ differently . ) the matter equations of motion ( eom ) are $$ \tag{7} ( i\gamma^{\mu} d_{\mu}-m ) \psi ~\approx~0 , \qquad d_{\mu}d^{\mu}\phi~\approx~m^2\phi+\frac{\lambda}{2} \phi^{\dagger}\phi^2 , \qquad \ldots . $$ ( the $\approx$ symbol means equality modulo eom , i.e. an on-shell equality . ) the infinitesimal global off-shell gauge transformation is $$ \delta a_{\mu} ~=~0 , \qquad \delta\psi~=~-i\epsilon \psi , \qquad \delta\overline{\psi}~=~i\epsilon \overline{\psi} , $$ $$ \tag{8} \delta\phi~=~-i\epsilon \phi , \qquad \delta\phi^{\dagger}~=~i\epsilon \phi^{\dagger} , \qquad \ldots , \qquad\delta {\cal l} ~=~0 , $$ where the infinitesimal parameter $\epsilon$ does not depend on $x$ . the noether current is the electric $4$-current$^1$ $$ \tag{9} j^{\mu}~=~e\overline{\psi}\gamma^{\mu}\psi - ie\{\phi^{\dagger} d^{\mu}\phi- ( d^{\mu}\phi ) ^{\dagger}\phi\}+\ldots . $$ noether 's theorem is a theorem about classical field theory . it yields an on-shell conservation law $$ \tag{10} d_{\mu}j^{\mu}~\approx~0 . $$ hence the electric charge $$\tag{11} q~=~\int\ ! d^3x~ j^0$$ is conserved on-shell . references : m . srednicki , qft . -- $^1$ interestingly , the electric $4$-current $j^{\mu}$ depends on the gauge potential $a_{\mu}$ in case of scalar qed matter .
there are a lot of methods to calculate free surface or multiphase flows , and most of them have some implementation of surface tension forces . a small list : volume of fluid method level set method marker and cell method moving mesh techniques
lumo gives a very nice step by step calculation of this sum and a good discussion of the importance and application of such summation techniques in qfts here : http://motls.blogspot.com/2011/07/why-is-sum-of-integers-equal-to-112.html such mathematical calculations are not unrelated to physics ; on the contrary they are important . . .
the moment of inertia is merely a generalisation/application of the ‘usual’ inertia to rotations . since translations and rotations are different kinds of motion , it appears sensible ( to me ) to have different kinds of inertia associated with them . regarding your second question : imagine a particle at position $ ( x , 0,0 ) $ which you would like to rotate with angular velocity $\omega$ about the $ ( 0,0 , z ) $ axis . to do so , you have to initially accelerate the particle along the $ ( 0 , y , 0 ) $ axis to velocity $v_y = \omega x , v_{x , z} = 0$ , as this is the velocity the particle would have at this point if it were already rotating . as you can clearly see , the momentum $p$ associated with this velocity is proportional to $r$ ( $p_y = m v_y = m \omega x$ ) , hence it takes more energy to accelerate a particle to angular velocity $\omega$ if it is further away from the centre of rotation . as this is exactly the quantity described by the ‘moment of inertia’ , the moment of inertia depends on the radial distance of the mass .
the electron-phonon scattering is a process that takes the electron across its fermi surface ( from an occupied state to an empty state , or vice versa ) by absorbing or emitting a phonon . compared to the electron energy in most solid state materials , the phonon energy is neglectable , such that the electron will ( almost ) not change its energy when scattering with a phonon . therefore the phonon can only scatter off the electrons on the fermi surface . however the greatest possible momentum transfer on the fermi surface is $2k_f$ ( assuming spherical fermi surface with radius $k_f$ ) . so all scattered phonon must have a momentum $q\leq 2k_f$ .
if the container full of air is spinning around you , the drag will eventually set you spinning as well , regardless of the rotational speed or the air density . low air density just means that it will take much longer . eventually the air and you will share the same rotation , so that as you speed up , the air and the container will slow down . only in ( complete ) vacuum will you never start spinning . but there is no such thing as a complete vacuum , there are always at least some atoms or molecules around . when the density gets too low , quantum effect will start to take over , as individual particles push you one way or the other .
from this google book hit , i think it is called a ketenyl radical . searching ketenyl radical seems to bring up hits with the same compound formula . incidentally , it was not that hard to find by searching hcco compound . ninth hit , i think . . .
i think your questions concerns somehow another question : what is the relation between the macroscopic observables ( like electrical current , temperature ) of system consisting of many particles and parameters of single particles forming this system . the answer on this question is given by statistical physics . first , it will be useful to think about an electron gas in a metal wire without any current . there is no current , but i dare you all particles are moving if the temperature is not equal zero . the electrical current results from averaging of velocities of all charge carriers and , therefore , equals zero . if we apply voltage all electrons gain an additional component to their chaotic velocities in the direction of the electric field . we have got electron drift , however it is not equal to microscopic current of each electron . is the velocity of the particle in the direction of the current ? it depends on the charge sign of charge carriers . if we have got electrons then their averaged speed is opposite to the direction of the electrical current .
only with this , as the waves have different wavelengths , i guess there can not be any interference , we will only see the difraction pattern , the two functions of the form sin2 ( x ) /x2 , with the principal maximums separated a distance d . am i right here ? sort of . the diffraction pattern is visible " at infinity " , which is in fact your case #3 . i will explain there . 1 . - in the first one , the system is configured such that the slits are far away from the lens . here , we can approximate the wave that arrives as a planar wave , and therefore the lens will perform the fourier transform in the focal plane of the screen . the diffraction of the slits also performs the fourier transform , so this configuration should lead to having only two bars of light in the screen , centered in the focus . am i right ? sort of . your lens has a limited diameter , so if you place it far from the slits , it will capture only the central portion of the diffraction pattern , i.e. the top of the sinx/x function . in other words , you will loose the fringe pattern and reconstruct slits without fringes . 2 . - the slits are in the focal plane on the lens , such that the lens is in the middle of slits-screen . here , the same thing should happen , right ? as the light comes from the focal plane , the lens must do the fourier transform with no extra things , and we should get the two bars , again both of them in the same line ( center of the screen ) . am i right here ? this will be somewhat different because you do the image of the slits at infinity , i.e. a blurry image at close distances . depending on how close is the lens , you may only be taking the " no-fringe " portion of the diffraction pattern . 3 . - the last one , i can not see . . . the lens is just behind the slits so the distance between slits and lens is 0 . that is exactly the typical school case . the diffraction pattern is , before the lens , located at infinity , or in other words , the fringes are defined as angles and not position ( $\sin\theta/\theta$ ) . the role of the lens is to bring these fringes at a finite distance ( the focal length ) . you probably learned that a lens makes an image , initially located at infinity , located at the focal length . that is the same with the diffraction fringes . now , as the two slits are both close to the lens , you will not do an image of them . that means that you should not see separated slit images , but instead , you should see two superimposed diffraction patterns , centered at the same point .
the rule is : with a diagonal metrics , the character of the coordinate $x^i$ , is given by the sign of $g_{ii}$ setting $t= \frac{u + v}{2}$ and $z= \frac{u - v}{2}$ , your metrics becomes ( with the constraint $z &gt ; 0$ ) : $$ds^2= -dt^2 + dz^2+ ( 1-t-z ) ^2dx^2+ ( 1+t+z ) ^2dy^2$$ so , the character of $t$ as a time-like coordinate and $z$ as a space-like coordinate ( with the constraint $z &gt ; 0$ ) appears clearly . and $x$ and $y$ are space-like coordinates too . the variables $u = t + z$ and $v = t - z$ are called light-cone coordinates , you could say that they are light-like coordinates . they are very often used in general relativity and string theory . they are also called null coordinates , because for instance , if $dx=dy=0$ , then you have $ds^2 = - 2dudv$ , so each particle with $u=$constant or $v=$constant corresponds to $ds^2=0$ , that is a light-like interval ( a light ray ) . but you cannot say that one of the coordinates $u , v$ is a time-like coordinate , and the other a space-like coordinate . in schwarzchild metric , it is not correct to say that $r$ and $t$ could change their sign . the schwarzchild metric describes a gravitational field only for $r&gt ; r_s$ . it is a limited description ( which does not cover the entire manifold ) , and you have to use kruskal-szekeres coordinates to have a glbal view of the black hole . mathematically , " these coordinates have the advantage that they cover the entire spacetime manifold of the maximally extended schwarzschild solution and are well-behaved everywhere outside the physical singularity . "
it does not matter . if you have a scalar $\alpha$ then $$\alpha ( \vec{b} \times \vec{c} ) = ( \alpha \vec{b} ) \times \vec{c} = \vec{b} \times ( \alpha \vec{c} ) . $$ you can prove this simply by writing out the components of each of the three expressions and showing that no matter which order you do it in you will get the same results .
is there a complete physics simulator that i can use to do general simulations for learning purposes ? any turing complete programming language . some assembly required . we often say that all models are wrong , so whatever problem you desire to simulate is working on some level of abstraction of more fundamental physical laws . these are generally problem-specific and why you should not expect a " general " physics simulator to answer your question ( at least not in 2012 ) . even if you had perfectly correct governing equations , when you perform numerical simulation inadequacies are introduced that range from a loss of accuracy to completely changing the system dynamics . my simple suggestion is to browse some library of physics codes . it sounds like you want something learning-oriented , fairly open in nature , easy to use , and powerful enough to do full fluid mechanics simulation . these software requirement are almost laughably daunting . for the record , relaxing the accuracy requirement for a computational fluid dynamics simulation does not make it easy . here is an ongoing effort that i strongly support , open source physics : http://www.compadre.org/osp/ you should browse through their libraries to find what comes closest to your requirements . hypothetically , there is nothing keeping one from using their format ( with java ) to write a full 3d simulation , but i must return to my point that you have completely underestimated the challenge of that task . here are some results of the fluid mechanics section of their library : http://www.compadre.org/osp/search/search . cfm ? gs=225 and b=1 and qc=compiled%20simulation i suspect you will find that unsatisfactory for your purposes . here is a specific applet that does a basic molecular dynamics ( md ) simulation for a gas in 2d : http://www.compadre.org/osp/items/detail.cfm?id=8624 i think it is a fantastic program , but the properties of the simulated fluid do not match what you want . it would be possible for you to rewrite their code , expanding into 3d and changing the particle interaction rules to fully answer the question at hand . it would take a lot of work , but if you do so , by all means , please submit your code and post a link .
the integral you get is correct , this one : $$e ( r ) =\int_{-\infty}^{\infty}\frac{\sigma dl}{2\pi\epsilon_0 r}$$ you make a mistake in the next step . you cannot equate this integral to $$e ( r ) =\frac{\sigma }{2\pi\epsilon_0r}\int_{-\infty}^{\infty}dl$$ simply because $r$ is not constant for every infinite line element you consider , and you cannot take it out of the integral . lets say that the point at which you want your electric field is $d$ distance above the infinite sheet , and it is directly above some line element $l_o$ . the field at this point due to line $l$ which is perpendicular distance $l$ away from $l_o$ will be given by $$de ( l ) =\frac{\sigma dl}{2\pi\epsilon_0\sqrt{l^2+d^2}}$$ , where $\sqrt{l^2+d^2}$ is the perpendicular distance ( otherwise written as $r$ ) of the point from this line . this is incomplete too . notice that the field due to every line element is not in the same direction . so you will have to vectorially add them , and you simply cannot directly integrate the above equation . an easy method to do this would be to consider the net field of two line elements placed symmetrically about $l_0$ . their net field will be perpendicular to the infinite plane sheet , and given by $$de ( l ) =2\frac{\sigma dl}{2\pi\epsilon_0\sqrt{l^2+d^2}} \frac{d}{\sqrt{l^2+d^2}}$$ now this equation you can integrate because this field has a direction perpendicular to the plane for all $l$ . so the answer simply is $$e=\frac{\sigma d}{\pi\epsilon_0} \int^\infty_{0}\frac{dl}{l^2+d^2}$$ notice that i only integrate from $0$ to $\infty$ because we have considered the net field of both lines at $l$ and $-l$ together in the above equation . so going from $0$ to $\infty$ , you also include all the elements from $-\infty$ to $0$ . this integral then correctly gives $e=\frac{\sigma}{2\epsilon_0}$
i think there is some interesting physics to be had here . the rate of change of temperature depends on the rate of heat flow in from the electric heating element and the rate of heat flow out as heat is lost to the air . if we write the heat capacity of the hotplate as $c$ ( $c$ is the traditional symbol for heat capacity ) then : $$ \frac{dt}{dt} = c \left ( \frac{dh_{in}}{dt} - \frac{dh_{out}}{dt} \right ) $$ where $dh_{in}/dt$ is the rate of heat flow in and $dh_{out}/dt$ is the rate of heat loss . $dh_{in}/dt$ is simply the power being supplied to the hotplate . you can measure the current the hotplate draws from the mains and calculate the power that way , or you could simply use a power meter . the power in watts , call this $w$ , is simply the energy in joules per second , so it is exactly what you need for $dh_{in}/dt$ . the rate of heat loss , $dh_{out}/dt$ is harder because it depends on how the hotplate is cooled . if the cooling is dominated by convention ( it probably is ) then the cooling will obey newton 's law of cooling and the heat loss will be given by : $$ \frac{dh_{out}}{dt} = a \space ( t - t_0 ) $$ where $t_0$ is the ambient temperature and $a$ is some constant to be determined experimentally . put all this together and you will get : $$ \frac{dt}{dt} = c \left ( w - a \space ( t - t_0 ) \right ) $$ you will need to measure $c$ and $a$ experimentally . if you have a copy of excel to hand you can use its solver to fit values of $c$ and $a$ . alternatively , you can get $c$ from the initial rate of temperature rise . when $t \approx t_0$ the heat loss is small and : $$ \frac{dt}{dt} \approx cw $$ so if you know $w$ you can calculate c . you can calculate $a$ by heating the hotplate then turning the power off and letting it cool . as it cools the temperature variation is : $$ \frac{dt}{dt} = -c a \space ( t - t_0 ) $$ so if you know $c$ you can calculate $a$ .
at the moment of the lunar eclipse both the earth and moon are moving tangentially to the line joining them to the sun , and their velocities are parallel . the diagram above shows the moon just before , during and just after the lunar eclipse . i am guessing the question is asking whether the velocity of the moon , $v_m$ , is greater or less than the velocity of the earth , $v_e$ . if so , then you just have to see what direction the earth 's shadow moves across the moon . if $v_m &gt ; v_e$ then the moon starts at the lower position and moves up past the earth , so the shadow starts at the top edge of the moon and moves down . if $v_m &lt ; v_e$ then the moon starts at the top position and moves down , so the shadow starts at the bottom edge and moves up . note that my diagram shows the top view of the solar system i.e. looking down on the north pole , so the top edge is the east edge and the bottom edge is the west edge .
why is that the case ? why does all the heat go towards its kinetic energy per unit mass ? that is my one question basically there are two places for the energy to go in the output stream : thermal energy kinetic energy the problem through ( a ) is a direct energy balance problem . in this part , we are treating $t$ as independent . if $t$ is the average of the inlet temperatures , then the velocity will be zero because all energy is accounted for , and $\delta q$ will be zero . mentally , i see a thermal cycle with the $t_1$ feeds the boiler and $t_2$ feeds the condenser . that cycle outputs useful work . the more useful work it produces , the more the average temperature of the streams is lowered . this is similar to a thermal power plant . the boiler produces more heat than the condenser rejects . typical efficiency is 33% , so the condenser removes only 2/3rd of the boiler 's heat . the rest of the heat went to turning the turbine because heat is a form of energy and energy is the ability to do work . in your case , the stream might have been accelerated by a pump that feeds into a cavity that has a nozzle where the stream is accelerated . the more work the pump does , the more the temperature of the water has to decrease because this is a closed system . practically , either the temperature change would be miniscule or the velocity would be gigantic . the reason is that thermal vibrations are fast compared to speeds we are used to . you can look at it this way in terms of your problem too ! the average kinetic energy of the molecules is the same going in and going out - it is just that some of that kinetic energy going out is from the bulk motion of the stream so we do not count it as temperature . that temperature would have to be measured by a thermometer moving along with the stream .
i think it could be simply that the dirac operator is invariant under isometries , so if $\phi$ is an isometry and $\psi$ a solution to $$d\psi = 0 , $$ then $\phi^* \psi$ is also a solution , where $\phi^*$is pullback . then it would be similar to how harmonic functions $f$ on the sphere -- $\nabla^2 f = 0$ -- come in representations of the rotation group , the $y^l_m$ . in more detail if $\phi$ is a diffemorphism , that in coordinates takes the form $y^\mu = y^\mu ( x^\nu ) $ ( not a tensor expression ) , and $v^\mu$ is a vector field , then we can define a vector field $$ ( \phi_* v^\mu ) ( \phi ( p ) ) = \frac{\partial y^\mu}{\partial x^\nu} v^\nu ( p ) $$ called the pushforward of $v^\mu$ . naturally we can pushforward any tensor , in particular the metric . by definition $\phi$ is an isometry if $$ ( \phi_* g_{\mu\nu} ) ( \phi ( p ) ) = g_{\mu\nu} ( \phi ( p ) ) . $$ this means that if we have any tetrad ( also known as a vierbein or a frame ) , that is a set of vector fields $e_a^\mu$ such that $e_{a\mu} e^\mu_b = \eta_{ab}$ for some symmetric matrix $\eta_{ab}$ with signature $+---$ , it is pushed forward to another tetrad . i let $\eta_{ab}$ be general because in spinor problems it is more natural to use a null tetrad $$\eta_{ab} = \begin{pmatrix} 0 and 1 and 0 and 0 \\ 1 and 0 and 0 and 0 \\ 0 and 0 and 0 and -1 \\ 0 and 0 and -1 and 0\end{pmatrix} . $$ since $\eta_{ab}$ has zeros on the diagonal all the tetrad vectors are null . it is well known ( see for example spinors and space-time or the newman-penrose paper ) that to every null tetrad corresponds exactly two bases for two-spinors , called dyads , say $ ( o^a , \iota^a ) $ and $ ( -o^a , \iota^a ) $ . thus at least for isometries connected to the identity , the pushforward of tetrads lifts to a pushforward of dyads . ( however when the isometry group is not simply connected this might not be continuous globally , but i think it does not matter here , since we can consider isometries close to the identity , which will take us to lie algebra representations , and then we integrate them , and discard the representations that require passing to the simply connected cover . ) since we can pushforward dyads we can pushforward two-spinors ( by linearity ) , since we can pushforward two-spinors we can pushforward dirac spinors . $\newcommand{\dslash}{\ ! \not d}$ in particular for a dirac spinor $\psi$ , $\dslash\psi$ is of course also a dirac spinor , so $$\beta = \phi_* ( \dslash\psi ) = \phi_* ( \dslash \phi^* \phi_* \psi ) $$ makes sense , where $\phi^*$ as the inverse of $\phi_*$ so the second equality is just inserting the identity . now $\phi_* \dslash \phi^*$ defines a differential operator , it is the transformed dirac operator under the isometry $\phi$ . but since the dirac operator is defined by the metric and $\phi$ preserves the metric , this must be just the dirac operator again . ( you can probably make this argument more convincing . ) thus we have established that $$\beta = \dslash ( \phi_*\psi ) . $$ in particular if $\beta = 0$ , so that $\psi$ is a zero mode for the dirac operator , then $\tilde{\psi} = \phi_* \psi$ is also a solution . thus the isometry group ( or at least its lie algebra ) acts on zero modes .
the rc circuit that approximates a myelinated fiber looks like a series of resistors along the length of the fiber with capacitors connected to ground at the gaps between the schwann cells . the time to reach discharge voltage at the next junction after depolarization at the prior junctions will be determined by the time it takes to raise the capacitor voltage to that threshold for depolarization . decreasing the capacitance decreases the time to charge a capacitor , so decreasing capacitance shortens the time for the signal to " jump " to the next node . i did a bit of searching and thought that the cable theory material you found was not entirely on point for myelinated nerves . it looked more germane to brain neuronal transmission . the best synopsis i found was in the ' nerve conduction ' section of an online biophysics text : http://www.centenary.edu/biophysics/bphy304/book/#/116/ it is also available separately : http://www.centenary.edu/attachments/biophysics/bphy304/11a.pdf
feynman in multiple writings suggested thinking about " exchanging particles " in terms of exchanging them as they move through time . that is , they can either move in two parallel paths as they move forward , or they can cross paths ( exchange roles ) . the antisymmetric cancellation applies to the latter , but not to the former . now if you think that through , it means that the parallel path remains strong even as the crossover paths cancel out , resulting in the two particles avoiding each other and maintaining unique paths ( wave functions ) . the net result is not full cancellation , but cancellation at the edges , where the particles would cross . ( feynman goes into a lot more detail about rotations , but frankly that part can get you sidetracked a bit ; it is the " anti-crossover " part that counts in terms of actual outcomes . ) another consequence of identical fermions cancelling each other out is that packing more fermions into a tight space forces their space-filling wavelengths to become shorter also . since in quantum mechanics the spatial wavelength of a particle defines its momentum , particles that are squeezed in this fashion also get very , very hot . a neutron star is a good example . pauli exclusion -- the " constriction of space because crossover cancels but parallel does not " -- allows neutrons to pack together very densely indeed . there are limits , however . when gravity gets too monumental , even pauli exclusion is unable to keep up with the pace , and the entire star collapses , very quickly . thus is born a stellar-sized black hole , or at least this is one example of how one can form .
the acceleration of uniform circular motion is a very basic computation that we do for first year students . $$ a = \frac{v^2}{r} $$ which for someone standing on the earth 's equator comes to $$ a_\text{equator} \approx \frac{\left ( 465\text{ m/s}\right ) ^2}{6400\text{ km}} = 0.03\text{ m/s}^2$$ or less than 1% of g . that is a measurable quantity , but not very significant . indeed fluxuations of local $g$ at that level can ( and do ) occur simple due to local deposits of heavy ore . mining and oil companies use precise gravitation maps in surveys for exactly this kind of reason .
the van de graaff generator itself has stored potential energy , even without the extra charged object . like a capacitor , there will be energy stored in the electric field . when you turn the generator off , this energy does not vanish by itself but either dissipates slowly through corona discharge , or quickly by arc discharge . the potential energy in moving a charged object close to the generator would go through the same pathway . by bringing the object close to the generator , you will alter the electric field , increasing the electric potential on the generator . more energy will then be dissipated in any arc or corona discharge .
note ; i will use the summation convention throughout here . in the context of differential geometry , the indices on tensorial objects are raised and lowered with the metric on the space ( manifold ) being studied . so for example $$ t^i_{\phantom i j} = g^{ik}t_{kj} $$ and $$ t^{ij} = g^{ik}g^{jl}t_{kl} $$ notice that if the metric is simply that of euclidean space , namely if $g_{ij} = \delta_{ij}$ , then raising and lowering does not change the numerical values of tensor components . in particular , one would have $$ t^{ij} = t_{ij} $$ notice that in expressions like $$ t^{ij}t_{ij} $$ both indices are being summed over , where as in the expressions $$ ( t^{ij} ) ^2 , \qquad ( t_{ij} ) ^2 $$ one usually ( this is actually a matter of notational preference ) does not intend for their to be any implied summation , so typically its notationally safe to assume that $$ t^{ij}t_{ij}\neq ( t^{ij} ) ^2 , \qquad t^{ij}t_{ij}\neq ( t_{ij} ) ^2 $$ but if the metric satisfies $g_{ij} = \delta_{ij}$ , then it is true that $$ ( t^{ij} ) ^2= ( t_{ij} ) ^2 $$
it is simple . for people with myopia ( nearsighted ) the corrective lenses are concave . in the other case , the corrective lenses are convex . so just by looking at the type of lens you can tell the difference . if the dioptres are small , look to the edges of the lens , there you will see the difference .
the lightest gas that is stable at room temperatue is $h_2$ ( two hydrogen atoms bonded to each other ) . at very high temperature and/or low pressure , the hydrogen molecule dissociates to become atomic hydrogen . at even higher temperature , the hydrogen ionizes . the electron is no longer bound to the proton . this is a plasma state . electrons and protons are still both present . if just bare protons were present , they would react with the teflon . see this for more information on states of hydrogen .
so i think you should check out this problem for some useful commentary . problem understanding sign of volume integral in minkowski space essentially if you look at the integral on the lhs we see it takes the form explained in the problem above $$ i^{\mu\nu} [ f ] = \int d^4k\ , f ( k^2 ) k^\mu k^\nu $$ where f is just 1 the integral is clearly lorentz invariant ( as shown in the question above ) . our answer therefore must be the metric tensor ( which is a rank 2 lorentz invariant tensor ) times a scalar . by contracting each side with $$ g_{\mu\nu} $$ we find that scalar has to be equal to $$ \frac{1}{4} \int d^4x x^2 $$
here are ray diagrams that show what is going on . in the top case , a weak ( thin ) lens does not have the power to pull the rays together tight enough . an object farther away than the tree would make rays converge on the retina . this is farsightedness . remember the fundamental formula for thin lenses ( using some appropriate sign convention ) : $$ {1\over f}={1\over d_1}+{1\over d_2} $$ if $d_1$ increases , then $d_2$ must decrease . thus rays of farther away objects converge in a shorter distance after the lens , hopefully on the retina . in the second diagram , the lens is of proper thickness , and each part of the tree and nearby paraphernalia focus on the retina . just for comparison , the bottom diagram shows a lens too strong ( thick ) causing rays to converge too quick , before reaching the retina . to meet at the retina , a slightly longer distance , the outside world object must decrease its distance - this is nearsightedness . note that in all cases , the tree image on the retina is upside down . the only difference is if it is sharp or blurry .
the only force which works is gravity$^1$ . so , change in gravitational potential energy equals final kinetic energy ( assume initial is zero ) . $$mgh=mv^2/2$$ $$v=\sqrt{2gh}$$ here $h$ is vertical height traversed . see the velocity does not depend on angle of string , mass of body too . . let 's see the kinematics of body . the length of string is $h cosec\theta$ ( $\theta $ being angle with horizontal assumed $\pi/6$ ) acceleration of body along the string=$g\sin\theta$ now $\text{using} : v^2=u^2+2as$ $$v^2=0+2\times h cosec\theta\times g \sin\theta$$ $$v=\sqrt{2gh}$$ working in differentials for $v$ along the rope . $$dv/dt=v\dfrac{dv}{dx}=a$$ $$\int_0^{v_f} v . dv=\int_0^{hcosec\theta} a . dx=ax\bigg|_0^{hsosec\theta}$$ $$\dfrac{v_f^2}2=gsin\theta . hcosec\theta \ \ ; \ \ a=gsin\theta$$ $1 ) $assuming the pulley being used to slide to be friction less . though not possible . also the rope is assumed to be in-extensible and straight .
according to most sources such as this university page on albedo and this modeling paper on albedo versus wavelength , the typical albedo of snow is virtually 1 throughout the visible region , so it seems unlikely that it is the snow itself which is causing this effect . also , i have never personally observed this as john rennie stated , so perhaps it is caused by something else in your case , such as an atypical source of illumination .
the up quark has a charge of $+2/3$ , the down has a charge of $-1/3$ . if you have a bound state of charged particles , the total charge is just the charge of the elementary constituents . the neutron consists of one up quark and two down quarks , so the total charge $q$ is : $$q = 2/3 + 2 \times ( -1/3 ) = 0$$
yes it can be seen , given that you do not get much noise and that your sensor is sensitive enough for it to be able to detect the signal . as you have not specified anything about the light not much can be concluded , more than that it depends on the sensor , the transmitter and the noise from everything else .
both " perfectly open " ( zero acoustic impedance ) and " perfectly closed " ( infinite acoustic impedance ) boundary conditions are only idealizations that never occur in practice . for the case of the human vocal tract , they are not even very good approximations . the " bottom end " of the resonating cavity is not , in fact , the lungs , but the vocal folds ( as georg pointed out ) . this end has some acoustic impedance that is neither extremely low nor extremely high . i am sure the impedance also changes somewhat with the pitch and volume of the phonation . the " top end " of the cavity is of course the mouth , and its impedance changes with the vowel sound you are pronouncing . for aptly-named " open " vowels such as " ah " , " oh " , and so on , the impedance is actually low enough that it might be a good approximation to say it is perfectly open . but for closed vowels ( "ee " , " oo " , . . . ) and especially for humming with closed lips , the acoustic impedance is much higher ( but still far from infinite ) .
assume for simplicity that the speed of light $c=1$ . the existence of the gauge $4$-potential $a^{\mu}= ( \phi , \vec{a} ) $ alone implies that the source-free maxwell equations $$\vec{\nabla} \cdot \vec{b} ~=~ 0 \qquad ``\text{no magnetic monopole"}$$ $$ \vec{\nabla} \times \vec{e} + \frac{\partial \vec{b}}{\partial t} ~=~ \vec{0}\qquad ``\text{faraday 's law"}$$ are already identically satisfied . to prove them , just use the definition of the electric field $$\vec{e}~:=~-\vec{\nabla}\phi-\frac{\partial \vec{a}}{\partial t} , $$ and the magnetic field $$\vec{b}~:=~\vec{\nabla}\times\vec{a}$$ in terms of the gauge $4$-potential $a^{\mu}= ( \phi , \vec{a} ) $ . the above is more naturally discussed in a manifestly lorentz-covariant notation . op might also find this phys . se post interesting . thus , to repeat , even before starting varying the maxwell action $s [ a ] $ , the fact that the action $s [ a ] $ is formulated in terms the gauge $4$-potential $a^{\mu}$ means that the source-free maxwell equations are identically satisfied . phrased differently , since the source-free maxwell equations are manifestly implemented from the very beginning in this approach , varying the maxwell action $s [ a ] $ will not affect the status of the source-free maxwell equations whatsoever .
suppose your robot walks vertically on two legs , and you want to mount a gyroscope in the center of the robot with a vertical spin axis . as a seat-of-the-pants engineer , i would ask how much rolling moment is needed to keep the robot from falling very far before it places a foot so it will stop falling . this depends on the robot 's mass , how high it is , how far apart the legs are , and how quickly they move . when you have decided on the maximum moment , consider how much precession you can tolerate . example . suppose the gyro consists of mass $m$ concentrated in a ring of radius $r$ spinning clockwise at angular velocity $w$ ( radians/sec ) when looked at from above . suppose you can pitch the vertical axis of the gyroscope forward or backward . if you move the vertical axis forward , that will have the effect of creating a gyroscopic reaction trying to roll the axis to the right ( and vice-versa ) . if $v$ is the angular velocity at which you pitch the axis forward , the roll moment will be : $m r^2 w v$ in newton-meters . the amount by which you pitch the axis forward depends on how long you need to apply the moment . btw , you can also apply a fore-and-aft moment by rolling the axis left or right . note : anti-rolling gyro . added in response to comment : ok , so you are concerned about pitch angle . the gyro axis could be in any direction except left-to-right . in order for the gyro to work , it needs to be free to precess . in some of the ship stabilizer applications , it simply has dampers ( springs and shock absorbers ) holding the axis in place , but giving it freedom to move . here 's how i think about it . suppose in front of your segway there was a stream of water from a fire hose being sprayed from left to right . you could , holding a flat plate in your hand , put it in the water stream in such a way as to deflect the water down or up at an angle . that would provide a corrective force that you could use to balance your segway . you can work out the physics of how much force you could get as a function of the deflection angle and the amount and speed of the water . now instead of a stream of water , you have a stream of metal . instead of going in a linear stream , it is going in a circle . instead of simply being deflected downward at an angle , it is axis of rotation is changing so as to deflect the direction of the metal downward on the side where it is moving left-to-right ( and upward on the return trip ) . so you need to get an estimate of how much corrective force you want to be able to get , and how much axis-movement you want to pay for to get it . i would experiment , first trying something like a bicycle wheel , maybe filled with water , maybe put heavy metal weights around the outside . spin it up by some means ( cord , electric drill , or direct motor ) and measure the effect . you can double the effect by doubling the mass or doubling the rotation rate . you can quadruple the effect by doubling the radius .
this question is very difficult to answer , and in the end , the answer is going to be more semantic than it is going to be physcial . the reason for this is that it is very difficult to pull apart what is done by " gravity " and what is done by the matter content of the universe . the safest answer to this is to say that during inflation , the matter content of the universe has a predominantly negative pressure , which causes objects to expand . during a truly inflationary period , the density of this substance does not decrease as the universe expands , so the rate of expansion is approximately exponential . there are various ways to create a scenario like this if you assume different properties for the quantum field theoretic vacuum or various types of matter coupled to gravity .
when no current is flowing , the system is in thermal equilibrium . the electrons do transfer kinetic energy to the atoms through collisions , but the atoms also transfer kinetic energy to the electrons , and these two processes happen at the same rate , so there is no net energy transfer and the system neither heats up nor cools down . this is just the same as any other case of thermal equilibrium : effectively , the electrons and the atoms are at the same temperature , and that is why there is no heat flow . however , when you switch the voltage on there is an electric current accelerating the electrons , which increases their kinetic energy . now they have , on average , more kinetic energy to give to the atoms than the atoms have to give to them . this means that there is a net transfer of energy from the electrons to the atoms . moving an electron in an electric field changes its potential energy , and this is where the energy for the heating ultimately comes from .
i hope i understand this question properly . the text appears to focus on time loops , or closed timelike curves ( ctcs ) , while the title of the question concerns the creation of a universe from nothing . the holographic principle of black holes tells us the field theoretic information of strings on the event horizon is completely equivalent to field theoretic information in the spacetime one dimension larger outside . this physics is observed on a frame stationary with respect to the black hole . the question naturally arises ; what physics is accessed by the observer falling through the event horizon on an inertial frame ? this question is important for the black hole small enough to exhibit fluctuations comparable to its scale . a sufficiently small quantum black hole will be composed of strings in a superposition of interior and exterior configurations or states . the motion of a string onto a black hole approximates the dynamics of a string in a rindler wedge . the rindler wedge is defined by the frame of an accelerated observer , which is equivalent to the frame of a stationary observer near a black hole event horizon . the observer witnesses the final emission of radiation by the string just above the event horizon , where upon the string becomes frozen eternally on the particle horizon . of course in the rindler wedge case the string proceeds onwards on its geodesic or string world sheet with no apparent change due to this observed state of affairs . this is approximated as well with the black hole , where the string passes through the event horizon unaffected so long as the radius of curvature is much smaller than the string length . this picture persists until the string approaches the center or singularity of the black hole . at this point the rindler wedge model departs from reality . the interior perspective or the physics of a string as measured by an observer falling with the string , is outside the domain of the holographic principle . once the string passes through the event horizon it evolves on a domain of causal support not included in the data set accessible to an observer on an accelerated frame stationary with respect to the black hole . the observer that falls through the event horizon observes the further evolution of the string beyond the frozen state the stationary observer finds as its final state . further , the string evolves into a different state as it approaches the singularity . there the string will begin to experience a rapidly growing weyl curvature . the stationary observer measures transverse modes of the string on the black hole horizon , while longitudinal coordinates are compressed to near the planck length . the observer falling in with the string will witness the string distended by the growing weyl curvature in the direction of motion . consequently , this observer witnesses the extension of longitudinal extension of the string . the frozen state of the string measured by the exterior observer is cancelled by hawking radiation which escapes later . the string is entangled with the black hole , and there is a superposition of configuration spaces for the string ; the exterior and interior configuration variables . this may put a new twist on the holographic result that events in spacetime do not have the a realism as understood classically . quantum mechanics removes a measure of reality with the existence of incompatible measurements of observables which exist in incommensurate commuting sets of operators . the invariant interval , or event , is left as something which has an ontology or “realism . ” however , this indicates that a realism to any event is not supportable on fundamental grounds . further , superposition of exterior and interior states of a black hole prevents a sharp distinction between pre-selected and post-selected states . this means that cosmic censorship , chronology protection are aspects of “classical reality , ” where underneath the ontology of ordered events or their invariant meaning is lost . the creation of the universe from nothing might involves something called the biverse . the de sitter spacetime is a solution of the scale factor $a ( t ) ~=~\sqrt{\frac{3}{\lambda}}cosh ( t\sqrt{\frac{\lambda}{3}} ) $ , which has this strange backwards part of the hyperboloid . hawking and others have considered the idea of a universe connected at this minimal scale factor as two universes which are time reversed . it is possible that this is an elementary model for how a “blob” of vacuum energy in a spacetime cosmology quantum tunnels across a potential barrier to form a nascent spacetime cosmology . the blog of vacuum energy is annihilated at one side of the potential by its “opposite” ( other half of the biverse ) , where the “opposite” is annihilated at the other side of the potential by the occurrence of the vacuum blob . this is similar to the tunneling of an electron across a barrier , where we can think of it as annihilated by a positron traveling backwards in time , which in turn is connected to an electron on the other side of the potential barrier . at the end of the day one might question what are the role of the hawking-penrose energy conditions , such as the averaged weak energy condition $t^{00}~\ge~0$ . it makes sense that the gravitational states are the hartle-hawking states which are degenerate and zero . so ultimately the physical states of the universe globally are zero , or in other words the net total of everything is zero .
here we go : lets considere the lagrangian density of our free membrane . for convenience , we will use a cartesian set of coordinates , so that : $$\mathcal{l}=\frac{\rho}{2}\dot{h}-\frac{\kappa}{2}\left [ {\partial_x}^2 h+{\partial_y}^2 h\right ] ^2$$ apparently , this lagrangian depends on a quite large number of variables : $$\mathcal{l} ( x , y , t , h , \dot{h} , \partial^2_x h , \partial^2_y h ) $$ but still nothing to be afraid of ! : ) one can show ( see this ) that for this type of lagrangian density , euler-lagrange equation is : $$\sum_{\alpha\beta}\frac{\partial}{\partial\alpha}\left ( \frac{\partial\mathcal{l}}{\partial ( \partial_\alpha h ) }\right ) -\frac{\partial^2}{\partial\alpha\partial\beta}\left ( \frac{\partial\mathcal{l}}{\partial ( \partial^2_{\alpha\beta} h ) }\right ) =\frac{\partial\mathcal{l}}{\partial h}$$ where $ ( \alpha , \beta ) \in\{x , y , t\}$ computing this equation for our membrane lagrangian , it follows the differential equation : $$\rho\ddot{h}+\kappa\left ( \partial^4_x h + \partial^4_y h\right ) =0$$ then , simply by taking the ( space-time ) fourier transform $\tilde{h}$ of $h$ so that : $$h ( \vec{r} , t ) =\sum_\vec{q}\int\frac{d\omega}{2\pi} e^{i ( \vec{q}\cdot\vec{r}-\omega t ) }\tilde{h} ( \vec{q} , \omega ) $$ where $\vec{r}= ( x , y ) $ , and replacing it in the previous equation , you obtain the dispersion relation : $$\omega^2=\frac{\kappa q^4}{\rho}$$ for a quantum treatment , it feels like you will have to compute the associated hamiltonian $\mathcal{h}$ of $\mathcal{l}$ and then quantized it , using the correspondence $\{\cdot\}\rightarrow\frac{1}{i\hbar}\left [ \cdot\right ] $ between poisson brackets and commutators .
when we say that the spin of a silver atom ( in a magnetic field ) is $+1/2$ or $-1/2$ we mean its component in the direction of the magnetic field ( referred to as $s_z$ ) is $+1/2$ or $-1/2$ . the magnitude of the spin is the same in both cases , it is just the direction that is different . hund 's rule just tells you what the magnitude of the total spin is , and does not say anything about the direction that spin is pointing . for example with two unpaired electrons hund 's rule tells us the total spin will be $s = 1$ . however put that atom in a magnetic field and you can have $s_z = 1$ , $0$ or $-1$ . the two blobs in the stern-gerlach experiment correspond to the electrons with $s_z = +1/2$ and $-1/2$ , but all the electrons have the same total spin $s = 1/2$ . the two unpaired electron system with $s = 1$ would give us three blobs corresponding to $s_z = 1$ , $0$ and $-1$ .
given a vector $$\vec{v} = \begin{pmatrix} x \\ y \\ z \end{pmatrix} $$ and a general rotation axis $$ \vec{k} = \begin{pmatrix} k_x \\ k_y \\ k_z \end{pmatrix} $$ then the resulting vector after a rotation by $\theta$ is $$\vec{u} = \vec{v}\cos\theta + \left ( \vec{k} \times \vec{v} \right ) \sin \theta + \vec{k} \left ( \vec{k} \cdot \vec{v} \right ) ( 1-\cos \theta ) $$ where $\times$ is the vector cross product and $\cdot$ the dot product . if you are looking for the angle to rotate , then look up angle between two vectors . the the rotation axis is defined by the cross product of the original vector and the target vector . i am not sure how familiar you are with C# but there is the code to do what i think you are asking .
when i use the linear equation , i have always written it as $r_{2}=r_{1} ( 1+\alpha \ \delta t ) $ . for this problem , $r_{1}=100\omega$ and $r_{2}=200\omega$ . also substituting the value for $\alpha$ will give you the change in temperature from 100 degrees . so remember to add the 100 to the $\delta t$ for the complete answer . regarding why you got different answers , it is because linear approximations are linearized about a point and do not always hold . the question indicates that you are to linearize about $100^{\circ}$ . keep in mind that a slope of $0.005\ /^{\circ}c$ is 50% per $100\ ^{\circ}c$ , which equates to $50\ \omega\ per\ 100\ ^{\circ}c$ . if you start at $100\omega$ at $100^{\circ}$ , if you get $100^{\circ}$ colder you will lose $50\omega$ . this gives a different result at 0 than what you had .
whether the conformal symmetry is local or global depends on the theory ! more precisely , the symmetry that may be local is not really conformal symmetry but ${\rm diff}\times {\rm weyl}$ . for example , in all the cfts we use in the ads/cft correspondence , for example the famous ${\mathcal n}=4$ gauge theory in $d=4$ , the conformal symmetry is global – and , correspondingly , it is a physical symmetry with nonzero values of generators . this is related to the fact that the cft side of the holographic duality is a non-gravitational theory so it avoids all local symmetries related to spacetime geometry . the previous paragraph holds even if the dimension of the cft world volume is $d=2$ . in $d=2$ , it may happen that the global conformal symmetry is extended to the infinite-dimensional local symmetry where $\omega ( x ) $ depends on the location . however , such an enhancement looks " automatic " only classically . quantum mechanically , a nonzero central charge $c\neq 0$ prevents one from defining the general local conformal transformations . in all the cfts from ads/cft , we have $c\geq 0$ . such a nonzero $c$ leads to the " conformal anomaly " ( proportional to the world sheet ricci scalar and $c$ ) . on the contrary , the world sheet $d=2$ cft theories used to describe perturbative string theory always have a local diffeomorphism and local weyl symmetry . this is needed to decouple all the unphysical components of the world sheet metric tensor ; and a necessary condition is the incorporation of the conformal ( and other ) ghosts so that in the critical dimension , we have the necessary $c=0$ . we say that the world sheet cft is " coupled to gravity " as we add the world sheet metric tensor , the diff symmetry , and the weyl symmetry . the weyl symmetry is the symmetry under a general scaling of the world sheet metric by $\omega ( x ) $ that depends on the location on the world sheet . one may gauge-fix this local weyl symmetry along with the 2-dimensional diffeomorphism symmetry , e.g. by demanding the $\delta_{ij}$ form of the metric tensor . this gauge-fixing still preserves some residual symmetry , a subgroup of the originally infinite-dimensional " diff times weyl " symmetry . this residual symmetry is nothing else than the infinite-dimensional conformal symmetry generated by $l_n$ and $\tilde l_n$ . because its being infinite-dimensional , we may call it a local conformal symmetry but it is really just a residual symmetry from " diff times weyl " . the global $sl ( 2 , c ) \sim so ( 3,1 ) $ global subgroup is the mobius group generated by $l_{0 , \pm 1}$ and those with tildes , too . as far as i know , this local conformal symmetry is a special case of some $d=2$ theories . in higher dimensions , the weyl and diff are not enough to kill all the components of the metric tensor and the " partially killed " theories with a dynamical metric are still inconsistent as the usual naively quantized versions of general relativity . in all the cases above and others , it is true that the local symmetries – where the parameter $\omega ( x ) $ is allowed to depend on time and space coordinates ( if the latter exist ) – are gauge symmetries ( in the sense that the generators are obliged to annihilate physical states ) while the global symmetries are always " physical " in your sense of the charge 's being nonzero . these equivalences follow from some easy logical argument . when you have infinitely many generators of the ( space ) time-dependent symmetry transformations , it follows that all the quanta associated with these generators exactly decouple – have vanishing interactions – with the gauge-invariant degrees of freedom . so we always study the physical part of the theory only , and it is the theory composed of the gauge symmetry 's singlets . greetings to david .
it depends what you call " one state " . with only one species , the fock state basis is of the form $|n_1 , n_2 , n_3\rangle$ which gives the number of particle on the sites $1$ , $2$ , $3$ . this is one state of the system ( even though it is not a eigenstate ) . in the case with two species , one can trivially generalize the notation , with a basis $|n^a_1 , n^b_1 , n^a_2 , n^b_2 , n^a_3 , n^b_3\rangle$ . so your notation $ [ 1,0,1 ] $ is ambiguous if you do not explain what the number are , even in with one species ( it could be $ [ n_2 , n_1 , n_3 ] $ , etc . ) . once you have chosen a convention , there is no ambiguity left .
analysis for jesus 's molecule usage : our breathing rate changes a lot , but on average its about 1 breath every five seconds , or 12 breaths a minute , or 720 breaths an hour , or 17280 breaths a day or 6,307,200 breaths a year , and if we live for 32 years that gives us 201,830,400 breaths in his lifetime . how many atoms ? multiply 2.02e8 total breaths x 1.61e23 molecules per breath to get a total of 3.25e31 total molecules . . . . that means for jesus , there were 32,500,000,000,000,000,000,000,000,000,000 molecules ( 325 decillion ) that came into contact with his lungs during his lifetime . i did not see anything wrong with the author 's approach , so say 6,307,200 * 1.61e23 molecules/year ( based on 6 liters of air/breath not oxygen ) . since you want $o_2$ only then scale that down . at stp ( standard temperature and pressure : 1 atmosphere of pressure , 0c ) , one mole of any gas will occupy 22.4 liters of space . one mole of any substance contains 6.02 x 1023 particles . air is about 21% oxygen making 1 liter of air to be about 0.21 liters of oxygen . to find out how many moles that represents=0.21 / 22.4 . then use avogadro 's number 6.02 x 10^23 atoms/mole . 1 l = 5.64e21 atoms or 2.82e21 molecules/l . 6l/breath => 1.69e22 molecules/breath 6,307,200 breaths a year => 1.07e29 molecules of $o_2$/year . how long are you going to live * 1.07e29 = $o_2$ passing into your lungs . the only two caveats to this answer : some of these molecules will not be unique and 6l represents gas going into the lungs , not what is utilized . say you live 80 years , that is ~8.5e30 molecules of $o_2$ .
yes . special relativity satisfies your question somehow . probably , there are many discussions in se based on the topic . but , there are several cons i would like to correct in your question . . . if movement occurs faster , time freezes . more the movement , more the time freezes . according to einstein - length , mass , time and space are interdependent variables . these motions depend on the speed of light $c$ in vacuum which is the only constant here and is what the second postulate says . ok , let 's take your " time " part . . . time does not freeze out anytime . it may freeze if you move at $c$ which is impossible ( practically and theoretically ) to achieve in vacuum . and , you say , " more the movement , more it freezes " . it is not a state of matter . and , no laws have ever proved that " time depends upon temperature " . but , time really slows down when you approach a good velocity comparable to $c$ . time dilation could be predicted indeed ( taking lorentz factor into account ) using $$t=\frac{t_0}{\sqrt{1-\frac{v^2}{c^2}}}$$ where $t_0$ is the time measured by an observer at its rest frame . $t$ is the apparent time measured which is always less than $t_0$ and it reduces with increasing velocity . ( freezes more - in your words ) how come then theory of relativity claims that " moving faster may transport you ahead in time " ? short answer - no , there is no such thing in relativity . but , its the postulates that made the physicists think about the plausibility of time travel . these guys thought it in a different way . if there is a possibility for objects to travel faster than light ( assuming hypothetical particles called tachyons to study the behaviour ) , maybe it would forward time into future ( like that ) . . . but , i think it is unsuccessful . . !
in fluids , conservation of mass and momentum are still applicable , only not that easily , because there are internal forces in a fluid , i.e. caused by viscosity . in liquids , conservation of mass is described by the continuity equation , while the conservation of momentum is described by the navier-stokes equations of course , you can incorporate the effect the solid has in these equation , but this easier said than done , and large depends on the properties of the solid ,
the notation $t_{n1}$ may come from the fact that the mean is also referred to as the " first moment " , this is what the number $1$ as a lower label might stand for . for reference , see the wikipedia article on moments in mathematics . regarding the derivative of the energy : you are supposed to formally take the derivative of some expression with respect to some variable , even if it is a discrete one .
the form of your book 's value $\frac{\hbar^2}{2ma^2}$ makes it clear that that is a fully classical ( non-relativistic ) limit ( $\frac{p^2}{2m}$ , right ? ) , while yours is a fully relativistic one ( after all $e , p \gg m_e$ ) . the argument about the nuclear confinement is simply that both the gravitational and elctromagnetic potentials on the electron due to the nucleus are many orders of magnitude smaller than 40 mev ( the weak nuclear force too , but you may not know how to compute this ) , and the electron is not affected by the strong nuclear force .
you may just not bother to use a test function , here . this problem is so easy you can work it all just using the properties of the commutator . $$ [ xp_y , x ] =x [ p_y , x ] + [ x , x ] p_y$$ now $ [ p_y , x ] $ vanishes because of the fundamental commutation relation between $p_i$ and $x_i$ which is $$ [ p_i , x_j ] = -i\hbar \delta_{ij}$$ on the other hand $ [ x , x ] =0$ because anything commmutes with itself .
a quick google search leads one immediately to the wikipedia page on this particular theorem . the first paragraph of this page states : the bohr–van leeuwen theorem is a theorem in the field of statistical mechanics . the theorem states that when statistical mechanics and classical mechanics are applied consistently , the thermal average of the magnetization is always zero . this makes magnetism in solids solely a quantum mechanical effect and means that classical physics cannot account for diamagnetism , paramagnetism or ferromagnetism . the theorem applies to any form of magnetism . continuing to read the same wikipedia page , which supplies an intuitive as well as a more formal proof , one sees that the argument formally boils down to showing that the thermal average of the magnetic moment $\mu$ is zero : $$\langle \mu \rangle=0$$ this is done without any assumptions on the origin of the magnetic moment $\mu$ . i will now reproduce another proof , which is found in several textbooks . consider an $n$-particle system with only particles with charge $e$ and mass $m$ ( the proof is easily generalized ) . we define the ( thermal average of the ) magnetization as $$\langle \mu \rangle=\left\langle -\frac{\partial f}{\partial b}\right\rangle$$ where $f=-t\ln z$ is the free energy , and $z$ is the partition function . the classical partition function is $$z=\int d\vec{p}_1\dots d\vec{p}_n\int d\vec{r}_1 \dots d\vec{r}_n\ e^{-\beta h} $$ where $h$ is the classical hamiltionian . in the presence of a magnetic field , we have $$h=\frac{1}{2m}\sum_{i=1}^{n}\biggl ( \vec{p}_i-\frac{e}{c}\vec{a}_i\biggr ) ^2+ev ( \vec{r}_1 , \dots\vec{r}_n ) $$ by making the substitution $\vec{p}_i\to \vec{p}_i-\frac{e}{c}\vec{a}_i$ in each integral over the momenta we can completely eliminate the dependence of $z$ on $\vec{a}_i$ and therefore on the magnetic field $\vec{b}=\vec{\nabla}\times\vec{a}$ . therefore , $f$ does not depend on $b$ either , and $$\langle \mu \rangle= \left\langle -\frac{\partial f}{\partial b}\right\rangle=0$$ in conclusion , the bohr-van leeuwen theorem shows that magnetism cannot be accounted for classically , independent of the origin of the magnetization . when applying a magnetic field and allowing a solid to reach thermal equilibrium , there can be no net magnetization ( classically ) . in particular , it also rules out classical ferromagnetism .
the most authoritative solar system ephemeris system is the jpl horizons system . there is a web interface that will let you perform computations and extract solar system data from the jpl development ephemerides , the international standard .
$2\pi\omega \cdot a = v_{max}$ so try $v_{max}=\lambda \omega=a \cdot 2\pi\omega$ . solve for what you want , $a=\frac{\lambda}{2\pi}$ where $\lambda$ is different for each wave , as i enumerated in the comments , $\lambda_a=2$m and $\lambda_b=4$m . i hope i answered your question right from what you are telling me . i will stress that in order to get a solid response , post your full question in clear terms and make sure you post everything you know and tried . try to focus it down to conceptual questions . we want to help , but we also do not want to explicitly do your homework . hope this helps , cheers .
first , $\vec{r}^\prime$ is a vector that goes from the origin to the source of charge . if the source is a volumetric distribution , one must sum all contributions of charge , that is why one integrates over all the volume , say $\mathcal{v}$ ; the ( correct ) expression for the potential should be $$v ( \vec{r} ) = \frac{1}{4 \pi \epsilon _{0}} \int_\mathcal{v} \frac{\rho ( \vec{r}^\prime ) }{ℛ} d\mathcal{v}^\prime$$ so that all dependence of $v$ remains on $\vec{r}$ . then , $r^\prime$ is just the magnitude $|\vec{r}^\prime|$ , being the distance from the origin to the source of charge . second , usually , the series expansion of a function $f ( x ) $ about some point $x_0$ is useful because if you want to know the value of $f$ near $x_0$ , you may just take some few terms of the expansion ; it is as seeing the plot of $f$ with a magnifying glass . you should remember this from your first calculus courses , it is done a lot in physics . here the expansion about $\epsilon=0$ will be useful since $\epsilon\to0$ implies $r\to\infty$ ( just really big , if you will ) . the ( correct ) expression $$v ( \vec{r} ) = \frac{1}{4 \pi \epsilon _{0}} \sum ^{\infty}_{n=0}\frac{1}{r^{n+1}} \int ( r' ) ^n\ , p_{n} ( \cos \theta^\prime ) \ , \rho ( \vec{r}' ) \ , d\mathcal{v}'$$ is just another way of writing the series expansion in terms of $r$ , $r^\prime$ and $\theta^\prime$ , where $p_n$ are the legendre polynomials ( griffiths defines them there , ain't he ? ) . this expression is useful , as it means , explicitly , that $$v ( \vec{r} ) =\frac{1}{4\pi\epsilon_0}\left [ \frac{1}{r}\int\rho ( \vec{r}' ) \ , d\mathcal{v}'+\frac{1}{r^2}\int{r'}\cos\theta'\ , \rho ( \vec{r}' ) \ , d\mathcal{v}'+\frac{1}{r^3}\left ( \cdots\right ) +\ldots\right ] $$ so that if you want to evaluate the potential for points far from the source ( big $r$ ) , then you may just neglect higher order terms in $r$ and just take the $1/r$ ( monopole ) term ; and so on if you are considering a better approximation , you may take the $1/r^2$ ( dipole ) term , etc . . . that is the real usefulness of the series expansion ; in a lot of situations evaluating $v ( \vec{r} ) = \frac{1}{4 \pi \epsilon _{0}} \int \frac{\rho ( \vec{r}' ) }{ℛ} d\mathcal{v}'$ will get really ugly , and then , mostly , is when the multipole approximation will be useful .
if you search the iter site , iter being the international prototype fusion reactor which will demonstrated the possibility of getting megawat useful energy from fusion , one sees that their main aim is to demonstrate this feasibility : the main carrier of energy out of the plasma is the neutron , and methods to efficiently use this energy have not been developed yet , but wait for the commercial prototype . the helium nucleus carries an electric charge which will respond to the magnetic fields of the tokamak and remain confined within the plasma . however , some 80 percent of the energy produced is carried away from the plasma by the neutron which has no electrical charge and is therefore unaffected by magnetic fields . the neutrons will be absorbed by the surrounding walls of the tokamak , transferring their energy to the walls as heat . in iter , this heat will be dispersed through cooling towers . in the subsequent fusion plant prototype demo and in future industrial fusion installations , the heat will be used to produce steam and—by way of turbines and alternators—electricity . they have developed methods for cooling the system and dissipating the energy to the environment .
the energy obviously grows with the height $z$ ( assuming that positive means up and negative means down ) so it is $$ e = \frac{p^2}{2m} + mgz $$ this sign error spread everywhere .
it is important to understand the context in which statements like " there must be a singularity in a black hole " are made . this context is provided by the model used to derive the results . in this case , it was classical ( meaning " non quantum" ) general relativity theory that was used to predict the existence of singularities in spacetime . hawking and penrose proved that , under certain reasonable assumptions , there would be curves in spacetime that represented the paths of bodies freely falling under gravity that just " came to an end " . for these curves , spacetime behaved like it had a boundary or an " edge " . this was the singularity the theory predicted . the results were proved rigorously mathematically , using certain properties of differential equations and topology . now in this framework , spacetime is assumed to be smooth - it is a manifold - it does not have any granularity or minimum length . as soon as you start to include the possibilities of granular spacetime , you have moved outside the framework for which the original hawking penrose theorems apply , and you have to come up with new proofs for or against the existence of singularities .
i can not make this a comment since i do not have enough reputation . the metal box itself can absorb some of the heat ( by conduction ) and then give out energy in the form of electromagnetic radiation . if you want to do some real physics with such a system , you could idealize the box as a black body and continue .
the " mexican hat potential " ( although now more politically correctly called the " champagne bottle potential " after the punt at the base ) is the potential energy curve for the vacuum expectation value ( vev ) of the higgs field . think of the blue dot as being " the vacuum " , and the radial direction as turning up the strength of a background field that permeates all of space uniformly . when the radius is zero ( at the top of the hill ) , there is no background field , but the vacuum is not in its ground state ( lowest energy ) . therefore it is unstable and quantum fluctuations will cause it to jiggle a bit in one direction and roll to the bottom , so that the higgs field acquires a vev . this is called the decay of the vacuum , and happened a very long time ago ( it is possible that we are still in a metastable state , and the vacuum could decay to the true ground state ) . at the minimum , the whole universe is filled with a uniform background higgs field of " strength " 246 gev . note that these are not higgs particles , just a background field that all other particles must move through . the interaction coupling with the higgs field is what gives particles mass ; just like a bullet through jelly being slowed down , a massive top quark interacts strongly with the field and is " slowed down " ( made more massive ) . as always , quantum fluctuations ( embodied by the uncertainty principle ) will continue to jiggle the vacuum state back and forth along a radial line , and these oscillations give rise to higgs particles ( the " steepness " of the curve is what gives the higgs its particular mass , 126 gev ) . notice that there can also be rolling along the circle at the bottom for " free " ( it costs no energy ) . these motions are associated with the massless goldstone bosons that are " eaten " by the weak vector bosons to give them a mass ( the massiveness of the $w^{\pm}$ and $z$ bosons is why many nuclear decays take a while , and one reason why nuclear forces have such short range ) .
there are various possible reasons why the normal force of the floor on the box might not equal the weight of the box . for example : there could be another box on top of the box being asked about . the floor could be the floor of an elevator that is accelerating ( so the net force on the box would not be zero ) . the " floor " could be a board , with a c-clamp pressing the box to the board . the " floor " could be the bottom of a tank of water ( so there would be a buoyant force on the box ) . and of course you could put these together in combinations .
yes . provided you are only interested in the direction of the acceleration , and not it is magnitude . and further assuming your time samples are equally spaced , you can take the second derivative of the path and this will be proportional to the acceleration . a decent method in practice would be to use a second order central finite difference scheme wherein you say that : $$ a_x ( t ) = x ( t-1 ) - 2x ( t ) + x ( t+1 ) $$ and $$ a_y ( t ) = y ( t-1 ) - 2y ( t ) + y ( t+1 ) $$ this will give you decent estimates for the cartesian components of acceleration at every time , caveat to an overall scaling in magnitude that you will not know without knowing the actual timing , but the direction should be alright .
the effective gravity inside the iss is very close to zero , because the station is in free fall . the effective gravity is a combination of gravity and acceleration . if you are standing on the surface of the earth , you feel gravity ( 1g , 9.8 m/s 2 ) because you are not in free fall . your feet press down against the ground , and the ground presses up against your feet . inside the iss , there is a downward gravitational pull of about 0.89g , but the station itself is simultaneously accelerating downward at 0.89g -- because of the gravitational pull . everyone and everything inside the station experiences the same gravity and acceleration , and the sum is close to zero . imagine taking the iss and putting it a mile above the earth 's surface . it would experience about the same 1.0g gravity you have standing on the surface , but in addition the station would accelerate downward at 1.0g . again , you will have free fall inside the station , since everything inside it experiences the same gravity and acceleration ( at least until it hits the ground ) . the big difference , of course , is that the iss never hits the ground . its horizontal speed means that by the time it is fallen , say , 1 meter , the ground is 1 meter farther down , because the earth 's surface is curved . in effect , the station is perpetually falling , but never getting any closer to the ground . that is what an orbit is . ( as douglas adams said , the secret of flying is to throw yourself at the ground and miss . ) but it is not quite that simple . there is still a little bit of atmosphere even at the height at which the iss orbits , and that causes some drag . every now and then they have to re-boost the station , using rockets . during a re-boost , the station is not in free fall ; instead . the result is , in effect , a very small " gravitational " pull inside the station -- which you can see in this fascinating video .
you are very nearly there . you are correct to say that $power = fv$ , and that the velocity at the moment the train reaches the top of the slope is given by $v_{top} = 350/ ( 6 + 150gsin2 ) $ so the force at the top of the slope is $f_{top} = 1000 ( 6 + 150gsin2 ) $ . but the acceleration is the net force divided by the mass , and the net force is $f_{top}$ minus the 6kn frictional force i.e. $$ a = \frac{f_{top} - 6000}{m} = \frac {1000 ( 6 + 150gsin2 ) - 6000}{150000} = g sin2$$ which using $g = 9.81m/sec^2$ i get as 0.342 .
i thought that since i brought this up and i did not get any good explanations , i would answer it myself . plugging in some numbers helps . i used this for light speed = 299,792.458 km/sec . hubble constant = 72 km/sec/megaparsec . the hubble constant is necessarily an approximation because nobody knows exactly what it is and they also do not know if it is a constant through time . the hubble constant is used to calculate the speed of distant galaxies due to the expansion of the universe . so based on some simple calculations using the above numbers , i come up with this : if you go out 13,573,936,292 light years , the universe is expanding at 0.99999999994 of the speed of light . i might point out that this is about 203 feet per hour slower than the speed of light so it is pretty close . using the lorentz transform ( relativity ) we can figure out the effect on a galactic mass moving at that speed away from us . suppose we observe a galaxy about the size of the milky way or andromeda moving away from us at this speed . the rest mass of the milky way or andromeda is roughly 700 billion solar masses . give or take . applying the lorentz transform to this observation , the rest mass of the distant receding galaxy would be about 7,493,550 solar masses . so even at speeds this close to the speed of light , this is still a large mass . it is not a tiny particle as i thought it might be . therefore we do not even have to use the hand waving argument about the expansion of the universe does not really count in relativistic calculations . i guess the real theoretical issue and question would be : what happens at the singularity ? that is right at the outer edge of the expansion , objects are traveling exactly at the speed of light , and here relativity breaks down . but nobody knows the answer to that . the interesting thing about plugging in the numbers is that when you are really close to light speed , there are no contradictions . and my calculations are really close because 203 feet per hour is a lot slower than i walk . i could crawl faster than that .
this answer contains some additional resources that may be useful . please note that answers which simply list resources but provide no details are strongly discouraged by the site 's policy on resource recommendation questions . this answer is left here to contain additional links that do not yet have commentary . b . falkenburg , particle metaphysics , springer , 2007 . also by t.y. cao ( btw , nitincr 's suggestion is a compilation of articles by various authors from a number of points of view ) , conceptual developments of 20th century field theories , cambridge up , 1997 . s.y. auyang , how is quantum field theory possible , oxford up , 1995 . try the stanford encyclopedia of philosophy entry on qft , which includes ( not too ) many citations to books and papers . for conceptual issues on physics generally , this is one place to look . look at tom banks ' book modern quantum field theory : a concise introduction it is not a book , but it might be helpful : the conceptual basis of quantum field theory by gerard ' t hooft . the file is on his web page . there is a book , edited by t.y. cao , a rather notable philosopher of science at boston university , that might be of interest . the title is " conceptual foundations of quantum field theory " . a . zee 's " quantum field theory in a nutshell " is also a very good reference . the series by e . zeidler are really worth reading through . credit : peter morgan , and deleted answers by nitincr , curious george and physicists .
the tristimulus values for ciexyz can be obtained by integrating the spectral curve with three distinct weight curves . from xyz there are documented formulas to convert to lab , hsl , csv , . . . http://www.zeiss.com/c12567bb00549f37/contents-frame/80bd2fe43b50aa3ec125782c00597389 http://en.wikipedia.org/wiki/cie_1931_color_space http://en.wikipedia.org/wiki/lab_color_space#cie_xyz_to_cie_l.2aa.2ab.2a_.28cielab.29_and_cielab_to_cie_xyz_conversions
yes , by examining the statistical distribution of distances between molecules and angles separating two nearby about a third molecule . in general , correlations of 2nd and higher order of the positions of molecules relative to each other . for a gas , there are few molecules close together , but some due to molecules colliding and almost colliding . at far distances , it'll be more or less uniform . there would be nothing of interest in angular correlations . for a liquid , there would be none closer than about the size of a molecule , but at that distance many . there would be mushy peaks in the distribution of distances , and just uniform mush beyond a few molecule-sizes away . there would be strong angular correlations as nearby molecules try to pack tightly , with fleeting gatherings of several molecules in an approximate crystal , but always jiggling making the angular distribution mushy . for a crystalline solid , every molecule near or far is at a precise distance , and at precise angles with respect to any reference directions . the distance and angular distributions would look like bunches of dirac functions , slightly smoothed out due to thermal motion , phonons , impurites and so on . radial distributions explained by professors at oxford , with plots comparison of radial distribution functions , with plots
fluid dynamics problems such as this are generally best approached by control volume analysis . consideration of conservation of mass , momentum , energy , and sometimes angular momentum for an isolated control volume system generally provide an engineering answer . to figure out the force exerted on the pipe by the fluid it would seem appealing to isolate the fluid with a control volume . however , since the pipe is stationary , it is entirely equivalent to find the force acting on the pipe externally to keep it in place . assuming steady flow within the pipe , conservation of mass for an isolated pipe-fluid control volume gives $$\dot{m}_{in}a_{in}=\dot{m}_{out}a_{out}$$ with $\dot{m}_{in}=\overline{\rho_{in} \vec{u}_{in}\cdot\vec{n}_{in}}$ and $\dot{m}_{out} = \overline{\rho_{out} \vec{u}_{out}\cdot\vec{n}_{out}}$ where $\vec{n}$ are surface unit normal vectors and $\vec{u}$ is the surface velocity vector and $a$ is the pipe cross section area . conservation of momentum in vector form yields $$\sigma f = \vec{f}_{pipe}+\vec{f}_{gravity}+\vec{f}_{other}+\overline{p}_{in}a_{in}\vec{n}_{in}+\overline{p}_{out}a_{out}\vec{n}_{out} = \dot{m}\overline{\vec{u}_{in}}a_{in}-\dot{m}\overline{\vec{u}_{out}}a_{out}$$ if you further assume that the only force acting on the pipe is to resist the change in momentum of the fluid and that the pressure drop from entrance to exit is negligible , then you will arrive at the approximation $$\vec{f}_{pipe}= \dot{m}\overline{\vec{u}_{in}}a_{in}-\dot{m}\overline{\vec{u}_{out}}a_{out}$$
circularly polarized light should create a circle on the oscilloscope , which is a type of lissajous curve . any polarization of light produces a lissajous curve with the restriction that the two frequencies of the x and y parameters are the same . so the only possibilities are a circular , elliptical , and linear polarization . if your oscilloscope does not show a circle for circularly polarized light , something has gone wrong in the experiment . edit : this does assume your laser is a simple harmonic wave , so that the electric field varies sinusoidally and we therefore have in-phase x and y components . also , see polarization .
this is a pretty imprecise answer , because i have not heard the term in a long time . i think ( someone please correct me if this is wrong ) a " myriotic " field is one in which there are an infinite number of quanta , so you do not have a well-defined notion of number operator or vacuum state .
no , maxwell did not invent surface or line integrals . see history of stokes ' theorem , which explains surface integrals were in use earlier .
developments as of summer 2014 the hottest kid on the organic-pv block is perovskites : in february 2012 , hardin , snaith and mcgehee published an article in nature photonics announcing " the renaissance of dye-sensitized solar cells " . the inventors of one implementation , oxford photovoltaics ltd ( a spinoff of the university of oxford ) described their new technology in science , in november 2012 , and have a patent application pending for their dye-sensitized solar cell . for more , see recent papers by henry j . snaith 's team . folk have got excited because of the rapid increases in efficiency since 2009 , combined with the very low cost and abundance of the raw material : as of summer 2014 , the record was $17.9\pm0.8\%$ , held by the korea research institute of chemical technology ( image source ) confirmed organic pv records under standard test conditions progress in photovoltaics : research and applications publishes a biannual review of pv efficiency records , under the name solar cell efficiency tables . the 2014 june review gave a record efficiency under standard test conditions for an organic pv cell of $10.7\pm0.3\%$ . for an organic pv mini-module , the record is $9.1\pm0.3\%$ . ( nb the higher perovskite efficiencies discussed above had not passed audited standard tests in time for the latest biannual review ) cells have higher efficiencies than sub-modules , which tend to have higher efficiencies than modules . a ( sub- or mini- ) module is made up of several cells , and power is aggregated from all of the cells . the most efficient cell in a ( sub-/mini- ) module will , by definition , have an efficiency equal to or higher than all other cells in that ( sub-/mini- ) module ; and not all the module surface area is covered in cells ; therefore record cell efficiency will always be higher than ( sub-mini/ ) module efficiency . here 's the recent history of organic pv cell record efficiencies ( % , standard test conditions ) : 2012-10 $10.7\pm0.3$ 2011-10 $10.0\pm0.3$ 2010-11 $8.3\pm0.3$ 2006-12 $~5.15\pm0.3$ 2006-03 $~3.0\pm0.1$ overall pv record the record efficiency for any pv cell is $44.4\pm2.6\%$ , for a ingap/gaas/ingaas inverted metamorphic cell ( 2013-04 ) , measured at 302 suns , am 1.5 , cell temp $25^{\circ}c$ .
actually , the rotation group $so ( 3 ) $ does act " on physics " , even in the presence of spin . the thing is that the wave function $\psi ( \vec x , \sigma ) $ is a redudant description of a physical state . a wave function with a different overall phase $c\psi ( \vec x , \sigma ) $ describes exactly the same physical state . after all , the only quantities of interest are only the expectation values of observables $$\langle x\rangle_\psi:=\frac{\langle\psi|x|\psi\rangle}{\langle \psi|\psi\rangle} . $$ and these are invariant under a rotation $r$ $$\langle x\rangle_{r\psi} = \langle x\rangle_{\psi} . $$ mathematically , we can say that the action of the rotation group on physical states is a projective representation , i.e. it acts on lines $\lbrace\lambda\psi ( \vec x , \sigma ) , \lambda\in\mathbb c\rbrace$ ( one-dimensional subspaces ) in a hilbert space , but not on the individual vectors . however , as you can read in the wikipedia page above , every projective representation of a lie group like $so ( 3 ) $ can usually be obtained from a linear representation of its universal covering group like $su ( 2 ) $ . ( linear representation just means that the group acts on individual vectors . ) to summarize , the rotation group $so ( 3 ) $ acts on ordinary quantum mechanics , too , but for practical calculations , it is useful to generalize it to $su ( 2 ) $ instead . there is even a bit hair splitting as to whether you consider wave functions as physically relevant quantities and add an additional symmetry ( "up to phase" ) , or whether you take the quotient " wave function up to phase " as physically relevant quantities and work in the quotient space . as for the second question , i think it is possible to classify all lie groups $g$ with a homomorphism $g \to so ( 3 ) $ via group cohomology , but i am not familiar enough with this topic to give an answer .
note that in the sum $$=\sum_{s_1=\pm 1} . . . \sum_{s_n=\pm 1}\langle s_2| t_{_{nn}}^{\dagger}|s_1\rangle\langle s_1| t_{_{nnn}}|s_3\rangle\langle s_3| t_{_{nn}}^{\dagger}| s_2\rangle\langle s_2| t_{_{nnn}}|s_4\rangle . . . \langle s_1| t_{_{nn}}^{\dagger}|s_n\rangle\langle s_n| t_{_{nnn}}|s_2\rangle$$ every pair $|s_i\rangle\langle s_i|$ occurs twice with some operator/matrix between them . so you cannot simply execute the sum over $s_i = \pm1$ for both occurrences separately . as pointed out in the paper you linked , one solution via transfer matrix is to group two neighbouring spins to one 4-state spin . then the problem can be reduced to nn interactions only .
newton 's cradle fights air resistance and restitution ( efficiency of rebound ) . you then want the highest density , hardest balls with the highest restitution . cobalt-sintered tungsten carbide is magnetic . polished hardened tool steel ball bearings are a good start . http://www.wired.com/wiredscience/2011/10/what-went-wrong-with-the-mythbusters-newton-cradle/ platinum plus gallium and indium alloys heat treat to a very hard and springy state , density around 19 g/cm^3 versus less than 8 g/cm^3 for tool steel ( steve kretchmer , niessing co . , eastern smelting ; platinum sk ( tm ) alloys ) . for more shallow wallets , tungsten steel , thoriated tungsten . 95.5% pt , 3.0% ga , 1.5% in ; 95.2% platinum , 4.8% ga , in , cu ; 1550-1650 c melt . 700 c for 30 minutes and slow cool to harden ( not reducing atmospheres ) . vickers hardness 318/rockwell a 76/rockwell c 32 . 125,000 psi tensile , 104,000 psi yield .
you have to take the mass from $l$ to $x$ , since that is the mass which has to be accelerated by the centripetal force due to the tension at position $x$ . the remaining mass of the rod ( before position $x$ ) does not add anything to the tension in $x$ .
as mentioned in the wikipedia page $4 \times \frac{4 \pi r^3}{3}$ is the excluded volume per particle , so you have to sum over all the particles and divide by the number of particles . while summing up you divide by 2 , because a pair of particles only contribute once to the excluded volume .
i think you are misunderstanding the article . i also think that the name ' flying saucer ' is a bit misleading . i think referring to it with the actual project name : ' low density supersonic decelerator ' ( ldsd ) fits better . the concept of this project is that a spacecraft uses a inflatable saucer-shaped balloon to increase its reference area ( the surface area used in the drag equation ) to increase its atmospheric drag during atmospheric entry . during nasa 's test flight they want to test whether this system would suffice to land big spacecrafts safely on to mars . to simulate the atmosphere of mars they lift the test craft with a helium balloon high up into the atmosphere of earth such that the density is similar to that of mars at its surface . they than use a rocket to get up to speeds comparable with a atmospheric entry at mars . this video also gives a good explanation of the ldsd , and this video give a little longer and more detailed explanation .
this is known as ground effect . not to be confused with flaring , which is a technique used by pilots to gain lift by increasing the angle of attack as airspeed decreases . technicality , you can flare an aircraft at any altitude . the higher the altitude , the faster the airspeed of which you can flare an aircraft before stalling due to air thinning as altitude increases .
$\dot{z} ( t ) = \frac{\partial}{\partial t}p ( t , s ) z ( s ) + \frac{\partial}{\partial t}\int_s^t d\tau p ( t , \tau ) b ( \tau ) $ $= a ( t ) p ( t , s ) z ( s ) + p ( t , t ) b ( t ) + \int_s^t a ( t ) p ( t , \tau ) b ( \tau ) $ $= a ( t ) p ( t , s ) z ( s ) +b ( t ) + a ( t ) \int_s^t p ( t , \tau ) b ( \tau ) $ $= a ( t ) [ p ( t , s ) z ( s ) + \int_s^t p ( t , \tau ) b ( \tau ) ] + b ( t ) $ $= a ( t ) z ( t ) + b ( t ) $
you are right to use the general form of the uncertainty principle , namely : $$ \delta h_1 \delta h_2\geq\frac{1}{2}|\langle [ h_1 , h_2 ] \rangle| . $$ however , note that in the right hand side you have the expectation value of the commutator , so even if $ [ h , x ] \neq 0$ it can still be that $\langle [ h , x ] \rangle = 0$ . if this is the case then you can simultaneously measure position and energy . for example , if you have a simple one-dimensional hamiltonian with a potential : $$ h = \frac{\hat{p}^2}{2 m} + v ( x ) , $$ then you can easily show that $$ [ h , x ] = -\frac{i \hbar}{m} \hat{p} . $$ now you just have to check whether your system happens to be in a quantum state for which the expectation value of the momentum vanishes , i.e. $\langle \hat{p} \rangle = 0$ .
1 ) photons are bosons and can exist with the same quantum numbers with an indefinite number of photons . 2 ) photons have zero mass but have energy $e=h\times \nu$ , increasing their number and frequency increases the energy per cubic centimeter . 3 ) photons move with the velocity of light , and trapping them presupposes reflectors of one kind or another . from 1 ) the answer is " no limit " from 2 ) and 3 ) the limit would come from the melting of the reflectors due to the high energy density . the number would be large and would depend on the frequencies present .
alas , i must answer my own question : i found a very explicit example online description of someone who created a thick-film transmission hologram of a convex mirror . she ( or he ) describes seeing her own face clearly , even if only in monochrome . so , if i accept this description at face value , it clearly is possible to create a realistic mirror using only wave-exclusion diffraction effects . cool ! also , i am amused ( or is it chagrined ? ) that this reminded me of the importance of reading long articles all the way to the end , even if you feel you already got the point . this description of an actual holographic mirror was hidden at the very end of the long posting on i mentioned in my question about how transmission holograms cannot form mirrors .
the candela is the si base unit of luminous intensity ; that is , power emitted by a light source in a particular direction , weighted by the luminosity function ( a standardized model of the sensitivity of the human eye to different wavelengths ( wikipedia ) therefore , 7500 cd ( 3x2500 cd ) is more visible than 6000 cd .
i have not read that book , but i did read feynman 's discussion of ( sounds like ) exactly the same thing . easy : tell the aliens how to build a telescope , then describe the configuration of some galaxies near them . ok ok , but suppose we rule that out : we can not see any objects in common . easy : send them circularly-polarized radio waves ( thanks @anonymous coward ) . ok ok , let 's say our radio waves must be linearly polarized . easy : tell them to look at almost any phenomenon related to the weak force , for example the beta-decay of cobalt-60 in a magnetic field . but then there is one more catch--what if the aliens are made of antimatter and they actually were watching the beta-decay of antimatter-cobalt-60 ? in feynman 's discussion ( if i recall correctly ) , that is where it ends : there is no way to be really sure that the aliens understand right and left correctly , because they may be made of antimatter . but since 1964 , when cp-violation was observed , we can even eliminate that possibility : we tell the aliens how to watch kaons decay ( for example ) and then the aliens can figure out whether they are made of ( what we call ) matter or antimatter , and therefore they can figure out which way is left and right without any more ambiguity . so i guess that aspect is a slight update from pre-1964 descriptions . watching atoms decay in a magnetic field is a pretty simple thing to do by the standards of particle-physics experiments . i do not know of any parity-violating experiments that are much simpler than that . it just has to involve the weak force .
am radio typically transmits at around 1 mhz , fm radio at about 90 mhz . measurements of the rf spectrum of lightning strikes show a falloff with frequency of about 20 db per decade in that frequency range , so with fm about 2 decades above am , you had expect am to have about 40db higher interference from a lightning strike . in addition to that , fm signals attenuate faster with range , so depending on your distance from the lightning strike the effective am/fm interference ratio could be even larger .
thermistor with this particular temperature behavior are commonly semiconductors . in a semi-conductor , there is an energy gap between the ( filled ) valence and the ( empty ) conduction band . at zero temperature , no charges are in the conduction band and the resistance should be infinite as the system behaves basically like an insulator . if you turn on the temperature , some electrons will start to occupy the conduction band and thus contribute to conduction , lowering the resistivity .
consider making the substitution $k = p/\hbar$ in the first expression , while simultaneously defining $$ \sqrt{\hbar} \ , a ( p ) = \phi ( k ) $$ then the first integral will become $$ \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \sqrt{\hbar}\ , a ( p ) e^{i\left ( \frac{p}{\hbar} x - \frac{\hbar}{2m}\frac{p^2}{\hbar^2}t\right ) } \frac{dp}{\hbar} $$ which is exactly the second integral . it is just a change of notation !
the statement " zero modes are sensitive to the center of $su ( 2 ) $" means that the zero modes , overall parameters parameterizing the space of solutions , do change when one acts on the solution by $g\in su ( 2 ) $ which is $g\in z ( su ( 2 ) ) $ , an element of the center of $su ( 2 ) $ . " be sensitive " is the same thing as " [ for the solutions labeled by the zero modes ] not to be invariant " . note that the center of $su ( 2 ) $ is the subgroup of all $g\in su ( 2 ) $ that commute with all $h\in su ( 2 ) $ , $gh=hg$ . for $su ( 2 ) $ , this subgroup ( center ) is a $z_2$ consisting of the unit matrix and the minus unit matrix . so what they say is that when you act with the minus unit matrix in $su ( 2 ) $ symmetry group on a solution , you get a physically inequivalent solution . every element of $su ( 2 ) $ maps a solution to another solution in such a way that the distances between the solutions are preserved , so the $su ( 2 ) $ is a group of isometries of the moduli space ( the space of solutions = the space parameterized by the zero modes ) . the sensitivity is that each element of $su ( 2 ) $ does a different transformation of the moduli space , so the action of the minus unit matrix is not equivalent to the action of the unit matrix ( identity element of the group ) .
i take it you mean the determinant with the straight bars . . . ? if so , the only way to compute it for general background gauge field is , as nervxxx mentioned in the comments , to expand the determinant in $a_\mu$ . if you are considering a specific background gauge field ( like a constant magnetic field ) you should look whether you can find the eigenvalues of your operator , i.e. solve \begin{equation} ( i d\ ! \ ! \ ! / - m ) \psi = \lambda \psi \ ; . \end{equation} the determinant will then be given as the product of all the eigenvalues . ( in order to regularize the expression , it may be useful to rewrite it as the exponential of the sum of the logarithm of all eigenvalues , but that depends on the specifics of the problem . )
the answer is indeed the superposition principle . you have to calculate both the field of the ball with radius r and the field of a ball with radius r , placed away from the center at a distance b . then you substract the field of the smaller ball from the larger one and you are done . this is equivalent to a superposition of the fields of two balls with charge of opposite sign .
on the topic of the actual consequences of qm , here is answers with a few things that cannot be explained without qm . that aside . . . there is a golden rule that one should recite before all theoretical physics studies : " it all adds up to normality " . - greg egan , quarantine the thing with quantum mechanics and " logic " is that humans are really bad at qm and really good at " logic " . in fact , " logic " is an actual area of study in psychology : naïve physics . naive physics ( forgive me not using umlauts ) is an all-right approximation of anthropically scaled physical phenomena : object permanence , an exclusion principle based on volume , absolute time , a primitive notion of gravity . . . this is what humans think with , every day , on an instinctive level . over the times , you see refinements of naive physical concepts in works of aristotle and newton . object permanence , volume exclusion , gravity , absolute time . then relativity comes along and throws absolute time out the window . believe me , people cried " logic is meaningless " when relativity was new too . then comes quantum mechanics : throw away volume exclusion and even to a degree the intuitive notion of object permanence . there is one other concept of naive physics which qm seemingly violates : looking is a free action . suddenly you chance things by " looking " . so people do indeed cry out " logic is broken . " but it is not , because logic has nothing to do with qm . logic , and indeed all of mathematics is about axioms and theorems and the steps of inference in between . qm 's apparent weirdness has no effect on me using the peano axioms to say : $$ \begin{array}{l l}\vdash and 0 = 0 \\ \vdash and \forall x , y : s ( x ) = s ( y ) \iff x = y \\ \vdash and \forall x : x + 0 = 0 + x = x \\ \vdash and \forall x , y : x + s ( y ) = s ( x ) + y \\ \vdash and 2 = s ( s ( 0 ) ) \\ \vdash and 5 = s ( s ( s ( s ( s ( 0 ) ) ) ) ) \\ \vdash and 2 + 2 = 2 + s ( s ( 0 ) ) = s ( 2 ) + s ( 0 ) = s ( s ( 2 ) ) + 0 = s ( s ( 2 ) ) \\ \vdash and 2 + 2 \not = 5 \\ and \iff s ( s ( s ( s ( 0 ) ) ) ) \not = s ( s ( s ( s ( s ( 0 ) ) ) ) ) \\ and \iff s ( s ( s ( 0 ) ) ) \not = s ( s ( s ( s ( 0 ) ) ) ) \\ and \iff s ( s ( 0 ) ) \not = s ( s ( s ( 0 ) ) ) \\ and \iff s ( 0 ) \not = s ( s ( 0 ) ) \\ and \iff 0 \not = s ( 0 ) \\ \end{array} $$ captial l logic is set in mathematical stone and to say that qm 's " mysteries " imply $2 + 2 = 5$ is juvenile , trivially untrue , and shows that the speaker has not really understood anything at all . it is like saying : boy am i bad at abstracting from my everyday life and interactions , that i cannot even sit down and actually learn qm . i am going to put that on a t-shirt !
notice that the factors of $r^2$ cancel , and $\hat r\cdot\hat r = 1$ so the integral expression you wrote down reduces to $$ \frac{q}{4\pi\epsilon_0}\int \sin\theta \ , d\theta \ , d\phi $$ the bounds of integration are $0&lt ; \theta&lt ; \pi$ and $0&lt ; \phi&lt ; 2\pi$ so we really need to compute $$ \int_0^\pi \sin\theta\ , d\theta\int_0^{2\pi}d\phi = 2 ( 2\pi ) = 4\pi $$ so the factors of $4\pi$ cancel and we are left with $q/\epsilon_0$ as desired .
i have never seen actual figures but , in general , articles i have seen about flight state that " most " lift is generated from the angle of attack and relatively little from the bernoulli effect . i suspect the exact figures are rather variable and probably depend on whether the plane is climbing , descending , banking , etc and will also vary from plane to plane . maybe this is why exact figures seem not to be quoted . the pressure difference between the top and bottom of the wing is quite real , though note that on the top of the wing it is not a vacuum as the pressure does not decrease that much . the lowered pressure above the wing will indeed tend to pull the skin off the wing , or more precisely the air within the wing that is at normal atmospheric pressure will try to push the skin off . once again i can not give you exact figures - i must admit i thought ballpark figures would be easy to calculate , but google has failed me . incidentally , there is a good nasa article on this subject at http://www.grc.nasa.gov/www/k-12/airplane/wrong1.html and it even includes a java applet for you to play with the details of the wing . a longer slightly more staid article is at http://www.free-online-private-pilot-ground-school.com/aerodynamics.html later : if an approximate answer would be ok then you could could use bernoulli 's equation as described in http://en.wikipedia.org/wiki/bernoulli%27s_equation#incompressible_flow_equation . although this really only applies to incompressible fluids , and air is obviously compressible , the article suggests it would be a reasonable approximation for low speeds . rewriting the equation to make it more useful for our purposes gives : $$p = \rho a - \rho \frac {v^2}{2} - gh$$ where $a$ is some constant and $h$ is the height . we do not know the constant , but let $p_{bot}$ be the pressure below the wing and $p_{top}$ be the pressure above the wing then we can take the difference between them i.e. the pressure drop between the bottom and top of the wing . if we assume the height is constant i.e. we can ignore the thickness of the wing we get : $$\delta p = p_{bot} - p_{top} = 0.5 \rho ( v_{top}^2 - v_{bot}^2 ) $$ i do not know what speed you plane flies at , but let 's guess at 30 m/s and let 's guess that there is a 10 m/s difference between the air speed at the top and bottom of the wing , so that is $v_{bot} = 30$ and $v_{top}$ = 40 . google gives the density of air at ground level as 1.225 kg/m3 . $$\delta p = 0.5 \times 1.225 \times ( 40^2 - 30^2 ) = 429 pa$$ 429 pa is 4.29 grams per square cm or 0.06 pounds per square inch , so it is completely insignificant .
as it turns out , you can use it for the vast majority of the time . the ideal gas law is considered " ideal " because it assumes interactions between the gas molecules only occur due to collisions and no long-range forces are present . if the fluid you are modelling is non-polar and does not have things like van der waals forces between molecules , then the ideal gas law is a great approximation . it is also considered ideal because it assumes all collisions are equal -- so when two molecules hit one another , they react in the same way . this is because it assumes all molecules are the same size . so if you have a gas that has molecules that are roughly the same size ( like air , since nitrogen and oxygen molecules are quite close ) , then the approximation is a good one . but if you are doing some complex combustion processes where you have heavy organic fuels burning and releasing things like hydrogen trace species , then there is a wide range of molecule sizes and the model does not work so well anymore . and of course , whether you choose the thermally or calorically perfect version of the ideal gas law also depends on your conditions . calorically perfect works just fine when the change in temperatures in the fluid is small ( note , the temperature does not have to be small , just the variation in temperature -- you can choose the heat capacities to be accurate at any temperature you want , they are just fixed ) . thermally perfect is good over the range of temperatures that one has valid data for the heat capacities . so to summarize -- you can use it whenever : molecular interactions are due to collisions alone ( no long range forces ) molecules are close in size if the gas has a wide range of temperatures , the heat capacities need to vary with temperature ; otherwise , careful selection of constant values will work of course , it is quite common to use it anyway when the above assumptions are not all that great . many simulations of combustion use it despite the wide range in molecular sizes . it all depends on how accurate you want to be for the cost and how much data is actually available to use better models .
if you are looking at radio waves then the mirror will have to be made of thicker metal , because as you increase the wavelength you also have to increase the thickness of the metal to get the same reflectivity . that is actually how satellite dishes work . they are basically a big curved mirror that concentrates all of the microwaves coming down from the satellite . they are often full of holes to keep the weight down , and this does not matter because the wavelength of the waves is larger than the holes . this is the same principle as seeing a light on in your microwave . you can see the light escaping through the door but the microwaves are not escaping because they are too long . once you get beyond visible light into the shorter wavelengths ; ultraviolet light is easy to make mirrors for , but x-rays are very difficult . and so making x-ray telescopes is very difficult . sometimes they do it by using a bag of gas to act like a lens rather than mirrors or by using a metal mirror but at a very grazing angle which makes the mirror very large . mobile phone waves are at the microwave end of radio waves , and a sheet of aluminium would work nicely as a mirror for those . source : the naked scientists one can see that for making lenses , materials of different refractive indices can be used based on required convergence , divervence and dimensions can be compared to those of the above described mirrors , although there may a problem that making enormous lenses for radio waves require materials/machinery that we may not have at the moment , mirror though as you see are the big satellite dishes that we have seen many times .
actually , the sun outputs much more than these six wavelengths . the figure provided shows the spectrum of the sun : as you can see , the sun outputs light along a continuous curve at all visible wavelengths . the combination of which appears white to our eyes as a byproduct of having evolved in orbit of this star . the image also shows the absorption effects of the atmosphere . if you specifically discuss the blue scattering from the atmosphere , the following figure is a nice depiction . as you can see , the blue scattering leaves only the more red visible wavelengths , which makes sunlight appear more yellow around midday and more red near dawn/dusk . as for why we can simulate white light perfectly without having to reproduce the solar spectrum ; that has everything to do with how humans perceive colour . the human eye has special cone cells capable of perceiving colour . as shown below , there are only three true colours we can see ; red , gree , and blue ( with more green cells than the others again due to the solar spectrum ) . it is by using a combination of these three colours that we interpret all possible visible wavelengths . our brains interpret the relative strengths by which each different colour cone is stimulated and assigns a different colour to each combination . the next figure shows the sensitivity ranges of these cones . the three types of cones are sensitive over all visible wavelengths but are stimulated differently by each wavelength . thus , we can interpret a nice teal when a 500nm wave stimulates the cones appropriately , or we can simulate this for human eyes ( and this is what we do with things like computer screens ) by combining certain levels of 400 , 535 , and 700nm ( thanks to peter shor for the correction ) lights to stimulate the cones by the same ratio as the 500nm light . being the same ratio , our brain is tricked into seeing it as teal , but the summed light itself is not actually teal . in the same way , a combination of the six colours you mentioned would look white to us , but to make a true white , the sun must output across all wavelengths in the visible spectrum at varying amounts .
the magnetic field direction you are considering when facing this exercise is opposite to your book 's . the convention which appears in your book , of field lines coming out from the north pole and sinking into the south pole , is actually the standard convention that is used for the poles of a magnet . it is all a matter of convention : one could choose to name the poles in the opposite way , and then the lorentz torque would be in the direction you are describing . . . but i insist in that not being the usual convention .
is that correct that there are no nuclear warheads in service made of u-235 , as plutonium ones are much smaller and much more efficient maybe . who knows precisely what people use . but publicly available information from us warheads is that they moved away from uranium to plutonium fission devices soon after world war ii . is that correct , that the only current military uses for 80%+ u-235 are naval nuclear reactors and i would replace ' naval reactors ' with ' any small yet powerful reactor` . in most cases that might come down to the same thing , but it also allows the use of them in space probes etc . in rare occurrences - case and/or x-ray " reflector " in teller–ulam configuration as it is slightly better neutron breeder compared to more commonly used u-238 and allows to slightly reduce mass of " high-tech " plutonium charge at the same yield ? then you would replace the cheap , bountiful u238 with a more efficient but very expensive to separate u-235 ? and with a lot of u-235 , which would be sensitive to external neutron flux , thus forcing you to add extra shielding ( e . g with an extra layer of boron ) . i am not a bomb design expert , but thus sounds uneconomical . even if you save some plutonium .
this is one of those cases when the ends do not justify the means . . . just because you get a result that is true , from laws that are not supposed hold in that situation , does not mean that the laws can be used there . if you are asking about whether there is a deeper connection between using $\frac{1}{2}mv^2$ and getting the radius , to the best of my knowledge , there is none : it just happens to be a coincidence that they match . like michael brown said , the schwarzschild radius can only be derived from gr , in which the schwarzschild metric \begin{align} ds^2 = - ( 1-2gm/r ) dt^2 + ( 1-2gm/r ) ^{-1}dr^2 + r^2d\omega^2 \end{align} is the unique maximally symmetric solution to einstein 's equations in vacuum . ( i have set $c = 1$ ) here . note that something interesting is happening when $r = 2gm$ , and one can go on to show that $r = 2gm$ happens to be the boundary of the causal part of the future null infinity , which we identify as the event horizon .
this is more of an expansion of leongz 's answer . tl ; dr : the situation is incomplete . there may be an emf , and there may be a deflection . existence of emf and deflection are independent . we cannot calculate the value of the emf from the given data ( i.e. . , from a given time-varying $\bf b$ field ) your fundamental issue is that maxwell 's equations ( of which faraday 's law is one ) are not " cause and effect " . you cannot " plug in " a value of magnetic field and get a corresponding value of $\bf e$ field induced by the $\bf b$ field . all maxwell 's equations tell you is " which kinds of $\bf e$ and $\bf b$ fields can coexist given so-and-so conditions " . trying to solve the situation via maxwell 's equations i remember solving a similar situation via maxwell 's equations and being surprised by the answer . the " initial conditions " were $\mathbf {b}=\beta t\hat k$ , $\rho=0$ ( no charge ) , $\mathbf{j}=0$ ( no current ) . solving{*} for $\mathbf{e}$ , using the differential+microscopic form of maxwell 's equations ( since the integral form can only get you the value of $\bf e$ at certain positions at many times ) , i got : $$\mathbf{e}=\hat i ( lx + \frac{\beta}{2}y+az+c_1 ) +\hat j ( -\frac\beta{2}x+my+bz+c_2 ) +\hat k ( ax+by+nz+c_3 ) $$ where $a , b , l , m , n , c_1 , c_2 , c_3$ are arbitrary constants subject to $l+m+n=0$ note that this is a family of electric fields ( setting certain constants to zero , you get concentric ellipses iirc ) . all this means is that any $\bf e$ field of this type can coexist with a $\bf b$ field . implication for your problem this means that your initial conditions are insufficient/inconsistent . along with such a magnetic field , any type of electric field satisfying the above equations can exist--and must exist . so , in addition to knowing how your magnetic field is changing with time , you need to know : which one of these bajillion electric fields is present where is the rod in relation to this electric field ? these can usually be determined if you know the boundary conditions for the system . in a physical situation , these can be extracted from the setup . some more analysis let 's choose a simple solution and analyse it . i am taking the case where the coexisting electric field is just concentric circles . in this diagram , the blue stuff is the $\bf b$ field , and the green stuff is the $\bf e$ field . being lazy , i have not added arrows to it ( i also have not spaced the circles properly . there should be more space between the inner ones and less space between the outer ones ) . the other things are just rods and wire loops . to avoid confusion , when i refer to " emf " , i mean " the energy gained/lost in moving a unit test charge along a given path " . mathematically , the path integral $\int_{path}\mathbf{e}\cdot \mathrm{d} \vec l$ . i will come to voltmeters and the like later . let 's first look at the rods . the yellow rod $ab$ will have no emf across its ends , since the $\bf e$ field is perpendicular to its length at all points . on the other hand , the magenta rod $cd$ has an emf across its ends . this emf can be easily calculated via some tricks--avoiding integration--but let 's not get into that right now . you now can probably see why the second point " where is the rod in relation to this electric field ? " matters . on the other hand , this second point is not necessary for a loop . indeed , neither is the first point . going around the loop , both loops ( cyan and red in diagram ) will have an emf $-a\frac{\partial\mathbf{b}}{\partial t}$ . it is an interesting exercise to try and verify this without resorting to faraday 's law--take an electric field $\mathbf{e}=kr\hat\tau$ and do $\int \mathbf{e}\cdot\mathrm{d}\vec l$ around different loops of the same area . you should get the same answer . but , you cannot divide this emf by four and say that each constituent " rod " of the loop has that emf . for example , in the cyan loop $efgh$ , $ef$ has no emf , and the rest have different emfs . " dividing by four " only works ( in this case ) if the loop is centered at the origin . voltmeters voltmeters are an entirely different matter here . the issue with voltmeters is that , even for so-called " ideal " voltmeters , the p.d. measured depends upon the orientation of the voltmeter . reusing the same situation : here , the black wires are part of the current loop ( and peripherals ) . the yellow/magenta wires are to be swapped in and out for using the voltmeters . only one set of colored wires will be present at a given time . here , the " yellow " voltmeter will measure a pd three times that of the " magenta " one . this is due to the fact that it spans thrice the area and consequently has thrice the flux . remember , induced $\bf e$ fields are nonconservative , so voltmeters complicate things . they do not really tell you anything tangible , either . if the voltmeter were an everyday , non-ideal galvanometer based voltmeter , there would be extra complications due to there being a second loop . one more thing about rods a rod can additionally cause the extra complication of being polarizable/magnetizable . then , you have to consider the macroscopic maxwell equations , with the $\bf d , p , m , h$ fields and bound/free charges/currents . but then you need to know about the material of the rod . or , just find a hypothetical rod with $\mu_r=\varepsilon_r=1$ and use it . also , the charges in a rod will tend to redistribute , nullifying the electric field and thus the emf in the rod . conclusion the given data is incomplete . there is a truckload of different $\mathbf e$ fields that you can use here , and you are not sure which one it is . additionally , even if we knew which field it was , the orientation of the rod comes into the picture . so , the rod will have a motional emf , but this emf may be zero . the exact value of this emf cannot be calculated if you only know $\bf b$ . an ideal voltmeter , again , may show deflection . not necessarily , though . *solving simultaneous pdes in four variables is not too fun , so i did make some assumptions regarding the symmetry of the situation to simplify stuff . so the given family of solutions is a subset of the actual solution set . that does not hamper this discussion though .
in fact , maupertuis or euler principles of least action are historically the first formulation of a least action principle , but one have to wait lagrange and hamilton to have a modern version , with the so-called euler-lagrange equations , which allow us to obtain the equations of movement . if i am not mistaken , you cannot deduce directly the equations of movement from the maupertuis/euler principles . the problem i see is that you cannot know the dependence of the potential energy $v$ in $x$ , in seeing only the kinetic energy $t$ . now , as you state , but written differently , for a movement with conservative energy , one may see that the variation of the maupertuis ' action is equivalent to the variation of the lagrange/hamilton action , for instance , starting with maupertuis ' principle : $$ \delta\int ( 2t ) dt = 0$$ we have : $2t = t + ( e - v ) $ , so maupertuis ' principle can be written : $$ \delta\int ( e + ( t-v ) ) dt = 0$$ but $e$ being a constant , this is not useful in the variation , so finally , we have : $$ \delta\int ( t-v ) dt = 0$$ which is the usual lagrange/hamilton action . but , to really have the equations of movement , you have to use a functional , that is : $$ \delta\int l ( x , \dot x , t ) dt = 0$$ and this gives you , thanks to the euler/lagrange equations , the equations of movement .
i will firstly point out some apparent misconceptions in the question and subsequently i will explain what goes wrong when quantizing a theory of integer spin fields or particles with anticommutators , and vice versa . first , if we quantize a real klein-gordon field using anticommutators , the hamiltonian is going to vanish ( or be a field independent constant ) . at the level of fields , the hamiltonian for this field is a sum of squares $h=\sum_i a_i^2 ( x ) $ ( one $a_i$ is , for example , $\nabla\phi$ ) . since $\{a_i ( x ) , a_i ( y ) \}=0$ ( $\{\phi ( x ) , \phi ( y ) \}=0$ ) , $a_i^2=0$ for every $i$ , and therefore $h=0$ . at the level of creation and annihilation operators $h\sim \int_p\ , a_p^{\dagger}a_p+a_pa_p^{\dagger}\sim\int_p\ , \{a_p , a^{\dagger}_p\}$ . as $\{a_p , a^{\dagger}_q\}\sim\delta^3 ( p-q ) $ , the hamiltonian is an operator-independent constant . let 's see what happen when considering a complex scalar klein-gordon field , a more interesting case . complex scalar ( spin = 0 ) field quantized with anticommutators here , it is micro-causality what fails . consider a free complex , scalar field , and a bilinear , local , observable $\hat o ( x ) =\phi^{\dagger} ( x ) o ( x ) \phi ( x ) $ , with $o ( x ) $ a real c-number function . then , causality tell us that the commutator of two of these operators separated by a space-like distance is to vanish . one can check that : $$ [ \hat o ( x ) , \hat o ( y ) ] =o ( x ) o ( y ) [ \phi^{\dagger} ( x ) \phi ( x ) , \phi^{\dagger} ( y ) \phi ( y ) ] \\ =o ( x ) o ( y ) \left ( \phi^{\dagger} ( x ) \phi ( y ) -\phi^{\dagger} ( y ) \phi ( x ) \right ) \ , \{\phi ( x ) , \ , \phi^{\dagger} ( y ) \}$$ and , on the other hand , using expression of a complex , free klein-gordon field in terms of creation and annihilation operators , we can compute the anticommutator making use of the assumed canonical anticommutation relations between creation and annihilation operators . the result is ( you should check all this ) $$\{\phi ( x ) , \ , \phi^{\dagger} ( y ) \}=2\int d^3\tilde {\bf p}\ , \cos ( p ( x-y ) ) $$ where $d^3\tilde {\bf p}$ is a standard notation for the lorentz-invariant measure . using the lorentz invariance of the previous expression and the fact that it does not vanish for $x_0=y_0$ , we can conclude that $\{\phi ( x ) , \ , \phi^{\dagger} ( y ) \}$ and , as a consequence , $ [ \hat o ( x ) , \hat o ( y ) ] $ do not vanish for space-like separations , which violates causality . therefore , both real and complex scalar fields refuse to be quantized with anticommutators . spin $1/2$ field quantized with commutators starting with the dirac hamiltonian , one gets $$h\sim \int\ , a^{\dagger}a-bb^{\dagger}$$ then , in order to have a minimum-energy vacuum state , we need a hamiltonian that is bounded from below . the $b$-modes have a negative sign in the hamiltonian so that there are two alternatives : exchange the standard action of the $b$ operators on the hilbert space . that is , $b^{\dagger}$ is going to annihilate quanta and $b$ is going to create them , so that $$h|p\rangle_b\sim h\ , b|0\rangle_b \sim [ h , b ] |0\rangle_b \sim \sqrt{m^2+p^2}|p\rangle_b$$ where we have made use of $ [ b , b^{\dagger} ] \sim \delta^{3}$ . however , doing this we end with states of negative norm $$_b\langle p|p'\rangle_b=\langle 0|b^{\dagger}_p\ , b_{p'}|0\rangle=-2\ , \left|{\sqrt{m^2+p^2}} \ , \right|\ , \delta^3 ( p-p' ) \ , \langle 0|0\rangle \ ; , $$ which prevents from a probabilistic interpretation ( negative probabilities are nonsensical ) . the alternative is to use anticommutators ( i.e. . , fermi-statistics ) , which reverse the sign in the hamiltonian . this is the choice that works . these obstacles are a consequence of pauli 's spin-statistics connection theorem .
that function is not defined there because you want to use a different one , depending on what kind of motion you are modelling . using the obvious newton law $$ a \propto f , $$ you can think of it as calculating the force that acts on your object , e.g. the spring force . in the case of gravitation , you simply put in the constant gravity acceleration downwards . ( i would like to note that in this case , 4th order runge-kutta is something of an overkill as 2nd order is already exact ! but it is nevertheless a good idea to use it here , to be consistent : 4th-order runge-kutta is used in a very wide range of applications . )
you ask : is it true that any magnetic field induces an emf in a loop of wire ? no , not in any setup . only if there is a change in the magnetic lines passing through a loop . it is how electric generators work . here is a simple example of a loop in a changing magnetic field .
there is the basic confusion here , in my opinion , of the wave/particle identity . when we speak of photons we are in the quantum mechanical regime , it is an elementary " particle " . the quotation marks are necessary because it is not a particle like a billiard ball , and it is not a wave like an acoustic wave , or even a classical electromagnetic wave . it is a mathematically described " entity " which , depending on the experiment will act either like a billiard ball , i.e. a point in four dimensional space but with specific quantum numbers , ( in the case of the photon spin , polarization and zero mass ) or like a probability wave . probability wave in bold to emphasize that the energy of the entity when appearing as a wave is not distributed , as the energy in sound waves , in space . the entity will appear always with a specific ( x , y , z , t ) ( within the heisenberg uncertainty principle ) but the probability of finding it there will display the properties of waves , interference patterns . . the experiment you are setting up does not have the ability to detect the wave nature of the probability distribution of the photon . if it is done in vacuum the distance will play no role to the efficiency of detection . it will take a little longer due to the velocity of light but the material of the detector will not make a difference : the photon will either be there or not . in the delayed choice experiment the description makes the same mistake . the individual photon does not take both paths . the interference pattern appears because of the statistical accumulation of many photons which then display the probability wave aspect of the photon wave function . each individual photon takes a specific path , but the probability of taking it is affected by the interference setup .
i have always found the cornell to be a good source of info ( and also the former home of carl sagan ) : blue stragglers are stars which stay on the main sequence ( the normal , hydrogen-burning phase of a star 's lifetime ) longer than they are expected to . the color of a star is a measure of its temperature and its mass - blue stars are hotter and more massive than red ones . the more massive a star is , the faster it burns up its hydrogen , so blue stars are expected to spend less time on the main sequence than red stars . therefore , when you look at a color-magnitude diagram of a globular cluster ( whose member stars all formed around the same time ) you expect to see an orderly transition ; stars which are bluer than a certain value ( known as the " turnoff " point ) will have already left the main sequence , while those which are redder will still be on it . the location of the turnoff point can be used to estimate the age of the cluster . but , it is usually the case that several stars in a cluster are observed along the main sequence past the turnoff point , and these are referred to as blue stragglers . the most likely explanation for blue stragglers seems to be that they are the result of stellar collisions or mass transfer from another star . that way , a star which is red , cool and already somewhat old can get extra mass and turn bluer . it spent most of its life as a red star and therefore burnt its hydrogen at a slow enough rate to still be on the main sequence , but then at a certain point it gets extra mass and effectively " disguises " itself as a blue star , which makes us think it is younger than it really is . i think the key takeaway is that we really are not 100% sure . however , that in no way means that anyone can simply assert that their pet theory ( guess , lie , whatever ) is the answer either . especially if they do not bring any evidence , testable hypotheses , or data to examine to the table . solstation also has quite a bit of good info on blue stragglers and what we know about them . of particular interest is this graphic : this paper from cornell also has some additional info . abstract : in open star clusters , where all members formed at about the same time , blue straggler stars are typically observed to be brighter and bluer than hydrogen-burning main-sequence stars , and therefore should already have evolved into giant stars and stellar remnants . correlations between blue straggler frequency and cluster binary star fraction , core mass and radial position suggest that mass transfer or mergers in binary stars dominates the production of blue stragglers in open clusters . analytic models , detailed observations and sophisticated n-body simulations , however , argue in favour of stellar collisions . here we report that the blue stragglers in long-period binaries in the old ( 7 × 109-year ) open cluster ngc 188 have companions with masses of about half a solar mass , with a surprisingly narrow mass distribution . this conclusively rules out a collisional origin , as the collision hypothesis predicts a companion mass distribution with significantly higher masses . mergers in hierarchical triple stars are marginally permitted by the data , but the observations do not favour this hypothesis . the data are highly consistent with a mass transfer origin for the long-period blue straggler binaries in ngc 188 , in which the companions would be white dwarfs of about half a solar mass .
manifest lorentz symmetry means that one can see lorentz invariance directly from the way the theory is formulated ; typically when space and time are treated on the same footing as components of a 4-vector . in these cases , the lorentz group generators are represented in a simple way ( hence the ''manifest'' symmetry ) , but it is far less trivial to find a corresponding hilbert space of state vectors on which the interacting energy-momentum 4-vector acts . however , a theory can be lorentz invariant in a more indirect way , such as in the canonical formalism , where a hilbert space and an associated hamiltonian is specified directly . then lorentz invariance is established by proving the ( then far less trivial ) existence of 6 generators satisfying the commutation relations for the lorentz generators , such that the interacting hamiltonian and the free momentum generators transform jointly as a 4-vector .
try pyrex , vycor , fused silica - in that order .
dear andrew , despite moshe is expectations , i fully agree with him , but let me say it differently . in qft , we are talking about " first quantization " - this is not yet a quantum field theory but either a classical field theory or quantum mechanics for 1 particle . those two have different interpretations - but a similar description . when it is " second-quantized " , we arrive to qft . feynman diagrams in qft may be derived from " sums over histories " of quantum fields in spacetime ; for example , the vertices come from the interaction terms in the lagrangian , and the propagators arise from wick contractions of quantum fields . this is the " second-quantized " interpretation of the feynman diagrams . there is also a first quantized interpretation . you may literally think that the propagators are amplitudes for an individual particle to get from $x$ to $y$ , and the vertices allow you to split or merge particles . you may think in terms of particles instead of fields . in qft , this is an awkward approach because most particles have spins and it is confusing to write a 1-particle schrödinger equation for a relativistic spin-one photon , for example . however , in string theory , spin is derived and the first-quantized interpretation is very natural . so the cylindrical world sheet describes the history of a closed string much like a world line describes the history of a particle . and it is enough to change the topology of the world sheet to get the interactions as well . so in string theory , one may produce the amplitudes " directly " from the first-quantized approach because the changed topology of the world sheet , which we sum over , knows all about multi-particle states and their interactions , too . we say that the interactions are already determined by the behavior of a single string . needless to say , like any feynman diagrams , these sums over topologies are just perturbative in their reach . now , you may also write down string theory as a string field theory , in terms of quantized string fields in spacetime . somewhat non-trivially , an appropriate interaction term - that " knows " about the merging and splitting of strings - may be constructed in terms of a " star-product " ( a generalization of noncommutative geometry ) . in this way , string theory becomes formally equivalent to a quantum field theory with infinitely many fields in spacetime - for every possible internal vibration of the string , there is one string field in spacetime . it used to be believed that this formalism would tell us much more than the perturbative expansions because , for example , lattice qcd in principle can be used to define the theory completely , beyond perturbative expansions . however , this belief has been showed largely untrue . at least so far . it is been shown that string field theory indeed offers an equivalent way to calculate all the amplitudes of perturbative string theory - especially for bosonic strings with external open strings ( closed strings are possible , and surely appear as internal resonances , but they are awkward to include directly as external states ; superstrings are probably possible but require a substantially heavier formalism ) . also , string field theory has been very useful to explicitly verify various conjectures about the tachyon potential in bosonic string theory ( or , equivalently , about the fate of unstable d-branes which emerge as classical solutions in string field theory ) . these investigations , started by ashoke sen , led to some nice mathematical identities that had to work - because string theory works in all legitimate descriptions - but that were still surprising from a mathematical viewpoint . but all the physical insights confirmed by string field theory had already been known from more direct calculations in string theory . so because string field theory is widely believed not to tell us anything really new about physics , only a dozen of string theorists in the world dedicate most of their time to string field theory . moshe is surely no exception in thinking that it is not too important to work on sft . still , it is conceivable that sometime in the future , a more universal definition of string theory will be a refinement of string field theory we know today . however , it is also possible that this will never occur because it is not true : string field theory seems too tightly connected with a particular spacetime and with particular objects ( strings ) while we know that the true string theory finds it much easier to switch to another spacetime and other objects by dualities . cheers lm
4tnemele 's answer is great , but i thought i would try to give a simpler explanation of the main point of confusion . let 's say ( as in 4tnemele 's answer ) that there is only one relevant orbital ( or " energy level" ) at each site . that means there is two states a single electron can occupy : spin up and spin down . also , let 's say there is no magnetic field . you say " due to on-site coulomb repulsion the two electron levels are separated by u energy at a doubly occupied site . " this is wrong , because there is only one " level " ( orbital ) , not two . the coulomb energy penalty u only applies if there are two electrons in that level , one spin up and one spin down . at a doubly occupied site , the overall energy is increased by u , but that energy does not belong to one electron or the other . it comes from the interaction between the electrons . at a doubly occupied site , the two electrons still have exactly the same spatial wavefunction . the many-body wavefunction is spatially symmetric . this is why the spins have to be opposite . so the basic problem is that you should not say " the two electron levels are separated by u " . instead you should say " the state with two electrons at the same site is increased in energy by u relative to what you had otherwise expect ( twice the single-electron energy ) " .
it does not have to be coiled . magnetic fields will be created as long as you have a current through a wire . coiling may make the field strength stronger . how strong the field strength is in practice also depends on the current through the wire and whether your power cable has shielding .
the sort of rheometers normally used for industrial research work by applying a controlled stress then measuring the resulting shear rate . typically you program them to start at a high stress and end at a low stress , and the rheometer will go away and automatically take readings at intervals between the two . when it is finished it spits out a table of stress-strain readings . the software running the rheometer can display this in any way you want e.g. as a graph of viscosity vs shear rate . you can put whatever type of fluid you want in your rheometer . you just need to bear in mind that with strongly shear thinning fluids it can take a very long time to obtain the low stress readings because the rheometer has to wait a long time for the reading to stabilise . you also generally need a trial run to get a rough guide as to what initial and final stress to specify .
in the view of classical electrodynamics , electron-positron pair would definitely emit radiation when accelerated . but in quantum mechanics , positronium at the singlet state ( spin anti-parallel ) with 0 orbital angular momentum will be perfectly spherically symmetric in its rest frame . in other words , it will have no net charge , net dipole moment , net quadruple moment , . . . . so it will not radiate . triplet state and excited states may , however . neutron will surely emit radiation when accelerated , although very small , because it has non-zero magnetic dipole moment . that is not called bremsstrahlung however . and this would not be a good way to probe whether a particle is composite or not . the higher-order radiation is small and hard to detect , and it is much more straightforward to simply measure dipole or quadruple moments .
a nature news article ( entangled photons make a picture from a paradox , 27 august 2014 ) the news article refers to the article " quantum imaging with undetected photons " , gabriela barreto lemos et al . and anton zeilinger , nature 512 , 409–412 , ( 28 august 2014 ) . there is a corresponding arxiv article ( 1401.4318 ) with the same figures available . a way to form an image of an object out of photons that have not interacted with the object , but which are entangled with photons that have [ . . . ] have i misunderstood something about the setup [ . . . ] ? a key element of the schematic of the experiment ( i.e. . figure 1 ) is that the photons which have interacted with the object ( marked red ) are subsequently directed to a nonlinear crystal nl2 where in turn a photon originates ( marked yellow ) which eventually contributes to forming the image . ( i would say that the description of the news article about " certain pairs of photons being recombined " is a bit misleading on this point ; and there is apparently no mentioning of " recombination " in the arxiv article . ) the schematic of the experiment requires that " an effective light beam " runs , through several optical elements , from the object to the " screen" ; even though there are different photons ( marked red vs . marked yellow ) contributing to different segments of that " effective light beam " . in other words : any event at which " the image is made on the screen " is within , or at least on , the future light cone of the corresponding event at the " the object had been illuminated " . and this requirement stands in the way of paradoxial implementations . in [ other ] experiments you have to combine all the beams back again to make the image in the end , so there is no way to construct a paradox . for the experiment considered here it is also true that the two ( yellow ) signal pulses must be coincident at the final ( combining ) beam splitter bs2 ; after corresponding two ( green ) laser pulses had been generated ( in coincidence ) at beam splitter bs1 . but here the passages through the nonlinear crystals ( nl1 and nl2 ) effectively allows photons to be substituted ( "yellow for red" ) along the way . consequently , the frequency of photons ( red ) illuminating the object and the frequency of photons ( yellow ) which make the image may be different from each other ; which may be used to " practical/technical " advantage .
color-neutral gluons that have the component blue-antiblue do exist , much like red-antired and green-antigreen . however , the sum of these three possible kinds of gluons is unphysical , so there are only two " diagonal " types of gluons . none of these two types of gluons are " genuinely color-blind " or " completely color-neutral " . this is more manifest if you realize that the color dependence of the gluon field may be written as a traceless $3\times 3$ hermitian matrix . it is traceless because the gauge group is $su ( 3 ) $ rather than $u ( 3 ) $ whose dimension is 8 rather than 9 . ( there are 3 complex entries strictly above the diagonal , which are copied in the complex conjugate way beneath the diagonal , plus 2 or 3 real entries on the diagonal , depending on whether we require the trace to vanish . ) completely color-neutral gluons , if they were added , would be proportional to the identity matrix and they would couple to all three colors of the quarks equally . in other words , the interactions mediated by such gluons would only depend on the baryon number of the quarks . experimentally , this interaction does not exist . in beyond-the-standard-model physics , one may try to extend $su ( 3 ) $ to $u ( 3 ) $ in this way ( this is very common in braneworld models ) but because no new baryon-charge long-distance interaction is seen , the $u ( 1 ) $ in the $u ( 3 ) $ has to be spontaneously broken at a pretty high energy scale .
unitary operators are operators that satisfy some conditions . among other things , they have to be linear : http://en.wikipedia.org/wiki/unitary_operator the operation ( or " an operation" ) that maps any $\psi ( x ) $ to $\delta ( x-x_0 ) $ where $x_0$ is the random position resulting from a measurement can not be associated with any linear operator . it is easy to see why . take two functions $\psi_1 ( x ) $ and $\psi_2 ( x ) $ that have different supports : for example , the first one is localized in the vicinity of boston while the other sits near new york . linearity of the collapse operator $c$ requires $$ c ( \psi_1 + \psi_2 ) = c ( \psi_1 ) + c ( \psi_2 ) . $$ however , the first term of the right hand side is a delta-function localized somewhere near boston while the second term of the right hand side is a delta-function localized near new york . their sum therefore can not be a multiple of a single delta-function , so the left hand side can not be a " collapsed wave function " , proving that an operator that maps anything to a single delta-function can not be linear . there are of course other ways to prove that it can not be a unitary operator – which is a very strong condition . of course , the right resolution of this non-unitarity problem is that there is nothing such as the collapse of a wave function . the wave function is not a real wave : it is a set of complex amplitudes whose squared absolute values do not describe " the reality " but rather just the probabilities of " different realities " . the probability distributions mean that you always get just one outcome and the values of the probability distribution just tell you what the probabilities of different outcomes are . nothing has to " collapse " because the wave was not a " real observable wave " to start with . the idea that the wave function has to " collapse " and one has to look for a " mechanism " how it collapses is an artifact of a misinterpretation of the wave function .
the answer lies in the selection bias towards brighter stars . there are two reasons this makes the sun look relatively dense . the first is in martin 's answer . looking at a list of brightest stars , many ( e . g . betelgeuse , aldebaran , antares ) are red giants . these are stars that have finished burning hydrogen into helium in their cores and are much larger in size than main-sequence stars like the sun . as a result , their mean densities are small . the second effect is that the more massive a main-sequence star is , the smaller its mean density but the greater its luminosity . so again , more massive stars on the main-sequence ( e . g . rigel ) are easier to see but also have lower mean densities . if you compare the sun to stars from a list of sun-like stars , you will find it is not unusual .
their angular momentum stays nearly constant . you might be thinking of their angular velocity . there is a lot of simulation-based work out there on the inspiral of two compact supernova remnants ( nss or bhs ) , done partly to determine what the gravitational wave signals would look like for the ligo experiment . two nss would merge to form a bh , because their combined mass would be above the threshold for the fermi pressure of the ns to resist gravitational collapse . part of the ns material would briefly orbit the newly formed bh as an accretion disk , and some of the energy would be beamed out along the rotational axis , visible to us ( if we happen to be in the line-of-sight of the axis ) as a gamma ray burst .
the most important thing is that nonrelativistic qm ( as it is formulated traditionally ) cannot deal with changing particle number , because the position basis hilbert space changes dimension as you increase the particle number . quantum field theory allows particle number to change , and this is the main difference . this is also important in cases where the number of particles is indefinite , like the fixed phase description of a bec or a superfluid . in this case , the state of fixed phase macroscopic matter wave is a superposition of different numbers of particles . for this reason , nonrelativistic schrodinger fields are useful in condensed matter physics , even in cases where the particle number is technically conserved , because the states one is interested in are in a classical limit where it is better to assume that the particle number is indefinite . this is exactly analogous to introducing a chemical potential and pretending that the particle number can fluctuate in statistical mechanics , in those cases where the particle number is exactly fixed . or introducing a temperature in those cases where the energy is exactly fixed . it is still mathematically convenient to do so , and it does no harm in the thermodynamic limit . so it is convenient to use quantum fields to describe nonrelativistic situations where the particle number is fixed , but the behavior is best described by a classical collective wave motion .
in flat space the surface area of a sphere is $4\pi r^2$ . in positively curved space the surface area of a sphere is less than $4\pi r^2$ and in negatively curved space the surface area of a sphere is greater than $4\pi r^2$ . by $r$ i mean that if you start at the centre of the sphere with your trusty ( infinitesimal ) ruler and measure the distance to the sphere you will be measuring the quantity $r$ ( this is known as the proper distance ) . the area of the sphere is then measured by crawling over it and measuring it with the same ruler . if you are looking for an intuitive way to visualise the different curvatures i can not help - please let me know if you find one ! re your question 2: this is a somewhat vexed issue and different commentators have different views on whether it makes any sense to talk about the total energy of the universe . see for example the many and varied answers to total energy of the universe . you specifically ask about dark energy : you need to appreciate that ordinary matter , dark matter and dark energy all have a positive energy density and you can add them all together to get the total energy density . dark matter causes a repulsion because it has an unusual equation of state , not because it is exotic in any way . the zero energy universe idea is that the negative gravitational potential energy exactly balances out the positive combined energy density of matter and dark energy so the net energy is zero .
not only is it possible to remove the nucleus from an atom , but the rhic does it every day ! the rhic collides heavy nuclei like gold to measure the properties of nuclear matter at high densities . gold atoms have their electrons stripped off in the tandem van de graaff accelerator . the atoms are subjected to such strong electric fields that the positive nuclei and negative electrons are pulled apart . response to comment : see http://isnap.nd.edu/html/research_fn.html for a few more details on how the atoms can have their electrons stripped off ( this is a different accelerator from the one at the rhic ) . you start with singly ionised atoms . these are easily made e.g. by shining ultraviolet light on the atoms . the singly ionised atoms are accelerated to a high speed than crashed into a very thin carbon sheet . the heavy nuclei plough straight through while the electrons are scattered , and the nuclei are then accelerated away with a second electric field .
in the first action the $a_{\mu}$ are hermitian . in the second action the $a_{\mu}$ are anti-hermitian since we let $a_{\mu}\to\frac{i}{g}a_{\mu}$ . the commutator of anti-hermitian matrices are also anti-hermitian . if we have $\text{tr} ( m^{2} ) $ , with $m$ being anti-hermitian , then we can write it as $\text{tr} ( m^{2} ) =-\text{tr} ( ( im ) ^{2} ) $ , with $im$ being hermitian . since the eigenvalues of an hermitian matrix is real and we take the trace of the square it , it follows that $\text{tr} ( m^{2} ) \leq0$ . changing the anti-hermitian matrices to hermitian matrices changes the sign .
i think the usual way we approach this issue is to first obtain the field equations , in the classical manner , and obtain classical solutions by free wave expansion or green 's functions . for example in equation ( 2.55 ) and ( 2.60 ) , you can see that the propagators , which are derived from quantum mechanics , have " classical " solutions , that is they satisfy the classical field equation ( klein-gordon ) . so , your equation of motion will be eqn . ( 2.61 ) , \begin{align} ( \partial^2 + m^2 ) \phi ( x ) and = j ( x ) \end{align} take the fourier transform of this to momentum space , and you obtain , \begin{align} ( -p^2 + m^2 ) \tilde{\phi} ( p ) = \tilde{j} ( p ) \end{align} to solve this , you solve for the free green 's function , \begin{align} ( -p^2 + m^2 ) \tilde{\phi} ( p ) = -i \end{align} then after you obtain a solution for this $\tilde{\phi}_{\rm free} ( p ) $ , do this : \begin{align} \phi ( x ) = \int \frac{d^4 p}{ ( 2\pi ) ^4} \tilde{j} ( p ) \tilde{\phi}_{\rm free} ( p ) . \end{align} where is the quantization , you may ask . it is in the solution of the free green 's function . to explain the second part of your question , you need to include information about when your source was turned on . the $\phi ( x ) $ over time will also not be the one for the free field , since $\phi$ is a solution to the field equations , as we showed in 1 . in other words , \begin{align} \phi ( x ) \neq \int \frac{d^3 p}{ ( 2\pi ) ^3} a_{\vec{p}} e^{i p\cdot x} + a_{\vec{p}}^\dagger e^{-i p \cdot x} \end{align} .
the acceleration is a vector $\mathbf{g}$ throughout the motion , and $\mathbf{g}$ is always pointing downward . since you choose positive $x$ to be vertically downward , so $\mathbf{g}$ is along positive $x$ if we draw out the cartesian coordinate , then $\mathbf{g}$ must have positive value , $\mathbf{g}=g\ , \hat{\mathbf{x}}$ . if you choose vertically upward to be $x&gt ; 0$ , then acceleration $\mathbf{g}$ has negative sign , $\mathbf{g}=-g\ , \hat{\mathbf{x}}$ . it is just the matter how you choose the $x&gt ; 0 , y&gt ; 0$ directions . draw a diagram of $v$ , $g$ , force on the ball with $ ( x , y ) $ coordinates .
for $p=1$ , ctc 's do not exist in minkowski spacetime . in other $1+3$ spacetimes , in principle they are admitted in the absence of further requirements ( like globally hyperbolicity ) on the causal structure of the spacetime . they must be present if the spacetime is compact , for instance . for $p\geq 2$ , the answer is obviously yes . consider a manifold $m$ with metric $g$ with signature ( p , q ) and $p \geq 2$ . in a $p+q$-dimensional neighbourhood $u$ of any point $s\in m$ , using the exponential map at $s$ starting from a $p$ dimensional subspace generated by $p$ timelike vectors in $t_sm$ , you can construct an embedded $p$-dimensional submanifold $n$ passing through $s$ and whose metric ( induced by $g$ ) has signature $ ( p , 0 ) $ . this means that every vector tangent to a point in $n$ , considered as a manifold on its own right , is timelike . in local coordinates on $n$ around $s\in n$ , any circle surrounding $s$ is a closed timelike curve .
there is no 2-d metrics here , because we are working with the boundary $\partial m$ you could imagine a standard action $s_0 = \int_{\partial m} d\tau a_a \frac{d x^a}{d \tau}$ , where $a_a$ is constant . with a small perturbation , we will have : $a_a ( x ) = a_a + \epsilon_a ( x ) $ , and we have an action $s = \int_{\partial m} d\tau a_a ( x ) \frac{d x^a}{d \tau}$ a partition function would be $z = \int dx e^{-s_0 - v} = \int dx e^{-s_0} ( 1-v +\frac{1}{2} v^2 + \dots ) $ , with $v = \int_{\partial m} \epsilon_a ( x ) \frac{d x^a}{d \tau}$ $a_a ( x ) $ are coherent states of photons , as $g_{\mu\nu} ( x ) $ are coherent states of gravitons . in some sense , we may consider that the photon vertex operator corresponds to a very special ( non-coherent ) case where $\epsilon_a ( x ) = \zeta_a e^{ip . x}$ , in the same way as the graviton vertex operator is a very special ( non-coherent ) case where $h_{\mu\nu} ( x ) = \zeta_{\mu\nu}e^{ip . x}$
you are not wrong . however , there used to be an object exactly $1$ meter long until 1960 , because a meter was defined to be the length of a certain platinum-iridium rod at certain conditions . since then , the meter is defined in terms of interferometry , and now it is specifically the distance traversed by light in vacuum within a certain period of time . similarly , the kilogram has a prototype whose mass is $1$ kg by definition . there are some proposals to replace that definition , but it has not been done yet .
angle between $dl$ and $r$ at any point on the circular loop is $90^0$ . look at the below figure .
assume , as in the first part of the proof , that there exists a set $\gamma$ , determined from the outside , such that $\delta\subset\gamma\subset\lambda$ . moreover , by construction , the exterior boundary of $\gamma$ is entirely composed of $-$ spins ( not as on your picture ! $\gamma$ is exactly the interior of this circuit ) . therefore , by the strong markov property , we have , on this event , $$ \mu ( \cdot|\mathcal{f}_{\gamma^c} ) = \mu^-_{\gamma} . $$ let $f$ be an increasing local function with support inside $\delta$ . by fkg , $$ \mu^-_{\gamma} ( f ) \leq \mu^-_{\lambda} ( f ) \leq \mu_{\lambda'}^- ( f ) , $$ for all $\lambda'\supset\lambda$ . let $\lambda'\uparrow\mathbb{z}^2$ to conclude that $$ \mu^-_{\gamma} ( f ) \leq \mu^- ( f ) . $$ let $\mathcal{a}_{\lambda , \gamma}$ be the event that there is a $-\star$-circuit inside $\lambda$ , surrounding $\delta$ , and that $\gamma$ is the interior of the largest such circuit . let also $\mathcal{a}_\lambda=\bigcup_\gamma \mathcal{a}_{\lambda , \gamma}$ . then , $$ \mu ( f ) \leq \sum_{\substack{\gamma:\\\delta\subset\gamma\subset\lambda}} \mu ( \mu ( f | \mathcal{f}_{\gamma^c} ) 1_{\mathcal{a}_{\lambda , \gamma}} ) + \mu ( \mathcal{a}_{\lambda}^c ) \leq \mu^- ( f ) + \mu ( \mathcal{a}_{\lambda}^c ) . $$ since $\lim_{\lambda\uparrow\mathbb{z}^2}\mu ( \mathcal{a}_\lambda^c ) \leq \mu ( e^+ ) =0$ , we conclude that $$ \mu ( f ) \leq \mu^- ( f ) , $$ for all increasing local functions $f$ . this implies that $\mu\preccurlyeq\mu^-$ . but , since $\mu^-$ is stochastically minimal , this means that $\mu=\mu^-$ .
relevant article is : the path integral on the poincaré upper half-plane with a magnetic field and for the morse potential by christian grosche http://www.sciencedirect.com/science/article/pii/0003491688902837 abstract rigorous path integral treatments on the poincaré upper half-plane with a magnetic field and for the morse potential are presented . the calculation starts with the path integral on the poincaré upper half-plane with a magnetic field . by a fourier expansion and a non-linear transformation this problem is reformulated in terms of the path integral for the morse potential . this latter problem can be reduced by an appropriate space-time transformation to the path integral for the harmonic oscillator with generalised angular momentum , a technique which has been developed in recent years . the well-known solution for the last problem enables one to give explicit expressions for the feynman kernels for the morse potential and for the poincaré upper half-plane with magnetic field , respectively . the wavefunctions and the energy spectrum for the bound and scattering states are given , respectively . however , it is not freely available . for some information you can consult http://arxiv.org/abs/hep-th/9302053 - classification of solvable feynman path integrals co-authored by c . grosche - in the table at the end of article the authors claim that schroedinger equation for both morse potential and particle in constant magnetic field can be viewed as generalisations of radial harmonic oscillator problem . unfortunately they do not give much details .
basically what you are doing is calculating a discrete time series : you are finding an discrete series of positions $x_n$ ( where $n\in\mathbb{z}_+$ ) which are paired with times $t_n = t_0 + n\delta t$ . but there are a couple of ways you could interpret this time series , and the formula you are looking for depends on which one you use . exact positions one option is to say that $x_n$ represents the exact position of the particle at time $t_n$ . in order to find the asymptotic position as $n\to\infty$ ( that is , the position where the particle stops ) , you need to convert your iterative formulas , $$\begin{align}x_n and = x_{n-1} + v_{n-1}\delta t \\ v_n and = cv_{n-1}\end{align}$$ into direct formulas . hopefully you can see that , because the speed gets multiplied by $c$ at every step , the speed at the $n$'th step will be given by $$v_n = c^n v_0$$ then figuring out the formula for $x_n$ is probably most easily done by finding a pattern : $$\begin{align}x_1 and = x_0 + v_0\delta t \\ x_2 and = x_1 + v_1\delta t \\ and = x_0 + v_0\delta t + cv_0\delta t \\ x_3 and = x_2 + v_2\delta t \\ and = x_0 + v_0\delta t + cv_0\delta t + c^2v_0\delta t\end{align}$$ you wind up with $$x_n = x_0 + v_0\delta t \sum_{k=0}^{n}c^k$$ in the limit as $n\to\infty$ , this simplifies to $$x_\infty = x_0 + \frac{v_0\delta t}{1-c}$$ which you can solve for $c$ . this equation reproduces the sample results you listed . it is important to note that the value of $c$ you get from this interpretation depends on your choice of $\delta t$ . to simulate the same motion using a different time step , you will need to change the value of $c$ . only the ratio $\frac{\delta t}{1-c}$ is fixed . discrete approximation the other way in which one could interpret your time series is as a discrete approximation to some continuous function $x ( t ) $ that describes the actual motion . if you find it strange that the result depends on the time step $\delta t$ , this might be worth looking into . to motivate this interpretation , you need to know something about finite difference approximations . in a nutshell , when you want to use a computer to numerically solve a differential equation , you can replace the derivative operator with a finite difference operator : $$\frac{\mathrm{d}f}{\mathrm{d}t} \to \frac{f ( t + \delta t ) - f ( t ) }{\delta t}$$ the thing on the right here is merely the simplest example of a finite difference operator . ( it also happens to be quite inaccurate . ) this particular one looks a lot like the definition of the derivative , except that $\delta t$ is finite , not infinitesimal ( hence the name ) . your iteration equations can be written in the form $$\begin{align}\frac{x_n - x_{n-1}}{\delta t} and = v_{n-1} \\ \frac{v_n - v_{n-1}}{\delta t} and = \frac{c - 1}{\delta t}v_n\end{align}$$ you can assume that this is the finite difference approximation to some exact differential equation , and work backwards to find the equation . for example , by replacing the finite difference operator in the position equation with a derivative , you get $$\frac{\mathrm{d}x}{\mathrm{d}t} = v ( t ) $$ if you try to do the same thing with the velocity , well , you can not , because the right side does not have a defined limit as $\delta t\to 0$ . but there are a couple of little hacks you can use , for example : choose some characteristic time scale $\tau$ and set $\delta t = \tau$ on the right , while still taking the limit on the left . you wind up with $$\frac{\mathrm{d}v}{\mathrm{d}t} = \frac{c - 1}{\tau}v ( t ) $$ solving this differential equation gives you $$\begin{align}v ( t ) and = v ( 0 ) e^{-t ( 1-c ) /\tau} \\ x ( t ) and = x ( 0 ) + \frac{v ( 0 ) \tau}{1-c} \bigl ( 1 - e^{-t ( 1-c ) /\tau}\bigr ) \end{align}$$ and in the limit as $t\to\infty$ , $$x ( \infty ) = x ( 0 ) + \frac{v ( 0 ) \tau}{1-c}$$ which , again , you can solve for $c$ . it turns out to be the same thing as before , only with $\tau$ instead of $\delta t$ . this is only the case because this is an exceptionally simple equation , and because of the particular way in which i chose to define the time scale $\tau$ . in general , these two methods will not give identical results , in part because of the inaccuracies of the particular finite difference approximation i used .
since a quark $q_i$ is a spin $\frac{1}{2}$ dirac particle with charge $z_i e$ and constituent mass $m_i$ with $i=u , d , s . . . $ theory predicts its magnetic moment as $ \mu_i = \frac{z_i e \hbar}{2 m_i}$ . note that the formula in your question is only valid in the constituent quark model where a baryon consists only of three constituent quarks . therefore you have to use the constituent quark mass in the formula for the magnetic moment too . a list of constituent quark masses is eg . found here . these masses are obtained from hadron spectroscopy . i am not aware of any direct measurements of quark magnetic moments .
you are massively overthinking the problem . the collector current is given ( by the diagram ) to be 150x the base current . the sum of base and collector current has to flow through the emitter . . . that is all you need to solve this . in particular , a current source will look to a circuit like " whatever resistance " it needs to be in order for the correct current to flow . so you can not apply ohm 's law to the right hand circuit - instead you have to assume that the current source is doing its job , and that a current of 150 $i_b$ is flowing in the collector . incidentally - as drawn this circuit has a fatal flaw : there is no return path for the base current . . . normally you would have a common ground between the left and right circuits , where the base current could return ( and which would allow the emitter current to be $151 i_b$ . so really - as drawn , this circuit does not quite work . . .
great question . a little background first . note that any force $\boldsymbol{f}$ moment $\boldsymbol{m}$ system on a point a can be equipollently translated into the screw axis s leaving only the components of $\boldsymbol{m}$ that are parallel to $\boldsymbol{f}$ . the location is found by $$ \boldsymbol{r} = \frac{\boldsymbol{f} \times \boldsymbol{m}}{\boldsymbol{f}\cdot\boldsymbol{f}} $$ also the moment components parallel to $\boldsymbol{f}$ are described by a scalar pitch value $h$ found by $$ h = \frac{ \boldsymbol{m} \cdot \boldsymbol{f}}{\boldsymbol{f} \cdot \boldsymbol{f}} $$ in reverse , a moment is defined by a force vector $\boldsymbol{f}$ passing through an axis located at $\boldsymbol{r}$ with pitch $h$ $$ \boldsymbol{m} = \boldsymbol{r} \times \boldsymbol{f} + h \boldsymbol{f} $$ have you noticed how difficult it is to apply a pure moment on a rigid body , without applying a force ? this is because you cannot have one without the other . a moment is really a result of the line of action of forces . so the scalar potential of a moment is really the same as the one for forces with $$ \boldsymbol{m} = - \boldsymbol{r} \times \nabla v - h \nabla v = -\left ( \left [ 1\right ] h + \boldsymbol{r}\times \right ) \nabla v $$ the problem is that in rigid body mechanics forces are not treated as scalar fields , but spatially constant , and temporally varying . furthermore , i cannot think of a case where spatially varying moments arise that are not due to a force at a distance . i suppose you can come up with a tensor pitch $h$ instead of a scalar which is spatially varying for a definition like $\boldsymbol{m} = -\left ( h + \boldsymbol{r}\times \right ) \nabla v$ , but then you will be making things up that do not have any physical meaning that i know of .
yes , subsystems of an entangled state – if this subsystem is entangled with the rest – is always in a mixed state or " statistical mixture " which is used as a synonym in your discussion ( or elsewhere ) . if we are only interested in predictions for a subsystem $a$ in a system composed of $a , b$ , then $a$ is described by a density matrix $\rho_a$ calculable by " tracing over " the indices of the hilbert space for $b$: $$\rho_a = {\rm tr}_{i_b} \rho_{ab}$$ note that if the whole system $ab$ is in a pure state , $$\rho_{ab}= |\psi_{ab}\rangle\langle \psi_{ab}| $$ if $\psi_{ab}$ is an entangled i.e. not separable state , i.e. if it cannot be written as $|\psi_a\rangle\otimes |\psi_b\rangle$ for any states $|\psi_a\rangle$ and $|\psi_b\rangle$ , then the tracing over has the effect of picking all the terms in $|\psi_{ab}\rangle$ , forgetting about their dependence on the $b$ degrees of freedom , and writing their probabilities on the diagonal of $\rho_{ab}$ . that is why the von neumann entropy will be nonzero – the density matrix will be a diagonal one in a basis and there will be at least two entries that are neither $0$ nor $1$ . take a system of two qubits . we have qubit $a$ and qubit $b$ . there are 4 natural basis vectors for the two qubits , $|00\rangle$ , $|01\rangle$ , $|10\rangle$ , and $|11\rangle$ where the first digit refers to the value of $a$ and the second digit to $b$ . a general pure state is a superposition of these four states with four coefficients $\alpha_{ab}$ where $a , b$ are $0,1$ , matched to the corresponding values . if $\alpha_{ab}$ may be written as $\beta_a\gamma_b$ i.e. factorized in this way , the pure state is separable . $|01\rangle$ is separable , for example . if it is not , then it is entangled . for example , $|00\rangle+|11\rangle$ is not separable so it is entangled . the mixed state is a more general state than a pure state . in this case , it is given by a $4\times 4$ hermitian matrix $\rho$ . the matrix entries are $\rho_{ab , a'b'}$ where the unprimed and primed indices refer to the values of qubits $ab$ in the bra and ket vectors , respectively . if these matrix entries may be factorized to $$\rho_{ab , a'b'} = \alpha^*_{ab}\alpha_{a'b'}$$ for some coefficients $\alpha_{a'b'}$ and their complex conjugates that specify a pure state $|\psi_{ab}\rangle$ , then the density matrix $\rho$ is equivalent to the pure state $|\psi_{ab}\rangle$ and we say that the system is in a pure state . in the more general case , $\rho$ can not be written as this factorized product but only as a sum of similar products . if you need at least two terms like that to write $\rho$ , then the state is mixed and the von neumann entropy is therefore nonzero .
we normally consider the various states of matter to be separated by a phase transition , and generally this is a first order phase transition ( an exception is the second order glass-liqid transition ) . so for example the solid to liquid transition is ( usually ) a first order phase transition , and likewise the liquid to gas transition . however if we move from the liquid to the supercritical fluid by increasing the temperature , as shown by the arrow in this diagram : then we measure neither a first or second phase transition . the system changes continuously . you had get a similar result by starting with the gas and increasing the pressure to move into the supercritical region . you will hear arguments about what constitutes a separate phase of matter , e.g. about plasmas or superfluid states , and i am sure someone somewhere will have referred to the supercritical fluid as a separate state . however there is no thermodynamic reason to do so .
here we will only consider the leading semi-classical approximation of a $1$-dimensional problem with hamiltonian $$ h ( x , p ) ~=~ \frac{p^2}{2m} + \phi ( x ) , $$ where $\phi ( x ) &lt ; 0$ is the potential function . let us for simplicity assume that the potential $\phi ( x ) =\phi ( -x ) $ is an even function and strongly monotonically increasing for $x\geq 0$ with limit $\lim_{|x|\to \infty}\phi ( x ) =0$ . let $e_n&lt ; 0$ denote the energy of the $n$'th bound state , ordered increasingly $e_1 &lt ; e_2&lt ; e_3&lt ; \ldots$ , so that $e_1$ denotes the ground state energy . there is also a positive continuous unbounded spectrum $e\geq0$ , which we shall not discuss further . we shall here construct a counterexample of two potentials $\phi^{\prime} ( x ) $ and $\phi^{\prime\prime} ( x ) $ such that the quotient of potentials satisfies the limit $$ \lim_{|x|\to \infty} \frac{\phi^{\prime\prime} ( x ) }{\phi^{\prime} ( x ) } ~=~1 , \qquad ( 1 ) $$ but where the corresponding quotient of bound state energies satisfies $$ \lim_{n\to \infty} \frac{e_n^{\prime\prime}}{e_n^{\prime}} ~\neq~1 . \qquad ( 2 ) $$ the idea is to seek for a potential $\phi$ that would generate a bound state spectrum of the form $$e_n= -r e^{-\mu n} , \qquad ( 3 ) $$ where $r&gt ; 0$ is a rydberg-like constant of dimension energy , and $\mu&gt ; 0$ is a dimensionless positive constant . [ the hydrogen atom is for comparison $e_n= -r/n^2$ . ] thus the number of states $n ( e ) $ below energy-level $e$ should roughly satisfy $$e~\approx~ -r e^{-\mu n ( e ) } \qquad \leftrightarrow \qquad n ( e ) ~\approx~\frac{1}{\mu}\ln ( -\frac{e}{r} ) . $$ this answer provides a semi-classical inversion formula for the potential $\phi$ that we will use . the length $\ell ( v ) $ of the classically accessible region of the potential well at potential energy-level $v$ becomes $$ 2\phi^{-1} ( v ) ~=~\ell ( v ) ~\approx ~\hbar\sqrt{\frac{2}{m}} \frac{d}{dv}\int_{v_{0}}^v \frac{n ( e ) ~de}{\sqrt{v-e}} $$ $$~\approx~\frac{\hbar}{\mu}\sqrt{\frac{2}{m}} \left ( \frac{2\arctan\sqrt{\frac{v_0}{v}-1}}{\sqrt{-v}} - \frac{\ln ( -\frac{v_0}{r} ) }{\sqrt{v-v_0}}\right ) . \qquad ( 4 ) $$ one may check that the accessible length function $\ell ( v ) $ is a monotonically increasing function for $v\in [ v_1,0 [ $ for some choice of the constants $v_0$ and $v_1$ with $v_0&lt ; v_1&lt ; 0$ . asymptotically , such potential $\phi ( x ) $ behaves as an inverse square potential $-c/x^2$ for $|x|\to\infty$ , where $c&gt ; 0$ is a positive constant . we now construct the potential functions $\phi^{\prime} ( x ) $ and $\phi^{\prime\prime} ( x ) $ such that they are given by formula ( 4 ) in the outer region $x\geq x_1$ , where $x_1:=\ell ( v_1 ) /2&gt ; 0$ ; and arbitrarily monotonically increasing in the inner region $0\leq x\leq x_1$ . this implies that the quotient of potentials satisfies $$ \frac{\phi^{\prime\prime} ( x ) }{\phi^{\prime} ( x ) } ~=~1 \qquad {\rm for}\qquad |x|\geq x_1 , $$ so that condition ( 1 ) is satisfied . for large enough states $n\geq n_1$ , after the inner potential well is filled , the spectrum $e_n$ eventually becomes exponentially a la ( 3 ) . we now choose the inner potentials $\phi^{\prime} ( x ) $ and $\phi^{\prime\prime} ( x ) $ such that there fits one more bound state into the profile $\phi^{\prime\prime} ( x ) $ than $\phi^{\prime} ( x ) $ for $|x|\leq x_1$ . then the labeling of states $e_{n+1}^{\prime\prime}\approx e_n^{\prime}$ would be off by one for $n\geq n_1$ , yielding the inequality ( 2 ) , $$ \frac{e_n^{\prime\prime}}{e_n^{\prime}} ~\approx~e^{\mu} ~\neq~1 . \qquad {\rm for}\qquad n\geq n_1 . $$
this question is sort of difficult to answer in an objective way , because it depends very strongly on your definition of " best . " natural selection favors traits which provide a reproductive advantage ; no more , no less . could our eyes be better by the standards of modern optical design , in terms of precision and features ? sure . i could easily design a camera with a larger aperture , better resolution , less abberation , a broader wavelength sensitivity , etc . there are even some precedents for this in nature . some animals can see ultraviolet or infrared ; some can even detect polarization ( mostly birds i think ? ) . i believe there are some fish that can swivel their eyes around through nearly a full circle in any direction . cats and dogs have higher sensitivity in low light due to the reflective layer behind their retina , and squid have eyes with truly huge apertures . it is worth pointing out though that the human eye , despite what i have said above , is still a pretty good camera . it can re-focus pretty quickly , it is got a fantastic image processing computer attached to it that can do incredible amounts of pattern recognition , noise reduction , and image stabilization . while its wavefront aberration is not perfect , it is pretty good for a biological system . they have very low chromatic aberration , and are quite compact . additionally , our eyes have a curved focal plane ( our retina ) which is a lens designers dream -- it eliminates the problem of field curvature , which is an inherent aberration in any optical system that is nearly impossible to eliminate .
good question . assume we have one cube of ice in a glass of water . the ice displaces some of that water , raising the height of the water by an amount we will call $h$ . archimedes principles states that the weight of water displaced will equal the upward buoyancy force provided by that water . in this case , $$\text{weight of water displaced} = m_\text{water displaced}g = \rho vg = \rho ahg$$ where $v$ is volume of water displaced , $\rho$ is density of water , $a$ is the surface area of the glass and $g$ is acceleration due to gravity . therefore the upward buoyancy force acting on the ice is $\rho ahg$ . now the downward weight of ice is $m_\text{ice}g$ . now because the ice is neither sinking nor floating , these must balance . that is : $$\rho ahg = m_\text{ice}g$$ therefore , $$h = \frac{m_\text{ice}}{\rho a}$$ now when the ice melts , this height difference due to buyoancy goes to 0 . but now an addition mass $m_\text{ice}$ of water has been added to the cup in the form of water . since mass is conserved , the mass of ice that has melted has been turned into an equivalent mass of water . the volume of such water added to the cup is thus : $$v = \frac{m_\text{ice}}{\rho}$$ and therefore , $$ah = \frac{m_\text{ice}}{\rho}$$ so , $$h = \frac{m_\text{ice}}{\rho a}$$ that is , the height the water has increased due to the melted ice is exactly the same as the height increase due to buoyancy before the ice had melted .
the electric field due a uniformly charged sphere with its radius increasing at a constant rate at any point outside the sphere is the same as if the radius were not changing at all . in this scenario , you can still invoke the radial symmetry argument and use the gauss 's law along with invariance of electric charge to obtain the electric field which turns out to be the same as that of a point charge at the centre of the sphere . any change in the rate of increase of radius of the sphere does not lead to a change in the electric field at any point farther from the sphere than the maximum radius the sphere can attain while it is pulsating . so the electric field at all points whose distance from the centre of the sphere is larger that the maximum radius that can be attained by the pulsating sphere is constant over time . so beyond the maximum attainable radius there is no electromagnetic radiation . however , i think there will be some electromagnetic radiation in the region between the minimum attainable radius and the maximum attainable radius . this could be in the form of standing waves confined to that region . at any point whose distance from the centre of the sphere is less than the minimum attainable radius , i think you can again apply the above mentioned logic and conclude that there is to electromagnetic radiation in that region . so , to conclude , i feel there should be no net electromagnetic radiation emanating from the sphere . p.s. i have not done a detailed study of electromagnetic radiation . so , i am not sure if the above analysis is correct . i would be glad if somebody were to post the correct analysis of the scenario in question .
yes , you can show this using only the fact that the clifford algebra has a unique representation up to similarity transformation in any dimension . this is shown in the first few pages of http://arxiv.org/pdf/hep-th/9811101.pdf then you observe that if $\gamma^\mu$ obeys the clifford algebra , then so does $- ( \gamma^\mu ) ^t$ . $\mathcal{c}$ is then defined as the similiarity transformation between the two representations , whose existence is guaranteed by the uniqueness of the representation of the clifford algebra .
in principle , the gravitational potential energy should be included into total internal energy , but in practice , most often it is not . i know of two reasons . because for systems that are discussed in thermodynamics , it is believed that gravitational energy is negligible compared to electromagnetic potential energy of the constituting particles ; because it is difficult to include $1/r^2$ forces such as electromagnetic or gravitational force to calculations based on standard statistical physics in a unique and convincing way .
as the voltage between the capacitor 's plates decreases , so should the current flowing through the circuit . i do not follow your reasoning here . recall that , for an ideal capacitor , we have : $$i_c = c\frac{dv_c}{dt}$$ in words , the current through the capacitor is proportional to the rate of change of the voltage across , not the instantaneous value of the voltage . so , for example , if the voltage across the capacitor is sinusoidal $$v_c = v \sin\omega t$$ the current is $$i_c = \omega v \cos \omega t$$ which means ( 1 ) that the maximum current ( magnitude ) occurs when the voltage is zero and ( 2 ) that the maximum voltage ( magnitude ) occurs when the current is zero . now , for this simple lc circuit , the voltage across the capacitor is identical to the voltage across the inductor : $$v_c = v_l$$ thus , $$i_c = c\frac{dv_l}{dt}$$ for an ideal inductor , we have : $$v_l = l\frac{di_l}{dt}$$ but , the inductor current is $$i_l = - i_c$$ thus , $$i_c = -lc\dfrac{d^2i_c}{dt^2}$$ which means that the current is sinusoidal $$i_c = a \sin \omega t + b \cos \omega t $$ where $$\omega = \frac{1}{\sqrt{lc}}$$ since , in your example , the initial current is zero and the initial voltage is $v$ , we have $$i_c ( t ) = -\frac{v}{\omega l} \sin \omega t$$
yes . higher frequencies are attenuated more over distance than lower frequencies are , which has a rounding effect on the square wave as the upper harmonics are reduced . reference do low frequency sounds really carry longer distances ?
it is easy to prove the formula if you just look at the individual basis vectors of the tensor product . let 's use $ ( 2j+1 ) $ and $ ( 2s+1 ) $ eigenvectors of $j^2 , j_z$ and $s^2 , s_z$ called $|j , j_z\rangle$ and so on . now let 's ask about the multiplicity of basis vectors of the tensor product with a given eigenvalue of $j_z = j_z+s_z$ . the maximum eigenvalue of $j_z$ in the tensor product is $j+s$: it can be obtained if we choose $$ |j , j_z=j+s\rangle = |j , j\rangle \otimes |s , s\rangle $$ there are no higher eigenvalues of $j_z$ ; this proves that no representation with $j&gt ; j+s$ is included in the tensor product . however , the $j=j+s$ representation must be included exactly once to obtain one basis vector with $j_z=j+s$ ; representations with lower values of $j&lt ; j+s$ would not contribute any vectors with $j_z=j+s$ . so the tensor product $$ rep ( j ) \otimes rep ( s ) = rep ( j+s ) \oplus rep ( rest ) $$ i have used the fact that reducible representations of simple compact groups may be written as direct sums . now , what about the remaining representation ( s ) $rep ( rest ) $ ? it is the linear envelope of a set of basis vectors in which we have already removed all the $ ( 2j+1 ) $ basis vectors with $j=j+s$ . well , in the rest , the maximum allowed $j$ is $j+s-1$ . from the original bases , we see that the original space was 2-dimensional : the old basis included $$|j , j-1\rangle \otimes |s , s\rangle , \qquad |j , j\rangle \otimes |s , s-1\rangle $$ but we have already included one combination to $rep ( j+s ) $ ; so the $rep ( rest ) $ representation only contains the other one . by the same argument as above , we may see that the multiplicity of $rep ( j+s-1 ) $ in the tensor product is also one . by induction , this algorithm may continue : at the beginning , the number of eigenvectors with a given $j_z$ is increasing by one every time we decrease $j_z$ by one . however , this behavior stops once we get to too low values of $j_z$ that would require too negative values of $j_z$ , either $j_z&lt ; -j$ or $s_z&lt ; -s$ . when that happens , the number of basis vectors no longer jumps by one ; it stays constant . it happens when $$j_z^{max} \equiv j = |j-s|$$ so $j=|j-s$ is the lowest-$j$ representation included in the decomposition of the tensor product . another way to see that at this moment , we have already written down all components , is either to notice that $j$ can not be smaller than $|j-s|$ because the minimum is obtained by adding " oppositely directed vectors " and can not be further shortened ; or , alternatively , we may check that the dimensions of your formula work : $$ ( 2j+1 ) ( 2s+1 ) = \sum_{j=|j-s|}^{ ( j+s ) } ( 2j+1 ) $$
i am not sure i understand what you do not understand . i am adding a answer since this would be too long for a comment . . . given the cosmological principle and the flat space time observation , the idea is that the flat spacetime is infinite , or at least very much larger than our horizon . so , yes , an observer that is , say 7 billion light years away from us would see parts of the universe that are beyond our horizon , and we would see parts of the universe beyond his horizon . there is no contradiction in this . this other observer at 7 billion years would , for example , also see a cmb with properties similar to ours but all the ripples would be different . it is true that this could also be a flat spacetime that has the topological property of a torus but that is not at all required . this would give a flat spacetime that is finite . do you think the universe cannot be infinite ? by the way , the cosmological principle is more than a principle , there is observational support for it . for example the cmb looks the same in all directions and the large scale structure of galaxy cluster seems uniform on the largest scales . edit : thinking about it more , you probably are thinking of the big bang as being one point ( or very small region ) in space at $t=0$ that then " explodes " into our universe . i had that same problem / misconception when i started learning about this . the problem is , we cannot say anything about t=0 of the big bang , because that would effectively be like a singularity filling all of space . instead think of $t=\epsilon$ , just very slightly after the big bang . at that time , you would have an infinite space filled with extremely high energy density at a very high temperature and the space would be expanding extremely rapidly . as time goes on from there the expansion rate slows , the density and temperature go down and you get the infinite universe we live in . does that help ?
this was an idea einstein had soon after developing gr , and it is developed in the classical unified field literature , with the starting point being einstein 's " do gravitational fields play a role in the constitution of the elementary particles ? " this is one of the defining program papers of the unified field framework . this idea is also discussed off and on within heuristic models of charges throughout the 1950s-80s . the essential points are that as you make the electron smaller , at some point , gravity will become dominant . the problem with such ideas is that they generally do not have a good idea of quantum gravity to make the microscopic model precise . all these classically inspired ideas are obsoleted by string theory and subsumed into it . within string theory , the fundamental objects are dual to black holes , so that their classical limit is identifiable as a recognizable extremally charged black hole of the classical limit supergravity theory . aside from identifiable black holes , there is no other matter ( arguably--- there is the question of whether orbifolds count as " matter" ) . so for example , for the m-theory , the objects are the extremally charged m2-branes which are the extremally charged black holes you can make using the 3 form gauge field , and their 5-brane magnetic duals ( the magnetically extremally charged black holes ) . that is it for m theory . the brane-spectrum of a theory is the answer to classical question " what extremal black hole can i form ? " the identification of black holes with matter is important , because the internal construction of strings is fully specified by the theory . so that the electron , if it is a string theory excitation , is an object whose internal structure is completely known , because you know the scattering off the electron at arbitrarily high energies . further , in the strong scattering case , we can continuously link the electron to both netural and extremally charged black holes , so that the theory is a full realization of einstein 's program . since i believe string theory is the correct theory of everything , i do not think there is much point in investigating these types of ideas in a different direction . but some people who like loops disagree .
it is a very hard problem . if magnetism behaved like gravity , you could break up the plane and magnet into small dipoles and sum up all of the dipole-dipole interaction energies for all possible pairs ( this would make a hex-tuple integral ! ) . the force is simply the rate of change of the total energy . however , you need to first calculate the induced magnetization of iron , which is complicated and has a " saturation " behavior . the paper : http://www.gris.uni-tuebingen.de/people/staff/spabst/magnets/magnets_in_motion.html lays out the needed calculations . of course , the cylindrical symmetry will reduce the calculations needed .
think about a 2d fermi surface ( or a 2d section of a 3d fermi surface ) . now look in the extended brillouin zone - thats the one where you take many copies of the first brillouin zone and use it to tile the plane . now we are in 2d so the fermi surface is a bunch of curves in the extended bz . there are three possibilities about the shape of the shape of this curve . your fermi surface forms closed loops that enclose occupied electron states . this is an electronlike orbit . your fermi surface froms closed loops that enclose unoccupied electron states . this is a holelike orbit . your fermi does not form closed loops . this is an open orbit . the reason for these names is that we are thinking of applying a perpendicular magnetic field $b_z$ . in that case we have $$\frac{\partial k}{\partial t} = v_k \times b_z , $$ where the group velocity $v_k = \nabla_k e ( k ) $ . since $v_k$ is pointed in the opposite direction of $k$ in holelike orbits , the electrons orbits in the " wrong direction " . that is it orbits in the direction that , naively , a positively charged particle would orbit in . hence it is called holelike .
it will work just as well provided the part of the coil making contact with the supports is not insulated . the rest of it can be insulated . the important property is that current has the ability to run through the coil because it is this current that experiences a force due to the magnetic field of the permanent magnets causing the coil to rotate . for a bit more detail and pictures : http://hyperphysics.phy-astr.gsu.edu/hbase/magnetic/mothow.html
you did net work on the body . gravity did negative net work on the body . the over all work done was zero . the original confusion arose because the work-energy theorem demands we calculate the change in kinetic energy using the net force on the body , but your question considered only the force exerted by you , and ignored that exerted by gravity . if the body has mass $m$ , you were putting a force $mg$ on it to raise it at constant speed . the work-energy theorem says that if you had done this when there were no other forces on the body , the body would have gained kinetic energy $mgh$ as you moved it from the floor to the table . that analysis ignores gravity , though . gravity pulled down on the box with force $-mg$ . this means the work done by gravity was $-mgh$ , and so the total work done on the box was zero . this makes sense because if you lift the box at constant speed , the net force on the box is zero by $f = ma$ . if the box starts by sitting stationary on the floor , there would have to be some small net work done on the box to get it started going up . there would have to be some small negative net work on it to get it to stop .
false . to quote from the specific section in the list of common misconceptions : glass does not flow at room temperature as a high-viscosity liquid . although glass shares some molecular properties found in liquids , glass at room temperature is an " amorphous solid " that only begins to flow above the glass transition temperature , though the exact nature of the glass transition is not considered settled among theorists and scientists . panes of stained glass windows are often thicker at the bottom than at the top , and this has been cited as an example of the slow flow of glass over centuries . however , this unevenness is due to the window manufacturing processes used at the time . normally the thick end of glass would be installed at the bottom of the frame , but it is also common to find old windows where the thicker end has been installed to the sides or the top . no such distortion is observed in other glass objects , such as sculptures or optical instruments , that are of similar or even greater age . one researcher estimated in 1998 that for glass to actually " flow " at room temperatures would take many times the age of the earth .
the most physical and understandable definition of nekrasov 's partition function to me uses five-dimensional gauge theories . namely , any 4d n=2 susy gauge theory has a 5d version with the same matter content , so that compactifying it on a small $s^1$ brings it back to the original 4d theory . then we put the theory on the so-called omega background : it is $\mathbb{r}^4 \times [ 0 , \beta ] $ , but $ ( \vec{x} , 0 ) $ and $ ( \vec{x'} , \beta ) $ are identified by a rotation $$ \vec x'=\begin{pmatrix} \cos \beta\epsilon_1 and \sin\beta\epsilon_1 and 0 and 0\\ -\sin \beta\epsilon_1 and \cos\beta\epsilon_1 and 0 and 0\\ 0 and 0 and \cos \beta\epsilon_2 and \sin\beta\epsilon_2\\ 0 and 0 and -\sin \beta\epsilon_2 and \cos\beta\epsilon_2 \end{pmatrix}\vec x . $$ then we take the limit $\beta\to 0$ , keeping $\epsilon_{1,2}$ fixed . ( strictly speaking we also need to add a background $su ( 2 ) _r$ symmetry gauge field , so that some of the susy is preserved . ) most of what nekrasov did using his cohomological framework can be seen directly in this higher-dimensional setup . see e.g. sec . 3.2 of my review article in preparation , available here .
the comparison is viable , here 's why : let 's choose the positive $x$-direction to point upward , perpendicular to the water 's surface . by archimedes ' principle , the magnitude of the buoyant force on an object of volume $v$ equals the weight of the displaced water ; $f_b = \rho_w v g$ where $\rho_w$ here denotes the density of water . the buoyant force points in the positive $x$-direction . in addition to the buoyant force , an object submerged in water will experience a gravitational force $f_g = \rho v g$ pointing in the negative $x$-direction , where $\rho$ is the density of the object . it follows that the net vertical force on the water is $f_x = f_b - f_g = ( \rho_w-\rho ) v g$ let 's assume that the object is rigid , so that its volume changes negligibly with depth , and let 's make the approximation that the density of water varies negligibly as a function of depth as well , then the work done in moving such an object to a depth $d$ below the water 's surface along the $x$-axis will simply be $w = f\delta x = ( \rho_w-\rho ) v g ( 0-d ) = \boxed{ ( \rho-\rho_w ) v g d}$ notice how similar this looks to $mgd$ , the change in potential energy of moving an object upwards under the influence of gravity by an amount $d$ . in fact , we could think of the water as effectively decreasing the mass of the object so that it has an effective mass $m_\mathrm{eff} = ( \rho- \rho_w ) v$ , and then the analogy becomes clear an important caveat to all of this is that water is viscous , so they will be an additional drag force you have to contend with that will change the answer in the event that you submerge the object quickly . however , by moving the object sufficiently slowly , you can make this contribution as small as you had like . you can also relax the constant water density and constant object volume assumptions if you know how these things change with depth . if you had like more details on this , then let me know ! cheers !
i not sure how you obtained the last expression . the standard sommerfeld expansion ( for details , see e.g. ashcroft and mermin ) gives a slightly different result , which is $$ e_{f} \approx \mu\left [ 1+\frac{\pi^{2}}{8}\left ( \frac{k_{b}t}{\mu}\right ) ^{2}\ , \right ] ^{2/3} \approx \mu\left [ 1+\frac{\pi^{2}}{12}\left ( \frac{k_{b}t}{\mu}\right ) ^{2}\right ] $$ to leading non-trivial order in $k_{b}t/\mu$ , i.e. , $o\big ( ( k_{b}t/\mu ) ^{2}\big ) $ . ( i have set $e_{0}=0$ in the expression from wikipedia . ) we can invert this relation by substituting $\mu = e_{f}\left [ 1 + c_{2} ( k_{b}t/e_{f} ) ^{2} + \cdots \right ] $ into the above . that is , $$ e_{f} = e_{f}\left [ 1 + c_{2} ( k_{b}t/e_{f} ) ^{2} + \cdots \right ] \left\{1+\frac{\pi^{2}}{12}\left ( \frac{k_{b}t}{e_{f}}\right ) ^{2}\left [ 1 + c_{2} ( k_{b}t/e_{f} ) ^{2} + \cdots\right ] ^{-2}\right\} . $$ comparing the zeroth order terms in $k_{b}t/e_{f}$ on both sides of the above equation , we simply obtain $e_{f}=e_{f}$ . comparing the second order terms , we have $0 = \frac{\pi^{2}}{12} + c_{2}$ . hence $$ \mu = e_{f}\left [ 1-\frac{\pi^{2}}{12}\left ( \frac{k_{b}t}{e_{f}}\right ) ^{2}\right ] $$ up to $o\big ( ( k_{b}t/e_{f} ) ^{2}\big ) $ . to determine the next-order correction to $\mu$ , you should include one higher order term in the sommerfeld expansion , which gives $$ e_{f} \approx \mu\left [ 1+\frac{\pi^{2}}{8}\left ( \frac{k_{b}t}{\mu}\right ) ^{2}+\frac{7\pi^{4}}{640}\left ( \frac{k_{b}t}{\mu}\right ) ^{4}\ , \right ] ^{2/3} . $$ expanding this up to $o\big ( ( k_{b}t/\mu ) ^{4}\big ) $ , then substituting $\mu = e_{f}\left [ 1 + c_{2} ( k_{b}t/e_{f} ) ^{2} + c_{4} ( k_{b}t/e_{f} ) ^{4}+\cdots\right ] $ ( where we have already determined $c_{2}$ before ) into it , and then matching the coefficients of both sides up to $o\big ( ( k_{b}t/e_{f} ) ^{4}\big ) $ will lead to the desired result .
note : i have tried to make my answer a little more general , with detail , so that it will be useful for more people . the question is what boundary conditions do we apply to our wavefunction either side of a dirac delta function ? in your example we have the potential $$v ( x ) =\begin{cases}\infty and \text{ if } x &lt ; 0\\ \alpha~\delta ( x-a ) and \text{ if } x \geq 0 \end{cases}$$ we are interested in the boundary conditions either side of $x=a$ . what information do we have ? well , due to the probabilistic interpretation of the wavefunction we require continuity of the wavefunction . that is , our first boundary condition is $$ ( 1 ) ~~~~~~~~~~~~\boxed{\psi_{-} ( a ) = \psi_{+} ( a ) }$$ where the $\pm$ subscripts represent the right and left sides of $x=a$ respectively . what other conditions can we set ? well , usually we would ask that the first derivative is also matched either side of $x=a$ ( you should be asking yourself why do we do this ? ) , but in this case this is not the right condition to impose . let 's see why . where does the boundary condition on $\frac{\partial \psi}{\partial x}$ come from ? our wavefunction is a solution to the 1-d time-independent schrödinger equation : $$h~\psi ( x ) = -\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\psi ( x ) +v ( x ) \psi ( x ) = e~\psi ( x ) $$ looking at this equation , we see that we can get a boundary condition on $\frac{\partial \psi}{\partial x}$ at any point $a$ by integrating it w.r. t $x$ over the region $ [ a-\epsilon , a+\epsilon ] $ , taking $\epsilon\rightarrow 0$: $$\lim_{\epsilon\rightarrow 0}\left [ -\int^{a+\epsilon}_{a-\epsilon}dx\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\psi ( x ) +\int^{a+\epsilon}_{a-\epsilon}dx~v ( x ) \psi ( x ) \right ] = e~\lim_{\epsilon\rightarrow 0}~\int^{a+\epsilon}_{a-\epsilon}dx~\psi ( x ) \\ \implies -\frac{\hbar^2}{2m}\left [ \frac{\partial \psi_{+} ( a ) }{\partial x}-\frac{\partial \psi_{-} ( a ) }{\partial x}\right ] + \lim_{\epsilon\rightarrow 0}\int^{a+\epsilon}_{a-\epsilon}dx~v ( x ) \psi ( x ) = 0$$ where we have used the continuity of $\psi$ to evaluate the rhs as zero . note that for any $v ( x ) $ the second term does not necessarily vanish . but , if $v ( x ) $ is continuous , this term will vanish for the same reason the rhs did , and in those cases we yield the usual boundary condition $$\boxed{\frac{\partial \psi_{+} ( a ) }{\partial x}=\frac{\partial \psi_{-} ( a ) }{\partial x}}~~~~~~~ ( \mbox{when $v ( a ) $ is continuous at $x=a$} ) $$ we note that the for example given here , $v ( x ) $ is definitely not continuous at $x=a$ , where it diverges to infinity . so this second term does not vanish , in fact $$\lim_{\epsilon\rightarrow 0}\int^{a+\epsilon}_{a-\epsilon}dx~v ( x ) \psi ( x ) = \alpha\lim_{\epsilon\rightarrow 0}\int^{a+\epsilon}_{a-\epsilon}dx~\delta ( x-a ) \psi ( x ) = \alpha~\lim_{\epsilon\rightarrow 0}~\psi ( a ) = \alpha~\psi ( a ) $$ so rearranging our results , the second boundary condition for the problem is $$ ( 2 ) ~~~~~~~~~~~~\boxed{\frac{\partial \psi_{+} ( a ) }{\partial x}-\frac{\partial \psi_{-} ( a ) }{\partial x} = \frac{2m\alpha}{\hbar^2}\psi ( a ) }$$ explicitly using the form of your wavefunctions ( using $b=-a$ to eliminate $b$ ) $$\psi_{+} ( x ) = fe^{ikx} , ~~~~~~\psi_{-} ( x ) = a\left ( e^{ikx}-e^{-ikx}\right ) $$ this boundary condition becomes $$\boxed{ikfe^{ika}-ik\left ( ae^{ika}+e^{-ika}\right ) = \frac{2m\alpha}{\hbar^2}\psi ( a ) }$$ as required .
a falling tree is basically an inverted pendulum . the period of a pendulum of length $h$ for small oscillations is $2\pi \sqrt{h/g}$ , with $g$ the acceleration due to gravity , about $10\ m/s$ . for an inverted pendulum near the top of its arc , there is no period , but the quantity $\sqrt{h/g}$ does represent a characteristic time scale for this system . the tree will take a few of these characteristic times to fall . $h$ for a tree is an " effective height " , and depends on the mass distribution of the tree . if all the mass is at the top , $h$ is the height of the tree . if the tree is uniform , $h$ is $2/3$ the true height . for a tree with $h = 40\ m$ , the characteristic time is $2\ s$ . for small angles , the angle the tree makes with the vertical will be multiplied by $e$ in this time . let 's start the tree at $1^{\circ}$ so that it needs to multiply its angle by $90$ to fall . $\ln ( 90 ) = 4.5$ so the tree takes about $9\ s$ to fall . this is mathematically an underestimate because the characteristic time increases slightly as the tree falls , but not too much . give it a nice round $10\ s$ and you get something that matches the first youtube video i found .
solar photons arrive to the earth about 500 seconds after leaving the photosphere . however , the very energetic photons created in the sun 's core take many millions of years to arrive on earth as they traverse the radiation and convection zones before arriving at the photosphere .
i do not know why physicists are interested in complex network theory , but well , whenever you can create a physical model describing some behavior you could call it " physics " ( econophysics , sociophysics , etc ) , and this is likely the reason why they study complex network . i will just answer the 2nd part of the question — the concept of small-world network . short average distance the small-world phenomena is best known as the six degree of separation , i.e. two persons are related to each other by at most 6 steps in a network of human relationship . in this network , people are represented by nodes , and if two people knows each other directly we create a link between the two nodes . if there are two nodes without a direct link , we could possibly take other routes . the smallest number of links need to be traversed is called the distance ( a . k.a. path length ) between the two nodes . a small-world network ( with size n ) needs to satisfy a condition , that the average distance $\langle\ell\rangle$ over all possible node pairs is " short " , i.e. $ \langle\ell\rangle \sim \log n $ . many natural and artificial complex networks , like the human relationship network above , neural network , metabolic networks etc . have short average distance , which is why this property is being studied , because physicists wanted to have a model that describes most of these complex networks . the erdős-rényi model — which is essentially a network that the links are constructed with some constant probability — has the short average distance property . high clustering coefficient the er model was one of the first models used to describe real-world complex networks , but was soon found to be insufficient because it does not have another important property — a " high " clustering coefficient . the ( local ) clustering coefficient was introduced by duncan j . watts and steven strogatz in 1998 as a measure of how well nodes are " clustered " together locally . it is defined as $$ c_{v} = \frac{\text{number of triangles containing }v}{\frac{k_v ( k_v-1 ) }2} $$ where $k_v$ is the number of links connected to $v$ , i.e. its degree . the local clustering coefficient is actually a ratio of " neighbors of $v$ that knows each other " , and " maximum number of potential neighbors of $v$ that can know each other " . for example , in this graph : the clustering coefficient of the central green node is $$ c = \frac{\text{number of triangles}}{\frac{k ( k-1 ) }2} = \frac{4}{\frac{5 ( 5-1 ) }2} = 0.4 . $$ if the local clustering coefficient is 0 , then all your friends do not know each other , which is not true in the social network . the expected behavior is a large clustering coefficient , where groups of your friends know each other ( thus forming triangles , i.e. a knows b , b knows c , c knows a ) . this property is where the er model breaks down — its clustering coefficient is close to zero with the same number of nodes and links with a real network . when a network has both short average distance and high average local clustering coefficient , we call it a small-world network . and beyond the watts-strogatz model was invented to address the small-world property . however , it was soon determined that even the ws model is not good enough as it is not scale-free . and then the barabási-albert model was created to describe why real-world networks both small-world and scale-free , although it also cannot explain other properties like the clustering coefficient distribution , hierarchical structure , etc , and of course more and more sophisticated models are proposed as well . in the end , these properties are studied to construct a universal model that describes all ( if not possible , " most" ) real-world complex networks , and use it to test and improve behaviors such as error and attack tolerance , evolution dynamics etc . if you do not fear a lot of mathematics , the 2002 review paper statistical mechanics of complex networks by réka albert and albert-lászló barabási is a ( imo ) must-read classic for anyone beginning to study complex networks . all of the above can be found in this paper .
you are sloppy with the $\delta$ 's . $\delta v$ is not equal to $s/t$ , even if $a$ is constant ; it is equal to $\delta s/\delta t$ . this is where your factor of 2 trouble lies . for constant acceleration ( and $s ( 0 ) =v ( 0 ) =0$ ) you have $s=\frac{1}{2}at^2$ . now simply compare the two expressions : $$ \frac{s}{t} = \frac{\frac{1}{2}at^2}{t} = \frac{1}{2}at \ , $$ whereas $$ \frac{\delta s}{\delta t} = \frac{s ( t+\delta t ) -s ( t ) }{\delta t} = \frac{\frac{1}{2}a ( t+\delta t ) ^2-\frac{1}{2}at^2}{\delta t}=\frac{at\delta t+\frac{1}{2}a\delta t^2}{\delta t} \approx at \ , $$ where the last "$\approx$" holds for small $\delta t$ . the second expression is the correct one for the velocity . the first one is wrong ( it only holds for constant velocity and $s ( 0 ) =0$ ) .
although emilio 's answer is insightful , i do not think it directly answers your question . i will attempt to do that here . this answer proceeds in two parts : we will show that the operator you try to write down is hermitian with appropriate domain , but that it is not self-adjoint and has no self-adjoint extensions . we will show that your formal manipulations have errors . part 1 . we set $\hbar=1$ for convenience throughout , and let $s ( \mathbb r ) $ denote schwartz space . recall that the $l^2 ( \mathbb r ) $ inner product is defined as follows : \begin{align} \langle \psi , \phi\rangle = \int_{-\infty}^{\infty} dx\ , \psi^* ( x ) \phi ( x ) \end{align} we are going to use the following definition which appears on page 138 of reed and simon 's methods of modern mathematical physics volume ii ( fourier analysis , self-adjointness ) : definition . for a symmetric operator $a$ , we define its deficiency indices by \begin{align} n_+ ( a ) and = \dim\mathrm{ker} ( ii-a^\dagger ) \\ n_- ( a ) and = \dim\mathrm{ker} ( ii+a^\dagger ) \\ \end{align} we are also going to need the following result which is part of a corollary on page 141 of reed and simon : lemma . let $a$ be a closed hermitian operator with deficiency indices $n_+ ( a ) $ and $n_- ( a ) $ , then $a$ is self-adjoint if and only if $n_+ ( a ) = 0 = n_- ( a ) $ , and $a$ has at least one self-adjoint extension if and only of $n_+ ( a ) = n_- ( a ) $ . with this lemma , we can prove the following claim . what we are going to prove here tells us that there is no way to define $h$ self-adjoint operator on some domain in $l^2 ( \mathbb r ) $ . claim . the operator $h$ with domain $d ( a ) = s ( \mathbb r ) $ defined by \begin{align} h\psi ( x ) = -ix^3\frac{d\psi}{dx} ( x ) -i \frac{d}{dx} ( x^3\psi ( x ) ) \end{align} is closed and hermitian but not self-adjoint . furthermore , $h$ has no self-adjoint extensions on $l^2 ( \mathbb r ) $ . proof . we will show that $a$ is hermitian and closed but that $n_- ( h ) = 1$ while $n_+ ( h ) = 0$ . the desired result then follows immediately from the lemma . to show that $h$ is hermitian , it suffices to show that $\langle \psi , h\phi\rangle = \langle h\psi , \phi\rangle$ for all $\phi , \psi\in d ( h ) = s ( \mathbb r ) $ . we have \begin{align} \langle \psi , h\phi\rangle and = \int_{-\infty}^\infty dx\ , \psi^*\left ( -ix^3\frac{d\phi}{dx} -i \frac{d}{dx} ( x^3\phi ) \right ) \\ and = -2i\psi^*x^3\phi\big|_{-\infty}^\infty +i\int_{-\infty}^\infty dx\left ( \frac{d}{dx} ( \psi^*x^3 ) +\frac{d\psi^*}{dx} ( x ) x^3\right ) \phi \\ and = \int_{-\infty}^\infty dx\left ( -i\frac{d}{dx} ( \psi ( x ) x^3 ) -i\frac{d\psi}{dx}x^3\right ) ^*\phi \\ and = \langle h\psi , \phi\rangle . \end{align} the boundary term in the second equality vanished because $\phi$ is rapidly decreasing . this operator is closed ( admittedly this is actually something i have not been able to prove ) . because $h$ is hermitian , its adjoint $h^\dagger$ has the same action on elements of its domain as $h$ itself . moreover , let $d'$ the set of all $\psi\in l^2 ( \mathbb r ) $ for which $h\psi$ is well-defined and also an element of $l^2 ( \mathbb r ) $ . then the computation be performed above to demonstrate hermiticity shows that if $\phi\in d'$ , then $\langle\phi , h\psi\rangle = \langle h\phi , \psi\rangle$ for all $\psi\in d ( h ) $ , so $d ( h^\dagger ) = d'$ . in particular , this domain is larger than that of $h$ which is therefore not self-adjoint . now if $\psi\in \mathrm{ker} ( ii - a^\dagger ) $ , then $\psi$ obeys the following differential equation : \begin{align} i\psi - \left ( -ix^3\frac{d\psi}{dx} -i \frac{d}{dx} ( x^3\psi ) \right ) =0 \end{align} this differential equation can be simplified to give \begin{align} ( 1+3x^2 ) \psi + 2x^3\frac{d\psi}{dx} =0 \end{align} for $x&gt ; 0$ and $x&lt ; 0$ , we can separate variables and integrate to solve this differential equation . the result is ( i used mathematica for this ) \begin{align} \psi_&gt ; ( x ) and = \frac{e^{1/ ( 4x^2 ) }}{x^{3/2}} + c_&gt ; \\ \psi_&lt ; ( x ) and = \frac{e^{1/ ( 4x^2 ) }}{ ( -x ) ^{3/2}} + c_&lt ; \end{align} these solutions both diverge at the origin , so our differential equation does not yield an $l^2 ( \mathbb r ) $ solution . this gives $\mathrm{ker} ( ii - a^\dagger ) = \{0\}$ so $n_+ ( h ) = 0$ . on the other hand , if $\psi\in \mathrm{ker} ( ii + a^\dagger ) $ then \begin{align} i\psi + \left ( -ix^3\frac{d\psi}{dx} -i \frac{d}{dx} ( x^3\psi ) \right ) =0 \end{align} this differential equation can be simplified to give \begin{align} ( 1-3x^2 ) \psi - 2x^3\frac{d\psi}{dx} =0 \end{align} which admits the normalized solution \begin{align} \psi ( x ) =\left\{ \begin{array}{cc} \frac{1}{\sqrt{2}}\frac{1}{|x|^{3/2}}e^{-1/ ( 4x^2 ) } and , x\neq 0 \\ 0 and , x=0 \end{array}\right . \end{align} in fact , this is ( up to normalization ) exactly the function you wrote down in the original question statement . this function is in $d ( h^\dagger ) $ . it follows that $\mathrm{ker} ( ii + a^\dagger ) = \mathrm{span}\{\psi_1\}$ so that $n_- ( h ) = 1$ , as desired $\blacksquare$ . part 2 . as for you manipulations , even if you were to enlarge the domain of $h$ to include $\psi_\lambda$ , they would be wrong . notice , for example , that you got a nonzero boundary term in the following computation : \begin{align} \langle \psi_\lambda , h\psi_\lambda\rangle and = \int_{-\infty}^\infty dx\ , \psi_\lambda^*\left ( -ix^3\frac{d\psi_\lambda}{dx} -i \frac{d}{dx} ( x^3\psi_\lambda ) \right ) \\ and = -2i\psi_\lambda^*x^3\psi_\lambda\big|_{-\infty}^\infty +i\int_{-\infty}^\infty dx\left ( \frac{d}{dx} ( \psi_\lambda^*x^3 ) +\frac{d\psi_\lambda^*}{dx} ( x ) x^3\right ) \psi_\lambda \\ and = -2i\ , \mathrm{sgn} ( x ) e^{-\lambda/ ( 2x^2 ) }\big|_{-\infty}^\infty +i\int_{-\infty}^\infty dx\left ( \frac{d}{dx} ( \psi_\lambda^*x^3 ) +\frac{d\psi_\lambda^*}{dx} ( x ) x^3\right ) \psi_\lambda \\ and = -4i +\int_{-\infty}^\infty dx\left ( -i\frac{d}{dx} ( \psi ( x ) x^3 ) -i\frac{d\psi}{dx}x^3\right ) ^*\psi_\lambda \\ and = -4i+\langle h\psi_\lambda , \psi_\lambda\rangle \end{align} which we could have seen more easily by simply noting that \begin{align} \langle\psi_\lambda , \psi_\lambda\rangle = \frac{2}{\lambda} \end{align} which gives \begin{align} \langle \psi_\lambda , h\psi_\lambda\rangle and = -i\lambda \langle\psi_\lambda , \psi_\lambda\rangle = -2i \\ \langle h\psi_\lambda , \psi_\lambda\rangle and = ( -i\lambda ) ^* \langle\psi_\lambda , \psi_\lambda\rangle = 2i \end{align} in particular , both of these computations show that \begin{align} \langle \psi_\lambda , h\psi_\lambda\rangle \neq \langle h\psi_\lambda , \psi_\lambda\rangle \end{align} in contradiction with what you claim .
the electron motion does feed back to the oscillator , but that is another diagram , known as the bubble diagram , in which you calculate the self-energy correction of the oscillator . that self-energy presumably contains imaginary part , which is then interpreted as the damping of the oscillator . you can either calculate the self-energy corrections self-consistently , or you can simply neglect it in the weak coupling limit away from the non-fermi-liquid criticality . the momentum $q$ should be the momentum of the phonons ( quanta of the oscillator motion ) . the potential $f ( r ) $ also carries a momentum , but this momentum is on the vertex ( where the electron emits/absorbs the phonon ) . more precisely , due to the presence of the potential $f ( r ) $ , momentum is not conserved on the vertex , the non-conserved amount of momentum is provided by the potential scattering .
i do not think there is a lower bound . set $x$ horizontal , $y$ vertical , $ ( 0,0 ) $ at the center of the circle . we will find $x$ and $y$ for the point of impact . say the ball takes time $t$ to cross the cylinder . it falls a distance $\frac{1}{2} g t^2$ , so $y = -\frac{1}{2}gt^2$ . we also have $x = vt - r$ , or $t = ( x+r ) /v$ . using the fact that for a circle $dy/dx = -x/y$ , the slope of the wall where it hits is $\frac{2x}{gt^2} = \frac{2xv^2}{g ( x+r ) ^2}$ . the y-velocity of the ball at impact is $-gt$ . the ball 's trajectory 's slope at impact is $v_y/v = \frac{-gt}{v} = -\frac{g ( x+r ) }{v^2}$ . multiplying the slopes gives $\frac{-2x}{x+r}$ . since $0&lt ; x&lt ; r$ for large $v$ , we see that the slopes multiply to something $0&gt ; product&gt ; -1$ . that is , the slope of the ball 's trajectory is not quite perpendicular to the wall . instead , it is a little shallower than that . that means the ball bounces back up off the wall at a slightly higher angle than it hit the wall , and therefore bounces right back out almost the way it came , but slightly higher , in the limit of high $v$ . having the ball come off the wall at a higher angle clearly makes it bounce out of the cylinder in this limit . that means there is no minimum time , since the time inside shrinks as $v$ increases .
followup to my comment : take a look at the table here http://hyperphysics.phy-astr.gsu.edu/hbase/starlog/staspe.html . ignoring for the moment the spectral absorption lines , which should not matter much to the mark-i human eyeball , use the nominal temperatures of each star class in combination with the standard planck distribution curves for a black body radiator , and that'll give you some idea . now , that said , the aforementioned eyeball is very good at doing color balance correction ( which is why we perceive objects to have about the same color in broad daylight , near sunset , and under incandescent lamps ) , so it may turn out that the sky will just look " blue " even tho your a- or b- class star is a lot " bluer " than sol .
it sounds like you feel the discharge when touching a conductive object after having picked up some static electricity . this is common , and the object does not need to grounded . your body is a conductor , and as such has some capacitance to its surroundings . when you shuffle accross a polyester carpet or something , this capacitor gets charged up . since the other side of the capacitor is largely connected to ground , touching a well grounded object , like a metal water pipe dug into the ground , will provide a good discharge path . other sufficiently large objects will do too , but a grounded object will usually have a larger result because most of your capacitance is with respect to ground . grabbing a ground object hard with your hands does not change this at all . what it does is provide so much other touch sensation that the zap feeling can get lost in the noise . in other words , it does not reduce the discharge , only makes it less likely you will feel it by increasing noise and therefore decreasing the signal to noise ratio . to not get zapped to the level you can feel it , you can discharge yourself slowly before touching a grounded object . a good way to do this is to touch some object with high resistivity , but not so high that static charges do not bleed off after a few seconds . a wooden door is often good for this . wood has high resistivity , but due to some inevitable moisture , it allows enough current to flow so that a static charge can not be maintained on it for very long . as a experiment , find a way to reliably charge youself up . this might be shuffling accross the right carpet with the right kind of socks on a dry day , for example . once you find a good method , you can reliably touch something grounded and feel a zap . now try touching that same object first thru a 10 m&omega ; resistor or something . with a high enough resistance , the discharge current is so low that you can not feel it . but , your body capacitor is still discharged , so when you then touch ground directly the charge is gone and there is no zap . for example , if your body forms a 10 pf capacitor to ground and you discharge it thru a 10 m&omega ; resistor , the time constant is 100 &micro ; s , so you are still discharged instantly on a human scale .
the energy density of the cosmic microwave background ( cmb ) is $4.19 \times 10^{-14} j/m^3$ . this value can be calculated from the cmb temperature being 2.728 k and $energy$ $density = at^4$ where $a$ is the radiation constant . you can multiply the energy density by your volume of interest such as that of the observable universe if you want total energy .
the difference lies in the way we count the number of states of the system in quantum and classical cases . the formulas you wrote are actually for the grand canonical partition functions for a single energy state , not for the whole system including all the energy states . the total grand canonical partition function is $$\mathcal{z} = \sum_{all\ states}{e^{-\beta ( e-n\mu ) }} = \sum_{n=0}^\infty\sum_{\{e\}}{e^{-\beta ( e-n\mu ) }}$$ now , if the particles are bosons , then the energy eigenstates are countable as $\{\epsilon_i\}$ and $\mathcal{z}$ would be $$\mathcal{z} = \sum_{\{n_i\}}e^{-\beta\sum_{i}{n_i ( \epsilon_i-\mu ) }} = \sum_{\{n_i\}}\prod_ie^{-\beta n_i ( \epsilon_i-\mu ) }=\prod_i \mathcal{z_i}^{b-e}$$ where $n_i$ is the number of particles in $i$-th energy state , thus $\sum_i{n_i}=n$ , and $$\mathcal{z_i}^{b-e} = \sum_{n=0}^\infty e^{-n\beta ( \epsilon_i-\mu ) }$$ here , $\mathcal{z_i}^{b-e}$ is the grand canonical partition function for one energy eigenstate with energy $\epsilon_i$ in bose-einstein statistics . on the other hand , in classical regime the energy of a particle take any energy . in this case , one point in the $6n$-dimensional phase space denotes one state of system . therefore , the energy states are not countable as there is an infinite number of points in the phase space within any phase space volume . to count the states we take the " semi-classical " approach by taking the phase space volume of one state of the system to be $ ( 2\pi\hbar ) ^{3n}$ . we can then integrate over the whole phase space and divide the integral by this unit volume to get the number of states . however , as the particles are assumed to be indistinguishable , any permutation of the system configuration ( the set of $\{\vec{x}_n , \ \vec{p}_n\}$ ) would actually be the same state of the system . therefore , when we integrate over the whole phase space volume we overcount the total number of states by $n ! $ . that is why we need to divide the integral by gibbs factor $n ! $ . for a system of non-interacting particles , the n-particle canonical partition function then can be written as $z_n = \frac{z_1^n}{n ! }$ where $z_1$ is the canonical partition function for one particle . now , the grand canonical partition function for a classical system would be $$\begin{align} \mathcal{z} and =\sum_{n=0}^\infty{\int_0^\infty de\ \omega ( e , n ) e^{-\beta ( e-\mu n ) }} =\sum_{n=0}^\infty e^{\beta\mu n} z_n = \sum_{n=0}^\infty e^{\beta\mu n} \frac{z_1^n}{n ! }\\ \end{align}$$ if we want to derive the partition function $\mathcal{z_i}$ for a single energy state similar to the bose-einstein statistics , we can assume the energy of a single particle to be discrete and countable as $\{\epsilon_i\}$ . this can be achieved by dividing the one particle phase space into s of unit volume and assigning one representative energy to every unit volume section . then , the grand canonical partition function is , $$ \begin{align} \mathcal{z} and = \sum_{n=0}^\infty \frac{e^{\beta\mu n}}{n ! } ( \sum_i e^{-\beta\epsilon_i} ) ^n\\ and =\sum_{n=0}^\infty \frac{e^{\beta\mu n}}{n ! } \sum_{\{n_i\} , \sum n_i = n} \frac{n ! }{\prod_i n_i ! } e^{-\beta\sum_i n_i\epsilon_i}\\ and =\sum_{\{n_i\}}\prod_i \frac{1}{n_i ! } e^{-\beta n_i ( \epsilon_i-\mu ) }\\ and =\prod_i \sum_{n=0}^\infty \frac{1}{n ! } e^{-\beta n ( \epsilon_i-\mu ) } = \prod_i \mathcal{z_i}^{m-b} \end{align}$$ this $\mathcal{z_i}^{m-b}$ is the single energy state grand canonical partition function in maxwell-boltzmann statistics . maxwell-boltzmann statistics is the classical limit for bose-einstein statistics . the condition for the system to be classical is the single state occupation number $\bar{n}$ to satisfy $\bar{n} \ll 1$ , in other words , the total number of single particle states $m$ should satisfy $n \ll m$ . as , the single particle partition function $z_1$ is actually a weighted sum over all the states , $n\ll z_1$ will satisfy $n \ll m$ for the system to be classical .
i feel stupid for forgetting about potential energy , but any way i figured it out . what i what i had to do was equate the final kinetic energy k with gravitational potential energy u . i actually attempted that before but the problem was i did it a little wrong . i did not include it in my work here on stackexchange because i wanted only the part where i knew what to do and stop where i did not know where to go next . but since i understood where i had to go next i answered my own question which is not what i was expecting . solution : $$g= [ 9.8 ] \frac{m}{s^2}$$ $$0.5 ( m+m ) ( v' ) ^2 = u$$ $$m ( v' ) ^2= ( m + m ) gh$$ $$m ( v' ) ^2=2mgh$$ $$ ( v' ) ^2=2gh$$ $$\frac{ ( v' ) ^2}{2g}=h$$ $$\frac{36}{g}= [ 1.8 ] m$$ p.s. thanks for anybody who gave me tips . i appreciate it .
i think you have your geometry wrong . you need to set up the speed in three dimensions : $\dot{\bf x} = ( \dot x , \dot y , \dot z ) $ . then $\dot{\bf x}^2 = \dot x^2 + \dot y^2 + \dot z^2$ . convert that into spherical coordinates $ ( r , \theta , \phi ) $ , with $\theta$ as the angle down from the z-axis towards the xy-plane , and $\phi$$ as the angle around the z-axis , starting from the x-axis . the z-axis passes through a diameter of the ring , and the ring rotates about the z-axis . $\omega = \mathrm{d}\phi/\mathrm{d}t$ ; the $\omega^2\sin^2 \theta$ term comes from the rotation of the hoop , and the $\dot\theta^2$ term comes from the motion of the particle along the hoop . after you plug in the definitions of $ ( r , \theta , \phi ) $ in terms of $ ( x , y , z ) $ and apply the restriction that $a^2 = x^2 + y^2 + z^2$ , the rest is algebra . most of the terms cancel and/or simplify down to those two terms .
whether or not shielding against radiation is possible depends on the type of radiation . as an example the perforated metal shield in the front of a microwave oven is pretty effective . this type of radiation is called electromagnetic radiation . at lower frequencies it becomes radio waves , at higher frequencies light . at still higher frequencies we encounter x-rays , and at even higher frequencies , $\gamma$-radiation . x-rays and $\gamma$-radiation are dangerous , for x=rays lead is a useful shielding material . there also exists $\beta$-radiation ( electrons ) and $\alpha$-radiation ( atomic nuclei ) . the " rayguard " you mention is likely not to work . even if there is electronics involved it must be able to detect all frequencies of interest . this is pretty hard if not impossible to do so in the x-ray regime . in my opinion this rayguard is on the same level as the boxes that protected against " earth rays " . there the scam was even more severe , earth rays are not supposed to exist . about radiation from mobile phones being harmful opinions differ . their radiation can be rather persistent . i have seen a demonstration with one such phone placed in a metal box as used for biscuits . when connecting to it from a second phone outside the box it still responded . of course the radiation level inside the box is very low but still it was large enough to let the phone ring .
i can only help with the equations of motion . the controls part you have to develop on your own . here are the transformation rules between two coordinate systems , in order to transform your equations of motion . see answer stackexchange-url for details on how to transform from the center of mass c to another arbitrary point a . $$ \begin{aligned} \sum \vec{f} and = m \vec{a}_a - m \vec{c}\times \vec{\alpha} + m \vec{\omega}\times\vec{\omega}\times\vec{c} \\ \sum \vec{m}_a and = i_c \vec{\alpha} + m \vec{c} \times \vec{a}_a - m \vec{c} \times \vec{c} \times \vec{\alpha} +\vec{\omega} i_c \vec{\omega} + m \vec{c} \times \left ( \vec{\omega} \times \vec{\omega} \times \vec{c} \right ) \end{aligned} $$
$e^2 = m^2c^4 + p^2c^2$ , where $m$ is rest mass and $p$ is momentum . if a molecule is moving faster it would have more momentum and more energy , but the same rest mass . some have defined " relativistic mass " as opposed to " rest mass " as $e=m_rc^2$ , so yes the faster moving molecule would have a greater so-called relativistic mass .
if $p^{a}$ are finite-dimensional matrices , then i found that your algebra actually implies that $p^{a}=0$ . i think that it is a consequence of the fact that $su\left ( n\right ) $ is a simple group , i.e. , there is no a normal subgroup in $su ( n ) $ . if we assume that $p^{a}$ are hermitian then your identities : $$ \left [ p^{a} , p^{b}\right ] =0 , \qquad\left [ q^{a} , p^{b}\right ] =if^{abc}p^{c} , \qquad\qquad ( 1 ) $$ are the algebra of an invariant subgroup . the formal proof that $p^{a}=0$ is as follows . if $p$ satisfies the algebra ( 1 ) then a linear independent subset of $p$ also satisfies eq . ( 1 ) , therefore without loss of generality we can assume that all $p^{a}$ are linear independent . let me now split $p$ into the hermitian and anti-hermitian parts : $$ p=\frac{p+p^{\dagger}}{2}+i\frac{p-p^{\dagger}}{2i}=r+ii , $$ where $r$ and $i$ are hermitian matrices . taking into account that all $f^{abc}$ are real and $$ \left [ q^{a} , p^{b}\right ] =if^{abc}p^{c}\quad\longrightarrow\qquad\left [ q^{a} , p^{\dagger b}\right ] =if^{abc}p^{\dagger c} , $$ one can conclude that : $$ \left [ q^{a} , r^{b}\right ] =if^{abc}r^{c} , \qquad\left [ q^{a} , i^{b}\right ] =if^{abc}i^{c} . $$ therefore $r^{a}$ and $i^{a}$ are hermitian traceless matrices thus they can be expressed as linear combinations of $q$ . thus , we find that matrices $p^{a}$ are linear combinations of $q$: $$ \qquad\qquad\qquad\qquad\qquad p^{a}=m^{ab}q^{b} , \qquad\qquad\qquad\qquad\qquad ( 2 ) $$ where $m^{ab}$ is some $\left ( n^{2}-1\right ) \times\left ( n^{2}-1\right ) $ compex-valued matrix . again from eq . ( 1 ) we obtain : $$ m^{ad}\left [ q^{d} , p^{b}\right ] =\left [ p^{a} , p^{b}\right ] =im^{ad}% f^{dbc}p^{c}=0 . $$ since all $p^{e}$ are linear independent then $m^{ad}f^{dbc}=0$ , thus $0=m^{ad^{\prime}}f^{d^{\prime}bc}f^{dbc}=c_{a}m^{ad}=0$ , hence $p^{a}=0 . $ there is another way to show the same . let 's consider the commutator $\left [ q^{a} , p^{b}\right ] $ and use the relation ( 2 ) : $$ \left [ q^{a} , p^{b}\right ] =m^{bc}\left [ q^{a} , q^{c}\right ] =m^{bc} if^{ace}q^{e}=if^{abc}p^{c}=if^{abc}m^{ce}q^{e} $$ comparing the coefficient of $q^{e}$ , we find : $$ m^{bc}f^{ace}=f^{abc}m^{ce}\quad\longrightarrow\qquad m^{bc}\left ( c^{a}\right ) _{ce}=\left ( c^{a}\right ) _{bc}m^{ce}\quad\longrightarrow \qquad\left [ m , c^{a}\right ] =0 , $$ where $\left ( c^{a}\right ) _{ce}=-if^{ace}$ are the generators of irreducible adjoined representation . therefore , according to schur 's lemma $m$ is a scalar matrix , i.e. , $m^{ab}=\lambda\delta^{ab}$ . if one requires that $p$ should be commutative then $\lambda=0 . $ the question is what about the case where $p^{a}$ are not finite-dimensional matrices . but in this case i do not know how to define the trace .
the flight time from the usa to china may be different to the return trip , but if so i would gues this is due to the jet stream rather than the rotation of the earth . anyhow , your intuition about the train is correct . the two passengers a and b see the a to b and b to a speeds of the ball to be the same , while an external observer sees them to differ by twice the speed of the train . the situation you describe is an example of galilean invariance . this can be surprisingly hard for non-phycisists to get their heads round . after all it took mankind about 2000 years ( from the first greek scientists until galileo ) to understand it . this is the way i look at it : suppose you are passenger a in the train , and we also need to assume the train is travelling at a constant speed - this is important because if you are in a train you only feels any force when the train is speeding up or slowing down . you throw the ball to b at e.g. 5 m/sec . if the train is moving at constant speed the ball also moves at a constant speed , so it reaches b at 5 m/sec . b now throws the ball back to you at 5 m/sec , and again the ball does not change speed so it reaches you at 5 m/sec . so the speed is the same in both directions . the key bit is that once you have thrown the ball it does not speed up or slow down during it is flight . an object , like the ball , can only speed up or slow down if there is some external force acting on it , but once the ball is in flight there is no force trying to speed it up or slow it down ( we are ignoring air resistance ) . things get more interesting if the train is accelerating , but that is another question .
being a mathematician , i feel that i should point out a common misconception here . do not feel too bad , plenty of mathematicians ( including myself ) have fallen into this trap . basically , if you want to fit data to power law using least squares methods , then you should not fit a straight line in log log space . you should fit a power law using non-linear least squares to the original data in linear space without any kind of transformation . the basic intuition behind this is this . least squares method assume that the error distribution is normal meaning that there is one true ( unknown ) value which you are trying to measure and the measurements you are taking are above/below the true value staying close to the true value so the distribution is gaussian . the problem is that log is not a linear function so when you take log log of the data , numbers bigger than one are pushed together and numbers less than one are spread apart and namely , the error distribution is not normal any more in log log space . . . . meaning that least squares is not guaranteed to converge to the true values any more . in fact it has been proven that fitting in log-log space consistently gives you biased results . fitting a straight line in log-log was fashionable back in the day before ghz processors . now whatever you are using to do the computation , most likely has the ability to do non-linear least squares power law fit to the original data so that is the one you should do . since power-law is so prevalent in science , there are many packages and techniques for doing them efficiently , correctly , and fast . i refer you to these published results and a rant here . this is the my question which sparked this discussion . addendum : so by op 's request here is an actual example . the actual function is $y=f ( x ) =2x^{-4}=ax^b$ . i take a bunch of points from $x\in [ 1,2 ] $ and $x=10$ . note that $f ( 10 ) =0.0002$ and i change this $y$-value to $y=0.00002$ while keeping all others the same and then fitting them using least squares to estimate $a$ and $b$ . using non-linear least squares fit to the functional form $y=ax^b$ gives us \begin{eqnarray} a and = and 2\\ b and = and -4\\ r^2 and = and 0.9999 . \end{eqnarray} using linear least squares fit to the functional form $y=mx+n$ in log-log space gives us \begin{eqnarray} a and = and exp ( n ) = 2.46\\ b and = and m = -4.57\\ r^2 and = and 0.9831 . \end{eqnarray} the $95\%$ bounds for $b$ are $ ( -4.689 , -4.451 ) $ which does not even include the true value for $b$ . basically log pushes numbers bigger than one together and spreads apart numbers less than one . so in linear space changing $y=0.0001$ to $y=0.00001$ is inconsequential . the algorithm does not care too much about it . the change in the residual is very tiny . in log-log space the difference is rather huge ( an entire order of magnitude ) and the point has become an " outlier " from the nice linear trend . the change in residual is now large . since the algorithm is trying to minimize the sum of the squares , the algorithm can not ignore the deviation and must try to accommodate the outlier point so the line is skewed messing up the slope and the intercept . here is the matlab code and the graphs . you can easily reproduce this and play with it yourself . blue are the data points . black is the power fit in linear space . red is the fitted line from log-log least squares fitting . the top panel shows all three in linear space and the bottom panel shows all three in log-log space to emphasize the differences . the exponent is off by more than a half between the two methods . but how see how good the second fit it ( $r^2=0.98$ ) and the confidence interval is nowhere near the true value of $b$ . the algorithm is very confident that $b\neq-4$ . the same effect will work in reverse with large changes in large numbers . for example , changing a data point from 10000 to 20000 in linear space will cause a huge change but in log-log space the change is not a big deal at all so the algorithm will again give misleading results . second addendum : does non linear least squares manage a set of points with experimental/numerical errors ? i am assuming by manage you mean if nllsf will give us an unbiased estimate of the parameters . the answer to that is , least squares fitting works if and only if the errors are gaussian . so if the errors are random errors ( with the expected value and arithmetic mean being zero ) then least squares gives you an unbiased estimate of the true parameter values . if you have any type of systematic error then least squares is not guaranteed to work . we often assume/want-to-think that we only have random error unless there is good evidence to the contrary . and also does give the uncertainties in both a and b based on those parameters ? again assuming errors being normally distrubuted , there are standard methods for estimating the error deviation . the mean is already assumed to be zero . we estimate the standard error for each parameter and then we use it to compute the confidence intervals . so yes , if i only have experimental data and i have no idea what the real values of the parameters are , then i can still compute the confidence intervals . now an introductory bibliographic reference other than wikipedia would be very helpful . i do not know of a good stats/regression book i can recommend . i would say just pick your favorite intro to stats book . and there are those papers by clauset that i linked to both available on arxiv for free . tl ; dr physicist think everything is a power law . it is not . even if the data looks like its power law , it probably is not . if you do legitimately have a power law , do not fit a straight line in log-log space . just use nllsf to fit a power law on the original data . if there is still any doubt , i can easily make up a numerical example and show you concretely how different the two answers can be .
from amateur photographers in art galleries : assessing the harm done by flash photography . by martin h . evans : the flash built into a digital compact has , typically , a gn value of about 6 to 9 ( though some manufacturers are rather coy about revealing the gn rating ) . if one extends the calculations used by saunders and by evans to these little units , then one finds that if one of these is fired at full power at about 2.5 metres ( ca 8 feet ) , it exposes the object to about the same quantity of light as that falling on it every one-eighth of a second in a 200 lux ( ca 18.6 footcandle ) gallery , or every half second in a dark 50 lux ( ca 4.6 footcandle ) gallery . is it worth getting steamed up about such a tiny extra quantity of light , as far as pigment fading is concerned ? several photographers have already suggested that any trifling damage done by a few hundred of these little flashes in a day could be fully offset by closing the gallery and turning off the lights a few minutes early . a ban would be justified in rare cases , where large numbers of photographers might be taking many flash photographs very close to something that could reasonably be considered photosensitive . so it appears to me the main reason for the ban is not related to the photoelectric effect .
let 's restrict to the case of special relativity based on the comments under the original post . let $x^\mu ( t ) = ( t , \mathbf x ( t ) ) $ be the path of a timelike particle as measured in the lab frame . in some inertial frame . suppose that at some instant $t_0$ , the particle is measured to have zero velocity in this frame ; $$ \frac{d \mathbf x}{dt} ( t_0 ) = 0 $$ then this inertial frame is , by definition , comoving with the particle at time $t_0$ . indulging in a slight abuse of notation , suppose that $t ( \tau ) $ gives the inertial time as a function of proper time . then we have $$ \frac{d}{d\tau}\mathbf x ( t ( \tau ) ) = \frac{d\mathbf x}{dt} ( t ( \tau ) ) \frac{dt}{d\tau} ( \tau ) $$ and $$ \frac{d^2}{d\tau^2}\mathbf x ( t ( \tau ) ) = \frac{d^2\mathbf x}{dt^2} ( t ( \tau ) ) \left [ \frac{dt}{d\tau} ( \tau ) \right ] ^2 +\frac{d\mathbf x}{dt} ( t ( \tau ) ) \frac{d^2t}{d\tau^2} ( \tau ) $$ evaluate this at $\tau_0$ satisfying $t ( \tau_0 ) = t_0$ ; in other words $\tau_0$ is just the proper time at which the frames are comoving . the second term vanishes by the comoving assumption , moreover the factor $\frac{dt}{d\tau} ( \tau_0 ) $ just equals $1$ because recall that $dt = \gamma d\tau$ and since the particle is momentarily at rest relative to the comoving frame , $\gamma = 1$ at that instant . we therefore get $$ \frac{d^2}{d\tau^2}\mathbf x ( t ( \tau ) ) \big|_{\tau = \tau_0} = \frac{d^2\mathbf x}{dt^2} ( t_0 ) $$ the left hand side is the proper acceleration at the " comoving instant " of the particle by definition . the right hand side is the acceleration as measured by the comoving observer , so this is the equality you wanted .
you are right . the $i$ indices of the occupation numbers $n_i$ can be thought of as multi-indices meaning that each $i$ encodes both the wavevector $k$ ( which is quantized once boundary conditions are imposed ) and the spin sign $\sigma$ . in general , $i$ will enumerate all the allowed combinations of quantum numbers that characterize the single particle states . it is also true that only the entries $n_i$ of those states which are populated will be non-zero . if the total particle number $n$ is fixed , the constraint $\sum_in_i=n$ has to hold . i added this in response to your comment : the space the occupation number states naturally live in is referred to as fock space . it can be written as $\mathcal f^\pm = \bigoplus_{n=0}^\infty\mathcal h_n^\pm = \mathcal h_0 \oplus \mathcal h_1 \oplus \mathcal h_2^\pm \oplus \dots$ where $\mathcal h_n^\pm$ refers to the usually $n$ particle hilbert spaces and the $\pm$ indicates whether we restict ourselves to symmetric or antisymmetric states , i.e. bosons and fermions respectively . it is convenient to work in fock space because annihilation and creation operators will take you from one hilbert space with fixed particle number to another . now , assuming that you mean ' dimension ' when you talk about the ' size ' of a hilbert space , keep in mind that you have to obey the ( anti- ) symmetry constaint . this reduces the dimension of each of the $n$ particle subspaces . as an example , consider a two state fermionic system , e.g. electrons with spin either $\downarrow$ ( $i=1$ ) or $\uparrow$ ( $i=2$ ) . due to the pauli principle ( i.e. . as a consequence of the antisymmetry requirement ) the only occupation numbers allowed are 0 and 1 . $\mathcal h_0$ is one-dimensional as always , it contains only the vacuum state $|00\rangle$ ( and multiples thereof , but we are interested in normalized linear independent states ) . the one particle hilbert space is two-dimensional as the electron can be in either one of the states ( and symmetry still does not matter since there are no two particles to swap ) . the two particle hilbert space $\mathcal h_2^-$ has again only one linear independent state , $|11\rangle$ , because mr . pauli does not allow us to populate any of the two states twice . thus , the two-state fermionic fock space is four-dimenional . more generally , the $m$-state fermionic fock space will be $\sum_{n=0}^m\binom{m}{n}=2^m$ dimensional ( which follows from simple combinatorics ) and bosonic fock spaces are infinite dimensional because bosons can occupy the same state as many times as they want .
in my opinion there is no physical reason . to cook food the same energy is needed , and to burn it too . it is behavioral differences because of the form of heat : one is aware of the dangers of gas and is much more careful in turning it off on time , when food just starts to smell " singed " . electric : we may turn it off and leave the pot on the still hot element and a small singe becomes large . turning gas off zeroes the heat supplied . turning the electric off has residual heat . recipes , in my experience , are timed on gas , all good cooks cook with gas because of the control of temperature it gives . using the same times on electric and leaving the pot on the still hot element will turn a slight gold color to brown . having changed from gas to electric i turn off the electric about 10 minutes before the recipe 's end time and let it use the energy of the element to finish cooking .
physical meaning of $a$ is the length of an arc swept out by velocity vector .
this is what i did and i think is simple and right : assume $v$ linear speed of centre of mass downwards and $\omega$ angular speed around it . use the fact the bottom point has no vertical speed to find relation between $v$ and $\omega$ . and i am done .
if i read your question correctly , it centers on what is the cause of objects to be opaque and white . there is a common thread through salt , beer foam and talcum powder – all of these objects have embedded in them transparent particles . for example , if we look at salt , at its “smallest scale” are transparent particles called grains . if the transparent particles are small but larger than the wavelengths involved , light entering each transparent grain will act as a scattering center . that is , light will get reflected and refracted , and will reemerge with its frequency unchanged . that’s why the object appears white but it does not explain why it is opaque . its opaqueness comes from the fact that there is a change in index of refraction from air and salt’s transparent grains . the amount of light that is reflected is given by its reflectance , which in turn is determined by the differences between the index of refractions ( $n_1 =$ air and $n_2 =$ salt ) . when light is at normal incidence , the reflectance is given by $$r=\frac{ ( n_1-n_2 ) ^2 }{ ( n_1+n_2 ) ^2}$$ the larger the change in the two indexes ( $n_1−n_2$ ) , the more light is reflected and the material will appear more opaque . for optical frequencies , the index of refraction for salt is 1.54 and its reflectance off one surface is r = 0.04 . however , a salt pile is made with its numerous pieces of salt crystals that will reflect light off of numerous surfaces , increasing the number of reflections back into the incident medium . therefore , the reflected light reaching the observer will be white and opaque . now we can apply this to water vapor or beer foam , snow ( ice crystals ) , paper ( transparent fibers ) or talcum powder ( crystal ) or even ground glass , each of which is composed of transparent particles . so white light is reflected off of multiple surfaces with its frequency unaltered , they will appear white and opaque . as for white paint , i am guessing here , but i expect that white paint is composed of small clear transparent particles floating in a clear adhesive so that the paint will stick to a wall when painted . the question of why paper , snow or talcum powder appears off-white or gray must be due to the reflectance decreasing for the material since less light is being reflected . this makes sense because a material getting wet is now surrounding the transparent particles with a higher index of refraction ; that is , the incident medium goes from air to water . the key point being that it’s the difference ( $n_1−n_2$ ) that determines how much the reflectance decreases ( see the above equation ) . for example , i assume that snow has an index of refraction of ice , $n_2$ ( ice ) =1.30 . since $n_1$ ( water ) =1.33 , the reflectance dramatically decreases for wet snow to r ( wet snow ) $=1.3×10^{-4}$ ! this dramatic decrease implies that a lot more of the light is getting through and not reflected , therefore , its gray look . so if water surrounds paper or talcum powder , it is the reduction of ( $n_1−n_2$ ) that leads to objects appearing off-white . this was my favorite part of your question and the reason why i answered it . great question !
these guys seem to be doing just what you asked about . have a look here : http://www.liquidcrystaltechnologies.com/products/lcdshutters_2.htm
here we will outline a strategy to prove the sought-for operator identity $ ( 4 ) $ from the following definitions of what the commutator and the normal order of two mode operators $\alpha_m$ and $\alpha_n$ mean : $$ [ \alpha_m , \alpha_n ] ~=~ \hbar m~\delta_{m+n}^0 , \qquad\qquad ( 1 ) $$ $$ :\alpha_m \alpha_n:~=~\theta ( n-m ) \alpha_m \alpha_n ~+~ \theta ( m-n ) \alpha_n \alpha_m , \qquad\qquad ( 2 ) $$ where $\theta$ denote the heaviside step function . 1 ) note that the current $j ( z ) ~=~j_{-} ( z ) + j_{+} ( z ) $ is a sum of a creation part $j_{-} ( z ) $ and an annihilation part $j_{+} ( z ) $ . 2 ) recall that the radial order ${\cal r}$ is defined as $${\cal r} ( j ( z ) j ( w ) ) ~=~\theta ( |z|-|w| ) j ( z ) j ( w ) ~+~ \theta ( |w|-|z| ) j ( w ) j ( z ) . \qquad\qquad ( 3 ) $$ 3 ) rewrite the sought-for operator identity as $$\cal{r} ( j ( z ) j ( w ) ) ~-~:j ( z ) j ( w ) :~=~\frac{\hbar}{ ( z-w ) ^2} . \qquad\qquad ( 4 ) $$ 4 ) notice that each of the three terms in eq . $ ( 4 ) $ are invariant under $z\leftrightarrow w$ symmetry . so we may assume from now on that $|z|&lt ; |w|$ . 5 ) show that $$j ( w ) j ( z ) ~-~:j ( z ) j ( w ) :~=~ [ j_{+} ( w ) , j_{-} ( z ) ] . \qquad\qquad ( 5 ) $$ 6 ) show ( under the assumption $|z|&lt ; |w|$ ) that $$j ( w ) j ( z ) ~-~r ( j ( z ) j ( w ) ) ~\stackrel{|z|&lt ; |w|}{=}~0 . \qquad\qquad ( 6 ) $$ 7 ) subtract eq . ( 6 ) from eq . ( 5 ) : $$\cal{r} ( j ( z ) j ( w ) ) ~-~:j ( z ) j ( w ) :~\stackrel{|z|&lt ; |w|}{=}~ [ j_{+} ( w ) , j_{-} ( z ) ] . \qquad\qquad ( 7 ) $$ 8 ) evaluate rhs . of eq . ( 7 ) : $$ [ j_{+} ( w ) , j_{-} ( z ) ] ~=~\ldots~=~\frac{\hbar}{w^2} \sum_{n=1}^{\infty}n \left ( \frac{z}{w}\right ) ^{n-1}~=~\ldots~=~ \frac{\hbar}{ ( z-w ) ^2} . \qquad\qquad ( 8 ) $$ in the last step we will use that the sum is convergent under the assumption $|z|&lt ; |w|$ .
suppose the spaceship is moving at speed $v$ and covers a distance $d$ . the light emitted when the spaceship left reaches us at a time : $$ t_0 = \frac{d}{c} $$ while the time the spaceship reaches us is : $$ t_1 = d/v $$ the apparent velocity is distance divided by time so : $$\begin{align} v_a and = \frac{d}{d/v - d/c} \\ and = \frac{vc}{c - v} \end{align}$$ your question asks for the real velocity , so we rearrange to get : $$ v = \frac{cv_a}{c + v_a} $$ if you substitute your value for $v_a$ you get $2.603×10^8\text{ m/s}$ .
relative position versus time is linear when relative velocity is constant i.e. when relative acceleration eqauls zero . in this case , both stones are acted upon by gravity only and hence their acceleration always equals $g$ directed downwards . therefore , relative position varies linearly for the entire duration of motion which stops when the stone thrown with $15$ m/s reaches the ground . to calculate that $-200 = 15t - 0.5gt^2$ which on solving gives $t = 8$ sec so , the textbook is absolutely right ! ! !
