gravity is mediated by a spin two particle . electromagnetism by spin 1 . here is a link that answers your question : even and odd spin do differ in that they require a product of charges with different signs to get attraction or repulsion : spin even : $q_1 q_2 &gt ; 0$: attractive $q_1 q_2 &lt ; 0$: repulsive spin odd : $q_1 q_2 &lt ; 0$: attractive $q_1 q_2 &gt ; 0$: repulsive in the case of gravity , mediated by spin 2 particles , charge is mass , which is always positive . thus , $q_1 q_2$ is always greater than zero , and gravity is always attractive . for spin 0 force mediators , however , there is no restriction on the charges and you can very well have repulsive forces . a better rephrasing of the question is : " why do particles of odd spin generate repulsive forces between like charges , while particles of even spin generate attractive forces between like charges ? " goes on to derive this
no polarity reversal has occurred - you are dealing with a magnet that has an axial field ( pointing out through the flat face . ) when you break it , each half has similar field , pointing in the same direction , which is unstable . one piece will want to flip so that the fields line up antiparallel ( lower energy situation ) .
you are correct that there is no such thing as a rigid body in reality . any time a force is applied to an object at one point ( such as a boulder being placed on one end of the see-saw ) , it only immediately applies to the molecules that it is touching . the displacement of those molecules propagates to the rest of the object as a " deformation wave , " which is basically the same thing as a sound wave ( though there are differences to the way sound is carried in fluids like air ) . in your example , what you would see as a distant observer is the boulder being placed on one end of the see-saw and the person sitting down on the other end simultaneously . both ends would dip downward due to the force on them , so you would see curves begin to appear in the bar . those curves would travel as waves along the bar toward the center , pass each other in the center , and then would eventually reach the opposite end from where they started , where they would have the effect you had normally expect them to have ( i.e. . person b might be pushed back up ) . note that since the speed of sound is much slower than that of light , it would in general take longer than a second for the effect to propagate from one end of the bar to the other .
the question asks for a black hole splitting such that " the product black holes would exceed the area of the original black hole " . in the above answer i have argued that to do so requires at least two black holes colliding . however , the question continuous with the remark that such a splitting into black holes with larger horizon area " seems to be a statistically favorable transition by the fact alone that would be a state with larger entropy than the initial state " . the edit in my answer above suggests this assertion to be correct . however , this is not the case . to determine what is a statistically favorable transition requires a comparison between alternative results . if there is an outcome that can be realized in overwhelmingly more ways than any of the alternatives , that is the statistically favorable outcome . let 's see how this works out for two colliding black holes . as an example we take two black holes of 4n planck masses each . let 's consider two alternative scenarios : a ) ' splitting': 4n + 4n --> 6n + n + n b ) ' merging': 4n + 4n --> 8n a black hole containing n planck masses has entropy $s = 4\pi n^2$ . therefore , the initial state has total entropy $s = 128\pi n^2$ and can be realized in $e^s = e^{128\pi n^2}$ ways . the end products from scenario a ) has larger entropy ( $s = 152\pi n^2$ ) and can be realized in $e^{152\pi n^2}$ ways . for large n this number is way larger than the number of realizations for the initial state . yet , scenario a ) does not represent the statistically favorable transition . this is because scenario b ) leads to entropy $s = 256\pi n^2$ encompassing overwhelmingly more microscopic states : $e^{256\pi n^2}$ . the conclusion is that although entropy-increasing black hole splitting reactions can be defined , these are not realizable from a statistical physics perspective .
those first two equations you mentioned only work in the case of constant acceleration ( for more info on this type of kinematics , go here : when can i use equations of motion ? ) . in your case , we clearly do not have constant acceleration if the force ( which defines the acceleration ) depends on how fast the object is going . just picture it this way : first the object starts out with some speed , so there is air resistance which slows it down , so now it has less speed , therefore the air resistance is lesser . so there is a changing acceleration , and you can not apply those seemingly standard kinematics equations . i am afraid if you do not know a bit of differential equations ( or at least basic differential calculus ) it'll be impossible for you to understand how to solve the problem ( so learn calculus ! ) . if you do know calculus , here 's a really nice look at different cases with quadratic air resistance ( the type of air resistance that is acting in your problem ) . also , as is mentioned in the page i linked to , you will find that in your particular case which is fairly general , there is no general solution ( pardon the redundancy ) to the differential equation that comes out of your situation ; you can get an approximate numerical solution though .
the answer to this question begins and ends with the canonical commutation relationship ( ccr ) , the ubiquitous $ [ \hat{x} , \ , \hat{p} ] =i\ , \hbar\ , \mathrm{id}$ where of course $\hat{x}$ and $\hat{p}$ are the position and momentum observables . the ccr alone , as i show in this answer here implies there is an orthogonal co-ordinate system ( which we call position co-ordinates ) for the quantum state space wherein : $$\begin{array}{lcl}\hat{x} f ( x ) and = and x\ , f ( x ) \\ \hat{p} f ( x ) and = and -i\ , \hbar\ , \mathrm{d}_x\ , f ( x ) \end{array}$$ this being so , if we want to transform to a co-ordinate system wherein the momentum , and thus the derivative $\mathrm{d}_x$ becomes a multiplication operator , then the basis states ( strictly speaking , as tempered distributions as discussed here ) have to be the eigenvectors of the derivative operator . these are , of course , distributions of the form $e^{i\ , k\ , x}$ , and to find their relative weights in a quantum state , i.e. superposition , we decompose that superposition with the fourier transform . so there you are !
nice problem . if the rope is split into two pieces , each being a two force member ( can only have tension ) then treat the problem first with infinite friction at the base to find the required friction force for equilibrium . $$ \begin{align} p_x - a_x and = 0 \\ a_y - m g - m g and = 0 \\ m g \frac{\ell}{2} \cos\theta + m g \ell \cos\theta - p_x \ell \sin \theta and = 0 \end{align} $$ three equations for three unknowns $a_x , \ a_y , \ p_x$ . now set $a_x$ according to static friction and find $m$ . the rest follows . the above is solved for : $$ a_y = ( m+m ) g \\ a_x = ( m+\frac{m}{2} ) g \cot\theta \\ p_x = a_x $$ and thus $m \le \frac{m ( \cos\theta - 2 \mu_s \sin\theta ) }{2 ( \mu_s \sin\theta-\cos\theta ) }$ . the system is stable only when $m&gt ; 0$ which occurs when $\frac{\cot\theta}{2} &lt ; \mu_s &lt ; \cot\theta $ .
no . there are numerous well-known mass distributions that start out non-singular , and which collapse to form a black-hole after a finite amount of time . black hole solutions , however , have a region of infinite density and infinite spacetime curvature , so they are not at all well-behaved in any ordinary sense .
whether or not neutrinos would be suitable for rapid trading , people have seriously considered their utility for signalling in difficult environments . i read an article a while back about a paper ( published in phys . lett . b , but i can not access that from here ) by patrick huber which proposed using neutrinos for through-the-earth communication to submarines as an alternative to elf , where bandwidths become competitive . the submarine would pick up the modulated cherenkov radiation produced by the generation of muons in seawater . this certainly allows faster-than-great-circle transmission times , but this is not the reason why the technique is attractive . the preprint indicates that calculated antipode to antipode bandwidth is only 10 b/s which does not seem ready for high-intensity trading . addendum : if we consider a continuous lossless fibre optic link between antipodes around the equator , the transmission time will be about 99 ms , whilst the through-earth travel time ( at $\approx{c}$ ) is 42 ms . obviously this counts for nothing if you have high-latency equipment at either end . whilst the improvement in transmission time hardly seems worth it , it occurs to me that this would be a useful technique for communicating between either side of a huge , highly oblate structure such as a wide but thin disk-shaped megastructure , however that is veering in to sci-fi territory .
so you have two rigid bodies in contact to the ground . let us call $n_a$ and $n_b$ the contact force ( normal to ground ) and $f_a$ , $f_b$ the frictional force at a and b ( arbitrarily chosen to act in a positive x direction ) . we can call the pin forces $p_x$ and $p_y$ acting on the bar ( an reacting on the disk ) . the sum of moments about the disk center is $$ r f_a + c = i \ddot{\theta} $$ and about the bar center $$ \frac{\ell}{2} \cos\theta ( n_b-p_y ) - \frac{\ell}{2} \sin\theta ( n_b+p_x ) = 0 $$ couple with the motion in the x and y axes of the disk $$ m \ddot{x} = m r \ddot{\theta} = f_a -p_x \\ m \ddot{y} = n_a -p_y - m g = 0 $$ and the bar $$ m \ddot{x} = f_b + p_x \\ 0 = n_b+p_y - m g $$ for the condition of motionless with $\ddot{x}=\ddot{\theta}=0$ the above is 6 equations for 6 unknowns ( $p_x , p_y , n_a , n_b , f_a , f_b$ ) which is solved by elimination . next find which is the smallest $c$ value that makes $|f_a| = \mu_s n_a$ and $|f_b| = \mu_s n_b$ if you get the correct result you will have $p_x&lt ; 0$ and $f_a&lt ; 0$ and $c \propto \mu_s r m g $
the following assumes that the distance to the star ( 100 light year ) was measured before you got in the spaceship and started moving . when moving close to $c$ in your frame of reference space around you will be contracted relative to what someone on earth will measure . thus , your 100 light year journey will actually be shorter in your frame of reference , and will thus take less than 100 years for you to make it to the star . if you are ever able to report to someone back on earth that the journey took you less than 100 years in your frame of reference they will agree with you , since from the reference frame of earth your spaceship and all its inhabitants underwent time dilation , the slowing down of time relative to another frame of reference . thus , you would both agree on the time the journey took in your frame of reference , but the person back on earth will say that according to their clocks in the earth 's frame your journey took 100 years . in summation , you will not conclude that in your frame of reference you traveled faster than $c$ because while in transit , due to length contraction , the journey you traveled was actually shorter than 100 light years . this is an answer to your bolded question , which is a different question than the one you posed at the end about whether or not your spaceship will accelerate forever . the answer to that question is , as you continue to accelerate away from your origin star the laser propelling your spaceship will be redshifted ( not blueshifted as you mentioned in your question ) so that you will have less energy per photon than you had at the beginning of your journey ( before the photons were redshifted ) . additionally , as you get further away from your origin , less and less photons from the laser will reach you ; although lasers produce very focused beams , there are quantum limits to how focused these beams can be , so far away from the laser the spread in the beam will cause less and less photons to actually reach you . these two effects combined will cause the energy you can extract from the laser to accelerate your ship to diminish as you get further away until effectively you can imagine that the acceleration the laser provides your ship vanishes .
due to the ever expanding universe and its decelerating rate of expansion , it took a while for light to reach us . consider this analogy to get the idea : there is a bomb . suddenly , it explodes and sends pieces flying off . the pieces are decelerating . on one piece , there lives a strange creature , which after some time after the explosion , turns his torch light on . now the other piece has covered quite some distance , so the light has some catching up to do . and thus it takes light more time than it should have if it had been emitted just at the time of explosion .
i have found an explanation . at the end of section 22.2 , working with euclidean path integrals , weinberg shows that $$-\frac{1}{32\pi^2} \int d^4x \epsilon^e_{ijkl}f_{\alpha ij} f_{\beta kl} \operatorname{tr} ( t_\alpha t_\beta ) = n_+ - n_-$$ where $t_\alpha$ are gauge group generators and $n_\pm$ is the number of zero modes of the gauge covariant dirac operator $i\gamma^\mu d_\mu$ with positive respectively negative chirality . this is the atiyah-singer index theorem at work according to weinberg . ( i have heard of this theorem before but i am not familiar with it . ) as a consequence of this , in the non-trivial sectors where the $\theta$ term is non-zero , there exists at least one zero mode . a massless field enters the path integral as $\overline\psi \gamma^\mu d_\mu \psi$ and it can be integrated out $$\mathcal z =\int d [ a_\mu , \ldots ] \det ( i\gamma^\mu d_\mu ) \exp ( s_\text{yang-mills} [ a_\mu ] + s_1 [ \ldots ] ) $$ where the dots and $s_1$ stand for other fields and their actions , and the determinant of the operator and not its inverse appears , since $\psi$ is grassmann . but if $\gamma^\mu d_\mu$ has a zero mode in every non-trivial sector . . . the determinant is 0 in all the non-trivial sectors . this is effectively the same as discarding the $\theta$-term ! ( weinberg 's proof uses that $i\gamma^\mu d_\mu$ and $\gamma_5$ anti-commute . since this is not the case for the bilinear $i\gamma^\mu d_\mu + m$ appearing for a massive field , we do not prove " too much " and rule out $p$ violations when all fermions are massive . ) i found the zero-mode argument in chapter 94 of srednicki 's book . srednicki argues that for physical reasons $i\gamma^\mu d_\mu$ must have a zero mode ; in weinberg 's book one finds the mathematical proof that this is the case .
to the best of my knowledge , most physicists do not believe that antimatter is actually matter moving backwards in time . it is not even entirely clear what would it really mean to move backwards in time , from the popular viewpoint . if i am remembering correctly , this idea all comes from a story that probably originated with richard feynman . at the time , one of the big puzzles of physics was why all instances of a particular elementary particle ( all electrons , for example ) are apparently identical . feynman had a very hand-wavy idea that all electrons could in fact be the same electron , just bouncing back and forth between the beginning of time and the end . as far as i know , that idea never developed into anything mathematically grounded , but it did inspire feynman and others to calculate what the properties of an electron moving backwards in time would be , in a certain precise sense that emerges from quantum field theory . what they came up with was a particle that matched the known properties of the positron . just to give you a rough idea of what it means for a particle to " move backwards in time " in the technical sense : in quantum field theory , particles carry with them amounts of various conserved quantities as they move . these quantities may include energy , momentum , electric charge , " flavor , " and others . as the particles move , these conserved quantities produce " currents , " which have a direction based on the motion and sign of the conserved quantity . if you apply the time reversal operator ( which is a purely mathematical concept , not something that actually reverses time ) , you reverse the direction of the current flow , which is equivalent to reversing the sign of the conserved quantity , thus ( roughly speaking ) turning the particle into its antiparticle . for example , consider electric current : it arises from the movement of electric charge , and the direction of the current is a product of the direction of motion of the charge and the sign of the charge . $\vec{i} = q\vec{v}$ positive charge moving left ( $+q\times -v$ ) is equivalent to negative charge moving right ( $-q\times +v$ ) . if you have a current of electrons moving to the right , and you apply the time reversal operator , it converts the rightward velocity to leftward velocity ( $-q\times -v$ ) . but you would get the exact same result by instead converting the electrons into positrons and letting them continue to move to the right ( $+q\times +v$ ) ; either way , you wind up with the net positive charge flow moving to the right . by the way , optional reading if you are interested : there is a very basic ( though hard to prove ) theorem in quantum field theory , the tcp theorem , that says that if you apply the three operations of time reversal , charge conjugation ( switch particles and antiparticles ) , and parity inversion ( mirroring space ) , the result should be exactly equivalent to what you started with . we know from experimental data that , under certain exotic circumstances , the combination of charge conjugation and parity inversion does not leave all physical processes unchanged , which means that the same must be true of time reversal : physics is not time-reversal invariant . of course , since we can not actually reverse time , we can not test in exactly what manner this is true .
the radiation from an ultrarelativistic ( $v \approx c$ ) particle on a circular path is called synchotron radiation . the total power radiated from such a particle is $$p = \frac{e^2 a^2}{6\pi \epsilon_0 c}\gamma^4$$ where $a$ is the acceleration and $\gamma$ is the lorentz factor , $\gamma^2 = 1/ ( 1-v^2/c^2 ) $ .
water vapor is invisible . i think you mean fog - fine water droplets condensed from water vapor . pressurized planes fly with an 8000 foot equivalent altitude and humidity in the cabin is low . but , at 35,000 feet ( called flight level 35 ) it is likely one would get a brief fog . if loss of pressure is fast , you would only get to watch for a few seconds . at those pressures , oxygen actually rapidly diffuses out of the blood in the lungs and unconsciousness comes much sooner than one would expect . there have been plenty of accidents dating back to the wwii fighters that cruised at 40,000 feet where a pilot thought he could manage briefly without his oxygen . something like this likely happened to the missing malaysian aircraft .
since i am new here and i cannot comment above i just write my comment as a small answer . i think in your question there is a small mistake . you ask how many degrees of freedom are there in a yang-mills theory although you want to ask how many polarizations does the gluon have . first of all , the number of generators of the gauge group will be the number of gauge bosons you will get . $su ( n ) $ groups have indeed $n^2-1$ generators , except $u ( 1 ) $ that has one . in qed this corresponds to the photon , in $su ( 2 ) $ to three gauge bosons ( not exactly the w 's and the z since these are produced by mixing $su ( 2 ) _l$ with $u ( 1 ) _y$ ) , and in $su ( 3 ) $ there are 8 gluons as siva already told you . now to your question , how many polarizations do these gauge bosons have . let us start with a massive vector boson . we can define its helicity states at the rest frame and then of course boost to the frame that the particle moves . under this boost , the longitudinal polarization has a term $e/m$ ( where $e$ the energy and $m$ the mass of the particle ) that goes to infinity when the mass of the particle goes to zero . this would cause problems concerning the unitarity of the theory since the contrubutions in the matrix elements would be huge . what one can do is arrange the interactions of the theory such that the contributions from longitudinal polarizations are suppressed by a factor of the order $m/e$ . in the case now the vector boson is strictly massless , the longitudinal contributions decouple completely . therefore , massless vector bosons have two physical states of maximal helicity while massive one have three . gluons are massless and so are photons , while w 's and z are massive . i hope i helped . i am pretty sure that you can find more details even in wikipedia if you want .
no , it does not have to be numbered unless , like @dmckee comments , you have received specific instructions to do the numbering . i think your question can best be split up in 2 parts for the answer : writing a report and writing a research paper . writing a report for a report the main goal is typically to show exactly what you have done and how you have done it . quite often the report will be used by other students to continue your work . for this purpose it is convenient to have a numbered list for the methods , because it immediately attracts attention and makes it easy to follow the ' recipe ' . writing a research paper when you write a research paper your goal is in general to resolve a particular issue that exists in the scientific community . your focus will be on the research question and the conclusions you can draw from the experiments/simulations that you did . in this case a numbered list for the method will draw way too much attention to it , much more than it deserves . it is , after all , easy to spot because it disrupts the flow of the paper . in some journals it is not even allowed to use numbered lists if they are not inline ( i.e. . 1 ) . . . 2 ) . . . ) . so in conclusion , if you do not have specific instructions from someone ' higher up': use the numbered list if you want to focus on the method , use the paragraph if you want to focus on a different part of the report/paper
a ( rank 2 contravariant ) tensor is a vector of vectors . if you have a vector , it is 3 numbers which point in a certain direction . what that means is that they rotate into each other when you do a rotation of coordinates . so that the 3 vector components $v^i$ transform into $$v'^i = a^i_j v^j$$ under a linear transformation of coordinates . a tensor is a vector of 3 vectors that rotate into each other under rotation ( and also rotate as vectors--- the order of the two rotation operations is irrelevant ) . if a vector is $v^i$ where i runs from 1-3 ( or 1-4 , or from whatever to whatever ) , the tensor is $t^{ij}$ , where the first index labels the vector , and the second index labels the vector component ( or vice versa ) . when you rotate coordinates t transforms as $$ t'^{ij} = a^i_k a^j_l t^{kl} = \sum_{kl} a^i_k a^j_l t^{kl} $$ where i use the einstein summation convention that a repeated index is summed over , so that the middle expression really means the sum on the far right . a rank 3 tensor is a vector of rank 2 tensors , a rank four tensor is a vector of rank 3 tensors , so on to arbitrary rank . the notation is $t^{ijkl}$ and so on with as many upper indices as you have a rank . the transformation law is one a for each index , meaning each index transforms separately as a vector . a covariant vector , or covector , is a linear function from vectors to numbers . this is described completely by the coefficients , $u_i$ , and the linear function is $$ u_i v^i = \sum_i u_i v^i = u_1 v^1 + u_2 v^2 + u_3 v^3 $$ where the einstein convention is employed in the first expression , which just means that if the same index name occurs twice , once lower and once upper , you understand that you are supposed to sum over the index , and you say the index is contracted . the most general linear function is some linear combination of the three components with some coefficients , so this is the general covector . the transformation law for a covector must be by the inverse matrix $$ u'_i = \bar{a}_i^j u_j $$ matrix multiplication is simple in the einstein convention : $$ m^i_j n^j_k = ( mn ) ^i_k $$ and the definition of $\bar{a}$ ( the inverse matrix ) makes it that the inner product $u_i v^i$ stays the same under a coordinate transformation ( you should check this ) . a rank-2 covariant tensor is a covector of covectors , and so on to arbitrarily high rank . you can also make a rank m , n tensor $t^{i_1 i_2 . . . i_m}_{j_1j_2 . . . j_n}$ , with m upper and n lower indices . each index transforms separately as a vector or covector according to whether it is up or down . any lower index may be contracted with any upper index in a tensor product , since this is an invariant operation . this means that the rank m , n tensors can be viewed in many ways : as the most general linear function from m covectors and n vectors into numbers as the most general linear function from a rank m covariant tensor into a rank n contravariant tensor as the most general linear function from a rank n contravariant tensor into a rank m covariant tensor . and so on for a number of interpretations that grows exponentially with the rank . this is the mathemtician 's preferred definition , which does not emphasize the transformation properties , rather it emphasizes the linear maps involved . the two definitions are identical , but i am happy i learned the physicist definition first . in ordinary euclidean space in rectangular coordinates , you do not need to distinguish between vectors and covectors , because rotation matrices have an inverse which is their transpose , which means that covectors and vectors transform the same under rotations . this means that you can have only up indices , or only down , it does not matter . you can replace an upper index with a lower index keeping the components unchanged . in a more general situation , the map between vectors and covectors is called a metric tensor $g_{ij}$ . this tensor takes a vector v and produces a covector ( traditionally written with the same name but with a lower index ) $$ v_i = g_{ij} v^i$$ and this allows you to define a notion of length $$ |v|^2 = v_i v^i = g_{ij}v^i v^j $$ this is also a notion of dot-product , which can be extracted from the notion of length as follows : $$ 2 v\cdot u = |v+u|^2 - |v|^2 - |u|^2 = 2 g_{\mu\nu} v^\mu u^\nu $$ in euclidean space , the metric tensor $g_{ij}= \delta_{ij}$ which is the kronecker delta . it is like the identity matrix , except it is a tensor , not a matrix ( a matrix takes vectors to vectors , so it has one upper and one lower index--- note that this means it automatically takes covectors to covectors , this is multiplication of the covector by the transpose matrix in matrix notation , but einstein notation subsumes and extends matrix notation , so it is best to think of all matrix operations as shorthand for some index contractions ) . the calculus of tensors is important , because many quantities are naturally vectors of vectors . the stress tensor : if you have a scalar conserved quantity , the current density of the charge is a vector . if you have a vector conserved quantity ( like momentum ) , the current density of momentum is a tensor , called the stress tensor the tensor of inertia : for rotational motion of rigid object , the angular velocity is a vector and the angular momentum is a vector which is a linear function of the angular velocity . the linear map between them is called the tensor of inertia . only for highly symmetric bodies is the tensor proportional to $\delta^i_j$ , so that the two always point in the same direction . this is omitted from elementary mechanics courses , because tensors are considered too abstract . axial vectors : every axial vector in a parity preserving theory can be thought of as a rank 2 antisymmetric tensor , by mapping with the tensor $\epsilon_{ijk}$ high spin represnetations : the theory of group representations is incomprehensible without tensors , and is relatively intuitive if you use them . curvature : the curvature of a manifold is the linear change in a vector when you take it around a closed loop formed by two vectors . it is a linear function of three vectors which produces a vector , and is naturally a rank 1,3 tensor . metric tensor : this was discussed before . this is the main ingredient of general relativity differential forms : these are antisymmetric tensors of rank n , meaning tensors which have the property that $a_{ij} =-a_{ji}$ and the analogous thing for higher rank , where you get a minus sign for each transposition . in general , tensors are the founding tool for group representations , and you need them for all aspects of physics , since symmetry is so central to physics .
because it is effect is smaller than the variation in g due to earth 's bulge ( caused by the same centrifugal force ) or the local geology - when you use 9.8m/s^2 that is just an approximation . the effect of the bulge and centrifugal force mean that ' g ' at the equator is about 0.5% lower than ' g ' at the poles edit : velocity at equator 40,000 km / 24 h = 1666.7 km/h = 0.463 km/s ' centrifugal g ' = ( 0.463 km/s ) ^2 ) / 6375 km = 0.03 m/s^2 or 0.3% of ' g '
the heat equation is an example of a convection-diffusion equation . your problem is one-dimensional in space ( only $x$ ) , which simplifies it a bit . the term on the left hand side is the time-rate of change of the internal energy $u$ ( often a multiple of the temperature ) . the second term is a diffusion term , as , in time , it diffuses or " smooths " peaks . the last term is a convective term , i.e. your medium is moving at constant velocity $v_0$ to the right .
so far , astronomers relies on indirect observations to detect black holes . black holes ' enormous gravitational pull not only attracts matter into its event horizon , but also strongly influences matter close to the event horizon . we may have chances to detect black holes ' hawking radiation directly during the last stage of the evaporation of light ( primordial ) black holes ( created in early universe , much less massive than other black holes ) , but these attempts all failed . check the wikipedia article about some details
the reason is shor error correction . shor demonstrated that by using 9 bits for every bit , you can reverse any decoherence event on any of the 9 bits by doing some measurements on auxiliary quantities . before the existence of error correction , it was plausible to say ( and unruh did say ) the quantum computers are unphysical , because they require no error in a macroscopic system . this is an impossible position to hold past 1996 . the error correction method has been made more efficient since , by shor and collaborators , and the upshot is that if you make a small quantum computer which is coherent for long enough time , and you can encode some dozens of qubits robustly so that you can reverse the errors faster than they occur , you can scale up the computation indefinitely without problems . this makes quantum computation feasable for sure , and there is no way to argue that it is impossible without arguing that quantum mechanics fails .
it is absolutely true that the bernoulli effect is not necessary in order for a wing to produce lift . ultimately a wing produces lift by directing air flowing over the wings downward . the can be achieved by ramming air downward through the wing 's " angle of attack " with respect to the air flow . this is why an airplane can fly upside down : the bernoulli effect plays only a minor role , and the angle of attack is the primary force in guiding air downwards . the shape of the wing is not simply designed with the bernoulli effect in mind : the goal is to produce smooth laminar flow of air over the wings so that the angle of attack ( along with the shape of the tailing edge of the wing ) can efficiently direct the air downward . a stall is when that laminar flow is disrupted such that the air no longer follows the shape of the wing and so can no longer be directed downward by the wing 's shape and angle of attack .
notice that $$\frac{l_{\bigodot}}{\pi r_{\bigodot}^2} = \sigma t_{\bigodot}^4$$ so that , dividing through the relation for an arbitrary star and that for the sun gives : $$\frac{l/l_{\bigodot}}{r^2/r_{\bigodot}^2} = t^4/t_{\bigodot}^4$$ using the other relations $$\frac{ ( m/m_{\bigodot} ) ^{3.5}}{m/m_{\bigodot}} = ( t/t_{\bigodot} ) ^4$$ or $$\left ( \frac{m}{m_{\bigodot}} \right ) ^{2.5} = \left ( \frac{t}{t_{\bigodot}}\right ) ^4$$
a voltage or current given as a complex constant is a phasor . a voltage given as the complex constant $v_z$ represents the real voltage $$v ( t ) = \operatorname{re} \left ( v_z e^{i\omega t} \right ) \ \ , $$ where $\omega$ is the voltage 's angular frequency and $t$ is time . currents represented as phasors work the same way .
it is a really complicated relationship that depends on the metallicity of the star . there is a paper that does show this though : see new grids of stellar models from 0.8 to 120 solar masses at z = 0.020 and z = 0.001 here are the geneva grids : http://obswww.unige.ch/~mowlavi/evol/stev_database.html an extensive and homogenous database of stellar evolution models for masses between 0.8 and 120 solar masses and metallicities from z=0.001 to 0.1 is available . in general the models include evolutionary phases from the main sequence up to either the end of carbon burning for massive stars , the early asymptotic giant branch phase for intermediate-mass stars , or core helium flash for low-mass stars . pre-main sequence tracks , both canonical ( i.e. . evolved at constant mass ) and accretion scenarios are also provided , as well as horizontal banches for low-mass stars . predictions regarding the spectral evolution of massive stars can further be obtained from the so-called " combined stellar structure and atmosphere models " ( costar ) . in addition to the evolutionary grids we also provide fortran codes for the calculation of isochrones and stellar population burst models . finally , references are also given to the serie of papers on stellar models with rotation . there are also older ad hoc models applied to the sun : see http://adsabs.harvard.edu/full/1981soph...74...21g and http://articles.adsabs.harvard.edu/full/1984ssrv...38..243s . they were referenced in 2011 publications though ( like pierrehumbert 's 2011 neoproterozoic climate paper ) with that being said , it is not perfect . we do not know about variations in luminosity - the sun 's luminosity has varied in cycles ( the 11-year sunspot cycle is one of them - but there may be others that last hundreds of years too - that could explain things like the maunder minimum ) . this is something that we might learn more of from kepler telescope data on stellar oscillation , as described in the chaplin et al . ( 2011 ) paper it also depends on the angle that we view the star . and we may have to correct our zero-age main sequence luminosities too ( since some stars can be unusually bright at zero-age ) . e.g. altair is unusually bright for its temperature , but several papers have concluded that it is zams rather than subgiant ( it is rotating unusually fast , which is something you mostly see in new stars )
this is where virtual particles come into play . http://youtu.be/k6i-qe8aige?t=3m23s essentially you can think of these virtual particles as temporary photons as carriers that dont exactly behave ver well with conservation of energy . the field is full of these non-conservative carriers for a very brief instant as a function of the mass of the carrier ( called a gauge boson ) . as ( rest ) massless particles photons can extend out ad infinium until they finally interact with another particle .
let us first restate the mathematical statement that two operators $\hat a$ and $\hat b$ commute with each other . it means that $$\hat a \hat b - \hat b \hat a = 0 , $$ which you can rearrange to $$\hat a \hat b = \hat b \hat a . $$ if you recall that operators act on quantum mechanical states and give you a new state in return , then this means that with $\hat a$ and $\hat b$ commuting , the state you obtain from letting first $\hat a$ and then $\hat b$ act on some initial state is the same as if you let first $\hat b$ and then $\hat a$ act on that state : $$\hat a \hat b | \psi \rangle = \hat b \hat a | \psi \rangle . $$ this is not a trivial statement . many operations , such as rotations around different axes , do not commute and hence the end-result depends on how you have ordered the operations . so , what are the important implications ? recall that when you perform a quantum mechanical measurement , you will always measure an eigenvalue of your operator , and after the measurement your state is left in the corresponding eigenstate . the eigenstates to the operator are precisely those states for which there is no uncertainty in the measurement : you will always measure the eigenvalue , with probability $1$ . an example are the energy-eigenstates . if you are in a state $|n\rangle$ with eigenenergy $e_n$ , you know that $h|n\rangle = e_n |n \rangle$ and you will always measure this energy $e_n$ . now what if we want to measure two different observables , $\hat a$ and $\hat b$ ? if we first measure $\hat a$ , we know that the system is left in an eigenstate of $\hat a$ . this might alter the measurement outcome of $\hat b$ , so , in general , the order of your measurements is important . not so with commuting variables ! it is shown in every textbook that if $\hat a$ and $\hat b$ commute , then you can come up with a set of basis states $| a_n b_n\rangle$ that are eigenstates of both $\hat a$ and $\hat b$ . if that is the case , then any state can be written as a linear combination of the form $$| \psi \rangle = \sum_n \alpha_n | a_n b_n \rangle$$ where $|a_n b_n\rangle$ has $\hat a$-eigenvalue $a_n$ and $\hat b$-eigenvalue $b_n$ . now if you measure $\hat a$ , you will get result $a_n$ with probability $|\alpha_n|^2$ ( assuming no degeneracy ; if eigenvalues are degenerate , the argument still remains true but just gets a bit cumbersome to write down ) . what if we measure $\hat b$ first ? then we get result $b_n$ with probability $|\alpha_n|^2$ and the system is left in the corresponding eigenstate $|a_n b_n \rangle$ . if we know measure $\hat a$ , we will always get result $a_n$ . the overall probability of getting result $a_n$ , therefore , is again $|\alpha_n|^2$ . so it did not matter that we measure $\hat b$ before , it did not change the outcome of the measurement for $\hat a$ . edit now let me expand even a bit more . so far , we have talked about some operators $\hat a$ and $\hat b$ . we now ask : what does it mean when some observable $\hat a$ commutes with the hamiltonian $h$ ? first , we get all the result from above : there is a simultaneous eigenbasis of the energy-eigenstates and the eigenstates of $\hat a$ . this can yield a tremendous simplification of the task of diagonalizing $h$ . for example , the hamiltonian of the hydrogen atom commutes with $\hat l$ , the angular momentum operator , and with $\hat l_z$ , its $z$-component . this tells you that you can classify the eigenstates by an angular- and magnetic quantum number $l$ and $m$ , and you can diagonalize $h$ for each set of $l$ and $m$ independently . there are more examples of this . another consequence is that of time dependence . if your observable $\hat a$ has no explicit time dependency introduced in its definition , then if $\hat a$ commutes with $\hat h$ , you immediately know that $\hat a$ is a constant of motion . this is due to the ehrenfest theorem $$\frac{d}{dt} \langle \hat a \rangle = \frac{-i}{\hbar} \langle [ \hat a , \hat h ] \rangle + \underbrace{\langle \frac{\partial \hat a}{\partial t} \rangle}_{=0\ , \text{by assumption}}$$
start with your $\hat{h} = \hbar \omega \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) $ . i will omit hat notation from this point . the commutator then reads as \begin{equation} \left [ h , a \right ] = \hbar \omega \left [ \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) a - a \left ( \hat{a}^\dagger\hat{a} + \frac{1}{2} \right ) \right ] = \hbar \omega \left ( a^\dagger a a - a a^\dagger a \right ) , \end{equation} which is nothing but \begin{equation} \left [ h , a \right ] = \hbar \omega ( a^\dagger a - a a^\dagger ) a = \hbar \omega \left [ a^\dagger , a \right ] a , \end{equation} but we know that \begin{equation} \left [ a^\dagger , a \right ] = -1 , \end{equation} therefore \begin{equation} \left [ h , a \right ] = -\hbar \omega a , \end{equation} qed . proof of the second relation is done in the same way .
antenna gain is often expressed in the following form , $g = \frac{4\pi a_{e}}{\lambda^{2}}$ , where $a_{e}$ is the effective area of the antenna and $\lambda$ is the operating wavelength . however , using the antenna equation , the effective area can be expressed in terms of the main beam width ( 3db width ) $\omega$ , $a_{e} = \frac{\lambda^{2}}{\omega}$ . assuming both antennas operate at the same wavelength , the following is true , $g_{b} = g_{a}\frac{\omega_{b}}{\omega_{a}}$ . i hope this is helpful . p.s. this essentially follows from the definition of gain .
it is not so easy to give a definition of time . we see things change when time changes , time is the fundamental measure of the evolution , which could be a local evolution , or the evolution of the entire universe itself . say that time is " slowing " would mean that there is another time-like quantity ( distinct of time ) which would be more fundamental , but , by definition , if time is a fundamental measure of the evolution , this is not possible . what you can do is the following : take an acceptable evolution variable , and use it instead of time . by acceptable evolution variable , i mean a variable $a ( t ) $ , such as there is a bijection between $t$ and $a ( t ) $ , for instance , such as $\large \frac{d a ( t ) }{dt} &gt ; 0$ you could now express physic laws with $a$ instead of t , if you wish , but it does not mean that $a$ is the fundamental measure of the evolution . for instance , in a expanding universe , the physical distance between 2 points increases with time , and in a class of models , the ratio of the physical distance at times $t_1$ and $t_2&gt ; t_1$ depends only on time and could be written : $\large \frac {\delta x ( t_2 ) }{\delta x ( t_1 ) } = \frac{a ( t_2 ) }{a ( t_1 ) } &gt ; 1$ , where $a ( t ) $ increases with $t$ . so here $a ( t ) $ is an acceptable evolution variable . note that the fact that the universe is expanding $\large \frac{d a ( t ) }{dt} &gt ; 0$ , is a different thing that saying that the expansion of the universe is accelerating , which is $\large \frac{d^2 a ( t ) }{dt^2} &gt ; 0$ dark energy ( also seen as positive cosmological constant ) is the origin of the acceleration of the expansion of the universe . ( so dark energy is not at all contradictory with the acceleration of the expansion of the universe , as you suppose )
let 's think clearly about length contraction . in the frame of reference in which an object is at rest , the measured length of an object is $l_0$ . in a frame of reference in which the object is moving with velocity $v$ parallel to the length of the object , the measured length of the object is $$l' ( v ) = l_0\sqrt{1 - \frac{v^2}{c^2}} $$ now , in your question , you wrote : however , in massless particles $v=c$ , so the lorentz factor becomes $\infty$ , meaning that an object traveling at $c$ will have $0$ length . the problem here is that , for a massless particle , there is no frame of reference in which the object is at rest ; a massless particle has speed c in all frames of reference . thus , the proper length , $l_0$ , does not exist so the formula above is not valid for massless particles . this should not be too surprising since the lorentz transformations , from which the length contraction formula above is derived , does not exist for $v = c$ since the lorentz factor $$\gamma = \frac{1}{\sqrt{1 - \frac{v^2}{c^2}}}$$ is undefined for $v=c$ . this brings us back to the question of whether or not a massless object can have extent along the direction of motion since we cannot use length contraction , as you have , to reason that it can not . i had intended to address the open question above as an update after spending some time thinking about it while mowing . in the meantime , another answer has been given that approaches this question in essentially the same manner . for what it is worth , here 's the addendum . consider the question : given the equation for the world line of a uniformly moving particle in some frame of reference , what is the equation in another relatively moving frame of reference ? working in 1-d and with standard configuration , assume a particle 's world line in the unprimed frame of reference is given by $$x ( t ) = ut + x_0$$ where $u$ is the velocity of the particle and $x_0$ is the position when $t=0$ . in the primed frame of reference , which has velocity $v$ in the unprimed frame , the particle 's world line is given by $$x' ( t' ) = u't ' + \frac{x_0}{\gamma_v ( 1 - \frac{uv}{c^2} ) } $$ where $$u ' = \frac{u - v}{1 - \frac{uv}{c^2}}$$ now , assume we have two particles with world lines given by $$x_1 ( t ) = ut$$ $$x_2 ( t ) = ut + d$$ clearly , we have $$x_2 ( t ) - x_1 ( t ) = d$$ $$x'_2 ( t' ) - x'_1 ( t' ) = \frac{d}{\gamma_v ( 1 - \frac{uv}{c^2} ) }$$ it is straightforward to show and , indeed , intuitive that , except for one special case , there is a maximum value when $v = u$ $$x'_2 ( t' ) - x'_1 ( t' ) = \frac{d}{\sqrt{1 - \frac{u^2}{c^2}}} = d_0 $$ the value $d_0$ has physical significance in that it is the separation of the world lines in the frame of reference in which the particles are at rest . this is physically significant since , in this frame of reference only , the separation can be measured without requiring synchronized , spatially separated clocks . this separation , $d_0$ is thus an invariant , an objective quantity of physical significance . in terms of $d_0$ and the speed $u$ , we can write $$d ( u ) = d_0 \sqrt{1 - \frac{u^2}{c^2}}$$ which is the familiar length contraction formula . however , for the case that $|u| = c$ ( the world lines are light-like ) , we cannot set $v = u$ ; the lorentz factor is not defined for $v = c$ . for $u = c$ , we have $$x'_2 ( t' ) - x'_1 ( t' ) = \frac{d}{\gamma_v ( 1 - \frac{cv}{c^2} ) } = d\sqrt{\frac{1+\frac{v}{c}}{1- \frac{v}{c}}}$$ thus , for the case that the particles are light-like , there is no maximum value of $d$ ; no $d_0$ that we can attach physical significance to .
these are all good questions . perhaps i can answer a few of them at once . the equation describing the violation of current conservation is $$\partial^\mu j_\mu=f ( g ) \epsilon^{\mu\nu\rho\sigma}f_{\mu\nu}f_{\rho\sigma}$$ where f ( g ) is some function of the coupling constant . it is not possible to write any other candidate answer by dimensional analysis and by parity ( assuming the current is the ordinary axial current . . . ) now we integrate both sides over $\int d^4x$ , and we find on the left hand side $\delta q$ , meaning , now that the current is violated , the charge can change while the system evolves , while the right hand side is $$f ( g ) \int d^4x \epsilon^{\mu\nu\rho\sigma}f_{\mu\nu}f_{\rho\sigma}$$ the object on the right hand side is a known topological invariant of the gauge bundle , and it is an integer ( if all the charges are appropriately quantized ) . so on the left hand side we get $\delta q$ , which must be an integer ( if all fundamental particles carry integer charge ) and the right hand side is an integer too , up to the function $f ( g ) $ . this means that the function $f ( g ) $ cannot , in fact , depend on $g$ . ( more precisely , there is a scheme where it does not . ) hence , it is exact at one loop . this is the modern proof ( without any computation ) of the abj theorem about one-loop exactness of the anomaly . so you see the deep connection between one loop and instantons . . . the violation of the conservation equation is at one loop , but to lead to interesting consequences we need to have a nontrivial gauge bundle . about some of the other comments you made : any regularization scheme that respects bose symmetry will lead to the anomaly , it is totally unavoidable . this is proven in http://inspirehep.net/record/154341?ln=en. another comment : anomalies can also arise from boson loops , for example , the trace anomaly . ( it is not one-loop exact in any sense i am aware of . )
nakahara 's book is a classic http://www.amazon.co.uk/geometry-topology-physics-edition-graduate/dp/0750306068/ref=sr_1_29?ie=utf8qid=1360865358sr=8-29 i would also suggest isham 's ( really useful for lie groups/algebras and gr ) : http://www.amazon.co.uk/differential-geometry-physicists-scientific-lecture/dp/9810235623/ref=sr_1_2?ie=utf8qid=1360865345sr=8-2
the velocity distribution is related to the temperature by the maxwell-boltzmann distribution . you cannot find the disordered kinetic energy of each particle because they are randomly distributed , however you can use the maxwell-boltzmann distribution to calculate what that random distribution looks like .
what you are suggesting is that there is a quantization law for magnetic fields , so that if the field is too small , it is actually exactly zero . this is not true in quantum electrodynamics , the field shrinks to zero as a power law , and the influence of the magnet is felt arbitrarily far out . but the experiment which you need to measure the field becomes larger and larger . given a particle of charge e , you can take it in a loop around the region with a magnetic field , to see interference fringes which depend on the area of the loop according to the phase law $$\delta \phi = \oint a dl = \int b da $$ to get an equal phase change , you need to make a bigger loop when you are far enough out . the question of the measurability of tiny fields is not purely academic . the phase method of detecting fields is extremely sensitive to tiny fields when you use a superconducting loop and measure magnetic fields by the phase the current gets around the loop , this is a squid . the squid can measure fields which would be too small to measure other ways , and it can be adjusted to measure fields that are infinitesimal with regards to less sensitive detectors . the philosophical position that all real quantities must be eventually discrete is not particularly useful , because the real quantities can arise from large-system-limits , and so be arbitrarily fine-grained as you get to larger and larger system sizes . for example , if the temperature is arbitrarily small , is it indistinguishable from exactly zero temperature ? this depends on the size of the system . if you make the system bigger , you can see the difference from zero temperature even finer . the limit of large number of photon exchanges , in a feynman particle view of the field , is the large number limit that makes a tiny field make sense . if the field is too tiny , you will only get a finite number of photons affecting your device , and the effects vanish . but if you make the device bigger , you become sensitive to more photons . these type of large n quantities can philosophically be real valued without any contradiction , because the grain-size for the real-number quantity is physical , and determined by the parameters of the system and measuring device .
you can do the transformation to the relative coordinate $\mathbf{r} = \mathbf{r}_1-\mathbf{r}_2$ and center-of-mass coordinates $m\mathbf{r} = m_1\mathbf{r}_1+m_2\mathbf{r}_2$ and do one of the integrals trivially provided the two functions inside the integrals depend only on $\mathbf{r}$ ( or only on $\mathbf{r}$ ) . otherwise you will still be left with two integrals one over $\mathbf{r}$ and the other over $\mathbf{r}$ . in the part that you refer to in pathria 's book the two functions $$f ( \mathbf{r}_1 ) = r \frac{\partial u ( r ) }{\partial r} ; \qquad f ( \mathbf{r}_2 ) = g ( \mathbf{r}_2 - \mathbf{r}_1 ) $$ depend only on $\mathbf{r}$ . a transformation to $\mathbf{r}$ and $\mathbf{r}$ coordinates then decouples the $\mathbf{r}$ integral which has given a factor of the system volume ( equation ( 16 ) in the image ) . further if the functions inside the integrand depend only on $r=|\mathbf{r}|$ , then one can transform to spherical coordinates as is done in pathria . .
i cannot claim to speak for " the community " ( whoever they might be ) , but so far i have only heard positive replies from knowledgeable people . of course , people will need to read the paper in close detail , there will be discussions in seminars etc . so it'll take at least a couple of months before there will be a serious consensus . let me give a few brief comments on the proof . first of all , much of the machinery feeds off the a-theorem proof by komargodski and schwimmer ( http://arxiv.org/abs/arxiv:1107.3987 ) and its refinements ( "lpr " http://arxiv.org/abs/arxiv:1204.5221 ) . the idea is that you can turn on a " dilaton " background and then probe the theory using these dilatons . before people studied 4-pt " on-shell " ( in a technical sense ) amplitudes of these dilatons , in the new paper they look at 3-pt functions that are off-shell . these dilaton amplitudes are connected to ( essential fourier transforms of ) matrix elements of the trace $t$ of the stress tensor . if $t = 0$ then the theory is conformal invariant . the new idea is to note that $t$ has integer scaling dimension ( $\delta = 4$ ) which means that there will be logs $$e \ln \frac{-p^2}{\mu^2}$$ in these amplitudes , and in a unitary theory $e \geq 0 . $ if $e = 0$ you can show ( using unitarity ) that $t = 0$ identically so you would be done . there is a final step ( which i have not internalized yet ) where they say that in a scale-invariant theory this $e &gt ; 0$ anomaly is not allowed because unitary gives bounds on operator dimensions , which in turn control the small-distance or large-momentum behaviour of the amplitude .
after a really brief cursory review of the literature , i think that a dalitz decay is a meson decay that involves two leptons in the final state , plus a photon . a double dalitz decay has four leptons in the final state : see this paper and this paper for examples of the usage . the dalitz decay is when a virtual photon from 2 photon decay of $\pi_0$ internally converts to a real lepton pair before it gets too far , and analogous thing for other meson or higgs processes ( two electrons from an internal photon conversion , plus a neutral object ) . i guess that the usage comes from the kinematic decay product phase space is described by a dalitz plot , hence the name . i do not think it is anything deep .
i do not think the particle-anti-particle picture is a very good one to grasp what is going on . essentially , it is a consequence of zero-point energy . in classical physics , the lowest energy state of a system , it is ground state , is zero . in quantum mechanics , its a non-zero ( but very small ) value . the easiest way to see how this zero point energy arises is through an elementary problem is quantum mechanics , the quantum harmonic oscillator . the classical harmonic oscillator is a system in which there is a restorative force proportional to the displacement . for example , a spring - the further you pull the end of a spring , the more force the spring resists your pull . modeling this system in classical physics is very easy . things are a bit different in quantum mechanics - the state of a particle is specified by its wavefunction , which encodes the probabilities of finding the particle in certain positions . another property of quantum systems is that their energies come in discrete energy levels . if you are interested in how it is worked out , you can see here . you can derive the following result for the energy levels of the particle $$e=\hbar \omega ( n+\frac {1}{2} ) $$ since n specifies the energy level , setting n to zero will give us the ground state . however , we can see this is not zero - so the lowest possible state of a quantum system still contains some energy . in a practical example , liquid helium does not freeze under atmospheric pressure at any temperature because of its zero-point energy . one very important thing to note is the following - zero-point energy does not violate the conservation of energy . a common explanation is that the uncertainty principle allows particles to violate it ' if they are quick enough ! ' . this simply is not true . from the wiki page on conservation of energy : in quantum mechanics , energy of a quantum system is described by a self-adjoint ( hermite ) operator called hamiltonian , which acts on the hilbert space ( or a space of wave functions ) of the system . if the hamiltonian is a time independent operator , emergence probability of the measurement result does not change in time over the evolution of the system . thus the expectation value of energy is also time independent . the local energy conservation in quantum field theory is ensured by the quantum noether 's theorem for energy-momentum tensor operator . note that due to the lack of the ( universal ) time operator in quantum theory , the uncertainty relations for time and energy are not fundamental in contrast to the position momentum uncertainty principle , and merely holds in specific cases ( see uncertainty principle ) . energy at each fixed time can be precisely measured in principle without any problem caused by the time energy uncertainty relations . thus the conservation of energy in time is a well defined concept even in quantum mechanics . now , on to your question - in quantum field theory , all particle are modeled as excitations of fields . that is , every particle has an associated field . for the particles that carry forces , these are the familiar force fields - such as the electromagnetic field . fields take a value everywhere in space . now , in classical mechanics , this value would be zero in most places . however , as we saw above , the ground state of a quantum field is non-zero . so , even in empty space ( or ' free space' ) these fields have a a very small value . so , empty space has vacuum energy .
nature has no preferences , and therefore entropy tends to increase . sounds paradoxical ? the point is that each microscopic state ( describing the exact position and velocity of each atom in the system ) is equally likely . however , what we typically observe is not a micro state , but a course-grained description corresponding to incredibly many micro states . certain macro states correspond to far fewer micro states than other macro states . as nature has no preference for any of these micro states , the latter macro states are far more likely to occur . the evolution to ever more likely macro states ( until the most likely macro state , the equilibrium state , is reached ) is called the second law of thermodynamics . the decrease of potential energy is the consequence of the first ( energy conservation ) and second ( evolution to more likely macro states ) law of thermodynamics . as macro states with a lot of energy stored in heat ( random thermal motion ) contain many more micro states and are therefore much more likely , energy tends to get transferred from potential energy to thermal energy . this is observed as a decrease in potential energy .
the color will not change . what you are not taking into account is the speed of light in the medium . it is not the same $ c $ en vacuo . the frequency stays the same . what changes is that speed of light in the refracting medium and as a result wavelength . this difference for speed is the exact reason we have refractive effects , and i believe was the observation that led to snell 's law . in symbols $$ \lambda = \frac {c} {\nu} $$ where $\lambda$ is the wavelength and $ \nu $ is the frequency . the speed of light changes because the photons have to have it is energy ( and therefore it is presence ) propagated across very long molecular chains . electrons have to absorb the incident photons , re-emit , and repeat this in a longitudinal direction . depending on what the material is made of , this will take variable time in variable media . that notion manifests itself in the different indices of refraction that different objects have .
gyrochronology is semi-empirical in the sense that there is some justification for the temporal dependence that you mention . there is a line of argument for the $t^{1/2}$ dependence and it can be found on pp7-8 of this pedagogical review by jerome bouvier . http://arxiv.org/pdf/1307.2891v1.pdf the basic idea is of a spherically symmetric , ionised wind that corotates with the star , held by its radial magnetic field , but which decouples from the magnetic field at some distance away from the star carrying away angular momentum . further assuming a linear dynamo model , such that the magnetic field scales linearly with rotation rate , yields an angular momentum loss rate that is proportional to rotation rate cubed . equating this to $i d\omega/dt$ ( assuming a constant moment of inertia ) leads to $\omega \propto t ^{-1/2}$ ( or $p \propto t ^{1/2}$ ) . some limitations of this model ( and gyrochronology in general ) are reviewed here ( by me ! ) . http://arxiv.org/abs/1404.7156 for instance on the pre-main-sequence you can not assume a constant $i$ ; there are different ideas for how magnetic field scales with rotation rate ; different ideas about magnetic topologies and different ideas for how the wind decouples from the magnetic field at large distances . all of these things mean it would be a surprise if rotation period was exactly proportional to the square root of time . the initial conditions also play a role at young ages - although the angular momentum losses lead to a convergence of rotation periods , this takes time , and is the dominant source of uncertainty ( along with differential rotation ) , even at older ages . so , the approach taken is to assume that rotation period can be represented by the product of a time-dependent function ( usually taken to be $t^{n}$ ) and another function representing a mass-dependence . observationally , the situation is that $n$ is found to be 0.52-0.57 by comparing the rotation periods of young sun-like stars and the sun itself . ( e . g . mamajek and hillenbrand 2008 , apj , 687 , 1264 ) . but this relationship is poorly calibrated at lower masses and also for stars older than the sun .
alright , i will try to answer why we need dirac eigenstates in this procedure , but i am not sure if it is anything more that the tautology that the fujikawa method is precisely defined by using the dirac eigenstates . let me briefly recap what the idea is : ( all of this is for a euclidean theory . ) we consider the infinitesimal local transformaton $$\psi ( x ) \mapsto ( 1 + \mathrm{i}\alpha ( x ) \gamma_5 ) \psi ( x ) \equiv \psi' ( x ) $$ which , as it is a gauge transformation , should not change the value of the path integral : $$ z = \int \mathcal{d}\psi\mathcal{d}\bar\psi \mathrm{e}^{\int\bar\psi \mathrm{i}\not{d}\psi\mathrm{d}^4x} \overset{ ! }{=} \int\mathcal{d}\psi'\mathcal{d}\bar\psi'\mathrm{e}^{\int \bar\psi\not{d}\psi\mathrm{d}^4x + \int\alpha\partial_\mu j_5^\mu\mathrm{d}^4x}$$ to derive the anomaly term , we must examine what the change of the measure $\mathcal{d}\psi \mapsto \mathcal{d}\psi'$ is . in general , we can say that it is , by the usual transformation formulae for grassmannian integrations , $\det ( m ) ^{-1}$ for the operator acting as $\psi ' = m\psi$ . now , we must recall that the functional measure $\mathcal{d}\psi$ is only defined in the limit of some ( uv ) regularized theory , it does not exist on its own . . so to actually calculate the correct $\det ( m ) $ , we must obtain it as the limit $\lim_{\lambda \rightarrow \infty}\det ( m_\lambda ) $ of some $m_\lambda$ and some uv cutoff scale ( not necessarily hard ) $\lambda$ . uv regulators suppress high momenta . and what are the eigenstates of the dirac operator ? . . . right , they are the momentum modes ! so , the most natural regularisation of our theory is to exponentially suppress states with high dirac eigenvalues as per $$ \widetilde{\psi}_n = \mathrm{e}^{-\frac{\not{d}^2}{2\lambda}}\psi_n$$ where the $\psi_n$ are the unregularised dirac eigenstates . and by choosing the regulator that way , the only consistent way to define the measure is to see it as the integral over the coefficients of these modes , i.e. $$\mathcal{d}\psi = \lim_{\lambda \rightarrow \infty}\prod_n \mathrm{d}\widetilde{a}_n$$ where $\psi = \sum_n \widetilde{a}_n\widetilde{\psi}_n$ . ( the sum over the $n$ is actually an integral if we use no ir cutoff , but it does not matter here , you could as well use one , but it clutters notation a bit and contributes nothing insightful . ) now , the rest of the fujikawa anomaly method follows through as hopefully described in your book . i hope this is at least an approximate answer to your question .
in principle , it is very simple and straightforward . the problem is to map out the region where the integer filling state is the ground state . suppose you have $l$ sites . take $n=l$ particles , find its ground state energy , which is denoted as $e_g ( l ) $ . note that here the hamiltonian does not contain the $\mu $ term . do it again for $n=l+1$ , the ground state energy is $e_g ( l+1 ) $ . then , you know below the line $\mu_+ = e_g ( l+1 ) - e_g ( l ) $ the $n=l$ state is the lower state with respect to the full hamiltonian containing $\mu$ . do it once again with $n=l-1$ , then you know above the line $\mu_- = e_g ( l ) - e_g ( l-1 ) $ the $n=l$ state is the lower state . therefore , between the two lines , the $n=l$ state is the lowest state . in this region , the unity filling state is the ground state . this is the first mott lobe . the idea is simple , but i really doubt you can get accurate results with ed . you had better do it with dmrg .
the simplest way to think of avogadro 's number is as a unit conversion factor . just as there are 2.54 centimeters in an inch , there are avogadro 's number atomic mass units in a gram . accordingly , if you know the atomic mass of an element you can count the number of atoms in one gram of it . let 's take carbon as an example . the most common isotope of carbon has an atomic mass of 12 . there are rare forms of carbon that are heavier because they have extra neutrons , but we will ignore them here . thus 1 gram of carbon contains $\frac{\text{avogadro&#39 ; s number}}{12}$ carbon atoms . likewise if we had $6.02\times10^{23}$ carbon atoms , we had have 12 grams of carbon .
the easiest way to get the exact behavior is from thinking about light as a classical wave interacting with the atoms in the solid material . as long as you are far away from any of the resonant frequencies of the relevant atoms , this picture is not too bad . you can think each of the atoms as being like a little dipole , consisting of some positive and some negative charge that is driven back and forth by the off-resonant light field . being an assemblage of charges that are accelerating due to the driving field , these dipoles will radiate , producing waves at the same frequency as the driving field , but slightly out of phase with it ( because a dipole being driven at a frequency other than its resonance frequency will be slightly out of phase with the driving field ) . the total light field in the material will be the sum of the driving light field and the field produced by the oscillating dipoles . if you go through a little bit of math , you find that this gives you a beam in the same direction as the original beam-- the waves going out to the sides will mostly interfere destructively with each other-- with the same frequency but with a slight delay compared to the driving field . this delay registers as a slowing of the speed of the wave passing through the medium . the exact amount of the delay depends on the particulars of the material , such as the exact resonant frequencies of the atoms in question . as long as you are not too close to one of the resonant frequencies , this gives you a really good approximation of the effect ( and " too close " here is a pretty narrow range ) . it works well enough that most people who deal with this stuff stay with this kind of picture , rather than talking in terms of photons . the basic idea of treating the atoms like little dipoles is a variant of " huygens 's principle , " by the way , which is a general technique for thinking about how waves behave .
the correct option is really option 3 . most of the time when a physicist says a theory is renormalizable , they mean that the theory is a relevant deformation of some conformal field theory . this is a non-perturbative definition . it contains the physically meaningful content that the other more technical definitions about counterterms in perturbation theory are attempting to capture . indeed , it implies them . ( however , the reverse implication is not always true . for example , perturbative qed is renormalizable in the sense of option 2 , but there is no underlying non-perturbative qed , so one can not even ask about option 3 . ) it is good practice to always try to think about the non-perturbative meaning of the physical formalism you are studying . perturbation theory is a sometimes useful tool for computations , but it can obscure the physics in a cloud of virtual technicalities . so what does it mean for a theory to be a relevant deformation of a cft ? it means that there is a cft whose observables are essentially the same as the observables in your qft , and that you can compute any correlation function in the qft as $\langle \mathcal{o} \mathcal{o}' . . . \rangle_{qft} = \langle \mathcal{o} \mathcal{o}' . . . e^{\sum_i g_i \int\mathcal{o}_i} \rangle_{cft}$ where the $\mathcal{o}_i$ are relevant operators in the cft and $g_i$ are ( dimensionful ) coupling constants . knowing that your qft is near a cft in this sense is what allows you to study the behavior of the qft 's expectation values under changes of scale , which is the heart of the renormalization group analysis . edit : first , an easy example : the free scalar field theory is a conformal field theory . this theory is basically described by $\langle \mathcal{o} \rangle_{cft} = \int \mathcal{o} ( \phi ) e^{i\int |d\phi|^2} \mathcal{d}\phi$ . in this theory , in dim > = 3 , the operator $\phi^2 ( x ) $ is relevant , so we can deform with this term and get a non-conformal field theory . the expectation value in this qft is then described by $ \langle \mathcal{o} \rangle_{qft} = \langle \mathcal{o} e^{i m^2 \int \phi^2 ( x ) dx}\rangle_{cft} = \int \mathcal{o} ( \phi ) e^{i\int [ |d\phi ( x ) |^2 + m^2\phi^2 ( x ) ] dx} \mathcal{d}\phi . $ so , not surprisingly , the theory we get by deforming the free scalar cft with a mass term is the massive free scalar . second , a subtlety that i should point out . the first equality above is not exact in most situations . the problem is the deformations we want may not be integrable with respect to the cft 's path integral measure , thanks to uv singularities . this is dealt with by regularizing . so , in most qfts , what we get is a family of approximations $\langle \mathcal{o} \mathcal{o}' . . . \rangle_{qft} \simeq \langle \mathcal{o} \mathcal{o}' . . . e^{\sum_i g_i ( \lambda ) \int\mathcal{o}_i ( \lambda ) } \rangle_{cft}$ where the relevant operators and the coupling constants depend on a cutoff scale $\lambda$ and the errors vanish as $\lambda \to \infty$ .
it is a bad question . for one thing , answer ( c ) is utter nonsense . ( maybe that is a bit harsh . it might be just regular nonsense . ) in order for something to convert gravitational potential energy into kinetic energy , it has to drop to a lower height under the influence of gravity . this does not happen during a collision . collisions in physics are effectively instantaneous events ; they occur at one point in space and time and then they are over and done with . there is no change in height by which gpe could be converted into ke during the collision . whatever ( kinetic ) energy the balls run away with , they had to obtain it from the kinetic energy that the cart had coming into the collision . now , the kinetic energy of the cart at the point of the collision was converted from the gravitational potential energy that the cart had higher up the ramp . but that conversion was done by the cart alone ; the balls had nothing to do with it . the other reason i do not like this problem is that they do not tell you at which point on the ramp the cart has the speed of $5\text{ m/s}$ . it is possible that the cart maintains a constant velocity as it goes down the incline , but that would require some mechanism to keep the cart from accelerating , and if some such mechanism is involved , it should be mentioned in the problem . if that is the case , the gravitational potential energy that the cart started out with would have been converted into some other form of energy , not kinetic . it might be heat , electricity , spring energy , etc . but there is no way to know unless they tell you what mechanism is keeping the cart from accelerating . in a pinch , if you encountered this problem on the test and did not have any opportunity to ask for clarification , i would just assume that $5\text{ m/s}$ is the speed at the end of the ramp , immediately prior to when the cart hits the balls . why ? the alternative is that the problem is unsolvable . if the speed of the cart coming into the collision is not $5\text{ m/s}$ , you have no other information that would allow you to calculate what it is . ( self-check : do you understand why this is the case ? ) once you assume that the speed of the cart coming into the collision is $5\text{ m/s}$ , you have a collision of 3 objects , each of which has a mass and initial and final velocities . all 3 masses , all 3 initial velocities , and two of the final velocities are known , so you should have enough information to solve for the third . if you do not find any solution , then the situation is impossible and the answer is ( d ) ; on the other hand , if you do find a solution for the final velocity of the cart , then that velocity will distinguish between choices ( a ) ( $v_f = 0$ ) , ( b ) ( $v_f &lt ; 5\text{ m/s}$ ) , and ( c ) ( $v_f = 5\text{ m/s}$ , if you ignore the stuff about energy being converted ) .
i do not understand question 1: where does he equate a speed to a position ? as far as question 2 is concerned , it is basically what davephd said , but maybe i can extend it a bit more saying something about the conservation of linear momentum : along the x-direction , there is no external force ( because gravity points downwards only , assuming a flat surface ) so the linear momentum of the projectile is conserved . since $p_x = mv_x$ , $v_x$ is constant .
suppose you start with a linear charge density $\lambda^+$ of positive charges and $-\lambda^-$ of negative charges in the wire , everything at rest . case 1: no current , test charge stationary you assume you have a neutral wire with no current . therefore $\lambda^- = \lambda^+$ . there is no other frame worth considering , since nothing is in motion anyway . even if you did go into another frame , any change in charge density will affect electrons and nuclei equally . thus the wire is neutral in all frames , and test charges are entirely unaffected by it . case 2: nonzero current , test charge moving with electrons now suppose you have a wire with a current . again , the wire is neutral in the lab frame $s$ , where the bulk of it is not moving . in this frame , we still must have $\lambda_s^- = \lambda_s^+$ , even though the electrons are moving and the nuclei are not . if we slip into the rest frame $s'$ of the bulk electron motion , then the spacing between electrons must be different , and in fact it must be larger . since charge does not change when changing frames , we know $\lambda_{s'}^- &lt ; \lambda_s^-$ . similarly , the nuclei spacing will be length-contracted , so $\lambda_{s'}^+ &gt ; \lambda_s^+$ . in this frame , then , $\lambda_{s'}^+ &gt ; \lambda_{s'}^-$ , so the wire looks positively charged , and any ( positive ) test charge at rest in this frame $s'$ will be repelled . as you can check , this is exactly what the lorentz force law tells you . if the electron bulk motion is in the $-z$-direction , then the current is in the $+z$-direction , and the magnetic field along the $+x$-axis ( assuming the wire coincides with the $z$-axis ) is in the $+y$-direction . a positive charge with velocity in the $-z$-direction in a magnetic field in the $+y$-direction will experience a force in the direction of $ ( -\hat{z} ) \times ( +\hat{y} ) = +\hat{x}$ , away from the wire . case 3: nonzero current , test charge stationary now consider the setup as follows . in $s$ , the nuclei and test charge are stationary , but the electrons are moving in the $-z$-direction . just as before , we can transform into the electrons ' rest frame , where we will find that the wire is positively charged . however , we also have that the test charge is moving in the $+z$-direction in $s'$ , and that there is a current of positive charges in the $+z$-direction ( which we could neglect earlier ) . here the full lorentz force law tells us there is a $qe$ repulsion , and also a $q \vec{v} \times \vec{b}$ attraction , and in fact they perfectly balance in this frame , so there is still no net force . summary the space between electrons expands only if you keep yourself in their rest frame as you accelerate them . the spacing measured by an observer who does not accelerate is unchanged , in keeping with the assumption that the wire stays neutral in the lab frame . you can only use the electrostatic coulomb 's law if you are in the frame where the test charge of interest is stationary . if you are in a frame where the charge is still moving , you need the full lorentz law , using whatever electric and magnetic fields are present in that frame .
regarding your first question : what is written down in salmhofer 's book is a very technical approach to the generating functional method . it allows one to express correlation functions as functional derivatives with respect to some auxiliary field $j$ , which is set to zero before arriving at the final result . a less mathematically rigorous explanation is given in chapters 6-8 of srednicki and chapter 9 of peskin and schroeder . regarding your second question and wick ordering in general : it implies that the integral of any wick ordered polynomial vanishes under the given measure of integration , i.e. a gaussian measure for a given correlation function $c$ . for a discussion of the formalism , i would refer to these notes .
since this is a homework problem , i will only provide a sketch of the solution . from the conservation laws , we have the three equations $$\begin{align} \tag{1} m_1v_{1i} - m_1v_{1f}\cos \theta and = m_2v_{2f} \cos \phi , \\ \tag{2} m_1v_{1f}\sin \theta and = m_2v_{2f}\sin \phi , \\ \tag{3} m_1v_{1i}^2 - m_1v_{1f}^2 and = m_2v_{2f}^2 . \end{align}$$ summing the squares of ( 1 ) and ( 2 ) eliminates $\phi$ . the rhs of the resultant equation contains $v_{2f}^2$ which can be eliminated using ( 3 ) . then , one would obtain a quadratic equation in terms of $\frac{v_{1f}}{v_{1i}}$ , which can be solved to obtain the desired equation $$\frac{v_{1f}}{v_{1i}} = \frac{m_1}{m_1+m_2}\left [ \cos \theta \pm \sqrt{\cos^2 \theta - \frac{m_1^2-m_2^2}{m_1^2}}\right ] . $$ for the next equation , we rotate the axes to obtain the angle $\theta+\phi$ more easily . here , the conservation laws are $$\begin{align} \tag{4} m_1v_{1i}\cos\phi - m_1v_{1f}\cos ( \theta+\phi ) and = m_2v_{2f} , \\ \tag{5} m_1v_{1i}\sin \phi and = m_1v_{1f}\sin ( \theta+\phi ) , \\ \tag{6} m_1v_{1i}^2 - m_1v_{1f}^2 and = m_2v_{2f}^2 . \end{align}$$ first , we square ( 4 ) and use ( 6 ) to eliminate $v_{2f}$ . then , we use ( 5 ) to eliminate $v_{1i} , v_{if}$ from the resultant equation , obtaining an equation in terms of $\phi$ and $\theta+\phi$ . using trigonometric identities , we get an equation in terms of $\tan\phi$ and $\tan ( \theta+\phi ) $ only . then , this equation can be rewritten as a quadratic equation in $\frac{\tan\phi}{\tan ( \theta+\phi ) }$: $$\left [ 1-\frac{\tan\phi}{\tan ( \theta+\phi ) } \right ] ^2=\frac{m_2}{m_1}\left [ 1-\frac{\tan^2\phi}{\tan^2 ( \theta+\phi ) }\right ] , $$ which can be solved to obtain the desired equation $$\frac{\tan ( \theta +\phi ) }{\tan ( \phi ) }=\frac{m_1+m_2}{m_1-m_2} . $$
to understand this paradox it is best to forget about everything you know ( even from sr ) because all of that just causes confusion and start with just a few simple concepts . first of them is that the space-time carries a metric that tells you how to measure distance and time . in the case of sr this metric is extremely simple and it is possible to introduce simple $x$ , $t$ coordinates ( i will work in 1+1 and $c = 1$ ) in which space-time interval looks like this $$ ds^2 = -dt^2 + dx^2$$ let 's see how this works on this simple doodle i put together the vertical direction is time-like and the horizontal is space-like . e.g. the blue line has " length " $ds_1^2 = -20^2 = -400$ in the square units of the picture ( note the minus sign that corresponds to time-like direction ) and each of the red lines has length zero ( they represent the trajectories of light ) . the length of the green line is $ds_2^2 = -20^2 + 10^2 = -300$ . to compute proper times along those trajectories you can use $d\tau^2 = -ds^2$ . we can see that the trip will take the green twin shorter proper time than the blue twin . in other words , green twin will be younger . more generally , any kind of curved path you might imagine between top and bottom will take shorter time than the blue path . this is because time-like geodesics ( which are just upward pointing straight lines in minkowski space ) between two points maximize the proper time . essentially this can be seen to arise because any deviation from the straight line will induce unnecessary space-like contributions to the space-time interval . you can see that there was no paradox because we treated the problem as what is really was : computation of proper-time of the general trajectories . note that this is the only way to approach this kind of problems in gr . in sr that are other approaches because of its homogeneity and flatness and if done carefully , lead to the same results . it is just that people often are not careful enough and that is what leads to paradoxes . so in my opinion , it is useful to take the lesson from gr here and forget about all those ad-hoc sr calculations . just to give you a taste what a sr calculation might look like : because of globally nice coordinates , people are tempted to describe also distant phenomena ( which does not really make sense , physics is always only local ) . so the blue twin might decide to compute the age of the green twin . this will work nicely because it is in the inertial frame of reference , so it'll arrive at the same result we did . but the green twin will come to strange conclusions . both straight lines of its trajectory will work just fine and if it were not for the turn , the blue twin would need to be younger from the green twin 's viewpoint too . so the green twin has to conclude that the fact that blue twin was in a strong gravitational field ( which is equivalent to the acceleration that makes green twin turn ) makes it older . this gives a mathematically correct result ( if computed carefully ) , but of course , physically it is a complete nonsense . you just can not expect that your local acceleration has any effect on a distant observer . the point that has to be taken here ( and that gr makes clear only too well ) is that you should never try to talk about distant objects .
time does run more slowly inside a massive spherical shell than outside it , however you could not stop time this way because if you made the shell massive enough it would collapse into a black hole . you need to be very careful talking about gravitational time dilation as it is easy to misunderstand what is happening . no observer will ever see their own clock running at a different speed , that is every observer still experiences time passing at the usual one second per second . however if two observers in different places compare their clocks they may find that their clocks are running at different rates . the best know example of this is the static black hole , which is described by the schwarzschild metric . if an observer at infinity and an observer at a distance $r$ from the black hole compare their clocks they will find the clock near the black hole is running more slowly . the ratio of the speeds of the clocks is : $$ \frac{\delta t_r}{\delta t_\infty} = \sqrt{1 - \frac{2gm}{c^2r}} \tag{1} $$ if we graph this ratio as a function of $r/r_s$ , where $r_s = 2gm/c^2$ , we get : and you can see that the ratio goes to zero , i.e. time freezes when $r/r_s = 1$ . this value of $r$ is actually the position of the black hole event horizon , and what we have discovered is the well known phenomenon that time stops at the event horizon of a black hole . but note that when i say time stops i mean it stops relative to the observer at infinity . if you were falling into a black hole you would not notice anything odd happening to your clock as you crossed the event horizon . you specifically asked about a spherical shell . the easy way to calculate the time dilation is to use the weak field expression : $$ \frac{\delta t_r}{\delta t_\infty} = \sqrt{1 - \frac{2\delta\phi}{c^2}} $$ where $\delta\phi$ is the difference in gravitational potential relative to infinity . if we have a spherical shell of mass $m$ and radius $r$ then the time dilation at the outer surface is just : $$ \frac{\delta t_{outer}}{\delta t_\infty} = \sqrt{1 - \frac{2gm}{c^2r_{shell}}} \tag{2} $$ so it is the same as the expression for the black hole given in ( 1 ) . if we can assume the thickness of the shell is negligable the potential remains constant as we cross the shell and go inside it , so everywhere inside the shell the time dilation remains constant and is given by equation ( 2 ) . now you can see why you can not stop time inside a spherical shell . to make the ratio of times zero you need to decrease the shell radius to $r_{shell} = 2gm/c^2$ . but this is the black hole radius so our shell has now turned into a black hole .
you need to work out the tensor product and will find a direct sum of different contributions \begin{multline} [ ( 1/2 , 0 ) \oplus ( 0 , 1/2 ) ] \otimes [ ( 1/2 , 0 ) \oplus ( 0 , 1/2 ) ] =\\ \big ( ( 1/2 , 0 ) \otimes ( 1/2 , 0 ) \big ) \oplus \big ( ( 1/2 , 0 ) \otimes ( 0 , 1/2 ) \big ) \oplus \quad \\\big ( ( 0 , 1/2 ) \otimes ( 1/2 , 0 ) \big ) \oplus \big ( ( 0 , 1/2 ) \otimes ( 0 , 1/2 ) \big ) = \\ ( 0 , 0 ) \oplus ( 1 , 0 ) \oplus ( 1/2 , 1/2 ) \oplus ( 1/2 , 1/2 ) \oplus ( 0 , 1 ) \oplus ( 0 , 0 ) \end{multline} the states now can be classified : $ ( 0 , 0 ) $ is a scalar or pseudoscalar , i.e. the $\bar \psi \psi$ you are looking for as well as $\bar \psi \gamma_5 \psi$ $ ( 1/2 , 1/2 ) $ is the vector / pseudovector component $\bar \psi \gamma^\mu \psi$ or $\bar \psi \gamma^\mu \gamma_5 \psi$ ( 1 , 0 ) and ( 0 , 1 ) are the ( anti ) -self dual parts of the tensor $\bar \psi \sigma^{\mu \nu } \psi$ all these transform well-definedly under lorenty boosts . the $ ( 0 , 0 ) $ part tells you that this rep will transform neither under the left-chirality nor the right-chirality $sl ( 2 ) $ that you classify the reps by . edit : let me add that the distribution law i used above to get from the first to the second line is one of reasons we speak of a " direct sum " vs . " direct product " .
he is not counting the earth in $a$ or $b$ . $a$ counters the earth and so does $b$ . having them both is overkill and gives you an upward force .
first , the critical dimension . there are many ways ( seemingly inequivalent ways but ultimately bound to give the same result ) to calculate $d=10$ for the superstring that mirror the methods to calculate $d=26$ for the bosonic string . for the bosonic string , one may use a conformally invariant world sheet theory . because of the residual conformal symmetry , it has to have $bc$ ghosts . the central charge of the $bc$ system is $c=1-3k^2$ where $k=2j-1$ where $j$ is the dimension of the $b$ antighost , in this case $j=2$ . you see that my formulae imply $k=3$ and $c=1-27=-26$ so one has to add 26 bosons , i.e. 26 dimensions of spacetime , to get $c=0$ in total . now , for the superstring , the local symmetries on the world sheet are enhanced from the ordinary conformal group to the $n=1$ superconformal group . one needs to add the $\beta\gamma$ ( bosonic ) ghosts for the new ( fermionic ) generators . their dimension is $j=3/2$ , different from $j=2$ of $bc$ by $1/2$ , as usual for the spin difference of things related by supersymmetry . you see that $k=2j-1=2$ and $3k^2-1=12-1=11$ . now , the central charge of $\beta\gamma$ is $3k^2-1$ and not $1-3k^2$ , the sign is the opposite one , because they are bosons . so the $bc$ and $\beta\gamma$ have $c=-26+11=-15$ . this minus fifteen must be compensated by 10 bosonic fields and 10 fermionic fields ( whose $c=1/2$ per dimension : note that a fermion is half a boson ) and $10+10/2=15$ so that the total $c=0$ . if some of the steps are not understandable above , it is almost certainly because the reader is not familiar with basics of conformal field theory and it is not possible to explain conformal field theory without conformal field theory . it is a whole subject , not something that should be written as one answer on this server . in this formalism with the new world sheet fermions $\psi^\mu$ transforming as spacetime vectors , one has to protect the spin-statistics relationship . vector-like fermions violate it so they are only allowed in pairs . this is achieved by the gso projection  well , there are actually two gso projections , one separate for left-movers and one for right-movers . only 1/4 of the states are kept in the spectrum . the projection is a flip side of having four sectors  the left-moving and right-moving fermions may independently be periodic or antiperiodic . i wrote about the gso projection a month ago : http://motls.blogspot.com/2012/11/david-ian-olive-1937-2012.html?m=1 again , if anything is incomprehensible and incomplete , it is because it is not really one isolated insight that a layman may understand from one sentence . it is one of many technical results that follows from a large subject  string theory  that has to be systematically studied if one wants to understand it .
some links may be helpful : how to learn physics ? ( an answer of mine ) specifics about what to learn , how to learn . the baez crackpot index very important . protects you from crackpots and in a humorous way . you may want to change your username after reading that , too : ) gerard ' t hooft 's advice quite incomplete , i hope it will be completed soon . basically , follow the order ( for learning ) i give in my linked answer , since it prevents you from becoming a crack , and helps you go in the right direction . learn the math ( s ) . mathematics is a generalisation of physics , and physics is all about finding which is the right special case of mathematics . do not get too excited by popular science , such as mkaku 's books , which may make you a crackpot , too . do not get misdirected by trolls , either . do not get into the mathematician 's world either , rigorously proving everything obvious . i honestly get very scared by math overflow for this very reason . read trf read ron maimon 's , lubos motl 's , mitchell porter 's , urs schreiber 's , moshe is , squarks 's and dilaton 's posts on [ physics . se ] / . .
[ a black hole ] has the same mass of the star that gave it origin no , not really . a stellar-mass black hole is formed after a star with mass around 20 times that of our sun collapses due to lack of core fusion and by some ( as of yet ) unknown process , rebounds and explodes . this explosion , a type ii supernova , kicks off something like 90% of its matter . thus , the black hole would be something around the 5 solar mass range . is there a way to calculate the mass of a black hole well you can not actually observe black holes in free space ( because you can not see them ) , you need to infer it by using something around it ( either a star or gas cloud ) and computing the mass from kepler 's laws .
the units for torque , as you stated , are newton-meters . although this is algebraically the same units as joules , joules are generally not appropriate units for torque . why not ? the simple answer is because $$w = \vec f \cdot \vec d$$ where $w$ is the work done , $\vec f$ is the force , $\vec d$ is the displacement , and $\cdot$ indicates the dot product . however , torque on the other hand , is defined as the cross product of $\vec r$ and $\vec f$ where $\vec r$ is the radius and $\vec f$ is the force . essentially , dot products return scalars and cross products return vectors . if you think torque is measured in joules , you might get confused and think it is energy , but it is not energy . it is a rotational analogy of a force . per the knowledge of my teachers and past professors , professionals working with this prefer the units for torque to remain $n \ m$ ( newton meters ) to note the distinction between torque and energy . fun fact : alternative units for torque are joules/radian , though not heavily used .
you have many comments to the effect that " topology is needed to describe continuity , calculus concepts , the notion of " looks like " , homeomorphism and so forth " . and these are all altogether right , but i am getting that your question is about the global picture . also , the following is mainly about a toplological or differentiable manifold ; joshphysics 's link shows that there are many other concepts of manifold . we begin with the notion of " locally looks like $\mathbb{r}^n$" ; but you can have a set $\mathbb{m}$ of any kind of weird creatures whose subsets you can put into one-to-one , surjective correspondence with some open ( more about this below ) subset of $\mathbb{r}$ ( wontedly a simply connected neighbourhood of the origin ) . for one of these subsets $\mathcal{n}$ you have a " labeller " map $\lambda:\mathcal{n}\to\mathbb{r}^n$ . then you notions of open , neighbourhood and all the rest of it arise by definition : a subset $\mathcal{o}\subset\mathcal{n}$ is open iff $\lambda ( \mathcal{o} ) $ is open in $\mathbb{r}^n$ . likewise a " path " $\sigma:\mathbb{r}\to\mathcal{n}$ is $c^0 , \ , c^1\ , c^\omega$ or whatever iff $\lambda\circ \sigma:\mathbb{r}\to\mathbb{r}^n$ has the same property . all topology , neighbourhood , calculus , differentiability and so forth concepts are then defined by " fiat " , and the need for the concepts is why we want our zoo of weird creatures to " locally look like $\mathbb{r}^n$" in the first place so this is all highly intuitive and obvious . so i am guessing ( also by reading your other probing questions on this site ) that you already understand all this . so the crucial question is then that of transistion maps and how we glue all our local copies of $\mathbb{r}^n$ together . going back to our subset $\mathcal{n}\subset\mathbb{m}$:there are other " local copies " of $\mathbb{r}^n$ that bestow our topological / calculus and so forth concepts on subsets of $\mathbb{m}$ other than $\mathcal{n}$ . but these subsets must overlap , because , when we are doing calculus or toplogy or dynamics or whatever , we do not want suddenly to run into a " co-ordinate wall " and have to jump suddenly from one co-ordinate system to another . as an example , suppose we have a spacecraft in an einstein manifold ( universe that is a vacuum solution to the efes ) . for calculus , measurement and other mathematical concepts , we need always to be able to define the manifold in a neighbourhood around the spaceship : so , as the spaceship nears the boundary of one co-ordinate system , it must also be describable by another co-ordinate system wherein we can carve out a " neighbourhood": we could not do this if our co-ordinate systems did not overlap but instead partitioned the manifold $\mathbb{m}$ . otherwise put , in relativity , the boundary between co-ordinate systems is an artifact of our particular mathematical description of the physics , it does not belong to the physics . another , dramatic , example is the phenomenon of gimbal lock in euler angle charts for the unit sphere that very nighly cost the apollo 11 astronauts their lives , cost many pilots their lives in the years before then and is the reason why the software processing signals from fibre ring sagnac gyros that keep you safe in a commercial jetliner either manipulate the aeroplane 's calculated orientation in two overlapping charts covering $so ( 3 ) $ or , more recently , model the aeroplane 's orientation by unit quaternions in $so ( 3 ) $ 's double cover $su ( 2 ) $ . so , our overlap is very much needed , so many , if not all , regions in the manifold can be described by more than one local copy of $\mathbb{r}^n$ with more than one labeller . so , suppose we have two regions $\mathcal{n}_1 , \ , \mathcal{n}_2$ with labellers $\lambda_1:\mathcal{n}_1\to\mathbb{r}^n$ , $\lambda_2:\mathcal{n}_1\to\mathbb{r}^n$: we must make sure that these labellers yield consistent notions of opennes , neighbourhood , differentiability and all the rest of it in a region $\mathcal{n}_1\cap\mathcal{n}_2$ . so , a set $\mathcal{o}\subseteq\mathcal{n}_1\cap\mathcal{n}_2$ must be open as reckonned by labeller $\lambda_1$ and $\lambda_2$ and so $\lambda_1\circ\lambda_2^{-1}$ and $\lambda_2\circ\lambda_1^{-1}$ , the " transition maps " between charts , must be local homeomorphisms , analytic , diffeomorphisms , or whatever the relevant notion is for the kind of manifold in question . likewise for all other calculus and topological concepts we wish to speak of . this is most readily achieved if the charts ( ranges of the labellers $\lambda_j$ ) are open , and their intersections are open as reckonned by all local copies of $\mathbb{r}^n$ that are applicable to the overlap . so we have two axioms for manifolds further to the obvious one that every point in the manifold must belong to the preimage of at least one labeller : an intersection between two " patches " ( domains of labellers ) must be open in the topology as reckonned by each of the two labellers for the overlapping charts ; the transition maps must be local homeomorphisms , diffeomorphisms , . . . . some authors also add the axiom that the manifold should be hausdorff ( $t_2$ ) in each chart but in many fields , notably lie groups , $t_2$ is enforced by other structure ( the group laws ) so this axiom is redundant here . the easiest way to do this is to kit the manifold globally with a topology whose base is the open sets as reckonned by their images under the labellers , or , written backwards , the base for the topology is the collection of all preimages of sets open in $\mathbb{r}^n$ under the labellers . hopefully you can see that the notion of consistency as reckoned by overlapping charts , and thus the notion of the global manifold topology , is very much bound up in the physical concept of covariance and the copernican notion that nature 's behaviour cannot depend on our mere description of it .
firtree is correct - i will just try to flesh out his answer a bit . ( 1 ) your last question first - charge ( or current ) at a point is like mass at a point . for finite masses , if you want to see how much is contained in an infinitely small volume ( i.e. . , at a point ) , the answer is zero . so instead , people consider the mass density which can have non-zero values at a point . you probably understand the relationship between mass and ( mass ) density quite well . similarly for a finite current , the amount of current at a point ( i.e. . , in an infinitely small volume ) is zero . the current density is the limit of the amount of current in a small volume around a point as the volume goes to zero - just like mass density , but with current instead . so , just as one speaks of mass density at a point and not mass at a point ( for extended bodies ) , one speaks of charge density at a point and not charge at a point or current density at a point and not current at a point ( we are ignoring point particles for now - they do fit into this formalism , but you need dirac delta functions ) . ( 2 ) now , in analogy with mass flow , your picture of flow of charge is correct . mass density times velocity gives a mass current density . $\vec{j}_{m} ( t , \vec{x} ) :=\rho_{m} ( t , \vec{x} ) * \vec{v} ( t , \vec{x} ) $ . if you have a mass current density $\vec{j}_{m}$ and want to know the mass flow $\dot{m}$ through some area a , then you take \begin{equation} \dot{m} = \int \vec{j}_{m} \cdot d\vec{a} \end{equation} similarly , charge density times velocity gives a charge current density . $\vec{j}_{q} ( t , \vec{x} ) :=\rho_{q} ( t , \vec{x} ) * \vec{v} ( t , \vec{x} ) $ . if you have a charge current density $\vec{j}_{q}$ and you want to know the flow of charge $\dot{q}$ through some area a , then you take \begin{equation} \dot{q} = \int \vec{j}_{q} \cdot d\vec{a} \end{equation} so , the picture in your head is quite close - just picture charge or a fluid of charged particles flowing .
it depends on your definition of " any memories " . if you do not remember what a second is , there is no solution . if you remember the " old " definition ( a day has 24 h on 60 minutes , each of it is 60 s ) , and live on earth not to far from now , you can rebuild an approximate time standard . if you remember the modern definition , i.e. ( the duration of 9,192,631,770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom . ) you can reconstruct it as good as we can now , even if you are in a spaceship far away from earth . that would give you a second duration , from which you can define a time-scale . the next-step is to synchronize your new time-scal with the old time-scale . this is basically what historian do when they translate egyptian or mayan date in our modern system of datation . to have something precise , you basically need some record of an astronomical event . the best analogue i can see is the work discussed here and here , where the difference between the solar time and atomic time is extrapolated in the past , before the atomic clock era . there are 10 s error bars around 1700 , and 3h error bar for the year 1000 bc .
as explained by iwo bialynicki-birula in the paper quoted , the maxwell equations are relativistic equations for a single photon , fully analogous to the dirac equations for a single electron . by restricting to the positive energy solutions , one gets in both cases an irreducible unitary representation of the full poincare group , and hence the space of modes of a photon or electron in quantum electrodynamics . classical fields are expectation values of quantum fields ; but the classically relevant states are the coherent states . indeed , for a photon , one can associate to each mode a coherent state , and in this state , the expectation value of the e/m field results in the value of the field given by the mode . for more details , see my lectures http://www.mat.univie.ac.at/~neum/ms/lightslides.pdf http://www.mat.univie.ac.at/~neum/ms/optslides.pdf and chapter b2: photons and electrons of my theoretical physics faq .
the cooling over large distances is a challenge but the engineers and physicists are working on it . the current record is several km but not enough to connect cities , etc . this basically only works if the cables are in a kind of vacuum tube to reduce the cooling power . it is not practical to transmit electric energy if you need liquid helium temperatures . the cooling costs are prohibitive . the current state of the art are cables using thin films of bscco ( phys . org ) . they can operate at 77 k without problems . the current world record for such a cable in a vacuum tube is several kilometers but after some distance you need a small building along the cable to cool the liquid nitrogen inside the cable again . there is a tremendous research effort to find superconductors with higher critical temperatures and currents but that is not so easy . the usage for practical applications is increasing but the progress is rather slow . in more exotic applications such a cern or iter you absolutely need superconducting cables , if it is only for space reasons :
as you and christoph already pointed out , the difference comes from the fact that these contiuous " bases " do not belong to the respective hilbert space themselves . this is why they are not actually bases at all in the general sense . rather , they are useful mathematical tools to expand any states actually belonging to the hilbert space . as they obey certain orthonormality and completeness conditions they are also referred to as " continuous bases " ( rough summary of section 2.1.3 of " quantum mechanics " by claude cohen-tannoudji ) .
i am not an expert in critical mass calculation , but the minimum critical mass for u-235 must be less than ( or equal to ) 780 g as shown in " mass estimates of very small reactor cores fueled by uranium-235 , u-233 and cm-245" for a solution type core .
charging and discharging a capacitor periodically surely creates electromagnetic waves , much like any oscillating electromagnetic system . the frequency of these electromagnetic waves is equal to the frequency at which the capacitors get charged and discharged . that means that if you have just dc , the frequency is de facto zero and the resulting electromagnetic waves will be pretty invisible . for the frequency to be that of the visible light , the circuit would have to be as small as an atom . ideally , it would have to be an atom because atoms are the " circuits " that naturally emit visible light .
but in the particle 's rest frame , the process is absorption rather than emission , and it can not have some fixed rate . welcome to the joys of non-locality , where the picture of emission and absorption of a tachyon particle travelling from a to b does not really work : if we go to the critical frame , a tachyonic interaction looks like an instantaneous transfer of momentum without energy transfer - a spooky action at a distance ( cough non-local realism cough ) . non-critical observers ( including the interacting partners ) may claim to see tachyon emission and absorption , but as you already mentioned , in general will not agree on who is the emitter and who is the absorber ; as a tachyon 's worldline is space-like , it does not come with a rest frame and we can not have a clock riding along with it and tell us what ' really ' happened . the rate has to be determined by how many tachyons are available in the environment to be absorbed . the interaction is non-local , and at any given moment all matter at space-like distance is potentially available for tachyon ' exchange ' . we do not have to wait for tachyons to arrive in the neighbourhood . the real question - what makes a tachyonic interaction take place - of course still remains . also , one needs to think about how the situation changes in a general relativistic setting , where we do not have a single critical frame that covers the whole interaction .
the multipole coefficients associated with a $1/|r|$ distribution $\rho$ depends on the choice of origin . for example , if you have a point charge and you choose the origin to be at that point charge , then it will have a pure monopole character . however , if you choose the origin to be elsewhere , it will have nonzero expansion coefficients other than the monopole . this is an artifact of your choice of coordinate system . to make this rigorous , let $\mathbf{i}=\{i_0^0 , i_1^{-1} , i_1^{0} , i_1^1 , . . . \}$ and $\mathbf{r}=\{r_0^0 , r_1^{-1} , r_1^{0} , r_1^1 , . . . \}$ be the set of irregular and regular solid harmonics . then the potential $v ( \mathbf{r} ) $ due to $\rho$ admits the exterior and interior multipole expansions $$v=\sum_{j=0}^\infty \mathbf{i}_j\rangle\left\langle\mathbf{r}_j , \rho\right\rangle\qquad\text{when }|\mathbf{r}|&gt ; r_\text{max} \\ v=\sum_{j=0}^\infty \mathbf{r}_j\rangle\left\langle\mathbf{i}_j , \rho\right\rangle\qquad\text{when }|\mathbf{r}|&lt ; r_\text{min} $$ or in matrix notation , $$v=\mathbf{i}\mathbf{r}^\dagger\rho\qquad\text{when }|\mathbf{r}|&gt ; r_\text{max} \\ v=\mathbf{r}\mathbf{i}^\dagger\rho\qquad\text{when }|\mathbf{r}|&lt ; r_\text{min} . $$ in the case where $\rho$ is purely real , we can use the real solid harmonics $\mathbf{i}'$ and $\mathbf{r}'$ , which are related to the standard solid harmonics by a unitary block diagonal matrix $\mathbf{u}$ via $\mathbf{i}'=\mathbf{i}\mathbf{u}$ from which we obtain the analogous real expansions $$v=\mathbf{i}\mathbf{r}^\dagger\rho=\mathbf{i}\mathbf{u}\mathbf{u}^\dagger\mathbf{r}^\dagger\rho= [ \mathbf{i}' ] [ \mathbf{r}' ] ^\dagger\rho= [ \mathbf{i}' ] [ \mathbf{r}' ] ^\mathsf{t}\rho\qquad\text{when }|\mathbf{r}|&gt ; r_\text{max} \\ v=\mathbf{r}\mathbf{i}^\dagger\rho=\mathbf{r}\mathbf{u}\mathbf{u}^\dagger\mathbf{i}^\dagger\rho= [ \mathbf{r}' ] [ \mathbf{i}' ] ^\dagger\rho= [ \mathbf{r}' ] [ \mathbf{i}' ] ^\mathsf{t}\rho\qquad\text{when }|\mathbf{r}|&lt ; r_\text{min}$$ which has the advantage that the list of multipole moments $ [ \mathbf{i}' ] ^\mathsf{t}\rho$ or $ [ \mathbf{r}' ] ^\mathsf{t}\rho$ are purely real . so , why does a point charge not located at the origin have moments other than a monopole ? it is for the same reason why a washing machine with a raccoon inside of it will shake around when it is on a wash cycle : it is not balanced , as the charges ( or mass ) are not located at the center of the relevant coordinate system . as an explicit proof of why a point charge not located at the origin can not have a pure monopole moment , suppose otherwise . then a test charge will be uniformly accelerated towards the center of the coordinate system , instead of towards the point charge . this is a contradiction . therefore , there must be higher moments involved . alternatively , a detailed justification can also be obtained by applying the addition theorem for spherical harmonics , but hopefully the proof given in the previous paragraph is sufficiently illuminating to show why higher moments will appear when a point charge is not located at the chosen origin . here 's a numerical example to compute the moments of a single point charge located at spherical coordinate $ ( r , \pi/2,0 ) $ in mathematica ( it also computes the potential $v$ at an arbitrary point and compares it to the potential obtained from direct application of $v=1/|\mathbf{r}-\mathbf{r}_0|$ ) : 0.332219 0.332273 $$\left ( \begin{array}{c} \{q\} \\ \left\{\frac{q r}{\sqrt{2}} , 0 , -\frac{q r}{\sqrt{2}}\right\} \\ \left\{\frac{1}{2} \sqrt{\frac{3}{2}} q r^2,0 , -\frac{q r^2}{2} , 0 , \frac{1}{2} \sqrt{\frac{3}{2}} q r^2\right\} \\ \left\{\frac{1}{4} \sqrt{5} q r^3,0 , -\frac{1}{4} \sqrt{3} q r^3,0 , \frac{1}{4} \sqrt{3} q r^3,0 , -\frac{1}{4} \sqrt{5} q r^3\right\} \\ \left\{\frac{1}{8} \sqrt{\frac{35}{2}} q r^4,0 , -\frac{1}{4} \sqrt{\frac{5}{2}} q r^4,0 , \frac{3 q r^4}{8} , 0 , -\frac{1}{4} \sqrt{\frac{5}{2}} q r^4,0 , \frac{1}{8} \sqrt{\frac{35}{2}} q r^4\right\} \\ \end{array} \right ) $$ note that there are nonzero moments of all orders whenever $r\neq 0$ . however , the potential at the test location is correct up to parts per thousand accuracy when the sum runs up to $l=4$ . how do i prove that a single point charge only has monopole ? set $r=0$ in the above triangle of numbers . everything vanishes except the monopole term .
gerard ' t hooft 's " quantum field theory for elementary particles . is quantum field theory a theory ? " ( phys . rept . 104 nos . 2-4 ( 1984 ) , 129-142 , author 's eprint ) is a beautifully written review . from the abstract , what i would like to point out is that renormalizability is just one step in an evolutionary process of quantum field theory . in order to illuminate this point of view i will present a survey of the evolution of quantum field theory into its present form . however we will not follow the historical development , but rather , for my convenience , the lines of logic . as is well known , that is quite something different . ' t hooft also has a longer introduction to the subject : the conceptual basis of quantum field theory . gerard ' t hooft . in philosophy of physics ( j . butterfield and j . earman , eds . , elsevier/north-holland : amsterdam , 2007 ) . author 's eprint . this reads more like a textbook geared at readers with fairly solid quantum mechanics and a good understanding of special relativity , and covers a rather wide range of topics , so it is a little more advanced .
just compare the resolution of the two : prism depending on n , there is no good material n> 1.7 ( besides diamond ) depending on base length if you use a equilateral triangle have to use more than one to overcome this prism absorb light , you have got scattering ( stray light ) too now a grating : optimize it for your wavelength choose lines per milimeter resolution depending on the number of lines that are illuminated compact device just transmission gratings have got absorption , you can do your measurement in reflection with a blazed grating design your blazed grating to get the most light in e.g. 2nd order quantitatively prism : $\frac{\lambda}{\delta \lambda} = t \frac{dn}{d\lambda}$ grating : $\frac{\lambda}{\delta \lambda} = \frac{zd}{g}=zn$ where t is your base length , z . . . order of spectrum , g . . . grating constant , d . . . entrance beam diameter , n . . . number of illuminated lines so just use a grating , nowadays they can be fabricated in excellent quality . on my university learning the pros of a diffraction grating is part of the 1st year laboratory exercises .
interesting problem . i think my approach and answer is very close to other posted solutions . i also added a possible scenario . the basic summary is it is the change in the average momentum of the water in the wagon that causes the wagon to move . requiring the water to distribute it self evenly in the wagon causes this relation : average momentum of water in the wagon = $l\times$ mass flow out of wagon in cases where the wagon has been and forever shall expel water at a constant rate , the wagon stands still . imagine it being refilled from above its center of mass . you can actually do this same problem with an empty cart being filled from above instead of emptying below . with $l$ being the horizontal point from the wagon 's center of mass at which the water falls down . the wagon does move if there is some fluctuation in the mass flow out of the wagon either by abrupt starts/stops or by running out of water . variables $t_{c}\to$ time when wagon runs dry $l\to$ distance from center of mass of wagon to nozzle , positive $l$ implies nozzle is on the right side of the wagon $x ( t ) \to$ center of mass of wagon $x_{cm} ( t ) \to$ center of mass of everything $h ( t ) \to$ height of water in the container $m ( t ) \to$total mass of the wagon including any water it holds $m_{w}\to$ mass of initial water $m_{c}\to$ mass of the wagon ; the c is for the critical point of $m ( t ) $ when all the water is gone . originally c was for container but it makes sense $m ( t_{c} ) =m_c$ frame of reference $x ( 0 ) =0$ $\dot{x} ( 0 ) =0$ drainage i am going to side step the issue of initial conditions for now . i am going to treat the system as if the nozzle was always open and water has always been running . only concerned with how a container with a constant cross section , s , would drain . torricelli 's law : mass flow =$-\dot{m} ( t ) $ : mass of system $$v ( t ) =\sqrt{2 g h ( t ) }$$ $$-\dot{m} ( t ) =\rho s v ( t ) $$ $$m ( t ) =\rho s h ( t ) + m_{c}$$ combine to eliminate $m ( t ) $ and $v ( t ) $ $$\frac{\dot{h}}{\sqrt{h ( t ) }}=-\frac{s}{s}\sqrt{2 g}$$ the answer to the differential equation : $$h ( t ) =h ( 0 ) {\left ( 1-t\sqrt{\frac{g {s}^{2}}{2 {s}^{2} h ( 0 ) }}\right ) }^{2}$$ $$h ( t ) =h ( 0 ) {\left ( 1-\frac{t}{t_{c}}\right ) }^{2}$$ where $t_{c}=\sqrt{\frac{2 {s}^{2} h ( 0 ) }{g {s}^{2}}}$ and $h ( t&gt ; t_c ) =0$ from there we get $m ( t ) $: $$m ( t ) =\rho s h ( 0 ) { ( 1-\frac{t}{t_{c}} ) }^{2} + m_{c}$$ $$m ( t ) =m_{w} { ( 1-\frac{t}{t_{c}} ) }^{2} + m_{c}$$ and for $m ( t&gt ; t_{c} ) $ is simply $m_{c}$ , the mass of the wagon center of mass in order to find the center of mass we will account for all of it . at $t=0$ , $x_{cm} ( 0 ) =x ( 0 ) $=0 since all the mass is in the wagon and we assumed equally distributed . the wagon and its contents $$m ( t ) x ( t ) $$ water that has left the wagon if water leaves the the wagon at $t=\tau$ , then it will have speed $\dot{x} ( \tau ) $ . therefore its location is $f ( t , \tau ) $: $$f ( t , \tau ) = l+x ( \tau ) +\dot{x} ( \tau ) ( t-\tau ) $$ then we just integrate to get their contributions . we get their infinitesimal masses from our mass flow : $$\int_0^t f ( t , \tau ) [ -\dot{m} ( \tau ) ] d\tau$$ combine $$m ( 0 ) x_{cm} ( t ) =m ( t ) x ( t ) -\int_0^t f ( t , \tau ) \dot{m} ( \tau ) d\tau$$ differentiating gives us : $$m ( 0 ) \dot{x_{cm}} ( t ) =\dot{m} ( t ) x ( t ) +m ( t ) \dot{x} ( t ) -f ( t , t ) \dot{m} ( t ) -\int_0^t \frac{df ( t , \tau ) }{dt}\dot{m} ( \tau ) d\tau$$ simplifying : $$f ( t , t ) =x ( t ) + l$$ $$\frac{df ( t , \tau ) }{dt}=\dot{x} ( \tau ) $$ integration by parts : $$\int_0^t\dot{m} ( \tau ) \dot{x} ( \tau ) d\tau=m ( t ) \dot{x} ( t ) -\int_0^tm ( \tau ) \ddot{x} ( \tau ) d\tau$$ repalce : $$m ( 0 ) \dot{x_{cm}} ( t ) =\dot{m} ( t ) x ( t ) +m ( t ) \dot{x} ( t ) -\dot{m} ( t ) ( x ( t ) + l ) -m ( t ) \dot{x} ( t ) +\int_0^tm ( \tau ) \ddot{x} ( \tau ) d\tau$$ explanation - in order these terms stand for : mass dissapearing from wagon at the center of mass momentum of wagon and its contents mass appearing outside of wagon at the nozzle last two terms account for momentum of water outside of the wagon combining the first and third terms gives us the average momentum the water in the wagon must have to maintain its even distribution horizontally in the container . they are not evidence for instantaneous dissapearance from the center and reappearance at the nozzle . result : $$m ( 0 ) \dot{x_{cm}} ( t ) =-\dot{m} ( t ) l+\int_0^tm ( \tau ) \ddot{x} ( \tau ) d\tau$$ where : $$m ( t ) =m_{w} { ( 1-\frac{t}{t_{c}} ) }^{2} + m_{c}$$ wagon w/ brakes in this scenario , the wagon has been losing water before $t=0$ . however the force of the brakes keeps $\dot{x} ( t ) =0$ . at $t=0$ the brakes are released and it is allowed to move . this avoids any instantaneous jump in velocity by the wagon . it also allows $x_{cm}$ to be a non-zero constant after $t=0$ . setting $t=0$: $$m ( 0 ) \dot{x_{cm}} ( 0 ) =-\dot{m} ( 0 ) l+\int_0^0m ( \tau ) \ddot{x} ( \tau ) d\tau$$ $$m ( 0 ) \dot{x_{cm}} ( 0 ) =-\dot{m} ( 0 ) l$$ $$\dot{x_{cm}} ( 0 ) =-\frac{\dot{m} ( 0 ) }{m ( 0 ) } l$$ $$\dot{x_{cm}} ( 0 ) =\frac{2 l m_w}{t_c m ( 0 ) }$$ for $t&gt ; 0$ there is no force from the brakes : $$\ddot{x_{cm}} ( t\ge0 ) =0$$ $$\dot{x_{cm}} ( t\ge0 ) =\frac{2 l m_w}{t_c m ( 0 ) }$$ in other words in this situation at $t=0$ the momentum of the whole system matches that of the water in side the wagon . the only question now is as time evolves how is that momentum transfered to the wagon and water leaving the moving wagon . differentiate the system 's momentum : $$m ( 0 ) \ddot{x_{cm}} ( t ) =-\ddot{m} ( t ) l+\frac{d}{d t}\int_0^tm ( \tau ) \ddot{x} ( \tau ) d\tau$$ $$0=-\ddot{m} ( t ) l+m ( t ) \ddot{x} ( t ) $$ $$\ddot{x} ( t ) =\frac{\ddot{m} ( t ) l}{m ( t ) }$$ physical considerations therefore we have a simple system as long as $\ddot{m} ( t ) $ is continuous . the physical explanation is that if we abruptly closed the nozzle the water in the wagon does not come to an immediate stop relative to the wagon . it sloshes around and after a certain relaxation time redistributes its momentum to the system as a whole . similarly with the quick turn on , the water in the container can not just gain an average momentum to match $-\dot{m} ( t ) l$ . again there must be some relaxation time for the water to hit that equilibrium where it can evenly distribute itself in the wagon . it is not that these situations are impossible but that my equations would not take into account these relaxation times . my situation just avoids that . the water in the wagon has already hit some equilibrium before $t=0$ . also having the water move under its own weight provides a slow turn off . velocity of wagon combining the results from previous sections : $$\ddot{x} ( t ) =\frac{2\frac{m_w}{{t_c}^2}l}{m_{w} { ( 1-\frac{t}{t_{c}} ) }^{2} + m_{c}}$$ $$\ddot{x} ( t ) =\frac{2 l m_w}{{t_c}^2 m_c}{\left [ \frac{m_w}{m_c}{ ( 1-\frac{t}{t_c} ) }^{2}+1\right ] }^{-1}$$ $$\int\frac{du}{1+u^2}=\arctan ( u ) $$ $$u=\sqrt{\frac{m_w}{m_c}} ( 1-\frac{t}{t_c} ) $$ $$\dot{x} ( t ) =-\frac{2 l}{t_c}\sqrt{\frac{m_w}{m_c}}\int\frac{du}{1+u^2}$$ $$\dot{x} ( t ) =\frac{2 l}{t_c}\sqrt{\frac{m_w}{m_c}}\left [ \arctan\sqrt{\frac{m_w}{m_c}}-arctan\sqrt{\frac{m_w}{m_c}}\left ( 1-\frac{t}{t_c}\right ) \right ] $$ extremely heavy wagon : $\sqrt{\frac{m_w}{m_c}}\ll1$ $$\arctan ( x ) \to x-\frac{1}{3}x^3$$ $$\dot{x} ( t_c ) =\frac{2 l m_w}{t_c m_c}$$ $$\dot{x_{cm}} ( t\ge0 ) =\frac{2 l m_w}{t_c m ( 0 ) }$$ this makes physical sense . the wagon 's final momentum is just about equal to our initial momentum . the higher order terms would account for the momentum that the dispensed water has . regular wagon : $\sqrt{\frac{m_w}{m_c}}\gg1$ $$\arctan ( x ) \to \frac{\pi}{2}$$ $$\dot{x} ( t_c ) =\frac{\pi l}{t_c}\sqrt{\frac{m_w}{m_c}}$$ $$\dot{x_{cm}} ( t\ge0 ) =\frac{2 l m_w}{t_c m ( 0 ) }$$ $$p_{cm} ( t\ge0 ) =\frac{2}{\pi}\sqrt{\frac{m_w}{m_c}}p ( t ) $$ this case has the wagon with a significantly smaller portion of the systems momentum .
the complete relevant text in the book is the de broglie wave equation relates the velocity of the electron with its wavelength , $\lambda = h/mv$ . . . however , the equation breaks down when the electron velocity approaches the speed of light as mass increases . . . . actually , the de broglie wavelength should be $$ \lambda = \frac hp , $$ where $p$ is the momentum . while $p = mv$ in classical mechanics , in special relativity the actual relation is $$ \mathbf p = \gamma m \mathbf v = \frac{m\mathbf v}{\sqrt{1-\frac{v^2}{c^2}}} $$ where $m$ is the rest mass . if we still need to make the equation $p = mv$ correct , we introduce the concept of " relativistic mass " $m = \gamma m$ which increases with $v$ .
consider an expression of the form $$\int_x^c f ( x , y ) \mathrm{d}y$$ where $c$ is a constant . you can think of this as a mapping from real numbers to real numbers : you pick any real number $x$ , plug it in , and calculate the value of the integral . that is exactly what a single-variable function is . so you can label this function $i_c$ , define it as $$i_c ( x ) = \int_x^c f ( x , y ) \mathrm{d}y$$ and then it should make sense that you can integrate it like any other function : $$\int_a^b i_c ( x ) \mathrm{d}x = \int_a^b\int_x^c f ( x , y ) \mathrm{d}y\ , \mathrm{d}x$$ so that tells you that integration over a variable that appears in the limit of an inner integral is a perfectly reasonable thing to do , and conceptually , there is nothing complicated about it . actually coming up with a symbolic expression for the double integral is another matter , of course . in this case , the major step in going from equations ( 4 ) and ( 5 ) is integrating over $\varphi$ , so let 's do that : for the term on the left side , $$\int_{\color{red}\varphi}^{\varphi_w}\epsilon_o\frac{\mathrm{d}}{\mathrm{d}\varphi}\biggl ( \frac{e^2}{2}\biggr ) \mathrm{d}\varphi = \epsilon_o\biggl ( \frac{e_w^2}{2} - \frac{\color{red}{e}^2}{2}\biggr ) \tag{a1}$$ where $e_w = e ( \varphi_w ) $ and $\color{red}{e} = e ( \color{red}{\varphi} ) $ and for the first term on the right , $$\int_{\color{red}\varphi}^{\varphi_w}\sqrt{\frac{m_e}{2e}}\frac{j_{eo}}{\sqrt{\varphi}}\mathrm{d}\varphi = \sqrt{\frac{m_e}{2e}}j_{eo}\bigl ( 2\sqrt{\varphi_w} - 2\sqrt{\color{red}{\varphi}}\bigr ) \tag{a2}$$ the reason i am using some red variables , by the way , is that when a variable of integration appears as one of the limits , you should consider it to be a separate variable inside and outside the integral . so think of $\color{red}{\varphi}$ and $\varphi$ as different variables . if you prefer , you could give the variable a different label instead of using a different color ( so you could call it , say , $\varphi_b$ , instead of $\color{red}{\varphi}$ ) . anyway , that was easy . the tricky term is $$\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\int_\varphi^{\varphi_w}\frac{\varphi'/\varphi_i - 1}{\sqrt{\varphi ' - \varphi}}\frac{\mathrm{d}\varphi'}{e'}\mathrm{d}\varphi\tag{a3}$$ in this one you can not do the integral over $\varphi'$ because you do not know $e ' = e ( \varphi' ) $ and you do not have enough information about it to express it as something you can integrate . what they have done in the paper is exchange the order of integration . this is a procedure you can use on any multiple integral where the limit of the inner integral depends on an outer variable of integration . for example , in a double integral of the form $$\int_a^b \int_x^b f ( x , y ) \mathrm{d}y\ , \mathrm{d}x$$ the region of integration is $$\begin{align} a and \leq x \leq b and x and \leq y \leq b \end{align}$$ which is the blue shaded region in this picture : but you can express the same region as $$\begin{align} a and \leq y \leq b and a and \leq x \leq y \end{align}$$ as shown by this picture : which gives you this identity $$\int_a^b \int_x^b f ( x , y ) \mathrm{d}y\ , \mathrm{d}x = \int_a^b \int_a^y f ( x , y ) \mathrm{d}x\ , \mathrm{d}y$$ using this procedure on equation ( a3 ) from above turns it into $$\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\int_{\color{red}{\varphi}}^{\varphi'}\frac{\varphi'/\varphi_i - 1}{\sqrt{\varphi ' - \varphi}}\frac{1}{e'}\mathrm{d}\varphi\ , \mathrm{d}\varphi'$$ which is a fairly simple function of $\varphi$ , integrable as $\int 1/\sqrt{\varphi'-\varphi}\mathrm{d}\varphi = -2\sqrt{\varphi'-\varphi}$ . the full result after performing the inner integral over $\varphi$ is $$\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\biggl ( \frac{\varphi'}{\varphi_i} - 1\biggr ) \bigl ( -2\underbrace{\sqrt{\varphi ' - \varphi'}}_{0} + 2\sqrt{\varphi ' - \color{red}{\varphi}}\bigr ) \frac{\mathrm{d}\varphi'}{e'}\tag{a4}$$ putting together all the terms , ( a1 ) , ( a2 ) , and ( a4 ) , we get $$\begin{multline}\epsilon_o\biggl ( \frac{e_w^2}{2} - \frac{\color{red}{e}^2}{2}\biggr ) = 2\sqrt{\frac{m_e}{2e}}j_{eo}\bigl ( \sqrt{\varphi_w} - \sqrt{\color{red}{\varphi}}\bigr ) \\ - 2\sqrt{\frac{m_i}{2e}}\frac{j_{eo}}{\lambda_i}\int_{\color{red}{\varphi}}^{\varphi_w}\biggl ( \frac{\varphi'}{\varphi_i} - 1\biggr ) \sqrt{\varphi ' - \color{red}{\varphi}}\frac{\mathrm{d}\varphi'}{e'}\end{multline}$$ then divide both sides by $2\sqrt{\frac{m_e}{2e}}j_{eo}$ and you get equation ( 5 ) , except for one factor of 2 on the left side which i can not seem to account for ( maybe it is lost somewhere in my calculations ) .
why is there no curvature outside this spherically symmetric , non-rotating , uncharged body that still has mass ? i suspect you are getting confused by the fact that the ricci tensor $r_{\mu\nu} = 0$ and therefore the scalar curvature $g^{\mu\nu}r_{\mu\nu} = 0$ . this is always the case in regions of space where the stress-energy tensor is zero . the curvature is certainly not zero in the sense that spacetime is flat . for example the kretschmann scalar is non-zero : $$ r_{abcd} r^{abcd} = \frac{12 r_s^2}{r^6} $$
outside the light-cone , distances are spacelike and not timelike . this means that you can always find a frame of reference such that your " spacially displaced past " is in the future for some other inertial frame of reference . by definition , a spacelike distance is neither in the past nor the future . by itself , this would not create any causality violations . since wormholes ( as shown in other answers ) could set up a scenario where you do violate causality , i think you will find you cannot use wormholes to travel outside of your light-cone .
i give the answer with the general method even though a much straightforward way would be to guess the result because it is simple . the unit of $h$ is the inverse of time denoted by $ [ \mathrm t^{-1} ] $ . the dimension of a temperature is denoted by $ [ \theta ] $ . to find the numerical value of $t$ in kelvin , one should find a combination of $c : [ \mathrm{lt^{-1}} ] $ , $\hbar= [ \mathrm{ml^2t^{-1}} ] $ , $\mathcal g : [ \mathrm{m^{-1}l^3t^{-2}} ] $ and $k_{\mathrm b}: [ \mathrm{ml^2t^{-2}\theta^{-1}} ] $ ( $ [ \mathrm l ] $ and $ [ \mathrm m ] $ represent length and mass respectively ) such that $$ c^x \ ; \hbar^y \ ; \mathcal g ^z\ ; k_{\mathrm b}^u\times h$$ has the dimension of a temperature ( $x$ , $y$ , $z$ and $u$ are unknown ) . this gives the system $$\left\{\begin{array}{rcc} y-z+u and =0 and \quad [ \mathrm m ] \\ x+2y+3z+2u and =0 and \quad [ \mathrm l ] \\ -x-y-2z-2u and =1 and \quad [ \mathrm t ] \\ -u and =1 and \quad [ \theta ] \end{array}\right . $$ the solution is $$\left\{\begin{array}{cl} x and =0\\ y and =1\\ z and =0\\ u and =-1 \end{array}\right . $$ we obtain thus $$t=\frac{\hbar}{2\pi k_{\mathrm b}}h . $$ the value of $h$ is $h=67.8\ , \mathrm{km . s^{-1} . mpc^{-1}}=2.194\times 10^{-18}\ , \mathrm{m . s^{-1}}$ . we find a temperature of $t=2.67\times10^{-30}\ , \mathrm k$ .
i would definitely recommend david griffiths ' book on particle physics . i do not have my copy with me right now , but as i recall , the book explains what the different particles of the standard model are , as well as the various properties of particles that are important in modern particle physics . it also introduces the basics of quantum field theory , just enough to allow you to calculate cross sections and decay rates for various reactions . toward the end , it shows you the basic ideas behind spontaneous symmetry breaking and the higgs mechanism , which shows you where this prediction of the higgs boson comes from . if you want to get into more mathematical detail , another book i could recommend is halzen and martin . it dates back to 1984 but the physics is still basically correct . i have found that that book takes a lot more effort to work through - that is , you actually have to slow down and think about what you are reading , and work through some of the math , but as long as you put the time in , the understanding you gain is well worth it .
as john says , the mohs criterion is useful because it may be immediately applied . one may try to rob the two materials with any force but the magnitude of the force really does not matter because once the force exceeds a certain threshold , the materials ' atoms or molecules start to rearrange . scratches  whatever is their exact definition  will begin to develop and the force you exerted gets reduced . the point of the mohs scale is that when the materials start to get modified  develop scratches  to relieve the external pressure , it is far more likely that the softer material according to this scale is the one that will " surrender " first and get damaged by the scratches . this is no exact law . the harder material may sometimes gets damaged , too . but the quantitative difference between the amount of scratches is huge and very sensitive on the scale . for every temperature , you should in principle quantify the hardness again , from scratch , to use the word again . so materials will surely get softer near the melting point . if they start to " self-repair " because they are partly liquids , this is certainly a proof of their being less hard , not harder ! liquids would have the lowest rating on this scale . a melted piece of the material may not fit the definition of a " scratch " but it is " at least as bad " as a scratch . a material that is really without scratches has individual atoms or molecules sitting tightly at the prescribed points of the lattice or another structure . freely moving molecules of a liquid violate this rule . the extended mohs scale assigns hardness 1 to all liquids , see http://www.rockroost.com/mohs-hardness-scale-tips.shtml
$$ \newcommand{\ket} [ 1 ] {|{#1}\rangle} \newcommand{\bra} [ 1 ] {\langle{#1}|} \newcommand{\braket} [ 2 ] {\langle{#1}\ , |\ , {#2}\rangle} \newcommand{\bracket} [ 3 ] {\langle{#1}\ , |\ , {#2}\ , |\ , {#3}\rangle} $$ well , i think by specifying mass $m$ and charge $q$ you simply define your system , a single electron , and then its complete non-relativistic description is indeed given by a wave-function $\psi ( \vec{r} , m_s , t ) $ as you mentioned . wave-function of the form $\psi ( \vec{r} , m_s , t ) $ , also known as spin-orbital , arises as follows . there is a postulate of qm which says that the state space $h$ of a system , composed of $n$ subsystems each associated with its own space $h_{i}$ , is the tensor product of this spaces , \begin{equation} h = h_{1} \otimes h_{1} \otimes \dotsm \otimes h_{n} \ , . \end{equation} the notion of a system composed of subsystems in the postulate above is not to be take literally : the state space can be written as a tensor product of state spaces which are not even associated with real physical systems . for instance , we can subdivide electron with position in space and spin system into 2 subsystems : electron only with position in space and electron only with spin . taking spin into account the state space for a particle is the tensor product of \begin{equation} h = h_{\text{space}} \otimes h_{\text{spin}} \ , , \end{equation} where $h_{\text{space}}$ is a state space spanned by eigenvectors $\ket{r}$ of the position operator \begin{equation} \widehat{\vec{r}} \ket{\vec{r}} = \vec{r} \ket{\vec{r}} \ , , \end{equation} and $h_{\text{spin}}$ is a state space spanned by eigenvectors $\ket{m_{s}}$ of a spin component operator conventionally chosen to be $\widehat{s}_{z}$ \begin{equation} \widehat{s}_{z} \ket{m_{s}} = m_{s} \hbar \ket{m_{s}} \ , . \end{equation} the resulting space $h$ is spanned then by $\ket{\vec{r} , m_{s}} = \ket{\vec{r}} \otimes \ket{m_{s}}$ by the property of a tensor product space and the expansion of a state vector $\ket{\psi} \in h$ then takes the following form \begin{equation} \ket{\psi ( t ) } = \sum_{m_s=-s}^{s} \ , \int\limits_{\mathrm{r}^{3n}} \psi_{m_s} ( \vec{r} , t ) \ket{\vec{r} , m_s} \mathrm{d}^3 \vec{r} \ , , \end{equation} where \begin{equation} \psi_{m_s} ( \vec{r} , t ) = \braket{\vec{r} , m_s}{\psi} \ , . \end{equation} it should be clear now that coefficients in the expansion of a state vector $\ket{\psi}$ over the basis $\ket{\vec{r} , m_{s}}$ are given by $2s + 1$ functions $\psi_{m_s} ( \vec{r} , t ) $ and that all them are required for the complete description of a state . thus , for instance , the state of an electron can be represented by a two-row vector \begin{equation} \ket{\psi ( t ) } \longleftrightarrow \begin{pmatrix} \psi_{+1/2} ( \vec{r} , t ) \\ \psi_{-1/2} ( \vec{r} , t ) \end{pmatrix} \ , , \end{equation} known as the two-component wavefunction . alternatively , $\psi_{+1/2} ( \vec{r} , t ) $ and $\psi_{-1/2} ( \vec{r} , t ) $ can be combined into single piecewise function $\psi ( \vec{r} , m_s , t ) $ which can also be used as the description of the state of an electron \begin{equation} \ket{\psi ( t ) } \longleftrightarrow \psi ( \vec{r} , m_s , t ) = \begin{cases} \psi_{+1/2} ( \vec{r} , t ) , and m_s = +1/2 \\ \psi_{-1/2} ( \vec{r} , t ) , and m_s = -1/2 \end{cases} \ , . \end{equation} another equivalent representation of the same idea is one which is usually used in quantum chemistry , where the spin dependence is separated out by introducing the " spin up " and " spin down " spin functions \begin{equation} \alpha ( m_s ) = \begin{cases} 1 , and m_s = +1/2 \\ 0 , and m_s = -1/2 \end{cases} \ , , \quad \quad \quad \beta ( m_s ) = \begin{cases} 0 , and m_s = +1/2 \\ 1 , and m_s = -1/2 \end{cases} \ , , \end{equation} and writing down $\psi ( \vec{r} , m_s , t ) $ in the following way \begin{equation} \psi ( \vec{r} , m_s , t ) = \psi_{+1/2} ( \vec{r} , t ) \alpha ( m_s ) + \psi_{-1/2} ( \vec{r} , t ) \beta ( m_s ) \ , . \end{equation} and since spin is relativistic phenomenon , such description is , in a sense , already relativistic , though , partly . fully relativistic description is different . the wave function is four-component , rather than two-component , and although , i could not tell you how it arises , the role of two additional components is to describe the " spin-up " and " spin-down " state of associated positron .
the variations you mention are variations in the brightness of x-rays emitted by cygnus x-1 , and we expect the x-rays to be emitted towards the inner edge of the accretion disk . as matter swirls in towards the event horizon it is heated and ionised and this causes it to emit x-rays . so when there is an increase in brightness it probably means more than the usual amount of matter is reaching the inner edge of the accretion disk , and when there is a decrease in brightness this probably means less matter than usual is flowing in . now , if different parts of the accretion disk were changing brightness randomly we expect they would cancel each other out . that is , some bits would be brighter than usual , some would be darker than usual , and on average the brightness would not change much . if we see a big change of brightness this suggests the disk as a whole is getting brighter or dimmer , and it is doing so due to some external stimulus . but if the brightness changes within some time $t$ the external stimulus must be affecting the whole accretion disk within some time $t$ . since nothing can travel faster than light this means the whole disk must have a size of less than $ct$ . it is not impossible that the fluctuations are due to small areas of the disk , so we can not be absolutely sure that the size limit is $ct$ , but it seems highly probable .
i think that you are asking whether there is an example of a naturally radioactive material , or an irradiated material , whose decay is quick enough that you can prepare a sample with one set of physical and chemical properties , wait a finite amount of time , and have a sample that is visibly changed . this would require you transform a chemically significant amount of material , which in general can not be observed by eye in a small laboratory . for example , let 's suppose we have a reaction where the decay energy is 1mev . if we wanted to transmute one mole of this material , the total energy released would be $$ \mathrm{ 1\ , mev \cdot 6\times10^{23}\ , atoms = 10^{11}\ , joules } $$ if you wanted the transformation to take place over a year ( $\pi\times10^7$seconds ) you had have a constant power of about 3kw ( mostly carried by fast decay products ) that you had have to remove from your sample . that sounds nice and everything , but there just are not any reactions in that energy and speed range . the best-known reaction whose rate can be engineered is uranium fission , where each fission releases about 200mev . typically less than 5% of the mass of uranium fuel undergoes fission in a several-week fuel cycle . i assume you have some idea of the precautions necessary to handle spent nuclear fuel  it is doable , but not a lab demo . as another example , if each fission releases 23 neutrons and 200mev of energy , the ~60terajoule explosion over hiroshima in 1945 involved about half a mole of fissioning uranium and about a mole  one gram of free neutrons . your other option for an observable transmutation would be the decay of tritium to helium , which has a fairly short half-life ( 12 years ) and quite low decay energy ( around 0.020mev ) . of course , both tritium and helium are colorless gases when pure at room temperature , so you had have to use some other property to observe the decay . ( for instance helium-3 has twice the pressure of hydrogen-3 at a given mass density , since hydrogen forms h$_2$ molecules and helium is monoatomic . )
indeed , nothing can get under the horizon . the stuff close to the event horizon does move outwards as the bh radius increases . even more with any bh deformations such as waves on its surface , the tidal deformations or the change of the rotation speed , all the oblects close enough to the horizon remain " sticked " to it and follow all the changes of the bh form . all objects close enough to a rotating bh horizon , rotate with it at the same speed . you may ask then , how a black hole can appear then and the horizon form . it is conjectured that they cannot , and the only possible black holes are the hypothetical primordial black holes that existed from the very beginning of the universe . the objects that can be very similar to black holes are called collapsars . they are virtually indistinguishable from actual black holes after a very short time of the formation . they consist only of matter outside the radius of the event horizon of a bh with the same mass . this matter is virtually frozen on the surface like with actual bh , due to high gravity level . such collapsars possibly can become bhs for a short time due to quantum fluctuations and thus emit hawking radiation . astrophysicists do not separate such collapsars from actual black holes and call all them bhs due to practical reasons because of their actual indistinguishability . here is a quote from one paper that supports such point of view : our primary result , that no event horizon forms in gravitational collapse as seen by an asymptotic observer is suggestive of the possibility of using the number of local event horizons to classify and divide hilbert space into superselection sectors , labeled by the number of local event horizons . our result suggests that no operator could increase the number of event horizons , but the possibility of reducing the number of pre-existing primordial event horizons is not so clear and would require that hawking radiation not cause any primordial black hole event horizons to evaporate completely . source
susskind and polychronakos - and , independently , hellerman and van raamsdonk - have constructed a matrix model - based on string-theoretical d0-branes , which is the right fundamental way to think about any adjoint matrix model - to describe the fractional quantum hall effect . a review is here : http://iopscience.iop.org/1751-8121/42/30/304006 see also a hellerman-susskind realization of the quantum hall effect in string theory http://arxiv.org/abs/hep-th/0107200 moreover , there has been a whole new industry started by the 1997 maldacena 's ads/cft correspondence . on the cft side , one most typically has matrix-valued field theories , even though they are usually not just quantum-mechanical models but field theories in additional dimensions . chern-simons matrix models also have played a role . ads/something dualities have been successful to one extent or another in describing perfect fluids , fermi liquids , non-fermi liquids , superconductors , hydrodynamics , heavy ion physics , and other things . phenomena are usually translated to the behavior of black holes in a higher-dimensional spacetime .
first let us calculate the potential for a ring of radius $a$ at a distance $x$ from the center along the axis potential due to an infinitesimal mass element $dm$ will be $$\frac{-gdm}{\sqrt{a^2+x^2}}$$ potential due to the ring is then $$\int{\frac{-gdm}{\sqrt{a^2+x^2}}}=\frac{-g}{\sqrt{a^2+x^2}}\int{dm}=\frac{-gm}{\sqrt{a^2+x^2}}$$ since $g , a , x$ are constant now let us break the disc into infinitesimal rings of mass $dm=2\pi rdr\frac{m}{\pi a^2} ( =area * density ) $ the potential due to a ring of radius $r$ and mass $dm$ as given above is $$\frac{-gdm}{\sqrt{r^2+x^2}}=\frac{-2gmrdr}{a^2\sqrt{r^2+x^2}}$$ integrating this from $0$ to $a$ $$\int{\frac{-2gmrdr}{a^2\sqrt{r^2+x^2}}}$$ $$=\frac{-gm}{a^2}\int{\frac{2rdr}{\sqrt{r^2+x^2}}}$$ putting $t^2=r^2+a^2$ and $2rdr=2tdt$ $$=\frac{-gm}{a^2}\int{\frac{2tdt}{\sqrt{t^2}}}$$ $$=\frac{-gm}{a^2} [ 2t ] ^{\sqrt{a^2+x^2}}_{x}$$ $$=\frac{-2gm}{a^2} ( {\sqrt{a^2+x^2}}-{x} ) $$ for intensity , it can be seen by symmetry that it is along the axis thus we work only with axial components so , for ring $$\int\frac{-gdmcos\theta}{a^2+x^2}$$ where $\theta$ is half of the angle subtented by the point on the ring $$cos\theta=\frac{x}{\sqrt{a^2+x^2}}$$ $$k=\int\frac{-gxdm}{ ( a^2+x^2 ) ^{3/2}}=\frac{-gxm}{ ( a^2+x^2 ) ^{3/2}}$$ for a disc , based on the same reasoning as in potential , it is $$k=\int\frac{-gxdm}{ ( r^2+x^2 ) ^{3/2}}$$ $$=\int\frac{-2gmxrdr}{a^2 ( r^2+x^2 ) ^{3/2}}$$ $$=\int\frac{-2gmxtdt}{a^2 ( t^2 ) ^{3/2}}$$ $$=\int\frac{-2gmxdt}{a^2t^2}$$ $$=\frac{-2gmx}{a^2} [ \frac{-1}{t} ] ^{\sqrt{a^2+x^2}}_x$$ $$k=\frac {2 g m}{a^2} \left ( \frac{x}{\sqrt{a^2+x^2}}-1 \right ) $$
in order to be " stable " , the black hole 's hawking radiation temperature would need to be equal to the temperature of the cosmic microwave background , which is currently 2.7 k . ( assuming this is what you meant by " stable " ? ) from wikipedia : " a black hole of $4.5  10^{22}$ kg ( about the mass of the moon ) would be in equilibrium at 2.7 kelvin , absorbing as much radiation as it emits " so then , the schwarzschild radius of such a black hole would be : $r_\mathrm{s} = \frac{2g ( 4.5  10^{22} ) }{c^2}$ = 0.00007m edit : even if a black hole is slightly lighter than the above mass , it would still take an extremely long time to evaporate completely ( on the order of $10^{40}$ years ) . and if the black hole is heavier than the above mass , it will still evaporate , but only after the cmb cools down sufficiently ( $10^{100}$ years for supermassive black holes ) . with these kinds of time scales , the notion of " stability " starts to blur a bit !
the electric field $\vec{e}$ builds up because of the misbalance of charge on the left and right side of your model . to a test charge $q$ , which we place in the middle of the charge clusters , the left side cluster appears more negatively charged than the cluster on the right side . thus you can already distinguish between the cathode ( left , negative ) and the anode ( right , positive ) of the system . what you constructed is just a very simple form of a capacitor . so your question is the same to : towards which direction do electrons flow in a capacitor ? the force on a test charge is $\vec{f}=q\cdot\vec{e}$ . it is parallel to the electric field . but your question was concerned about the direction . one could try to answer it simply with the coulomb force , which tells us that , equally charged particles are repelling . the anode will exert a less repelling force on the test charge than the cathode . thus the direction is towards the anode .
let me try to address your questions , even though just the first one seems quite heavy in itself . spacetime discreteness : let me give you links to references that are relevant to your questions , than i will make some general comments . an introduction to spin foam models of quantum gravity and bf theory ; spacetime in string theory ; the quantum structure of spacetime at the planck scale and quantum fields ; meaning of noncommutative geometry and the planck-scale quantum group ; causal sets : discrete gravity ( notes for the valdivia summer school ) ; on the origins of twistor theory this should get you going . as for string theory and spacetime discreteness , let me say that , in a crude way , the $\alpha$ that appear in the action in this link superstring theory , called ' string tension ' , is basically what ' measures ' this . other approaches not listed : i think your list is fairly complete . but , you did not list loop quantum gravity  maybe you were thinking of it , or maybe it fits in one of your named categories : i just thought i would make it explicit . gr discreteness : to me , this is a subtler question , in the sense that once you have discretized spacetime ( for one reason or another ) , you should not expect that the other [ geometric ] structures remain ' continuous ' in fact , there is a whole branch of research dealing with ' quantum groups ' and ' discretized ' ( or ' latticized': think computer simulations ) theories . the point being that if you discretized all of your ingredientes , you still maintain a certain relation among them ( e . g . , discrete gauge symmetry , or $q$-gauge symmetry ) . the bottom line is that you can perfectly define a theory where all ingredients are properly ' discretized ' , and so it maintains its relevant features ( recovering the continuum theory in some limit ) . as a side note , it is worth seeing that it is possible to discretize theories at the level of differential forms ,  la discrete differential forms , gauge theories , and regge calculus ( pdf ) ( and similar constructions by several other folks ) . in this sense , many of the relevant properties are kept even after discretization ( quite robust method ) . i hope this can get this discussion started .
matt strassler goes into detail with lhc data here : http://profmattstrassler.com/articles-and-posts/largehadroncolliderfaq/whats-a-proton-anyway/checking-whats-inside-a-proton/
l ( angular momentum ) = ( moment of inertia ) x ( angular velocity )
not at all , because the time-dilation is a local thing , it happens in small regions that do not care about the global expansion of the universe , and it has to do with the speed with which you traverse a closed loop in space , just like the length of a spiral along one turn is different if the spiral is stretched or smooshed . if you have a muon going in a circle fast in a magnetic field , the time-dilation is only a function of the speed , as can be seen by how fast the muon decays . it has no relation to the expansion of the universe , which is only visible on galactic distance scales . the effect of time-dilation is very simple , it is geometry , except with a different sign in the pythagorian theorem between time and space . you can learn about it here : what are the mechanics by which time dilation and length contraction occur ?
it is not hideously difficult to calculate the trajectory of a light beam in the schwarzschild metric . you will find the calculation in any introductory book on general relativity . the result is that light will orbit the mass ( whether it is a black hole or not ) at a distance of $r = 3m$ , where $m$ is the geometrical mass $gm/c^2$ . so : $$ r_{orbit} = \frac{3gm}{c^2} $$ or : $$ m = \frac{r_{orbit} c^2}{3g} $$ for a 1km orbit i get the mass to be $4.5 \times 10^{29}$ kg . later : since the duplicate question does not give you the period of the orbit i will go ahead and calculate it . the radius is 1000m , and this is the radius in schwarzschild co-ordinates , so by definition the circumference of the orbit is just $2 \pi r$ or $2000\pi$ . but , if we are schwarzschild observers time is dilated at a radius $r$ , and the dilation factor is $\sqrt{1 - 2m/r}$ . at $r = 3m$ the factor is $\sqrt{1/3}$ so we measure the light to be moving at $c/\sqrt{3}$ . that means that we observe the period to be : $$ \tau = \frac{2000\pi}{c/\sqrt{3}} $$ which is about 36$\mu$s . response to comment : i do not think this is a good place to go through the derivation of the photon orbit radius because any gr textbook will have it . my preferred introductory book " a first course in general relativity " by bernard f . schutz derives this at the beginning of chapter 11 . the derivation takes three pages and even then relies on results from earlier in the book . so instead i will just outline the derivation . the symmetry of the metric can be used to obtain an expression for the 4-momentum . using this it is relatively easy to calculate an expression for the orbit , and from this we extract an effective potential , which for a photon is : $$ v^2 ( r ) = \left ( 1 - \frac{2m}{r}\right ) \frac{l^2}{r^2} $$ where $l$ is the angular momentum . the reason the effective potential is so useful is that we get a circular orbit only when $v ( r ) $ is at a minimum or maximum . a minimum give a stable orbit and a maximum gives an unstable orbit . so we look for a minimum or maximum by differentiating $v ( r ) $ with respect to $r$ and setting this to zero . actually we will make life easier by differentiating $v^2 ( r ) $ and setting this to zero to get : $$ \frac{-2l^2}{r^3} - \frac{-6ml^2}{r^4} = 0 $$ and this has just one solution at $r = 3m$ . this turns out to be a maximum , so the orbit at $r = 3m$ is unstable i.e. if we perturb the orbit the tiniest distance away from a perfect circle the photon will either fly away from or fly into the black hole .
the solution is easier seen with a free body diagram . you will need 2 equations , so use two points on the rope : one attached to the seat , and one where the kid holds the rope . for the first , you have got the weight of the kid mg pulling downward , the force F the kid is exerting on the rope upward , ( since he is lightening the load on the seat by effectively distributing his weight elsewhere ) and the tension T in the rope pulling upward equal to ma: $$t + f -mg = ma $$ the second fbd gives you the tension in the rope pulling upward , and the force the kid is exerting pulling downward . since it is a weightless point , the ma portion is zero : $$ t - f = 0$$ combining and simplifying : $$ 2f - mg = \frac{1}{5}mg $$ $$ 2f = \frac{6}{5}mg $$ $$ f = \frac{3}{5}mg $$ of course , this is assuming the rope has no weight per unit length , and there is no friction in the drum .
what are you saying is completely wrong because einstein just extended the rules of newton for high speeds and velocities only with some new ideas . for normal applications of mechaics newtonian ideas are almost equal to reality .
if you had a perfect scale , the reading would fluctuate based on $$\delta w = m\ddot{x}_{cm}$$ $\delta w$ is the size of the fluctuation in the reading , $m$ the total mass on the scale ( including fly and air ) , and $\ddot{x}_{cm}$ the acceleration of the center of mass . integrating over time , $$\int_{time} \delta w ( t ) = m\delta ( \dot{x}_{cm} ) $$ here , $\delta ( \dot{x}_{cm} ) $ is the change in velocity of the center of mass over the period you observe the readings . because the velocity of the center of mass cannot change very much , if you integrate the fluctuations over time , you wind find that their average tends towards zero . if the fly begins and ends in the same place and the air is still , the fluctuations integrate out to exactly zero . whenever the fly is accelerating up , we expect the reading to be a little higher than normal . when the fly accelerates down , we expect the reading to be a little lower than normal . if the fly hovers in a steady state , the reading will be the same as if the fly were still sitting on the bottom . a real scale cannot adjust itself perfectly and instantaneously , so we would need to know more details of the scale to say more about the real reading .
this is true if the wall is so strong that it does not move and is not damaged in the course of the crash , so this can be only approximately true . so why is this ( approximately ) true ? because , on the one hand , the consequences of a crash are determined by the accelerations in the course of the crash , on the other hand , due to the symmetry , accelerations will be approximately the same in the crash of two identical vehicles as in a crash of a vehicle against a very strong wall ( in both cases , no parts of the first vehicle can travel beyond the plane of symmetry of the two vehicles or beyond the nearest surface of the wall ) . however , some differences are still possible , as parts of the vehicle can rebound in a different way in these two cases ( the collision can be ( in ) elastic to a different extent in these two cases ) .
the answer to the question depends largely on various factors , therefore there is no point to list all books i know of . a couple books , however , stand out for an " informed " laymen . one by george ellis and ruth williams , and another by bob geroch . http://www.amazon.com/flat-curved-space-times-george-ellis/dp/0198506562/ref=ntt_at_ep_dpt_1 http://www.amazon.com/general-relativity-b-robert-geroch/dp/0226288641/ref=sr_1_1?s=booksie=utf8qid=1316698577sr=1-1 both are elementary , but very lucid and did not sacrifice sophistication of physical principles behind . i would , indeed , recommend both to any serious students of relativity . another good source [ will be better soon ] is relativity site of compadre , the spacetime emporium . http://www.compadre.org/relativity/ you may find many online sources there .
i am going to go with a programmer metaphor for you . the mathematics ( including " a field is a function that returns a value for a point in space " ) are the interface : they define for you exactly what you can expect from this object . the " what is it , really , when you get right down to it " is the implementation . formally you do not care how it is implemented . in the case of fields they are not matter ( and i consider " substance " an unfortunate word to use in a definition , even though i am hard pressed to offer a better one ) but they are part of the universe and they are part of physics . what they are is the aggregate effect of the exchange of virtual particles governed by a quantum field theory ( in the case of e and m ) or the effect of the curvature of space-time ( in the case of gravity , and stay tuned to learn how this can be made to get along with quantum mechanics at the very small scale . . . ) . alas i can not define how these things work unless you simply accept that fields do what the interface says and then study hard for a few years . now , it is very easy to get hung up on this " is it real or not " thing , and most people do for at least a while , but please just put it aside . when you peer really hard into the depth of the theory , it turns out that it is hard to say for sure that stuff is " stuff " . it is tempting to suggest that having a non-zero value of mass defines " stuffness " , but then how do you deal with the photo-electric effect ( which makes a pretty good argument that light comes in packets that have enough " stuffness " to bounce electrons around ) ? all the properties that you associate with stuff are actually explainable in terms of electro-magnetic fields and mass ( which in gr is described by a component of a tensor field ! ) . and round and round we go .
youre thinking of velocity addition the usual way : if youre in a bus going 30 mph and you throw a ball forward at 10 mph then the ball will be going 40 mph according to someone outside the bus . but it turns out that this isnt quite correct when youre going at speeds ~10% or more the speed of light : the velocities have to add so that nothing goes faster than light . instead of using the usual formula , $$ v_\mathrm{total} = v_1 + v_2 , $$ we use the relativistic formula $$ v_\mathrm{total} = \frac{v_1 + v_2}{1 + \frac{v_1 v_2}{c^2}} . $$ ( to be clear , the relativistic formula is always correct ; its just that when $v_1$ and $v_2$ are smallas they always are in everyday lifethe denominator of this fraction will be basically 1 , and so the formula reduces to the simpler one that were more used to . ) so the answer to your question is that its not a problem for a human to be going close to the speed of light . as sb1 pointed out , the electrons in your brain dont know or care how fast your body is moving ; as long as your entire body is moving at the same speed , all of your biological processes will continue exactly as before , because your body is at rest with respect to itself . the other objection you raised was that the electrons in your brainwhich are moving very quickly relative to your bodywould be moving faster than $c$ . as you can see from the second formula above , velocities always add so that their sum is less than $c$: even if your body is travelling at $0.99c$ ( relative to someone at rest ) and it contains an electron going at $0.99c$ ( relative to your body , and in the same direction ) , that person at rest will measure the electron as moving at $0.99994c$ .
edit the method you want to use is ok , and gives a quick result . here it is : $$i=\int\prod d\theta^{*}d\theta\theta_{k}^{*}\theta_{l}exp ( \theta^{*}b\theta+\eta^{*}\theta+\theta^{*}\eta ) =\left ( \frac{\partial}{\partial\eta_{k}^{*}}\right ) \left ( \frac{\partial}{\partial\eta_{l}}\right ) \int\prod d\theta^{*}d\theta exp ( \theta^{*}b\theta+\eta^{*}\theta+\theta^{*}\eta ) $$ from where it follows that $i$ is equal to $$\left . i=\mathrm{det}b\left ( \frac{\partial}{\partial\eta_{k}^{*}}\right ) \left ( \frac{\partial}{\partial\eta_{l}}\right ) \mathrm{exp} ( \eta_{j}^{*} ( b^{-1} ) _{ij}\eta_{i} ) \right\vert_{\eta^{*}=\eta=0}$$ $$i=\mathrm{det}b ( b^{-1} ) _{kl}$$ note : i use the shift $\theta_{i}\rightarrow\theta_{i} + ( b^{-1} ) _{ij}\eta_{j}$ and $\theta_{i}^{*}\rightarrow\theta_{i}^{*} +\eta_{j}^{*} ( b^{-1} ) _{ji}$ . here i used the fact that the second moments can be calculated as derivatives in the following way . $$\left . \langle\eta_{l}\eta_{k}^{*}\rangle=\left ( \frac{\partial}{\partial\eta_{k}^{*}}\right ) \left ( \frac{\partial}{\partial\eta_{l}}\right ) \mathrm{exp} ( \eta_{j}^{*} ( b^{-1} ) _{ij}\eta_{i} ) \right\vert_{\eta^{*}=\eta=0}= ( b^{-1} ) _{ij}$$ more or less , you can take this as a definition . but if you still want to prove this , do the following : $$j=\left ( \frac{\partial}{\partial\eta_{k}^{*}}\right ) \left ( \frac{\partial}{\partial\eta_{l}}\right ) \mathrm{exp} ( \eta_{j}^{*} ( b^{-1} ) _{ij}\eta_{i} ) =\left ( \frac{\partial}{\partial\eta_{k}^{*}}\right ) \left ( \frac{\partial}{\partial\eta_{l}}\right ) \prod_{ij} ( 1-\eta_{j}^{*} ( b^{-1} ) _{ij}\eta_{i} ) $$ after straight forward differentiation you get $$j=\prod_{ij}\delta_{kj}\delta_{li} ( b^{-1} ) _{ij}= ( b^{-1} ) _{kl}$$ you can ignore whats below the line , that was my first answer . but i will leave it because it may be instructive for others . since i cannot comment yet , i will sketch a proof for a simpler case $$i=\int\prod_{i}d\theta_{i}^{*}d\theta_{i}exp ( \theta_{i}^{*}b_{ij}\theta_{j} ) =det ( b ) $$ and hope that this will help you compute your integrals . after expanding the all the exponential we arrive at $$i=\frac{1}{n ! }\int d\theta_{1}^{*}d\theta_{1}\dots d\theta_{n}^{*}d\theta_{n} ( \theta_{i_1}^{*}b_{i_{1}j_{1}}\theta_{j_{1}} ) ( \theta_{i_2}^{*}b_{i_{2}j_{2}}\theta_{j_{2}} ) \dots ( \theta_{i_n}^{*}b_{i_{n}j_{n}}\theta_{j_{n}} ) $$ at this point some explanations are in order . the factor $1/n ! $ appears from the expansion of the exponentials . to see how the integrand was obtained , let 's look at the case when $n=2$ . we will have something like this $$\int d\theta_{1}^{*}d\theta_{1}d\theta_{2}^{*}d\theta_{2}\left ( 1+\theta_{i}^{*}b_{ij}\theta_{j}+\frac{ ( \theta_{i}^{*}b_{ij}\theta_{j} ) ^2}{2 ! }+\cdots\right ) $$ it is obvious that only the quadratic term will contribute to the above integral , because only this term can saturate the number of grassmann variables in the integral measure . $$\int d\theta_{1}^{*}d\theta_{1}d\theta_{2}^{*}d\theta_{2}\left ( \frac{ ( \theta_{i}^{*}b_{ij}\theta_{j} ) ^2}{2 ! }\right ) =\frac{1}{2}\int d\theta_{1}^{*}d\theta_{1}d\theta_{2}^{*}d\theta_{2}\sum_{i_{1} , i_{2} , j_{1} , j_{2}}^{2} ( \theta_{i_1}^{*}b_{i_{1}j_{1}}\theta_{j_{1}} ) ( \theta_{i_2}^{*}b_{i_{2}j_{2}}\theta_{j_{2}} ) $$ expanding the sum and performing all four integrals , we get $$\int d\theta_{1}^{*}d\theta_{1}d\theta_{2}^{*}d\theta_{2}\left ( \frac{ ( \theta_{i}^{*}b_{ij}\theta_{j} ) ^2}{2 ! }\right ) =\frac{1}{2 ! } [ 2 ( b_{11}b_{22}-b_{12}b_{21} ) ] =detb$$ now , let us return to our original integral $i$ . the next step before performing the integrals is to reorder the integrals and the grassmann numbers . $$i=\frac{1}{n ! }\int d\theta_{1}^{*}\dots d\theta_{n}^{*}\theta_{i_1}^{*}\dots\theta_{i_1}^{*}\int\theta_{1}\dots d\theta_{n}\theta_{j_1}\dots\theta_{j_1}b_{i_{1}j_1}\dots b_{i_{n}j_n}$$ from where we finally arrive at $$i=\frac{1}{n ! }\epsilon_{i_{1}\dots i_{n}}\epsilon_{j_{1}\dots j_{n}}b_{i_{1}j_1}\dots b_{i_{n}j_n}=\mathrm{det}b$$ ( note : the ordering result $a_{1}b_{1}\dots a_{n}b_{n}=a_{1}\dots a_{n}b_{1}\dots b_{n} ( -1 ) ^{n ( n-1 ) /2}$ has to be used twice for the integrals ) . i found this method to be the most simple one of all when dealing with these sort of integrals . i hope this helps you prove those relations . the same method can be applied very easy in your case ( just a strait forward extension ) . and with the method you described , it seams you are over complicating yourself . however , i will work on it a bit and see what comes up .
the step from 1 to 2 is : multiply by $\gamma^0$ and trace over the result . 2-> 3: $tr ( ( \gamma^0 ) ^2 ) =tr ( 1 ) =1$ on the left hand side and $2tr ( \gamma^0\gamma^\mu ) p_\mu=8\eta^{0\mu}p_\mu=8p_0=8p^0$ on the right hand side . moreover the anticommutator gives $2q_r^2$ so you can just strip off a factor of 2 on both sides . 3-> 4: $p^0$ is the total energy by definition , as the right hand side is in fact mulitplied by an unit operator ( as $q$ are operators ) one has $p^0 1 = h$ giving the final result . for some magic involving the gamma matrices see the wikipedia article http://en.wikipedia.org/wiki/gamma_matrices
no , this stuff is complete balderdash . it is nonsense of the first degree .
from the definition of the commutator , $ [ p_x , p_y ] = p_xp_y - p_yp_x$ where , $p_x = -i\hbar\frac{\partial }{\partial x}$ and , $p_y = -i\hbar\frac{\partial }{\partial y}$ therefore , $p_xp_y \psi = i\hbar\frac{\partial}{\partial x} ( ih\frac{\partial \psi}{\partial y} ) $ $ = -\hbar^2\frac{\partial^2 \psi}{\partial x \partial y}$ similarly , $p_yp_x\psi = i\hbar\frac{\partial }{\partial y} ( ih\frac{\partial \psi}{\partial x} ) $ $ = -\hbar^2\frac{\partial^2 \psi}{\partial y \partial x}$ $ = -\hbar^2\frac{\partial^2 \psi}{\partial x \partial y}$ therefore , $ [ p_x , p_y ] \psi = p_xp_y\psi - p_yp_x\psi = -\hbar^2\frac{\partial^2 \psi}{\partial x \partial y} - -\hbar^2\frac{\partial^2 \psi}{\partial x \partial y} = 0$ therefore , $ [ p_x , p_y ] = p_xp_y - p_yp_x = 0$ note the $\psi$ was not used here , but i included it anyway for teaching purposes , since occasionally evaluating commutators is easier when applying them to a test function , $\psi$ .
how does an electrical field really work ? there are two formulations that describe the known data on electric and magnetic fields . a ) the classical electromagnetic theory ruled by maxwell 's equations . this works well in describing the macroscopic data , of which the electric field is a component . b ) the quantum mechanical formulation that leads to an explanation of how fields are built up , which is necessary to explain effects like the " photelectric effect " , the behavior of atoms and molecules , the internals of atoms and molecules . for a ) the electric field is a fundamental component of the behavior of matter . for b ) the electric field is built up coherently by innumerable virtual particle exchanges , mainly virtual photons , between the generators of the field and the detectors of its existence , so it is not fundamental . charge is fundamental in this framework , and charge is quantized ( +/-1/3 , +/-2/3 , +/-1 ) in absolute value electron charge units . that is why it is a quantized theory of the world . how come that an electron can exert a force on another electron without physical contact ? for a ) it is an action at a distance , the field of the electron exerts a force on other charged matter ; similar to classical newtonian gravity , where the masses exert a force on each other . what is it in an electron that creates the field , where does the energy for doing the work come from ? it is the charge of the electron . when we are talking of electrons we are really in the realm of b ) , quantum mechanics , because its size is of the size where quantum mechanics has to be used to understand the data . in qm language the electron , when looked at individually , is continually exchanging virtual photons with the boundaries of its containment . virtual means that energy and momentum are not conserved because nothing real is exchanged with the other electrons/ions except an information " i am here " . when many electrons are involved , the surface of a charged metal sphere for example , the collective electric field is built up out of those exchanges . the energy was supplied in this case by the experimenter who provided work to separate the electrons from the rest of the molecules , turning them into ions . either by the triboelectric effect or the classical generators of electricity , using magnetic fields and providing a current of electrons in metals . ultimately it is kinetic energy turned into electric energy . ( actually sun energy stored in fuels or water works , turned into kinetic energy . . . ) now magnetic fields , used to generate most of our electricity , are a bit of a different story , but similar and again needing quantum mechanics to be understood . in scenario a ) they also are fundamental .
the balloon analogy is useful in some respects , but it is misleading in one important respect . in the balloon analogy the curavture of the balloon surface is extrinsic while in gr the curvature of the universe is intrinsic . extrinsic curvature is easy to understand . the surface of a balloon , or the hills and valleys on a landscape , or ( to make a 1d analogy ) a railway line are extrinsically curved because there is another dimension external to the surface that allows the surface to curve . we say that our surface is embedded in a manifold with a dimensionality one greater than the surface . intrinsic curvature is much harder to understand because it is counter intuitive . i described intrinsic curvature in my answer to universe being flat and why we can&#39 ; t see or access the space &quot ; behind&quot ; our universe plane ? but let me try a simpler example . suppose you watch an ant walking along an elastic rope , and you see the ant changing speed . you would assume the ant is accelerating . but suppose we had stretched some bits of the rope and compressed others : the dotted lines show equally spaced divisions on the unstretched rope , so when we compress the rope the dotted lines get closer together and when we stretch the rope the dotted lines get farther apart . the key feature of intrinsic curvature ( and gr ) is that the ant sees all the divisions as equally spaced no matter how much we stretch the rope . so if the ant crawls one division per second on the unstretched rope it still crawls at one division per second on the stretched rope . so we see the ant moving more slowly at the left end of the rope than at the right end , and we might explain this by saying the ant is being accelerated by some force ( like gravity ) . but actually the ant is moving in an intrinsically curved space . this is what happens in gr . the curvature of spacetime is like some bits of spacetime being compressed and other bits being stretched , and this is what causes the acceleration that we describe as gravity . there is no external dimension that the universe is being curved in . you started off by asking about the metric expansion of space . well this is like the elastic rope being continuously stretched , but the rope is infinite and has no ends . so the rope is not being stretched into anything - all the stretching is internal . likewise the universe is not expanding into anything .
actually , a wave equation is any equation that admits wave-like solutions , which take the form $f ( \vec{x} \pm \vec{v}t ) $ . the equation $\frac{\partial^2 f}{\partial t^2} = c^2\nabla^2 f$ , despite being called " the wave equation , " is not the only equation that does this . if you plug the wave solution into the schroedinger equation for constant potential , using $\xi = x - vt$ $$\begin{align} -i\hbar\frac{\partial}{\partial t}f ( \xi ) and = \biggl ( -\frac{\hbar^2}{2m}\nabla^2 + u\biggr ) f ( \xi ) \\ i\hbar vf' ( \xi ) and = -\frac{\hbar^2}{2m}f'' ( \xi ) + uf ( \xi ) \\ \end{align}$$ this clearly depends only on $\xi$ , not $x$ or $t$ individually , which shows that you can find wave-like solutions . they wind up looking like $e^{ik\xi}$ .
the coin will not move . first , to differentiate between centrifugal and centripetal , i will start by stating the definitions first . centrifugal force is the apparent force that draws a rotating body away from the center of rotation . it is caused by the inertia of the body as the body 's path is continually redirected . centripetal force is a force that makes a body follow a curved path : its direction is always orthogonal to the velocity of the body , toward the fixed point of the instantaneous center of curvature of the path . centripetal force is generally the cause of circular motion . this means centrifugal force is a fictitious force ( or pseudo force ) . it is not actually there . any body travelling in a straight tries to resist its direction from being changed ( inertia ) . when a body moves in a circle , it tries to go straight at every point on the circumference . it is centripetal force which makes the body keep rotating by pulling it back to the circular trajectory . so centripetal force is basically the cause of circular motion . that is why when you rotate anything over your head and release it , it goes straight , as a tangent to the circle . coming back to the question , friction itself will act as a centripetal force in this case . if there is no friction , there is no centripetal force and hence , there is no centrifugal force . therefore , the coin does not move .
spontaneous decay rate are considered to be a kind of intrinsic property and it should not depend on anything else , see the answers in the related question . usually when there are particles interacting with the atom , the decay rate can change but it is not spontaneous . i want to add an experimental results . there are occasional reports of the observing decay rate fluctuate less than 1% . recently , an experimental results claimed that the effect of the sun on the beta decay can be large and its variation depends on the hour of day and the date of year . details and discussion are given in a blog post . the experiment have record the beta decay of $^{214}pb$ and $^{214}bi$ with 28733 measurements of gamma radiation . the time series is shown in figure 1 below . the power of the signal over days and years are shown in figure 2 and it is very close to the power of sun in figure 3 . the data here is pretty clearly show that there is large fluctuation overtime and there is a relation to the sun . though , it is hard to tell whether the spontaneous decay rate has been changed . but i think it might be some unknown interaction mechanism with particles from sun . figure : gamma measurements , normalized to mean value unity , as a function of time . figure : gamma measurements as a function of date and time of day . the color-bar gives the power , $s$ , of the observed signal . figure : solar elevation as a function of date and time of day . the color-bar gives the power , $s$ , of the observed signal . edit : i want to add a note about the discussion of the previous question , and pointed by @dmckee that this result and others are conducted by the same author . that is these are not independent results . it is therefore less trustable . however , the results produced by their group this time is much clear and stronger . i think there are other research groups interested to reproduce it . this experiment should be much easier than the previous one for other groups to valid its correctness ( or figure out stupid experimental problem ) . hence , it could be expected that we can see more independent results to verify it soon .
in gr you can use any frame that is convenient to solve the einstein equation and calculate the curvature and/or metric . that is because these tensors are co-ordinate independant . if you choose polar co-ordinates centred on the sun you get a mildly perturbed schwarzschild metric , which is indeed nice and simple . once you have the curvature you can transform it to any co-ordinate frame you want e.g. geocentric co-ordinates , then use it to calculate the evolution of the system . but i am not sure this would be any easier than using a newtonian geostationary frame and all the fictitious forces it creates . i suspect that trying to write down a representation of the curvature or metric tensors in a co-ordinate frame ill suited to the task would be just as messy and difficult . a simple test might be to take a model solar system with just one extremely light planet , so you can take the schwarzschild metric as the solution , then try to express this metric in a co-ordinate frame centred on the planet . i can not help with this i am afraid - it is far beyond my skills !
the lhc is a discovery machine . it was designed to maximize the probability of seeing new physics outside the standard model , but it is a poor experiment for discriminating between two or three states that the only signature they have is missing energy and small to zero missing mass . the reason is that the strong interactions initiating the scattering are a many body problem , 3 quarks on three quarks and a great number of gluons on gluons , thus the only " closed " system is a subset defined by a high transverse momentum transfer , and this is contaminated by debris that is not really relevant to the deep scattering . the use of monte carlo simulations is imperative , and mc means that a model of the interactions is assumed in the creation of the sample . not exactly circular , but not helping in fine details . if the lhc finds photino or neutralino candidates a leptonic collider would be needed to be able to discriminate theories like the one proposing a fourth neutrino . imo of course
it is widely believed ( but it has not been rigurously proved so far ) that to sustain stable closed timelike curves , you need copious ( i.e. of the order of the total mass of the universe ) quantities of exotic matter . exotic matter is just a generic term to describe matter for which the stress-energy tensor satisfies $$ g^{\mu \nu} t_{\mu \nu} &lt ; 0 $$ that is , exotic matter violates the strong energy condition , which is known to hold in all known quantum physical theories . this applies not only to ctc , but as well as stable wormholes , warp drives , or anything actually fun . this is in itself , outstanding evidence of the catholic nature of god , since interestellar travel would raise the need for some akward explaining from the pope ( i know , bs , but let me have my punchline please )
more physically than a lot of the other answers here ( a lot of which amount to " the formalism of quantum mechanics has complex numbers , so quantum mechanics should have complex numbers ) , you can account for the complex nature of the wave function by writing it as $\psi ( x ) = |\psi ( x ) |e^{i \phi ( x ) }$ , where $i\phi$ is a complex phase factor . it turns out that this phase factor is not directly measurable , but has many measurable consequences , such as the double slit experiment and the aharonov-bohm effect . why are complex numbers essential for explaining these things ? because you need a representation that both does not induce time and space dependencies in the magnitude of $|\psi ( x ) |^{2}$ ( like multiplying by real phases would ) , and that does allow for interference effects like those cited above . the most natural way of doing this is to multiply the wave amplitude by a complex phase .
as djbunk mentions the delta function is symmetric $$\delta ( x ) =\delta ( -x ) $$ so you certainly have $$\delta ( x-x' ) =\delta ( x'-x ) . $$ but you should also know that in general we have $$\langle a | b \rangle = \langle b | a \rangle^* $$ and since in this case the inner product is real , you will also have $$\langle x | x ' \rangle = \langle x ' | x \rangle . $$ so it does not matter which way you write the delta function or the inner product .
it does not quite work like that . for one thing , stars ' orbital speeds , while reasonably fast by human standards ( often hundreds of km/s ) , are ( in most cases ) incredibly slow by relativistic standards - in other words , they are miniscule fractions of the speed of light . so the difference between the " relativistic mass " ( or energy , as i would call it ) and the rest mass is entirely negligible for all except perhaps a few stars in any given galaxy . certainly there is no way it could account for the missing mass attributed to dark matter . besides , it is not the case that the stars ' orbital speeds steadily increase over time in response to increased gravity . instead , there is going to be some equilibrium at which the effects of the increased orbital speed balance out the effects of the increased gravity . the stars will quickly reach that equilibrium during the galaxy 's formation , and then , simply speaking , they will remain at that speed , so the entire galaxy exists in a steady state of orbital motion . there would be no further correlations of orbital speed with age beyond this point . and anyway , the models that people used to determine the presence of dark matter do take this effect into account ( in the sense that they have determined that it has no noticeable effect on the calculations ) .
though total potential energy of the system of solid earth + oceans + moon + sun would remain approximately constant the energy of one of these can increase at the expense of the other three . thats how the tidal energy comes up . tidal friction does contribute to the reduction of the total gravitational potential energy of the entire system . it also causes reduction in the rotation speed of the earth .
since the cable is not moving horizontally you know the horizontal component of tension is the same at both ends . the total tension is the horizontal component divided by the cosine of the angle . so the ratio is the tensions is the ratio of the cosines . since you know the shape of the curve you should be able to take it from here . update the general equation for a catenary ( with lowest point at x=0 ) is $$y = a \cosh \frac{x}{a}$$ where $$a = \frac{h}{w}\\ h = \text{horizontal tension}\\ w = \text{weight per unit length}$$ for a given horizontal distance and vertical displacement , we have to figure out the location of the lowest point and the tension - two equations , two unknowns . from wikipedia . org/wiki/catenary#determining_parameters : given $s$ , $v$ , and $h$ , then $a$ can be solved for numerically : $$\sqrt{s^2 - v^2} = 2a \sinh \frac{h}{2a} , a &gt ; 0$$ where $h$ is the horizontal distance between ends , $v$ is the vertical distance between ends , $s$ is the length of the cable , and $a$ is the y coordinate of the lowest point . next , we just need to find the position of the lowest point relative to the ends . to get the actual locations of $x_1$ and $x_2$ ( the horizontal distances from the lowest point to the the left and right ends , respectively ) you now have to solve $$\begin{align}\\ v and = a ( \cosh \frac{x_2}{a} - \cosh \frac{x_1}{a} ) \\ h and = x_2 + x_1 \tag1 \\ v and = a\left ( \cosh \frac{x_2}{a} - \cosh \frac{x_2-h}{a}\right ) \tag2 \end{align}$$ solve ( 2 ) for $x_2$ then substitute into ( 1 ) to get $x_1$ finally , the ratio of tensions comes from the ratio of cosines of the angle at the point of suspension : $$\frac{t_2}{t_1} = \frac{\cos\theta_1}{\cos\theta_2}$$ we know the tangent at $x$ is given by $$tan\ , \theta = \frac{dy}{dx} = \sinh \frac{x}{a}$$ combine with the trig identity $$cos\ , \theta = \frac{1}{\sqrt{1+\tan^2\theta}}$$ you finally obtain $$\frac{t_1}{t_2} = \sqrt{\frac{1+\sinh^2\frac{x_2}{a}}{1+\sinh^2\frac{x_1}{a}}}$$
your question mentions ' fusing ' cracks . i think you may be asking whether there are actual examples of what is sometimes referred to as " reversible crack growth . " this is often the ideal case discussed when students are learning crack mechanics theory . the closest approximation to reversible crack growth demonstrated in the laboratory that i am aware of are experiments performed using cleavage cracks in mica . the cracks must be made under a vacuum to avoid contamination of the surfaces . when the applied stress is removed the crack can ' heal . ' it is never perfectly reversible , because of reasons that can include surface contamination and plastic deformation near the crack tip . another strain mechanism that might be approximated ( under simple conditions ) is possibly stress induced twinning . if the applied stresses are removed and the twinned crystal is heated , the twins might reverse . ' removing ' a dislocation from a crystal lattice means moving the dislocation to a point where it is ' annihilated ' by reaction with other defects or a surface . a dislocation may move to a free surface , and out of the lattice . a dislocation loop may shrink down to no radius . two dislocation segments of exactly opposite burgers vector may move towards each other cancel out . dislocation motion means plastic strain . it would be rare that all the strains would cancel out exactly , so in most real cases annealing probably result in at least a little residual strain .
let us for simplicity consider $n$ point charges $q_1$ , $\ldots$ , $q_n$ , at positions $\vec{r}_1$ , $\ldots$ , $\vec{r}_n$ , in the electrostatic limit , with vacuum permittivity $\epsilon_0$ . now let us sketch one possible strategy to prove gauss ' law from coulomb 's law : deduce from coulomb 's law that the electric field at position $\vec{r}$ is $$\tag{1} \vec{e} ( \vec{r} ) ~=~ \sum_{i=1}^n\frac{q_i }{4\pi\epsilon_0}\frac{\vec{r}-\vec{r}_i}{|\vec{r}-\vec{r}_i|^3} . $$ deduce the charge density $$\tag{2} \rho ( \vec{r} ) ~=~\sum_{i=1}^n q_i\delta^3 ( \vec{r}-\vec{r}_i ) . $$ recall the following mathematical identity $$\tag{3}\vec{\nabla}\cdot \frac{\vec{r}}{|\vec{r}|^3}~=~4\pi\delta^3 ( \vec{r} ) . $$ ( this phys . se answer may be useful in proving eq . ( 3 ) , which may also be written as $\nabla^2\frac{1}{|\vec{r}|}=-4\pi\delta^3 ( \vec{r} ) $ ) . use eqs . ( 1 ) - ( 3 ) to prove gauss ' law in differential form $$\tag{4} \vec{\nabla}\cdot \vec{e}~=~\frac{\rho}{\epsilon_0} . $$ deduce gauss ' law in integral form via the divergence theorem .
comment to the question ( v2 ) : p and s is using the notation of a ' same-spacetime ' functional derivative . to illustrate this notation , let us for simplicity stay within first variations , and leave it to the reader to generalize to higher-order variations . i ) first of all , functional/variational derivatives should not be confused with partial derivatives . in practice , from an operational point of view ( if we are not worried about mathematical details about existence and boundary terms ) , all we need to know is the following rules : the formula $$\tag{a} \frac{\delta \phi^{\beta} ( y ) }{\delta\phi^{\alpha} ( x ) } ~=~\delta^{\beta}_{\alpha}~\delta^n ( x-y ) , $$ where $n$ is the spacetime dimension . appropriate generalizations of elementary rules in calculus , such as , e.g. , the chain rule , integration by parts , commutativity of derivatives , and the dirac delta distribution . for instance , by these rules 1 and 2 , we have that $$ \frac{\delta}{\delta\phi^{\beta} ( y ) } \frac{\partial}{\partial x^{\mu_1}}\ldots \frac{\partial}{\partial x^{\mu_r}}\phi^{\alpha} ( x ) ~=~ \frac{\partial}{\partial x^{\mu_1}}\ldots \frac{\partial}{\partial x^{\mu_r}} \frac{\delta}{\delta\phi^{\beta} ( y ) }\phi^{\alpha} ( x ) $$ $$\tag{b}~\stackrel{ ( a ) }{=}~\delta_{\beta}^{\alpha}~\frac{\partial}{\partial x^{\mu_1}}\ldots \frac{\partial}{\partial x^{\mu_r}}\delta^n ( x-y ) . $$ similarly , by rules 1 and 2 , we can deduce that the action $$\tag{c}s~=~\int d^nx ~{\cal l} ( x ) , \qquad {\cal l} ( x ) \equiv {\cal l} ( \phi ( x ) , \partial \phi ( x ) , \ldots , x ) , $$ has the euler-lagrange expression as its functional derivative $$ \tag{d}\frac{\delta s}{\delta\phi^{\alpha} ( x ) }~=~ \frac{\partial{\cal l} ( x ) }{\partial\phi^{\alpha} ( x ) } - d_{\mu} \left ( \frac{\partial{\cal l} ( x ) }{\partial\partial_{\mu}\phi^{\alpha} ( x ) } \right ) +\ldots . $$ the ellipsis $\ldots$ in eqs . ( c ) and ( d ) denotes possible contributions from higher-order spacetime derivatives . ii ) from formula ( a ) it becomes clear that it does not makes sense to consider the functional derivative $\frac{\delta {\cal l} ( x ) }{\delta\phi^{\alpha} ( x ) }$ wrt . the same spacetime argument $x$ , because that would lead to infinities , cf . $\delta^n ( 0 ) =\infty$ . nevertheless , it is tempting to introduce the notation of a ' same-spacetime ' functional derivative $$\tag{e}\frac{\delta {\cal l} ( x ) }{\delta\phi^{\alpha} ( x ) }~:=~ \frac{\partial{\cal l} ( x ) }{\partial\phi^{\alpha} ( x ) } - d_{\mu} \left ( \frac{\partial{\cal l} ( x ) }{\partial\partial_{\mu}\phi^{\alpha} ( x ) } \right ) +\ldots . $$ we stress that eq . ( e ) is only a notational definition . it becomes meaningless if we try to interpret the lhs . of eq . ( e ) using the above rules 1 and 2 . iii ) similarly , p and s talk about second-order ' same-spacetime ' functional derivative $$\tag{f}\frac{\delta^2 {\cal l} ( x ) }{\delta\phi^{\alpha} ( x ) \delta\phi^{\beta} ( x ) } . $$ we recommend to first work out the ordinary second-order functional derivative $$\tag{g}\frac{\delta^2 s}{\delta\phi^{\alpha} ( x ) \delta\phi^{\beta} ( y ) }$$ using rules 1 and 2 . then it should be fairly straightforward to translate ( g ) into the ' same-spacetime ' functional derivative language ( f ) , if needed . [ in particluar , eq . ( g ) contains a $\delta^n ( x-y ) $ while eq . ( f ) does not . ] iv ) finally we should mention that in field theory one often suppresses the spacetime indices $x , y , \ldots$ , by using dewitt 's condensed notation .
imaginary time has no physical meaning . it is an assumption physicists make , namely , that the math will endure the analytic continuation of the time variable onto the complex plane , which makes some calculations easier , but it is not absolutely necessary ( e . g . you can do instantons without imaginary time ) .
temperature means energy . the heat energy is still here . it is just that the " object " ( the universe ) grown bigger so this energy had to spread through it . the more energy in a single point , the hotter it is . that is why they say it got cooler . it is like the expanding gas from your spray deodorant is cold when it leaves the can , but it was at room temperature inside the can . the energy is still the same . please note that this is just a simple analogy and it should be aknowledged that there are much more complex processes involved . it was asked for an layman 's answer .
there are lots of different types of rock , and water can vary in it is acidity , so there are a few variables to consider . one obvious example is the formation of caves . these are usually in limestone , which is calcium carbonate . if carbon dioxide is present , calcium carbonate will dissolve in water to form calcium bicarbonate . so pure water ( ok with a bit of co2 :- ) will happily dissolve limestone . another example is in forming gorges like the grand canyon . if the surrounding rock is limestone then the gorge forms by dissolution just like a cave ; in fact many gorges are actually collapsed caves i.e. a cave forms first then the roof collapses . in the case of the grand canyon the rock contains some limestone layers but there is also a lot of sandstone . sandstone is silica ( silica grains loosely bonded ) so it is not soluble in water and would not be eroded just by solution . in this case the erosion is mostly by abrasion of rocks carried in the water , though any limestone present will dissolve and undercut sandstone layers above it . sandstone is quite soft and easily abraded . if the rock is granite this is very insoluble and very hard so it erodes only very slowly . i would guess the main water based ersion method would be freeze thaw cracking . you expanded your title to ask about any erosion of a hard material by a soft material , but rock ersion by water is not a bad place to start as it is quite varied . martin beckett mentioned water jets , and this is another good example if a bit specialised . if you fire the jet of water fast enough it is momentum means it will break off bits from a hard material . i can not think of any example where this is important in nature .
it seems that these clouds are in fact reasonably well understood . roger smith , who seems to be something of an expert on these clouds , has several papers discussing the physics of morning glory clouds , many of which are linked from his " list of publications " page and can be viewed for free . this paper seems to be particularly good . i am not a meteorologist , either , but from what i have read they appear to be solitary waves ( a . k.a. , solitons ) which are travelling in a sort of atmospheric waveguide . i am sorry i can not give you a better answer than that ; please do look at the linked references . cheers !
in 1st case : you do not have a battery , current flows and creates a backward force on the wire given by f = ilb , to overcome this force you need to apply a force and do work against it . this work done gets converted to joules heat . however in case 2 since no current flows , no force is applicable and f = 0 , since there is no force to work against there is no work done in 2nd case and hence no mechanical energy to conserve , if the rod is moving it continues to move without slowing .
what would be the potential of gnd here ? gnd is the reference node which , by definition , measures 0v . to see this , consider that every one of the node voltages in the circuit are referenced to the gnd node . in other words , if you wish to physically measure the voltage at a node in the circuit , you connect the black lead of your voltmeter to the gnd node and your red lead to the node your wish to find the voltage of . clearly , if you put the red lead on the gnd node , you have connected the red and black leads together and thus , you will read 0v there . so , at the node that is labelled $-10v$ , the voltmeter placed between that node and gnd will read $-10v$ . if you move the red lead to node b , you will measure $-9.3v$ since , given the voltage source between the two nodes , node b must be $0.7v$ more positive than the $-10v$ node . but there is no current flowing through r1 there must be a current through $r_1$ since there is $-9.3v$ volts across it . the current is ' up ' through the resistor , ' up ' through the $0.7v$ source and through the $-10v$ source ( not shown ) back to gnd . i think your confusion lies with the $-10v$ node - it appears to be disconnected but , in fact , it is assumed that there is a $-10v$ voltage source connected there .
show me a distribution of remote mass that would provide the behavior we see for both jupiter in orbit around the sun the many moons in orbit around jupiter which both appear to be $1/r^2$ forces . now try to generalize to support all the moons and planets in the solar system . you can not do it because the system is highly over-constrained .
that is a really good question ! there are three cases , the third of which is the most fundamental and most interesting . the first case is incomplete absorption , such as a gamma ray knocking loose a few electrons as it passes . in that case the differences are taken care of locally and fairly trivially by allocating energy , momentum and spin appropriately between the parts that were hit and the remaining photon . the second case is flexible absorption , which is when the target receptor is sufficiently large and complex to absorb whatever the difference is between the light that was emitted and atomic-level receptors of the target . a good example of this kind of flexible absorption is the opsin proteins in the retina of your eye . these proteins are sufficiently large and complex that , like pitcher 's mitts in baseball , the molecule as a whole can absorb the mismatches energy , momentum , and spin of any photon that falls within a certain rather broad range of frequencies and polarizations . so , it is some variant of this category that takes care of the flexibility needed in most forms of photon absorption . the third case and most curious case happens when you start looking at the quantum side of the question . because of quantum mechanics , no photon has a truly exact location , energy , momentum , or polarization ( or spin , basically its angular momentum ) . a photon that has traveled for a long time through interstellar space , for example , does pretty well on the precision of its frequency ( energy and momentum ) , but is frankly all over the place in terms of where it could wind up in space . nonetheless , it still has some residual uncertainty in its frequency , even after a long trip . as with the local flexibility of receptors such as the opsins , this bit of quantum frequency uncertainty also allows some leeway in whether or not a photon will be absorbed by an electron in an atom . the wave function description of the photon in that case allows it to behave like any of a small number of close but distinct frequencies , one of which will be selected when it arrives even at a flexible receptor such as an opsin . however , this final form of flexibility is a bit strange . if energy ( which is the same as its frequency for a photon in space ) is absolutely conserved , is not this bit of ambiguity in how the photon is " registered " with an opsin protein in your eye going to cause a slight deviation somewhere in the total energy of the universe ? for example , what if the original atom that emitted the photon ended up in its energy accounting books as having emitted the emitted the photon at the lower end of the likely envelope , but the atoms in your opsin protein interpreted it as having energy in the upper end of that envelope ? if that happens , has not your eye in that case just created a tiny bit of energy that did not exist in the universe as a whole before , and so violated energy conservation by just a tiny bit ? the answer is intriguing and not at all understandable from a classical viewpoint . while quantum uncertainty does allow certain degree of freedom that makes absorption possible over a range of frequencies , it does so at a cost to our usual concepts of locality . specifically , every such event " entangles " the emission and absorption of the photon into single quantum event , no matter how separated the events may appear to be in ordinary clock time . when i say " entangle " i mean the word in exactly the same somewhat mysterious way it is used by people describing quantum computation . entanglement is a bit of physics that crosses ordinary boundaries of space and time in very odd ways , but some aspect of it is always involved in quantum events . how odd ? well , if you live in the northern hemisphere try this some night : figure out where the andromeda galaxy is , and go out and look at it . so : did you see it ? if so , you just ensured that the frequencies ( energies ) of every photon you saw is now exactly balanced in the unforgiving conservation books of the universe with the formerly uncertain energies of photon emission events that took place roughly 2.5 million years ago . this balancing in a quite real sense did not occur until you took a look at the andromeda galaxy and forced those photons to give up their former uncertainty . that is how all entanglement works : the wave function remains open and uncertain until a firm detection occurs , then suddenly and frankly rather magically , everything balances out . and all this time you thought there was nothing particularly strange about ordinary light-based vision , yes ? notice , however , that this third entanglement-enabled form of photon reception flexibility only works within the constraints of the photon 's wave function . that observation suggests an experiment that is closely related to your original question , which is this : if you could make the wave function of the photon so tightly and narrowly defined that the slop enabled by entanglement no longer applies ? would your question about " zero probability " then apply , at least at the limit of a wave function with no uncertainty at all in it ? the answer is yes . as it turns out , you can approximate that " no ambiguity left " limit in photon wave functions simply by increasing the energy of the photon . in particular , when you get up into the range of gamma photons , exact , " kick-free " absorption ( versus emission ) of a photon starts becoming a rare event indeed . this specificity of gamma photons can be demonstrated experimentally by using something called the mssbauer effect , which is itself a beautiful and decidedly strange example of quantum effects impinging on everyday large-scale life . in mssbauer , groups of atoms within ordinary chunks of elements at room temperature behave as if they were completely motionless . ( how they do that is beyond the scope of this question , but it has to do with a novel form of bose-einstein condensation within the vibration modes of the atoms . ) the mssbauer effect allows your question to be explored experimentally in an ordinary lab . one group of " motionless " gamma emitting atoms sends gamma rays to another " motionless " group of atoms that can absorb exactly that frequency of gamma photons . next , you try messing with the frequencies of the gamma photons every so slightly by putting one of the groups of atoms into linear motion relative to the other one . the question then becomes this : how fast do you have to move one of the groups of atoms ( which one does not matter ) before the receiving group can no longer " see " the frequencies and absorb the gamma photons ? you might think that you had have to go thousands of miles per hour to have such a profound effect on something as energetic as gamma rays , but it is the other way around ! even a tiny , tiny velocity of a centimeter or so per second is enough to cause a big drop in the level of absorption of the gamma photons . and that is why your question really is an interesting one : because you are right . while it takes some work to set it up and some pretty unusual effects to test it , in the end it really is vanishingly unlikely to get an exact match between the frequency emitted by one atom ( or nucleus of an atom ) and the frequency expectations of the absorbing atom . it is only through three compensating factors -- incomplete absorption , locally forgiving absorption , and quantum entanglement -- that you get the high levels of " practical " photon absorption that make the world as we know it possible .
i depends on the book you have chosen to read . but usually some basics in calculus , linear algebra , differential equations and probability theory is enough . for example , if you start with griffiths ' introduction to quantum mechanics , the author kindly provides you with the review of linear algebra in the appendix as well as with some basic tips on probability theory in the beginning of the first chapter . in order to solve schrdinger equation ( which is ( partial ) differential equation ) you , of course , need to know the basics of differential equations . also , some special functions ( like legendre polynomials , spherical harmonics , etc ) will pop up in due course . but , again , in introductory book , such as griffiths ' book , these things are explained in detail , so there should be no problems for you if you are careful reader . this book is one of the best to start with .
let 's start by assuming you believe general relativity ( on the grounds that it has given the right answers every time it is been tested ) . there are various ways to get repulsive gravity in gr . the most obvious way is to have matter with a negative mass . see http://en.wikipedia.org/wiki/negative_mass for a discussion of this . such matter has never been observed , so i suppose we can not absolutely rule it out . however it seems unlikely . your example of two " universes " racing away from each other would mean the universe is uneven on large scales , and the observations so far show the universe is basically the same in all directions . in addition you had have to come up with a mechanism for the positive and negative matter to have separated in the first place . there are ways to get repulsive gravity that do not require negative mass . for example there appears to be dark energy ( http://en.wikipedia.org/wiki/dark_energy ) generating a gravitational repulsion . also it is widely believed that at very early times the universe underwent a massive expansion , called inflation ( http://en.wikipedia.org/wiki/inflation_(cosmology) ) due to repulsive gravity .
the mass of jupiter is about $10^{27}$ kg which , via $e=mc^2$ , translates to $10^{44}$ joules . if one turned the planet into thermonuclear fuel in some way and detonated it immediately , about 1% or $10^{42}$ joules would be released . because the diameter of jupiter is about 130,000 km , the blast would last at least half a second or so . so we have $10^{42}$ joules per half a second . it is $2\times 10^{42}$ watts . the sun only releases $4\times 10^{26}$ watts of power , so the blast would be $2\times 10^{16}$ times stronger than the sun . however , looking at the effects on the earth , we must realize that jupiter is about 5 times further from the earth than the sun , reducing the energy flux by a factor of $5^2=25$ . so the half-second blast seems about $10^{15}$ times stronger than the sunshine . the equilibrium temperature is , because of the $\sigma t^4$ law , about $10^4$ times higher than that from the sunshine , about a million degrees . the sun warms the earth by a degree in hours or so . a source that is $10^{15}$ times stronger obviously needs a tiny fraction of a second to reach thousands of degrees and evaporate the matter on the surface . so no doubt about it , the thermonuclear blast of jupiter would burn and evaporate all nearby sides of all the planets  all of them are comparably far from the ground zero . on the other hand , would the incoming energy be able to evaporate the whole earth ? we would be getting $10^{15}\times 342\times 4\pi \times 6,378,000^2\sim 2\times 10^{32}$ watts for half a second , about $10^{32}$ joules per the blast and per the surface of the earth . the specific heats of materials are comparable to $1,000$ joules per celsius degree and kilogram so we have $10^{29}$ kilogram-degrees to be heated . divide it by the earth mass below $10^{25}$ kg to see that you may still heat the material by tens of thousands of degrees by the incoming light . so i do think that this could evaporate the whole earth but not the largest planets like saturn . needless to say , the sun itself would be pretty much untouched . its surface already has 6,000 degrees or so . the strong radiation from jupiter could bring it to a million of degrees , by the calculation above , but it is the same as the temperature of the interior layers . so the sun would get destabilized a bit but it would quickly converge back to the sun we know , i guess . the calculations above are completely unrealistic because at most , one could think about turning jupiter into a small star that would still burn very slowly and would be far weaker than the sun .
i think that sunrise equation and declination of the sun provide enough information . you put the equation from the second link into the equation from the first link . you get hours by multiplying the positive solution $\omega_0$ by $2 \cdot \frac{24\text{h}}{2\pi}$ . if the equation from the first link has no solution ( $\tan\phi \cdot \tan\delta> 1$ ) , this means day is either $24\text{h}$ or $0\text{h}$ long . as far as i checked equations ' output , they seem to be consistent .
how do you know something is electrically charged ? well , it interacts with other charges . classically , this is described by the maxwell equations , i.e. by fields . one special case of such an electromagnetical field , the plane wave , is what we call light ( amongst other phenomena , radio transmission etc . ) . that is pretty much it ! perhaps it is actually easier understand from a more " modern " point of view : in qft , the electromagnetical fields are quantised to particles called photons . in " slow " processes like electrostatic repulsion of balloons , those photons have low frequency , and thus tiny energy $e=\hslash\cdot \omega$ , where $\hslash$ is the minute ( by classical measures ) planck quantum . therefore , we recognise the interactions readily as smooth fields . at higher frequencies , such as light ( $\approx 10^{18}\:\mathrm{hz}$ ) , the energy for each quantum is much higher , so when an atom emits light it is much more naturally single photons we are dealing with . but in principle , both are the same thing .
if you were close enough to be hit by a significant amount of something ( material or just radiation ) being ejected from the star , then you would probably hear it hitting the walls of whatever is keeping air around you ( spaceship , space-suit , whatever ) . this would not last very long though , since the noise would be the sound of cracking or boiling or burning ( depends very much on the materials ) , and then you die . :d if you are close enough , you would be just vaporized and i doubt your brain would have the time to process any sensation of sound .
both electric and magnetic field lines are introduced as an aid for visualizing electric both fields . these lines are imaginary straight or curved paths along which a free ( isolated ) pole would travel when placed in magnetic field . in case of electric field lines , the term unit charge is used . the other properties are almost similar for both . these concepts of field lines were introduced by faraday and both of these fields are related by maxwell equations . the direction of these lines is from north pole to south pole outside the magnet whereas in the inside , it is the other way around . they are thought to flow but , no actual movement occurs . and , they are always continuous closed loops extending throughout the magnetic source and never intersect each other . the crowding or sparsity of these field lines depend upon the intensity of the magnetic source . these field lines are best understood by a home-experiment which has been followed for years . take a wire and cardboard . throw some metal fillings over the board . pass some current through the wire and tap the board gently . the fillings rearrange themselves in the form of curves . and the direction of this field is given by maxwell 's right hand cork screw rule . or , just use a bar magnet instead of catching a live-wire . why are tangents then ? place a magnetic compass somewhere in a magnetic field ( say , a bar magnet like the one below ) . the needle would align tangentially to the magnetic lines of force . still , if you are not able to follow , have a look at " how tangents are drawn " . . . 
newton 's law says that the force $\vec f$ exercing on an object produces an acceleration $\vec a$ such as : $$\vec f = m_i \vec a$$ where $m_i$ is the inertial mass of the object . on the other side , in your experience , the force is the gravitationnal force ( the weight ) $\vec p$ which is $\vec p = m_g \vec g$ , where $m_g$ is the gravitational mass , and $\vec g$ is the gravity acceleration . the equivalence principle says that the inertial mass and the gravitational mass are equal , so $m_g = m_i$ . you have $\vec f =\vec p$ , that is $m_g \vec g = m_i \vec a$ but $m_g = m_i$ , so the acceleration is $\vec a = \vec g$ , and this does not depends on the mass .
a ( somewhat idealized plot ) is shown on page 6 of this paper . in a three body decay , the energy peaks close to the maximum due to phase space . the maximum is $53$ mev and the peak looks to be around $45-48$ mev
this is a famous period doubling experiment which is reprinted in cvitanovic 's " universality in chaos " , along with other foundational papers on the period doubling route to chaos . the paper you read is probably due to libchaber and maurer " a rayleigh bernard experiment : helium in a small box " proceedings of the nato advanced studies institute on nonlinear phenomena at phase transitions p . 259 ( 1982 ) . or else giglio , musazzi , perini " transition to chaotic behavior via a reproducible sequence of period doubling bifurcations " prl 47 , 243 ( 1981 ) or else libchaber , laroche , fauve " period doubling cascade in mercury , a quantitative measurement " j . phys . lett 43 l211 ( 1982 ) i just copied these from cvitanovic 's table of contents , i read this paper you are talking about and it is one of these . i should point out that it is not surprising theoretically that the period doubling cascade is the same as any other one-dimensional map , because this is the most likely way for a system to make a period doubling cascade--- to have two directions unstable at once is measure zero , so you have a one-dimensional instability from a fixed point , and this is modelled by feigenbaum , and it is universal , so universally applicable .
using the method of images , you can calculate the force between the ring of charge and the sphere . assume the sphere is on the z axis with it is center on the point $z$ , a radius of $r_s$ and the ring 's radius is $r_r$ with a charge density $\lambda$ . so $z$ denotes the center of the sphere . to calculate the force , you can replace the sphere with a charged ring ( method of images ) with a charge density $\lambda'$ placed at a distance $d$ below the sphere 's center and a point charge $q'$ at the center of the sphere to make the sphere electrically neutral : $$\lambda'=-\lambda\frac{ \sqrt{r_r^2+z^2}}{r_s}$$ $$d=\frac{zr_s^2}{z^2+r_r^2}$$ $$q'=q\frac{r_s}{\sqrt{z^2+r_r^2}}$$ now the problem reduces to finding the force between the main ring and the induced ring and point charge . the image below shows the choose of parameters : i assume that $r_{\text{ring}}&gt ; r_{\text{sphere}}$ . the equations for the case of $r_{\text{ring}}&lt ; r_{\text{sphere}}$ are the same , except that the motion is restricted to $z&gt ; \sqrt{r_s^2-r_r^2}$ . the field of the main ring differs for points with $r&gt ; r_r$ and $r&lt ; r_r$: ( $q$ is the ring 's total charge ) $$\phi ( r , \theta ) =\frac{q}{4\pi\epsilon_0}\sum_{l=0}^\infty \mathrm{p}_{2l} ( \cos \theta ) \cases{\frac{r_r^{2l}}{r^{2l+1}}\mathrm{p}_{2l} ( 0 ) \ , \ , \ , \ , \ , \ , \ , \ , \ , \ , \text{$r&gt ; r_r$}\\\frac{r^{2l}}{r_r^{2l+1}}\mathrm{p}_{2l} ( 0 ) \ , \ , \ , \ , \ , \ , \ , \ , \ , \ , \text{$r&lt ; r_r$}}$$ and $$\mathbf{e}=-\frac{\partial \phi}{\partial r}\hat r-\frac{1}{r}\frac{\partial \phi}{\partial \theta}\hat \theta$$ because of the rotational symmetry of the induced charges , on which we want to find the force , the force will be in the $z$ direction : ( i omitted the lengthy calculations ) $$\mathbf{f}_{\text{total}}=\hat z \frac{q^2}{4\pi \epsilon_0} \left [ \frac{z}{ ( r_r^2+z^2 ) ^2}-\frac{1}{r_r^2\sqrt{z^2+r_r^2}}\cases{{\times f_{\text{out}} ( r , \theta ) }\\{\times f_{\text{in}} ( r , \theta ) }} \right ] $$ where $f_{\text{out}}$ is used when $r&gt ; r_r$ and $f_{\text{in}}$ is used when $r&lt ; r_r$ . these are dimensionless functions that appear when differentiating the above potential : $$f_{\text{out}}=\sum_{l=0}^\infty\mathrm{p}_{2l} ( 0 ) \left ( \frac{r_r}{r}\right ) ^{2l+2}\left ( ( 2l+1 ) \mathrm{p}_{2l} ( \cos \theta ) \cos\theta +\mathrm {p}^1_{2l} ( \cos \theta ) \sin\theta \right ) $$ $$f_{\text{in}}=\sum_{l=0}^\infty-\mathrm{p}_{2l} ( 0 ) \left ( \frac{r}{r_r}\right ) ^{2l-1}\left ( 2l\mathrm{p}_{2l} ( \cos\theta ) \cos \theta-\mathrm{p}^1_{2l} ( \cos \theta ) \sin \theta \right ) $$ and $r$ and $\theta$ are functions of $z$: $$r=\left| z-\frac{r_s^2}{\sqrt{z^2+r_r^2}} \right|$$ $$\cos \theta=\frac{z-\frac{zr_s^2}{z^2+r_r^2}}{r}$$ $$\sin \theta=\frac{\frac{r_rr_s^2}{z^2+r_r^2}}{r}$$ $\mathrm {p}^1_{2l} ( \cos \theta ) $ s are associated legendre functions , and appear when differentiating $\mathrm {p}_{2l} ( \cos \theta ) $ w.r.t. $\theta$ . the reason of summing over even terms ( indexes are $2l$ instead of $l$ ) is that $\mathrm {p}_{l} ( 0 ) $ is nonzero for even terms , as is equal to : $$\mathrm {p}_{l} ( 0 ) = \frac 1 {2^l} \sum_{k=0}^l {l\choose k}^2 ( -1 ) ^{l-k}$$ it seems impossible to say anything intuitively , but the above equations can be used to simulate the system ( with a rather good accuracy , since the higher order terms in the above series go as $r^{2l-1}$ instead of $r^l$ ) .
no . it does not necessarily mean that the acceleration of $a$ is greater than the acceleration of $b$ . here 's an explicit counterexample : object $a$ is moving at $10\ , \mathrm{m/s}$ with constant velocity while object $b$ is moving at $5\ , \mathrm{m/s}$ with an acceleration of $1\ , \mathrm m/\mathrm s^2$ . in this case , the acceleration of $a$ is zero , so $b$ 's acceleration is greater , but it is velocity is lower . note that the initial conditions of the motions of the two objects are irrelevant ; we are talking about instantaneous velocities and accelerations , and given any two objects , one can completely independently pick their velocities and accelerations .
the other answers provide a first-order approximation , assuming uniform density ( though adam zalcman 's does allude to deviations from linearity ) . ( summary : all the mass farther away from the center cancels out , and gravity decreases linearly with depth from 1 g at the surface to zero at the center . ) but in fact , the earth 's core is substantially more dense than the outer layers ( mantle and crust ) , and gravity actually increases a bit as you descend , reaching a maximum at the boundary between the outer core and the lower mantle . within the core , it rapidly drops to zero as you approach the center , where the planet 's entire mass is exerting a gravitational pull from all directions . this wikipedia article goes into the details , including this graph . and there are other , smaller , effects as well . the earth 's rotation results in a smaller effective gravity near the equator , the equatorial bulge that results from that rotation also has a small effect , and mass concentrations have local effects .
those laws of physics would be maxwell 's equations . i will not go into too much detail but from those equations you can get a wave equation for light . the speed of the wave is determined by two fundamental constants , $\epsilon_{0}$ and $\mu_{0}$ . if the speed of light was variable in anyway then those constants would also be variable . no experiment has ever shown these constants to be different , therefore the speed of light must of constant . since we know that light is composed of photons then photons must move at light speed .
the phase space dynamics of the discrete dynamical system is just what you describe--- x ( n+1 ) as a function of x ( n ) . the phase space itself is the range of values of the x ( n ) , whatever space they might live on , while the dynamics is the function that specifies the evolution in one step in time . the connection with mechanical phase space is provided by a poincare section . the poincare section describes a dynamical continuous system by its intersections with a given surface in the full phase space . for a 1d motion , you can consider the half-line x=0 , p> 0 , or in canonical action-angle coordinates $\theta$ fixed , j arbitrary . when you have a separable integrable motion , you take any one of the $\theta$ variables and define a surface by setting it to zero . then the motion will intersect this surface once every period . in mechanical phase space , the phase-space volume is conserved , but this is not so for maps . the condition of transversal intersection means that the map from the poincare surface to itself can get the topological properties of maps on topological properties the properties of maps on spaces are as complicated as you like . the question is then which topological properties are you interested in ? the simplest topological theorems on maps is the brouwer fixed point theorem , which can be restated as follows : link the points x and f ( x ) by a path . if you draw a contractible sphere , and you find that as you go around the boundary , this x-f ( x ) map has a nonzero winding , then there is a fixed point inside this sphere . the winding of a sphere around another sphere is the index of the map--- it is how many times the sphere covers the other sphere in the map . the brouwer theorem is classical . another classical theorem of this sort is sharkovskii 's theorem : there is a linear order on periods of periodic cycles in 1d maps , such that each periodic orbit of length l implies that there is a periodic orbit of length l ' whenever l ' is greater than l . some other results are given by symbolic dynamics , the coarse grained position as a function of time . the notions of the entropy of a dynamical system is related to this . these results are not really topological in character , but they are general , and give qualitative insight , so they are similar . many further results can be found here , http://elib.tu-darmstadt.de/tocs/35981431.pdf
photons are not pure energy - they are a particle like all other particles . admittedly photons are massless but then so are gluons , and indeed above the electroweak phase transition temperature so are all particles . so pair production from photons and annihilation into photons is just a scattering process like any other particle interaction . however if is possible to convert kinetic energy to matter , and indeed particle colliders do this every day . this is how the higgs boson was produced at the lhc . see the question what keeps mass from turning into energy ? for more on this .
the centripetal force is the one which mantains an object in a circular motion ( changing the direction of velocity ) . when i am driving a car and making a curve , i feel being pushed away from the center of the curvature rather than towards it . in this case you are feeling inertia , your body tries to continue in straight motion . however , from you accellerated frame you can describe inertia as a centrifugal force . so the problem is that you are confusing centripetal and centrifugal force . the former pushes you to the center , while the latter is a virtual force which pushes you to the outside .
this is inevitably going to be an unsatisfactory answer because your question is vastly more complicated than you ( probably ) realise . i will attempt an answer in general terms , but you have to appreciate this is a pale shadow of the physics that describes this area . anyhow , einstein was the first to spot that energy and mass were equivalent , and you have no doubt heard of his famous equation $e = mc^2$ . these days we write this as : $$ e^2 = p^2c^2 + m^2c^4 $$ where $p$ is the momentum and $m$ is the rest mass . however relativity does not explain how matter and energy can be interchanged . that had to wait several decades for the development of quantum field theory ( qft for short ) . if you have never encountered qft it will strike you as a very odd way of looking at the world . we are used to thinking of particles like electrons as objects , much like macroscopic objects except smaller and fuzzier . however in qft there is an electron field that pervades the whole universe , and what we think of as an electron is an excitation in this field . similarly there is a photon field , and photons are excitations in the photon field . in fact all elementary particles are excitations in their corresponding quantum field . qft explains matter-energy conversion because you can , for example , add energy to the electron field to excite it and thereby create an electron . alternatively an excitation in the electron field , i.e. an electron , can disappear by transferring energy to something else . so , for example , in the large hadron collider two quarks meet with huge kinetic energies and they can transfer some of this energy into excitations of various quantum fields to produce a shower of particles . but this can not happen in any way you please . qft gives us the equations to describe how the kinetic energy of particles can excite quantum fields and thereby create matter . this is why , to return to your question , mass can not just keep turning into energy . quantum field excitations only occur in specific ways described by quantum field theory . and that i think is about all that can be said at this level .
all materials emit thermal radiation ( such as light ) . the hotter the material , the more the radiation is shifted to high frequencies ( shorter wavelengths ) . the radiation comes from oscillating electrons ( regardless of whether there is an electric current ) . welding reaches temperatures high enough to cause significant emission of uv light . oxyacetylene and oxyhydrogen flames can both be over 3000 c degrees and therefore can produce hazardous amounts of uv light . arc welding is even hotter and produces more uv light . running hundreds of amps of current through a nail would be similar to arc welding . http://www.mapfre.com/fundacion/html/revistas/seguridad/n124/articulo1en.html
massless particles do not always have zero chemical potential . suppose that you have a box full of photons and other particles , and it is possible for the photons to exchange energy with other particles , but the number of photons cannot change . then the system will reach a thermal equilibrium in which the photons are described by a bose-einstein distribution with ( in general ) a nonzero chemical potential . the reason this does not usually happen with photons is that the number of photons is often not conserved in situations like this . if there are photon-number-changing processes , then the equilibrium state for the photons will have zero chemical potential ( since otherwise entropy could go up by creating or destroying a photon ) . in summary , the rules are that the chemical potential must be zero if particle-number-changing interactions are possible , but not otherwise . that distinction often coincides with the massless or massive nature of the particles , but not always . by the way , there was a period of time in the early universe when we were in precisely this situation : photons could thermalize via compton scattering with electrons , but at the temperature and density at the time , photon-number-changing processes essentially did not occur . that means that the cosmic microwave background radiation today could have a nonzero chemical potential . people have tried to measure the chemical potential , but as it turns out it is consistent with zero to quite good precision . this makes sense , as long as the photons and electrons came into thermal equilibrium at an earlier epoch ( when photon-number-changing processes did occur ) , and nothing happened during the later epoch to mess up that equilibrium . various theories in which particle decays inject energy into the universe during the constant-photon-number epoch are ruled out by this observation .
unfortunately if you want to read haliday , resnick , walker , you will need to learn calculus . if you are motivated , it is entirely do-able on your own . have a look at khan academy 's tutorials . how long it takes to get to the level required for hrw will depend how much mathematics you know already . you can learn some physics without hardcore calculus though . start with popular physics and khan academy ( again ) . let the things you learn from these sources motivate you to learn more mathematics , which in turn will allow you to learn more physics . does your high-school have a physics cirriculum ? if not , you could look up what they learn in another school system - for example , a-level physics in the united kingdom , or the international baccelauriate . these courses will be designed to get you a good grounding in physics without needing to know lots of mathematics .
yes , most likely , unless there is something fundamentally wrong with our understanding of gravity . the most promising candidate for detection is advanced ligo , which is currently in the process of being designed and built . the website has some really interesting information listed , including the construction schedule ( pdf ) , and the upgrades , such as upgrading from a 10w to a 200w laser . according to wikipedia , they are expecting to start operations sometime in 2014 , which will be after they have completed construction and calibrated the instrument . of particular note is that the higher power laser will make calibrating the mirrors more challenging , so right now they still have one interferometer ( the shorter one ) in operation and are performing a squeeze test . once advanced ligo is complete , they are expecting a sensitivity increase by a factor of 10 , pushing the detection rate to possibly daily . it may also be good to note that they are still processing the data from the old data runs ( by means of einstein@home ) , so it is still possible that a detection will turn up within the data , although it will be be of a different type .
i suspected that one needed to go back to the definition of the currents and indeed , in doing so one can derive the result . here 's a short version . the electron current is defined as [ see equation ( 6.6 ) in 1 ] $$\tag{1}j_\mu ( x ) = -e\bar{u}_f\gamma_\mu u_i \times\mathrm{exp} [ ( p_f-p_i ) \cdot x ] $$ which we write as $$\tag{2}j_\mu ( x ) = j_\mu\mathrm{exp} [ ( p_f-p_i ) \cdot x ] . $$ we will also need to use $$\tag{3}q = p_i^a-p_f^a = p_f^b-p_i^b$$ then the integral $ ( 1 ) $ in the original post can be written $$ \tag{4} t_{fi} = -i\int \ ! dt_ad^3x_a d^3x \ , \ , j^aj^b e^{i ( p_f^{a0}-p_i^{a0} ) t_a}e^{i ( p_f^{b0}-p_i^{b0} ) t_a}\frac{1}{|\vec{x}|}e^{i\vec{q}\cdot\vec{x}} . $$ now shifting $\vec{x}=\vec{x}_b-\vec{x}_a$ with $d^3x=d^3x_b$ and using $ ( 3 ) $ and $$ ( \vec{p}_f^a-\vec{p}_i^a ) \cdot ( \vec{x}_b-\vec{x}_a ) = - ( \vec{p}_f^b-\vec{p}_i^b ) \cdot\vec{x}_b- ( \vec{p}_f^a-\vec{p}_i^a ) \cdot\vec{x}_a , $$ equation $ ( 4 ) $ becomes $$\tag{3} t_{fi} = -i\int \ ! dt_a\int d^3x_a\int d^3x_b \ , \frac{j_0^a ( t_a , \vec{x}_a ) \ , j_0^b ( t_a , \vec{x}_b ) }{4\pi|\vec{x}_b-\vec{x}_a|} , $$ where $j_0^a ( t_a , \vec{x}_a ) $ corresponds to the op 's $a ( t_a , \vec{x}_a ) $ and so on . references : see appendix of j . h . field , classical electromagnetism as a consequence of coulomb 's law , special relativity and hamilton 's principle and its relationship to quantum electrodynamics
1 . i am in love with fecko 's differential geometry and lie groups for physicists . despite not being just about mechanics ( but rather about more or less all rudimentary modern theoretical physics ) it discusses both lagrangian and hamiltonian formalism . it also provides countless exercises ( with nice hints ) so that you can really get a feel for the matter . 2 . i can not think of any major drawbacks . of course , if the problem has no symmetry you sometimes have no other choice than to go back to some coordinates and solve numerically . but this is probably non-issue for you because i suppose you first want to understand physical problems with some structure . 3 . there are countless benefits . to list just few of them . relation to symmetries and conserved quantities becomes obvious . noether 's theorem in hamiltonian formalism is so amazingly simple statement ( hamiltonian is constant for symmetry flow if an only if the generator of the symmetry is constant for hamiltonian flow ) that one has to wonder where all the long-winded coordinate calculations went . not only are the calculations short , one also gains valuable geometrical insights e.g. about the flow of the configuration on the manifold . it is a beautiful formalism . i do not know about others but whenever i have to calculate in coordinates i become nervous . i can compute the results but after few pages when most of the quantities mysteriously cancel , you do not really know why what you derived is true . so then you go back to geometry and lo and behold , the derivation is just few lines and obvious . of course i am exaggerating now but that is the way i feel . it is the basis for all of modern physics . if the above four points were true in classical mechanics , they are even more true when dealing with things like gauge theories ( and that is where the full beauty and power of mathematics comes out ) .
it is a surprisingly complicated question . given your mention of friction , probably the main point is that for a car tyre the friction is not linearly dependant on load . wikipedia has some information about this here . if you had perfectly smooth surfaces the friction is actually proportional to the area of contact and independant of the load . this is because friction is an adhesive effect between atoms/molecules on the surfaces that are in contact . however in the real world surfaces are not smooth . if you touch two metal surfaces together the contact is between high spots on the two surfaces so the area that is in contact is much less than than the apparent area of contact . if you increase the load you deform these high spots and broaden them , so the effect of load is to increase the real area of contact . the real area of contact is approximately proportional to the load , and the friction is proportional to the area of contact , so the friction ends up being approximately proportional to the load . however a rubber type is a lot softer than metal , and a road is a lot rougher than a metal plate . even at low loads the tyre deforms to key into the irregularities in the road , so increasing the load has a lesser effect . that is why you get the sub-linear dependance described in the wikipedia article . but this is only the start of the complexity . if you use a wider tyre the contact patch area is not necessarily bigger . a wider tyre has a wider shorter contact patch while a narrow tyre has a narrower longer contact patch . the contact patch area depends on the tyre pressure , the deformation of the sidewalls and probably lots of other things i can not think of at the moment . and anyway , if by " grip " you mean grip when cornering , the grip is not just controlled by the contact patch area . when a car is cornering the contact patch is being twisted . this is known as the slip angle . the wider shorter contact patch on a wide tyre has a smaller slip angle and as a result grips better .
suppose you imagine the battery to be a variable voltage , and start with the voltage at zero . obviously everything is uncharged . now turn the battery up to 1v . as you do this positive charge leaves the positive terminal and an equal and opposite negative charge leaves the negative terminal . we know the charges leaving the positive and negative terminals must be the same because the battery is a conductor and can not develop a net charge like a capacitor . let 's call the charge that leaves the battery $q$ . the only place the charge that leaves the battery can go is onto the capacitors , so both capacitors now have a charge of $q$ on them . we know that for a capacitor of capacitance $c$ , the voltage across the capacitor is given by : $$ v = \frac{q}{c} $$ call the voltage of the top ( 1$\mu$f ) capacitor $v_1$ , and the voltage of the bottom ( 2$\mu$f ) capacitor $v_2$ . then : $$ v_1 = \frac{q}{c_1} $$ $$ v_2 = \frac{q}{c_2} $$ dividing the first equation by the second plus a bit of quick rearrangement gives : $$ v_1 = \frac{c_2}{c_1} v_2 $$ the two voltages must add up to 1v because we have a 1v battery , therefore : $$ v_1 + v_2 = 1$$ if you substitute for $v_1$ you get : $$ \frac{c_2}{c_1} v_2 + v_2 = 1$$ and dividing through by $ ( 1 + c_2/c_1 ) $ gives : $$ v_2 = \frac{1}{1 + c_2/c_1} $$ tidy this up by multiplying to top and bottom of the right hand side by $c_1$ and you get the equation you are trying to prove : $$ v_2 = \frac{c_1}{c_1 + c_2} $$ just to check , feed in $c_1 = 1$ and $c_2 = 2$ and $v_2$ does indeed come out as 1/3v .
oh , but the edge of the atmopheres of jupiter and saturn ( and the others ) are fuzzy ! look at these cassini images from a few years ago , at the ciclops website : " adrift at saturn " ( pia 07667 ) , " beyond the limb " ( pia 10426 ) , " off saturn 's shoulder " ( pia 09791 ) there is so much gas , such strong gravity , that it gets thicker and thicker quite rapidly as you descend from above . it is many kilometers , but small compared to the whole planet , where the gas is so thick you can not see through it . the density of an atmosphere above ground , or lacking that , above some convenient reference point , is often approximated by an exponential function $\rho ( z ) = a exp ( -z/h ) $ with a , h constants , z height , and h the " scale height " which for saturn is about 60km in altitudes of interest . as you scan from outside the limb toward the planet , density goes up , light scattering increases , and the " optical depth " , which is how far you can see through the gas , shrinks from infinity to planet-scale to mere kilometers ( as in a thin fog ) to meters ( as in a thick fog ) . we also see refraction in the part of the atmosphere between where it is too thick to see through and too thin to matter : " atmospheric illusion " ( pia 07555 ) and " limb scan " ( pia 10442 ) jupiter has a shorter scale height , so it is harder to see fuzziness at the edges , for exmaple in this famous image " the greatest jupiter portrait " at ciclops
the velocity of the orbiting space junk is a vector , with both a radial and a tangential component . $$\vec{v}_f = \dot{r}_f\hat{r} + r_f\dot{\theta}_f\hat{\theta}$$ ( my $r_f$ is your $r$ ) the equation for conservation of angular momentum involves only the tangential component of velocity , because it comes from the cross product of the radius vector and the velocity . $$\vec{l}_f = m\vec{r}_f\times\vec{v}_f = mr_f\hat{r}\times\bigl ( \dot{r}_f\hat{r} + r_f\dot{\theta}_f\hat{\theta}\bigr ) = mr_f ( r_f\dot{\theta}_f ) \hat{z} = mr_fv_f\hat{z}$$ but the equation for energy conservation involves both components . $$e_f = \frac{1}{2}mv_f^2 - \frac{gmm}{r_f} = \frac{1}{2}m\dot{r}_f^2 + \frac{1}{2}mr_f^2\dot{\theta}_f^2 - \frac{gmm}{r_f}$$ the combination $r_f\dot{\theta}_f$ corresponds to your $v_f$ , but the equation as you have written it in the question is missing the $\frac{1}{2}m\dot{r}_f^2$ term , which corresponds to the energy of motion toward or away from the moon . at apapsis and periapsis ( furthest and closest points of the orbit ) , $\dot{r}_f = 0$ momentarily , so you can ignore this term , as is done in the problem you are asking about . but for other points on the orbit , that term is nonzero , which means you have an extra variable . that prevents you from finding a unique solution without specifying which point in the orbit you are at .
the random character of the process itself does not depend on any specific virtual particles - instead , the random character of all microscopic processes in the world is a basic consequence of quantum mechanics . the existence of virtual particles is a consequence of quantum mechanics , too . obviously , virtual particle pairs are important - in a particular computational scheme - to calculate the actual decay rate of a particular unstable object . so the virtual pairs are not the cause of the randomness itself - they are a consequence of the randomness - but in any calculation , the virtual pairs are a part of the cause of the actual numerical value of the decay rate .
never underestimate the power of googling . i googled " high speed nuclear explosion photos " and got a large number of them . here is one : this image captures two common elements : the spikes ( called " rope tricks" ) and an uneven surface shape the duration of the exposure is typically 10 nanoseconds at this stage of the detonation the surface of the fireball has a temperature of 20,000 degrees , three times hotter than the sun 's surface . at such temperatures the amount of thermal radiation ( light ) given off is so enormous anything it touches is vaporized ahead of the expanding fireball . the three spikes in this image result from the guide wires supporting the tower on which the bomb was located absorbing enough heat to turn into light emitting plasma . because thermal radiation travels faster than the fireball , the spikes extend out ahead of it . p.s. i should have remembered one frame cameras , since i had been using " gun cameras " from wwii to get images of cosmic rays in spark chambers by triggering , back in 1967 .
to leading order in $\alpha$ , the volume expansion of your container does not depend on its shape , and is equal to $\delta v = 3v\alpha\delta t$ . it is fairly simple to verify this for a cylinder , cube or sphere . also the expansion coefficient for aluminum is going to be quite a bit smaller than glycerol , so you may be intended to simply neglect the expansion of the container .
take for example $\mathcal{n}=2$ supersymmetry . the algebra , including a central charge $z$ , is given by $$\{q_\alpha^a , q^\dagger_{\dot{\alpha}b}\}=2\sigma^\mu_{\alpha\dot{\alpha}}p_\mu \delta^a_b$$ $$\{q_\alpha^a , q_\beta^b\}=2^{3/2}\epsilon_{\alpha\beta}\epsilon^{ab}z$$ $$\{q^\dagger_{\dot{\alpha} a} , q^\dagger_{\dot{\beta} b}\}=2^{3/2}\epsilon_{\dot{\alpha}\dot{\beta}}\epsilon_{ab}z . $$ we can now define $$a_\alpha=\frac12\left ( q_\alpha^1+\epsilon_{\alpha\beta}\left ( q_\beta^2\right ) ^\dagger\right ) $$ and $$b_\alpha=\frac12\left ( q_\alpha^1-\epsilon_{\alpha\beta}\left ( q_\beta^2\right ) ^\dagger\right ) , $$ which reduces the algebra to $$\{a_\alpha , a_\beta^\dagger\}=\delta_{\alpha\beta} ( m+\sqrt{2}z ) $$ and $$\{b_\alpha , b_\beta^\dagger\}=\delta_{\alpha\beta} ( m-\sqrt{2}z ) . $$ a bps-state satisfies $m=\sqrt{2}z$ , hence the second part of the algebra reduces to $$\{b_\alpha , b_\beta^\dagger\}=0 . $$ this tells us that the operators in the second half of the algebra , which now vanishes , only generate states of zero norm . this is how you should understand the statement you were asking about .
if the moon 's orbit around the earth were in exactly the same plane as the earth 's orbit around the sun , we had have a total solar eclipse every month ( but 100% totality would be seen only from the tropics ) . but in fact they are not in the same plane . the earth 's spin axis is tilted by about 23 degrees relative to the earth 's orbit around the sun , and the moon 's orbit is closely aligned with the earth 's spin . as a result , the sun and the moon do not follow the same path in the sky . we get a solar eclipse only when ( a ) there is a new moon , so the sun and moon are in the same position east-to-west , and ( b ) the new moon happens when the sun and moon happen to be closely aligned north-to-south . since the sun and moon are both about half a degree wide ( as we see them in the sky ) , the 23-degree offset of their paths makes solar eclipses relatively rare events . lunar eclipses , which occur during the full moon when moon passes into the earth 's shadow , are more common because the earth is bigger than the moon , and so has a much wider shadow .
the black hole shrinks due to negative energy of the particle it absorbs when the other particle escapes but it does not matter whether or not it was the matter of the antimatter that was absorbed . for a virtual pair to become real particles , lets take an e+e- pair , energy must be provided by an energy source . in the simple pair creation by a real gamma ray hitting the field of a nucleus , the energy for the creation of e+e- is provided by the real gamma ray while the virtual from the field of the nucleus balances momentum . in the case of the black hole the virtual pairs must get the energy from the great potential well that the bh is . when the random quantum mechanical fluctuations at the bh horizon give enough energy for the materialization of an electron or positron and enough energy that it escapes the hole , the energy comes from the potential well of the hole . this energy must be subtracted , that is why it is called negative , from the total potential to keep energy conservation at the horizon locally . is the black hole losing mass if it absorbs only the normal matter half of a virtual particle and the antimatter half escapes ? the electron and the positron have the same mass , positive , same is true for all particles antiparticles . as in relativistic energies e=mc^**2 it makes no difference if it is a particle or antiparticle that becomes real , energy will be subtracted from the total of the bh .
nothing will change with maxwell 's equations due to the discovery of the higgs bosons . maxwell 's equations describe the continuum of electromagnetic theory and know nothing about particle physics .
pressure is defined as force per unit area applied to an object in a direction perpendicular to the surface . and naturally pressure can cause stress inside an object . whereas stress is the property of the body under load and is related to the internal forces . it is defined as a reaction produced by the molecules of the body under some action which may produce some deformation . the intensity of these additional forces produced per unit area is known as stress ( pretty picture from wikipedia ) : edit per comments overburden pressure or lithostatic pressure is a case where the gravity force of the object 's own mass creates pressure and results in stress on the soil or rock column . this stress increases as the mass ( or depth ) increases . this type of stress is uniform because the gravity force is uniform . http://commons.wvc.edu/rdawes/g101ocl/basics/earthquakes.html included in lithostatic pressure are the weight of the atmosphere and , if beneath an ocean or lake , the weight of the column of water above that point in the earth . however , compared to the pressure caused by the weight of rocks above , the amount of pressure due to the weight of water and air above a rock is negligible , except at the earth 's surface . the only way for lithostatic pressure on a rock to change is for the rock 's depth within the earth to change . since this is a uniform force applied throughout the substance due to mostly to the substance itself , the terms pressure and stress are somewhat interchangeable because pressure can be viewed as both an external and internal force . for a case where they are not equal , just look that the image of the ruler . if pressure is applied at the far end ( top of image ) it creates unequal stress inside the ruler , especially where the internal stress is high at the corners .
it is not really a single principle - it is a philosophy and in the context of philosophical discussions about science , it is usually known as positivism . http://en.wikipedia.org/wiki/positivism as any philosophy , it cripples the penetrating power of science if it is extended too far - and every philosophy ultimately fails . the thought experiment about the earth in the universe is just one among millions of examples . positivism could have a problem with the whole concept of thought experiments . while it was very useful and important for the development of quantum mechanics to realize that science does not have to talk about things that can not be measured , i.e. that theories that deny the existence of things that can not be observed are just fine , it still remains true that science also can talk about concepts that can not be observed , such as quarks . it is up to the scientific method to decide whether an auxiliary concept or a theory that is not accessible to observations has an explanatory power that justifies its validity - and the answer may be different in each individual situation .
for the path element $d\vec{l}$ around a circle with radius $a$ you can write $d\vec{l}=rd\phi\vec{e_\phi}$ with $\vec{r}=-r\vec{e_r}$ ( note the minus sign , since the vector points from the wire to the center ) you get $$\vec{h}=\frac{i}{4\pi}\oint \frac{r^2 ( -\vec{e_\phi}\times \vec{e_r} ) }{r^3}d\phi$$ substituting $a$ for $r$ and integrating $\phi$ from 0 to $2\pi$ and realizing $-\vec{e_\phi}\times\vec{e_r}=\vec{e_z}$ $$\vec{h}=\frac{i}{4\pi}\frac{1}{a}\vec{e_z}\int_0^{2\pi}d\phi = \frac{i}{2a}\vec{e_z}$$
since relativistic momentum p = $\gamma m_0 v$ , then : $\vec{f} = \frac{d\vec{p}}{dt} = \gamma m_0 \frac{d\vec{v}}{dt}$ , which , when solved using vectors is equal to : $\gamma m_0 \frac{v^2}{r}$
do not get too confused by the " bladeless fan " marketing babble . something , probably a traditional blower , is pushing air around inside the device . this is ducted so that the flow blows in one direction from little nozzles on the inside of a ring . that causes a lot more air to be moved by bernoulli 's principle . basically , the ring and nozzles converts high pressure low flow air into low pressure high flow air . overall , this system is likely to be ( i do not have any numbers , just a guess on my part ) less efficient than a traditional fan . the claimed advantage is that you do not feel pulses as individual fan blades spin around . i find that argument rather hard to swallow since i never noticed pulses from a traditional fan . after a relatively short distance the flow will break up and become turbulent anyway , even if it started out perfectly smooth , so this whole issue smells strongly of marketing bs to me .
black is not a color . it is just absence of light . so , a torch can not project light having such color . however , for the second part ( dis-illuminate everything ) , i have an answer : light is wave and waves can cancel each other . projecting specially programmed adjustable light wave which create fully destructive interference with other available light waves can achieve your goals . but , the problem is : full 100% destructive interference is not possible at this time . you asked for future , so just wait . it may be possible in future .
yes , the comination $j_1 + j_2$ determines the spin of the particle . note however , that this is an addition of angular mementum which may be complicated . furthermore , you can count the degrees of freedom : in $ ( j_1 , j_2 ) $ , each contribute $2j_1 + 1$ states and we construct a tensor product , so $ ( j_1 , j_2 ) $ gives $ ( 2j_1 + 1 ) * ( 2j_2 + 2 ) $ degrees of freedom . for the vector we have $ ( 1/2 , 1/2 ) \mapsto 2 * 2 = 4$ degrees of freedom . if the representation is reducible , i.e. of the form $ ( j_1 , j_2 ) + ( k_1 , k_2 ) $ , then you simply add the d.o.f. you get from each pair . the dirac spinor has $ ( 1/2 , 0 ) + ( 0 , 1/2 ) \mapsto ( 2 ) + ( 2 ) = 4$ degrees of freedom , the field-strength tensor has $ ( 1 , 0 ) + ( 0 , 1 ) \mapsto 3 + 3 = 6$ d.o.f. as a sidenote : the representations $ ( 1 , 0 ) $ do not corresond to vectorlike degrees of freedom , but rather to antisymmetric self-dual tensors . $ ( 0 , 1 ) $ is the antisymmetric anti-self-dual tensor . the vector ( and the only way to get a vector out of this ) is $ ( 1/2 , 1/2 ) $ !
this sounds suspiciously like a homework problem , which means we are only allowed to discuss general concepts . your professor would not be best pleased if we just gave you the answer to their problems :- ) the amplitude $y$ is a function of time and distance . since you just want the frequency the $x$ dependance does not matter and you can just look at the amplitude at a fixed value of $x$ . i would choose $x = 0$ as the simplest option . now you have the amplitude as a function of just time . you have to work out how long it takes for the amplitude to go through one cycle . for example suppose you start at $t = 0$ , then as you increase $t$ the value $y ( t ) $ will fall to zero , go negative , start rising again and eventually return to it is original value . that time is the period . once you know the period the frequency is just the reciprocal of the period .
for elementary particles one uses the angular distributions of the decay products in the center of mass and the spin properties of the decay products themselves . in this particular case , because the resonance decays into two photons it has to be either spin 0 or spin 2 . the angular distribution of the photons is distinctly different in the two cases . there is a talk on this at the cern site , so the next announcements will probably be about the spin .
one aspect that comes to my mind is the concept of causality . superluminal propagation would allow for a violation of this principle , creating various paradoxical phenomena .
i am not an expert on this , so i would appreciate if errors are pointed out . to my understanding , the difference between the total energy and the free energy is due to statistical mechanics . your simulation works on the level of the smallest constituents of the system , and does not directly look for the lowest energy configuration , but samples the space of all configurations with a probability for each state that is a function of its energy and the temperature . all statistical phenomena should automatically emerge from this . for example , a state that is qualitatively equal to many other states ( essentially corresponding to a macrostate with high entropy ) will be sampled more often than a state of lower energy that is very unlikely . the transition from a lower energy state to this higher energy state should happen in your simulation exactly when the associated free energy change negative , which will depend on temperature .
in the following , i will describe to you how in principle one can compute higher order corrections to the bohr-sommerfeld condition . in order to find higher order corrections to the bohr-sommerfeld formula , we need to include higher order corrections of the wavefunction of the form : $\psi ( x ) = \sum_n \hbar^n a_n ( x ) exp ( \frac{i}{\hbar} \int^x \sqrt{2m ( e-v ( y ) ) }dy ) $ inserting in the schrodinger equation and requiring fullfilment to each order of $\hbar$ , we obtain : $a_0 = \frac{const . }{ ( 2m ( e-v ( y ) ) ) ^{\frac{1}{4}} }$ and $ a_k \delta s + 2 \nabla a_k . \nabla s = i \delta a_{k-1}$ where:$ s= \int^x \sqrt{2m ( e-v ( y ) ) } dy$ . in one dimensions these equations can solved in closed form . the quantization condition is achieved through the requirement that the wave function phase does not change over a closed loop : $\oint^x \frac{1}{\psi} \frac{d\psi}{dx} dx = 2\pi i n\hbar $ please see , the following article by a . voros where this procedure was applied to the anharmonic oscillator . equation ( 4.4 ) , describes an explicit solution of the wave function to the first higher order . remark : voros uses the bargmann representation of the phase space instead of the usual coordinate-momentum representation .
the uncertainty principle should be understood as follows : the position and momentum of a particle are not well-defined at the same time . quantum mechanically , this is expressed through the fact that the position and momentum operators do not commute : $ [ x , p ] =i\hbar$ . the most intuitive explanation , for me , is to think about it in terms of wave-particle duality . de broglie introduced the idea that every particle also exhibits the properties of a wave . the wavelength then determines the momentum through $$p=\frac{h}{\lambda}$$ where $\lambda$ is the de broglie wavelength associated with the particle . however , when one thinks about a wave , it is clear that the object described by it will not be easy to ascribe a position to . in fact , one needs a specific superposition of waves to create a wave that is essentially zero everywhere except at some position $x$ . however , if one creates such a wave packet , one loses information about the exact wavelength ( since a wave with a single , well-defined wavelength will simply extend throughout space ) . so , there is an inherent limitation to knowing the wavelength ( i.e. . momentum ) and position of a particle . on a more technical level , one could say that the uncertainty principle is simply a consequence of wave-particle duality combined with properties of the fourier transform . the uncertainty is made precise by the famous heisenberg uncertainty principle , $$\sigma_x\sigma_p\geq \frac{\hbar}{2}$$ more generally , for two non-comuting observables $a$ and $b$ ( represented by hermitian operators ) , the generalized uncertainty principle reads $$\sigma_a^2\sigma_b^2\geq \left ( \frac{1}{2i}\langle [ a , b ] \rangle\right ) ^2\ \implies \ \sigma_a\sigma_b \geq \frac{|\langle [ a , b ] \rangle| }{2}$$ here , $\sigma$ denotes the standard deviation and $\langle\dots\rangle$ the expectation value . this holds at any time . therefore , the measurement occurring right now , having occurred in the past or occurring in the future has nothing to do with it : the uncertainty principle always holds .
this is really a comment rather than an answer , but it got too long to put in a comment . flatness problem : on flatness problem , inflation etc why does inflation ( the inflaton field ) push omega down closer to zero ( flatten the universe ) ? how does inflation drive  close to 1 ? http://en.wikipedia.org/wiki/flatness_problem monopoles : how does inflation solve the magnetic monopole problem ? http://en.wikipedia.org/wiki/monopole_problem#magnetic-monopole_problem horizon problem : i could not find a match question on this site but see http://en.wikipedia.org/wiki/horizon_problem . each of the three subjects , the flatness problem , the horizon problem and the monopole problem is a long answer in it is own right . i would suggest you have a look at the links above and ask a new quaestion about any specific issues you do not understand .
the binding energy curve , again in wikipedia , shows iron as the one with the smallest binding energy per nucleon . though in the table , the following is stated : 56fe has the lowest nucleon-specific mass of the four nuclides listed in this table , but this does not imply it is the strongest bound atom per hadron , unless the choice of beginning hadrons is completely free . iron releases the largest energy if any 56 nucleons are allowed to build a nuclidechanging one to another if necessary , the highest binding energy per hadron , with the hadrons starting as the same number of protons z and total nucleons a as in the bound nucleus , is 62ni . thus , the true absolute value of the total binding energy of a nucleus depends on what we are allowed to construct the nucleus out of . if all nuclei of mass number a were to be allowed to be constructed of a neutrons , then fe-56 would release the most energy per nucleon , since it has a larger fraction of protons than ni-62 . however , if nucleons are required to be constructed of only the same number of protons and neutrons that they contain , then nickel-62 is the most tightly bound nucleus , per nucleon . one sees that there is a leeway when constructing models in end of the universe scenaria . there is so much speculation in the time lines . observation tells us that ni-62 is not abundant , while fe is . it seems that in the sequence of supernova explosions iron wins out ; according to the quote above , this would mean that it is the number of nucleons that is important and the charges statistically arrange themselves . anyway , in a continuously expanding universe with a stable proton it is hard to see how the expanding gases of helium and hydrogen can tunnel into anything as they expand so that " all matter " would end up as fe or ni atoms . not to worry , the proton will decay according to all models of particle physics anyway .
i can not go into much detail here but let me say that exponential growth brings many things that we see around us right now : absence of magnetic monopoles , a homogeneous universe in which no section is a " preferred " section i.e. has more matter density , and many more observable quantities . actually after a brief search i found a wiki article stating most of the things i said above and much , much more : http://en.wikipedia.org/wiki/cosmic_inflation#observational_status
entropy never increases during this " mixing process " as it normally would , because it is not really a mixing process . the only thing that happens is a huge distortion of the distribution of the colors , but no information about it is lost . compare it to performing a fourier transform on some function : this also results in something that looks completely different , sometimes chaotic and high-entropy-like , but it is really just an invertible transformation .
there is no difference between light and visible light . english use of the word light is based on an understanding that predates fundamental scientific description of light and photons . however , it would be generally interpreted that if you say , " the light from the stars shines down on us " would mean visible light . a physicist discussing light shining from a star could mean the entire spectrum of photon energies , including visible light . the term visible light is specific to the photon energies that are visible to human beings . when discussing photons with other energy levels , it is not uncommon for physicists to talk about the photons as light , even if it is not visible to the human eye . not including the word " visible " is primarily a short-cut .
to deduce this , you have to specify the kind of decay and the nature of the " compund " is it a crystal , a small molecule in gas phase , a organic material ? beta decay shifts the nucleus one position upward in pse , thus any " compound " will be transformed into a cation by loss of an electron , and whre say a iodide ion had been , there will be an xe atom . ( which will not " fit " chemically of course ) there will be some recoil in this process , which can cause the nucleus to leave its place . the electron will ionize everything along its path , those products of ionisation can alter/ destroy the molecule ( compound ) where the electron was emitted . similar is the case of alpha , with a strong recoil and severly ionisation . the decaying nucleus is shifted two " down " in pse . the alpha particle will stay in the crystal , if it is big enough . think of helium gassing out of pechblende when heated . for gamma , recoil will be less , ionisation is distributed along a long path ( maybe meters ) there are special cases in crystals , when the recoil is taken not by the emitting nucleus alone , but collectively ba the crystal lattice . ( mbauer effect ) in general , radioactive decay is so energetic , that any chemical bonds/lattice forces are broken . what happens then is very complicated and not to be answered by a simple scheme .
you could make an analogy between the pressure distribution of a sound wave and the mass density distribution of a realistic spring undergoing vibrations , but it would not give you the explanation you are looking for . as a matter of fact , that would be more like explaining a sound wave in terms of springs , rather than what you are trying to do , i.e. explaining a spring in terms of waves . although i am not intimately familiar with the details , basically what goes on at the microscopic level of a spring is that , when the spring is at equilibrium , the atoms are set in some sort of rigid structure . any given pair of atoms has a potential energy which is a function of the distance between those two atoms , so the entire spring has a potential energy determined by all the distances between every possible pair of atoms : $$u = \sum_{i , j} u_{ij} ( r_{ij} ) $$ in equilibrium , the spring will take a shape which minimizes this total potential energy . if you think about it , a metal spring might typically be formed by heating some metal to make it malleable ( or even melting it ) , and then forming it into the desired shape before it cools . the heat allows the atoms to move around relatively freely so that they can reach the equilibrium configuration that minimizes their potential energy , then once the spring cools , they are frozen in place . of course , the atoms are not completely frozen in place . as i see that georg has already written in his answer , the potential energy between two atoms ( $u_{ij} ( r_{ij} ) $ ) has a minimum at their equilibrium distance and goes up on either side . if you add some energy into the system , say by exerting a force on it , you can get the atoms to move closer together or further apart . when you stretch or compress a string , you are really just doing this to all the ( pairs of ) atoms in the spring simultaneously . the atoms will , of course , " try " to return to their equilibrium position , i.e. they will " try " to minimize their potential energy , and this is what you feel as the restoring force of a spring under tension .
i am highly skeptical of this result , primarily because the theories promoted by black light power are improbable to the point of being gibberish . the energy states of hydrogen can be calculated exactly , and have been both calculated and measured spectroscopically to extremely high precision , and experiment and theory are in perfect agreement . if the modern understanding of quantum physics ( including qed ) were incomplete enough to leave room for mysterious lower-energy states in hydrogen , there would've been some indication of this in one of the countless experiments that have been done on hydrogen . another good reason to be skeptical of this result is that the report in question seems to have been " released " only via black light power 's web site . the only mention of the authors of this report in conjunction with " hydrinos " that google can find come from black light power . this result has not appeared in any scientific journal known to google . or even the rowan university web site . this is not what i would call a ringing endorsement of the work . as for the report itself , it is entirely concerned with chemical nmr spectra , and i do not have any first-hand experience with those . i know just enough about the field to know that there can be subtle issues involved with the recording and interpretation of these . i am more inclined to believe that the mysterious peaks seen in their samples are some nmr artifact than that they are the signature of radically new physics . it is conceivable , barely , that this really does represent some dramatic new discovery , and has not yet appeared in print because it is working its way through the peer review process , taking a long time because extraordinary claims require extraordinary scrutiny . the principal person behind black light power has been making claims like this since i was in grad school in the 1990 's , though , and has yet to produce anything solid . i would not hold my breath waiting for this to appear in a reputable peer-reviewed journal , if i were you .
at the level of understanding the data and observations we have up to now , general relativity describes well what we perceive of the cosmos and quantum field theory what we observe in the microcosm of elementary particles and their interactions . the two have not been joined up to now , i.e. there is no accepted unified theory that joins smoothly these two mathematical frameworks . string theory is the only known theory that has both quantization of gravity and the groups structures that can accommodate the elementary particle standard model , but it is still at the research level , due to the complexity of the mathematical systems possible . at the moment gravitons are hypothetical particles on par with photons gluons and z-w mesons in string theory . string theory predicts the existence of gravitons and their well-defined interactions . a graviton in perturbative string theory is a closed string in a very particular low-energy vibrational state in the same class as the other particles , which are also particular vibrational levels of strings . the classical stress energy tensor that macroscopically is described by general relativity as space-time distortions will emerge by the confluence of innumerable gravitons , in an analogous way that the electric and magnetic fields appear from the confluence of innumerable photons , which electromagnetic fields are well described by maxwell 's equations . even when we manage to have a theory of everything ( toe ) , each observational region will be described at its own level of mathematical complexity . for example when one is doing optics one forgets that light is made up of photons and uses the classical equations very successfully except in regions where quantization is important for understanding .
spectral geometry is one of the many ways mathematicians think about geometry . the general idea is that if you have some manifold equipped with a metric , you can cook up some canonical differential operators . these operators can be thought of as linear operators , acting on ( infinite-dimensional ) vector spaces of functions , tensors , spinors , and the like . each such linear operator will have a set of eigenvalues . spectral geometry is concerned with relationships between these eigenvalues and the geometry of the manifold you started with the most obvious linear operator to associate to a metric is the laplacian , which is a linear operator on the space of functions on the manifold . in the early/middle part of the 20th century , mathematicians started wondering " if you know the set of eigenvalues of the laplacian , can you reconstruct the manifold ? " , or as mark kac famously put it : " can one hear the shape of a drum ? " the answer is no ; the set of eigenvalues alone does not let you reconstruct the manifold and its metric . the map which sends a manifold with metric to the set of eigenvalues of the laplacian is not invertible . but it is such a pretty idea that people have not given up on it . alain connes , for example , figured out that the answer to a slightly different question is " yes " . if you have a commutative spectral triple ( basically , the dirac operator on a compact spin manifold ) , you can reconstruct the metric from this data . the physicists interviewed in the linked article are trying a slightly different variation on the spectral geometry problem . they are considering systematic finite-dimensional approximations to the " derivative " of the map $f$ which sends a manifold to its set of eigenvalues of its laplacian acting on tensors of low-degree , and trying to show that these approximations are invertible . this should let them show that this map $f$ is invertible in small regions of the space of manifolds . it is a nice idea , and looks like some fun experimental mathematics . trying to write down a theory of gravity in explicitly gauge invariant terms is also a good idea . but i would be quite surprised if this line of thinking bears any fruit . it seems more likely to me that we need some essentially new physical ideas than a clever way of rewriting what we already have .
there are ways to define " temperatures " for single molecules or atoms , but they are subtly different than the way in which temperature is defined for bulk material . temperature is essentially a statistical quantity as averaged over the various internal states of the material , and for large composite systems at equilibrium , the internal energy levels are boltzmann distributed , meaning that there is a probability distribution for the possible states the system can occupy . higher energy states are less likely than lower energy states , with the exact relation determined by the " temperature " , which tells you the total average energy content of the material . for single molecules , you can define temperature in the same way , namely by the occupancy of the various energy levels , but this can lead to bizarre ramifications , such as negative temperatures in lasing materials , and multiple definitions of temperature for molecules whose quantum states are not at equilibrium ( often you will see authors make a distinction between " translational temperature " and " vibrational temperature " , for example ) .
qmechanic basically answered it in a comment , but to reiterate , the wikipedia article on free-fall details this exact scenario : the time as a function of separation becomes $$t ( y ) =\sqrt{\frac{y_0^3}{2g ( m_1+m_2 ) }}\left ( \sqrt{\frac{y}{y_0}\left ( 1-\frac{y}{y_0}\right ) }+\cos^{-1}\left ( \frac{y}{y_0}\right ) \right ) $$ where $y_0$ is the initial separation . letting the radii be $r_1 , r_2$ , this yields a collision time $$t=t ( r_1+r_2 ) . $$
well , first we know that the coriolis force acting on the scale of a toilet is going to be a pretty small force . so the question can generally be tackled in the following way : estimate the magnitude of the coriolis force on the toilet water . use this to estimate the magnitude of effect of the coriolis force on the toilet water spin . enumerate the possible other sources of spin for the toilet water . the two major ones off the top of my head would be any angling of the water jets , and the shape of the toilet bowl . estimate the magnitude of the effect of these sources of spin . compare the two effects . you will probably find an answer which is something of the form : " if the angling of the toilet jets is less than $\theta$ for a jet velocity of $v$ , then the coriolis force should dominate " essentially what i am doing is asking " how idealized would this toilet have to be for the coriolis force to be the dominant source of spin in the toilet water ? "
if you observe a star of radius $r$ from the distance $l$ , you will see it as a small disk under the angle $2r/l$ so the solid angle the photons from the star will cover will scale like $ ( r/l ) ^2$ . that is the percentage of the retina that will be receiving photons : the solid angle measures the " percentage of directions " in which the photons from the star are flying . the number of photons from the star that hit your eyebulb scales like $ ( r/l ) ^2$ as well ( the dependence $1/l^2$ is what i care about here ) , because they are divided to all points on the sphere of area $4\pi l^2$ , so by dividing these two expressions , you may easily see that the number of photons per unit area of the retina is actually independent of $l$ . the star will look smaller as it gets further but the number of photons per unit time that hit a small area of the retina is $l$-independent . if you are really worried that the star does not emit enough photons to satisfy your eyes , note that the sun emits roughly $4\times 10^{44}$ photons each second . the earth-sun distance is roughly 150 million km which is 15 trillion times the radius of the eyebulb . square it and you will still get that the number of eye-sized areas on the surface of the 1-au-radius-large sphere is just of order $10^{26}$ , still giving you $10^{18}$ photons to each eyebulb per second . so if you allow 100 photons per eyebulb to be enough to see it , you may still allow the star to be $10^{8}$ times further than the sun . the sun is 8 light minutes and if multiplied by 100 million , you get something like 200 light years . so with this minimal required number of photons per eyebulb ( 100 per second ) , you may see stars up to hundreds of light years away ( i can not ) . of course , telescopes are collecting starlight from a much larger area than the eyebulb ( and they may also patiently collect the photons for a much longer time ) so they may see stars much further than that .
a particle as a point mass does not have rotation defined . so the question does not apply to point masses . in fact , rotations are used to describe the position of a point mass riding on a moving coordinate frame . i see rotation as a property of the frame of reference , and not necessarily of the masses tracked .
one way of understanding this which has always had intuitive appeal to me is the so-called huygens principle which basically states that every point on a wavefront can be considered a point source for a new spherical wave , and that the evolution of the wavefront can be determined by superposing all of these spherical waves at later times . the wikipedia article that i linked to has some really nice pictures of this . diffraction effects can then be explained using this principle . imagine , for example , that you shine light through an extremely small slit , say a slit about the size of the wavelength itself , then when plane waves pass through this slit , the part of the wave that goes through the slit acts as a point source and generates a spherical wave , so the light diffracts . if the slit is larger , however , then the part of the wavefronts that pass through the slit act as multiple little points sources for spherical waves , and these spherical waves interfere with each other to give an interference pattern . in this way , the diffraction pattern is very much like multiple slit interference , except instead of multiple slits , the wave front itself splits into a bunch of adjacent point sources that interfere with each other .
i think it is a bit of a wording issue . the cauchy stress tensor can be decomposed as the sum of a hydrostatic and a deviatoric component as $\sigma=\mathbf{s}+\frac{1}{3}\mbox{tr} ( \sigma ) \mathbf{i}=\mathbf{s}+p\mathbf{i}$ where $p$ is the pressure , and since $f=\sigma\cdot\hat{\mathbf{n}}$ , the force related to the hydrostatic pressure component simply reduces to $f_{hyd}=p\hat{\mathbf{n}}$ , whose magnitude is evidently independent of direction $\hat{\mathbf{n}}$ . more intuitively , if you imagine a bubble floating in the air , the interior walls of the bubble are being bombarded by air molecules flying in all directions , but the air molecules have no preferred direction , and they exert force in all directions . so we say that pressure is independent of direction in a region where the pressure is equal everywhere . this does not necessarily mean that pressure can not be manipulated to exert a force in a certain direction to do work , though . for example , inside a gun as it fires , you essentially have a cylinder ( with one end blocked , and the other end fitted with a bullet ) with a high pressure gas inside . the gas exerts force isotropically ( in all directions equally ) , so the back face of the bullet is feeling the same force per area as the walls of the gun are . but , the bullet is free to move ( while the walls are not , unless the pressure so high that the gun explodes ) , and so it is forced out of the gun . but throughout the whole process , the gas was in fact exerting force isotropically in all directions . similarly , in the case of a nail being driven into a piece of wood , at the nail-wood interface there is an extremely high pressure , and it exerts force in all directions . however , just like the bullet ( and not the walls ) was able to yield in the case of the gun , the wood ( and not the nail , nor the person driving it in ) is able to physically yield under the pressure . thus the wood is moved out of the way under the pressure , even though pressure at the interface applies in all directions .
maybe you are not differentiating anything but you should . the functions $b ( t ) $ express the density of energy per unit interval of frequencies  or unit interval of wavelengths . but the width of the unit intervals are not the same . in particular , the total energy in an interval $ ( \nu , \nu+d\nu ) $ is $$ e ( \nu , \nu+d\nu ) = b_\nu ( t ) \cdot d\nu \sim \frac{\nu^3}{c^2} \cdot d\nu $$ that is what the formula for the density per unit frequency means . when you want to translate it to the language of the wavelength , you must realize that $$ \nu = \frac{c}{\lambda} $$ and therefore ( the line below is obtained just by taking a derivative ) $$ d\nu = -\frac{c}{\lambda^2} \cdot d\lambda $$ the minus sign should be ignored because it just means that the graphs should be left-right-reflected . however , this $d\nu$ may be substituted to the first equation to get $$ e ( \nu , \nu+d\nu ) \sim \left ( \frac{c}{\lambda}\right ) ^3\cdot \frac{1}{c^2} \cdot \frac{c}{\lambda^2}\cdot d\lambda \sim \frac{c^2}{\lambda^5}d\lambda $$ which gives you the right dependence on $\lambda$ and $c$ , namely $c^2/\lambda^5$ . all the other factors are the same and there is no change in the numerical prefactor . note that i can call this energy in the interval $e ( \lambda , \lambda+d\lambda ) $ because throughout the text , i was assuming it was physically the same interval , just parameterized either via $\nu$ or by $\lambda$ .
i am almost certain there used to be an answer to this question , but it seems to be gone , i will write another one . the earth and sun both orbit their mutual barycenter ( disregarding any other objects of course ) . that one point is a focal point of both ellipses , and all three focal points are collinear . it may appear asymmetric because the sun 's motion is so small , which comes from the asymmetry in the masses . imagine smoothly scaling the sun down to an earth mass . as you did so , the ellipses would approach each other in size , and your missing symmetry would be restored . the key is to notice that there are three , not just two , foci in the system .
1 . not all states produced by $\text{cnot} \ ; ( h \otimes i ) $ are entangled . for the first part of your question : no , not all states which arise as the output of $\text{cnot} \ ; ( h \otimes i ) $ are entangled . specifically , you can consider $$\begin{align*} |\psi\rangle \ ; and =\ ; ( h \otimes i ) \ ; \text{cnot} \ ; \bigl [ \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |10\rangle \bigr ) \bigr ] \\ \ ; and =\ ; ( h \otimes i ) \bigl [ \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |11\rangle \bigr ) \bigr ] \\ \ ; and =\ ; \tfrac{1}{2} \bigl ( |00\rangle + |01\rangle + |10\rangle - |11\rangle\bigr ) , \end{align*}$$ which is a maximally entangled state ( notice from the second line above that it differs from a bell state by a local unitary ) . however , by construction , if you apply $\text{cnot} \ ; ( h \otimes i ) $ to $|\psi\rangle$ , you will get back out the state $$ \text{cnot} \ ; ( h \otimes i ) \ ; |\psi\rangle \ ; =\ ; \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |10\rangle \bigr ) \ ; =\ ; |+\rangle\otimes|0\rangle\ ; , $$ which has no entanglement at all . even if you are interested only in inputs which are product states , we can see that the circuit maps $|+\rangle \otimes |0\rangle$ to another product state &mdash ; specifically , $|0\rangle|0\rangle$ . 2 . all maximally entangled two-qubit states can be easily described by a similar circuit to $\text{cnot} \ ; ( h \otimes i ) $ acting on the standard basis . for the second part of your question : if you allow yourself arbitrary single-qubit unitaries , then for any maximally entangled two-qubit state $|\psi\rangle$ , you can certainly construct a circuit which constructs $|\psi\rangle$ from standard basis states , and which allows you easily to see that the state is maximally entangled . every two-qubit state has a schmidt decomposition , $$ |\psi\rangle \ ; =\ ; u_0|\alpha_0\rangle|\beta_0\rangle \ ; +\ ; u_1|\alpha_1\rangle|\beta_1\rangle \ ; , $$ where $u_0 \geqslant u_1 \geqslant 0$ &mdash ; in particular , $u_1 = 0$ for $|\psi\rangle$ a product state &mdash ; and where $\{ |\alpha_0\rangle , |\alpha_1\rangle \}$ and $\{ |\beta_0\rangle , |\beta_1\rangle \}$ are each orthonormal bases for $\mathbb c^2$ . in order for $|\psi\rangle$ to be maximally entangled , we need $u_0 = u_1 = \tfrac{1}{\sqrt 2}$ . consider single-qubit unitary matrices $a$ and $b$ , such that $$\begin{alignat*}{4} a |x\rangle \ ; and =\ ; |\alpha_x\rangle and \quad and \text{for $x \in \{0,1\}$} , \\ [ 1ex ] b |y\rangle \ ; and =\ ; |\beta_y\rangle and and \text{for $y \in \{0,1\}$} ; \end{alignat*}$$ then we can describe $|\psi\rangle = ( a \otimes b ) \ ; \text{cnot}\ ; ( h \otimes i ) \ ; |0\rangle|0\rangle$ , that is , the effect of applying $ ( a \otimes b ) $ to the bell state $|\phi^+\rangle = \tfrac{1}{\sqrt 2}\bigl ( |00\rangle + |11\rangle ) $ . furthermore , it is not hard to show that any standard basis state is mapped by that circuit to some maximally entangled state similar to $|\psi\rangle$ , and that any circuit of this form ( whatever $a$ and $b$ might be ) maps any standard basis state to a maximally entangled two-qubit state .
i was able to use the feedback from the physics forum ultimately it was that response that gave me the most detail and enabled me to complete my own app .
most of the popular science tv programmes and magazine articles give entirely the wrong idea about how the higgs mechanism works . they tend to give the impression that there is a single higgs boson that ( a ) causes particles masses and ( b ) will be found around 125gev by the lhc . the mass is generated by the higgs field . see the wikipedia article on the higgs mechanism for details . to ( over ) simplify , the higgs field has four degrees of freedom , three of which interact with the w and z bosons and generate masses . the remaining degree of freedom is what we see as the 125gev higgs boson . in a sense , the higgs boson that the lhc is about to discover is just what is left over after the higgs field has done it is work . the higgs boson gets its mass from the higgs mechanism just like the w and z bosons : it is not the origin of the particle masses . the higgs boson does not have zero rest mass . a quick footnote : matt strassler 's blog has an excellent article about this . the higgs mass can be written as an interaction with the higgs field just like e.g. the w boson . however matt strassler makes the point that this is a coincidence rather than anything fundamental and unlike the w and z the higgs boson could have a non-zero mass even if the higgs field was zero everywhere .
the water will probably turn into plasma really fast . i am too lazy here to look up exactly how to figure this out but i can provide an outline to answering your question : ( 1 ) figure out how much energy is needed to split water into its atomic constituents . a ) figure out how much energy is needed to pull two hydrogens off of oxygen . then see if 2000 degrees c is more or less energy using the temperature to energy relationship , e = 0.5*kt , ( e is average kinetic energy of particles in question , k is boltzmann 's constant , and t is temperature in kelvins . ) note . this will give an approximate value of the energy in terms of temperature . if you have more than enough energy to do this , then use the remaining energy you have left and go onto part b . b ) figure out how much energy is needed to ionize an electron from hydrogen . multiply that by 2 , because you have two hydrogen atoms . then figure out how much energy is needed to ionize all of oxygen ( thats a lot of freaking energy ) . then again , see how much energy you have left . if you still have more than enough energy , then move onto part c . c ) so by now you have 2 protons , 2 electrons from the 2 hydrogens , and an oxygen nucleus and its how ever many electron . you can then figure out how much energy is needed to break apart the oxygen nucleus into free protons and neutrons , ( this is a lot of freaking energy , and i do not know how to calculate this . . . good luck finding that . ) if you still have more than enough energy , then move onto part d . d ) figure out how much energy is needed to split protons into quarks , lol . and see if you still have enough energy left . this is kinda overkill . again , do not know how to do this . sorry . so if you figured out how much energy is needed to break one water molecule into whatever part you get to , b , c , or d . multiply that by the number of water molecules you have to figure out the total amount of energy needed . so in conclusion , its just simply comparing how much energy is needed to break apart water molecules and seeing what it turns into .
evaporation of sweat from the skin surface has a cooling effect due to the latent heat of evaporation of water ( from wikipedia ) . basically , the heat is transfered from your skin to the water , the water then evaporates , taking with it a bit of the energy stored in it . further references here
laplace 's determinism is not physically correct over long periods of time . that is , it neglects chaos/"sensitive dependence on initial conditions"/exponential growth of microscopic perturbations already in newtonian dynamics , which was seriously thought about only in the 20th century . being true , this also will not be overcome . stochasticity enters some classical dynamical paths with time . there is subtlety here . in classical mechanics , or the evolution of the wave function , there is a kind of microdeterminism , so that what occurs in the next instant is fully determined by what occurred up until that point . it is in the longer time evolution of a chaotic system that stochasticity creeps in . by the way , lapalace said " we ought to regard the present state of the universe as the effect of its antecedent state and as the cause of the state that is to follow . " this part remains true in chaotic classical mechanics . however , he then continued " an intelligence knowing all the forces acting in nature at a given instant , as well as the momentary positions of all things in the universe , would be able to comprehend in one single formula the motions of the largest bodies as well as the lightest atoms in the world , provided that its intellect were sufficiently powerful to subject all data to analysis ; to it nothing would be uncertain , the future as well as the past would be present to its eyes . the perfection that the human mind has been able to give to astronomy affords but a feeble outline of such an intelligence . ( laplace 1820 ) " this is the part that classical chaos invalidates . you might also read http://plato.stanford.edu/entries/determinism-causal/ finally , there are also questions whether , in light of general relativity , black holes , etc , we can even speak of a " state of the universe as a whole " there may not be such a god 's eye view altogether . these issues need a philosophy forum , however .
there are many physical intuitions often presented in various texts on fluid dynamics . i will not mention those here . i will , however , mention that mathematically the passage from a particle point of view to a continuum point of view is still a largely un-resolved problem . ( with suitable interpretation , this problem was already posed by hilbert as his 6th of 23 problems . ) we can interpret the problem as one of starting from " a newtonian description of particles interacting through collisions " and try to end up with " an approximation of the physical system by a continuum obeying certain laws of fluid dynamics ( euler , navier-stokes , etc . ) " most work up through now takes an intermediate step through the boltzmann equation : in this kinetic theory model , instead of individual particles we consider distributions of particles , where the " density " of particles is given based on both position and velocity . so it makes one level of continuum approximation . but it still keeps the facet of newtonian theory where particles interact through direct collisions . under an assumption known as molecular chaos ( more on this later ) , that boltzmann 's equation follows from newtonian laws of motion have been demonstrated , to various degrees of rigour , by boltzmann himself , as well as grad , cercignani , and lanford , building on the work of bogoliubov , born , green , kirkwood , and yvon . for a mathematically sophisticated , but more or less self-contained description one can refer to uchiyama 's write-up . there are a few issues with this derivation . the problem of potentials . the derivations listed above made assumption that the particles are hard spheres : that the only interaction between two particles is when they actually collide ( so no inter-molecular forces mediated by electromagnetism , like hydrogen bonds and such ) , and that the particles are spherical . this is satisfactory for monatomic gases , but less so for diatomic molecules or ones with even stranger shapes . most people do not think of this as a big problem though . the derivation is only valid under the so-called grad limit assumption . to take the continuum limit , generally the assumption is made that the particle diameter decreases to zero , while the number of particles ( per unit volume ) increases to infinity . exactly how these two limits balance out affects what the physical laws look like in the continuum limit . the grad limit assumes that the square of the particle diameter scales like the inverse of the number density . this means that the actual volume occupied by the particles themselves ( as opposed to the free space between the particles ) decreases to zero in this limit . so in the grad limit one actually obtains an infinitely dilute gas . this is somewhat of a problem . the derivation also makes use of what is called molecular chaos : it assumes that , basically speaking , the only type of collision that matters is that between two particles , and that the particle , after its collision , " forgets " about its previous zig-zag among its cousins in the dilute gas . in particular , we completely ignore the case of three or more particles colliding simultaneously , and sort of ignore the billiards-trick-shot like multiple bounces . while both of this can be somewhat justified based on physical intuition ( the first by the fact that if you have a lot of small particles spaced far apart , the chances that three of them hit at the same time is much much much smaller than two of them colliding ; the second by the fact that you assume some sort of local thermodynamic equilibrium [ hence the name molecular chaos ] ) , one should be aware that they are taken as assumptions in the boltzmann picture . starting from the boltzmann equation , one can arrive at the euler and navier-stokes equations with quite a lot of work . there has been a lot of recent mathematical literature devoted to this problem , and under different assumptions ( basically how the reynolds and knudsen numbers behave in the limit ) one gets different versions of the fluid equations . a decent survey of the literature was written by f . golse , while a heavily mathematical discussion of the state-of-the-art can be found in laure saint-raymond 's hydrodynamic limits of the boltzmann equation . it is perhaps important to note that there are still regimes in which the connection between boltzmann equation and the fluid limits are not completely understood . and more important is to note that even were the connections between the kinetic ( boltzmann ) picture and the fluid limits , there is still the various assumptions made during the derivation of the boltzmann equation . thus we are still quite far from being able to rigorously justify the continuum picture of fluids from the particle picture of newtonian dynamics .
i do not think any of the other answers have made the following point clear enough , so i am going to give it a try . both scenarios are very similar before the collision , but they differ greatly afterwards . . . from a stationary reference , you see the cars driving towards each other at 50mph , but of course if you choose a reference frame moving with the first car , then the second will be headed toward it at 100 mph . how is this different from the wall scenario ? well , from a stationary reference frame , after the crash both cars remain at rest , so the kinetic energy dissipated is $2\times \frac{1}{2}mv^2$ . from the reference frame moving with the first car , the kinetic energy before the crash is $\frac{1}{2}m ( 2v ) ^2=4\times\frac{1}{2}mv^2$ , but after the crash the cars do not remain at rest , but keep moving in the direction of the second car at half the speed . so of course the kinetic energy after the crash is $2\times\frac{1}{2}mv^2$ , and the total kinetic energy lost in the crash is the same as when considering a stationary reference frame . in the car against a wall , you do have the full dissipation of a kinetic energy of $4\times\frac{1}{2}mv^2$ .
before i answer , a couple caveats : as adam said , the universe is not going to start behaving any differently because we discovered something . right now it seems much more likely ( even by admission of the experimenters ) that it is just a mistake somewhere in the analysis , not an actual case of superluminal motion . anyway : if the discovery turns out to be real , the effect on theoretical physics will be huge , basically because it has the potential to invalidate special relativity shows that special relativity is incomplete . that would have a " ripple effect " through the last century of progress in theoretical physics : almost every branch of theoretical physics for the past 70+ years uses relativity in one way or another , and many of the predictions that have emerged from those theories would have to be reexamined . ( there are many other predictions based on relativity that we have directly tested , and those will continue to be perfectly valid regardless of what happens . ) to be specific , one of the key predictions that emerges out of the special theory of relativity is that " ordinary " ( real-mass ) particles cannot reach or exceed the speed of light . this is not just an arbitrary rule like a speed limit on a highway , either . relativity is fundamentally based on a mathematical model of how objects move , the lorentz group . basically , when you go from sitting still to moving , your viewpoint on the universe changes in a way specified by a lorentz transformation , or " boost , " which basically entails mixing time and space a little bit . ( time dilation and length contraction , if you are familiar with them ) we have verified to high precision that this is actually true , i.e. that the observed consequences of changing your velocity do match what the lorentz boost predicts . however , there is no lorentz boost that takes an object from moving slower than light to moving faster than light . if we were to discover a particle moving faster than light , we have a type of motion that can not be described by a lorentz boosts , which means we have to start looking for something else ( other than relativity ) to describe it . now , having said that , there are a few ( more ) caveats . first , even if the detection is real , we have to ask ourselves whether we have really found a real-mass particle . the alternative is that we might have a particle with an imaginary mass , a true tachyon , which is consistent with relativity . tachyons are theoretically inconvenient , though ( well , that is putting it mildly ) . the main objection is that if we can interact with tachyons , we could use them to send messages back in time : if a tachyon travels between point a and point b , it is not well-defined whether it started from point a and went to point b or it started from b and went to point a . the two situations can be transformed into each other by a lorentz boost , which means that depending on how you are moving , you could see one or the other . ( that is not the case for normal motion . ) this idea has been investigated in the past , but i am not sure whether anything useful came of it , and i have my doubts that this is the case , anyway . if we have not found a tachyon , then perhaps we just have to accept that relativity is incomplete . this is called " lorentz violation " in the lingo . people have done some research on lorentz-violating theories , but it is always been sort of a fringe topic ; the main intention has been to show that it leads to inconsistencies , thereby " proving " that the universe has to be lorentz-invariant . if we have discovered superluminal motion , though , people will start looking much more closely at those theories , which means there is going to be a lot of work for theoretical physicists in the years to come .
it is not clear to me what " block-earth " system is supposed to mean , either , particularly when there are two blocks . the key word in the problem is that they say it is an elastic collision . therefore , the total momentum and energy of the two blocks are conserved quantities . that is all one really needs to know in order to solve the problem . edit : a full analysis of the problem . prior to collision , block 1 has $e = m_1 gh$ and block 2 has no energy . just prior to collision , all this energy is kinetic , so $p^2/2m_1 = m_1 gh \implies p = m_1\sqrt{2gh}$ . after the collision , total momentum and energy must be conserved . $$\begin{align*} p_1 + p_2 and = p \\ \frac{ ( p_1 ) ^2}{2m_1} + \frac{ ( p_2 ) ^2}{2m_2} and = e\end{align*}$$ a straightforward way to attack this system of equations is to solve the first by $p_2 = p-p_1$ and substitute into the second , yielding $$m_2 ( p_1 ) ^2 + m_1 ( p^2 - 2pp_1 + [ p_1 ] ^2 ) = 2 m_1^2 m_2 gh$$ solve this for $p_1$ , the momentum of block 1 after the collision , using the usual methods for solving quadratic equations . once you have the momentum , you should be able to find the final height as the block goes back up the ramp .
i do not understand all the nitty details , but the gist goes as follows : you have a constant probe beam , which is cast on a sample and analysed later . suppose the sample has a very predictable behaviour , with an event every $24\mu \text{s}$ . then the analysis of the beam will show exactly that . this is the blue line in fig . 4 . now , suppose you want to hide these events , but you do not want to divert the beam around the sample ( for whatever reason ) . they do it in the following way : increase/decrease the wavelength of that part of the probe beam , which would encounter the event . this only serves as " marking " the part of the wave you want to manipulate . speed up waves with lower frequency and slow down waves with higher frequency . this creates a beam gap in the middle . that means , the sample will see the probe beam " turned off " during the event , and the probe beam is not affected . after the sample , reverse step 2 , i.e. slow down waves with lower frequency and speed up waves with higher frequency . reverse step 1 . now your probe beam looks roughly as if it was not messed with at all and probed the sample at all times . note , that this is a layman explanation , and the actual speeding up/slowing down is a rather complicated feat .
i would suggest griffiths and/or kane . read the reviews to see whether this is the kind of books you are after . there is also a pretty nice ( if quite popular ) flip tanedo 's blog series on the topic . if you like that , i also recommend the rest of his posts $-$ they are pretty cool .
your problem is deeper than it might seem at the first glance . apart from numbers , it involves a remarkable phenomenon , which was studied in detail only a few years ago in the paper phys . rev . lett . 90 , 248302 ( 2003 ) . suppose the puck is an infinitely thin disc , and it is sent across the ice sliding and rotating . what will stop first , translational or rotational motion ? the answer is they will stop simultaneously , and this does not depend on the initial translational and angular velocities . the origin is the intrinsic friction-mediated coupling between rotation and sliding . if rotation is too fast for a given speed $v$ , the translational friction will be very small . and vice versa , if rotation is too slow , the rotational deceleration is small . thus , there exists a " magic " value of $\epsilon=v/r\omega$ towards which every initial condition is attracted . in the paper cited above this magic value was found to be approximately 0.653 . this value fully characterizes the asymptotic motion of the puck and allows you to calculate the friction force distribution , deceleration and finally the path ( which is a straight line in this approximation ) . in fact , a part of that was already found in the paper cited above . however , this concerns only an infinitely thin disc , for which the pressure distribution is equal everywhere under the puck . with a thick disc you have more pressure under the front part of the puck , which additionally complicates the problem ( still , see some discussion in the paper ) . finally , if you change the shape of the pack ( as you in fact did by taking a ring instead of a disc ) , the magic value of $\epsilon$ also changes . see another paper that discusses this : phys . rev . lett . 95 , 264303 ( 2005 ) . see also this comment to this old paper on puck motion on the ice .
here 's a slightly different but equivalent way to think about it . forces describe interactions between two objects . if two objects are interacting , they exert forces on each other . if two objects are not interacting , they do not exert forces on each other . thus , an object does not " carry around " a force with it . a force is not a property of an object , just as dmckee explains . instead , we describe interactions between two objects using the more-abstract concept of force . in your block-hits-other-block scenario , it is tempting to ask where did the force come from if colliding object had $f_\text{net}=0$ ? but when forces are viewed as interactions , it becomes more apparent that the force did not come from anywhere within one of the objects . there simply was not an interaction before they collided , so we would not ascribe the existence of a force force .
a battery generates a voltage by a chemical reaction . there is a class of chemical reactions called redox reactions that involve the transport of electrons , and you can use the reaction to drive electrons through an external circuit . this is the basis of a battery . the battery will continue to provide power until all the reagents have been used up and the reaction stops . a battery generates a potential difference that is related to the free energy change of the reaction occurring in the battery . note that there is no charge separation in a battery until you connect it to something and current flows . a capacitor is completely different . it has a potential only because charge has been stored on it , and when you connect the capacitor to an external circuit a current only flows until all the charge has drained . unlike a battery , the voltage on a capacitor is variable and is proportional to the amount of charge stored on it .
the universe has no edge so to speak . it is , however , finite in age , so light can only have traveled a given distance to get to us . call this distance $r$ , the " radius " of the universe . any observer , anywhere , will see out to a distance $r$ in all directions from their location . now two different observers will have different origins for their respective observable universes , and so will see slightly ( or vastly ) different patches of the " full universe . " ( be careful when talking about things outside your observable patch by the way - it is very easy to end up talking about impossible scenarios that produce nonsensical results . ) this can happen even if the universe is closed ( read : finite ) , so long as its size is bigger than something like $r$ . so no , no one is at the " edge " of the universe . by the way , general relativity in no way requires homogeneity and isotropy . these are simply assumptions cosmologists make in order to take an utterly intractable problem ( evolving the whole universe ) and make it absurdly simple ( see the frw metric , which , although it may look complicated at first , is pretty much the must trivial thing you can do with general relativity ) . the homogeneous/isotropic assumptions , by the way , turn out to be justified on cosmological scales , though this was discovered only after the early days of gr-based cosmology , once we had very deep galaxy surveys .
the wikipedia diagram is giving the breakdown by mass not by volume . baryonic/leptonic ( i.e. . non-dark ) matter is only about 5% of all matter and of that four fifths of it is in the form of free hydrogen and helium . of the remaining 1% about half is neutrinos or heavy elements . that means only 0.5% of the mass/energy in the universe is in stars .
as michael brown mentioned in the comments , no one will explain this as well as feynman ( at least , no one we know of that is alive ) . but that does not mean your question does not deserve at least our attempt . so here is mine and i will try to keep this in the simplest terms i can . ( aside : to all of the physicists reading this , i apologize in advance but in my simplification , i may intentionally omit or contradict the true physics . for instance , i doubt i will be saying how the photon arises as a gauge boson in local u ( 1 ) symmetry ) . see ( 2 ) see ( 1 ) . just kidding . i have lumped 1 and 2 together because to explain what a photon is is to essentially explain where it comes from . hopefully , everyone reading this will be aware of the wave-particle duality that most ( all ) things enjoy . in that way , as mentioned , a photon is a particle in its own right . but does that mean that people can think of the photon as a tiny billiard ball ? no , that would be silly . the photon is a wave packet that , for all intents and purposes , is indivisible . consider a vibrating electron , it is motion one way or the other constitutes a current , which radiates a magnetic field . since this magnetic field is changing as the electron speeds up and slows down , it induces an electric field that radiates outward . since this electric field is also changing continuously , this in turn induces a magnetic field that radiates outward . rinse and repeat . the result is a self-propagating combination of electric and magnetic fields travelling outward from the electron . this is em radiation . the photon is the unit of an em wave . what is one photon ? say we shine a laser , then we block half the beam with a metal plate . the other half still comes through . if there were only one photon in the beam , when you block half of it with the metal plate , none of it would come through . one photon is the largest amount of energy of an em wave where this would still be true . in physics terms , we write it as $e=h\nu$ . the energy of one photon of a wave is equal to h ( a very small constant ) times the wave 's frequency . because the photon is indivisible , we can say it represents the smallest unit of energy of that particular em wave . a different wave would have a different smallest energy . to address what was mentioned in the comments , a photon may have full particle status , but it is not similar to an electron . photons have no mass , they are not matter and , when you examine the properties of a photon , there is no denying that they are packets of energy in every respect ; fluctuations in the background em plane . i will admit , at first glance it does seem very silly for us to say electric and magnetic fields can produce action at a distance and then say , " no , you need photons to cover that distance and actually do the interacting " . but it is true . at least , we can say it is true . without teaching everyone about field theory and symmetries , my short answer to this would be that in advanced physics , we have a certain equation that originally did not work out . as we have done a million times in the past , we had to modify this equation to work and we found that we could only do that by introducing a massless particle that mediates the em force . afterward , we noticed that this particle happened to have the same properties of a photon . in fact , if we tried the hypothetical situation where we assumed this particle was a photon , this one equation produced all of the laws and equations from electromagnetics that we already knew and loved . thus , we said , " we are pretty sure this equation is the right one to use . we assumed this particle we invented was a photon , and it resulted in the equations and laws we have in the real universe . so this must be the way it actually is ! " . having said that , we can never actually observe the photon as it mediates the force . this is simply because if we were to observe the photon , it would no longer be able to mediate the force because we have no method of observing a photon without destroying it . i have already explained how they are created , how they are destroyed is much simpler . when a photon hits something , it can either reflect , transmit , or be absorbed . the latter is destroying it . when it is absorbed , this means whatever it struck ( usually an electron ) absorbs all of the energy of that photon . that is it . how we know they exist . . . we know because we can do experiments with just one photon . we can see the effects of one photon . but most importantly , theoreticians say , " if a photon did not actually exist , what would happen in some experiment ? well jim , we would see outcome . and if they do exists , we should see different outcome . " then experimentalists perform the experiment and 10 times out of 10 we always see the outcome predicted by the existence of photons . this particle/wave packet is called a " photon " because it was first theorized specifically about light . i believe " photo " is from the greek word meaning light and the extra n was added because all of the particles known at that date ended with an n ( proton , electron , neutron . why not photon ? ) i am willing to bet i have missed something important , so let me know . if i can merge what i have missed with the general form of the answer , i will be happy to put it in .
i think a very good orignally german qm book is : straumann : " quantenmechanik : ein grundkurs ber nichtrelativistische quantentheorie " there is also a second volume : straumann : " relativistische quantenfeldtheorie " another good book is from g . grawert : quantenmechanik you may also have a look at thirrings books about qm ( it is mathematically more advanced ) . i think straumanns book is not " canonical " . often used by students are the books by nolting and greiner . from the experimental point of view you may have a look at demtrders book .
the longitudinal components are derivatives of scalars , $\partial_\mu\phi$ . for a massive spin-1 $v_\mu$ they appear only in the mass term $m^2_v v_\mu^2$ . that is where the dimension $2$ comes from .
how does one calculate the capacitance of two bodies with different charges ? to be clear , capacitance does not depend on the amount of charge ; the capacitance is determined by the geometry of the bodies . if you have two conductors , there are actually three capacitances to consider , the self-capacitance of each and mutual capacitance of the two conductors . in electrical circuits , the term capacitance is usually a shorthand for the mutual capacitance between two adjacent conductors , such as the two plates of a capacitor . however , for an isolated conductor there also exists a property called self-capacitance , which is the amount of electrical charge that must be added to an isolated conductor to raise its electrical potential by one unit ( i.e. . one volt , in most measurement systems ) . [ 20 ] the reference point for this potential is a theoretical hollow conducting sphere , of infinite radius , centered on the conductor . let $q_1$ be the charge on conductor 1 and $q_2$ the charge on conductor 2 . further let $c_1$ be the self-capacitance of conductor 1 , $c_2$ the self-capacitance of conductor 2 , and $c_{12}$ the mutual capacitance . we can then write : $$q_1 = c_1 v_1 + c_{12} ( v_1 - v_2 ) $$ $$q_2 = c_2 v_2 + c_{12} ( v_2 - v_1 ) $$ for an intentional capacitor , the self-capacitance of the conductors is insignificant , i.e. , the mutual capacitance dwarfs the self-capacitance of the conductors and we speak of the capacitance of the capacitor which is understood to be the mutual capacitance .
apparently this is a simple question with a not-so-simple answer . i believe the general consensus is that there is a thin layer of liquid water on the surface of the ice . this thin layer and the solid ice below it are responsible for the slipperiness of ice ; the water easily moves on the ice . ( well , why is that ? perhaps another se question . ) however , this is no real agreement as to why there is a thin layer of liquid water on the surface of ice to begin with . see here for a 2006 nyt article . and if are interested in the actual physics paper that the news article is based on , see here ( doi ) . one idea states that the molecules on the surface of ice vibrate more than the inner molecules , and that this is an intrinsic property of water ice . since the outer molecules are vibrating faster , they are more likely to be in a liquid state . another idea is that the movement of an object over ice causes heating , though i found conflicting sources as to whether there is a consensus on this . there is a popular idea that many hold but does not appear to hold water . ( heh . ) this idea posited that the added pressure on the ice from a foot or skate causes the melting point to rise , which would cause the thin layer of liquid to form . however , calculating the resulting pressure and increase in melting point does not line up with observation ; the melting point certainly does rise , but not enough .
the electrons in a metal are whizzing around due to thermal energy . lattice vibrations excite the electrons , the electrons travel some distance then scatter off the lattice again and transfer energy back to the lattice . just like a gas , the electrons have some average mean free path , and this depends on the temperature . you would think the mean free path would increase with temperature , because the lattice transfers more energy to the electrons , but the rate of scattering off the lattice also increases with temperature . how the mean free path behaves with temperature depends on the trade-off between these two effects , and the varation with temperature can be positive or negative . anyhow , if you heat one end of a metal rod and cool the other then the mean free path will be different at the two ends , and the electrons at the end with the higher mean free path will tend to diffuse into the end with the lower mean free path . the result is a net charge movement , and this creates the potential difference . there is a quantitative treament of the seebeck effect here .
do i understand your question correct ? you ask : if the s-matrix is the only quantity which really can distinguish interesting models from free models ? then i would say no . but there are no interesting models in 4d , fullfilling let 's say the wightman axioms , are there ? in lower dimension for example conformal field theories are almost like free ones , but they have a very interesting representation theory , with anyionic sectors braiding etc , which make them also mathematically interesting . but there scattering is trivial . your example taking quasi free fields ( with a certail kallen-lehmann density ) does not give something new , neither does taking wick polynomials of these fields , because it gives you essential the same model ( same borchers class ) .
electroweak symmetry breaking requires that $\partial v/\partial h_{u , d} = 0$ . combining the two expressions , we find the superpotential bilinear $\mu$ , $$ |\mu|^2 = \frac12 \left [ \frac{|m_{h_d}^2 - m_{h_u}^2|}{\cos 2\beta} - m_{h_u}^2 - m_{h_d}^2 - m_z^2\right ] _{\text{ew-scale}} $$ in the focus-point region $|m_{h_u}^2|$ and $|m_{h_d}^2|$ are " focused " to $\lesssim m_z^2$ at the electroweak scale ; this is insensitive to their values at the high-scale . careful with the sign of $m_{h_u}^2$ , though ; it is parameter unto itself , rather than the square of a real parameter , and it is typically negative at the electroweak scale . as a result $|\mu| \lesssim m_z$ , and certainly $|\mu| \ll m_1 , m_2$ . ( note that in minimal models like the cmssm , $\mu$ is calculated in this way , but in more general models , we can trade e.g. a free parameter $m_{h_d}^2$ for $\mu$ , and have $\mu$ as an input parameter and $m_{h_d}^2$ calculated . ) the lightest neutralino/chargino is therefore higgsino-like , with $$ m_{\chi_{1,2}} \approx |\mu| , \\ m_{\chi_{3}} \approx m_1 , \\ m_{\chi_{4}} \approx m_2 , \\ m_{\chi^\pm_{1}} \approx |\mu| , \\ m_{\chi^\pm_{2}} \approx m_2 . $$
yes , you can simply set $m=0$ in the fierz-pauli equation ( if it is written correctly : ) ) . the only thing to remember is that at $m=0$ it becomes gauge invariant under $\delta h_{\mu\nu}=\partial_\mu\xi_\nu+\partial_\nu\xi_\mu$ . it is this gauge invariance that reduces properly the number of degrees of freedom . from the gravity point of view the gauge invariance is nothing but linearized diffeomorphisms .
if something is infinitely dense , must it not also be infinitely massive ? nope . the singularity is a point where volume goes to zero , not where mass goes to infinity . it is a point with zero volume , but which still holds mass , due to the extreme stretching of space by gravity . the density is $\frac{mass}{volume}$ , so we say that in the limit $volume\rightarrow 0$ , the density goes to infinity , but that does not mean mass goes to infinity . the reason that the volume is zero rather than the mass is infinite is easy to see in an intuitive sense from the creation of a black hole . you might think of a volume of space with some mass which is compressed due to gravity . normal matter is no longer compressible at a certain point due to coulomb repulsion between atoms , but if the gravity is strong enough , you might get past that . you can continue compressing it infinitely ( though you will probably have to overcome some other force barriers along the way ) - until it has zero volume . but it still contains mass ! the mass can not just disappear through this process . the density is infinite , but the mass is still finite .
if the object floats : water level stays the same if the object sinks : water level decreases consider the force balance . the earth exerts an upward force on the lake . anything floating on the water is included in the weight of the lake . since water is constant density , the upward force on the lake is a direct function of the water level - a higher level results in a higher pressure on the bottom , and a lower level results in a lower pressure on the bottom . thus , no matter how you rearrange items supported by buoyancy on ( or in ) the lake , the water level will stay the same . an object that sinks touches the floor of the lake , so the prior argument no longer applies . the force balance dictates that the weight of the lake is less ( as we consider the sunk object to no longer be a part of the hydrostatic body , since it is supported by a separate and unconnected normal force ) , and thus the water level will be less .
i think all of the existing answers miss the real difference between energy and momentum in an inelastic collision . we know energy is always conserved and momentum is always conserved so how is it that there can be a difference in an inelastic collision ? it comes down to the fact that momentum is a vector and energy is a scalar . imagine for a moment there is a " low energy " ball traveling to the right . the individual molecules in that ball all have some energy and momentum associated with them : the momentum of this ball is the sum of the momentum vectors of each molecule in the ball . the net sum is a momentum pointing to the right . you can see the molecules in the ball are all relatively low energy because they have a short tail . now after a " simplified single ball " inelastic collision here is the same ball : as you can see , each molecule now has a different momentum and energy but the sum of all of all of their momentums is still the same value to the right . even if the individual moment of every molecule in the ball is increased in the collision , the net sum of all of their momentum vectors does not have to increase . because energy is not a vector , increasing the kinetic energy of molecules increases the total energy of the system . this is why you can convert kinetic energy of the whole ball to other forms of energy ( like heat ) but you can not convert the net momentum of the ball to anything else .
suppose the angle of the cone is $\theta$ , the radius of the ring is $r$ and the length density of the ring is $\rho$ . take a small segment of the ring that corresponding to an angle $\phi$ , then the mass of this segment would be $$m=r\phi \rho$$ the normal force from the cone would be $$n=mg\mathrm{sin} ( \theta/2 ) $$ if the tension on the ring is $t$ , then we have $$2t\mathrm{sin} ( \phi /2 ) =n\mathrm{cos} ( \theta/2 ) $$ and consequently we have $$2t\mathrm{sin} ( \phi /2 ) =r\phi \rho \mathrm{cot} ( \theta/2 ) $$ now we take $\phi$ to be very small , which means $\mathrm{sin} ( \phi /2 ) \approx \phi /2$ , then we have $$t=r\rho \mathrm{cot} ( \theta/2 ) $$ which only depends on the size and density of the ring and the angle of the cone , just as expected .
this question is somewhat of a physics history question about when and why . i dont have a specific reference as to who first introduced symplectic geometry into physics , but there are some broader indications as to what has happened . the symplectic manifold idea is a formalisation ( in terms of differential geometry ) of the phase space idea . looking up phase space in wikipedia gives this useful reference and explanation : in mathematics and physics , a phase space , introduced by willard gibbs in 1901 , is a space in which all possible states of a system are represented , with each possible state of the system corresponding to one unique point in the phase space . for mechanical systems , the phase space usually consists of all possible values of position and momentum variables . a second factor that would have been relevant in this case was liouville 's theorem . this theorem asserts that the density of points in phase space is conserved . it is about volumes in phase space , thus suggesting a differential geometry treatment of phase space . once it became clear ( i dont know whether this was gibbs or later ) that the symplectic form exists as a geometric object similar in some ways to a metric , and closely connected with the poisson bracket and the rest of classical mechanics , then it would be natural to describe the whole of classical mechanics in this geometric way . this could be another topic that was first noticed by mathematicians , but only noticed by physicists after the advent of quantum mechanics ( 1920s or 1930s ) when explicit connections between the two theories ( classical and quantum ) were searched for . names associated with this are weyl and moyal ( in 1949 ) . more on these early phase-space to qm ideas in this wikipedia link . it also needs to be remembered as we discuss post general relativity and post quantum mechanics eras , that phase space formulations of gr have been sought as an aid to quantization . all this is a highly geometric approach to quantum gravity , associated with many names although ashtekar worked on this overlap . eventually this led to his ( co- ) formulation of loop quantum gravity .
your procedure gives : $$ a_{xz} = \sqrt{a_x^2 + a_z^2} $$ then : $$ a_{total} = \sqrt{a_y^2 + a_{xz}^2} $$ but if you substitute for $a_{xz}$ in the second equation you get : $$ a_{total} = \sqrt{a_y^2 + ( \sqrt{a_x^2 + a_z^2} ) ^2} = \sqrt{a_y^2 + a_x^2 + a_z^2}$$ so you do not need to split the calculation into two steps . your accelerometer may already exclude the acceleration due to gravity . if it does not then yes you need to use the inclination to work out the three components of gravity then subtract them from $a_x$ , $a_y$ and $a_z$ . it is hard to say exactly how to do this without knowing how your phone reports it is inclination . response to comment : suppose you have your device held flat so $a_z$ = -1 . now move the device downwards at and angle of $\theta$ as shown below : assuming it is moving in the $xz$ plane the value of $a_z$ will be decreased a bit and the value of $a_x$ will increase from zero . suppose you are applying an acceleration to the phone of $2g\space cos ( \theta ) $ - you will see why i have chosen this value in a moment . now the values of $a_x$ and $a_z$ are : $$ a_x = 2g\space cos\theta \space sin\theta $$ $$ a_z = g - 2g \space cos^2 \theta $$ you now calculate $a_{total}$ by just squaring and adding as we discussed above to get : $$a_{total}^2 = 4g^2 \space sin^2\theta \space cos^2\theta + g^2 + 4g^2 \space cos^4\theta - 4g^2 \space cos^2\theta $$ and a bit of rearrangement gives : $$a_{total}^2 = g^2 + 4g^2 \space cos^2\theta \left ( sin^2\theta + cos^2\theta - 1\right ) $$ and because $sin^2\theta + cos^2\theta = 1$ the quantity in the brackets is zero so you end up with : $$a_{total}^2 = g^2 $$ that is : $$a_{total} = g $$ which is the same as when the phone is stationary . so it is possible to be accelerating the phone and still have the total acceleration come out as $g$ ( $g$ = -1 in the phone 's units ) . that is why just subtracting one is not a reliable way to tell if the phone is accelerating .
let 's take a simple original picture to look at - just two nearby dots on a white background . if you have bad vision , the dots look blurred . the way good vision works is to ensure that all the light hitting any particular small area of your retina comes from the same direction in front of you . conversely , all the light coming from one direction hits one specific spot on your retina . when you have bad vision , the light from a locus of nearby directions all hits on the same part of your retina , and the light from a particular direction is smeared out over an area on your retina . hence , blurred vision is an averaging effect . when you look at the dots , you will see them smear out into each other . you might try to compensate for this by making a " counter-blurred " image where the source dots are smaller , but if the original dots are close enough that light from the center of one dot is spilling over to overlap light from the center of the second dot , making the dots smaller will not fix that problem . hence , the dots will always appear blurred . you can not create the impression that the original has for someone with good vision . a photograph is really just a bunch of nearby dots , and so the same problem applies . i do not know about the 3d monitor , though . i suppose if it can control the direction of light coming off it , it could be modified to focus the light some and create a sharp image for someone with blurred vision .
let me start from your comment on lubos ' answer : if we have a electron near a coil of wire that has current running through it , certainly the electron will move a certain direction right ? no , it is not that simple . for a given coil of wire producing a given magnetic field , the electron can experience a force in any direction that is perpendicular to the field . it depends on which way the electron is moving . ( the force is always perpendicular to both the field and the electron 's velocity . ) in fact , if the electron is just sitting at rest , or is moving parallel to the magnetic field , it experiences no force at all . you might be confused because you are thinking of the electrostatic force . that one is always parallel to the electric field ; it does not matter how the particle is moving , and that is why you can draw electrostatic force lines . but that does not work with the magnetic force .
first of all , the standard model does not treat bosonic fields as classical . they are quantum mechanical i.e. non-classical , they are just not anticommuting or grassmann-odd . second , a consistent theory just requires the relationship between spin and statistics , see e.g. the http://en.wikipedia.org/wiki/spin-statistics_theorem combining integer spin with fermi statistics leads to ghost or energy or the norm that is not positively definite , and vice versa ( half-integer spin with bose statistics ) . it was proved by pauli . however , your very question did not actually talk about the integer vs half-integer spin at all . it was talking about the relationship between fermions and anticommuting fields . this is almost a tautology . a fermion is a particle whose wave function for many particles is antisymmetric , $\psi ( x_1 , x_2 ) =-\psi ( x_2 , x_1 ) $ etc . , so the fields that create these particles must be anticommuting , $a^\dagger ( x_1 ) a^\dagger ( x_2 ) =-a^\dagger ( x_2 ) a^\dagger ( x_1 ) $ . the multiparticle state in qft is written as $$ |\text{2 fermions}\rangle = \int d^3 x_1 d^3 x_2\ , \psi ( x_1 , x_2 ) a^\dagger ( x_1 ) a^\dagger ( x_2 ) |0\rangle $$ because the wave function $\psi$ is antisymmetric , only the antisymmetric combination $a^\dagger ( x_1 ) a^\dagger ( x_2 ) - a^\dagger ( x_2 ) a^\dagger ( x_1 ) $ contributes to the state , and in fact , only this combination is nonzero . the sum  the anticommutator  vanishes . that is why the antisymmetry of $\psi$ is " automatic": if there were a non-antisymmetric part of $\psi$ , it would vanish in the integral above because the product of the creation operators is antisymmetric . the same for bosons and " commuting " , without the minus sign . the answer to your " why " question is that your statement is really a tautology , pretty much a definition of bosons and fermions , up to the possibly confusing comments about " one antisymmetry " implying the " other antisymmetry " above . of course , you could also ask why one uses commuting or anti-commuting fields to describe particles at all . well , nature just works in this way . quantum fields naturally reduce to multi-body quantum mechanics with the automatic symmetry or antisymmetry  and they may give rise to automatically lorentz-invariant theories , too ( something that would be hard in the " non-relativistically styled " multiparticle quantum mechanics ) .
it should not be too hard with a van de graaff generator . assuming a generator of radius to the order of a decimeter , we need to generate a potential of $\frac{q}{r} \tilde~ \frac{10^9}{0.1}\tilde~10^{10} v$ to get a charge of one coulomb . that would be rather hard , though if we want a microcoulomb , that can be arranged with a ( van de graaff ) generator capable of producing voltages in megavolts . once we have this charg on the generator , we can transfer it to the electrode via conduction ( which will only transfer a fraction of it ) , or induction ( which will induce an opposite and equal charge on the electrode ) so yes , charges in nanocoulomb/microcoulomb/millicoulomb are not that hard to generate and collect . 1 coulomb of charge -- not so much . note that it is not too hard to have 1c of net charge in some given volume--the earth has some net charge which is probably in coulombs . the issue comes when you have to concentrate it enough to be able to transfer it . i somehow forgot about capacitors . capacitors can store a large amount of charge ( till one kilocoulomb ) though the net charge stored is zero . however , it is generally hard to transfer this high charge elsewhere without neutralizing it or pushing it into another capacitor .
good question ! as you guessed , nothing can escape from a black hole , so it is impossible to see one directly . ( quantum field theory does predict that black holes give off an extremely tiny amount of thermal radiation , but it is so little that it we can not detect it from earth . ) scientists assume that black holes exist based mainly on the predictions of general relativity . in particular , general relativity tells us that if an amount of mass $m$ is contained in a spherical volume of radius $r_s = \frac{2gm}{c^2}$ , space will be warped so drastically that all possible paths within that sphere lead inward toward the central point . the surface of that sphere is the event horizon , the boundary of the black hole . now , you might wonder how we can be so certain that general relativity works for such strong gravitational fields and thus that event horizons exist . well , we can not directly confirm this , but gr does work for everything else we have tested it against , so there is really no reason to doubt the prediction of event horizons . having established that event horizons ( and , thus , black holes ) are allowed by the theory , how do we go about actually detecting one ? the original method of detecting a black hole is by looking for very intense x-ray and gamma ray emissions . these come not from the black hole itself , but from the accretion disc , the dust and gas particles that have become trapped in the black hole 's gravity well and are circling it in preparation to fall in . when the particles get very close to the event horizon , they bump into each other very energetically , and those collisions emit high-energy radiation which we can detect . obviously this only occurs if there is enough gas and dust in the vicinity of the black hole to form an accretion disc . it is possible for other very dense objects to have accretion discs , but based on the properties of the radiation , we can tell how quickly the particles are moving , and thus put some limits on the size and mass of the object they are orbiting . if its radius is less than $r_s$ for its mass , then we assume it is a black hole . more recently , similar observations have been made for stars orbiting the centers of our galaxy and other galaxies . by observing the positions of the stars over time , we can analyze their orbits to determine the characteristics ( size and mass ) of what they are orbiting . if the size is smaller than $r_s$ , then again , general relativity tells us the object is a black hole .
cheng and li 's appendix gives the generic symmetry factor $s^{-1}$ with $$s=g\prod_{n\geq 2}2^{\beta} ( n ! ) ^{\alpha_n} , $$ where $\alpha_n$ are the number of pairs of vertices connected by $n$ identical self-conjugate lines , $\beta$ is the number of lines connecting a vertex with itself , and $g$ is the number of permutations of vertices that leave the diagram unchanged with fixed external lines . for your diagram , as long as the number of vertices $n&gt ; 2$ , all of the $\alpha_n=0$ ( i suppose $\alpha_1=n$ , but this does not affect the symmetry factor ) . you also have no tadpoles , so $\beta=0$ . finally , $g=1$ since you can not permute the vertices without changing the connectivity of the external lines . so the symmetry factor of the diagram is just one . that is not to say that there are not many ( $ ( n-1 ) ! $ in fact ) other diagrams with the same kinematic structure that might need to be included in a final calculation of scattering amplitudes , just with permuted vertices .
i am sure this is a duplicate question , but i have searched and have not found an exact duplicate . anyhow , an analogy commonly used is to imagine the electricity flowing in a circuit as water flowing in a water pipe . in this analogy the electrons are the water and the voltage is the water pressure . the battery in the circuit would be represented as a pump . if you suddenly increase the pressure of the water e.g. by turning the pump on ( equivalent to connecting the battery ) then the pressure change will flow round the circuit fast . in fact it would move at the speed of sound in water , which is about 1.5 km/sec , even though the water would be moving much more slowly than this . going back to the electric circuit , the signals travelling round the circuit are changes in voltage and move at around the speed of light just as the pressure does in a water circuit . however the electrons would be moving far slower than this .
the sign is purely based on convention . usually it is just written as $ f_{n} = -mg $ which does point up , but only because we take $g$ to be negative already . the fact that it is based on convention should make things easier for you , since you can chose which representation that you are comfortable with . typically though , if you are not dealing with vectors in vector notation , you do just want to go with the lengths . questions that do not present info to that depth do not merit answers to the same depth . that said , your book leaves out the negative sign because it is indeed only dealing with magnitudes . enjoy !
the metal plates the magnets are glued to are an iron and nickel alloy that has a very high magnetic permeability called a mu-metal . i do not understand all of the details of magnetism or how mu-metal works but that should get you started .
normal matter structure is entirely constructed from the electronic bindings , so it is in the realm of the possible to engineer how the atoms are binded together exactly and this is the aim of lower-level nanotechnology . however , it is with current technogy and physics , impossible to create complex structures at lower scales ( i.e. : nuclear scale ) . and i do not think that is something that is in principle possible unless some dramatic breakthrough occurs in how we obtain larger-scale nuclear matter , which from the experimental limitations that there are to obtain high z nucleus that might probe the regions of higher islands of stability , one can safely infer that this is a very challenging problem in of itself update : this is not entirely related , but it shows an example of how assumptions as the one i have made above about manipulation of small scales being out of engineering reach can be twisted : this slide about non-homogeneous diffraction crystals shows how to do something that most physicists have thought for long to be essentially impossible ; x-ray and gamma-ray optics
you blow away the flame from its fuel source . if you would blow less hard the flame might burn harder because more air is supplied to the flame ( similar to a bunsen burner ) . because normally the flame of a candle gets its oxygen through a convectional airflow generated by the heat of the flame . the reason why the flame is blown away from the candle is because the air you blow towards it moves faster than the speed of the flame front . so the air you blow at it moves the flame away from its fuel source , where the flame burns out due to the lack of fuel .
the sound speed $c_s$ of any fluid in which the fluid pressure $p$ is a function of the fluid density $\rho$ is going to be given by $c_s^2 = \frac{dp}{d\rho}$ . if that expression is not already familiar to you , it is a good exercise to write down the fluid equations for conservation of mass and momentum in one dimension and then write down what happens when the density and velocity are perturbed from their equilibrium , static values . you will get the wave equation for the perturbed values with the wave speed given by the above equation . if $\rho$ can be treated as a function of $a$ , then applying the chain rule gives $c_s = \sqrt{\frac{dp}{da} \ , \frac{da}{d\rho}}$ . if $a$ is proportional to $\rho$ with the same proportionality constant everywhere , you get the expression you have asked about . even if the relationship between $a$ and $\rho$ is not exactly that simple , you had expect to get the same answer to order of magnitude since $a$ and $\rho$ should change by values comparable to themselves . edit : it turns out that you can get the same expression for the sound speed if $a$ is inversely proportional to $\rho$ . in that case both $\frac{dp}{da}$ and $\frac{da}{d\rho}$ are negative , which might be useful to keep in mind . and an inverse relationship is exactly what you had expect if the flow is steady or nearly so ( $\rho$ and $u$ at each position do not vary with time ) . in that case conservation of mass implies that $\rho u a$ is a constant at each position . and if $u$ is not varying much with position , then you indeed get $a \propto 1/\rho$ .
the magnetic field lines are in the same direction as the upwards velocity of the electrons , so the v  b term in the lorentz force is 0 . magnetic field lines are curved , so they cannot be everywhere in the same direction . magnetic field in the metallic wire is not everywhere parallel to the velocity of the wire and produces electromotive intensity $\mathbf v\times \mathbf b$ circulating in the horizontal plane that is non-zero both below and above the center of the magnet . near the center of the magnet , the induced intensity is low due to fact you mention , i.e. velocity is parallel to magnetic field .
yes , everything generates a gravitational field , whether it is massive or massless like a photon . the source of the gravitational field is an object called the stress-energy tensor . this is normally written as a 4 x 4 symmetric matrix , and the top left entry is the energy density . note that mass does not appear at all . we convert mass to energy by multiplying it by $c^2$ ( as in einstein 's famous equation $e = mc^2$ ) and then put in the energy . so even a photon generates a gravitational field because although it has no mass it does have energy . it is surprising what else is in the stress-energy tensor and can therefore generate a gravitational field . for example pressure and shear stress appear . it is even been suggested that a gravitational field could be generated by gravty itself i.e. the energy of the gravitational field generates the curvature that creates the field . the resulting object is called a geon , though i should emphasise that no-one has proved these could exist and most of us think they probably can not .
javier , try looking at what is an intuitive picture of the motion of nucleons ? to start . the short answer is that it is as reasonable to say that they are identical as it is to say that the configuration of electrons in multiple atoms of a single element are identical . that is , there is a set of position ( or momentum 1 ) distributions to which they conform . 1 the position and momentum distributions turn out to be linked to each other by fourier transformations , so information required to specify one is the same as that required to specify the other . nuclear physicist mostly concern themselves with the momentum distributions .
the condition for double beta decay to be the preferred mode are that the energy ( mass ) of the three states the initial nucleus ( m ) the would-be single-beta-decay daughter plus the electron and the anti-neutrino ( s ) the double-beta-decay daughter plus the two electrons and the two anti-neutrinos ( d ) have a relationship like $$ e_d &lt ; e_m &lt ; e_s \ , . $$ the result is that it is not possible for a single decay to proceed ( there is not enough energy ) but it is possible for a double decay to happen ( though it will be a slow process ) .
this diagram is my attempt to show the situation first when the rock is in the boat and secondly when you have chucked the rock over the side . the mass of the boat of $m$ and the mass of the rock is $m$ . the density of water is $\rho_w$ and the density of the rock is $\rho_r$ . in the first case archimedes ' principle tells us that the volume of water displaced is : $$ v_{disp1} = \frac{m + m}{\rho_w} $$ in the second case the volume of water displaced is : $$ v_{disp2} = \frac{m}{\rho_w} + \frac{m}{\rho_r} $$ where the second term is just the volume of the rock . if we take the difference of these two we get : $$ v_{disp1} - v_{disp2} = \frac{m}{\rho_w} - \frac{m}{\rho_r} $$ i think it is safe to assume that $\rho_r$ > $\rho_w$ , i.e. the rock sinks in water , and in that case $ v_{disp1} - v_{disp2}$ is positive i.e. more water is displaced when the rock is in the boat , so the water level falls when you chuck the rock overboard .
if you want to make a small very-hot spot , spatial coherence is important . contrary to what you say , the sun has quite high spatial coherence ( not as high as a laser , but higher than most other bright light sources ) . that is why you can focus sunlight very well . if you focus sunlight perfectly , you get a spot the shape of the sun . that spot would have the same light intensity as if you were standing on the surface of the sun , looking down . if you perfectly focus the light from an incandescent light bulb , you can get a spot shaped like the tungsten filament . that spot would have the same light intensity as if you were a tiny person standing on the surface of the tungsten filament , looking down . ( this intensity is much lower than the the sun 's ) . an extreme example of low spatial coherence is the blue light from the blue sky . you cannot use a lens to focus that light into a bright blue spot on the ground . try it ! this blue light has almost no spatial coherence , which means you cannot focus it . there is plenty of blue light coming at you , but it already has as much intensity as it is capable of having . this is quantified by the law of conservation of etendue . light starts out with a certain radiant intensity , and then it can never be increased , no matter what kind of lenses or mirrors you use . lasers can have far higher radiant intensity than any other light source . it is not just how many watts they emit , it is how they emit it -- with high spatial coherence , which means it can be focused very effectively . lasers come in all shapes and sizes , and intensities , and wavelengths , and form-factors , and prices . red diode lasers cost a few cents each . other lasers cost $100,000 or more . lasers are basically a generic way to create coherent ( and therefore high-radiant-intensity ) light , in many different systems . so if you need a high-radiant-intensity light source , the best one for the job is quite likely to be a laser .
the best non-technical explanation i have seen of this is matt strassler 's blog article , though even this is still fairly technical , so let me see if i can interpret it a bit . the key point is that an electron is not a particle . in quantum field theory it is described as an excitation in the electron quantum field . this excitation will propagate in spacetime , and from the way it propagates e.g. the propagation velocity and how much momentum it carries , we can calculate it is mass . and in fact if the electron field has no interaction with the higgs field the propagation speed would be $c$ so the electron would be massless . the higgs field is another quantum field , and the higgs field and electron field interact . that means you cannot just write an electron just as an excitation of the electron field , but instead it has to be written as an excitation of both the electron and higgs fields together . because the interaction is relatively small we can write the excitation as a slightly perturbed electron field excitation , that is we write it as an excitation of the electron field plus a bit of the higgs field . if we now calculate how this excitation propagates we find it travels at less than the speed of light i.e. the excitation of the combined fields has a mass . the amount of mass is proportional to the strength of the interaction between the electron and higgs fields . the idea of an electron bouncing off higgs bosons is more easily visualised , but it is a very crude analogy , and like all analogies when pushed too far it ceases to be helpful . that is why you are ( quite reasonably ! ) confused about the time dilation the electrons would suffer when moving at light speed in between collisions . it is not correct to think of electrons as intermittently bouncing off higgs bosons . the electron field and higgs field mix so the electrons cease to be pure electrons . this applies to all fermions , i.e. electrons , muons , taus and quarks ( and i think neutrinos though i am not sure about this ) . the vector bosons like the w and z particles get their mass through a different interaction . photons and gluinos remain massless .
it is possible to define kilogram , but right now the accuracy would be worse than for the prototype . and yes , the mass of the prototype is changing a bit , so efforts are being made to introduce a definition of kilogram .
it has to do with wavelength . a tube is of the correct size for sound waves , not for light waves that have very much smaller wavelengths . an optical fiber does bend light .
suppose you throw a ball upwards at some velocity $v$ . when you catch it again it is traveling downwards at ( ignoring air resistance ) a velocity of $-v$ . so somewhere in between throwing and catching the ball it must have been stationary for a moment i.e. it is instantaneous velocity was zero . obviously this was at the top of its travel . when you throw the ball it immediately starts being accelerated downwards by the earth 's gravity , so it has a constant acceleration downwards of $-9.81ms^{-2}$ ( the acceleration is negative because it is reducing the velocity of the ball ) . so this is an example of how there can be a non-zero acceleration ( of $-9.81ms^{-2}$ ) but there can be a moment when the ball 's instantaneous velocity is zero .
even a physical quantity which changes by discrete amounts can often be well approximated by a continuous function of time . the derivative is a property of a mathematical function . any differentiable function must necessarily be continuous , and a continuous function will change by arbitrarily small values for an arbitrarily small change in inputs . the fact that one can calculate the derivative of a function does not imply that the physical quantity that is approximated by that function can also be changed by arbitrarily small amounts .
two obvious desirable features of this definition are : when you put two systems next to each other , considering them as one system , the total number of possible microstates $\omega_t$ is equal to the product of $\omega$s of the two systems , $\omega_t=\omega_1\times \omega_2$ . but for this system the entropy is the sum of the entropies , indicating the necessity of an exponential definition . the $\ln$ function has the property that the entropy of a system with one microstate $ ( \omega=1 ) $ is zero , which is desirable . this relation can be obtained form the assumption of equal a priori probabilities , i.e. that the equilibrium corresponds to the macrostate with the maximum number of microstates : consider two isolated systems , separately in equilibrium , each with macrostates $e_i^{ ( 0 ) } , v_i , n_i$ ( energy , volume , number of particles ) . each of them has a total number of $\omega_i ( n_i , v_i , e_i^{ ( 0 ) } ) $ possible microstates . now we bring them into thermal contact so that they can exchange energy . after this point , we will have $e_t=e_1'+e_2'=\text{constant}$ . $n$ and $v$ for each system remain unchanged . the total number of possible microstates for each system would be $\omega_i ( n_i , v_i , e_i' ) $ and for the composite system : $$\omega=\omega_1 ( n_1 , v_1 , e_1' ) \times \omega_2 ( n_2 , v_2 , e_2' ) =\omega_1 ( e_1' ) \omega_2 ( e_2' ) =$$ $$\omega ( e_t , e_1 ) $$ with the assumption of equilibrium occurs at the point of having maximum $\omega$ , we find the value of $e_1^*$ ( and hence $e_2^*$ ) that maximizes $\omega ( e_t , e_1 ) $: $$d\omega=0\to\left ( \frac{\partial\omega_1 ( e_1 ) }{\partial e_1}\right ) _{e_1=e_1^*} \omega_2 ( e_2^* ) +\omega_1 ( e_1^* ) \left ( \frac{\partial\omega_2 ( e_2 ) }{\partial e_2}\right ) _{e_2=e_2^*}\frac{\partial e_2}{\partial e_1}=0\tag{1}$$ $$\frac{\partial e_2}{\partial e_1}=-1\to$$ $$\beta_1=\left ( \frac{\partial \ln \omega_1 ( e_1 ) }{\partial e_1}\right ) _{e_1=e_1^*}=\left ( \frac{\partial \ln \omega_2 ( e_2 ) }{\partial e_2}\right ) _{e_2=e_2^*}=\beta_2\tag{2}$$ naturally we expect these quantities $\beta_1$ and $\beta_2$ to be related to temperatures of the systems . from thermodynamics we know that $$\left ( \frac{\partial s}{\partial e}\right ) _{n , v}=\frac{1}{t}\tag{3}$$ comparing $ ( 2 ) $ and $ ( 3 ) $ , we can conclude that : $$\frac{\partial s}{\partial ( \ln \omega ) }=k$$ or $$\delta s=k\ln \omega$$ where $k$ is a constant .
there are two meanings usually attached to the word " quantum " in quantum theory , one colloquial and one technical . as you know , electromagnetic radiation behaves in ways characteristic of both waves and particles . for non-specialists , it is easy to think of a particle as being a " unit " of the wave , and since " quantum " means a unit of something , the word has gotten associated with " particle . " but in reality , the idea of a particle is not precisely defined . when people talk about a particle of light , the em field associated with what they probably mean could be described as a wave packet , which you can think of as an electromagnetic wave that is localized to some small region in space . for example , something like this : this is just an example , of course ; wave packets can have all sorts of shapes . the more precise , technical meaning of " quantum " has to do with fourier decomposition . as you may know , any function can be decomposed into a sum of sine waves ( or complex exponentials ) , $$f ( x ) \propto \int e^{ikx}\tilde f ( k ) \mathrm{d}k$$ for any given momentum $k$ , the amplitude $\tilde f ( k ) $ represents the contribution of the sine wave with that frequency to the overall wave . now , classically the value of $ [ \tilde f ( k ) ] ^2$ at each $k$ represents a bona fide contribution to the energy of the light . but the assumption that makes quantum theory quantum is that $ [ \tilde f ( k ) ] ^2$ instead represents the probability that there is a contribution to the energy of the light coming from that frequency . the actual contribution that can come from any given frequency can only be one of a set of specific values , which are integer multiples of some unit $\hbar c/k$ . " quantum " is the word for that unit of energy .
you should know that $\partial_\mu$ is shorthand for $\frac{\partial}{\partial x^\mu}$ where $x^\mu= ( t , x , y , z ) $ . furthermore , it can be seen from your question that the metric convention that is used is $+ - - -$ , so that $x_\mu= ( t , -x , -y , -z ) $ . then , one can easily write out your equation . the einstein summation convention is used . \begin{align} ( \partial_\mu s+ea_\mu ) ^2 and =m^2\\ ( \partial_\mu s+ea_\mu ) ( \partial^\mu s+ea^\mu ) and =m^2\\ \partial_\mu s\partial^\mu s+e^2a_\mu a^\mu+2e\partial_\mu s a^\mu and =m^2 \end{align} your final equation follows pretty much trivially , once you set $a_0=-\frac{\alpha}{er}$ and $a_i=0$ .
like all good physicists we will start by assuming the animal is spherical . actually the calculation i am going to describe is basically the same whatever shape the animal is , but choosing a sphere means i can write down some simple formulae . if the radius of the spherical animal is $r$ , then the total area of its skin is the surface area of the sphere : $$ a = 4 \pi r^2 $$ and the volume and mass are : $$ v = \frac{4}{3}\pi r^3 $$ $$ m = \rho v = \rho\frac{4}{3}\pi r^3 $$ where $\rho$ is the density of the animal . under most circumstances the dominant mode of cooling will be convection , and the rate of heat loss per unit area of skin is given by newton 's law of cooling : $$ \frac{dq}{dt} = k a \delta t \tag{1}$$ where $\delta t$ is the difference between the animal 's temperature and the temperature of the air around it , and $k$ is a constant ( technically known as a fudge factor ) that depends on the details of how the wind is blowing around the animal . now , the animal will have an ( average ) specific heat , $c$ , and this tells us how much the temperature of the animal falls when it loses a quantity of heat $\delta q$: $$ \delta t = \frac{\delta q}{c m} $$ the rate of cooling of the animal is $dt/dt$ , and we get this by dividing both sides of the above equation by $\delta t$ and letting the changes $\delta$ become small so they turn into differentials : $$ \frac{dt}{dt} = \frac{1}{cm} \frac{dq}{dt} $$ and we already have an expression for $dq/dt$ from our equation ( 1 ) above . if we substitute this we get an expression for the rate of cooling : $$ \frac{dt}{dt} = \frac{1}{cm} k a \delta t $$ the last step is to replace the mass $m$ and the skin area $a$ with the equations from above , and do some rearranging to tidy up the expression , and we get : $$ \frac{dt}{dt} = \frac{3k\delta t}{c} \frac{1}{r} $$ and since $k$ , $\delta t$ and $c$ are all constants we get : $$ \frac{dt}{dt} \propto \frac{1}{r} $$ so the rate of cooling is inversely proportional to the radius of the animal . big animals cool slowly and small animals cool fast . although i started by assuming the animal is a sphere , the result really only depends on the ratio of surface area to volume , and for all shapes this scales inversely with size . so the result applies to any shape of animal though the constants in the equation will change with shape . also note that i have simplified the calculation by assuming the thermal conductivity of the animal is high enough that its internal temperature is everywhere constant . in practice the animal 's skin will get cold while its core is still warm . still , the calculation does give you a basic idea of why larger animals cool more slowly than smaller animals .
in theory , you could get almost $100\%$ efficiency from a solar cell exposed to light with the photon energy just above the band gap . each absorbed photon generates an electron of almost the same energy . the problem is that there are only so many band gaps available , so you have to find a light source at the correct wavelength to match one . the intensity of the light does not change the efficiency , just how much power you can get out of the cell . over a wide range , if the intensity doubles , you can generate twice as much power . as an aside , the energy of a photon is measured in energy units : ev , ergs , joules . watts are power , or energy/time .
the proper derivation of the centripetal accelerationwithout assuming any kinematic variables are constantrequires a solid understanding of both the stationary cartesian unit vectors $\hat{i}$ and $\hat{j}$ as well as the rotating polar unit vectors $\hat{e}_r$ and $\hat{e}_\theta$ . the cartesian unit vectors $\hat{i}$ and $\hat{j}$ are stationary and always aligned with the x and y axes respectively , while the polar unit vectors $\hat{e}_r$ and $\hat{e}_\theta$ rotate with an angular velocity of $\omega=\|\dot{\theta}\|$ and point in the directions of increasing radius and angle ( respectively ) . the included graphic below shows the two basis vector pairs overlaid on top of one another . the position vector of the object is obviously defined as : $\vec{p} ( t ) =x\hat{i}+y\hat{j}=rcos ( \theta ) \hat{i}+rsin ( \theta ) \hat{j}$ , with $\|\vec{p} ( t ) \|=\sqrt{ ( rcos{\theta} ) ^2+ ( rsin{\theta} ) ^2}=\sqrt{r^2 ( sin^2 ( \theta ) +cos^2 ( \theta ) ) }=r\sqrt{ ( 1 ) }=r$ less obviously , it can be shown that the polar unit vectors $\hat{e}_r$ and $\hat{e}_\theta$ can be expressed solely in terms of the cartesian unit vectors $\hat{i}$ and $\hat{j}$ and the angular position $\theta$ as , $\boxed{\hat{e}_r=cos ( \theta ) \hat{i}+sin ( \theta ) \hat{j}}$ and $\boxed{\hat{e}_\theta=-sin ( \theta ) \hat{i}+cos ( \theta ) \hat{j}}$ . these two equations are extremely important , as they will be the key to expressing the cartesian acceleration in polar coordinates , of which one of the terms will be our desired $v^2/r=\omega^2r$ centripetal acceleration . moving forward , the vector acceleration of the object in cartesian coordinates is simply $\vec{a} ( t ) =\frac{d^2}{dt^2}\left [ \vec{p} ( t ) \right ] =\ddot{x}\hat{i}+\ddot{y}\hat{j}$ . starting with $x=rcos ( \theta ) $ and $y=rsin ( \theta ) $ and differentiating once , we have $\boxed{\dot{x}=\dot{r}cos ( \theta ) -r\dot{\theta}sin ( \theta ) }$ and $\boxed{\dot{y}=\dot{r}sin ( \theta ) +r\dot{\theta}cos ( \theta ) }$ . differentiating again , we will have $\ddot{x}=\ddot{r}cos ( \theta ) -\dot{r}\dot{\theta}sin ( \theta ) -\dot{r}\dot{\theta}sin ( \theta ) -r\frac{d}{dt}\left [ \dot{\theta}sin ( \theta ) \right ] $ $=\ddot{r}cos ( \theta ) -2\dot{r}\dot{\theta}sin ( \theta ) -r\left [ \ddot{\theta}sin ( \theta ) +{\dot{\theta}}^2cos ( \theta ) \right ] $ , such that $\boxed{\ddot{x}= ( \ddot{r}-r\dot{\theta}^2 ) cos ( \theta ) + ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) ( -sin ( \theta ) ) }$ . similarly , the y acceleration $\ddot{y}$ becomes $\ddot{y}=\ddot{r}sin ( \theta ) +\dot{r}\dot{\theta}cos ( \theta ) +\dot{r}\dot{\theta}cos ( \theta ) +r\frac{d}{dt}\left [ \dot{\theta}cos ( \theta ) \right ] $ $=\ddot{r}sin ( \theta ) +2\dot{r}\dot{\theta}cos ( \theta ) +r\left [ \ddot{\theta}cos ( \theta ) -{\dot{\theta}}^2sin ( \theta ) \right ] $ , such that $\boxed{\ddot{y}= ( \ddot{r}-r\dot{\theta}^2 ) sin ( \theta ) + ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) cos ( \theta ) }$ . now , we must plug these scalar derivatives into our formulation for the vector acceleration . in cartesian coordinates , this is $\vec{a} ( t ) =\ddot{x}\hat{i}+\ddot{y}\hat{j}=\{ ( \ddot{r}-r\dot{\theta}^2 ) cos ( \theta ) + ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) ( -sin ( \theta ) ) \}\hat{i}+\{ ( \ddot{r}-r\dot{\theta}^2 ) sin ( \theta ) + ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) ( cos ( \theta ) ) \}\hat{j}$ which can be rearranged into the following form : $\vec{a} ( t ) = ( \ddot{r}-r\dot{\theta}^2 ) \{cos ( \theta ) \hat{i}+sin ( \theta ) \hat{j}\}+ ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) \{-sin ( \theta ) \hat{i}+cos ( \theta ) \hat{j}\}$ but as we have already seen , this is simply equal to $\boxed{\boxed{\vec{a} ( t ) = ( \ddot{r}-r\dot{\theta}^2 ) \hat{e}_r+ ( r\ddot{\theta}+2\dot{r}\dot{\theta} ) \hat{e}_\theta}}$ as we can now appreciate from carrying out the full derivation , there are actually two components each to both the radial and tangential accelerations . the $\ddot{r}$ term is straightforwardly equal to the second derivative of the position vector magnitude . the second term , $r\dot{\theta}^2$ , is our long sought-after centripetal acceleration $r\dot{\theta}^2=\omega^2r=v^2/r$ , and ( as expected ) it points in the negative radial direction . the tangential terms are perhaps a bit less intuitive . the $r\ddot{\theta}$ term is the acceleration that occurs whenever the radius and angular acceleration $\ddot{\theta}$ are both non-zero ( imagine the tangential acceleration of a turbine blade of a jet engine as the engine spools up ) . the final term $2\dot{r}\dot{\theta}$ is what is commonly known as the coriolis acceleration , and it occurs whenever the radius and angle change simultaneously . it arises because , for a given angular velocity , the arc length travelled every second increases with radius ( tangential velocity increases with radius ) . thus , an object with a given angular velocity will have different tangential velocities at different local radii of rotation . if the radius changes with time ( $\dot{r}\not=0$ ) and the angular velocity $\dot{\theta}$ is not equal to zero , then the tangential velocity will change with time , which is by definition a tangential acceleration .
the su ( n ) representation refers to the space of the unitary matrices with determinant 1 . by using the exponential form we can write an arbitrary su ( n ) representation as $$ u = e^{it_{n}a_{n}} , $$ where $t_{n}$ are the generators of the su ( n ) group , $a_{n}$ are continuous parameters . let 's get the relations for them . $$\tag{1} u^{\dagger} = 1 \rightarrow t_{n}^{\dagger} = t_{n} . $$ $$\tag{2} det u = 1 \rightarrow tr ( t_{n} ) = 0 . $$ by $ ( 1 ) $ and $ ( 2 ) $ we can reduce the number of independent parameters ( or the generators ) by which we can represent the arbitrary su ( n ) matrix . the starting number is $2n^{2}$ ( matrix rank $n\rightarrow n^{2}$ parameter , the complexity of matrix - multiplication of $n^{2}$ by two ) . $ ( 1 ) $ reduce this number to $$ 2n^{2} - n - ( n^{2} - n ) = n^{2} $$ ( here we have the first "-" summand as the result of the bounds for diagonal component and the second summand as the result for nondiagonal components ) . $ ( 2 ) $ reduce the number to $n^{2} - 1$ . let 's have the example : the su ( 3 ) group . let 's use approximate representation : $u \approx e + it_{n}a_{n} = e + a$ ( it is more convenient for finding the generators ) . $ ( 1 ) $ gives the zero imaginary part for each diagonal component and the connection $$ a_{ij} = a_{ij} + ib_{ij} , \quad a_{ji} = a_{ij} - ib_{ij} . $$ the $ ( 2 ) $ fixes $\sum_{i} a_{ii} = 0$ . so one of possible parametrization of the $u$-matrix is $$ u = e + i\begin{pmatrix} a_{3} + a_{8} and a_{1} - ia_{2} and a_{4} - ia_{5} \\ a_{1} + ia_{2} and a_{8} - a_{3} and a_{6} - ia_{7} \\ a_{4} + ia_{5} and a_{6} + ia_{7} and -2a_{8} \end{pmatrix} . $$ by the expanding the second matrix to a sum $t_{n}a_{n}$ you will get gell-mann matrices .
dark energy is an unknown or unattributed form of energy that is separate and distinct from the other forms of energy . it is not anti-engery . it is dark energy . anti-energy ( were such a thing to exist ) would annihilate any form of energy . dark energy is called " dark " because we are not exactly sure what it really is or what causes it . the most abundant forms of energy in the universe right now as a percentage of all the energy are ( approximately ) : dark energy at 72.8% dark matter at 22.7% baryons at 4.5% radiation at around 0.009% there is a margin of error on these numbers but anything that does not add up to 100% we attribute to the curvature of the universe ( it too has an effective amount of energy but is not actually energy per se ) . additionally , these numbers refer to the energy density ratios for the observable universe . however , as the universe is assumed to be homogeneous in the $\lambda$-cdm model , these ratios should apply to the entire universe as well .
let 's back up for a second . before going into the complexities of non linear waves , let 's ask what a linear wave is . actually , let 's go even further back and ask " what do we mean when we say linear ? " " linear " comes from the study of things like vector spaces . we have objects ( call them vectors , or arrows , or whatever ) that can be both added together and scaled by a number , with the result being another object of the same type . any collection of objects that satisfies certain conditions ( which basically boil down to " addition and scalar multiplication behave as expected" ) can be considered a vector space . now let 's talk about waves . but to keep things simple , let 's just talk about the effect of some waves at a single point , where the effect can change in time . one wave might have a value $\psi_1 ( t ) = \sin ( \omega_1 t ) $ at this point . another might have a different frequency : $\psi_2 ( t ) = \sin ( \omega_2 t ) $ . suppose we scale the waves by factors of $a$ and $b$ , and suppose we have them both affect the point together . if the waves ' effects just scale and add in the sensible way , then the value of the combined wave at the point will be $$ \psi_{ ( a\otimes1 ) \oplus ( b\otimes2 ) } ( t ) \equiv a \sin ( \omega_1 t ) + b \sin ( \omega_2 t ) = a \psi_1 ( t ) + b \psi_2 ( t ) . $$ here i am using the symbol "$\otimes$" to mean " physically scaled by the preceding factor " and "$\oplus$" to mean " combined physically . " in this particular case , $\otimes$ and $\oplus$ reduced to sensible scalar multiplication of the the wave value and regular addition of the values of two waves . we call these " linear " waves . one of their characteristics is that you can think of the waves as noninteracting $\psi_2$ will add its effect to the total in the same way , regardless of how much amplitude $\psi_1$ has already contributed . but i did not have to have that structure . in some cases , driving a physical displacement with twice the force does not result in twice the displacement , and having two different driving forces work together does not result in a force that gives a displacement that is is the sum of the independent displacements . for instance , perhaps the rule is $$ \psi_{ ( a\otimes1 ) \oplus ( b\otimes2 ) } ( t ) \equiv \sqrt{a \sin ( \omega_1 t ) + b \sin ( \omega_2 t ) } \neq a \psi_1 ( t ) + b \psi_2 ( t ) . $$ this then would be a nonlinear wave . they are defined by having the definition of how disturbances scale ( $\otimes$ ) and combine ( $\oplus$ ) be incompatible with scalar multiplication and regular addition of the waves ' values . that is , our physical definitions of $\otimes$ and $\oplus$ did not yield the structure of a vector space - at least not in any obvious way . the physics question remaining then is whether or not this situation is ever actually realized . the above discussion defines nonlinear waves , but it does not prove any such things exist . as it turns out , though , many waves important to physics show nonlinear behavior if you push them far enough . the classic example in optics is when the amplitude of an electromagnetic wave is so great that electrons in nearby atoms ( thinking classically here ) are pushed and pulled quite far from the " sweet spot " distance they want to have from their nuclei . then the restoring force that pushes them back to that sweet spot is not simply directly proportional to their displacement , their motion is anharmonic , and the wave becomes nonlinear .
the water gets colder the longer you run it ( in the uk at least ) because the water mains pipes buried in the ground are colder than the ones in your house , so sadly this is not evidence for any fundamental physical effect . in principle any fluid flowing in a pipe gets hotter because energy is dissipated in viscous flow . you could in principle calculate the energy dissipated using the pressure drop per length of pipe , which is described by the darcy-weisbach equation , but this would be a somewhat involved calculation for real pipes/taps and in any case it is not relevant to the core of your question . when you relate velocity to temperature you are presumably thinking of the maxwell-boltzmann distribution for the temperature dependance of the velocity profile in gases . the trouble is this distribution is arrived at by considering redistribution of energy between gas molecules due to collisions between them . if you simply add a constant velocity to every gas molecule you are not making any difference to the way the gas molecules collide with each other , because it is only their relative velocities that matter . although water is a liquid not a gas the same argument applies . it is the velocities of the water molecule relative to each other that determine the temperature . so just adding a constant velocity to every water molecule makes no difference .
there is a more general statement : all 4d lorentz invariant field theories flow to cfts in the uv and the ir . a proof was given last year by luty , polchinski , and ratazzi in http://arxiv.org/pdf/1204.5221.pdf . their argument has some assumptions but they are fairly weak .
you can callculate the standart deviation as show in this link http://en.wikipedia.org/wiki/standard_deviation#generalizing_from_two_numbers the standart deviation $\sigma=\sqrt{\frac{\sum_{i=1}^n a_i^2}{n}-\left ( \frac{\sum_{i=1}^n a_i}{n}\right ) ^2}$ , where $a_i$ is the $i$-th number in your set and $n$ is the number of numbers you have in your set ( in your example $a_1=32.5$ , $a_2=32.0$ , $a_3=32.3$ so $n=3$ ) using the numbers from your question i got $\sigma \approx 0.20548$
first of all write down an explicit expression for the summation over all microstates . edit since you are treating the system classically this includes an integral over phase-space and a summation over all possible spin-configurations . $$ \sum_{\mu_s} = \sum_{\{s_i\}}\int\frac{d^np d^nq}{h^{3n}n ! }$$ the second thing is to realize that your hamiltonian is non-interacting and the canonical density $e^{-\beta h} $ is just a product of one-particle hamiltonians $$ e^{-\beta h} =\prod_{i=1}^ne^{-\beta h_i} $$ where of course $$ h_i = {\frac{ ( p_i + \gamma s_i ) ^2}{2m} -b s_i} $$ ( i renamed $\beta$ to $\gamma$ , because you do not mean the inverse temp . here ) so you have to evaluate $$ \frac{1}{n ! }\sum_{\{s_i\}}\prod_{i=1}^n\int\frac{dp_i dq_i}{h^{3}} e^{-\beta h_i} = $$ because $h$ ist non-interacting , the n-particle phase-space-integral factorizes into n integrations over a 1-particle phase-space . similarily one may interchange the spin-summation and the product ( convince yourself that this is true ! e . g that one ends up with the same terms , whether you sum over spin first or not . ) that means , instead of summing over all the many-body microstates , one first sums over the possible configurations of a single particle and accounts for the fact , that there are many afterwards . additionally all $h_i$ are equivalent . they each just carry a different but redundant index : $$ z = \frac{1}{n ! }\prod_{i=1}^n\sum_{s_i=\pm1}\int\frac{dp_i dq_i}{h^{3}} e^{-\beta h_i} = \frac{1}{n ! }\left ( \sum_{s=\pm1}\int\frac{dp dq}{h^{3}} e^{-\beta h}\right ) ^n$$ where $h$ is $h_i$ but without an index , because the reference is not to a specific particle anymore . /edit you will have to think about , what to do with the momentum integration . i have not calculated the result , but it might be you will not end up with a solution in closed form . there might be an approximation needed to do the momentum summation . edit i think a gaussian integration will do the trick . /edit let us know what you end up with !
the magnetic field is a " pseudovector " ( more properly , a 2-form ) , as opposed to the electric field , which is a vector ( or a 1-form ) . that is , under parity , $\mathbf b$ is left unchanged . you can see this from the lorentz force , $$\mathbf f = q ( \mathbf e + \mathbf v\times \mathbf b ) $$ where since force is a vector , $\mathbf e$ must also be a vector . since $\mathbf v \times \mathbf b$ is a product , if $\mathbf b$ were a vector , this term would not transform properly under parity . thus $\mathbf b$ does not change when we perform a parity transformation . however i think the more correct way to see this is from the relativistic formulation of electrodynamics . introduce the electromagnetic field tensor $$f_{\mu\nu} =\begin{pmatrix} 0 and e_x and e_y and e_z \\ -e_x and 0 and -b_z and b_y \\ -e_y and b_z and 0 and -b_x \\ -e_z and -b_y and b_x and 0 \end{pmatrix}$$ and note that the purely spatial part $f_{ij}$ , $1 \le i , j \le 3$ is equivalent to the magnetic field . since there are two indices , the components are invariant under parity transformations . the electric field is given by $e_i = f_{0i}$ , so it changes sign under parity , it is a vector . the more sophisticated yet way to see this decomposition is that if there is timelike 1-form $dt$ we can decompose the field strength 2-form as $$f = e\wedge dt + b . $$ we see that $e$ is a 1-form ( equivalent to a vector after raising the index ) , but $b$ is a 2-form ( often called pseudovector , because not enough people know about the wonders of differential forms ) . now your transformation is not quite $p : ( x , y , z ) \mapsto ( -x , -y , -z ) . $ it is $rp : ( x , y , z ) \mapsto ( -x , y , z ) $ , or $p$ , then a rotation by $\pi$ in the $yz$-plane . here the $x$-axis is along $\mathbf v$ and the $z$-axis along $\mathbf b$ . since $\mathbf v$ is perpendicular to the plane of the rotation , it is just affected by the reflection , and is to the left . $\mathbf b$ , lying in the plane of rotation , is rotated half a revolution , and is now out of the page , so the force is upward . the force being a vector , this is precisely what we expect . i hope this answers your question .
a few quick references before you close the question : there is a rather technical discussion on what is special about four dimensions on the math overflow ( way over my head ! ) . the article i was thinking of is actually on wikipedia . this picture from the article : succinctly explains why our space is 3+1 dimensional .
warning : this is a long and boring derivation . if you are interested only in the result skip to the very last sentence . noether 's theorem can be formulated in many ways . for the purposes of your question we can comfortably use the special relativistic lagrangian formulation of a scalar field . so , suppose we are given an action $$s [ \phi ] = \int {\mathcal l} ( \phi ( x ) , \partial \phi_{\mu} ( x ) , \dots ) {\rm d}^4x . $$ now suppose the action is invariant under some infinitesimal transformation $m : x^{\mu} \mapsto x^{\mu} + \delta x^{\mu} = x^{\mu} + \epsilon a^{\mu}$ ( we will not consider any explicit transformation of the fields themselves ) . then we get a conserved current $$j^{\mu} = {\partial {\mathcal l} \over \partial \phi_{\mu}} \phi^{\nu} a_{\nu} - {\mathcal l} a^{\mu} = \left ( {\partial {\mathcal l} \over \partial \phi_{\mu}} \phi^{\nu} - {\mathcal l} g^{\mu \nu} \right ) a_{\nu} . $$ we obtain a conserved charge from it by letting $q \equiv \int j^0 {\rm d}^4x$ since from $\partial_{\mu}j^{\mu} =0$ we have that $$ {\partial q \over \partial t} = \int {\rm div}{\mathbf j}\ , {\rm d}^3 x = 0$$ which holds any time the currents decay sufficiently quickly . if the transformation is given by translation $m_{\nu} \leftrightarrow \delta x^{\mu} = \epsilon \delta^{\mu}_{\nu}$ we get four conserved currents $$j^{\mu \nu} = {\partial {\mathcal l} \over \partial \phi_{\mu}} \phi^{\nu} - {\mathcal l} g^{\mu \nu} . $$ this object is more commonly known as stress energy tensor $t^{\mu \nu}$ and the associated conserved currents are known as momenta $p^{\nu}$ . also , in general the conserved current is simply given by $j^{\mu} = t^{\mu \nu} a_{\nu}$ . for a lorentz transformation we have $$m_{\sigma \tau} \leftrightarrow \delta x^{\mu} = \epsilon \left ( g^{\mu \sigma} x^{\tau} - g^{\mu \tau} x^{\sigma} \right ) $$ ( notice that this is antisymmetric and so there are just 6 independent parameters of the transformation ) and so the conserved currents are the angular momentum currents $$m^{\sigma \tau \mu} = x^{\tau}t^{\mu \sigma} - x^{\sigma}t^{\mu \tau} . $$ finally , we obtain the conserved angular momentum as $$m^{\sigma \tau} = \int \left ( x^{\tau}t^{0 \sigma} - x^{\sigma}t^{0 \tau} \right ) {\rm d}^4 x . $$ note that for particles we can proceed a little further since their associated momenta and angular momenta are not given by an integral . therefore we have simply that $p^{\mu} = t^{\mu 0}$ and $m^{\mu \nu} = x^{\mu} p^{\nu} - x^{\nu} p^{\mu}$ . the rotation part of this ( written in the form of the usual pseudovector ) is $${\mathbf l}_i = {1 \over 2}\epsilon_{ijk} m^{jk} = ( {\mathbf x} \times {\mathbf p} ) _i$$ while for the boost part we get $$m^{0 i} = \left ( t {\mathbf p} - {\mathbf x} e \right ) ^i $$ which is nothing else than the center of mass at $t=0$ ( we are free to choose $t$ since the quantity is conserved ) multiplied by $\gamma$ since we have the relations $e = \gamma m$ , ${\mathbf p} = \gamma m {\mathbf v}$ . note the similarity to the ${\mathbf e}$ , $\mathbf b$ decomposition of the electromagnetic field tensor $f^{\mu \nu}$ .
it can be justified using distribution theory .
for the lower atmosphere , where most of the air is , the temp , pressure , and density of air is given by : $t=t_0-lh$ $p=p_0\left ( \frac{t}{t_0}\right ) ^{\frac{gm}{rl}}$ $\rho = \frac{pm}{rt} $ using the following constants : sea level standard atmospheric pressure p0 = 101325 pa sea level standard temperature t0 = 288.15 k earth-surface gravitational acceleration g = 9.80665 m/s2 . temperature lapse rate l = 0.0065 k/m universal gas constant r = 8.31447 j/ ( molk ) molar mass of dry air m = 0.0289644 kg/mol the force due to air resistance can be written : $f = -\rho v^2 c_d a$ where $c_d$ is the coefficient of drag , $v$ is the velocity , and $a$ is the surface area of the projectile . the goal is to get out of the atmosphere ( where force of gravity is roughly constante ) with the earth 's escape velocity , $11.2$ km/s . for a bullet-shaped 1-kg projectile of steel , $c_d \approx 0.04$ and $a \approx 4\times 10^{-4}$ m$^2$ . this leads to a initial velocity of $13.5$ km/s . while not much higher than the vaccuum value , it is still high enough that the bullet would probably vaporize in the atmosphere . interestingly , if one used a sphere instead of a bullet , then $c_d=0.4$ , $a=4\times 10^{-3}$ m$^2$ , the air resistance is 100 times higher , and thus a much greater velocity is needed . but since the drag scales like $v^2$ , this leads to much higher drag . so much so , that even if one could launch the ball at the speed of light ( newtonians only , please ! ) , it still could not make it out of the atmosphere !
the lift basically depends on the the velocity of the plane and the the density of the air around it . that means , in higher altitude , a plane needs a higher velocity to maintain its height , but : the velocity is not just for staying up , but obviously is a desired feature of an airplane , thus , they would not speed down even it was possible while maintaining height . on the other hand , the higher the pressure and thus the density of the surrounding air , the higher the drag . as we already find out that velocity is desired , the drag results in more energy needed to maintain the velocity . thus , it is more efficient to fly at higher altitudes .
i am not sure it makes sense to ask if nature is " imitating " quantum mechanics . quantum mechanics is a mathematical model that gives predictions that are in excellent , well so far perfect , agreement with what we actually see . i guess the question is whether qm is just a good approximation to the real world or whether it is an exact description of the real world . we will never be able to prove it is an exact description , but someday someone may find an experiment where qm gives the wrong answers . if so this would prove it is just an excellent approximation .
this happens because the gravitational mass of an object is equal to the inertial mass of an object . because of this , the acceleration of an object in a gravitational field does not depend on the mass of the object . for example , if you double an object 's mass , the force required to accelerate it by the same amount as before doubles , but the gravitational force pulling it down also doubles . the effect of any mass changes cancel out .
you have actually solved the problem yourself already ! the method you described , solving for the acceleration and plugging it into the formula for displacement , work just fine .
the integral expression is lorentz-covariant , too , and it may be made manifestly lorentz-covariant , too . the integral measure $\int d^3 r ' / ||\vec r - \vec r'||$ is equal to and may be rewritten as the four-dimensional integral with a delta-function ( and step function ) added : $$2\int {d^4 x'} \cdot \delta [ ( x-x' ) ^2 ] \cdot \theta ( t-t' ) $$ it is understood that $j$ is substituted at the point $j ( x' ) $ . note that the step function $\theta$ ( equal to one for positive arguments and zero otherwise ) is lorentz-covariant assuming that the points $x , x'$ are not spacelike-separated ( because the ordering of a cause and its effect is frame-independent ) , and they are not spacelike-separated as guaranteed by the delta-function that is only non-vanishing near/at the null separation of $x , x'$ the step function guarantees that the cause precedes its effect . the argument of the delta-function is a lorentz invariant , $ ( x-x' ) ^2$ which means $ ( x-x' ) ^\mu ( x-x' ) _\mu$ . the sign convention for the metric does not matter becase this invariant is the argument of an even function ( delta-function ) . finally , the equivalence of the two integrals may be shown by performing the integral over $t'$ . the theta-function implies that we only integrate over the semi-infinite line $t'\lt t$ . the delta-function implies that the integral is only sensitive on the value of $j ( t' ) $ where $c|t-t'| = |r-r'|$ where the delta-function vanishes . finally , the delta-function also automatically generates the $1/|\vec r - \vec r'|$ factor because $$\delta ( y^2 ) = \delta ( y_0^2-|\vec y|^2 ) = \delta [ ( y_0+|\vec y| ) ( y_0-|\vec y| ) ] =\dots $$ which is equal , because $\delta ( kx ) = \delta ( x ) / |k|$ , to $$\dots = \frac{\delta ( y_0-|\vec y| ) }{y_0+|\vec y|}=\frac{\delta ( y_0-|\vec y| ) }{2|\vec y'|}$$ you see that the factor of two was needed , too .
i assume you are interested in using light sent from earth . you could use a laser which does not spread out so much as it travels and so less of it would be wasted . if you used some bulb radiating in one direction , you would use the inverse square law to get the power at a distance $x$ , the distance to jupiter . then multiply the power per unit area at that distance by the area of the circle formed by jupiter 's radius to get the reflected power . then use the square law again in the same way to find the power returned to earth . the power you will need will depend on the sensitivity of the photodiode , used to measure the light . the laser is better because the power per unit area goes down way slower with distance .
none , really . such junctions form in semiconductor crystals . those are remarkable materials . let 's look at electrons in solids first . many atoms have weakly bound valence electrons in outer orbits . these orbits would have specific energies if the atom existed in isolation . but when man atoms are packed together and their outer orbits overlap , the energy of these orbits shift slightly . electrons in those orbits now can have a band of possible energies . now those weakly bound electrons are good for carrying currents , but to hop through the material the overlapping orbits better not be completely filled . there would not be room for the moving electron . in metals , there is in fact a band which is about half filled . ideal - there is plenty of room for electrons to move , but also still enough electrons to move . semiconductors have a full and an empty band close together , and with some external help ( doping , electric fields , etc ) this can be used to switch from conducting to non-conducting . regardless , as the outer orbits overlap and join to form a band , the electrons in that band no longer belong to a single orbit and therefore a single atom . at the quantum level , the probability function of the electron is smeared out over the crystal . and thus it is not sensible to talk about the exact atoms ionized .
i think you are asking : if i turn on a lightbulb , i can imagine a sphere of light spreading radially outward from the bulb at the speed of light . how fast is the diameter of the sphere increasing in the lightbulb 's frame ? the answer is 2*c .
when you see the history of the universe plotted against time , the time used is the comoving time i.e. the time measured by a clock that is at rest with respect to the universe around it . this is the time co-ordinate used in the flrw metric , which is a solution to the equations of gr that , as far as we can tell , gives a good description of the universe back to very early times . earlier than around the planck time after the big bang we expect the notion of time to become imprecise because it is not possible to measure times shorter than the planck time . without a working theory of quantum gravity it is not possible to comment further . however for all times later than the planck time we expect time to be a good co-ordinate and be well behaved . this allows physicists to calculate at what time the various stages in the evolution of the universe happened . response to comment let me attempt to phrase my answer more broadly . the first point is that in relativity ( both special and general ) you need to be careful talking about time . for example you have probably heard that time runs more slowly when you move at speeds near the speed of light . however there is a well defined standard time that cosmologists use for describing the history of the universe . we call this comoving time . so when you hear statements like " the universe is 13.7 billion years old " we mean it is 13.7 billion years old in comoving time . you do not need to know how comoving time is defined , just that it gives us a good timescale for describing the history of the universe back to the planck time . which brings us to . . . you have probably also heard of heisenberg 's uncertainty principle . again i will gloss over the details , but one side effect of the uncertainty principle is that it is impossible to measure times less than about 5 $\times$ 10$^{-44}$ seconds . i do not know of a simple way to explain this to someone who is not familiar with quantum mechanics , so i am afraid you will have to take this on faith . and this brings us back to hawking 's programme . as long as the times we are interested in are greater than 5 $\times$ 10$^{-44}$ seconds we can define the time using comoving time so we can assign reliable times to cosmological events like the electroweak transition . but for times close to 5 $\times$ 10$^{-44}$ seconds the whole notion of a " time when something happens " becomes meaningless because it is fundamentally impossible to measure times that short . i would guess this is what hawking means when he says time ceases to exist .
yes , it was . see also physics.stackexchange.com/q/87239/2451 hilbert 's approach is not really the same as physicists searching for the theory of everything . physicists are concerned about truth , but not so much about the logical relations between one truth and another . admittedly , a theory of everything would try to derive all physical laws from a very few ones , but hilbert was explicitly interested in more than just deriving true laws from a few axioms . ( for one thing , definitions are just as important as proofs . now , physicists are often good enough at proofs , but rarely good at logical definitions . nor should discovery be made to wait until the logic is all clear . statistical mechanics made great strides even though it suffered from logical contradictions and meaningless statements . they were all cleared up later . hilbert was interested in false physical theories as well as in true ones , for the sake of the light they might shed on the logic of true physical theories . he did something parallel to this in his researches on geometry : he would investigate a non-archimedean geometry , which is obviously false , in order to show that the archimedean axiom was necessary , could not be proved from the other axioms . he asked that the necessity of physical axioms be inveestigated in this way . hilbert influenced the axiomatisations of quantum mechanics by von neumann , and although he had no influence on dirac 's axiomatisation , the two axiomatisations are really very similar except in some details . but these axioms for quantum mechanics are unsatisfactory from a hilbertian viewpoint . wigner understood this viewpoint from his time in hilbert 's circle , and explained what was unsatisfactory : the same physical situation , the interaction of a particle with a measurement apparatus , can be analysed using the wave-function unitary time evolution axioms of qm , and you get one result : the future state of the particle + apparatus system is deterministically determined , but they are entangled . but if you analyse the same physical situation with the other axioms , the axioms about observables and the reduction of the wave packet , you get a probabilistic answer and one in which the measurement apparatus is treated as if it were classical , in a definite state , and not entangled . and the axioms do not tell you how these two results are related . there is no definition of measurement , event , or probability . no definition of " macroscopic state of the measurement apparatus " . j.s. bell also analysed these logical defects . in 1900 hilbert did not yet know about qm , but he was already asking for clarification of the logical status of probability in the relation between the micro-description of a gas via the classical mechanics of the individual molecules , and the macro-description of the gas as a whole in terms of temperature , the probability distribution of velocities of the molecules given by the maxwell distribution which is parametrised by the temperature . he asked for a clarification of when micro-laws determine macro-behaviour , and how sensitive the macro-behaviour is to the exact form of the micro-laws . he also wanted a converse investigation : from the macro-behaviour , how much could you infer about the micro-laws ? the basic problems for classical statistical mechanics were all cleared up , logically , by darwin and fowler in the 1920 's . khintchine then popularised their logical framework in his book , mathematical foundations of statistical mechanics , moscow , 1943 . it turns out that this is highly relevant to quantum measurement , as well . what one means physically by " macro-state " and " probability " has to be rigorously defined , and then the actual physics of the measurement process has to be analysed to see how it generates these results . much progress has been made , see the cited link above . it is not clear whether advances in our understanding of the logical nature and logical defnitions of " physical probability " , " measurement " , " macroscopic " and so on will lead to interesting physics . but hilbert did not think physicists would be either interested in or capable of investigating this hilbert problem , hilbert 's sixth problem . but mathematicians who have no physical intuition or do not know how physics is done are not competent to contribute , either . hilbert himself always hired a physics postdoc to explain to him the latest results , and his seminar often invited physicists to give talks ( einstein , for example ) .
suppose you pick two people at random . from one , you pluck a single hair from their head . is it possible to tell who had the hair plucked by weighing the people ? technically , plucking a hair makes a person very slightly lighter , so you get a tiny bit of information about who had the hair plucked by weighing the people . but the information is very slight because the effect is so small that for practical purposes it may be ignored . similarly , heavier objects are mathematically predicted to collide with earth very slightly faster because of their gravitational effect on the earth . but the effect is so preposterously small as to be meaningless , so it commonly ignored . the answers you linked do not disagree with each other . one is not right and the other wrong . they simply need to be interpreted in their own context . people do not try to account for every little possible influence , like whether a hair was plucked from someone 's head or whether they have trimmed their fingernails recently when talking about a human 's weight . similarly , they do not always try to account for every tiny phenomenon and make every statement perfectly precise when they talk about physics .
what exactly is meant by lorentz invariance ? a physical quantity that is unchanged by a lorentz transformation , i.e. , a coordinate system ( or frame ) independent quantity that is independent of the spacetime coordinates . another term used is lorentz scalar . often lorentz invariant quantities are prefixed with the word proper : proper time , proper distance , proper acceleration . sometimes , they are prefixed with the word invariant , e.g. , invariant mass . this is in contrast with lorentz covariance . an equation is said to lorentz covariant if the equation holds in all inertial reference frames , i.e. , if the equation is valid in one inertial frame , the lorentz transform of the equation is valid . other lorentz covariant objects are four-vectors ( and their duals ) and higher order four-tensors . update to address a comment : " a quantity that is independent of the spacetime coordinates . " that is not right . $\mathbf e \cdot \mathbf b$ is dependent on the spacetime coordinates , yet it is lorentz invariant . no , $\mathbf e \cdot \mathbf b$ is not coordinate dependent though it can vary from event to event in spacetime . i would like to address this comment so there will not be any confusion of the phrase " independent of spacetime coordinates " . to say that a quantity is independent of spacetime coordinates is not to say that it is independent of spacetime . consider a scalar field $\phi$ on spacetime . $\phi$ is a rule that assigns a number $\phi ( \mathcal p ) $ to each event $\mathcal p $ in spacetime . clearly , the number $\phi ( \mathcal p ) $ is coordinate independent ; $\phi ( \mathcal p ) $ does not depend on the coordinates we choose to assign to the event $\mathcal p$ however , coordinate independence does not mean that $\phi ( \mathcal a ) = \phi ( \mathcal b ) $ for two distinct events $\mathcal a$ and $\mathcal b$ . thus , while the numbers may change from event to event , no choice of coordinates can change the number associated with a particular event .
the job of the friction is to enforce the no slip constraint . so let us find the relative acceleration of the two parts and find the force $f$ which makes it zero . the cylinder eom are ( positive is to the left ) $ f = m \dot{v} $ and $r f = i \dot{\omega}$ and the tangential cylinder acceleration at p is $\dot{v}_p = \dot{v} + r \dot{\omega}$ to make the tangential acceleration equal to the rug you have $$ a = \dot{v}_p = \frac{f}{m} + \frac{r^2 f}{i} \\ f = \left ( \frac{1}{m} + \frac{r^2}{i}\right ) ^{-1} a $$ where the part in the parenthesis is the effective mass of a rolling cylinder on its surface . for a solid cylinder the mass moment of inertia is $i=\frac{m}{2} r^2$ and thus $$ f = \left ( \frac{1}{m} + \frac{2}{m}\right ) ^{-1} a = \frac{m}{3} a$$
consider the system in equilibrium , with the rod hanging straight down . imagine taking a marker and drawing a vertical diameter across the disk . if the rod were fixed to the disk 's center so that the disk could not rotate , then would this marker line still be vertical mid swing ? what does this tell you about the rotation of the disk around its central axis relative to its starting position ? if the disk were not rigidly attached as in the former case , would there now be any torque on the disk around its central axis ? what would the marker line look like mid swing in this case ?
besides textbooks such as peskin and schroeder , comphep has some of the functionality you are looking for as well , although i have never used it for this purpose . from the overview page : " the symbolic part of comphep has the following possibilities : [ . . . ] calculate analytical expressions corresponding to squared diagrams by using the fast built-in symbolic calculator ; " to test calculations against actual experimental data , most modern particle physics experiments today will submit their results to hepdata . you might try a search for [ re e+ e- --> e+ e- and obs sig ] one of the results it : " measurement of hadron and lepton-pair production in e+ e- collisions at s** ( 1/2 ) = 192-gev to 208-gev at lep "
regarding the meaning of " universe": unqualified , the universe is all there is , was , and will be . in other words , there is nothing that is outside of , independent of , stands apart from , the universe . the universe is commonly defined as the totality of existence however , it is often the case that " universe " is short for " observable universe ": the observable universe consists of the galaxies and other matter that can , in principle , be observed from earth in the present day because light ( or other signals ) from those objects has had time to reach the earth since the beginning of the cosmological expansion . . . . both popular and professional research articles in cosmology often use the term " universe " to mean " observable universe " . this can be justified on the grounds that we can never know anything by direct experimentation about any part of the universe that is causally disconnected from us , although many credible theories require a total universe much larger than the observable universe . regarding the big bang , you ask : so if the universe was initially a point then what was outside that point ? this is a common question and the answer takes some getting used to . the metric expansion of space does not require that there be something ' outside ' within which space is expanding . regardless of the overall shape of the universe , the question of what the universe is expanding into is one which does not require an answer according to the theories which describe the expansion ; the way we define space in our universe in no way requires additional exterior space into which it can expand since an expansion of an infinite expanse can happen without changing the infinite extent of the expanse . all that is certain is that the manifold of space in which we live simply has the property that the distances between objects are getting larger as time goes on . this only implies the simple observational consequences associated with the metric expansion explored below . no " outside " or embedding in hyperspace is required for an expansion to occur . the visualizations often seen of the universe growing as a bubble into nothingness are misleading in that respect . there is no reason to believe there is anything " outside " of the expanding universe into which the universe expands .
in statistical mechanics and thermodynamics you are describing systems with an extremely large number of possible variables or degrees of freedom , so describing exactly what happens becomes impossible . instead , you describe the average . to do this , you consider all physically possible configurations of your system , and say they are all equally probable . you use this ensemble of hypothetical systems to determine the average values of physical properties of your system . in nearly every case , the average value is dominated by the configurations " near " to your most probable configuration , and fringe cases ( like all the atoms in your room being in a cubic cm volume at the same time ) are impossibly rare . however , if you have arrangements of your system that are close enough to the most probable , they have a reasonable chance of occurring . so , for example , energy can transfer from a hot system to a cold system just from pure random chance of the collision between molecules in each system . this random movement around the most probable configuration of your system is " noise , " and you can apply similar logic to any system described in a statistical manner .
it is a matter of opinion , largely dependent on what you mean by settled : is information lost or is evolution unitary ? there are arguments based on ads/cft which make it almost certain that the evolution of a system from pre-collapse matter to after collapse radiation is unitary . this is because there is a dual description - an equivalent description of the system using different variables - in which unitary evolution is automatic . if this is true for black holes in ads , it is hard to believe it is somehow different for black holes in flat space which are for all intent and purposes as close to their ads cousins as we wish them to be . based on that , most people i know are convinced what the right answer is . how is the information preserved , what is the mechanism for the unitary evolution ? in particular , what is wrong with hawking 's original arguments to support information loss ? i think it is fair to say that we do not know the answer , at least we do not know how to phrase the answer in the original , gravitational , language . i think there is a lot to learn by making this answer , even if we do not dispute it , as explicit as possible . as a subset of that , were the arguments given by hawking , based on maldacena 's superficially similar arguments , convincing ? lots of people are skeptical , including myself . there are some technical and conceptual assumptions that do not seem quite right in that proposed solution .
electric conduction : at atmospheric pressure , air and other gases are poor conductors ( insulators ) of electricity . ' cause they do not have any free electrons to carry current . but , once free electrons are produced in gas by ionization ( they become plasmas ) , discharge of electricity through gases appears . this could be done in many ways such as applying large potential difference across a gas column at very low pressure or by allowing high frequency em-waves such as the x-rays through the gas . this question is not proper to ask . indeed , gases become plasmas once they are ionized . . ! how ? all dielectrics have a certain value of breakdown potential - a potential which provides sufficient energy to break some covalent bonds and produce free electrons . these free electrons are accelerated by the applied electric field and they collide and ionize other atoms to produce more free electrons ( thus , they multiply by collision ) . this process is called avalanche breakdown . as there are large number of free electrons available for current flow examples : discharge tubes , lightning , etc .
i will assume that both lasers are of the same type , i.e. that both are generated from the same physical mechanism . all lasers have some tuning range over which their wavelengths can vary . so , even though both are very monochromatic ( single frequency ) they will , in general , not have the exact same frequency . a hene laser , for example , has a tuning range of $1\ \text{pm}$ or $\sim1\ \text{ghz}$ . which frequency actually lases depends on many things , but it can be tuned by changing the temperature of the laser or by adjusting the resonator length inside of the cavity . so , what happens when you overlap two different lasers is that you will get an interference pattern between the two beams . the interference pattern will oscillate between bright and dark at the difference frequency of the two lasers ( the beat note ) . since your eye can only see frequencies up to $\sim30\ \text{hz}$ you will not generally be able to see the interference pattern . if you use a lens to focus the two beams onto a fast photodiode , you can see this beat note on an oscilloscope or spectrum analyzer though . if your lasers are adjustable , then you can use this beat note between the two lasers to phase lock them to each other . doing so requires some knowledge of electronics and control theory , but it will lock the frequency of the two lasers together so that as one laser drifts the other follows identically . if you phase lock the two lasers , then you will be able to see the interference pattern with your eye .
if the man pulls a length $l$ of rope through his hands , both he and the counterweight will rise by $l/2$ . the rope is assumed to be massless , and hence the force of tension is uniform throughout it at all times . as the man does work to pull the rope through his hands , he increases the magnitude of the tension force in the rope to greater than his weight . since tension is uniform and the counterweight has the same mass , both he and the counterweight accelerate upwards at the same rate .
i disagree with the premise of your question that energy is a " simply mathematical object " . indeed , the fact that it can have density and flux is one the reasons why it ought to be considered a real , tangible thing instead of just a bookkeeping device . while energy in on all its forms is a property of something else , being a property does not reduce a physical thing to being simply a mathematical object . as dmckee said , the idea is that when you have a field ( any field , not just the em field ) you can meaningfully discuss the energy at a particular point . if a field can store energy , then it makes sense that different sized " chunks " of the field will store different amounts of energy . think , for example , about electromagnetic waves that can deliver power to a receiver . a big box that contains many wave periods will obviously have more energy inside it than a small box that contains very few wave periods , since the wave delivers a certain amount of energy per period . there is nothing special about waves vs . other field configurations , so the same reasoning applies to any form of the electromagnetic , or any other , field . well , if it is meaningful to talk about the total field energy contained in some box then its meaningful to compute the average density of energy by dividing out the box volume . if i take the limit of smaller and smaller boxes around a point , this average density approaches the density at that point . we can go through the same sort of reasoning with momentum density ( which is closely related to the energy flux ) . the fact that , as you observed , energy appears to flow like a fluid is because conservation of energy is actually a much stronger statement than you are imagining . you are used to talking about energy conservation in global terms : the change in the energy of a system is equal to the work done on the system by its environment minus the work done on the environment by the system . in field terms , this means that if you draw any sized box , then the change in the total energy in the box equals the total flux going through the sides of the box . by making the box arbitrarily small , we get ( using the divergence theorem ) a local statement of conservation of energy : if $\rho ( \vec{x} , t ) $ is energy density and $j ( \vec{x} , t ) $ is the energy flux then $\frac{\partial\rho ( \vec{x} , t ) }{\partial t} + \nabla \cdot j ( \vec{x} , t ) = 0$ . this is a very powerful statement and is more fundamental than the global statement . for instance , in general relativity local conservation of energy still holds but , because of the complications introduced by curvature , the global statement fails .
2012-07-11 addendum based on excellent inputs , in particular from @annav , my answer is now " no , even a direct worst-case hit by the oh-my-god particle would not kill you , even by radiation , because there is insufficient distance and angle to generate a fatal radiation cone . thanks all , and be sure to look at the earlier answer that @dmckee pointed out . ** original answer** ( my answer seems to differ from the earlier ones that @dmckee aptly pointed out , so i will go ahead and risk posting it . my main difference is that i suspect that a head on collision with a large nucleus could produce a wide enough horizontal-splatter radiation cone to produce a fatal event . ) since the 1991 oh-my-god particle was most likely a proton and had the kinetic energy of a fast baseball , i am going out on a limb and saying yes , you could be killed by a single particle . this harvard physics site suggests an approximate energy transfer of about 0.2% in transfers with heavy nuclei , which as i discuss below may be enough to do you in with that kind of particle . but it would be the ensuing radiation event and cone that would do you in , not the kinetic energy of the particle . the main issue is that your head does not have anything in it remotely solid enough to stop or even slow down a particle with that much momentum . so , like a locomotive passing through a cloud of fog , it is going to zip through pretty much as if your head is not there . the question , then , is to ask not what the fog will do to the locomotive , but what the locomotive will do to the fog -- that fog being your head . even a single solid , exactly head-on collision with a nice fat iron nucleus just as an ultra cosmic ray proton enters your head would probably not be a pretty event in terms of the resulting secondary radiation shower . i am guessing ( nothing more , i have not tried to calculate anything ) that outward splattering of a nice little quark plasma , one created as the iron nucleus vaporizes during the transition event , could produce a sufficiently wide cone of particle-zoo ejecta to irradiate a fatal percentage of your brain . rapid heating of your brain would not be a problem , however , since 1/500 of the approximately 50 joules of kinetic energy would work out to be only about 0.1 j of heat energy tops . by comparison a standard firecracker releases about 500 j of energy . and what are the real odds on such a dead-center strike on a large nucleus near the surface of your brain , assuming you were an astronaut unprotected by out atmosphere ? low almost beyond belief . look at the date on the oh-my-god particle : 1991 . we have not seen one quite that feisty since . as @dmckee aptly notes , ordinary cosmic rays or their secondary outputs hit us all the time , and astronauts watch direct collision buzz through their retinas without much harm .
the process of light propagation is described by the maxwell equations . $$ \nabla\cdot{\bf d} = \rho $$ $$ \nabla\cdot{\bf b} = 0 $$ $$ \nabla\times{\bf e} = - {{\partial{\bf b}}\over{\partial t}} $$ $$ \nabla\times{\bf h} = {\bf j} + {{\partial{\bf d}}\over{\partial t}} $$ these equations say ( in simple terms ) that : change in the electric field is causing a change in magnetic field , while change in magnetic field is causing a change in the electric field . the original source of electromagnetic waves is some oscillating charge ( for instance an electron ) which has an electric field around it . this field is changing ( because the charge is oscillating ) . therefore ( according to the fourth equation ) a magnetic field $h$ is formed . but the creation of magnetic field is in fact a change in magnetic field . this leads ( according to the third equation ) to creation of new electric field $e$ . but this change in $e$ leads to $h$ , which leads to $e$ , etc .
it seems that the cart is always released at the top . it is the first picket fence which is moved . thus smaller $d$ means you confine the measuring too the high speed regions of the track . at the last measuring you effectively measure the final velocity of the cart at the bottom . it is a nice lab , i remember a question about air track --- where the viscous friction take place ? standard error is the standard deviation of the error . just standard deviation of the results tells you how do the results scattered . standard error tells how accurate is the average of the results . you see , the more experiments you do , the more accurate is the average , right ? to answer is your calculation right , you should tell standard error of what quantity are you interested in ? in any case , there are too much digits in your stderr , you should leave in general only 1 significant digit , or maybe 2 for if the first digit is 1 . basically i suggest you reading your lab book , the beginning . it should tell what are the different quantities , how do you calculate them etc . it is really useful , you read it once for an hour and you are prepared for all the consequent labs , statistics courses , statistics modeling and so on .
this question has introduced me to the whole " entropic force " area which has several papers during 2010 . i see that there are references to " entropic force " explanations for coulomb 's law and other areas too . here is a link to a simple introduction to these ideas . the verlinde paper and others however are deriving newtons law , einstein 's gr etc as classical theories . the underlying formulation of course being a stochastic behaviour of unknown microstates . despite the presence of $\hbar$ and the motivation from the black hole area formulae the verlinde paper does not introduce an explicit link with quantum mechanics . thus there is no derivation of schrodinger 's equation and no introduction of $\psi$ . the kobakhidze paper says " one starts with a " holographic screen s which contains macroscopically large number of microscopic states which we denote as $\left|i ( z ) \right\rangle$ , $i ( z ) = 1 , 2 , . . . , n ( e ( z ) , z ) . $ the screen is then described by the mixed state $$\rho ( z ) =\sigma p_{i ( z ) }\left|i ( z ) \rangle \langle i ( z ) \right|$$ however verlinde does not explicitly introduce microstates as quantum states , with density matrices etc , although this is a tempting extension . now it might be that this is the only sensible quantum development of the stochastic basis of the " entropic idea " , but verlinde has not taken it . so what is disproved is a theory that verlinde has not written down . having said this , there is a resemblance between " entropy " and the idea of introducing " stochastics " into quantum theory . one such attempt is known as " stochastic quantum electrodynamics " described in this wikipedia overview . as you will see from the summary this has had successes with e.g. the unruh effect , but problems modelling genuine quantum phenomena . i dont know whether anyone has considered combining the two areas directly .
there are no very simple diagrams . you need at least one pair production and some kind of flavor changing reaction . this includes one pair production and a drell-yan flavor change . there will be others but they will presumably all be equally complicated and therefore unlikely . this will be a low rate event in such systems even when the energy available .
they are variants , different kinds of quantum field theory , but they are not mutually exclusive . the different adjectives you mention separate quantum field theory to " pieces " in different ways . the different sorts of variants you mention are being used and studied by different people , the classification has different purposes , the degree of usefulness and validity is different for the different adjectives , and so on . conformal quantum field theory is a special subset of quantum field theories that differ by dynamics ( the equations that govern the evolution in time ) , namely by the laws ' respect for the conformal symmetry ( essentially scaling : only the angles and/or length ratios , and not the absolute length of things , can be directly measured ) . conformal field theories have local degrees of freedom and the forces are always long-range forces , which never decrease at infinity faster than a power law . they are omnipresent in both classification of quantum field theories - almost every quantum field theory becomes scale-invariant at long distances - and in the structure of string theory - conformal field theories control the behavior of the world sheets of strings ( here , the cft is meant to contain two-dimensional gravity but the latter carries no local degrees of freedom so it does not locally affect the dynamics ) as well as boundary physics in the holographic ads/cft correspondence ( here , cfts on a boundary of an anti de sitter spacetime are physically equivalent to a gravitational qft/string theory defined in the bulk of the anti de sitter space ) . conformal field theories are the most important class among those you mentioned for the practicing physicists who ultimately want to talk about the empirical data but these theories are still very special ; generic field theories they study ( e . g . the standard model ) are not conformal . topological quantum field theory is one that contains no excitations that may propagate " in the bulk " of the spacetime so it is not appropriate to describe any waves we know in the real world . the characteristic quantity describing a spacetime configuration - the action - remains unchanged under any continuous changes of the fields and shapes . so only the qualitative , topological differences between the configurations matter . topological quantum field theory ( like chern-simons theory ) is studied by the very mathematically oriented people and it is useful to classify knots in knot theory and other " combinatorial " things . they are the main reason behind edward witten 's fields medal etc . axiomatic or algebraic ( and mostly also " constructive " ) quantum field theory is not a subset of different " dynamical equations " . instead , it is another approach to define any quantum field theory via axioms etc . that is why it is a passion of mathematicians or extremely mathematically formally oriented physicists and one must add that according to almost all practicing particle physicists , they are obsolete and failed ( related ) approaches which really can not describe those quantum field theories that have become important . in particular , aqfts of both types start with naive assumptions about the short-distance behavior of theories and are not really compatible with renormalization and all the lessons physics has taught us about these things . constructive qfts are mainly tools to understand the relativistic invariance of a quantum field theory by a specific method . then there are many special quantum field theories , like the extremely important class of gauge theories etc . they have some dynamics including gauge fields : that is a classification according to the content . qfts are often classified according to various symmetries ( or their absence ) which also constrain their dynamical laws : supersymmetric qfts , gravitational qfts based on general relativity , theories of supergravity which are qfts that combine general relativity and supersymmetry , chiral qfts which are left-right-asymmetric , relativistic qfts ( almost all qfts that are being talked about in particle physics ) , lattice gauge theory ( gauge theory where the spacetime is replaced by a discrete grid ) , and many others . gauge theories may also be divided according to the fate of the gauge field to confining gauge theories , spontaneously broken qfts , unbroken phases , and others . string field theory is a qft with infinitely many fields which is designed to be physically equivalent to perturbative string theory in the same spacetime but it only works smoothly for open strings and only in the research of tachyon condensation , it has led to results that were not quite obtained by other general methods of string theory . we also talk about effective quantum field theories which is an approach to interpret many ( almost all ) quantum field theories as an approximate theory to describe all phenomena at some distance scale ( and all longer ones ) ; one remains agnostic about the laws governing the short-distance physics . that is a different classification , one according to the interpretation . effective field theories do not have to be predictive or consistent up to arbitrarily high energies ; they may have a " cutoff energy " above which they break down . it does not make much sense to spend too much time by learning dictionary definitions ; one must actually learn some quantum field theory and then the relevance or irrelevance and meaning and mutual relationships between the " variants " become more clear . at any rate , it is not true that the classification into adjectives is as trivial as the list of colors , red , green , blue . the different adjectives look at the framework of quantum field theory from very different directions - symmetries that particular quantum field theories ( defined with particular equations ) respect ; number of local excitations ; ability to extend the theory to arbitrary length scales ; ways to define ( all of ) them using a rigorous mathematical framework , and others .
with drag force $- \alpha \left|{\dot{\bf r}}\right| {\dot{\bf r}}$ and gravitational force $-mg {\bf {\hat{y}}}$ , the equations of motion are ( see my answer to this question ) $$ \begin{align} {\ddot{x}} and = - \beta {\dot{x}} \sqrt{{\dot x}^2+{\dot y}^2} \\ {\ddot{y}} and = - g - \beta {\dot{y}} \sqrt{{\dot x}^2+{\dot y}^2} \end{align} $$ where $\beta = \alpha / m$ , $\alpha = \rho c_d a / 2$ , $\rho$ is the air density , $c_d$ is the drag coefficient , and $a$ is the cross-sectional area of the projectile . i have never seen the above equations solved analytically . here 's some python code that plots the solution with and without air resistance . to run , copy into a text file myfile . py and run python myfile . py from the command line .
in fact the electrons ( at least those in s-shells ) do spend some non-trivial time inside the nucleus . the reason they spend a lot of time outside the nucleus is essentially quantum mechanical . to use too simple an explanation their momentum is restricted to a range consistent with begin captured ( not free to fly away ) , and as such there is a necessary uncertainty in their position . an example of physics arising because they spend some time in the nucleus is so called " beta capture " radioactive decay in which $$ e + p \to n + \nu $$ occurs within the nucleus . the reason this does not happen in most nuclei is also quantum mechanical and is related to energy levels and fermi-exclusion . to expand on this picture a little bit , let 's appeal to de broglie and bohr . bohr 's picture of the electron orbits being restricted to a set of finite energies $e_n \propto 1/n^2$ and frequencies can be given a reasonably natural explanation in terms of de broglie 's picture of all matter as being composed of waves of frequency $f = e/h$ by requiring that a integer number of waves fit into the circular orbit . this leads to a picture of the atom in which all the electrons occupy neat circular orbits far away from the nucleus , and provides one explanation of why the electrons do not just fall into the nucleus under the electrostatic attraction . but it is not the whole story for a number of reasons ; for our purposes the most important one is that bohr 's model predicts a minimum angular momentum for the electrons of $\hbar$ when the experimental value is 0 . pushing on , can solve the three dimensional schrdinger equation in three dimesions for hydrogen-like atoms : $$ \left ( i\hbar\frac{\partial}{\partial t} - \hat{h} \right ) \psi = 0 $$ for electrons in a $1/r^2$ electrostatic potential to determine the wavefunction $\psi$ . the wave function is related to the probability $p ( \vec{x} ) $ of finding an electron at a point $\vec{x}$ in space by $$ p ( \vec{x} ) = \left| \psi ( \vec{x} ) \right|^2 = \psi^{*} ( \vec{x} ) \psi ( \vec{x} ) $$ where $^{*}$ means the complex conjugate . the solutions are usually written in the form $$ \psi ( \vec{x} ) = y^m_l ( \theta , \phi ) l^{2l+1}_{n-l-1} ( r ) e^{-r/2} * \text{normalizing factors} $$ here the $y$ 's are the spherical harmonics and the $l$ 's are the generalized laguerre polynomials . but we do not care for the details . suffice it to say that that these solutions represent a probability density for the electrons that is smeared out over a wide area near around the nucleus . also of note , for $l=0$ states ( also known as s orbitals ) there is a non-zero probability at the center , which is to say in the nucleus ( this fact arises because these orbital have zero angular momentum , which you might recall was not a feature of the bohr atom ) .
see also simple harmonic motion - what are the units for $\omega_0$ ? and https://en.wikipedia.org/wiki/joule#confusion_with_newton-metre here 's a somewhat shorter explanation reflecting my own ( possibly incorrect ) intuition : radians are not " real " units ; they are just a trick to keep track of which quantities involve angles and which do not , since it is usually a mistake to get those mixed up . however , it is occasionally valid to mix those two types of quantities , and then we drop the radians . torque is one such place . it is probably possible to be fully rigorous about this and make radians an actual unit , but i have never seen it done .
copying from the electronics . se since in properly constructed power network the neutral wire is maintained at a potential level close to ground potential , there is nearly no voltage between the neutral and the ground . hence , touching neutral will not cause current to flow through human body into ground . so by construction there is very little potential between the ground and the neutral . when a human touches the live wire he closes the circuit with the ground instead of the neutral because there is by construction so little difference between neutral and live .
i think the problem here is that you are being vague about the limits special relativity impose . let 's get this clarified by being a bit more precise . the velocity of any particle is of course limited by the speed of light c . however , the theory of special relativity does not imply any limit on energy . in fact , as energy of a massive particle tends towards infinity , its velocity tends toward the speed of light . specifically , $$e = \text{rest mass energy} + \text{kinetic energy} = \gamma mc^2$$ where $\gamma = 1/\sqrt{1- ( u/c ) ^2}$ . clearly , for any energy and thus any gamma , $u$ is still bounded from above by $c$ . we know that microscopic ( internal ) energy relates to macroscopic temperature by a constant factor ( on the order of the boltzmann constant ) , hence temperature of particles , like energy , has no real limit .
i will try to give a series of counterexamples , of decreasing triviality : no signalling plus little external agents implies microcausility as you point out in the question , when you have arbitrarily tiny external agents capable of measuring any bosonic field in an arbitrarily tiny region , then microcausality is obviously necessary for no signaling , since if you have two noncommuting operators a and b associated with two tiny spacelike separated regions , and two external agents wants to transmit information from a 's region to b 's , the agent can either measure a repeatedly or not , while another agent measures b a few times to see if a is being measured . the b measurements will have a probability of giving different answers , which will inform the b agent about the a measurement . this is the motivation for microcausility , and for the purposes of physics , the existence of semiclassical black holes means that you have classical point probes at any distance significantly larger than the planck length , and microcausality is necessary at least for these scales . this point is adressed in your question . from now on , i will ask the intrinsic question--- can observers in the theory signal using devices built up out of the fields in the theory , not using external probes , so that the question is nontrivial . two space no-gravity qft consider a quantum field theory with a bad localization . the theory is defined using a spacelike shift vector $\delta$ and the lagrangian gets with a displaced interaction $$ s= \int d^4x l_1 ( \phi ) + l_2 ( \chi ) + \phi ( x ) \chi ( x+\delta ) $$ where the l 's are some translationally invariant local actions for $\phi$ and $\chi$ , and the interaction mixes $\phi$ and $\chi$ at displaced points . this is clearly ridiculous-- the field $\chi$ has been misplaced , the correct local field associated with a given point x is $\chi ( x+\delta ) $ , not $\chi ( x ) $ , but the point is that you can define an algebra of observables using this completely wrong localization , and then microcausality obviously fails , because $\chi$ and $\phi$ are at the wrong point . but because there is a change of variables which makes microcausality work , there is no signalling for objects in the theory , intrinsically ( although for an external agent capable of making local measurements of $\phi ( x ) +\chi ( x ) $ , no signalling would fail ) . so the question should be better stated " does there have to be some collection of field variables which obey microcausility for no-signalling to work " . curved extra dimensions suppose you have a warped extra dimension , so that in 5+1 dimensions you have fields which are local , but the background is not a product . then you can consider the theory as a 4 dimensional quantum field theory , and in this framework , try to identify mutually local four-dimensional fields . this does not work , not in a way consistent with lorentz invariance , because time ticks at a different rate at different positions in the extra dimension , so that if you choose the fields to obey 4 dimensional lorentz invariance . but if the shortest distance between two points on your brane-world is a straight line on the brane-world , then no signalling still holds . gubser examines this situation in a recent preprint ( http://arxiv.org/ps_cache/arxiv/pdf/1109/1109.5687v2.pdf ) with an eye to reproducing the opera neutrino no-signalling claimed violations , and he says that the effective 4d theory only violates no signalling when the 5d theory violates of the weak energy condition . but the 5d theory will violate any attempted identification of a 4d microcausality generically . emergent dimensions i think that the best examples of where microcausality can fail , and still there is no intrinsic signalling , are within string theory . this is not quantum field theory , so it might not be included , but it is the starkest example of a nonlocal theory where no-signalling ( presumably ) works , but there is no microcausility , because there are not local fields in the bulk . in ads/cft the bulk theory is defined by a holographic projection of the boundary fields , and if you have n=4 gauge theory on the boundary , you only have boundary microcausality . you can define effective local fields in the bulk , which create a string excitation , but these fields will not commute at the string scale , since the strings are extended , and they are not fundamental things anyway , their localization is at a center of mass . so in my opinion , the best answer is no , although the answer might as well be yes outside of the quantum gravity regime , because at larger scales , tiny black holes can be used as point probes to make local measurements of fields .
i am by no means an expert in tuning fork design , but here are some physical considerations : different designs may have different " purities , " but do not take this too far . it is certainly possible to tune to something not a pure tone ; after all , orchestras usually tune to instruments , not tuning forks . whatever mode ( s ) you want to excite , you do not want to damp with your hand . imagine a single bar . if you struck it in free space , a good deal of the power would go into the lowest frequency mode , which would involve motion at both ends . however , clamping a resonator at an antinode is the best way to damp it - all the energy would go into your hand . a fork , on the other hand , has a natural bending mode that will not couple very well to a clamp in the middle .
yes it is . the total momentum vector of a system does not change at all ( constant length and direction ) , so the projection of it on a line ( or any function you apply to it ) will not change . projection is a linear operator , so that if you project each particle 's momentum on a line and then sum , you get the same result as summing first ( to get the total momentum ) then projecting .
if you build the square of the wave function , the result is a gaussian curve . if you compare your result with the general form of a normal distribution you can see that x0 is the expectation value . . . http://en.wikipedia.org/wiki/normal_distribution
here 's what you did in the derivation actually . you derived the lens law for a special case . i.e. for a particular position of the lens , object and image . like image on right of the lens and object on left or vice-versa . now the lens formula that you got was for this particular scenario . now suppose you apply the sign convention on this formula once again , the sign convention you applied in the derivation will be cancelled . it is like if ' u ' is negative in your formula , applying another '-' will cancel the effect and give you the general lens formula that will hold in all situations . its kind of like applying sign convention gives a particular formula and applying it twice cancels the effect of the sign convention at all , giving you the general lens formula . this general lens formula is what you were after which you can use in other special scenarios by applying the sign convention . if you want to see what i am saying , try deriving the lens formula , using these two conditions . ( 1 ) image on right and object on left of lens . ( 2 ) image on left and object on left . btw , do you know why you use sign convention in the first place ? because the formula can not distinguish otherwise between right and left , as you primarily using only magnitude of lengths in the derivation through similar triangles . also , do you know a virtual image is kind of an optical illusion only , as to there is nothing there behind the mirror .
short answer , no . long answer , sort of . short answer : no , the e and m fields may be coupled by the lorentz transformations , but it is only when they work together to make a self-propagating wave that we can call it a particle . to separate them as individual fields is physically meaningless . so giving each field their own particle is equally meaningless . long answer : this can be thought of as an " is the moon really there when we are not looking at it ? " problem . the only way we can observe that an e or m field is present is when it interacts with something via the lorentz force . so if the field is not interacting with anything , is it really there ? in advanced physics , that answer turns out to be a resounding " no " . what we can say is that the e or m field interacts with objects via a photon . that is , the magnetic or electric field can be said to exists , but we can also say that the source of this field is interacting with our sensors or other particles by exchanging photons to produce a force . thus , to answer your question , we can in a way " quantize " the separate fields and visualize them as a particle , but only if we visualize them not as fields but the exchange of photons between the sources of the field and the objects influenced by it . but full photons , not half a photon .
its funny you should ask this as i recently ran several simulations on matlab regarding the same thing except with atoms . effectively , i had a diatomic molecule ( h-h for example ) and an atom ( f lets say ) . the atom and diatomic both had some momentum relative to each other and the collision was setup to be perfectly collinear . now , what i noticed is that the initial energy of the reactant ( that is the incoming f atom ) was deposited into two modes . . . translational and vibrational energy . depending on the choice of the atom and diatomic more of one form over the other would be required for a successful reaction ( polanyi rules but we wont go into that ) . essentially , if the reaction was elastic then you would have an unreactive collision . the atom and diatomic coalesced to form a three body transition state and then the atom would just break off and head back in the direction it came from . in a reactive collision , which was always inelastic , there was always a change in vibrational energy between the reactants and the products i.e. upon form a new diatomic ( h-f from our above example ) there was reduced or increased vibrational energy . i do not know how useful this tangential discussion has been but i thought it might help you : ) edit : i also want to clarify that an unreactive trajectory could also be generated from an inelastic collision simply because the incoming atom had far too much kinetic energy . i have included the potential energy surface of a similar reaction below . the black line represents the trajectory , the ' wiggliness ' you see initially is the vibrational energy . . . . there is an obvious change in the ' wigliness ' as you go around the bend in the pes i.e. as you go from cl+h2 to h-cl + cl . if you would like i can send you these source files .
disperse into what ? all of space was completely filled with hot matter and radiation . space has expanded enough that the incredibly high temperatures that once were the equilibrium could cool down to the current temperature of the cosmic microwave background , 2.725 kelvin . that is very cold , but certainly not absolute zero ! moreover , it is certainly detectable .
you might need to say some more about what you want to do with this equation , because you can descend into as much complexity as you like . do you , for example , want to think about variable length strings , i.e. those where the tension lengthens the string and the tension itself is a function of position along the string ? do you want to think about a general , nontangential force ? you could pull out landau and lifshitz " theory of elasticity " or stephen timoshenko " strength of materials volume 2" or " theory of elasticity " and build something pretty complicated , but each new effect modeled is going to yield diminishing returns . assuming a constant tension $t$ in the string of linear density $\mu$ along its length $z$ and assuming still predominantly transverse motion $y ( z , \ , t ) $ in one plane , i get : $$t\ , \cos\theta ( z , t ) \ , \kappa ( z , t ) = \mu\ , \partial_t^2 y\quad\quad\quad ( 1 ) $$ where $\theta$ is the string 's angle made with the horizontal and $\kappa$ its curvature . substituting for $\cos\theta$ and $\kappa$ yields : $$t\ , \partial_z^2 y = \mu\ , ( 1+ ( \partial_z y ) ^2 ) ^2\ , \partial_t^2 y\quad\quad\quad ( 2 ) $$ which will give you a nice nonlinearity to chew on . next , you might consider a constant tension , constant length string with vibration but with motion in both transverse directions . so you are going to get two coupled nonlinear differential equations . let our two transverse displacement components be $x ( z , t ) $ and $y ( z , t ) $ , then the local tangent to the string be defined by the unit vector components $x = \partial_z x/\sqrt{1 + ( \partial_z x ) ^2+ ( \partial_z y ) ^2}$ and $y = \partial_z y/\sqrt{1 + ( \partial_z x ) ^2+ ( \partial_z y ) ^2}$ so that ( here $s$ is the arclength ) : $$t\ , \mathrm{d}_s x = t\ , \partial_z\left ( \frac{\partial_z x}{\sqrt{1+ ( \partial_z x ) ^2+ ( \partial_z y ) ^2}}\right ) \mathrm{d}_s z = \mu\ , \partial_t^2 x\quad\quad\quad ( 3 ) $$ $$t\ , \mathrm{d}_s y = t\ , \partial_z\left ( \frac{\partial_z y}{\sqrt{1+ ( \partial_z x ) ^2+ ( \partial_z y ) ^2}}\right ) \mathrm{d}_s z = \mu\ , \partial_t^2 y\quad\quad\quad ( 4 ) $$ whence ( since $\mathrm{d}_z s = \sqrt{1+ ( \partial_z x ) ^2 + ( \partial_z y ) ^2}$ ) : $$\left ( 1+ ( \partial_z y ) ^2\right ) \ , \partial_z^2 x- \partial_z x\ , \partial_z y\ , \partial_z^2 y = \frac{\mu}{t}\ , \left ( 1+ ( \partial_z x ) ^2+ ( \partial_z y ) ^2\right ) ^2\ , \partial_t^2 x\quad\quad\quad ( 5 ) $$ $$\left ( 1+ ( \partial_z x ) ^2\right ) \ , \partial_z^2 y- \partial_z x\ , \partial_z y\ , \partial_z^2 x = \frac{\mu}{t}\ , \left ( 1+ ( \partial_z x ) ^2+ ( \partial_z y ) ^2\right ) ^2\ , \partial_t^2 y\quad\quad\quad ( 6 ) $$ which reduce to eq . ( 2 ) when there is vibration in one plane only . you will get some really interesting effects from these coupled equations : whirling , coupling of energy from $x$ to $y$ and back again and so forth . the next step would be to think of the axial motion of the string and the attendant variable tension along the string 's length . this would only be apparent well into the nonlinear rgime and likely ( 5 ) and ( 6 ) should model most of the nonlinear effects you will need . energies in the string if you are seeking to find out the work done by the end of the string , then you would need a model of what it is linked to and therefore a tension to displacement expression - likely a differential equation , which will be a differential equation . now the tension $t$ is a function of time , so you are beginning to get seriously interesting ! you might also be interested in looking at a tension varying with length at this point , with the local tension defined by $e\ , a\ , \epsilon ( z , t ) = k_t\ , \epsilon ( z , t ) $ , where $e$ is the string 's young 's modulus , $a$ its cross-sectional area and $\epsilon ( z , t ) $ the strain . it makes more sense to use $k_t$ and measure experimentally : it is not going to be easy to work out $k_t$ from first principles from the material elastic constants for a braided or stranded string ! the sting 's curvature begets the strain : $\mathrm{d} s = \sqrt{1+ ( \partial_z x ) ^2 + ( \partial_z y ) ^2} \mathrm{d} z$ so that $$\epsilon ( z , t ) =\sqrt{1+ ( \partial_z x ) ^2 + ( \partial_z y ) ^2} - 1 \approx \frac{1}{2}\left ( ( \partial_z x ) ^2 + ( \partial_z y ) ^2\right ) \quad\quad\quad ( 7 ) $$ if you looking for loss in the string , a good model of air drag force is $\lambda\ , \partial_t x$ , $\lambda\ , \partial_t y$ ( i.e. . proportional to transverse velocity ) , which terms you will need to include in the dynamical equation , then work out loss from the power dissipated by these terms . internal material bending losses are complicated to model : often you can do this kind of thing by replacing material elastic constants with lossy elastic operators - so you would replace the young 's modulus $e$ for example by something of the form $e+e_t \partial_t$ , for some loss constant $e_t$ ; equivalently , you would work with $k_t + k_1 \partial_t$ for the string 's effective srping constant . but , at last , if you , as i now understand from your questions , are looking simply to find out the energy needed to set the vibration up ( the energy stored in the string ) in a lossless string , then you can work as follows . the kinetic energy per unit length is obvious : it is simply : $$k ( z , t ) = \frac{1}{2}\ , \mu\ , \left ( ( \partial_t x ) ^2 + ( \partial_t y ) ^2\right ) \quad\quad\quad ( 8 ) $$ now , if we assume that the displacement is small , such that the at first high tension $t$ does not change much as the string vibrates , then the work done by $t$ in straining a length $\mathrm{d}z$ of string is $t\ , \epsilon\ , \mathrm{d}z$ , so that the potential energy stored per unit length is , from eq . ( 7 ) : $$u ( z , t ) = t\ , \epsilon\ = \left ( \sqrt{1+ ( \partial_z x ) ^2 + ( \partial_z y ) ^2} - 1\right ) \ , t\approx\frac{1}{2}\ , t\ , \left ( ( \partial_z x ) ^2 + ( \partial_z y ) ^2\right ) \quad\quad\quad ( 9 ) $$ the approximation holding when $|\partial_z x| , \ , |\partial_z y|\ll 1$ . these are the general equations . to find the dispersion relationship for the uncoupled linear vibration equations $t\ , \partial_z^2 y = \mu\ , \partial_t^2 y$ , $t\ , \partial_z^2 x = \mu\ , \partial_t^2 x$ we study solutions of the form $\exp ( i\ , ( k\ , z\pm\omega\ , t ) ) $ where $k$ is the wavenumber and $\omega$ the angular frequency ; on substitution into the linear equations , we get $t\ , k^2 = \mu\ , \omega^2$ or : $$c = \left|\frac{\omega}{k}\right| = \sqrt{\frac{t}{\mu}}\quad\quad\quad ( 10 ) $$ so for such a wave , eq . ( 8 ) and eq . ( 9 ) ( the latter in the small vibration $|\partial_z x| , \ , |\partial_z y|\ll 1$ approximation ) can be combined to show that $u ( z , t ) = k ( z , t ) $ , as you state . likewise , by using this relationship as well as parseval 's theorem for fourier series for any superposition of frequencies such that the waveshape is periodic , you can prove that the total kinetic and potential energies integrated over a wavelength are equal . but this is for the linear rgime only . more generally , you must use eq . ( 5 ) and eq . ( 6 ) together with eq . ( 8 ) and eq . ( 9 ) separately . even with these equations , it would be altogether reasonable to assume the small vibration approximation with eq . ( 9 ) , because none of the above considers $z$-directed components of the force , which will become significant with angles that are big enough to make the small vibration approximation of eq . ( 9 ) invalid . therefore , your final set ( approximating the rhs of ( 5 ) and ( 6 ) in the same way as ( 9 ) ) might be : $$\begin{array}{rcl} \left ( 1+ ( \partial_z y ) ^2\right ) \ , \partial_z^2 x- \partial_z x\ , \partial_z y\ , \partial_z^2 y and = and c^2\ , \left ( 1+2\ , ( \partial_z x ) ^2+2\ , ( \partial_z y ) ^2\right ) \ , \partial_t^2 x\\ \left ( 1+ ( \partial_z x ) ^2\right ) \ , \partial_z^2 y- \partial_z x\ , \partial_z y\ , \partial_z^2 x and = and c^2\ , \left ( 1+2\ , ( \partial_z x ) ^2+2\ , ( \partial_z y ) ^2\right ) \ , \partial_t^2 y\\ k ( z , t ) and = and \frac{1}{2}\ , \mu\ , \left ( ( \partial_t x ) ^2 + ( \partial_t y ) ^2\right ) \\ u ( z , t ) and = and \frac{1}{2}\ , \mu\ , c^2\ , \left ( ( \partial_z x ) ^2 + ( \partial_z y ) ^2\right ) \end{array}\quad\quad\quad ( 11 ) $$ with $c$ defined by eq . ( 10 ) .
following larry 's response , but with approximate numbers : assume an " average " night means september 21 / march 21 ( nights about that length are more common than others ) . from http://www.sunrisesunset.com/calendar.asp , i get that the night lasts 11:45 in boulder , co ( which is at 40n ) . i would assume that everything about 10 degrees above the horizon is visible , and everything below is not - that is true in many locations either because of trees , atmosphere opacity , mountains , or city lights . if you are using a telescope , i would not ever go below 20 degrees . a general formula using a surface integral on the surface of a sphere : $a = \text{angle above horizon something is considered visible}$ $b = \text{latitude}$ $c = \text{number of degrees in the night = number of hours in the night / 24 * 360}$ $d = 180 - 2*a + c$ $e = ( 90-b ) -a$ $ \text{visible fraction} = \left ( \int_0^d \int_{90-e}^{180} \sin ( \varphi ) d\varphi d\theta \right ) / 4 \pi$ $= - ( \cos ( 180 ) - \cos ( 90-e ) ) * d / ( 4\pi ) $ $= d ( \sin ( e ) + 1 ) / ( 4\pi ) $ ( d and e must be converted to radians ) for an 11h45m night , that comes out to 38.0% , 29.9% , 22.0% for $a=$10 , 20 , and 30 degrees respectively . if you consider that $\cos ( 90-b ) / 2$ of the sky is never visible ( because it is always below the horizon ) , these become 61.6% , 48.5% , 35.7% of the sky that you could ever see . these calculations were somewhat hasty . . . i expect to be brutally corrected . the real answer is much more complicated - you need to do an integral over a sphere after rotating the pole , which gets into euler parameters and quaternions . still , i think my first guess is probably correct to within about 5-10% .
i believe you have a mistake in your formula as the self-inductance of a coil is given by $$l\approx\mu_0 \frac{n^2 a}{\ell} ; $$ here $n$ is the number of windings , $a$ is area of the cross-section , and $\ell$ is the length of the coil . your task is to maximize $l$ with the constraint that the length of the copper wire is $w$ . assuming that the solenoid is a cylinder , the cross-section read $a=\pi r^2$ with $r$ the radius of the cylinder . a solenoid with $n$ windings needs a wire of length $w= 2\pi rn$ . thus , $$ l \approx \mu_0 \frac{w^2}{\ell} . $$ we see that the inductance of the solenoid decreases with increasing length ( keeping the total length of the wire fixed ) . thus , we obtain the largest self-inductance having the smallest length which is a single loop with $n=1$ . for a single loop the formula given above is not correct ( as it assumes $\ell \gg \sqrt{a}$ ) and thus we have $$l\approx \mu_0 r \ln ( r/r ) \approx \mu_0 \frac{w}{2\pi} \ln ( w/r ) $$ with $r$ the radius of the wire .
as we cannot resolve arbitrarily small time intervals , what is ''really'' the case cannot be decided . but in classical and quantum mechanics ( i.e. . , in most of physics ) , time is treated as continuous . physics would become very awkward if expressed in terms of a discrete time . edit : if time appear discrete ( or continuous ) at some level , it could still be continuous ( or discrete ) at higher resolution . this is due to general reasons that have nothing to do with time per se . i explain it by analogy : for example , line spectra look discrete , but upon higher resolution one sees that they have a line width with a physical meaning . thus one cannot definitely resolve the question with finitely many observations of finite accuracy , no matter how contrived the experiment .
for the two-body problem : in any inertial frame , applying newton 's 2 nd law , we write : $$ m_1 \ddot{\vec{r_1}} = \frac{-gm_1m_2}{r^2}\hat{r} $$ $$ m_2 \ddot{\vec{r_2}} = \frac{gm_1m_2}{r^2}\hat{r} $$ as a side derivation , you can add the above two derivations to obtain the fact that total linear momentum of the system is constant , but we move on for now . to get relative motion , subtract the equations for $\vec{r_1}$ and $\vec{r_2}$ , and obtain : $$ \ddot{ ( \vec{r_1}-\vec{r_2} ) } = - ( \frac{1}{m_1} + \frac{1}{m_2} ) \frac{gm_1m_2}{r^2}\hat{r} $$ define ' reduced mass ' $\mu$ as $$ \frac{1}{\mu} = \frac{1}{m_1} + \frac{1}{m_2} $$ we then have $$ \mu \ddot{\vec{r}} = \frac{-gm_1m_2}{r^2}\hat{r} $$ rearranging the above equation , we can rewrite : $$ \ddot{\vec{r}} = \frac{-g ( m_1+m_2 ) }{r^2}\hat{r} = \frac{-\alpha}{r^2}\hat{r} $$ we have our equations ready with us . now , we solve them . but note that the quantities $j$ ( angular momentum ) and $e$ ( total energy ) are conserved defined as follows . you can easily check this fact . $$\vec{j} = \mu \vec{r} \times \dot{\vec{r}}$$ and $$e = k+v = \frac{1}{2} \mu \dot{\vec{r}}^2 - \frac{gm_1m_2}{r}$$ how do you know the motion is in plane ? since , $\vec{j}$ is always perpendicular to plane of motion and it is constant , the motion is in a plane . dividing motion into components , $$\ddot{x} = \frac{-\alpha}{r^3}x \ , \ \ddot{y} = \frac{-\alpha}{r^3}y$$ and $$r^2 = x^2 + y^2$$ and $$j = \mu ( x\dot{y}-y\dot{x} ) $$ solving the above three equations ( i leave that to the reader ) , we obtain : $$\ddot{r} = \frac{-\alpha}{r^3} + \frac{j^2}{\mu^2 r^3}$$ our interest is in equation of orbit $r ( \theta ) $ . note that $$\dot{\vec{r}} = \dot{r}\hat{r} + r \omega \hat{\theta}$$ which implies $$\vec{j} = \mu r^2 \omega \hat{z}$$ or $$j = \mu r^2 \omega$$ now , the action $$\frac{dr}{dt} = \frac{dr}{d \theta}\frac{d \theta}{dt} = \frac{dr}{d \theta} \omega = \frac{dr}{d \theta}\frac{j}{\mu r^2}$$ and then $$\frac{d^2r}{dt^2} = \frac{d^2 r}{d \theta^2} ( \frac{j}{\mu r^2} ) ^2 - \frac{2}{r^3}\frac{j}{\mu}\frac{j}{\mu r^2} ( \frac{dr}{d \theta} ) ^2$$ this is a very complicated equation for $r ( \theta ) $ . but the equation is simple in terms of $\rho ( \theta ) = \frac{1}{r ( \theta ) }$ . performing change of variables , we get $$\frac{d^2 \rho}{d \theta^2} + \rho = \mu \frac{gm_1m_2}{j^2}$$ this has a simple solution $$\rho ( \theta ) = a \cos \theta + \mu \frac{gm_1m_2}{j^2}$$ it is convenient to write $h = \frac{j}{\mu}$ . finally substituting for $r$ , $$r = \frac{\frac{h^2}{\alpha}}{1+\frac{ah^2}{\alpha}\cos \theta}$$ naming new variables $a$ and $e$ , the above equation becomes $$r = \frac{a ( 1-e^2 ) }{1+e \cos \theta}$$ you have been given incomplete information if you have been told this is an equation for ellipse . it is actually the equation for a conic section . based on values of $e$ , the trajectory can be anything . $$e = 0 , circle$$ $$0&lt ; e&lt ; 1 , ellipse$$ $$e = 1 , parabola$$ $$e&gt ; 1 , hyperbola$$ but that does not solve your doubt completely . here 's the catch . note that the values of $a$ ( semi-major axis ) and $e$ ( eccentricity ) of orbit , which are two orbital parameters , depend upon $j$ and $e$ i.e. the angular momentum and energy of the system . you can play with the equations mentioned above to derive various properties of the system . but , the important thing to keep in mind is the fact that values of $j$ and $e$ dynamically affect the values of $a$ and $e$ . it is pure co-incidence that in most of these cases , the trajectory takes the form of an ellipse , but it could be any conic section in theory . does this derivation and explanation satisfy you ? p.s. if you want to change the trajectory of say , earth , you will have to change $j$ and $e$ . that is the job of rockets that launch satellites . scientists , at the base level , take care that $j$ and $e$ fit right and the rest of the physics takes care of itself
the effect in which two objects get charged by rubbing and remain charged is called the triboelectric effect , http://en.wikipedia.org/wiki/triboelectric_effect where the root " tribo " means friction in greek ( the greek word $\tau\rho\iota\beta\omega$ means ' to rub' ) . friction is actually unnecessary : contact is enough in principle . this effect should not be confused with the ( volta or galvani ) " contact potential " between metals which only exists as long as the two metals remain in contact , and especially not with " contact electrification " which was a name of a scientifically incorrect theory of electricity at the end of the 18th century that attempted to overgeneralize the interpretation of the triboelectric effect . " electrophorus " was a gadget , first produced by volta , that used the triboelectric effect . the cause of the triboelectric effect is adhesion - the atoms on the surface literally form chemical bonds . materials such as fur are ready to lose electrons and become positively charged while the materials such as ebonite or glass gain electrons and become negatively neutral . to get some idea about which atoms are likely to lose or gain electrons , it is useful to know their electronegativity : http://en.wikipedia.org/wiki/electronegativity#electronegativities_of_the_elements the redder atom , the higher electronegativity , and the more likely it is for the atom to gain electrons and become negatively charged . that is especially true for light halogens ( fluorine , chlorine ) and oxygen . that is partly why glass - with lots of $sio_2$ - likes to get negatively charged in the triboelectric effect . even sulfur ( 40% of ebonite ) has a higher electronegativity than e.g. carbon and hydrogen that are abundant in the fur which is why fur loses electrons and becomes positively charged . of course , the actual arrangement of the atoms in the molecules matters , too . so this overview of the periodic table was just an analogy , not a reliable way to find out the results of the triboelectric effect .
how should one imagine a particle without dimensions - like an electron - to spin ? you do not . if you want to imagine , then you think classically and it is just a particle spinning . . . thinking like that does not give you any other insight of what spin really is ( an intrinsic angular momentum , behaving like an [ orbital ] angular momentum ) . how should one imagine a particle with spin 1/2 to make a 360 turn without returning to it is original position ( the wave function transforms as :  ) just imagine it . . . no big deal . again , classically this is not possible , but quantically it is . when spin is not a classical property of elementary particles , is it a purely relativistic property , a purely quantum-mechanical property or a mixture of both ? the spin of elementary particle is a pure quantum mechanical effect . edit : see @j . c . comment . relativity also plays a role . any other interpretation/calculation requires things like commutator , symmetry properties and group theory . the parallel between " real spinning " and " spin " ( which is just a name ) comes from the fact that the spin operator needed to account for properties of elementary particles behaves ( = has the same definition , based on commutators ) like orbital angular momentum operator . this again comes from symmetry properties of . . . nature . the goal of quantum physics is to provide a way to calculate properties . if you want to calculate or go deeper in the problem , then you do not need this classical interpretation .
let me try a more down to earth example : let 's say i formulate a law " i can kick with my leg in front of me without getting hurt . " this law is indeed true in many cases , but in some cases it is not because there is a wall right in front of me and my leg kinda hurts after kicking . that is , the world is not everywhere the same . say i come to the same place in fifty years after the wall has been torn down and try to kick in front of me - i do not get hurt . so i can surely say that the world is not everytime the same . thus , the same action at different places and different times gives different results . so what the heck are these physical laws saying that the world is the same everywhere and everytime ? that is the funny thing , this is how fundamental physical laws are defined and formulated - as the stuff that applies at any time and at any place . if it does not apply the same at different points of space and time , it is not considered a physical law . that is , we postulate that such a thing as physical laws does exist and then try to find them . we have to rid every situation of what is different and find and document repeating patterns . the difference of every point of space and time is expressed in things such as initial conditions and sources . if for example tomorrow we found out that the " laws of gravity have changed " or apply differently for different objects , we would not throw away physics , we would say that our previous law was only effective , " local " and seek a deeper law that would describe this change of our local effective law . we could however also postulate a source which affects the phenomena we observe . a case analogous to the latter is the phenomenon of dark matter - we consider it either as an unknown source of gravitation or a consequence of a modified gravitational law . in fact , this happens all the time throughout the history of science , but mainly through looking at different scales - we found different laws applying to the case of an atom and that of a macroscopic object . we did not say there are just particular laws , we formulated a unified theory called quantum mechanics . however , the most amazing thing is that every time so far , we have been able to find such a new law unifying more and more phenomena and have managed to reduce again and again the amount of types of sources and initial conditions ( even though bearing more and more specific information about the situation ) . this is just what we find , there is hardly any explanation to it . nevertheless , there are some reservations towards the amazing might of the physical-laws-method . this follows from the fact that from a certain point of accuracy in the microscopic world , we have not managed to find any kind of pattern in the results . our inability to predict this certain precision of results is reflected by the statistical nature of quantum mechanics and may as well be the final limit to the realm of physical laws . the last remark is that we would require to somehow explain the sources and initial conditions - why in the end space and time really are different at different points . this is a conundrum mainly discussed in the science trying to encompass the whole of space and time , cosmology . and sincerely , nobody really knows if the scientific method can even in principle answer the questions of the type why the world is the way it is . nevertheless , we presume the world will in our eyes continue to get organized into more and more wide and entangled patterns by more and more unified theories with more and more particularly arranged sources and configurations .
see http://www.sciencephoto.com/media/100845/enlarge for an absolutely awesome picture of a fly 's foot . it has two claws that can grip any irregularities . for smooth surfaces like glass it has a pad covered in tiny hairs , and each hair is coated in tiny oil drops . the capillary attraction of the oil drops holds the tiny hairs , and therefore the fly , to the surface .
as a theory of everything includes a theory of all particular things , it would be good if you start by learning about the theories that need to be unified . this means first some quantum mechanics , something about classical electromagnetism , something about special anf general relativity , then some quantum field theory , something about quantum electrodynamics , something about the standard model . so you should look at the recommendations for introductions to these subjects available at our book recommendations . unless you are content with such books as ''the elegant universe'' http://en.wikipedia.org/wiki/the_elegant_universe where you learn the buzzwords without a deeper understanding .
when you focus light from the sun you are actually creating an image of the sun . if the focal length of the lens is $f$ the radius of the image is given by : $$ r = \frac{r_s}{d_s} f $$ where $d_s$ is the distance to the sun and $r_s$ is the radius of the sun . the fraction $r_s/d_s \approx 10^{-3}$ , so if you choose a lens with a focal length of 10cm the radius of the image is about 0.1mm ( assuming the lens is perfect ) . the intensity of sunlight is around 1kw per square metre - the exact value depends on latitude , season , time of day , cloud cover , etc , etc so let 's just take 1kw/m$^2$ as a representative figure . all the light falling on your lens is being concentrated into the 0.1mm radius image of the sun , so if the radius of your lens is $r_l$ the power per unit area in the image is : $$ i = \left ( \frac{r_l}{0.1 mm} \right ) ^2 1kw/m^2 $$ so if the lens radius is 5 cm , which seems a fairly standard size for a lens , then the power per unit area in the focussed image of the sun is about 250mw/m$^2$ or 250,000 times the intensity of sunlight on the earth . that is why it is hot ! of course the total power is not very great , because even though the focussed light is very intense the area of the 0.1 mm image is only about $3 \times 10^{-8}$ square metres . the total power is just the area over which light is being collected ( the area of your lens ) times 1 kw . a bigger lens will capture more sunlight and focus more power .
the quantity you are describing ( $s ( \omega ) $ ) is called the power spectral density . i can not say if your interpretation of the power spectral density in this case is mistaken or not , because i have not encountered it myself . but in context of a stationary physical process , the power spectral density describes how the total power in the system is distributed over various frequencies . here power is taken to mean the square of the signal , i.e. , if the signal is $f ( t ) $ , then the total power is given as $$ p = \frac{1}{t} \int_{0}^{t} |f ( t ) |^2 dt$$ the function that you have described is actually the fourier transform of the autocorrelation function , a result given by the weiner-khinchin theorem . if your $x ( t ) $ is a stochastic variable , then $s ( \omega ) $ is the spectral power distribution for that variable/process .
in principle , yes , the ultimate source of energy for a tidal power plant is earth 's rotational energy , so these plants are slowing down the earth 's rotation . by conservation of angular momentum , that means they are pushing the moon further away as well , although i would not phrase it as being due to " waves in the gravitational field , " as that expression suggests a different phenomenon . the earth 's rotational kinetic energy is about $10^{29}$ j , and the world uses something like $10^{22}$ j/year , so you could power the entire world for millions of years before you had run out of rotational energy . to answer your numerical question , you should work out the rotational kinetic energy of the earth now , and also when the day is 25 hours long . the difference between those is the total energy required . the way to figure out the rotational kinetic energy is ${1\over 2}i\omega^2$ . here $i$ is the earth 's moment of inertia , which is about $0.4mr^2$ where $m$ and $r$ are earth 's mass and radius . $\omega$ is the earth 's rotation rate in radians per second -- that is , $2\pi$ over the time for one rotation .
you are correct , the equation is generalizable to higher dimensions . the equation you gave is simply the sum of the various forms of energy . in the equation $$de = \frac{\mu}{2} dx \left ( \frac{\partial y}{\partial t}\right ) ^2 + \frac{t}{2} dx \left ( \frac{\partial y}{\partial x} \right ) ^2 $$ the first term on the right hand side is the kinetic energy and the second term is the elastic potential energy . as mike said , the kinetic energy is just $p^2 / 2m$ . the elastic potential energy at any location on the string is given by adding up all of the force it took to get that piece there ( given by hooke 's law ) ; that is $$f_s ( x ) = -t y ( x ) $$ $$u = \int_0 ^y ty ( x ) dy = \frac{t}{2} y ( x ) ^2$$ the total energy is then just the sum of the kinetic and potential energies with appropriate modification using the mass density times a infinitesimal length as the mass . when you move to higher dimensions , you have to account for that in the kinetic and potential energies . the kinetic energy of a portion of the surface is the square of the momentum of that portion ( $mv$ ) divided by the mass of that portion . the potential energy is the spring constant of the membrane divided by 2 , multiplied by the square of the displacement from zero .
i found a good paper that can help you . however , due to copyright issues i cannot put the spectra here . try to get this article : " the distribution of energy in the visible spectrum of daylight " . a . h . taylor and g . p . kerr . j . opt . soc . am . 31 no . 1 , pp . 3-8 ( 1941 ) . also available here ( pdf ) .
yes , the mixing is happening in the eyes and brain ; no , an rgb mix of yellow is not the same as a pure yellow frequency ; but our eyes will see it as the same . the eyes have 3 ( or 2 , if you are colour-blind ) types of colour sensors , each of which responds with a different signal profile - each peaks at a particular frequency , and trails off for frequencies that differ from that . the brain merges the signals from those 3 ( or 2 ) different sensors , to make sense of the colour signals to create a single colour signal , and it can not tell whether that was a balanced combination of red and green , or a pure yellow frequency . see also this answer to a previous , related question . that explains most colours we see . except for when we see a combination of red and blue , with no signals in between . there is not a colour in the spectrum for that - the colours in between red and blue all feature higher signals in the middle , around green . to have signals from red and blue but not green , does not map to the spectrum . and our brain will not show a combination of two or more colours for a single point , it always maps a single point to a single colour . so our brain creates a new colour , not on the spectrum , for a combination of red and blue . hence , purple pigments are not real , in that sense - purple is the brain 's interpolation of red + blue + no green . purple is just a pigment of our imagination .
now that we have seen the higgs boson , all the particles predicted by the standard model have been discovered . the penultimate particle to be discovered was the tau-neutrino at fermi-lab in 2000 . the antepenultimate particle to be discovered was the top quark , also at fermi-lab in 1995 . for a complete timeline , see e.g. this wiki page . there are , of course , theories that predict even more particles ( so-called beyond the standard model physics ) , but no such particles have been directly observed .
not with currently known physics . sending a signal to yourself would require sending a faster than light signal to something that could retransmit it to you and that was itself moving almost at the speed of light . this works because of the way that we cut spacetime into $e^3$ slices . the transmitter moving at close to the speed of light would have its slices tilted relative to ours in such a manner that a superluminal particle could move into its future while moving into our past . the book " it is about time : understanding einstein 's relativity " by n . david mermin has the clearest exposition of this that i have seen . light , by definition , moves at the speed of light . so light could not be used . we would require not only particles with superluminal velocities but also the ability to control those particals to send a signal with them .
the field inside the sphere will not be zero if it is hollow and there is a point charge in the hollowed out part . the field will be zero in the conductor , because the field is always zero in a conductor in electrostatics . what you might be refering to is that the field will be zero inside the hollow sphere if it is charged , because the charges will distribute symmetrically over the sphere .
a timelike vector connects two events that are causally connected , that is the second event is in the light cone of the first event . a spacelike vector connects two events that are causally disconnected , that is the second event is outside the light cone of the first event . in that sense , the timelike vector can be considered to define a four-velocity direction of an observer and thus the time axis of that observer ( to be a four-velocity it should be normalised ) . on the other hand a spacelike vector can be considered as defining a space axis ( a spatial direction ) of an observer . seen like that the two vectors define a time interval or a length on the appropriate inertial frame , as vladimir said .
your formula for the generating function is wrong in a crucial sense . the formula you are after reads $$ \frac{1}{|\mathbf{r}-\mathbf{r}'|}=\sum_{l=0}^\infty \frac{r_&lt ; ^l}{r_&gt ; ^{l+1}}p_l ( \cos\theta ) . $$ note that the numerator and denominator of each term are powers of the lesser and greater , resp . , of $r$ and $r'$ . for the multipolar expansion your question asks about , you need the point of evaluation to be further away from the centre than the disk radius $r$ , which means that the powers will be in $r/p$ instead of $p/r$ . that will solve the divergence issues on the integrals . i find the key to quickly seeing which way the expansion will go is seeing it as a taylor series ( for fixed $\theta$ ) in the relevant small paramenter .
yes , the field is infinite , but it is only log divergent near the plate , so that it is hard to see the divergence numerically . you can see this easily by solving the problem of a uniformly charged infinite plate , which is a 2d problem . here the charges are uniform along the negative real axis , where the 2d space is imagined to be the complex plane . this problem can be understood as follows : the 2d electrostatic field of a point charge at the origin , written as a map from c to c can be written in complex form as : $$ e_x + i e_y = \frac{z} { 2\pi |z|^2 }= \frac{1}{ 2\pi \bar{z}}$$ it points radially outwards . this is a pure antiholomorphic function , except at the origin . it is more familiar to deal with holomorphic functions , so conjugate it ! $$ e_x - ie_y = e ( z ) = {1\over 2\pi z} $$ now you want to superpose all the charges on the negative z axis . this is a simple integral : $$ \int_{-\infty}^0 e ( z-a ) da = - {1\over 2\pi} \log ( z ) $$ where i threw away an infinite additive constant ( you should think of this as calculating the potential difference between the point z=1 and any other point ) . this is the function with a given fixed cut discontinuity on the negative real axis . so at the point $r , \theta$ , the electric field is $$ e_x = - {\rho\over 2\pi} \log ( r ) $$ $$ e_y = {\rho \theta\over 2\pi} $$ where i have restored the $\rho$ . the part in the y-direction is finite , as your intuition says--- the discontinuity is equal to the charge density ( this charge density is the cut discontinuity of the electric field analytic function , which is a way of making it obvious that the electric field goes as the log--- the log function as a constant cut discontinuity ) . the divergent part is in the x direction , and it is only invisible in the bulk disk because when you get close to the surface , you have cancellations from the left and from the right that wash it out . so the answer is yes , the e field is log divergent , but only the component in the plane of the disk pointing out . the solution of the disk asymptotes to the plane solution in the near disk limit .
there are three main feasible ways of detecting a black hole : gravitational lensing : the strong gravitational attraction of a black hole bends space time and the light coming from nearby stars ( nearby in the sense of being in the same are in our sky ) is bent inwards . there are a few well known distorsion types due to gravity , but mainly we can see galaxies , which are more or less elliptical , bent into pancake shapes . accretion disks and jets : as the black hole " sucks in " dust and other similar matter from nearby space , the matter is accelerated at relativistic velocities and it emits x-rays as it goes to die inside the event horizon . stars orbiting black holdes : if a star is orbiting a black hole , it will appear to be orbiting empty space ( since we can not basically see a black hole directly ) . other ways , like hawking radiation , are only theoretically possible for now -we could maybe be able to see old mini black holes " popping " but it is not really clear how that would happen exactly and none has been seen so far .
yup , this is true that the pressure is too small , but the true explanation is not justified yet . nevertheless the common sense is that there is a lubricating film of water or at least anomalous ice . for an overview , see : http://lptms.u-psud.fr/membres/trizac/ens/l3fip/ice.pdf
let 's look to your own statements . first , time derivative after transformations is not equal to an " old " derivative : for $\mathbf r ' = \mathbf r - \mathbf u t = \mathbf r - \mathbf u t ' \rightarrow \mathbf r = \mathbf r ' + \mathbf u t'$ $$ \partial_{t'} = ( \partial_{t'}\mathbf r ) \partial_{\mathbf r} + ( \partial_{t'}t ) \partial_{\mathbf t} = ( \mathbf u \cdot \nabla ) + \partial_{t} , \quad ( \mathbf u \cdot \nabla ) = u^{i}\partial_{x_{i}} . $$ so , with $\nabla ' = \nabla$ , " bianchi " equations transforms to $$ ( \nabla \cdot \mathbf b' ) = 0 , \quad [ \nabla \times \mathbf e ' ] + \frac{1}{c}\partial_{t}\mathbf b ' + \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf b ' = 0 . \qquad ( . 1 ) $$ second , the form of $\mathbf {e}' ( \mathbf r ' , t' ) , \mathbf b ' ( \mathbf r ' , t' ) $ is not equal to $\mathbf e ( \mathbf r , t ) , \mathbf b ( \mathbf r , t ) $ . let 's use the lorentz force expression , $$ \mathbf f = q\mathbf e + \frac{q}{c} [ \mathbf v \times \mathbf b ] . $$ it does not depend on acceleration , so the statement that $\mathbf f ' = \mathbf f$ under galilean transformation is true . it means that $$ \mathbf e + \frac{1}{c} [ \mathbf v \times \mathbf b ] = \mathbf e ' + \frac{1}{c} [ \mathbf v ' \times \mathbf b' ] . $$ by using galilean transformation for speed , $\mathbf v ' = \mathbf v - \mathbf u$ , this equation can be rewritten as $$ \mathbf e + \frac{1}{c} [ \mathbf v \times \mathbf b ] = \mathbf e ' + \frac{1}{c} [ \mathbf v \times \mathbf b ' ] - \frac{1}{c} [ \mathbf u \times \mathbf b' ] , \qquad ( . 2 ) $$ so the statement that $\mathbf e = \mathbf e ' , \quad \mathbf b = \mathbf b '$ is not correct . so you need to find expressions $\mathbf e ' $ and $\mathbf b'$ via $\mathbf e $ , $\mathbf b$ . by rewriting $ ( . 2 ) $ , $$ \mathbf e + \frac{1}{c} [ \mathbf v \times ( \mathbf b - \mathbf b ' ) ] = \mathbf e ' - \frac{1}{c} [ \mathbf u \times \mathbf b ' ] , $$ in a reason of arbitrary $\mathbf u $ you can get the solution : $$ \mathbf b ' = \mathbf b , \quad \mathbf e ' = \mathbf e + \frac{1}{c} [ \mathbf u \times \mathbf b ] . $$ by substitution these equations to $ ( . 1 ) $ you will get $$ ( \nabla \cdot \mathbf b ) = 0 , \quad [ \nabla \times \mathbf e ] + \frac{1}{c} [ \nabla \times [ \mathbf u \times \mathbf b ] ] + \frac{1}{c}\partial_{t}\mathbf b + \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf b = [ \nabla \times \mathbf e ] + \frac{1}{c}\partial_{t}\mathbf b = 0 , $$ because for $\mathbf u = const$ $$ [ \nabla \times [ \mathbf u \times \mathbf b ] ] = \mathbf u ( \nabla \cdot \mathbf b ) - ( \mathbf u \cdot \nabla ) \mathbf b = - ( \mathbf u \cdot \nabla ) \mathbf b . $$ so the first pair of maxwell 's equations is clearly invariant under galilean transformations . let 's look to the other pair of maxwell 's equations : $$ [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e = 0 , \quad ( \nabla \cdot \mathbf e ) = 0 . \qquad ( . 3 ) $$ by using an expressions which were derived above , you can rewrite $ ( . 3 ) $ as $$ [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e ' - \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf e ' = $$ $$ = [ \nabla \times \mathbf b ] - \frac{1}{c}\partial_{t}\mathbf e - \frac{1}{c} ( \mathbf u \cdot \nabla ) \mathbf e - \frac{1}{c^{2}}\partial_{t} [ \mathbf u \times \mathbf b ] - \frac{1}{c^{2}} ( \mathbf u \cdot \nabla ) [ \mathbf u \times \mathbf b ] = 0 , $$ $$ ( \nabla \cdot \mathbf e ) + \frac{1}{c} ( \nabla \cdot [ \mathbf u \times \mathbf b ] ) = ( \nabla \cdot \mathbf e ) -\frac{1}{c} ( \mathbf u \cdot [ \nabla \times \mathbf b ] ) = 0 . $$ the requirement of galilean invariance of second equation leads to te state that $\frac{1}{c} ( \mathbf u \cdot [ \nabla \times \mathbf b ] ) $ , which is not true in the general case . analogically reasoning can be used for the first equation . so the second pair of maxwell 's equations is not invariant under galilean transformations .
you know that swinging is oscillation of the total energy between kinetic and potential energy . the kinetic energy in the bifilar pendulum is mainly generated by the horizontal elongation of the bar . in the following we always consider oscillations with the same horizontal elongation angle amplitude . furthermore , we approximate the oscillation of the horizontal elongation as sinusoidal . under these assumptions the maximal kinetic energy is proportional to the squared oscillation frequency . the potential energy is proportional to the lifing of the bar . for one and the same horizontal elongation angle the bar is lifted the higher the larger d is . therefore , for larger d you have higher maximal potential energy in the oscillation with constant amplitude therefore you have higher maximal kinetic energy which implies higher oscillation frequency . this is the scheme , now you can try to get approximate formulae if you like . note : the pendulum would even work with d=0 where there is no lifting of the bar . but in this case the working principle is different . with d=0 you must consider torsion stiffness of the thread and the potential energy is stored in the torsion of the thread . for our considerations above we have neglected this effect .
imagine that the oscillator is a swing and you are the force pushing it . the phase shift is nothing more than the statement that you have to act differently than the swing . obviously , you should not push in the exact opposite direction ( which rules out a phase shift of $\pi$ ) . imagine the red line being the amplitude of the swing , and the green line is your push strength . what the optimal phase shift of $\pi/2$ ( which is equivalent to switching $\sin$ with $\cos$ ) tells you is that you change your pushing direction every time the swing is at its maximum amplitude . so , instead of pushing the strongest when the swing amplitude is the biggest , you push the strongest when the amplitude is 0 and do not push at all when the amplitude is at its maximum .
the $q_x$ and $q_y$ transitions are electronic excitations in the conjugated $\pi$ orbitals of the bchl a molecule . they involve two different sets of conjugated bonds . the $q_x$ involves a shorter chain of conjugated bonds so it occurs at a higher energy/frequency . i could not find a really good diagram to show which bonds are involved in the in the $q_x$ and which in the $q_y$ excitations , but figure 1 in this paper has a reasonable illustration .
according to universe today , for the intergalactic medium they state a figure of only one hydrogen atom per cubic meter . as a point of comparison , the university of california , san diego quotes an interstellar density of 1 atom per cubic centimeter . as to why it is important , all this material has photoionization effects on observations . even though it is incredibly diffuse , because temperature is defined by the excitement state of atoms , this intergalactic medium is millions of degrees ! this level of excitation could cause you to get erroneous results if you do not compensate for it . piero madau of clatech has a series of web pages that explains it much better than i can . that probably explains a great deal more than i could here , although it will require quite a few pages of reading .
let 's start from the main question Do electrons move when only one end/pole of the battery is connected ?  they do ! let 's say that you attach a wire to the positive terminal of the battery and this terminal is at " conventional " +5v . imagine a lot of positively charged particles accumulated there , now electrons would move as near them as possible , creating the same +5v potential throughout the wire . a very similar effect takes place in the charging of a capacitor by dc source , in that case too the circuit is never completed but we know that charges flow because that is how the capacitor charges . i could only understand the other question as Does attaching conductors to battery or other sources of electrical energy ionize them?  it would be wrong to say that something like this happens , because when atoms get ionized they have quite a bit of freedom of movement for example $\text{na}^+$ and $\text{cl}^-$ in water are ionized and move freely similarly ions in hot plasma move about freely . while in conductors the concept of free electrons is accepted because electrons rather than completely leaving the atom , get into contact with several atoms and even though they leave a charged specie behind , that specie does not have freedom of movement and hence cannot be called ionized .
i think this test conditions are more about comparability than completely testing any possible road condition . when you test products , such as car suspensions , you design your test to be standardized . a standardized test gives you the possibility to compare results over various models and over time . if every suspension manufacturer would test its suspensions on the worst road in a 50 mile radius around the factory , each manufacturer would end up with its own representation of " worst road conditions " . furthermore , these test conditions would also change over time , as a bad road might deteriorate or even be shut down . another comment , you might have noticed that the amplitude gets smaller for smaller periods . when testing a suspension there is a certain parameter range for the period of the road profile . a very large period would not excite the suspension are the car would follow the road profile like one solid body . if the period gets too small , then the great inertia of the car would damp the excitation away .
imagine a rock on a rope . as you rotate the rope faster and faster , you need to pull stronger and stronger to provide centripetal force that keeps the stone on the orbit . the increasing tension in the rope would eventually break the it . the very same thing would happen with bar ( just replace the rock with the bar 's center of mass ) . and naturally , all of this would happen at speeds far below the speed of light . even if you imagined that there exists a material that could sustain the tension at relativistic speeds you had need to take into account that signal can not travel faster than at the speed of light . this means that the bar can not be rigid . it would bend and the far end would trail around . so it is hard to even talk about rotation at these speeds . one thing that is certain is that strange things would happen . but to describe this fully you had need a relativistic model of solid matter . people often propose arguments similar to yours to show special relativity fails . in reality what fails is our intuition about materials , which is completely classical .
the fastest fully localized process is the evaporation of the smallest black hole worth the time - it takes one planck time or so , $10^{-43}$ seconds . there are other characteristic processes in quantum gravity that take a planck time - the shortest time scale for which the usual spacetime geometry works . however , if you allow changes in collective properties of large objects , there are much shorter times . for example , the mass of the visible universe is $3\times 10^{52}$ kilograms or so . the corresponding energy , via $e=mc^2$ , is $10^{61}$ joules . the periodicity of the corresponding quantum wave , via $e=\hbar\omega$ , is about $10^{-95}$ seconds . so the wave function of the universe periodically changes its phase $10^{95}$ times every second . i guess that you mean local processes , and moreover some processes accessible experimentally . that is a question with no permanent answer . the whole field of particle physics may be classified according to the time scale we can resolve . the time scale you mention is that of atomic physics ; the time scales in nuclear physics are up to 10 orders of magnitude shorter , $10^{-24}$ seconds or so . the lhc is probing time scales that are 4 orders of magnitude shorter than that . in principle , this progress could continue . the " higher energy " we have , the shorter times ( and distances ) we may resolve .
the answer to all questions is no . in fact , even the right reaction to the first sentence - that the planck scale is a " discrete measure " - is no . the planck length is a particular value of distance which is as important as $2\pi$ times the distance or any other multiple . the fact that we can speak about the planck scale does not mean that the distance becomes discrete in any way . we may also talk about the radius of the earth which does not mean that all distances have to be its multiples . in quantum gravity , geometry with the usual rules does not work if the ( proper ) distances are thought of as being shorter than the planck scale . but this invalidity of classical geometry does not mean that anything about the geometry has to become discrete ( although it is a favorite meme promoted by popular books ) . there are lots of other effects that make the sharp , point-based geometry we know invalid - and indeed , we know that in the real world , the geometry collapses near the planck scale because of other reasons than discreteness . quantum mechanics got its name because according to its rules , some quantities such as energy of bound states or the angular momentum can only take " quantized " or discrete values ( eigenvalues ) . but despite the name , that does not mean that all observables in quantum mechanics have to possess a discrete spectrum . do positions or distances possess a discrete spectrum ? the proposition that distances or durations become discrete near the planck scale is a scientific hypothesis and it is one that may be - and , in fact , has been - experimentally falsified . for example , these discrete theories inevitably predict that the time needed for photons to get from very distant places of the universe to the earth will measurably depend on the photons ' energy . the fermi satellite has showed that the delay is zero within dozens of milliseconds http://motls.blogspot.com/2009/08/fermi-kills-all-lorentz-violating.html which proves that the violations of the lorentz symmetry ( special relativity ) of the magnitude that one would inevitably get from the violations of the continuity of spacetime have to be much smaller than what a generic discrete theory predicts . in fact , the argument used by the fermi satellite only employs the most straightforward way to impose upper bounds on the lorentz violation . using the so-called birefringence , http://arxiv.org/abs/1102.2784 one may improve the bounds by 14 orders of magnitude ! this safely kills any imaginable theory that violates the lorentz symmetry - or even continuity of the spacetime - at the planck scale . in some sense , the birefringence method applied to gamma ray bursts allows one to " see " the continuity of spacetime at distances that are 14 orders of magnitude shorter than the planck length . it does not mean that all physics at those " distances " works just like in large flat space . it does not . but it surely does mean that some physics - such as the existence of photons with arbitrarily short wavelengths - has to work just like it does at long distances . and it safely rules out all hypotheses that the spacetime may be built out of discrete , lego-like or any qualitatively similar building blocks .
the thermal radiation associated with some object is typically described in terms of the " black-body " spectrum for a given temperature , given by the planck formula . this formula is based on an idealization of an object that absorbs all frequencies of radiation equally , but it works fairly well provided that the object whose thermal spectrum you are interested in studying does not have any transitions with resonant frequencies in the range of interest . as the typical energy scale of atomic and molecular transitions is somewhere around an ev , while the characteristic energy scale for " room temperature " is in the neighborhood of 1/40 ev , this generally is not all that bad an assumption-- if you look in the vicinity of the peak of the blackbody spectrum for an object at room temperature , you generally find that the spectrum looks very much like a black-body spectrum . how does this arise from the interaction between light of whatever frequency and a gas of atoms or molecules having discrete internal states ? the thing to remember is that internal states of atoms and molecules are not the only degree of freedom available to the systems-- there is also the center-of-mass motion of the atoms themselves , or the collective motion of groups of atoms . the central idea involved with thermal radiation is that if you take a gas of atoms and confine it to a region of space containing some radiation field with some characteristic temperature , the atoms and the radiation will eventually come to some equilibrium in which the kinetic energy distribution of the atoms and the frequency spectrum of the radiation will have the same characteristic temperature . ( the internal state distribution of the atoms will also have the same temperature , but if you are talking about room-temperature systems , there is too little thermal energy to make much difference in the thermal state distribution , so we will ignore that . ) this will come about through interactions between the atoms and the light , and most of these interactions will be non-resonant in nature . in terms of microscopic quantum processes , you would think of these as being raman scattering events , where some of the photon energy goes into changing the motional state of the atom-- if you have cold atoms and hot photons , you will get more scattering events that increase the atom 's kinetic energy than ones that decrease it , so the average atomic ke will increase , and the average photon energy will decrease . ( or , in more fully quantum terms , the population of atoms will be moved up to higher-energy quantum states within the box , while the population of higher-energy photon modes will decrease . ) for thermal radiation in the room temperature regime , of course , the transitions in question are so far off-resonance that a raman scattering for any individual atom with any particular photon will be phenomenally unlikely . atoms are plentiful , though , and photons are even cheaper , so the total number of interactions for the sample as a whole can be quite large , and can bring both the atomic gas and the thermal radiation bath to equilibrium in time . i have never seen a full qft treatment of the subject , but that does not mean much . the basic idea of the equilibration of atoms with thermal radiation comes from einstein in 1917 , and there was a really good physics today article ( pdf ) by dan kleppner a few years back , talking about just how much is in those papers .
light cannot move outwards inside the event horizon . i would guess you are thinking that an outgoing light ray might leave you in the outgoing direction , then slow to a halt and return - hence you would see yourself . however this does not happen . the light leaving you moves inwards not outwards , but since you fall inwards faster than the light does , the light still leaves you ( at velocity $c$ ) and never returns . this is discussed in some detail in the question if you shoot a light beam behind the event horizon of a black hole , what happens to the light ? . to show what happens to you and the light we draw a spacetime diagram . well assume all motion is radial , so the diagram will just show distance from the singularity and time . the trajectory of any object in spacetime is a curve on the diagram called a worldline , and when two objects meet their worldlines intersect . so to show the black hole cannot act as a mirror we draw your worldline and the worldline of the light and show that they only intersect once . the problem is that we cant use the usual coordinates $r$ and $t$ because these are singular at the event horizon . instead we use kruskal-szekeres coordinates $u$ and $v$ . im not going to go into how these coordinates are defined , see the wikipedia article for details , because wed be here all day . the $u$ coordinate is spacelike both outside and inside the event horizon , and likewise the $v$ coordinate is timelike both outside and inside the event horizon . using these coordinates the spacetime diagram of the black hole looks like this : on this diagram the diagonal dashed lines are the event horizon , and the red hyperbola at the top is the world line of the singularity . the blue curve is your worldline as you fall into the black hole . were only interested in the top right half of the diagram  the bottom half shows a white hole and a parallel universe linked by a wormhole ( ! ) but thats a discussion for another day . for our purposes the key feature of this diagram is that light rays follow straight lines with gradient $\pm 1$ . ingoing light rays travel from lower right to upper left ( gradient $-1$ ) while outgoing light rays travel from lower right to upper left ( gradient +1 ) . the worldlines of massive objects have a gradient closer to the $v$ axis than light rays , and the faster the object is travelling the closer its worldline gets to a gradient of $\pm 1$ . now were in a position to answer rijuls question , but lets zoom into the top right bit of the diagram so we can see what happens : at some point after youve crossed the event horizon you shine two light rays , one inwards and one outwards , and these are shown by the magenta lines . remember that light rays always travel at 45 on this diagram , so its easy to draw the worldlines of the light rays because they are just straight lines . your worldline is approximate in the sense that i didnt sit down and calculate it , but it must everywhere be at an angle greater than 45 , and as you accelerate the gradient approaches 45 . so your worldline will look something like the blue line ive drawn , and in any case the exact shape of your worldline doesnt matter for this proof . and with that were done ! the briefest glance at the diagram shows that your worldline and the worldlines of the light rays can only intersect at one point , i.e. the point you shine the light rays inwards and outwards . so the black hole cant act as a mirror . note also that both light rays end up intersecting the worldline of the singularity so even the light ray directed outwards ends up falling into the singularity .
indeed , without assuming it from first principles as in bogoliubov formulation , the invariance property of $s$ operator you mention holds when the interaction lagrangian does not include derivatives of fields like in qed . that is a consequence of dyson 's expansion in interaction picture . when , as said above , the interaction lagrangian does not include derivatives of fields , one has : $${\cal h}_i = -{\cal l}_i$$ so that $$s = \sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) \:d^4x_1\cdots d^4x_n\: . $$ it is worth noticing that $\hat{\cal l}_i ( x ) $ includes only free field operators as we are dealing with the so-called interaction picture , so everything is explicitely known , commutation relations of field operators in particular . since the lagrangian functions are scalars , we have : $$u_\lambda\hat{\cal l}_i ( x ) u^\dagger_\lambda = \hat{\cal l}_i ( \lambda^{-1} x ) \qquad ( 1 ) $$ moreover , in view of free fields commutation relations one also has:$$ [ \hat{\cal l}_i ( x ) , \hat{\cal l}_i ( y ) ] =0 \qquad ( 2 ) $$ if $x$ and $y$ are spacelike separated . the fact that $s$ is invariant under the action of orthochronous lorentz group is quite obvious $$u_\lambda su_\lambda^\dagger = \sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int u_\lambda t [ \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) ] u_\lambda^\dagger \:d^4x_1\cdots d^4x_n$$ $$=\sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t [ u_\lambda \hat{\cal l}_i ( x_1 ) u_\lambda^\dagger\cdots u_\lambda \hat{\cal l}_i ( x_n ) u_\lambda^\dagger ] \:d^4x_1\cdots d^4x_n$$ $$=\sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t [ \hat{\cal l}_i ( \lambda^{-1} x_1 ) \cdots \hat{\cal l}_i ( \lambda^{-1}x_n ) ] \:d^4x_1\cdots d^4x_n$$ $$=\sum_{n=0}^{+\infty} \frac{i^n}{n ! } \int\cdots \int t [ \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) ] \:d^4x_1\cdots d^4x_n$$ due to the lorentz invariance of the measure $d^4x$ . the identity : $$u_\lambda t [ \hat{\cal l}_i ( x_1 ) \cdots \hat{\cal l}_i ( x_n ) ] u_\lambda^\dagger= t [ u_\lambda \hat{\cal l}_i ( x_1 ) u_\lambda^\dagger\cdots u_\lambda \hat{\cal l}_i ( x_n ) u_\lambda^\dagger ] $$ is consequence of the definition of $t$-ordinator , ( 1 ) , and ( 2 ) for spacelike separated arguments , considering all cases concerning the time order of $x_1 , \ldots , x_n$ and the fact that $\lambda$ does not change the temporal order for causally related arguments as it belongs to the orthochronous subgroup . for more complicated theories the result is not obvious and it could be false in its elementary formulation based on canonical quantization , excluding the case of gauge theories , where it can be proved separately . regarding weinberg 's statement about lorentz covariance of the $s$ matrix and lorentz invariance of $s$ operator , if i understood well the definition , i think that it works like this . let us start from the full ( interacting ) theory . there are vectors $\psi^\pm_{\{p_i\}}$ describing states which , at late time ( respectively $t\to +\infty$ and $t\to -\infty$ ) evolve like free particle states with momenta $\{p_i\}$ . the correspondingly associated free states , always evolving in accordance witht he free theory , are indicated by $\phi_{\{p_i\}}$ . the $s$-matrix is the matrix of elements : $$\langle \psi^+_{\{q_i\}}|\psi^-_{\{p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: . \qquad ( 3 ) $$ in the rhs the $s$ operator takes place . in view of it , the scattering process is completely described in terms of free states . to say that the $s$ matrix is lorentz covariant should mean ( as far as i understand ) : $$\langle \psi^+_{\{\lambda q_i\}}|\psi^-_{\{\lambda p_i\}}\rangle = \langle \psi^+_{\{q_i\}}|\psi^-_{\{p_i\}}\rangle\quad \forall \lambda \in o ( 3,1 ) \uparrow\: , \forall \{\lambda q_i\}\: , \{\lambda p_i\}\: . $$ from ( 3 ) , it immediately entails : $$ \langle \phi_{\{\lambda q_i\}}|s\phi_{\{\lambda p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: . \qquad ( 4 ) $$ if $u_\lambda$ is the unitary representation of $o ( 3,1 ) \uparrow$ on free states , so that $\phi_{\{\lambda p_i\}}= u_\lambda \phi_{\{p_i\}}$ , we therefore have : $$\langle u_\lambda \phi_{\{ q_i\}}|s u_\lambda\phi_{\{p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: , $$ that is $$\langle \phi_{\{ q_i\}}|u^\dagger_\lambda s u_\lambda\phi_{\{p_i\}}\rangle = \langle \phi_{\{q_i\}}|s\phi_{\{p_i\}}\rangle\: , $$ and so : $$\langle \phi_{\{ q_i\}}| \left ( u^\dagger_\lambda s u_\lambda - s\right ) \phi_{\{p_i\}}\rangle = 0\: , $$ since the set of vectors $\phi_{\{p_i\}}$ forms a basis of the hilbert space ( of the free theory ) in view of the asymptotic completeness hypotheses , we conclude that : $$u^\dagger_\lambda s u_\lambda - s=0$$ i.e. $$s=u_\lambda s u^\dagger_\lambda\: , \quad \forall \lambda \in o ( 3,1 ) \uparrow\: . $$ in other words if the $s$ matrix is lorentz covariant , then the $s$ operator is lorentz invariant .
i do not have an answer to the question " why would one want to consider such crazy stuff in physics ? " since i do not know much physics , but as a mathematics student i do have an answer to the question " why would one want to consider such crazy stuff in mathematics ? " what physicists call grassmann numbers are what mathematicians call elements of the exterior algebra $\lambda ( v ) $ over a vector space $v$ . the exterior algebra naturally arises as the solution to the following geometric problem . say that $v$ has dimension $n$ and let $v_1 , . . . v_n$ be a basis of it . we would like a nice natural definition of the $n$-dimensional volume of the paralleletope defined by the vectors $\epsilon_1 v_1 + . . . + \epsilon_n v_n , e_i \in \{ 0 , 1 \}$ . when $n = 2$ this is the standard parallelogram defined by two linearly independent vectors , and when $n = 3$ this is the standard paralellepiped defined by three linearly independent vectors . the thing about the naive definition of volume is that it is very close to having really nice mathematical properties : it is almost multilinear . that is , if we denote the volume we are looking at by $\text{vol} ( v_1 , . . . v_n ) $ , then it is almost true that $\text{vol} ( v_1 , . . . v_i + cw , . . . v_n ) = \text{vol} ( v_1 , . . . v_n ) + c \text{vol} ( v_1 , . . . v_{i-1} , w , v_{i+1} , . . . v_n ) $ . you can draw nice diagrams to see this readily . however , it is not actually completely multilinear : depending on how you vary $w$ you will find that sometimes the volume shrinks to zero and then goes back up in a non-smooth way when really it ought to keep getting more negative . ( you can see this even in two dimensions , by varying one of the vectors until it goes past the other . ) to fix that , we need to look instead at oriented volume , which can be negative , but which has the enormous advantage of being completely multilinear and smooth . the other major property it satisfies is that if any of the two vectors $v_i$ agree ( that is , the vectors are linearly dependent ) then the oriented volume is zero , which makes sense . it turns out ( and this is a nice exercise ) that this is equivalent to oriented volume coming from a " product " operation , the exterior product , which is anticommutative . formally , these two conditions define an element of the top exterior power $\lambda^n ( v ) $ defined by the exterior product $v_1 \wedge v_2 . . . \wedge v_n$ , and choosing an element of this top exterior power ( a volume form ) allows us to associate an actual number to an $n$-tuple of vectors which we can call its oriented volume in the more naive sense . if $v$ is equipped with an inner product , then there are two distinguished elements of $\lambda^n ( v ) $ given by a wedge product of an orthonormal basis in some order , and it is natural to pick one of these as a volume form . alright , so what about the rest of the exterior powers $\lambda^p ( v ) $ that make up the exterior algebra ? the point of these is that if $v_1 , . . . v_p , p &lt ; n$ is a tuple of vectors in $v$ , we can consider the subspace they span and talk about the $p$-dimensional oriented volume of the paralleletope given by the $v_i$ in this subspace . but the result of this computation should not just be a number : we need a way to do this that keeps track of what subspace we are in . it turns out that mathematically the most natural way to do this is to keep in mind the requirements we really want out of this computation ( multilinearity and the fact that if the $v_i$ are not linearly independent then the answer should be zero ) , and then just define the result of the computation to be the universal thing that we get by imposing these requirements and nothing else , and this is nothing more than the exterior power $\lambda^p ( v ) $ . this discussion hopefully motivated for you why the exterior algebra is a natural object from the perspective of geometry . since einstein , physicists have been aware that geometry has a lot to say about physics , so hopefully the concept makes a little more sense now . let me also say something about how modern mathematicians think about " space " in the abstract sense . the inspiration for the modern point of view actually derives at least partially from physics : the only thing you can really know about a space are observables defined on it . in classical physics , observables form a commutative ring , so one might say roughly speaking that the study of commutative rings is the study of " classical spaces . " in mathematics this study , in the abstract , is called algebraic geometry . it is a very sophisticated theory that encompasses classical algebraic geometry , arithmetic geometry , and much more , and it is in large part because of the success of this theory and related commutative ring approaches to geometry ( topological spaces , manifolds , measure spaces ) that mathematicians have gotten used to the slogan that " commutative rings are rings of observables on some space . " of course , quantum mechanics tells us that the actual universe around us does not work this way . the observables we care about do not commute , and this is a big issue . so mathematically what is needed is a way to think about noncommutative rings as " quantum spaces " in some sense . this subject is very broad , but roughly it goes by the name of noncommutative geometry . the idea is simple : if we want to take quantum mechanics completely seriously , our spaces should not have " points " at all because points are classical phenomena that implicitly require a commutative ring of observables , which we know is not what we actually have . so our spaces should be more complicated things coming from noncommutative rings in some way . grassmann numbers satisfy one of the most tractable forms of noncommutativity ( actually they are commutative if one alters the definition of " commutative " very slightly , but never mind that . . . ) , and even better it is a form of noncommutativity that is clearly related to something physicists care about ( the properties of fermions ) , so anticommuting observables are a natural step up from commuting observables in order to get our mathematics to align more closely with reality while still being able to think in an approximately classical way .
the key output of the flrw metric is the scale factor $a ( t ) $ as a function of time . from this we can calculate the time derivative $\dot{a} ( t ) $ ( which is what the red shift measures ) then check whether or not it satisfies the equation : $$ \left ( \frac{\dot{a}}{a}\right ) ^2 = \frac{8\pi g}{3} ( \rho_{radiation} + \rho_{matter} + etc ) $$ where the etc includes dark energy and anything else you may wish to throw in . so basically the test is to measure the redshift as a function of distance . the problem is that this is extraordinarily hard to do on the scales where the matter distribution is homogeneous . there are various approaches being tried such as baryon acoustic oscillations and properties of galaxy clusters but it is still early days .
the short answer is : angular momentum conservation . angular momentum equates to the product of moment of inertia times angular ( rotational ) velocity . when a gas or dust cloud contracts under the influence of gravity and forms a more compact cosmic body , any nonzero total angular velocity will increase due to the moment of inertia decreasing .
to find the bound states for the potential $$v ( x ) ~=~\left\{\begin{array}{ccc}ae^{cx} and \text{for} and x&gt ; 0 , \\ \infty and \text{for} and x\leq 0 , \end{array} \right . $$ where $a , c&gt ; 0$ are two positive constants , one should solve the time-independent schrdinger eq . with the two boundary conditions $$ \psi ( x=0 ) ~=~0 \qquad \text{and} \qquad \lim_{x \to \infty}\psi ( x ) ~=~0 . $$ this boundary value problem does only have solutions for certain discrete values of the energy $e$ .
the clock runs slightly faster when the disk can spin . when the disk is attached rigidly to the pendulum , its center translates in space , but it also spins around its own center ( compared to gravity ) . the disk experiences net acceleration and net angular acceleration . when the disk can spin freely , the center of the disk still accelerates , but the disk has no angular acceleration because there is no torque on it . ( uniform gravity cannot produce a torque about the center of mass , and the force between the pendulum and the disk has no moment arm . ) this slightly reduces the energy change in the disk for a given movement of the pendulum because there is no energy going in and out of the spin of the disk . the net result is a lower effective moment of inertia in the pendulum+disk system . the torque on the pendulum due to gravity is the same , so the clock swings slightly faster when the disk is free to spin . variable definitions : $l$ length of pendulum $m$ mass of pendulum $\theta$ angle of pendulum with vertical $l$ distance between center of disk and pivot of pendulum $r$ radius of disk $m$ mass of disk $\phi$ angle the disk has rotated relative to gravity $\omega = \dot{\phi}$ $g$ gravitational acceleration we will find the lagrangian of the system and compare the two cases . when the disk is fixed , $\phi = \theta$ . when the disk is free , $\phi$ is a new degree of freedom . we need to find the kinetic and potential energies of the pendulum and the bob in terms of the variables above . assume the pendulum is a uniform rod pivoted at one end . then its moment of inertia is $ml^2/3$ . its kinetic energy is $\frac{1}{2} i \dot{\theta}^2 = ml^2\dot{\theta}^2/6$ . the potential energy of the pendulum is $mg$ times the height of its center of mass above rest , which is $l\cos\theta /2$ . the kinetic energy of the disk comes from the motion of its center of mass and from its rotation . it is $1/2 ( mv^2 ) + 1/2i\omega^2$ . the velocity is $v = l\dot{\theta}$ . the moment of inertia is $i = 1/2 m r^2$ . the total kinetic energy is then $1/2m ( l^2 \dot{\theta}^2 + 1/2 r^2 \dot{\phi}^2 ) $ the potential energy of the disk is $mg$ times its height above equilibrium height , or $mgl\cos\theta$ . putting these together , the lagrangian for the clock with spinning bob is $$\mathcal{l} = \dot{\theta}^2\left ( \frac{ml^2}{6}+ \frac{ml^2}{2}\right ) + \dot{\phi}^2\frac{mr^2}{4} + g\cos\theta\left ( \frac{ml}{2} + m l\right ) $$ which we abbreviate $$\mathcal{l} = \dot{\theta}^2 \frac{i_\theta}{2} + \dot{\phi}^2 \frac{i_b}{2} + g\cos\theta m_{eff}$$ there are two different cases to explore . if the bob of the pendulum is fixed , then $\phi = \theta$ because the bottom of the disk points the same way as the pendulum . that gives us $\dot{\phi} = \dot{\theta}$ . the lagrangian becomes $$\mathcal{l}_{fixed} = \dot{\theta}^2 \frac{i_\theta + i_b}{2} + g\cos\theta m_{eff}$$ or , setting $i_\theta + i_b = i_{fixed}$ , it is just $$\mathcal{l}_{fixed} = \dot{\theta}^2 \frac{i_{fixed}}{2} + g\cos\theta m_{eff}$$ when the disk is free to swing , $\phi$ is a degree of freedom on its own . the lagrangian has no cross-terms involving both $\theta$ and $\phi$ , meaning their evolutions are independent ; these degrees of freedom are decoupled . $\phi$ is a cyclic coordinate so the angular momentum of the disk is conserved . the evolution of theta is the same as in a lagrangian where $\phi$ does not exist . $$\mathcal{l}_{free} = \dot{\theta}^2 \frac{i_\theta}{2} + g\cos\theta m_{eff}$$ the only difference between $\mathcal{l}_{fixed}$ and $\mathcal{l}_{free}$ is that the moment of inertia is slightly larger in $\mathcal{l}_{fixed}$ . the potential energy terms are the same . if we make a substitution in the free-bob case , introducing a new time coordinate defined by $$t&#39 ; = t \sqrt{\frac{i_\theta}{i_{fixed}}}$$ the lagrangians take exactly the same form , except that they are expressed in different time coordinates . this means the free-bob pendulum will run in the same way , but faster by the factor $$\frac{\nu_{fixed}}{\nu_{free}} = \frac{t}{t&#39 ; } = \sqrt{\frac{i_{fixed}}{i_\theta}} = \sqrt{\frac{ml^2/3 + m ( l^2 + r^2/2 ) }{ml^2/3 + ml^2}}$$ where $\nu$ indicates a period . for tractability , we might look at some limit , such as $l = l$ , $m = m$ , $l&gt ; &gt ; r$ . the factor by which the free-disk clock is faster becomes $$1 + \frac{1}{3}\left ( \frac{r}{l}\right ) ^2$$
negative resistance is not uncommon . you see it in arc lamps , too . for your motor ( used as a generator ) , i would guess that it is most efficient at higher current , possibly having field coils and not permanent magnets . the voltage from a generator comes from moving a wire through a magnetic field . with field coils in series , the magnetic field is generated by the current flowing through the ( generator ) . there is probably a small residual permanent field in the iron , so you get some voltage even when there is no current . i bet if you could run up to higher currents , eventually you would get max power from the generator , and start seeing voltage go down again . i am going on this being a ' universal ' motor you have salvaged from somewhere . read about universal or series wound motors at the motor wiki : http://en.wikipedia.org/wiki/electric_motor i found a discussion group with one sensible ( to my mind ) answer amongst the cruft ( see engineertony 's reply ) here : http://cr4.globalspec.com/thread/77573/how-to-make-a-generator-from-a-universal-motor my own take is , go for it ! it kinda works , it looks like you can get real power from it , though the voltage is all over the map , you will want to regulate it .
there is hardly a book covering all physics , but for particular subjects there is some . for example : jammer : the conceptual development of quantum mechanics . whittaker : a history of the theories of aether and electricity .
yes , the photon energy has to be equal to the total energy of the atom before the transition ( including the kinetic energy ) minus the total energy of the atom after the transition ( including the kinetic energy ) . in practice , the kinetic energy of the atom is negligible . the mass of a nucleus is at least 1 gev/$c^2$ or so ( the mass of the proton ) while the photon 's energy is a billion times smaller , 1 ev/$c^2$ or so . this implies that the atom ( mostly nucleus ) gets the momentum kick of order $p=$1 ev/$c$ which means that its kinetic energy is of order $p^2/2m\sim ( 1ev ) ^2/gev\sim 10^{-9}ev$ . this recoil effect therefore modifies the calculated transition energy ( photon energy ) by something like one billionth of the original value which is lower than many other subleading corrections . but yes , it is measurable . these recoil energies become more important in the case of gamma rays emitted by nuclei in nuclear reactions and their proper incorporation is an important part of the pound-rebka experiment verifying general relativity .
this is a deep mystery , nobody knows for sure . it is possibly related to establishing an elastic wave profile along the bottom which is in a dynamic steady-state with the jostling of the contact points , but you need good measurements to make a theory , and good measurements are difficult because of the issue of solid-solid sticking . the research field which studies questions of this sort is moderately active still , and was very active twenty years ago . it is called depinning . in depinning , there are models which partially explain this effect , although whether these models are relevant for a real solid-on-solid interface remains to be seen . friction phenomenology is counter-intuitve sliding friction is approximately constant . this is unusual , because physical quantities tend to go to zero smoothly . for example , the force on a spring is zero at position zero , and it is linearly increasing as a function of displacement . similarly , the friction force for a slow moving object in a viscous fluid is zero at zero velocity , and is linear in the velocity . this is the general expectation that the friction depend analytically , or at least differentiably , in a small parameter . solid-on-solid friction violates this expectation . another way of saying why a constant friction force is counterintuitive is the belief that there should be a unique steady state motion at each force . if the microscopic contacts are doing some sort of markov chain , by general principles of probability , it should have a unique steady state distribution at any fixed force . two different steady states must be separated enough so that they cannot fluctuate into each other . if this is true , the force must determine the velocity . but this is not true if the force is constant as a function of the velocity--- you can keep an object sliding stably at any velocity with the same force . so the general expectation is that the friction force is not really completely constant , but slowly varying with the velocity , and that there is a point of statistical non-analyticity , an interesting phase transition , at zero velocity . why constant friction force ? the friction force times the velocity tells you the energy lost to heat per unit time , the work done by friction . this work is the constant friction force times the velocity . this means that friction will remove an equal increment of energy in equal distance . you can understand this rule heuristically as follows : the loss in energy is due to an elastic deformation in the solids relaxing . in order to slide to each position , the solid will rearrange its shape slightly to fit into the microscopic best-stick points on the matching surface , and stick to some bump or dirt on the surface . each rearrangement of the solid produces sound waves which carry away energy , and transform it into heat . each sticking and unsticking event produces a given amount of sound , and the sticking and unsticking is a property of the surface only . so that to slide a given distance , you need an average number of sticking/unsticking events , and the friction energy loss is equal to the number of such events times the average energy lost to sound per event . this model explains why there is a roughly the same amount of work done per unit distance moved . this model of sticking and unsticking in response to forcing is called " depinning " in the condensed matter literature , and it was widely studied in the 1990s . narayan fisher model solid on solid friction is only the most familiar of a host of phenomena which involve sticking and slipping when something is pushed forward . the traditional examples listed in papers on depinning are * charge density waves * pulling pinned vortices in superconductors * crack propagation , with starting and stopping the list of applications for some reason usually excludes solid-on-soild friction , even though this is the most familiar application . here is a classic depinning model : consider a position function defined on a two dimensional grid . you can think of the position function as representing the displacement of the surface of contact of the two solids . there is a constant force on every site , and there is an additional elastic force on each site , which is positive when the neighbors are further along and pull the site forward , and negative when the neighbors have not caught up yet , and hold it back . the total force is the constant $f$ plus $h ( u ) +h ( d ) +h ( l ) +h ( r ) - 4 h ( c ) $ where u , d , l , r are the four neighbors , h is the position function , and c is the center point . then there is a sticking force at each point $s ( x ) $ , which is a random number between 0 and 1 ( the details of the distribution are not so important , it could be a gaussian of unit width too , just do not make the sticking distribution a powerlaw ) . the law of motion is that wherever the total force is bigger than the sticking force , the site moves forward by one unit . this model has a second order phase transition at a critical force from a stationary state to a moving state . past the phase transition , the velocity goes as $ ( f-f_c ) ^\alpha$ , where f_c is the critical force and $\alpha$ is a critical exponent . because the transition is second order , this model does not explain static friction . nor does it produce a constant friction with velocity , the force goes up with velocity . but it does produce a critical force which is nonzero , and determined by the sticking force distribution in steady state . fisher 's overshoots the model above fails to account for static friction , and its analog in other depinning models . to fix this , fisher introduced the concept of a stress overshoot . the idea here is that right after a site depins and hops , it has made sound-waves all around it which jostle the site for a while , and these sound waves momentarity increase the force randomly in various directions . this means that the mean elastic force does not have to overcome the pinning force by itself , it can get some momentary help from the transient soundwave profile . the model for this is that right after a site hops , on the next time step , there is a parameter m called the overshoot parameter , which weakens the on-site pinning force , and that of the nearest neighbors . the total force only has to exceed the pinning force minus m in order to get the site to hop again . this model takes into account the local stress profile , and fisher 's physical intuition led him to propose that the transition in this model will be first order , not second order , because there should be hysteresis . the reason is simple : while the object is moving , it feels the overshoots from the transient stresses , and when it stops it does not , because the sound modes die away . so it should take more force to get something to move than to keep in moving once it starts . this means that you have to increase the force to more than the amount required to maintain the motion in order to start the motion , and this is a hysteresis loop . the hysteresis loop is usually a property of first order transitions . phony hysteresis fisher 's physical intuition is correct in natural models , but the original conclusion that the transition is first order is not correct . to explain this , the statistical theory of the steady state motion is a renormalization group fixed point , and the m perturbation is much like a mass term--- it adds a little bit of inertia to a site . a site which hopped tends to hop again . mass terms in viscous systems are generically irrelevant perturbations . they do not affect the statistical description at long wavelengths in any way . but these perturbations do produce hysteresis in some models , so there seems to be a contradiction with the general principles of renormalization group theory . the resolution of this mini-paradox is a little subtle . it is the subject of this paper http://arxiv.org/abs/cond-mat/0301495 ( nonfree version is slightly different in terminology , but not in content : http://prl.aps.org/abstract/prl/v92/i25/e255502 , i coauthored this with jennifer schwarz ) . i will run through the argument in the paper , which shows that the transition is hysteretic--- you need more force to get the thing started than to keep it moving . the transition is still second order , and in the same universality class as the fisher narayan model . these two conclusions are not contradicting . the demonstration is easiest by considering a series of models , each of which is progressively less obvious , but each of which have the same behavior . model 1: global overshoot model this model is not physical at all , it is just used for the sake of mathematical argument . consider the discrete narayan fisher hopping model on a lattice ( the stick-force lattice model described above without overshoots ) , and add to it an overshoot force which is global and nonadditive--- whenever at least one site somewhere has hopped , all sites feel an overshoot force m . when the system is moving , this means that the pinning force distribution is just shifted over by a constant m , and the steady state is exactly the same as the narayan fisher model with the shifted distribution . when the system is stopped , you need to make the force m units bigger than normal to get one site to hop , but the moment it does , the force on every site goes down by m , so you can quickly drop the external force by $m-\epsilon$ and keep it moving . so there is exactly m units of hysteresis in this model , but because of the stupid global nature of the force , it is manifestly obvious that this model is completely equivalent to the narayan fisher model , and it has the same second order phase transition . i lobbied to call this behavior " phony hysteresis " , because it is so fake . the hysteresis loop is not an indication of a first order phase transition , it is just a stupid thing overlayed on top of a fundamentally second order transition . prl 's referees did not like phony hysteresis , but this is i what i will call it below . model 2: local nonadditive overshoot model the local nonadditive overshoot model says that when a site hops , it feels m units of overshoot force , and its neighbors do too , on the next time step . this force is nonadditive , meaning that it is the same m if one neighbor hops or if all four neighbors hop . this model is superficially different from the global overshoot model , but they are actually the same in steady state . when you have a steady state motion , the only sites that hop on time t are those whose local environment has changed , and these are the sites whose neighbors have hopped . these sites feel the overshoot force . so when the steady state is set up , the hopping sites all feel the overshoot . further , the sites which do not hop felt the overshoot at the last time they or something around them moved , so that their stability at that time means that they do not move even if you put the overshoot on them too . so you might as well apply a nonadditive m overshoot to all the sites . it makes no difference in steady state . this proves the equivalence of the nonadditive model to the global model . you conclude that the nonadditive model has exactly m units of phony hysteresis , just like the global model . but the nonadditive model is local , so phony hysteresis is actually happens in realistic things . the transition is still second order , and completely rigorously equivalent to narayan fisher model 3: additive stress overshoot now you get to fisher 's original overshoot model , where the overshoot force is additive , and equal to m times the number of neighbors that hopped on the last timestep ( where neighbor includes yourself if you hopped then too--- this is crucial ) . in this case , you can separate out the model into two parts : m units of nonadditive stress overshoot , which is felt by every site whose neighborhood changed the last timestep . an additional stress overshoot equal to m times the number of hopping neighbors minus 1 . you analyze the two contributions separately . the second overshoot can be simulated directly , and it has a second order phase transition in the same universality class as the narayan fisher model . so this behaves as expected from renormalization theory , it has no effect on the continuum limit . the first contribution just tacks on exactly m units of phony hysteresis on top of the no-hysteresis second model . so the additive local overshoot model , the realistic model , has m units of phony hysteresis disguising a completely fine second order transition point . this is a realistic model with static friction greater than dynamic friction . it is not a correct model , because forces in a solid are nonlocal . complications fishers original model had m units of overshoot stress on the neighbors , but no overshoot stress on the site that just hopped . this is because you do not think that a site which just hopped would need to hop again . while this is true , the double-hops are rare , the lack of the self-overshoot ruins the proof of the phony hysteresis theorem , so you do not have phony hysteresis , and the system eventually settles down to a nonhysteretic steady state . but it takes forever because the system is so close to the system with true phony hysteresis . this led to much confusion . is static friction phony hysteresis ? the phony hysteresis means that the transition going down in force is second order , but you see a hysteresis going up , just because you did not set up the overshoots . the overshoots are models of sound waves in the bottom of the material , which locally help push the material , to overcome the sticking points . these soundwaves always fall off in amplitude slower than elastic deformations . so the overshoots are always important in a larger range than the elasticity . this is exactly the domain where you get phony hysteresis , of magnitude equal to the minimum of the overshoot stress on the moving filament of active sites . the filament is narrow , and the soundwaves are broad , so this minimum should be nonzero . because of this , i do believe that the static friction is the phony hysteresis . this means that the transition to zero velocity when going down in force is the true statistical stick-slip dynamics of depinning , while you need an extra force to get the thing moving because the soundwaves have not been set up yet . this is not a complete answer , because the actual magnitude of the hysteresis loop has not been estimated , and there is no real demonstration that the minimum overshoot on the active filament is nonzero . further , this predicts that the force for sliding friction should go up with a nonzero critical exponent as a function of the velocity , although when the exponent small as $1.6$ you can model the force as effectively constant , especially since the range of variations is defined by the enormous scale of the speed of sound in the material .
in general , the elasticity of a collision is dependent on the properties of the colliding objects . in a perfectly elastic collision , no kinetic energy is dissipated , which means the collision creates no heat , no sound , etc . in a perfectly inelastic collision , the maximum possible amount of kinetic energy is dissipated as heat , sound , etc . this corresponds to the two particles sticking together after the collision . in real life , most collisions are neither perfectly elastic nor perfectly inelastic , but rather somewhere in the middle . ( one major exception to this is gas molecule collisions , which are perfectly elastic . ) some objects collide nearly perfectly elastically , such as billiard balls or steel ball bearings , while others collide nearly perfectly inelastically , such as balls of putty or mud . without knowing the specific properties of the colliding objects ( such as their elasticity , etc . ) , it is impossible to predict how elastic a collision will be .
the fact that $k_b \ln \omega$ coincides with entropy $s$ defined in thermodynamics comes from microcanonical ensemble . there are many resources out there on microcanonical ensemble , for example , this . after you come to the conclusion that $$\beta=\left ( \frac{ \partial \ln \omega }{ \partial u }\right ) _{n , v}$$ fully characterizes thermal equilibrium , you know that it must be a function of thermodynamic temperature , and thermodynamic temperature alone , by virtue of zeroth law of thermodynamics . so $\beta=f ( t ) $ . compare this to $$\frac{1}{t}=\left ( \frac{ \partial s }{ \partial u }\right ) _{n , v}$$ and you get that s must be the function of $\ln \omega$ . the remaining question is the exact form of this function , and you already derive it from special cases .
i only know of this problem being discussed for scalars . in ads , there is a unique so ( d-1,2 ) invariant vacuum , so your question does not apply . in de sitter space , on the other hand , you have a one-parameter family of ds invariant vacua , labeled by a complex parameter alpha . switching between these vacua can be accomplished by what is called a mottola-allen transform , and this corresponds to perturbing the cft by some marginal deformation ( at least in three-dimensional ds ) . see bousso , maloney , and strominger for details . i am not really sure if these alpha vacua are so physical though . taking the standard euclidean vacuum , which is the analytic continuation of the vacuum from the sphere , corresponds to demanding that the fields start as plane waves , which sounds pretty reasonable . also , harlow and stanford show that analytically continuing the infrared wavefunction from ads gives the ds wavefunction with euclidean initial conditions , so the alpha vacua are in some sense not as " preferred " .
yes , this is correct . in 2 dimensions , so : $$\int_{c_1}\vec f\left ( r_1\right ) \cdot \mbox{d}\vec s=\int_{c_2}\vec f\left ( r_2\right ) \cdot\mbox{d}\vec s$$ now , would it make sense , ; if the gravitational force on two points on the very same circle , were not the same ? certainly not ! formally , to say it is the same , we could say it is an $so ( 2 ) $ symmetry . so , $$2\pi r_1 f_1=2\pi r_2 f_2$$ $$r_1 f_1=r_2 f_2$$ $$f_2=\frac{r_1f_1}{r_2}$$ so we have it that $f_2$ is inversely proportional to $r_2$ . to see how this exact form , $f=g\frac{m_1m_2}{r}$ ( in 2 dimensions , not 3 ) arises , c.f. the near - last part of my answer here , but apply it to 2 - dimensions instead .
i would like to add a bit of mathematical detail the ( correct ) statements by djbunk . let a scalar function $f$ be given ( let 's not restrict ourselves to the electric potential ) . for any unit vector $\mathbf n$ , we can define the directional derivative $d_\mathbf{n}$ of the function $f$ in the direction $\mathbf n$ as follows : $$ d_\mathbf{n}f ( \mathbf x ) = \mathbf n\cdot\nabla f ( x ) . $$ the directional derivative gives the rate of change of the scalar function $f$ in the direction of the unit vector $\mathbf n$ . notice that $$ \mathbf n\cdot \nabla f ( \mathbf x ) = |\nabla f ( \mathbf x ) |\cos\theta $$ where $\theta$ is the angle between $\mathbf n$ and $\nabla f ( \mathbf x ) $ , so the directional derivative is maximized when $\theta = 0$ , and is minimized when $\theta = -\pi$ . in other words ; the the rate of change of a scalar function $f$ at a point $\mathbf x$ is positive and greatest in magnitude in the direction of the gradient of $f$ at $\mathbf x$ . this confirms bjbunk 's statements .
general relativity is a mathematical model that relates the curvature of spacetime to an object called the stress-energy tensor . in many cases the stress-energy tensor is dominated by mass and you can simply consider the curvature as being related to the mass . however this is not always true as i will mention below . anyhow , we can put any numbers we want into the stress-energy tensor and then calculate the curvature . if we put in a positive mass we get ( in the newtonian limit ) the usual law of gravitation , but we could put in a negative mass and we had get a repulsion just as you do in electrostatics . matter with a negative mass is usually referred to as exotic matter and is a favourite trick for building weird objects like the alcubierre faster than light drive or wormholes . however just because we an put exotic matter into einstein 's equation does not mean it is physically reasonable to do so . no-one has ever observed exotic matter , no-one has ever come up with a convincing theoretical reason for it to exist . so while we can not prove exotic matter does not exist few of us think it does - though we had all love to be able to build a faster than light drive ! even though we have never observed exotic matter , we have ( we think ) observed dark energy . this is not matter , and does not have a negative mass , but it does cause a gravitational repulsion .
both the continuous spectrum and the characteristic lines are used ; however , in some cases a filter such as aluminum can be used to remove low energy xrays that are not needed , so as to reduce xray exposure , as explained here : http://www.cyberphysics.co.uk/topics/medical/xray.html
the particles of light waves - the photons - have the rest mass $m_0$ equal to zero . however , at the speed of light , $v=c$ , the total mass $$ m= \frac{m_0}{\sqrt{1-v^2/c^2}} $$ is increased to an indeterminate form , $0/0$ , which should be evaluated as a finite number . the photons - and everything else - carry the total mass that is proportional to the total energy via the famous $e=mc^2$ relation . yes , this mass may be measured . for example , uranium nuclear power plants burn the uranium and reduce its mass by 0.1 percent or so because the waste products ( the nuclei ) are actually a little bit lighter . this energy may be completely transformed to the radiation coming from light bulbs - and the light from these light bulbs carry 0.1 percent of the uranium mass away . this mass is a source of gravitational field and adds inertia to boxes with this light etc . sound is different . the speed of sound is much smaller than the speed of light . while " phonons " in low-temperature condensed matter physics - particles of sound - are analogous to photons in many respects , and $e=mc^2$ still applies , the same is not true for sound waves in the air etc . because the temperature of the air is nonzero , the " ground state " - the lowest-energy state at fixed conditions , with the minimum number of " sound quanta " or " phonons " - is not really unique . instead , there are many states of the air " without any sound " which correspond to chaotic configurations of the air molecules . so one can not consistently divide the energy of the air to the energy of its ground state and the energy of the phonons . but of course , if you produce some loud sounds , they will carry lots of energy in the air and the mass of the air will inevitably increase by $m=e/c^2$ which is , well , not too high because $c^2$ is a large number .
what you should be comparing is the time it takes for direct propagation ( which i would guess is the " energy transmitted without total internal reflection" ) versus the time it takes for guided propagation at the critical angle , which is the longest delay/broadening you will get out of the fibre at the other end . modes at angles higher than $\theta_c$ will leak energy into the substrate and will not make it to the other end , so you do not need to consider them . your error is in the calculation of the times each beam travels . for each length $l$ that the direct beam travels , the critical-angle beam travels a length $l'$ given by $$ \frac l{l'}=\sin ( \theta_c ) . $$ thus , if the direct beam travels a total length $d$ , the critical-angle beam will travel a length $d'=\frac{d}{\sin ( \theta_c ) }&gt ; d$ . since they are both travelling in the same medium , the real index of refraction is the same , and hence their travel times are $$ t_\text{direct}=\frac{d}{v}=\frac {dn_f}{ c}\text{ and } t_\text{c . a . }=\frac{d'}{v}=\frac{dn_f}{c}\frac1{\sin ( \theta_c ) } . \tag0 $$ the critical angle will be given by the total internal reflection limit at the boundary with either the substrate or the cover , whichever has a larger index of refraction . assuming wlog that $n_s&gt ; n_f&gt ; n_c$ , the critical angle is given by $\sin ( \theta_c ) =n_s/n_f$ . this means that the time delay is $$ \delta t=t_\text{c . a . }-t_\text{direct}=\frac{dn_f}{c}\left ( \frac{n_f}{n_s}-1\right ) =d\frac{n_f}{n_s}\frac{n_f-n_s}{c} . \tag1 $$ the inverse of this is the bandwidth of the fibre , given by $$ \frac{d}{\delta t}=\frac{n_s}{n_f}\frac{c}{n_f-n_s} . \tag2 $$ this is pretty close to the result you were asked for , $\frac{c}{n_f-n_s}$ . for one , it has a factor of $d$ , which is eliminated in your result , effectively , by calculating the ' bandwidth per unit length ' of the fibre , $1\text{ km}/\delta t$ , in the understanding that the actual bandwidth will vary inversely with the actual length . this makes a lot of sense : longer fibres make for longer distances travelled by the different beams and therefore longer delays . this is to be expected and should be factored out . other than that , some of the prefactors do not quite match up . for one , i must note that one of the equalities that you write as exact is not really so : $$ \frac{{2c{n_s}}}{{{{\left ( {{\rm{an}}} \right ) }^2}}}=\frac{2cn_s}{n_f^2-n_s^2}=\frac{2n_s}{n_f+n_s}\frac{c}{n_f-n_s} , $$ and this only equals $\cfrac{c}{n_f-n_s}$ in the limit where $n_f$ and $n_s$ are really quite close together . similarly , in that limit , $\cfrac{n_s}{n_f}\approx 1$ , so in that sense all three answers match . a bit further along those lines , the factor of $\cfrac{2n_s}{n_f+n_s}=\cfrac{2 n_s/n_f}{1+\frac{n_s}{n_f}}$ from the $1/\rm{an}^2$ answer sits kind of " in between " the exact answer , $n_s/n_f$ , so it is not so bad an approximation . i would therefore sum up the situation as saying that $$ \frac{d}{\delta t} =\frac{n_s}{n_f}\frac{c}{n_f-n_s} \approx \frac{{2c{n_s}}}{{{{\left ( {{\rm{an}}} \right ) }^2}}} =\frac{2n_s}{n_f+n_s}\frac{c}{n_f-n_s} \approx \frac{c}{n_f-n_s} , $$ where each approximation accumulates a slight loss of accuracy , from left to right , though of course everything tends to equality as $n_s/n_f\to1^-$ . thus , if it is convenient for some reason to include the numerical aperture in the formula for the bandwidth , then it makes some sense to put it in the picture .
yes , to some extent . once you choose which of the electron or positron is to be considered the normal particle , then that fixes your choice for the other leptons , because of neutrino mixing . similarly , choosing one quark to be the normal particle fixes the choice for the other flavors and colors of quarks . but i can not think of a reason within the standard model that requires you to make corresponding choices for leptons and quarks . in particle terms , you can think about it like this : say you start by choosing the electron to be the particle and the positron to be the antiparticle . you can then distinguish electron neutrinos and electron antineutrinos because in weak decay processes , an electron is always produced with an antineutrino and a positron with a normal neutrino . then , because of neutrino oscillations , you can identify the other two species of neutrinos that oscillate with electron antineutrinos as antineutrinos themselves , and in turn you can identify the muon and tau lepton from production associated with their corresponding antineutrinos . in terms of qft , the relevant ( almost- ) conserved quantity is the " charge parity , " the eigenvalue of the combination of operators $\mathcal{cp}$ .
momentum in this case is : $p = h / \lambda$ for a massless particle . the momentum is related to the de broglie wavelength of the particle with this formula . if you plug it in the equation you have stated you will get back the energy equation of a massless particle : $e = hc/\lambda = hf$
it should be obvious why the dirac operator is important in physics because of fermions . in mathematics , one could mention the following incomplete list . atiyah-singer index theorem for the ( twisted ) spin complex , see e.g. , nakahara , geometry , topology and physics , 1990 ; or berline , getzler and vergne , heat kernels and dirac operators , 2004 . connes 's noncommutative differential geometry . schrdingerlichnerowicz formula . kostant 's cubic dirac operator . for further information , see also nlab .
refer to the nice complement on coherent states in the book by cohen-tannoudji , diu and lalo , volume 1 . it starts off defining coherent states as neither of the ones you mention , and then derives all properties . to answer the question , if you start with definition 2 , you can easily show 1 , and then from 2 , 3 . first expand the exponential using baker-campbell-hausdorff formula : $$ e^{\alpha a^\dagger -\alpha^* a}=e^{\alpha a^\dagger}e^{-\alpha^* a}e^{\frac{-1}{2}|\alpha|^2 [ a^\dagger , -a ] } $$ and let it act on the vacuum state $|0\rangle$ to get $$ |\alpha \rangle = e^{-|\alpha|^2/2}e^{\alpha a^\dagger}e^{-\alpha^* a}|0\rangle \\ = e^{-|\alpha|^2/2}e^{\alpha a^\dagger}|0\rangle \\ = e^{-|\alpha|^2/2} \sum_{n=0}^{\infty}\frac{\alpha^n}{\sqrt{n ! }} |n\rangle $$ now that you have the expression for $|\alpha \rangle$ in terms of states you already know , you can operate $a$ on it to find that it is indeed an eigenstate of the lowering operator , showing that definition 2 implies definition 1 . property 3 follows from finding $\langle x^2 \rangle$ and $\langle p^2 \rangle$ for this state , by expressing the operators in terms of $a$ and $a^\dagger$ , a fairly standard exercise .
the state for generator a can be written more formally as , $$ | a \rangle = \sqrt{\frac{1}{4}} | 1 \rangle_{a} + \sqrt{\frac{1}{4}} | 2 \rangle_{a} +\sqrt{\frac{1}{4}} | 3 \rangle_{a} +\sqrt{\frac{1}{4}} | 4 \rangle_{a} \ , , $$ where $ | 1 \rangle_{a}$ represents the generator $a$ in " state " number 1 . the probablity of getting , for example , number 3 from generator $a$ is derived as , $$ |\langle 3| a \rangle|^{2} = \left| \sqrt{\frac{1}{4}} \langle 3 | 1 \rangle_{a} + \sqrt{\frac{1}{4}} \langle 3| 2 \rangle_{a} +\sqrt{\frac{1}{4}} \langle 3| 3 \rangle_{a} +\sqrt{\frac{1}{4}} \langle 3| 4 \rangle_{a} \right|^{2} = \frac{1}{4} $$ analogously $b$ is more formally given by $$ | b \rangle = \sqrt{\frac{1}{3}} | 2 \rangle_{b} +\sqrt{\frac{1}{3}} | 3 \rangle_{b} +\sqrt{\frac{1}{3}} | 4 \rangle_{b} \ , . $$ a " superposition " of these generators could be their direct sum , if we are considering the states or numbers $a$ and $b$ generate to be " different " . $$ | a \rangle \oplus | b \rangle = \sqrt{\frac{1}{4}} | 1 \rangle_{a} + \sqrt{\frac{1}{4}} | 2 \rangle_{a} +\sqrt{\frac{1}{4}} | 3 \rangle_{a} +\sqrt{\frac{1}{4}} | 4 \rangle_{a} + \sqrt{\frac{1}{3}} | 2 \rangle_{b} +\sqrt{\frac{1}{3}} | 3 \rangle_{b} +\sqrt{\frac{1}{3}} | 4 \rangle_{b} $$ where it is now understood that the kets $ | \ldots \rangle_{a}$ and $| \ldots \rangle_{b}$ live in the direct sum space $ a \oplus b $ . now the probability that $a$ generates number "2" is given by , $$ | \langle 2_a | a \oplus b \rangle |^{2} = |\langle 2| a \rangle|^{2} = \frac{1}{4} = p ( 2|a ) $$ and the probability that $b$ generates number "2" is given by , $$ | \langle 2_b | a \oplus b \rangle |^{2} = |\langle 2| b \rangle|^{2} = \frac{1}{3} =p ( 2|b ) \ , . $$ just as we had before . if we want the probabilities of each generator given the number "2" we have , $$ p ( a|2 ) = \frac{ p ( 2|a ) p ( a ) }{ p ( 2 ) } \ ; , \ ; p ( b|2 ) = \frac{ p ( 2|b ) p ( b ) }{ p ( 2 ) } $$ now we need $p ( a ) $ , $p ( b ) $ and $p ( 2 ) $ . if we regard the generators to be chosen equally as likely $p ( a ) = p ( b ) $ , we get for the likelihood ratio of the generators , $$ \frac{p ( a|2 ) }{p ( b|2 ) } = \frac{p ( 2|a ) }{p ( 2|b ) } = \frac{3}{4} $$
excellent question ! in short , there are two logical possibilities to explain the data : there is dark matter and a cosmological constant ( standard model ) gravity needs to be modified interestingly , both possibilities have historical precedent : the discovery of neptune ( by johann gottfried galle and heinrich louis darrest ) one year after its prediction by urbain le verrier was a success-story for the dark matter idea . ( of course , after its discovery by astronomers it was no longer dark . . . ) the non-discovery of vulcan was the a failure of the dark matter idea - instead , gravity had to be modified from newton to einstein . ( funnily , vulcan actually was observed by lescarbault a year after its prediction by urbain le verrier , but this observation was never confirmed by anyone else . ) so basically you are asking : are we in a neptune or a vulcan scenario ? and could not the vulcan scenario be more credible ? the likely answer appears to be no . modifications of gravity that seem to explain galactic rotation curves are usually either in conflict with solar system precision tests ( where einstein 's theory works extraordinarily well ) or they are complicated and less predictive than einstein 's theory ( like teves ) or they are not theories to begin with ( like mond ) . besides the gravitational evidence for dark matter , there is also indirect evidence from particle physics . for instance , if you believe in grand unification then you must also accept supersymmetry so that the coupling constants merge in one point at the gut scale . then you have a natural dark matter candidate , the lightest supersymmetric particle . there are also other particle physics predictions that lead to dark matter candidates , like axions . so the point is , there are no lack of dark matter candidates ( rather , there an abundance of them ) that may be responsible for the galactic rotation curves , the dynamics of clusters , the structure formation etc . note also that the standard model of cosmology is a rather precise model ( at the percent level ) , and it requires around 23% of dark matter . there are a lot of independent measurements that have scrutinized this model ( cmb anisotropies , supernovae data , clusters etc . ) , so we do have reasonable confidence in its validity . in some sense , the best evidence for dark matter is perhaps the lack of good alternatives . still , as long as dark matter is not detected directly through some particle/astro-particle physics experiment it is scientifically sound to try to look for alternatives ( i plead guilty in this regard ) . it just seems doubtful that some ad-hoc alternative passes all the observational tests .
a longer popular text why energy conservation becomes trivial ( or violated ) in general relativity is e.g. here : http://motls.blogspot.com/2010/08/why-and-how-energy-is-not-conserved-in.html to summarize four of the points : in gr , spacetime is dynamical , so in general , it is not time-translational-invariant . because it is not , one can not apply noether 's theorem to argue that there is a conserved energy . one can see this in details in cosmology : the energy carried by radiation decreases as the universe expands as every photon 's wavelength is getting bigger ; the cosmological constant has a constant energy density while the volume is increasing , so the total energy carried by the cosmological constant ( dark energy ) , on the contrary , grows ; the latter increase is the reason why the mass of the universe is large &mdash ; during inflation , the total energy grew exponentially for 60+ e -foldings , before it was converted to matter that gave rise to early galaxies . if one defines the stress-energy tensor as the variation of the lagrangian with respect to the metric tensor , which is ok for non-gravitating field theories , on gets zero in gr because the metric tensor is dynamical and the variation &mdash ; like all variations &mdash ; has to vanish because this is what defines the equations of motion . in translationally-invariant spaces such as the minkowski space , the total energy is conserved again because noether 's theorem may be revived ; however , one can not " canonically " write this energy as the integral of energy density over the space ; more precisely , any choice to distribute the total energy " locally " will depend on the chosen coordinate system .
once boiling starts , there is an equilibrium between the liquid and vapor phase . the pressure , temperature and partial molar gibbs energy are equal for each phase so that water molecules have no preference for one phase or the other . that is for intensive variables . however , the total enthalpy of the liquid and vapor is not fixed : it keeps increasing as more energy is brought in . the proportion of vapor to liquid is fixed through this energy balance . if the pressure is maintained externally , say by a piston , there is no possibility for the water bubbles to form at any other pressure than the vapor pressure . however when you boil water at the bottom of a pot , two phenomena alter the situation slightly . the first is the hydrostatic height of the water column , which increases the pressure at the bottom ( where bubbles form ) and raises the equilibrium temperature . the second is the surface tension of water , which increases the pressure required to form a bubble . if the bottom surface is perfectly smooth , nucleation of bubbles is difficult , and the onset of boiling can be delayed to higher temperatures as water remains in a metastable liquid state . note that when boiling water you will often see bubbles form at an early stage and then disappear as the temperature increases : those bubbles are not water vapor but dissolved gases .
comments to the question : under the ordering symbol ( such as , e.g. normal ordering $:\ldots:$ , time ordering $t ( \ldots ) $ , radial ordering ${\cal r} ( \ldots ) $ , etc ) all the operators ( super ) commute , e.g. $$ : \hat{a}\hat{b}: ~=~ ( -1 ) ^{|\hat{a}||\hat{b}|}: \hat{b}\hat{a}: , $$ even if the ( super ) commutator $ [ \hat{a} , \hat{b} ] \neq 0$ is non-vanishing . ordering of a single elementary ( =non-composed ) operator ( such as , e.g. $\hat{a}$ , $\hat{a}^{\dagger}$ , ${\bf 1}$ , etc ) is superfluous .
no . the standard metric for cosmology is given by : $$ds^{2} = - dt^{2} + a ( t ) ^{2}\left ( d^{3}{\vec x}^{2}\right ) $$ where the term inside the parenthees represents the 3-metric of a homogenous three space . as you can see , there is no difficulty with evaluating the age of the universe : $$ t = \int\sqrt{-g}\ , \ , x^{a}y^{a}z^{a}\epsilon_{abcd} = \int dt$$ where the integral is evaluated from the time when $a = 0$ to now .
well everyone at least knows a mnemonic for the planets in the solar system . " my very energetic mother just served us nine pizzas " http://en.wikipedia.org/wiki/planetary_mnemonic also to remember where the minus sign goes in $\sigma_2$ of the pauli matrices i learned : " the minus i rides high on the sigma y " edit : also found this list of some mnemonics http://members.chello.nl/r.kuijt/index-physics.htm
the conserved quantity corresponding to boost symmetry is $$ \int d^3 x ( p_0 x_i - p_i t ) $$ which is the relativistic analogue of $x_{cm} - v_{cm} t$ , the position of the center of mass at $t=0$ . it is quite a useless conserved quantity , and that is why people do not talk about it .
the process is coherent--- the same photon is bouncing off all the atoms at once , and you only get constructive interference when the angle of reflection is equal to the angle of incidence . the condition is that the surface is smooth on the scale of the wavelength of light , so that the light can excite each atom independently , and coherently add up all their contributions . this is feynman 's explanation in qed , i think you just misunderstood it as saying that reflection is assumed--- he just assumed rescattering , and then shows you that it happens in the reflected direction preferentially .
if you want a direct , physical measurement of curvature , here 's a plan that would take lots of money and decades , possibly centuries to set up . perfect for physics ! what you need are three satellites equipped with lasers , light detectors , precision aiming capabilities , and radio communication . these three satellites are launched into space and position themselves far away from each other so that they form the points of a very large triangle . the satellites then turn on two lasers , aiming each one at the other two . each satellite reports to the others when it is receiving the laser light . once the satellites are all reporting that they see the laser light from the others , they measure the angle between their own two laser beams . each satellite transmits this angle back to headquarters on earth . the overall curvature of space can be determined from these angles . if the sum is 180 degrees , like you learned in geometry class , then the space around the satellites is flat . if the sum is more than 180 degrees , then space has positive curvature there , like the surface of a sphere . you can picture the situation on earth by drawing a line from the north pole to the equator , continuing a quarter way around the world along the equator , then heading back to the north pole . you have just drawn a triangle with three 90 degree angles for a sum of 270 . if the sum of the angles is less than 180 , the region of space has negative curvature like a saddle . let 's say the satellites are surrounding a star . since light bends towards masses , the satellites will have to aim away from the star so that the light will bend around the star and hit the other satellites . this means that the angles of the resulting triangle will be larger than normal ( i.e. . , flat space ) , meaning the sum will be greater than 180 degrees . thus , we can conclude that space has positive curvature near a mass . you can probably exactly calculate the curvature from the resulting triangle , but i have not had sufficient math education to do so .
temperature is proportional to the average kinetic energy , not velocity , of the particles . kinetic energy is unbounded ; it goes to infinity as velocity approaches light-speed , proportional to $ ( 1 - v^2/c^2 ) ^{-1/2}$ .
your eye is a second optical system . it re -focuses the diverging rays to produce a real image on the retina . this process is exactly the same thing it does when looking at a nearby ( i.e. . not at effective infinity ) object .
dear humble , good questions . i can not answer all your questions , but : first , the name of the author is feigel , not fiegel , and the paper is also a preprint here : http://arxiv.org/abs/physics/0304100 the controversies about the stress-energy tensor were inevitable . only the total energy and momentum are conserved as a consequence of noether 's theorem , and how they are distributed in space may often be a matter of conventions . in particular , the poynting vector may be defined in several different ways - and various automatically conserved pieces may be added , too . the integrals will not change . but for example , if there are crossing electric and magnetic fields in the vacuum , the usual poynting vector $e\times b$ shows that the energy is flowing somewhere . this flow ends up " circular " if you look globally . feigel 's paper seems to be just a generalization of the casimir effect , with a slightly more complicated arrangement of things . instead of the casimir energy , he wants to play with the energy of the vacuum , and instead of the metallic boundaries , he plays with dielectric liquids . none of these things changes that the total energy and the total momentum are exactly conserved , because of the symmetries . whether the momentum is being extracted from the vacuum is a matter of interpretation . you may also say that it is extracted from a low-frequency electromagnetic wave that was emitted by another object . feigel suggests that there is a relevance of his setup for the abraham-minkowski arguments about the density of electromagnetic momentum in dielectric materials . but one must be careful . the energy carried by the casimir effect is distributed nonlocally - it comes from modes that look like standing waves in between the metallic plates . so the casimir energy is not " naturally " written as an integral of a density , i think . please correct me if i am wrong . i suspect this may be the case of any one-loop constructions of this sort , and feigel 's construction seems to be just a more complicated example . so i suspect the effect of the feigel 's phenomena are not just about changing some local densities either in the abraham or in the minkowski way . finally , i would say that in principle , quantum electrodynamics or the standard model have standardized prescriptions for the stress-energy tensor that should be calculable in any context , including dielectric liquids in crossed electric and magnetic fields . all the best , lm
you can not see clearly underwater for a couple of reasons . one is the thickness of your lens , but the main one is the index of refraction of your cornea . for reference , here 's the wikipedia picture of a human eye . according to wikipedia , two-thirds of the refractive power of your eye is in your cornea , and the cornea 's refractive index is about 1.376 . the refractive index of water ( according to google ) is 1.33 . in water , your cornea bends light as much as a lens in air whose refractive index is $$\frac{1.376-1.33}{1.33} + 1 = 1.034$$ that means you are losing about 90% of your cornea 's refractive power , or 60% of your total refractive power , when you enter the water . the question becomes whether your lens can compensate for that . i did not find a direct quote on how much you can change the focal distance of your lens , but we can estimate that your cornea is doing essentially nothing , and ask whether your lens ought to be able to do all the focusing itself . for a spherical lens with index of refraction $n$ sitting in a medium with index of refraction $n_0$ , the effective focal length is $$f = \frac{nd}{4 ( n-n_0 ) }$$ the refractive index of your vitreous humor is about 1.33 ( like water ) , and the refractive index of your lens , according to wikipedia , varies between 1.386 and 1.406 . let 's take 1.40 as an average . then , plugging in the numbers , the effective focal distance of a spherical eye lens would be five times its diameter . the wikipedia picture of a human eye makes this look reasonable - a spherical lens might be able to do all the focusing a human eye needs , even without the cornea . the problem is that your eye 's lens is not spherical . from the same wikipedia article in many aquatic vertebrates , the lens is considerably thicker , almost spherical , to increase the refraction of light . this difference compensates for the smaller angle of refraction between the eye 's cornea and the watery medium , as they have similar refractive indices . [ 2 ] even among terrestrial animals , however , the lens of primates such as humans is unusually flat . [ 3 ] so , the reason you can not see well underwater is that your eye lens is too flat . if you wear goggles , the light is refracted much more as it enters the cornea - the same amount as normal . if you want to wear some sort of corrective lenses directly on your eye like contact lenses , they should have a refractive index as low as possible . googling for " underwater contact lens " , i found an article about contact lenses made with a layer of air , allowing divers to see sharply underwater .
you give the answer yourself : special relativity forbids any perfectly rigid solid , or more quantitatively , give a bound on the elasticity a solid can have ( $y&lt ; \rho c^2$ ) . if you have a real solid , with nonzero elasticity , you can compute the speed of sound within this solid as a function of the elasticity/stiffness ( see e.g. on wikipedia for the formula ) . if you move an end of your big stick faster thant this speed of sound , it will compress the stick , and this deformation will take time to propagate to the other end . this question is one of the many non-working way of making faster than light communications . you have many of them debunked here . to go into " technical reasons " , if your stick is made of atoms , since the atoms see each other through electromagnetic interaction , there is no way the move of a bunch of atoms of your stick propagates to other of atoms faster than the speed of electromagnetic force . of course , this is a technical reason , which is not valid if your " stick " is made by an exotic material where other forces play a key role ( for example out of neutron-star mater , where nuclear force are important ) , but in this case the violation of relativity would come from the force themselves , which then would allow you to build a ( too ) stiff material .
a fluid motion in a vortex creates a dynamic pressure that is lowest in the center increasing radially ( $p \propto r^2$ ) . the gradient of this pressure that forces the fluid to rotate around the axis . this is usually represented by a vector called vorticity , and defined by $\omega = \nabla \times v$ . in simple terms , this means that the fluid is rotating around a certain point . if you placed a small ball on the flow , you would observe that is would rotate about the center and the direction of vorticity vector is given by the right-hand rule . the formation methods are many . for example , in the wake of an engine , there air has been given rotational momentum and will continue to have vorticity . when two opposite flows meet , they can also form as in planetary systems , like tornadoes or jupiter great red spot
it depends on the situation and interpretation , but it certainly can be the case that your shadow is faster than you . if you imagine a single spotlight shining in front of you , you would cast a shadow behind . if the spotlight moves to behind you , even if you stand still , your shadow will move to the other side . depending on the distance of the object onto which the shadow is cast , this " motion " of your shadow can be very fast indeed ; it could even be faster than the speed of light ! by another interpretation , you are only very slightly faster than your shadow . again , imagine the spotlight in front of you . this time the light stays still , but you hold your arm out and sweep it up . as light from the spotlight passes you , your arm reflects or absorbs some of it ; the wall behind you reflects only the light that falls on it which corresponds to the light that did not fall on your arm , this is the shadow . as you sweep your arm upwards , the pattern on the wall follows the movement . however , because it takes some time for light to travel from the point where it passes you to the wall , the shadow will slightly lag your movement . for real life cases , this lag is miniscule , but if the wall were a sufficiently long distance away , it could be significant . when you consider the added distance from the wall to the observer , the apparant lag time increases .
the answer can be found in the wikipedia page you linked to ! historically , heat had been considered a substance , called caloric . joule 's experiment proved that heat was actually a form of mechanical energy , so was a crucial step towards our modern understanding of the conservation of energy .
first of all , when we say that something is the " classical limit " , it does not mean that all interesting observables such as the pressure have to be finite ( or zero ? ) in the limit . it just means that we only keep the leading order terms in an expansion in $\hbar$ . so if the pressure diverged , there would not be a direct contradiction . second of all , whether the expression diverges depends on what one keeps fixed . you implicitly keep $k , t , c$ fixed . in the si macroscopic units , it is clear that not only $\hbar$ but also boltzmann 's constant $k$ is very small . it is natural to say that $k$  a bridge between microscopic and macroscopic physics  is not kept constant in the classical limit ; $k$ tells you the energy per kelvin per atom . in an answer that just appeared , ron keeps the number density $n$ ( particle number per volume ) constant in the limit . you may also scale $k$ with a power of $\hbar$ so that e.g. the gas constant $r=kn_a$ ( the product of boltzmann 's constant and avogadro constant ) is kept constant . in the classical limit , $n_a$ therefore goes to infinity as $1/k$ . because one mole of the gas has $p=rt/v$ ( all ideal gases obey that ) , and also $p= ( kt ) ^4/ ( hc ) ^3$ , you may cancel divide things to see $r/v=k ( kt/hc ) ^3$ . if you want to keep $r/v$ constant in the limit , you see that $k^4/h^3$ has to be kept constant as well , so $k$ scales like $h^{3/4}$ which is the hidden $h$-scaling you were probably asking for . in this scaling , $p$ according to your formula obviously remains finite in the limit . however , there also exist other ways how to scale quantities when you define " the " classical limit . there is not a single classical limit in a system in which many things may go to zero or infinity in various ways . this is a common theme in modern particle physics . for example , yang-mills theory with the coupling $g$ and $n$ colors has many classical limits . one may e.g. keep $n$ fixed and finite and send $g\to 0$ which is a kind of $\hbar\to 0$ in field theory , the weakly coupled classical limit . alternatively , one may keep $g^2 n$ constant and small and send to $g\to 0$ , while $n$ goes to infinity in the appropriate way ; $g^2 n$ is what more naturally measuring how much " quantum " the theory is . when the ' t hooft coupling $g^2 n$ is kept fixed but large , we seemingly obtain an " anticlassical " limit but this so-called ' t hooft limit may actually be equivalently described as ( leading topology , planar diagrams of ) gravity ( type iib string theory , more precisely ) on an anti de sitter space which is actually another limit of the original gauge theory . an inherently quantum system may have many classical limits .
for me it is axiomatic that machine miles are easier than real miles , but let 's analyze the situation . assume the runner maintains a constant velocity up the hill , or remains stationary in the frame of the gym on the treadmill . in both cases the runner 's acceleration is zero , so we know that her legs must provide a constant force with upward magnitude $mg$ , and the they have to do this against a surface passing by at an angle $\theta$ below the horizontal and moving with a velocity $v$ . the kinematics in the runners frame of reference look the same . this is not the cause of the difference in perceived difficulty . i have always assumed that the difference in difficulty was two fold : wind resistance is not really negligible . the treadmill presents a very uniform reliable surface and the runner need not lift her legs as high to insure non-tripping progress . also modern treadmill are designed to be relatively easy on the knees , and the accomplish this by having a slightly springy feeling which presumably returns some energy to the runner .
each particle of iron in a magnetic field becomes a tiny magnet , following the field lines . a small dipole . this means it will attract at its two poles and be neutral in the middle . this is a way of making a line . the pictures you show are made for the purpose of displaying the field lines . if the whole area were filled with iron dust the effect would be much less visible , though there would be a direction , a type of flow of the dust , due to the gradual orientation of the dipoles along the field lines . the field lines themselves fill the space continuously .
that is exactly what it means . that is the definition of magnetic north . that is true , but also irrelevant . a typical compass does not show you the direction of the tangent line- it shows you the projection of the direction of the magnetic field into the plane of the compass , whatever plane you are holding it in . usually , one holds them parallel to the surface of the earth . neither the compass needle nor the magnetic field will typically point directly towards the actual north pole through euclidean space , but that does not matter , because the direction of " north " that people care about for navigation on the surface of the earth is defined in relation to the spherically curved surface of the earth , in which " straight lines " are actually arcs of great circles . and the projection of the magnetic field into the plane of a compass held parallel to the surface of the earth does point along a great circle that intercepts the pole , even though a straight line from that point will just shoot off into space somewhere . if that same great circle also passes through the true geographic north pole , then the difference between them will be zero . as bowlofred noted , no matter what the difference between true and magnetic north is , that has to happen somewhere , along exactly one great circle . if the magnetic and spin axes are aligned , then it it happens everywhere . the key is understanding the difference between " straight lines " on the surface of the earth , vs . " straight lines " in space . in other words , yes , you are " misinterpreting this idea of direction " . a compass does not point straight at the pole through space , but it does point straight at the magnetic pole along the surface of the earth .
the use of the term " heisenberg limit " is somewhat misleading for outsiders ( that is non-quantum interferometers ) . if we recall the heisenberg uncertainty principle is a limit on simultaneous measurement of two complementary variables . in the case of ( quantum ) metrology one is only interested in the measurement of a single variable to high accuracy , and this does not ( directly ) conflict with the hup . in the case of interferometry the variable of interest is $\delta \phi$ the phase difference between two waves detected in two arms . in basic interferometry there were some limits as to how accurately this could be measured : quantum shot noise : $\delta \phi = 1/n^{1/2}$ heisenberg limit : $\delta \phi = 1/n $ here the n corresponds to how many quanta are required for the given accuracy , so the second is more accurate when it can be achieved , as was eventually done using entangled states , and perhaps squeezed light . if you cannot use these features of qm one gets just the quantum shot noise accuracy . well a few years ago it was noticed that the assumption behind the heisenberg limit calculation was that the hamiltonian was quadratic in its ( key ) variables : this corresponded to the assumption of linearity amongst the measuring quanta . if the hamiltonian could be made non-linear then an improvement on the heisenberg limit would be possible . this interaction between the measuring photons is discussed in the given paper , in arxiv form here .
no , a massive body is able to bend light around it , which is called gravitational lensing . this has been observed multiple times . edit photons are massless . otherwise , they would not travel at the maximum speed , which is called speed of light . keep in mind , that gravitational lensing is not a part of newtonian mechanics . you need general relativity for that . and in the context of general relativity , it is not mass , which exerts gravitation , but energy . and photons clearly carry energy . so , you could produce a gravitational well ( even a black hole ) entirely without massive particles .
we can calculate charge on baloon by assuming it a perfect sphere with uniform charge distribution . $$v=\frac{q}{4\pi\epsilon_0\times r}$$ where $r$ is radius of ballon .
in classical hamiltonian mechanics you have the symplectic matrix $\mathbb{j}$ that has the property that $\mathbb{j}^2 = -\mathbb{i}$ , and it plays a very similar role as $i$ in quantum theories . i have not really seen a situation in classical mechanics of particles where you could not avoid the use of the imaginary unit , but its fundamental symplectic structure ( which , footnote , is structurally analogous to unitarity in many ways ) does inevitably introduce $\mathbb{j}$ to phase space . but as mentioned in the comments , why would you chase around sines and cosines in a linear theory when $e^{i \omega t}$ works just as well ?
yes , indeed . there is a working group called the ' particle data group ' operating the particle data listing where they sum up all experimental results , have short reviews about the physics behind it , also explaining the implicit assumptions made and give bounds on the most popular extensions of the standard model .
i would say that charge is a theoretical prescription describing a way of how a particle interacts with electromagnetic field . since we are talking about a theory that should describe and predict various phenomena , we need to start with definition of fundamental object . if we are talking about newtonian mechanics we face phenomena related to interactions of particles with each other by a direct mechanical contact . we characterize these interactions by force , momentum , etc . fundamental characteristics of a body will be the mass . theoretically , you may consider objects of positive , negative or zero mass in mechanics . however , from experiment we know , that there are no objects with negative mass . the same is true for electrodynamics , where we see objects interacting through a field . now to describe the ability of an object to generate or to fill this field , we introduce the charge . so , as was already said by zeal charge is just a property of an object , same as mass . concerning your second question . firstly , one should note that any object may have any charge irrespective of electrons . however , we know that atom is a complex object composed from electrons , protons and neutrons . hence , in order to figure out charge of an atom we should assign some charges to its fundamental constituents . from experiments we know that electrons , neutrons and protons interact with each other and with electromagnetic field in such a way , that we may define $q_e=-1$ , $q_p=+1$ , $q_n=0$ . now , just by summing charges of constituents of a complex object we can derive its charge . hence , in brief : charge is such theoretical prescription in electrodynamics , that allows to predict electromagnetic phenomena .
the acceleration of an object spinning with angular velocity $\omega$ at a distance $r$ is given by : $$ a = r\omega^2 $$ the angular velocity of the earth is 2$\pi$ radians per day or $7.3 \times 10^{-5}$ per second , and the earth 's radius is $6.378 \times 10^6$ metres , so the acceleration is 0.034 ms$^{-2}$ or 0.0034g . so as a person standing on the surface you would not notice . however the acceleration does affect the shape of the earth . rock is viscous and will flow in response to a force but it does so very slowly . as a result of the earth 's rotation it is radius is about 31km greater at the equator than at the poles . when the rotation stopped it would gradually settle back to a sphere , though it would take a million years or so . the milankovic cycles are at leastly partly due to the fact the earth is not a perfect sphere , and these would stop or at least be changed when the earth settled back to a sphere . assuming you believe the milankovic cycles cause ice ages , one effct of rotationg stopping would be no more ice ages . having said that , stopping rotation would play havoc with the weather as you had get no coriolis force so no jet stream . mind you , there would be no hurricanes either .
( i can not quite comment on the previous post , so i will have to write a new answer ) . if we set the curvature of the earth to be non-negligible in our problem , yes , gravity would slow the baseball down by an extremely tiny amount , but , if we exclude this case ( which , again , i stress to be many orders of magnitude below anything considerable ) , then no , gravity itself does not slow the ball down since the force of attraction ( the direction of the vector of acceleration ) points exactly downwards and contributes nothing to the horizontal component .
i ) let us just consider $1$ dimension for simplicity . ( the generalization to higher dimensions is straightforward ) . then the volume factor $v$ is just a length factor $l$ . ii ) the standard fourier series formulas can be derived from $ ( 12.1.7 ) $ and $ ( 12.1.6 ) $ by taken the length $l$ to be $l=2\pi$ . then $ ( 12.1.7 ) $ and $ ( 12.1.6 ) $ become the standard fourier series formulas $$\tag{12.1.7'} c_{n} ~=~ \frac{1}{2\pi}\int_{-\pi}^{\pi} \ ! dx~ f ( x ) e^{-in x} , $$ $$\tag{12.1.6'} f ( x ) ~=~\sum_{n\in\mathbb{z}} c_n~e^{in x} ~=~f ( x+2\pi ) , $$ via the identifications $$\ell~=~ n~\in~\mathbb{z} , \qquad \tilde{f} ( \ell ) ~=~2\pi c_{n} . $$ iii ) going back to $3$ dimensions , the $1/v$ normalization in $ ( 12.1.6 ) $ is important . of course , in another convention , it could be put in $ ( 12.1.7 ) $ instead , or alternatively , symmetrically as $1/\sqrt{v}$ in both formulas $ ( 12.1.6 ) $ and $ ( 12.1.7 ) $ .
when people say that the decay rate depends critically on the $q$ value , they are talking about alpha decays compared to other alpha decays . when you compare alpha decay to emission of other small clusters , the dependence on the atomic number $z_c$ of the emitted cluster is much more prominent . the reason is as follows . in the gamow model of beta decay , we assume that the decay rate is the product of three factors : ( 1 ) a hand-wavy probability of preformation of the cluster ; ( 2 ) the frequency with which a cluster assaults the coulomb barrier ; and ( 3 ) the probability of transmission through the barrier . ( re #1 , do not take ohanian too seriously when he says this factor is 0.1 to 1 . actually the literal existence of any cluster bouncing around inside an atomic nucleus would violate the exclusion principle . the whole thing is just a model . ) the critical factor is the tunneling probability $p$ , which can be estimated using the wkb approximation , which looks like $\exp ( -\int\ldots ) $ , where the integral is over the classically forbidden region . the integral depends on the q value , because a higher q value both shrinks the classically forbidden region and reduces the value of the integrand within that region . however , the height of the coulomb barrier is proportional to the product $z_cz_d$ of the atomic numbers of the cluster and the daughter nucleus . if you want all the gory details , you can google the geiger-nuttall equation . but the result turns out to be of the form $\ln p=a-b$ , where the $z$-dependence is dominated by the term $a= ( 1/\hbar ) \sqrt{32z_cz_d m_c r ke^2}$ . for the alpha decay of uranium , we have $a\approx 74$ . in yval 's example , decay by emission of 9be basically doubles the value of $a$ , which reduces the decay rate by a factor of $e^{-74}\approx10^{-32}$ . in the question , yuval estimated that emission of be should only be down by a factor of $e^{-2}$ relative to emission of alphas . this was an algebra mistake . we have an expression of the form $e^{-zu}$ , where $u$ is a constant . changing this expression from $e^{-2u}$ to $e^{-4u}$ does not just reduce its value by a factor of $e^{-2}$ , it reduces it by $e^{-2u}$ , which is a huge factor . actually , as pointed out by joehobbit , the real mystery is not why we do not emit larger clusters , it is why we do not emit lighter objects like protons or deuterons . a proton does not have to worry about preformation , and its tunneling probability would be much higher . possibly this is due to the lower $q$ value for proton emission . this comes into the geiger-nuttall equation because $b\propto z_cz_d/\sqrt{q}$ . in fact proton emission does occur , but it is only competitive for extremely proton-rich nuclei . there is also neutron emission , which does not involve any coulomb barrier at all ; as you had expect , its half-life is very short ( on the order of the assault frequency ) when it is energetically allowed .
newtonian mechanics are out of the question , but at least i can explain without using grad-level general relativity . at the same time , i am quite sure of the answer . the mechanics internal to the black hole are , indeed , difficult . but if you think about it , we can basically draw a system boundary around the black hole . its gravitational influence actually goes on forever , but we will use the approximation of the sphere of influence . once the photon is far enough , it is basically no longer affected by the black hole . that means that the black hole just acts like a mirror as far as we are concerned . to see this , we need to consider the case of a stationary black hole , and adjust this by reference frame transformations . in the reference frame of the black hole , there is no frequency shift of the photon between entering and exit . only momentum is transferred . as the photon gets close to the circular orbit , it is energy has increased a great deal , but it will give this energy back to the gravity well as it exits . just like driving down a hill and then back up . in order for me to convince you that the photon 's energy does not change from entering to exiting , i will argue that the black hole 's kinetic energy does not change . since we are in the black hole 's reference frame , it is velocity is zero . with a differential change to velocity , $v^2$ will be effectively zero . we can use the reverse logic by assuming the photon does transfer energy , and show a contradiction . since the black hole gains no kinetic energy , if the photon 's energy changed , that must be exhibited in some property of the black hole . but there is no property subject to change . the entering and exiting processes are time-symmetric so the black hole 's rest mass can not have changed . hopefully i have convinced you that this black hole operates identically to a mirror . with that info , we can simply apply the relativistic doppler effect . $$1 + z=\frac{1}{\sqrt{1-\frac{u^2}{c^2}}} $$ this equation describes the relativistic redshift factor , $z$ , of a photon when you transfer from one reference frame to another moving at some relative velocity ( in the same direction as the photon 's motion ) . we must apply that twice , because the black hole is a moving reference frame . here is my description of the sequence of events : photon moves toward black hole in lab frame with initial frequency $f_1$ black hole observes photon 's frequency as $f_1'$ , shifted by $u$ , in its reference frame black hole emits photon at same frequency in its reference frame , formally $f_1'=f_2'$ lab frame observes a $f_2$ , which is $f_2'$ shifted by $u$ this would be true for any mirror moving at relativistic velocity . the situation you described is just a fancy mirror . i do not have your numbers , but for closure , i will give this equation ( from definition of redshift factor ) : $$1+z = \frac{f'}{f}$$ you might need to give some more thought to the sign of $u$ in applications here . basically , apply these equations such that $f_1'&lt ; f_1$ and $f_2'&gt ; f_2$ .
space does indeed contain some matter . it is probably safe to assume , however , that moving further from stellar objects the density of this matter approaches 0 . regardless , this density is low enough to often be ignored .
a quick comment on your terminology . the description " non-newtonian " just means the stress/flow rate graph is not linear i.e. there is not a single constant viscosity coefficient . the fluid you describe is what we colloid scientists call " dilatant " , and it is certainly non-newtonian . however there are lots of other non-newtonian fluids such as tomato ketchup and shampoo that behave in different ways . see are there good home experiments to get a feel for the behavior of yield-stress liquids ? for a related question . anyhow , kleingordon has explained why the dilatant effect occurs , but let me try a slightly different approach to the explanation . oobleck is a suspension of solid ( starch ) particles in water . suppose you had a very dilute suspension i.e. lots of water and a little starch . in this case the spacing between the starch grains is large so the grains can flow around without hitting each other , and the suspension just behaves like water . as you increase the amount of starch the spacing between the grains decreases , until at some point the spacing between the grains becomes less than the size of a grain . at this point , when you try apply a large force to suspension the starch grains bump into each other and lock together to form a framework . the water in the suspension now has to flow through the small pores in the starch grain " framework " and this requires a lot of force . hence you can stand on the suspension for a moment . if the apply a small force the water/starch grains move slowly and this gives time for the starch grains to slide around between each other so they will flow . this is why the chap in the white shirt could run on the oobleck , but when he stood still he gradually sank .
there is a quite instructive paper g . a . alekseev and v . a . belinski , equilibrium configurations of two charged masses in general relativity , phys . rev . d76 ( 2007 ) 021501 ; arxiv:0706.1981 [ gr-qc ] , e.g. they mentioned a work about non-existence of static equilibrium configurations of two charged black holes by p . chrusciel and p . tod , commun . math . phys . , 271 577 ( 2007 ) ; arxiv:gr-qc/0512043 and found condition for equilibrium of two charged masses : $m_1 m_2 = ( e_1-\gamma ) ( e_2+\gamma ) $ with $\gamma = ( m_2 e_1-m_1e_2 ) / ( l+m_1+m_2 ) $ .
answer expected by following author 's hints . \begin{align} \mathbf{l}_{eg} and = \dfrac{1}{4\pi}\int \mathbf{r'} \times \left [ \mathbf{e} \times \mathbf{b} \right ] d^3r'\\ and = \dfrac{1}{4\pi} \int \left [ \left ( \mathbf{b . r'} \right ) \mathbf{e} - \left ( \mathbf{e . r'} \right ) \mathbf{b} \right ] d^3r'\\ and = \dfrac{1}{4\pi} \int \left [ \left ( \dfrac{g}{r'^3}\mathbf{r ' . r'} \right ) \mathbf{e} - \left ( \mathbf{e . r'} \right ) \dfrac{g}{r'^3} \mathbf{r'} \right ] d^3r'\\ and = \dfrac{g}{4\pi} \int \dfrac{1}{r'} \left [ \mathbf{e} - \left ( \mathbf{e . \hat{r}'} \right ) \mathbf{\hat{r}'} \right ] d^3r ' \\ and = \dfrac{g}{4\pi} \int \left [ \mathbf{e . \nabla'}\right ] \mathbf{\hat{r}'} d^3r ' . \end{align} or , let $\mathbf{u}$ and $\mathbf{v}$ be arbitrary vectors : \begin{equation} \left [ \mathbf{u . \nabla}\right ] \mathbf{v} = \left [ \mathbf{u . \nabla}v^i\right ] \mathbf{e}_i , \end{equation} where $ ( \mathbf{e}_i ) _{1\leq i \leq 3}$ denotes the cartesian basis . by integrating by parts we have : \begin{align} \mathbf{l}_{eg} and = \dfrac{g}{4\pi} \int \left [ \mathbf{e . \nabla'}\right ] \mathbf{\hat{r}'} d^3r'\\ and = \dfrac{g}{4\pi} \int \mathbf{e . \nabla'} ( \hat{r}'^i ) d^3r ' \mathbf{e}_i \\ and = \dfrac{g}{4\pi} \int \left [ \mathbf{\nabla ' . } \left ( \mathbf{e}\hat{r}'^i \right ) \mathbf{e}_i - \left ( \mathbf{\nabla ' . e} \right ) \mathbf{\hat{r}}'\right ] d^3r'\\ and = \dfrac{g}{4\pi}\ \left [ \oint \mathbf{\hat{r}'} \left ( \mathbf{e . da}\right ) - \int \left ( \mathbf{\nabla ' . e}\right ) \mathbf{\hat{r}'} d^3r ' \right ] . \end{align} but the field $\mathbf{e}$ vanishes at infinty so it comes : \begin{equation} \mathbf{l}_{eg} = -\dfrac{g}{4\pi} \int \left ( \mathbf{\nabla ' . e}\right ) \mathbf{\hat{r}'} d^3r ' . \end{equation} and finally , using the maxwell equation :$\mathbf{\nabla ' . e} = 4\pi e \delta^{ ( 3 ) } ( \mathbf{r} - \mathbf{r'} ) $ , we get the result : \begin{equation} \mathbf{l}_{eg} = -eg\mathbf{\hat{r}} . \end{equation}
the mirror gets proportionally smaller . the explanation is the similarity of triangles . the eye and the marks on the mirror form a triangle , while the eye and the two points on the image form another triangle . the two triangles are similar , with ratio 1/2 , no matter the distance .
the confusion is coming form the fact that you are thinking in terms of the bra-ket physics notation without understanding how the underlying vector spaces are constructed . " kets " are vectors in a vector space , i.e. a set of objects on which vector-vector addition and vector-scalar multiplication is defined ( for some field of scalars ) . " bra"s are covectors , aka one-forms , defined as linear functions from a vector space to its field of scalars . they also form a vector space , and they exist even if we do not define an inner product on the set of kets . since the space of bras is a vector space , it can be tensored with another vector space such as the space of kets . this is defined just like any other tensor product of two vector spaces ( which is the cartesian product , equipped with an intuitive definition of addition and multiplication ) . you could write this down in two equivalent ways : $\vert \uparrow \rangle \otimes \langle \downarrow \vert$ or $\langle \downarrow \vert \otimes \vert \uparrow \rangle$ . they are structurally the same thing , just with a different convention for the ordering . ( likewise , coordinates in 3d can be ordered $ ( x , y , z ) $ or $ ( z , y , x ) $ without changing anything . ) the first case is often abbreviated $\vert \uparrow \rangle \langle \downarrow \vert$ since there is no risk of confusing it with anything else . ( of course , that notation is a bit dangerous since it suggest we have a way of associating a bra $\langle x \vert$ with every ket $\vert x \rangle $ , but that would be a mistake because we have not defined an inner product . ) on the other hand , if you tried to write the second case as $\langle \downarrow \vert \vert \uparrow \rangle$ or $\langle \downarrow \vert \uparrow \rangle$ , it could be misinterpreted as letting the bra ( which is a linear functional from the space of kets to the scalars ) act on the ket , which produces a scalar . that is a very different mathematical object . if the bras and kets have vector space dimension $n$ , then the object $\langle \downarrow \vert \uparrow \rangle$ has dimension 1 while the object $\langle \downarrow \vert \otimes \vert \uparrow \rangle$ has dimension $n^2$ .
this is only being posted as an answer because i cannot comment ( not enough points yet ? ) : mark everitt is wrong . . . the simplest argument would be appealing to this explanation of a microphone : http://www.ccmr.cornell.edu/education/ask/index.html?quid=1212 you can always evaporate masses to membranes and have them oscillate . . . this was in fact used in my research to study gravity at extremeley small length-scales . that being said , yes you can couple a membrane to a circuit . check out the field of optomechanics , which does this with laser cooling .
what you are describing is the difference between brittle and ductile behaviour . most materials show both properties under the appropriate conditions . for example glass becomes ductile as the temperature rises towards the glass transition , while metals become ductile at low temperatures . a brittle material may be superficially unaffected by a blow , but it is likely that there will be some effect at the atomic scales . if you study the surface with an electron microscope you will probably find the blow has caused small defects on the surface , and these could nucleate a crack under stress . alternatively the blow could have caused very small cracks , that again could lead to failure under stress . to what extent this happens depends on the force of the blow . in metals repeated small deformations can lead to the well known phenomenon of metal fatigue . analogous processes do happen with brittle materials , though i have to confess this is outside my area of expertise . a quick google for something like " fatigue in brittle materials " will find lots of related articles like this one .
the normal force is not really due to any of the four force of nature . the forces of nature are not all the forces in the macroscopic sense , they are just the fundamental bosonic particles in a modern quantum field theory description . the normal force is due to the pauli exclusion principle almost exclusively . this is because electrons have the property that two electrons cannot be in the same quantum state . two electrons can not be at exactly the same point . but you might be thinking , " two point particles in three dimensions can not ever be at the same point , it is infinitely improbable ! " in quantum mechanics , the particles are spread out in a wavefunction , and the condition that they can not be at the same point means that wherever their spread-out-ness overlaps , the wavefunction is zero . the wavefunction is in 6 dimensions for 2 particles , so it is hard to visualize , but the zeros appear on the diagonal part , where the two positions for the particle coincide . when you bring two objects to touch , the electron wavefunctions are squeezed together , and the average scale of variation increases slightly , because of the exclusion . the rate of change of the wavefunction is the momentum of the electron , and as you push them closer , it costs energy . this is the source of the normal force . it would not exist if electrons were elementary bosons .
forget about relativistic mass ; it is an outdated and , in this case , irrelevant concept . the higgs boson has a rest mass of about $125\ \mathrm{gev}/c^2$ assuming it is in fact what the lhc has found . anyway , i would say that the higgs boson does not actually give other particles mass directly ; instead , it is a side effect of the mechanism by which those other particles become massive . it just naturally turns out that the particle produced by this mechanism has to be a massive particle itself . or to put it another way , the higgs field would not be able to give other particles mass if it were not itself massive . take a look at the " mexican hat " potential shown in this site 's logo . the bump in the middle arises because the higgs field has an associated mass , the mass of the higgs boson . that bump pushes the " natural " state of the higgs field off center , which means the field has a nonzero " default " value , called the vacuum expectation value . it is that vacuum expectation value that gives other particles mass . without the bump , the minimum of the potential would be in the center , which means the vacuum expectation value of the higgs field would be zero , which in turn would render it incapable of giving other particles mass . i will refer you to another answer of mine for some of the mathematical detail .
the solution for a black hole with non-zero spin and non-zero charge is completed with the vector potential associated with the electromagnetic field . if both spin and charge are nonzero , the vector potential will have $a_{i}$ that depend on the spatial coordinates , and thus , the black hole will have a nonzero magnetic field . in spheriodial coordinates , the vector potential comes out to : $\begin{equation} a_{a} = -\frac{er}{r^{2}+a^{2}\cos^{2}\theta}dt_{a} + \frac{era\sin^{2}\theta}{r^{2}+a^{2}\cos^{2}\theta}d\phi_{a} \end{equation}$ taking the curl of the spatial part ( thus , assuming that we are going to calculate ' the magnetic field observed by someone who is not moving relative to $r , \theta , \phi$ , for instance ) , we find the relevant two components of the maxwell tensor : $\begin{align} f_{r\phi} and =\frac{ ( a^{2}\cos^{2}\theta - r^{2} ) ea\sin^{2}\theta}{ ( r^{2}+a^{2}\cos^{2}\theta ) ^{2}}\\ f_{\theta\phi} and =\frac{ ( r^{2}+a^{2} ) era\sin ( 2\theta ) }{ ( r^{2}+a^{2}\cos^{2}\theta ) ^{2}} \end{align}$ we know that $f_{r\phi}$ is proportional to $b_{\theta}$ , while $f_{\theta\phi}$ is proportional to $b_{r}$ ( the exact factors require calculating the determinant of the metric tensor , and i do not think calculating these terms exactly is the point of this excersise ) . the angular dependence of this field , though , should make it clear that the field is different than that of a true dipole . the question about the field lines crossing the horizon is a trickier one , since these coordinates are singular on the horizon , and that calculation would have to be carried out in a coordinate system that is non-singular there . but i would expect there to be a normal component of the magnetic field to the horizon , since there is nothing in these $f_{ab}$ terms that is singular on the horizon .
quantum mechanics " lives " in a hilbert space , and hilbert space is " just " an infinite-dimensional vector space , so that the vectors are actually functions . then the mathematics of quantum mechanics is pretty much " just " linear operators in the hilbert space . quantum mechanics linear algebra ----------------- -------------- wave function vector linear operator matrix eigenstates eigenvectors physical system hilbert space physical observable hermitian matrix
in this book they first non-dimensionalize the ns equations and then , assuming terminal velocity , small temperature differences , and using scaling arguments they arrive at the following relationship for the terminal speed of a particle moving buoyantly in a stratified flow : $v = \frac{g \alpha \delta t r^2}{6 \pi \nu}$ $\alpha$: coefficient of thermal expansion $\delta t$: temperature difference $g$: gravitational acceleration $r$: radius of spherical fluid element under consideration $\nu$: kinematic viscosity this is roughly what happens , i assume a lot of things as it is necessary for a sane understanding of the subject . initially , a ( stationary ) stably stratified homogeneous fluid is above the stove element . there are roughly three stages , generation , evolution , steady state . in the first stage , the element is turned on , fluid particles nearest to the element experience a change in temperature ( positive ) , kinetic energy increases and nearby fluid parcels propagate such information upwards ( can not go down , forget sides ) via diffusive heat transfer ( in the initial stages ) . at this point in time particles have barely moved from the surface ( considering the total timescale we are interested in ) . however , this process effectively gives rise to considerable upwards fluid velocity in fluid that is adjacent to the heat element . in the next stage , some fluid has accelerated to the point that we can now physically discern fluid motion . diffusion is no longer important ( it never really was ) , at this point advection takes the lead . a fluid parcel along the bottom , the warmest one with respect to all the other fluid parcels along the bottom , will , at this stage , begin to feel a little different from its neighbors . since the fluid parcel is warmer , it is also lighter than all the fluid around it , so , according to archimedes it must go up . this is very similar to the way the sun heats up the earth every morning and generates thermals . there will be lots of different convection cells owing to imperfections in the heating element , which will in turn give rise to eddying motion and turbulence . at steady state , the fluid will be moving away from the element at a steady pace in a way that you can average out all the nuances of fluid dynamics , so that you can arrive at that formula . sometimes though , i just wish we could give someone a million dollars .
$400$ - $700\text{ nm}$ corresponds to about $430$ - $750\text{ thz}$ ( $10^{12}\text{ hz}$ ) , not $\text{mhz}$ ( $10^6\text{ hz}$ ) . to convert from wavelength to frequency , use $$ f = \dfrac{c}{\lambda} , $$ where $\lambda$ is the wavelength , $f$ is the frequency and $c$ is the speed of light . so , for $400\text{ nm}$ , this is : $$ f = \dfrac{299'792'458\ \mathrm{m}/\mathrm{s}}{400 \cdot 10^{-9}\ \mathrm{m}} \approx 7.49 \cdot 10^{14}\ \mathrm{s}^{-1} = 7.49 \cdot 10^{14}\ \mathrm{hz} . $$ if one would build a half wavelength dipole antenna for a $700\text{ nm}$ wave , this would be $350\text{ nm}$ wide , i.e. only visible with a microscope . with a typical distance between atoms in matter of about $0.1\text{ nm}$ , such an antenna would only span around 3500 atoms in length .
i think you can apply euler bernoulli beam theory . this means that the highest stress should take place closest to the wall . why a tree branch does not break there is because it gets thicker closer to the trunk spreading the load over more material .
the correct statement is that we can always construct a geodesic such that $x ( s ) =y ( s ) =0$ for every value of the affine parameter $s$ . all that independently from our initial choice of the origin and orientation of orthogonal cartesian coordinates $x , y , z$ in the $3$-manifolds normal to $\partial_t$ ( the natural rest space of the considered spacetime ) . the geodesics are solutions of the euler-lagrange equations of the lagrangian $${\cal l} = \sqrt{|-\dot{t}^2 + a ( t ( \xi ) ) ^2 ( \dot{x}^2+\dot{y}^2+\dot{z}^2 ) |}\: , \tag{1}$$ where the used parameter is a generic one $\xi$ ad the dot denotes the $\xi$-derivative . as ${\cal l}$ does not explicitly depend on $x , y , z$ , from e-l equations , we have the three constants of motion : $$\frac{\partial {\cal l}}{\partial \dot{x}}\: , \quad \frac{\partial {\cal l}}{\partial \dot{y}}\: , \quad \frac{\partial {\cal l}}{\partial \dot{z}}\: . $$ passing to describe the curves with the geodesical length $s$ , with $$ds = \sqrt{|-\dot{t}^2 + a ( t ( \xi ) ) ^2 ( \dot{x}^2+\dot{y}^2+\dot{z}^2 ) |} d\xi$$ these constants read , in fact , $$a ( t ( s ) ) ^2 \dot{x} ( s ) \: , \quad a ( t ( s ) ) ^2 \dot{y} ( s ) \: , \quad a ( t ( s ) ) ^2 \dot{z} ( s ) \: , $$ where now the dot denotes the $s$-derivative . in other words , there is a constant vector $\vec{c}\in\mathbb r^3$ , such that , for every $s$: $$a ( t ( s ) ) ^2 \frac{d\vec{x}}{ds} = \vec{c}\tag{2}$$ where $\vec{x} ( s ) = ( x ( s ) , y ( s ) , z ( s ) ) $ . the geodesics are described here by curves $$\mathbb r \ni s \mapsto ( t ( s ) , \vec{x} ( s ) ) \tag{3}\: . $$ looking at the lagrangian ( 1 ) , one sees that it is invariant under spatial rotations . that symmetry extends to solutions of e-l equations . in other words we have that , if ( 3 ) is a geodesics , for $r\in so ( 3 ) $ , $$\mathbb r \ni s \mapsto ( t ( s ) , \vec{x}' ( s ) ) := ( t ( s ) , r\vec{x} ( s ) ) \tag{4}$$ is a geodesic as well . correspondingly , due to ( 2 ) we have the new constant of motion $$a ( t ( s ) ) ^2 \frac{d\vec{x}'}{ds}= a ( t ( s ) ) ^2 \frac{dr\vec{x}}{ds} = a ( t ( s ) ) ^2 r\frac{d\vec{x}}{ds} = r\vec{c}\tag{2'}$$ unless $\vec{c}=0$ ( * ) , we can rotate this constant vector in order to obtain , for instance , $r\vec{c} = c \vec{e}_z$ . this means that the new geodesic verifies $$a ( t ( s ) ) ^2 \frac{d\vec{x}'}{ds}\:\: ||\:\: \vec{e}_z$$ the spatial part is parallel to $\vec{e}_z$ . i will omit the prime $'$ in the following and i assume to deal with a geodesic with spatial part parallel to $\vec{e}_z$ and thus , as $a\neq 0$ , it holds $x ( s ) = x_0$ , $y ( s ) =y_0$ constantly . let us finally suppose that the initial point of the geodesic is $\vec{x} ( 0 ) = \vec{x}_0$ . as the lagrangian is also invariant under spatial translations , we also have that if ( 3 ) is a geodesic , for $r\in so ( 3 ) $ , $$\mathbb r \ni s \mapsto ( t ( s ) , \vec{x}' ( s ) ) := ( t ( s ) , \vec{x} ( s ) + \vec{r}_0 ) \tag{5}$$ is a geodesic as well . choosing $\vec{r}_0 := - \vec{x}_0$ , we have a geodesic with $x ( s ) =y ( s ) =0$ as requested . ( * ) we can always choose $\vec{c}\neq 0$ assuming that the initial tangent vector of the geodesic verifies this requirement ( notice that $a^2 \neq 0$ ) . and we know that there is a geodesics for every choice of the initial conditions .
well to be hones both of your questions are related . let me start by rewriting your equations of $y_1$ and $y_2$ ( this will make the discussion easier ) , your version of $y_1$ and $y_2$ can be rewritten as : $$y_1=a\sin\left ( \frac{2\pi}{\lambda} ( x+vt ) \right ) , $$$$y_2=-a\sin\left ( \frac{2\pi}{\lambda} ( x-vt ) \right ) , $$where i put a minus outside of $y_2$ by using $\sin ( -x ) =\sin ( x ) $ . let 's now first look at question 2 . question 2 : the answer to this question can be found by looking at the wavefronts of $y_1$ and $y_2$ , which in its turn can be done by looking at a constant value for the arguments of $y_1$ and $y_2$ ( since a constant argument yields a constant value of $y_1$ and $y_2$ and hence a wavefront ) . let 's call this constant value of the argument $x_0$ , then the arguments of $y_1$ and $y_2$ become : $$x_0=x+vt \text{ for the argument of $y_1$} , $$$$x_0=x-vt \text{ for the argument of $y_2$} . $$ both arguments can be rewritten as:$$x=x_0-vt \text{ for the argument of $y_1$} , $$$$x=x_0+vt \text{ for the argument of $y_2$} , $$ where we see that the introduced constant $x_0$ denotes the position of the wavefront at $t=0$ . what this tells us is that $y_1$ represents a wave travelling in the negative $x$-direction ( when the time increases the value of $x$ decreases ) and $y_2$ represents a wave travelling in the positive $x$-direction ( when time increases the value of $x$ increases ) . in general . you can do this analysis for every kind of wave , and you will find that waves with an argument of the form $ ( x+vt ) $ are waves travelling in the negative x-direction ( als called ''left travalling waves'' ) and waves with arguments of the form $ ( x-vt ) $ are wave travelling in the positive x-direction ( also called ''right travelling waves'' ) . on hyperphysics a few more drawings and discussions are available ( should you be interested ) . $$$$ question 1 : the answer to question 1 can be given by the fact that te amplitudes of waves can be summed . this is because of the fact that each wave tells you what displacement $y$ is causes at a given point $x$ on a time $t$ , when you have two waves which interact , the displacements should be summed . now when you look at reflection ( of sound our light or whatever wave you are looking at ) , there are 3 things that can happen : ( first case on the figure ) : when you reflect the reflected wave picks up a phase $\phi=\pi/2$ ( this happens when you reflect on a dense medium ) . in that case your reflected wave picks up a minus sign since $\sin ( x+\pi/2 ) =-\sin ( x ) $ , this is probably the case you are looking at ) . ( second case on the figure ) : when you reflect the reflected and indecent wave are in anti-phase , so they cancel eachother out . ( third case on the figure ) : when you reflect , the reflected wave does not pick up a sign ( and they are in phase ) , the amplitude of the wave doubles .
yes , as mentioned in the comments , the frame-dragging of a satellite orbiting the earth was measured by the gravity probe b mission . the gyroscopes on the gravity probe b measured a frame-dragging drift rate of $37.2 \pm 7.2$ mas/yr , where the theoretical prediction was $39.2$ mas/yr ( mas = milliarcsec ) . the results can be found in this paper . the theoretical frame-dragging value follows from the schiff equation $$ \boldsymbol{\omega} = \frac{gi}{c^2r^3}\left ( \frac{3 ( \boldsymbol{\omega}\cdot \boldsymbol{r} ) \boldsymbol{r}}{r^2}-\boldsymbol{\omega} \right ) , $$ where $\boldsymbol{r}$ is the position vector of the satellite , $i$ is the moment of inertia of the earth , and $\boldsymbol{\omega}$ is the angular velocity of the earth . you can see this equation in the figure below ( source ) : this equation can be derived from gravitomagnetism ; see this article , this article , or weinberg 's gravitation and cosmology , section 9.6 ( precession of orbiting gyroscopes ) . in order to find the average frame-dragging , we have to integrate the equation over an orbital revolution . fortunately , gravity probe b has a polar orbit , for which the average value becomes $$ \boldsymbol{\omega}_\text{av} = \frac{gi\boldsymbol{\omega}}{c^2r^3}\frac{\int_0^{2\pi} ( 3\cos^2\theta - 1 ) \text{d}\theta}{\int_0^{2\pi}\text{d}\theta} = \frac{gi\boldsymbol{\omega}}{2c^2r^3} , $$ where $\theta$ is the angle between $\boldsymbol{r}$ and $\boldsymbol{\omega}$ . we have , using this source , $$ \begin{align} i and \approx 8.02 \times 10^{37}\ , \text{kg}\ , \text{m}^2 , \\ \omega and = \frac{2\pi}{86164\ , \text{s}} = 7.29 \times 10^{5}\ , \text{rad}\ , \text{s}^{-1} , \\ r and = 6371 + 642 = 7013\ , \text{km} . \end{align} $$ combining this , i find $\omega_\text{av} = 40.8$ mas/y , close to the value cited in the gravity probe b paper .
the paper by dr . stephen hawking does not say that black holes do not exist . what he says is that black holes can exist without " event horizons " . to understand what an event horizon is , we first have to understand what is meant by escape velocity . this last one is the speed you need to escape a body . now , here is where the event horizon and the escape velocity comes in play : the event horizon is the boundary between where the speed needed to escape a black hole is less than that of light , and where the speed needed to escape a black hole is greater than the speed of light . so hawking says that instead of event horizon , there may be " apparent horizons " that would hold light and information only temporarily before releasing them back into space in a " garbled form " .
in general emulsifiers do not work by lowering the surface tension . they do lower the surface tension , but it is still thermodynamically favourable for droplets to coalesce . given the above i am sure you have already guessed that the emulsifier creates a kinetic barrier . if you take an oil in water emulsion stabilised with a typical anionic surfactant , to make two drops coalesce you have to displace surfactant from the oil/water interface into either the water or the oil . there is an energy barrier associated with this process , and this prevents coalescence . you can still break emulsions given enogh shear . for example oil in water emulsions are used in some hydraulic systems . the extreme shear involved , in e.g. pumps , overcomes the kinetic barrier and breaks the emulsion , and this is used to provide oil for lubrication where the shear is highest .
comets are some of the material left over from the formation of the planets . our entire solar system , including comets , was created by the collapse of a giant , diffuse cloud of gas and dust about 4.6 billion years ago . much of the matter merged into planets , but some remained to form small lumps of frozen gas and dust in the outer region of the solar system , where temperatures were cold enough to produce ice . a comet is generally considered to consist of a small nucleus embedded in a nebulous disk called the coma . the nucleus , containing practically all the mass of the comet , is a dirty snowball conglomerate of ices and dust . for one , of the observed gases and meteoric particles that are ejected to provide the coma and tails of comets , most of the gases are fragmentary molecules , or radicals , of the most common elements in space : hydrogen , carbon , nitrogen , and oxygen . the radicals , for example , of ch , nh , and oh may be broken away from the stable molecules ch4 ( methane ) , nh3 ( ammonia ) , and h2o ( water ) , which may exist as ices or more complex , very cold compounds in the nucleus . 3 . many astronomers believe that these small objects never became planets or other large objects because of the gravity of the large planets . for example , the pull of jupiter 's kept ' stirring the pot ' of the asteroid belt , so that the gravitational pull of the asteroids on each other was constantly being disturbed . for the kuiper belt and oort cloud , there is a popular theory called ' planetary migration . ' the main idea behind this theory is that the large outer planets of our solar system started out much closer to the sun when the solar system was formed . as they migrated outward through the cloud of small objects still there , the gravity of these large planets pulled a lot of the small objectsout of their orbits . some were pulled into the planets , and some were flung far into the outer reaches of the solar system . the objects that were flung very far out by jupiter became the oort cloud . the object that were not flung out quite as far by the movement of neptune became the kuiper belt . source
your mistake is in the equation $$22.22t = 27.78t - 236.1$$ everything up to there made good sense , but if the police officer has already traveled 236 meters , you should add that to his distance traveled , not subtract it . you will also need to account for the way the police officer only began traveling at full speed 15 seconds into the chase . anyway , it is much easier to do the problem by thinking about the relative speeds . during the first ten seconds , the car is going 80kph and the police officer is going 40kph on average . so the police officer loses ground at an average of 40kph for 10 seconds . we can think of this as 10 seconds ' worth of loss , and ask how many seconds ' worth of loss the police officer gains as he speeds up further . in the next segment , the police officer gains ground at an average of 10kph for 5 seconds . he is gaining ground 1/4 as fast as he lost it earlier and does it for 5 seconds , so this makes up for 5/4 of a second 's worth of loss , leaving 8 3/4 seconds ' lost ground remaining . finally he gains ground at 20kph until he catches up . he is gaining here at half the rate he was originally losing ground , so it takes him double the remaining seconds ' worth time , or 17.5 seconds , to finish the pursuit . this method is much simpler to calculate , eliminating many opportunities for errors .
if the temperature is not much below freezing , the rate of heat transfer from your plants ( and particularly from the earth around their roots ) is low , if there is a lot of water present , the high heat of fusion means that it will take a long time to actually freeze much of it . so maybe the plant makes it through the night without too much damage . note that if it does not warm up enough the next day the second night will kill them because it starts close to freezing .
you should consider a particle with some finite energy $e$ and use that constraint to take the $v\rightarrow c$ limit . with lorentz factor $\gamma = 1/\sqrt{1-v^2/c^2}$ , the relativistic total energy is $e = \gamma mc^2$ . therefore , $p/e = v/c^2$ . with the particular case of $v = c$ , it follows that $e = pc$ . although really , you should simply consider $e = pc$ for massless particles to be more fundamental . the general relation is $ ( mc^2 ) ^2 = e^2 - ( pc ) ^2$ , which corresponds to the the norm-squared of the four-momentum vector in relativity .
space actually has energy , vacuum energy . it has been shown by various experiments and can be explained by quantum mechanics . so more space means more energy . although it probably can not be used to do work , it does act to increase expansion . check out the wikipedia page for more info . although galaxies are massive , they are far away and thus the resulting acceleration towards each other is weak . if the space between galaxies is expanding at a faster rate than their mutual atraction ; galaxies will move apart . imagine a football player running from one endzone toward the other as fast as he could . however the distance between them endzones doubles every 4 seconds . the endzones are not moving ; the space between them is growing . there is no way the player could hope to reach his goal . in a short time he would not be able to see the other end . the scary thing is this not only goes for the field but everything . the player himself would be ripped apart as his own body parts moved farther and farther apart . if space was expanding fast enough the attractive forces keeping black holes or even atoms together would not be enough . this what is commonly referred to as the big rip . luckily the expansion is slow enough that only " distant " galaxies are moving away from each other . gravitational forces in a solar system or a galaxy are more than enough to withstand expansion . the larger the scale of the system the more expansion has to play . when we talk about expansion we are not talking about objects moving away because of their great velocities against a static backdrop . what you are describing is like marbles on a grid . the marbles get farther away due to their velocities relative to the grid pointing in opposite directions . instead expansion is like the grid growing in scale . this effects the distances between the marbles independently of their velocities ; which is why we observe all far away objects as moving away from us . if it was not for expansion we would expect a more random distribution . it is only at smaller scales where expansion is less of a factor that we can see objects moving toward us such as andromeda 's galaxy . we are not in any sense moving away from the center of the universe . the idea of the big bang and inflation is that not only every thing , matter and energy ; but everywhere was contained in the big bang .
force is defined as something which can change the speed or direction of motion of an object . in other words , force causes acceleration . if you invert this statement , you see that a body undergoing acceleration must mean there is some force acting on the body . therefore , acceleration and force must be related somehow . therefore , $f = m v~$ is incorrect . the correct relation is $f = ma$ edit : from the referenced url : so if a car ( say 2000kg ) is travelling at a constant speed of say 70mph on the motorway , and i like to have a picnic in the center lane , and that car hits me , the force it exerts should be zero because f=2000 x 0 which is zero so no force exerted on me ? the car is traveling at a constant speed , so there is no net external force being exerted on the car . when the car hits you , obviously your velocity changes - you will not be enjoying the picnic anymore because you had be in pieces , but if you were a rigid body , you would be intact but now you had be moving at some velocity . that is to say , you underwent acceleration . the force on you , therefore , would be non-zero . by newton 's third law , the force on the car would also be non-zero and , without anything to counter this force ( like the engine powering the wheels ) , the car would decelerate .
they can not be the same thing . as wikipedia says , it is possible to calculate certain properties of glueballs from qcd , including their masses , and the masses do not come close to what we have observed for the higgs boson . also , the higgs does not have color charge , so it does not interact with gluons , whereas a glueball would . that would make a large difference in the cross section of any strong interaction , and we do not detect that .
there are many things on earth that do persist over time by being made of thermodynamically stable substances . a common example is a good solid lump of granite . these can " survive " unchanged for billions of years ( much longer if it was not for plate tectonics ) . however , we do not say they are alive . there are a much smaller number of other things that persist over time in a different way : by using energy to build up a thermodynamically unstable structure , and then later breaking down that same structure to get the energy back , which is then used to capture more energy and build more structure , etc . living organisms are a good example of this , although it also applies to other things , such as fire , thunderstorms and rivers . the general term for these is " dissipative structures " , because they persist by downgrading , or " dissipating " energy . of course it would be nice to have the best of both worlds , where you are made of a thermodynamically stable substrate and you are also able to grow by extracting energy from your environment . i guess a growing crystal would be an example of this in a sense - but the problem is that thermodynamically stable structures tend not to be very interesting . it would be impossible to find a thermodynamically stable structure that could extract energy from sunlight to build more of itself , because as soon as it is absorbed a photon and converted that energy into a chemical bond , it is out of thermodynamic equilibrium and therefore unstable . in general the more complex a structure is on the molecular level , the lower its entropy tends to be . if you want to be more complicated than , say , a rock , you do not really have much choice other than to be a far from equilibrium structure , which means you contain stored energy . you are right that all batteries leak . if you have decided to persist by being a living organism , that is just something you have to put up with . as a human , most of the energy you extract from the food you eat will go into maintaining your cells against the continual forces of decay that degrade them , with only a much smaller proportion going into useful things like growth and movement . a more prosaic answer is that life does not seem to have much of a choice about what it is made of . all life on earth is made of water , proteins , and smaller amounts of lipids and other organic compounds . at the origins of life this probably would not have been so far from equilibrium , since in a reducing atmosphere amino acids can form spontaneously . however , the combined result of photosynthesis and limestone formation is that the atmosphere is now highly oxidising , which means that on today 's earth protein burns rather nicely . in summary , the reasons that living organisms are made of unstable matter are that ( i ) if they did not extract energy from their environment , we would not consider them alive ; ( ii ) if you extract energy from your environment , you become thermodynamically unstable ; ( iii ) it is difficult if not impossible to have a complicated , yet organised , molecular structure without being far from equilibrium ; and ( iv ) the decision to be made of proteins , which are particularly unstable , was made a long time ago , when they were not anywhere near as flammable as they are today .
( i try to answer to my own question , after some reflections made with the help of lubo . ) for an incompressible and irrotational flow , the conditions $\nabla\times \boldsymbol u=\boldsymbol 0$ and $\nabla\cdot \boldsymbol u = 0$ imply , $\nabla^2\boldsymbol u =\boldsymbol 0$ . indeed : $$\nabla^2\boldsymbol u = \nabla ( \nabla\cdot\boldsymbol u ) -\nabla\times ( \nabla\times \boldsymbol u ) = \boldsymbol 0$$ this forces us to write down the navier-stokes equation for the motion of the fluid without the viscous term $\mu\nabla^2\boldsymbol u$ , no matter the viscosity : $$ \rho ( \partial_t \boldsymbol u + u\cdot\nabla \boldsymbol u ) = -\nabla p \ \ \ , \ \ \ \nabla\cdot \boldsymbol u = 0$$ now , it could seem that this implies the flow is automatically a high-reynolds number flow ( for which we could have wrote down the same equation , but for a different reason : $\mu=\rho\nu\simeq 0$ , and this would have been an approximation ) . but , even if the viscosity is far from neglectable , we can make another kind of approximation , saying that the inertia , represented by the left-hand-side terms in n-s equation , can be neglected because of $re=ul\rho/\mu\ll 1$ ( this can happen in a lot of situations : microobjects , extra-slow flows , and - of course - high viscosity . in this case , the equations of motion become : $$-\nabla p = \boldsymbol 0\ \ \ , \ \ \ \nabla\cdot \boldsymbol u = 0$$ which are , in fact , the equations we would arrive at if we started by the stokes equation ( for inertia-less flows ) for irrotational flows . then , an irrotational flow is not necessarily governed by the euler equation , i.e. , it is not necessarily inviscid .
as the comments say , you have to be precise about your reference point when you talk about time dilation . time dilation is always relative to something else . but there is an obvious interpretation to your question . suppose you have an observer well outside the solar system and stationary with respect to the sun . for that observer your clock on earth is ticking slowly for two reasons : you are in a gravitional well so there is gravitational time dilation . you are on the earth which is hurtling round the sun at about ( it varies with position in the orbit ) 30 km/sec . the earth 's surface is also moving as the earth rotates , but the maximum velocity ( at the equator ) is only 0.46 km/sec so it is small compared to the orbital velocity and we will ignore it . as it happens the problem of combined gravitational and lotentz time dilation has been treated in the question how does time dilate in a gravitational field having a relative velocity of v with the field ? , but this has some heavy maths so let 's do a simplified calculation here . the gravitational time dilation , i.e. the factor that time slows relative to the observer outside the solar system is : $$ \frac{t}{t_0} = \sqrt{1 - \frac{2gm}{rc^2}} $$ where $m$ is the mass of the object and $r$ is the distance from it . for the sun $m = 1.9891 \times 10^{30}$ kilograms and $r$ ( the orbital radius of the earth ) $\approx 1.5 \times 10^{11}$ so the time dilation factor is $0.99999999017$ . for the earth $m = 5.97219 \times 10^{24}$ kilograms and $r$ ( the radius of the earth ) $\approx 6.4 \times 10^{6}$ so the time dilation factor is $0.999999999305$ . the lorentz factor due to the earth 's orbital motion is : $$\begin{align} \frac{1}{\gamma} and = \sqrt{1 - v^2/c^2} \\ and = 0.999999995 \end{align}$$ and to a first approximation we can simply multiply all these factors together to get the total time dilation factor : $$ \frac{t}{t_0} = 0.999999984 $$ to put this into context , in a lifetime of three score and ten you on earth would age about 34 seconds less than the observer watching from outside .
first of all , this is by no means a trivial problem . the usual method goes something like the following . the force from mass 2 on mass 1 is : $$f_{21} = g\frac{m_1 m_2}{ ( x_2 - x_1 ) ^2} = m_1 \ddot{x}_1$$ similarly : $$f_{12} = -f_{21} = -g\frac{m_1 m_2}{ ( x_2 - x_1 ) ^2} = m_2 \ddot{x}_2$$ canceling masses and subtracting the equations from each other gives : $$\ddot{x}_2 - \ddot{x}_1 = \frac{d^2}{dt^2} ( x_2 - x_1 ) = -g\frac{m_1 + m_2}{ ( x_2 - x_1 ) ^2}$$ if we define $~r=x_2-x_1$ as the separation between the masses , then our equation becomes : $$\ddot{r} = -g\frac{m_1 + m_2}{r^2}$$ now it gets a bit trickier . we use the fact that $\ddot{r}=\dot{r}~d\dot{r}/dr$ to separate the differential equation : $$\dot{r}~d\dot{r}=-g\frac{m_1 + m_2}{r^2}~dr$$ for $\dot{r}=0$ at $r_0$ ( they are initially at rest ) , the integral of the above yields : $$\frac{dr}{dt} = \sqrt{ \frac{2 g ( m_1 + m_2 ) }{r} - \frac{2 g ( m_1 + m_2 ) }{r_0}} = \sqrt{ \frac{2 g r_0 ( m_1 + m_2 ) - 2 g r ( m_1 + m_2 ) } {r\ r_0}}$$ so then : $$\delta t=\sqrt{\frac{r_0}{2 g ( m_1 + m_2 ) }}~\int_{r_0}^{r} \sqrt{\frac{r}{r_0 - r}}~dr $$ in your case you have set $g=10$ , $r_0=10$ , and the masses each to one . when they have each traveled four meters , $r=2$ . so you need to integrate the above from 10 to 2 .
welcome to the community wizzphiz . you do not state your age in your profile , but i would think &lt ; 20 ? there are no spontaneously physical electron positron pairs created from the vacuum . the reason is called " conservation of energy " it would take energy to create such a pair . the vacuum sea consists of " virtual particles " and the electrons going around the accelerator cannot " see " them as in addition to being virtual they are created and annihilated in a small delta ( time ) . in this link , which is a festschrift for a scientist , in paragraphs 2 and later there are a lot of explorations of electrons scattering off radiation , even very low black body radiation that exists in a vacuum , but these are not the dirac sea pairs you are asking about . the vacuum pairs do have experimental signatures in the casimir effect , and in the widening of the lamb shift . when one goes to general relativity the vacuum particles theoretically may become physical in accelerated systems but there is no solid experimental evidence of this . the beam energies in the accelerators we have are not in that ball park of acceleration .
if you take an isolated hydrogen atom then the electron sits in well defined atomic orbitals that are eigenfunctions of the schrodinger equation . this is a stable system that does not change with time . if you now introduce an oscillating electromagnetic field ( i.e. . light ) then this changes the potential term in the schrodinger equation and the hydrogen atomic orbitals are no longer eigenfunctions of schrodinger equation . so the electron can no longer be described as a $1s$ or $2s$ or whatever orbital , but rather the electron and the photon now have a single time dependant wavefunction that describes both . what happens next depends on how this new wavefunction evolves with time . as the photon moves away we expect the new wavefunction to evolve into one of three possible final states : the electron orbital is unchanged the electron in a different atomic orbital ( i.e. . it is been excited ) and no photon the electron in a different atomic orbital ( i.e. . it is been excited ) and a photon with a different energy you can not predict which will happen , but you can calculate the probability of the three final states . what you find is that the probability of ( 2 ) is only high when the photon energy is the same as the energy spacing between atomic orbitals , the probability of ( 1 ) approaches unity when the photon energy does not match an energy spacing in the atom , and the probability of ( 3 ) is generally negligable . so the photon does not need to know whether or not it has the correct energy . the photon and atom interact to form a single system , and this evolves with time in accordance with schrodinger 's equation .
to answer your second question : yes , you can write any arbitrary function $f ( x ) $ as a linear combination of eigenfunctions of $\mathcal{p}$: $$f ( x ) =\underset{=f_+ ( x ) }{\underbrace{\frac{f ( x ) +f ( -x ) }{2}}}+\underset{=f_- ( x ) }{\underbrace{\frac{f ( x ) -f ( -x ) }{2}}}$$ with $\mathcal{p}f_+ ( x ) =f_+ ( x ) $ and $\mathcal{p}f_- ( x ) =-f_- ( x ) $ .
note that it is the residue of the combination $j ( z ) \mathcal a ( z_0 , \bar{z_0} ) $ that we need : the poles come from the ope where the operators come together . now it could be that $j ( z ) \mathcal a ( z_0 , \bar{z_0} ) $ is more singular than $\frac{1}{z-z_0}$ at $z=z_0$ . only the simple poles contribute by the residue theorem . and if there is no simple pole ( maybe $\mathcal a$ is $1$ , and/or the combination is regular ) , then the answer will be zero .
no , there is no need for the permanent magnets to lose any internal energy or strength when they are used to do work . sometimes they may weaken but they do not have to . the energy needed to do the work is extracted from the energy stored in the magnetic field ( mostly outside the magnets ) , $\int b^2/2$ , and if the magnets are brought to their original locations , the energy is returned to the magnetic field again . the process may be completely reversible and in most cases , it is . imagine two ( thin ) puck-shaped magnets with north at the upper side and south at the lower side . if you place them on top of each other , the magnetic field in the vicinity of the pucks is almost the same as the magnetic field from one puck  the same strength , the same total energy . however , these two pucks attract because if you want to separate them in the vertical direction , you are increasing the energy . in particular , if you separate them by a distance much greater than the radius of the puck 's base , the total magnetic field around the magnets will look like two copies of a singlet magnet 's field and the energy doubles . if the magnets are close , the magnetic energy is $e$ ; if they are very far in the vertical direction , it is $2e$ . you may consider this position-dependent energy to be a form of potential energy ( although there are some issues with this interpretation in the magnetic case when you consider more general configurations : in particular , a potential-energy description becomes impossible if you also include electric charges ) , potential energy that is analogous to the gravitational one . gravitational potential energy may be used to do work but it may be restored if you do work on it ( think about a water dam where water can be pumped up or down ) . nothing intrinsic has to change about the objects ( water ) and the same is true for the magnets . let me mention that for a small magnet with magnetic moment $\vec m$ in a larger external magnetic field , the potential energy is simply $$ u = -\vec m\cdot \vec b $$ independently of the magnetic field at other points , the potential energy is given simply by $\cos\theta$ from the relative orientation of the magnetic moment and the external magnetic field ( times the product of absolute values of both of these vectors ) .
i greatly sympathize with your question . it is indeed a very misleading analogy given in popular accounts . i assure you that curvature or in general , general relativity ( gr ) describe gravity , they do not assume it . as you appear to be uninitiated i shall try to give you some basic hints about how gravity is described by gr . in the absence of matter/energy the spacetime ( space and time according to the relativity theories are so intimately related with each other it makes more sense to combine them in a 4 dimensional object called space-time ) is flat like a table top . this resembles closely with ( not completely ) euclidean geometry of plane surfaces . we call this spacetime , minkowski space . in this space the shortest distance between any two points are straight lines . however as soon as there is some matter/energy the geometry of the surrounding spacetime is affected . it no longer remains minkowski space , it becomes a ( pseudo ) riemannian manifold . by this i mean the geometry is no longer like geometries of a plane surface but rather like geometries of a curved surface . in this curved spacetime the shortest distance between any two points are not straight lines in general , rather they are curved lines . it is not very hard to understand . our earth is a curved surface and the shortest distance between any two points are great circles rather than straight lines . similarly the shortest distance between any two points in the 4 dimensional spacetime are curved lines . an object like sun makes the geometry of spacetime curved in such a way that the shortest distance between any two points are curved . this is called a geodesic . a particle follows this curved geometry by moving along this geodesic . einstein 's equations are mathematical descriptions of the relation of the geometry to the matter/energy . this is how gravity is described in general relativity .
i think the real question is actually posed most directly in your comment ( so you might want to consider editing some of this into the original question ) : i am more concerned about understanding whats going on and making sure that i know how to do it . i have heard that you should not worry about significant figure rules until you have your final answer . how many decimal places should you round a number like 50.0 cos ( -20.0 ) to ? do you always round to 2 decimals as in my problem ? yes , you are correct that you should never actually round a number off until you are done with the calculation . however , when you are writing out your intermediate steps , it is common practice to write rounded values , rather than copying every digit your calculator shows you , just to avoid burdening the reader with a lot of extra digits that do not really add anything interesting . keep in mind that this convention only affects what you write . you still keep the number to full precision in your calculator . as for choosing the number of digits to write out , you can use the significant figure rules , which go like so : addition and subtraction : find the last significant digit of each number , and choose the one with the larger place value . that place should be the last significant digit of your result . another way to think of this rule is that a digit in the sum ( or difference ) is not significant unless both digits that were added to produce it are significant . so , using gray shading to designate insignificant digits : $$\begin{align*}3 and . 146309\\+2 and . 71\\ =5 and . 85\color{red}{6309}\end{align*}$$ so you would round to the last significant digit in this case , i.e. you would write out $5.86$ . but if you use this result again : $$\begin{align*}5 and . 85\color{red}{6309}\\+4 and . 93101\\ =10 and . 78\color{red}{7319}\end{align*}$$ this time you would again round to the last significant digit , and write out $10.79$ . multiplication and division : your result should have the fewest number of significant digits of either of the numbers you are multiplying or dividing . $$\begin{align*} and 253.1\\\div and 45\\ = and 5.6\color{red}{2444\ldots}\end{align*}$$ and if you multiply this by the earlier result , $$\begin{align*} and 5.6\color{red}{2444\ldots}\\\times and 10.78\color{red}{7319}\\ = and 60 . \color{red}{67268\ldots}\end{align*}$$ these rules are a simplification of a slightly more complex ( but more accurate ) system , the error propagation rules , which physicists normally use in research . unfortunately , the error propagation rules for functions like the sine and cosine can not be simplified quite so easily , so in practice people often just use the multiplication/division rule ( write out the fewest number of significant digits ) for everything else not mentioned here . of course , you have to remember that , except for final results , it is really not that important how many digits you write , since you should never be rounding " behind the scenes " in your calculator anyway .
the electric and magnetic fields of a single photon in a box are in fact very important and interesting . if you fix the size of the box , then yes , you can define the peak magnetic or electric field value . it is a concept that comes up in cavity qed , and was important to serge haroche is nobel prize this year ( along with a number of other researchers ) . in that experiment , his group measured the electric field of single and a few photons trapped in a cavity . it is a very popular field right now . however , to have a well defined energy , you need to specify a volume . in a laser , you find an electric field for a flux of photons ( n photons per unit time ) , but if you confine the photon to a box you get an electric field per photon . i will show you the second calculations because it is more interesting . put a single photon in a box of volume $v$ . the energy of the photon is $\hbar \omega$ ( or $\frac{3}{2} \hbar \omega$ , if you count the zero-point energy , but for this rough calculation let 's ignore that ) . now , equate that to the classical energy of a magnetic and electric field in a box of volume $v$: $$\hbar \omega = \frac{\epsilon_0}{2} |\vec e|^2 v + \frac{1}{2\mu_0} |\vec b|^2 v = \frac{1}{2} \epsilon_0 e_\textrm{peak}^2 v$$ there is an extra factor of $1/2$ because , typically , we are considering a standing wave . also , i have set the magnetic and electric contributions to be equal , as should be true for light in vacuum . an interesting and related problem is the effect of a single photon on a single atom contained in the box , where the energy of the atom is $u = -\vec d \cdot \vec e$ . if this sounds interesting , look up strong coupling regime , vacuum rabi splitting , or cavity quantum electrodynamics . incidentally , the electric field fluctuations of photons ( or lack thereof ! ) in vacuum are responsible for the lamb shift , a small but measureable shift in energies of the hydrogen atom .
given that leftaroundabout and vonjd have addressed the fundamental place of the fourier transform in the formalism , let me talk a little about an experimental application . what is the shape and size of a atomic nucleus ? from rutherford we learned that the nucleus is rather a lot smaller than the atom as a whole . now , electron microscopy can just about provide vague picture of a medium or large atom as a out-of-focus ball , but there is no hope of employing that technique to something orders of magnitude smaller . what we do is scatter things off of the component parts of the nucleus . a nice reaction here is $$ e + a \to e + p + b $$ where $a$ represents that target nucleus and $b$ the remnant after we bounce a proton out . ( this is what nuclear physicists call " quasi-elastic scattering " . ) now , if ( 1 ) we are shooting a beam of electrons at a stationary target , ( 2 ) we have a precision measurement of the momenta of the incident and scattered electrons and the ejected proton , ( 3 ) we are willing to neglect excitation energy of the remnant nucleus , and ( 4 ) we assume that $p$ mostly did not interact with the remnant after being scattered , we know the momentum of the proton inside the nucleus at the time it was struck . collect enough statistics on this and we have sampled the proton momentum distribution of the nucleus . now , here 's the fun part : you can show that the spacial distribution of protons in the nucleus is the fourier transform of the momentum distribution . and bingo , a measurement of the size of the nucleus . do it with a polarized target and you can get info on the shape as well .
first , $u$ is surely not " any non-singular matrix " . for a given basis , $u$ is almost completely determined i.e. unique . it contains $\gamma_2$ because it is derived from the only imaginary pauli matrix . because of the basic dirac algebra $$ \{ \gamma_\mu , \gamma_\nu \} = 2\cdot 1_{2\times 2} \cdot g_{\mu\nu} $$ one may see that $\gamma_0$ is hermitian , $\gamma_0=\gamma_0^\dagger$ , while the spatial ones are anti-hermitian , $\gamma_i=-\gamma_i^\dagger$ . in your identity , you want to relate $\gamma^\mu$ to its transposition $\gamma^{\mu t}$ . up to the sign that depends on the spatial or temporal character of $\mu$ , the transposition is the same thing as complex conjugation . so a related problem is whether the complex conjugate matrices $\gamma^{\mu*}$ can be related to $\gamma^\mu$ by something like a conjugation . and the answer is yes . the main fact behind the exercise is that $\sigma^2$ is the only imaginary pauli matrix , so complex conjugation of pauli matrices is equivalent to the conjugation by $\sigma^2$ with an extra sign . this may be easily generalized if you also include the temporal 0th component and if you use the normal basis . you should check the identity you want to verify in a particular convenient basis , i.e. with an explicit form of the gamma matrices . the verification is most convenient if you write the gamma matrices in block form , with $2\times 2$ blocks being either multiples of pauli matrices or the unit matrix . in a more general representation , the dirac gamma matrices differ from those in the particular basis you will have verified by a conjugation only , and this may only mean that $u$ is changed in the formula , but the essence of the conjugation is unchanged . these equations are important because $c$ is related to the charge conjugation  the replacement of particles by antiparticles ( e . g . exchange of electrons and positrons ) . mathematically , the most important part of the charge conjugation is complex conjugation which is why we needed to express the " complex conjugate gamma matrices as some conjugations of the normal ones " . theories with a symmetry between matter and antimatter are symmetric under c - the charge conjugation symmetry . spinors are mapped to $\psi\to c\psi$ etc . and the only hard part of the symmetry of the lagrangian is a step that requires you to conjugate the gamma matrices by $c$ which is why it is good that we have a way to simplify $c^{-1t} \gamma^\mu c^t$ .
it is because the luminiferous aether was , by definition , composed out of some particles or elementary building blocks with a well-defined location in space . consequently , it picks a privileged reference frame , the rest frame of the aether . in this rest frame , the speed of light  vibrations of the aether  could be constant , $c$ . however , things moving relatively to this aether by the speed $v$ should detect a different speed of the light relatively to them  the speed would go from $c-v$ to $c+v$ , depending on the direction . however , this modification of the light speed , the so-called aether wind , was shown to be non-existent by the morley-michelson experiment which measured the speed to be $c$ regardless of the source and the observer . this falsifies the existence of the aether . the equivalent but even more robust refutation of the aether came from the theory . a physicist named albert einstein built a whole new theory of spacetime , the so-called special theory of relativity ( a picture of this physicist is often being shown by the ordinary people as well ) , that also assumes/guarantees that the speed of light is always constant and there can not be any privileged reference frame . relativity has been backed by the morley-michelson experiment as well as hundreds of much more specific experiments . one of the things it guarantees is that light ( electromagnetic radiation ) has to be made out of disturbances of the empty space , the vacuum itself , and not a localized material carrier .
you are of course right . experimentally established , known laws of physics - especially the equivalence principle - are enough to be certain that antimatter has the same gravitational properties - including universal attraction - as ordinary matter . http://motls.blogspot.com/2010/09/can-antimatters-gravity-be-repulsive.html the justification of further experiments by " tests of antimatter 's gravity " is partly based on ignorance and partly on deliberate deception to get funding .
because $\left ( i\gamma^\mu\frac{\partial}{\partial x^\mu}+m\right ) _{\xi\xi&#39 ; }i\delta ( x-x&#39 ; ) $ is a symbolic expression for a given analytical right-hand side . it is written so for convenience ( not yet calculated ) but it is a specific expression like $\delta ( x-x&#39 ; ) $ or $\delta ( x-x&#39 ; ) &#39 ; $ . it should not acquire any " gauge extension " by definition . this expression does not contain a " particle momentum " .
always start with a nice clear diagram/sketch of the problem . it all follows from there . here is a free body diagram i made for you . then you have ( the long detailed way ) : sum of the forces on body equals mass times acceleration at the center of gravity . $\sum_i \vec{f}_i = m \vec{a}_c $ $$ a_x = m a_x \\ a_y - m g = m a_y $$ sum of torques about center of gravity equals moment of inertia times angular acceleration . $\sum_i \left ( \vec{m}_i + ( \vec{r}_i-\vec{r}_c ) \times\vec{f}_i\right ) = i_c \vec{\alpha} $ $$ a_x \frac{l}{2} \sin ( \theta ) - a_y \frac{l}{2} \cos ( \theta ) = i_c \ddot \theta $$ acceleration of point a must be zero . $\vec{a}_a = \vec{a}_c + \vec{\alpha}\times ( \vec{r}_a-\vec{r}_c ) + \vec{\omega}\times ( \vec{v}_a-\vec{v}_c ) $ $$ a_x + \frac{l}{2} \sin ( \theta ) \ddot\theta + \frac{l}{2} {\dot\theta}^2 \cos ( \theta ) =0 \\ a_y - \frac{l}{2} \cos ( \theta ) \ddot\theta + \frac{l}{2} {\dot\theta}^2 \sin ( \theta ) =0 $$ now you can solve for $a_x$ , $a_y$ from 3 . and use those in 1 . to get $a_x$ , $a_y$ . finally use 2 . to solve for $\ddot\theta$ or do the shortcut of finding the applied torque on a and applying it to the effective moment of inertia about the pivot $i_a = i_c + m \left ( \frac{l}{2}\right ) ^2 $ to get $$ \ddot\theta = \frac{m g \frac{l}{2} \cos\theta }{ i_c + m \left ( \frac{l}{2}\right ) ^2 } $$
no . a helicopter that " stays stationary " does so in relation to the atmosphere around it and the atmosphere pretty much follows the ground underneath it . the atmosphere does not stand still while the earth rotates . if it did , we would experience constant winds on the order of 1000 km/h . that would not be pleasant .
the most important thing is conservation of momentum to describe the collisions . this part is actually quite straightforward , but before you get to collisions you should model the motion of single balls . obviously , you will describe the balls classically and probably not at relativistic speeds ( though that would be interesting . . . ) so pretty much all you need is newton $\mathbf{f}=m\cdot \mathbf{a}$ for the frictional forces . however , describing the balls as solid spheres , you do not just need directed forces but also torques to describe the rotation $\pmb{\omega}$ . it changes as $$ j\cdot\dot{\pmb{\omega}} = \pmb{\tau} $$ where the moment of inertia $j$ would , for a general rigid body , be a tensor but is for spherical objects just a number . the torque $\pmb{\tau}$ consists of the rolling friction , which occurs whenever a ball moves . it should be possible to model this simply by a multiple of the vector the ball would rotate about if it did properly roll , that is $$ \pmb{\tau}_\mathrm{roll} = \eta_\mathrm{roll}\:\mathbf{e}_z\times \mathbf{v} $$ where $\mathbf{v}=\dot{\mathbf{r}}$ is assumed to always lie in the $xy$ plane , disregarding the possibility of balls jumping . the sliding friction , that is , the part of the friction that causes the balls to roll in the first place rather than just gliding over the surface like a curling stone . this actually originates as a directed force rather than a torque : if the ball could not rotate , if would just be the aforementioned $\mathbf{f}=m\cdot \mathbf{a}=m\cdot\ddot{\mathbf{r}}$ . such a " dry " sliding force has a pretty much constant strength but always points into the direction the surfaces slide against each other . $\mathbf{f}=m\mu\frac{\mathbf{v}_\mathrm{rel . slide}}{\|\mathbf{v}_\mathrm{rel . slide}\|}$ with some constant $\mu$ and $$ \mathbf{v}_\mathrm{rel . slide} = \mathbf{v} - \mathbf{v}_\mathrm{ballbottom} $$ where ( $r$ is the ball radius ) $$ \mathbf{v}_\mathrm{ballbottom} = \left ( \begin{smallmatrix}\mathbf{e}_x\\\mathbf{e}_y\\\mathbf{e}_z\end{smallmatrix}\right ) \left ( \begin{smallmatrix}0 and -1 and 0\\1 and 0 and 0\\0 and 0 and 0\end{smallmatrix}\right ) \left ( \begin{smallmatrix}\mathbf{e}_x\\\mathbf{e}_y\\\mathbf{e}_z\end{smallmatrix}\right ) r\cdot\pmb{\omega} . $$ with a rotationable ball the same force also comes up as a torque because it is applied just to the low end of the ball . it is linked by the same linear ( matrix ) mapping , but with one additional contribution : the ball can also spin like a top . this does not directly interact with the movement but is neverthess important . $$ \pmb{\tau}_\mathrm{slide} = r\cdot\left ( \begin{smallmatrix}\mathbf{e}_x\\\mathbf{e}_y\\\mathbf{e}_z\end{smallmatrix}\right ) \left ( \begin{smallmatrix}0 and -1 and 0\\1 and 0 and 0\\0 and 0 and 0\end{smallmatrix}\right ) \left ( \begin{smallmatrix}\mathbf{e}_x\\\mathbf{e}_y\\\mathbf{e}_z\end{smallmatrix}\right ) \mathbf{f} + \mathbf{e}_z\:\eta_\text{spin}\omega_z . $$ that is enough to model , with some simple dynamic-system-simulator ( euler 's method will easily do and is , in this case , actually better than the normally more sophisticated runge-kutta methods , because you want fine steps ) , the behaviour of a single ball on the table . there are then two types of collisions you need to take care for : with the cushions and with other balls . i shall not discuss the cushions here . collisions between balls are basically simple : quite good elastic scatterings , so pretty much only directed momentum is transferred , none of balls ' own angular momenta and no energy is lost . to analyze such a collision , just enter the center of mass frame . at the collision point , both balls touch . the momentum ( or equivanently velocity ) of each of them is reflected off the plane going in between both pointsthat is all there is to it . the difficulty when doing this in a computer simulation is just that you will not have a continuous motion but discrete steps . none of these will actually catch the point where the balls just touch , not unless you enforce it . do do so , you need to check for collisions , and upon noticing one took place calculate , with a uniform motion , back to the point where the balls actually did touch . this is not really a physics problem , so i will not adress it here either .
the short version is that you are using an incorrect expression for the energy of a particle in motion . the correct , general expression for the kinetic energy of a particle of mass $m$ is $$ t = ( \gamma - 1 ) m c^2 \ , , $$ where $\gamma$ is the lorentz factor $$ \gamma = \frac{1}{\sqrt{1 - \left ( \frac{v}{c} \right ) ^2 }} \ , . $$ the version that you use , $t = \frac{1}{2}mv^2$ is only valid when $v \ll c$ .
since $\mu_i$ is proportional to the angular momentum , i am guessing that $\mu_i$ is the magnetic moment of some particle . if so , $\mu_i$ is a constant vector ; any derivative hitting on it will vanish . recall that magnetic moment is defined for a localized current distribution , which is the following integral , \begin{equation} {\bf m } = \frac{1}{2}\int {\bf x'} \times j ( {\bf x'} ) dv ' \end{equation} just like the total charge of a localized charge distribution , it has no spatial dependence .
" i have heard that . . . " is almost always identical to " it is not true . " here 's a paper which points out , among other that gas can move at 3x sound speed . the expansion of a gas-cloud into a vacuum
look at the question a different way : will the earth get " sucking into " the sun ? answer : no , it is in orbit . now , black holes are a little different because inside 3/2 of the schwartchild radius there are no stable orbits , but at very large distances gravity is gravity and orbits are orbits .
it is not poop . it is fly barf . a fly spends about 25% of its time re-digesting and it only can eat liquids . it mixes the eaten food with the appropriate enzyme for digestion . the fly does this by retrieving the eaten food from its digestive system ( a vomit of sorts ) , and drop by drop it is placed on the surface on which the fly is sitting . only then is it sucked back up after they are mixed . the small black dots , that are left in various places , such as the ceiling , are not fly droppings , but actually the remains which are not sucked back up . there are a variety of fluids that are mixed along with oils from the food . this creates a surface film much like when you write with your finger on a mirror . the growth of the spot is just the fluids slowly flattening out and spreading on the surface . the film prevents condensing water from beading to form a lens shape which defuses the light into the cloudy haze you see elsewhere . this is like an anti-fogging agent which works by minimizing surface tension , resulting in a non-scattering film of water instead of single droplets , an effect called wetting . anti-fog treatments usually work either by application of a surfactant film , or by creating a hydrophilic surface . the wetting may appear to spread near the edges of the film during evaporation because the droplets are on the cusp of being completely wetted and have more surface area to evaporate faster which gives the illusion of the spot spreading during evaporation . as for the poop ? well , the droppings fall to the ground and go undetected .
you cannot ' create ' power using magnets , you can only convert power from one form to another . as per the principle of conservation of energy , this can not be done . your set up of same poled magnets facing same poled magnets would rotate momentarily due to the repulsion , but soon come to rest at an equilibrium position - a position where the the repulsion from one magnet cancels off the repulsion due to another . what really can be done by using such set ups of magnets is to convert mechanical energy into electrical energy , or vice versa . to achieve this , you would need to understand faraday 's law , and the force law for current carrying conductors which underly the working principles of devices such as generators and motors . here are some other links which may get you started - http://hyperphysics.phy-astr.gsu.edu/hbase/magnetic/motorac.html#c2 , http://hyperphysics.phy-astr.gsu.edu/hbase/magnetic/motdc.html#c2 .
the buoyant block does exert a force on the water , it is force is equal to the mass of the displaced water , so the pressure of the water immediately beneath the block is exactly the same as the pressure of the water at that height in the rest of the container . indeed , the mass of the system is just the mass of container with water + mass of block
let me start by saying nothing is known about any possible substructure of the electron . there have been many experiments done to try to determine this , and so far all results are consistent with the electron being a point particle . the best reference i can find is this 1988 paper by hans dehmelt ( which i unfortunately can not access right now ) which sets an upper bound on the radius of $10^{-22}\text{ m}$ . the canonical reference for this sort of thing is the particle data group 's list of searches for lepton and quark compositeness . what they actually list in that reference is not exactly a bound on the electron 's size in any sense , but rather the bounds on the energy scales at which it might be possible to detect any substructure that may exist within the electron . currently , the minimum is on the order of $10\text{ tev}$ , which means that for any process occurring up to roughly that energy scale ( i.e. . everything on earth except high-energy cosmic rays ) , an electron is effectively a point . this corresponds to a length scale on the order of $10^{-20}\text{ m}$ , so it is not as strong a bound as the dehmelt result . now , most physicists ( who care about such things ) probably suspect that the electron can not really be a point particle , precisely because of this problem with infinite mass density and the analogous problem with infinite charge density . for example , if we take our current theories at face value and assume that general relativity extends down to microscopic scales , an point-particle electron would actually be a black hole with a radius of $10^{-57}\text{ m}$ . however , as the wikipedia article explains , the electron 's charge is larger than the theoretical allowed maximum charge of a black hole of that mass . this would mean that either the electron would be a very exotic naked singularity ( which would be theoretically problematic ) , or general relativity has to break at some point before you get down to that scale . it is commonly believed that the latter is true , which is why so many people are occupied by searching for a quantum theory of gravity . however , as i have mentioned , we do know that whatever spatial extent the electron may have cannot be larger than $10^{-22}\text{ m}$ , and we are still two orders of magnitude away from probing that with the most powerful particle accelerator in the world . so for at least the foreseeable future , the electron will effectively be a point .
here is a nice answer , taken from http://www.enchantedlearning.com/subjects/astronomy/stars/twinkle.shtml the scientific name for the twinkling of stars is stellar scintillation ( or astronomical scintillation ) . stars twinkle when we see them from the earth 's surface because we are viewing them through thick layers of turbulent ( moving ) air in the earth 's atmosphere . stars ( except for the sun ) appear as tiny dots in the sky ; as their light travels through the many layers of the earth 's atmosphere , the light of the star is bent ( refracted ) many times and in random directions ( light is bent when it hits a change in density - like a pocket of cold air or hot air ) . this random refraction results in the star winking out ( it looks as though the star moves a bit , and our eye interprets this as twinkling ) . stars closer to the horizon appear to twinkle more than stars that are overhead - this is because the light of stars near the horizon has to travel through more air than the light of stars overhead and so is subject to more refraction . also , planets do not usually twinkle , because they are so close to us ; they appear big enough that the twinkling is not noticeable ( except when the air is extremely turbulent ) . stars would not appear to twinkle if we viewed them from outer space ( or from a planet/moon that did not have an atmosphere ) .
andromeda is around 70,000 light years across ( depending on where you make the edge ) so yes the positions of individual stars are shifted . but since it typically takes 250 million years for a galaxy to rotate they are only shifted by 70/250,000 of a circle = 0.1deg . the rotation curve of a galaxy , which tells us the actual mass , depends on the velocity of stars relative to the center of the galaxy so their rotation position does not matter .
some digging revealed the answer . it is called triboluminescence . basically , when certain materials are subjected to mechanical shock , chemical bonds are asymmetrically broken . this creates a charge separation , which on recombination ionizes the nitrogen in the air . resnick-halliday-walker ( in the electric field chapter--its the ' chapter conundrum- ) states that the nitrogen emits uv light after ionozation . this uv light is absorbed by oil of wintergreen crystals in wintergreen lifesavers , which then emit blue light . all this absorption-emission of light is due to electrons jumping energy levels . the examples section of the linked wiki article mentions adhesive tape , so adhesive tape must have a similar mechanism . they are rather vague on the mechanism , though . another mechanism i have heard of is that the nitrogen in the air attaches to the broken bonds , releasing energy . that works , too .
question : where in this argument have i made any assumptions or mistakes , such that this formula applies only to a specific class of systems ? the only assumption made is that the two systems connected interact weakly , so that when the first system has average energy around $e_1$ , the macrostate of the joint system has phase volume $w_1 ( e_1 ) w ( e-e_1 ) $ . i think this is justified for all systems of classical thermodynamics . if the systems interacted strongly and the energy was not homogeneous function of number of particles and volume , the phase volume may not have been given by such formula . why can i not use this formula to determine the entropy of ( say ) one of the two systems i placed in thermal contact ( in the discussion above of temperature ) ? you can ; even if the sub-system 1 interacts with 2 and its energy $e_1$ varies in principle , for macroscopic systems this variation is negligible and only the average value $u_1 = \langle e_1\rangle$ is important for the entropy . one can treat the sub-system as if it was isolated system of the same volume and energy given by $e_1 = u_1$ . why is the gibbs formula the correct one for systems allowed to exchange energy ? the gibbs formula is not " the correct " formula for entropy . rather the point of the gibbs formula is that it is a functional of the probability distribution $p_k$ , with the important property that the maximum possible value of this functional for system with prescribed average energy $u_1$ gives entropy at this average energy ; numerically , $k_b \ln w_1 ( e_1 ) $ and $\max_{\sum_k \epsilon_k p_k = u_1}\left\{ k_b\sum_k -p_k\ln p_k\right\}$ are the same for dense systems of countable states $k$ ( $\epsilon_k$ is energy of the state $k$ . )
let the minkowski metric $\eta_{\mu\nu}$ in $d+1$ space-time dimensions be $$\tag{1}\eta_{\mu\nu}~=~{\rm diag} ( 1 , -1 , \ldots , -1 ) . $$ let the lie group of lorentz transformations be denoted as $o ( 1 , d ; \mathbb{r} ) =o ( d , 1 ; \mathbb{r} ) $ . a lorentz matrix $\lambda$ satisfies ( in matrix notation ) $$\tag{2} \lambda^t \eta \lambda~=~ \eta . $$ here the superscript "$t$" denotes matrix transposition . note that the eq . ( 2 ) does not depend on whether we use east-coast or west-coast convention for the metric $\eta_{\mu\nu}$ . let us decompose a lorentz matrix $\lambda$ into 4 blocks $$\tag{3} \lambda ~=~ \left [ \begin{array}{cc}a and b^t \cr c and r \end{array} \right ] , $$ where $a=\lambda^0{}_0$ is a real number ; $b$ and $c$ are real $d\times 1$ column vectors ; and $r$ is a real $d\times d$ matrix . now define the set of orthochronous lorentz transformations as $$\tag{4} o^{+} ( 1 , d ; \mathbb{r} ) ~:=~\{\lambda\in o ( 1 , d ; \mathbb{r} ) | \lambda^0{}_0 &gt ; 0 \} . $$ the proof that this is a subgroup can be deduced from the following string of exercises . exercise 1: prove that $$\tag{5} |c|^2~:= ~c^t c~ = ~a^2 -1 . $$ exercise 2: deduce that $$\tag{6} |a|~\geq~ 1 . $$ exercise 3: use eq . ( 2 ) to prove that $$\tag{7} \lambda \eta^{-1} \lambda^t~=~ \eta^{-1} . $$ exercise 4: prove that $$\tag{8} |b|^2~:= ~b^t b~ = ~a^2 -1 . $$ next let us consider a product $$\tag{9} \lambda_3~:=~\lambda_1\lambda_2$$ of two lorentz matrices $\lambda_1$ and $\lambda_2$ . exercise 5: show that $$\tag{10} b_1\cdot c_2~:=~b_1^t c_2~=~a_3-a_1a_2 . $$ exercise 6: prove the double inequality $$\tag{11} -\sqrt{a_1^2-1}\sqrt{a_2^2-1} ~\leq~ a_3-a_1a_2~\leq~ \sqrt{a_1^2-1}\sqrt{a_2^2-1} , $$ which may compactly be written as $| a_3-a_1a_2|~\leq~\sqrt{a_1^2-1}\sqrt{a_2^2-1}$ . exercise 7: deduce from the double inequality ( 11 ) that $$\tag{12} a_1\neq 0 ~\text{and}~ a_2\neq 0~\text{have same signs} \quad\rightarrow\quad a_3&gt ; 0 . $$ $$\tag{13} a_1 \neq 0~\text{and}~ a_2\neq 0~\text{have opposite signs} \quad\rightarrow\quad a_3&lt ; 0 . $$ exercise 8: use eq . ( 12 ) to prove that $o^{+} ( 1 , d ; \mathbb{r} ) $ is stabile/closed under the multiplication map . exercise 9: use eq . ( 13 ) to prove that $o^{+} ( 1 , d ; \mathbb{r} ) $ is stabile/closed under the inversion map . the exercises 1-9 show that the set $o^{+} ( 1 , d ; \mathbb{r} ) $ of orthochronous lorentz transformations form a subgroup . $^1$ $^1$a mathematician would probably say that eqs . ( 12 ) and ( 13 ) show that the map $$o ( 1 , d ; \mathbb{r} ) \quad \stackrel{\phi}{\longrightarrow}\quad \{\pm 1\}~\cong~\mathbb{z}_2$$ given by $$\phi ( \lambda ) ~:=~{\rm sgn} ( \lambda^0{}_0 ) $$ is a group homomorphism between the lorentz group $o ( 1 , d ; \mathbb{r} ) $ and the cyclic group $\mathbb{z}_2$ , and a kernel $$ {\rm ker} ( \phi ) ~:=~\phi^{-1} ( 1 ) ~=~o^{+} ( 1 , d ; \mathbb{r} ) $$ is always a normal subgroup .
i ) op is using the period formula $$\tag{1} t~=~2\pi\sqrt{\frac{i}{mgr}} $$ for a compound/physical pendulum ( in the small amplitude limit ) to estimate the gravitational acceleration constant $$\tag{2} g~=~\left ( \frac{2\pi}{t}\right ) ^2 \frac{i}{mr} . $$ here $i$ is the moment of inertia around the pivot point ; $r$ is the distance from cm to the pivot point ; and $m$ is the total mass . ii ) after doing the experiment op finds values for $g$ that are 3-5% too big . ( these results are close enough that op likely did not make any elementary mistakes with units . ) a finite amplitude of $$\tag{3} \theta_0 ~\approx ~25^{\circ}~\approx~ . 44~ {\rm rad}$$ makes the pendulum $$\tag{4} \frac{\theta_0^2}{8}~\approx~ 2\%$$ slower , as compared to the ideal pendulum ( 1 ) , cf . comment by prahar . so correcting for a finite amplitude makes op 's estimates worse , 5-7% too big , as keith thompson points out in a comment above . so the discrepancy is caused by something else . the culprit is likely that it is difficult to get a precise estimate for the moment of inertia $i$ . all the other quantities $t$ , $m$ and $r$ should be fairly easy to measure reliable . so op 's value for $i$ is likely too big . according to steiner 's theorem $$\tag{5} i~=~mr^2+i_0 , $$ where $i_0$ is the moment of inertia around the cm ( and the actual quantity which is poorly known ) . iii ) below follows a suggestion . plot op 's seven data points in an $ ( x , y ) $ diagram with axes $$\tag{6} x~:=~r^2 \quad\text{and}\quad y~:=~r\left ( \frac{t}{2\pi}\right ) ^2 . $$ theoretically , the $ ( x , y ) $ data points should then lie on a straight line $$\tag{7} y~=~ax+b$$ with slope $$\tag{8} a~=~\frac{1}{g}$$ and $y$-intercept $$\tag{9} b=~~\frac{i_0}{gm} . $$ in other words , find the best fitting straight line . this method should hopefully produce a good estimate for $g$ without having to know $i_0$ a priori . ( by the way , notice that we in principle also do not need to know the mass $m$ , cf . the equivalence principle ! ) iv ) finally , as always in experiments , estimate all pertinent uncertainties in the various measurements .
the same force applied anywhere on a rigid object causes the same translational accelleration . the difference is that forces not applied in the direction of the center of mass will also cause some rotational accelleration . remember that f and a in f=ma are vectors . you can therefore treat the vectors as components in any orthagonal system you like . one way to break up f is in the direction towards the center of mass and in the plane perpendicular to that . only the part in the plane perpendicular to the direction to the center of mass will cause roational accelleration . the entire f causes translational accelleration . another way to look at the applied force is to break it down into the force applied to the mass that only causes translational accelleration , and a torque that only causes rotational accelleration . the first is simply the force divided by the mass . for the purposes of finding the positional change of the center of mass , it makes no difference where the force is applied . the center of mass will move exactly the same way in your cases 1 and 2 . torque is the cross product of the vector from the center of mass to where the force is applied , and the force vector . that torque causes rotational accelleration , but has no effect on the position of the center of mass . in your case 1 , the torque is 0 since the vector from the center of mass to the force application point is zero . if you want to say that the rod has some thickness and that therefore the force is being applied one rod radius left of the center of mass , then the torque is still zero . that is because the force vector and the vector from the center of mass to the application point are parallel , so their cross product is zero . it should also make intuitive sense that case 1 does not cause the rod to rotate , just accellerate to the right . in case 2 , the cross product is clearly non-zero . let 's say the rod is 1 m long and the force is 3 n to the right . the vector from the center of mass to the force application point is therefore 1/2 m up . this vector cross the force vector is 1.5 nm into the plane of your diagram , which will cause a clockwise accelleration around the center of mass . since this is only a torque , it has no effect on the position of the center of mass .
mad props for a cool question . i am going to justify essentially the converse of the statement because it does not make much sense to talk of the temperature of a system that is in a pure state . let 's assume that we are talking about a quantum system with disrete energy spectrum ( with no accumuation points ) in thermal equilibrium . let $\beta = 1/kt$ be the inverse temperature . then recall that the boltzmann distribution tells us that the population fraction of systems in the ensemble corresponding to energy $e_i$ is given by $$ p_i = \frac{g_ie^{-\beta e_i}}{z} $$ where $g_i$ is the degeneracy of the energy level . in particular , note that the relative frequency with which energies $e_i$ and $e_j$ will be found in the ensemble is $$ p_{ij} ( \beta ) = \frac{g_ie^{-\beta e_i}}{g_je^{-\beta e_j}} = \frac{g_i}{g_j}e^{-\beta ( e_i - e_j ) } $$ in particular , let $i=0$ correspond to the ground level , then the frequency of any level relative to the ground level is $$ p_{i0} ( \beta ) = \frac{g_i}{g_0}e^{-\beta ( e_i - e_0 ) } $$ notice that since the ground level has the lowest energy by definition , we have $e_i - e_0 \leq 0$ , but zero temperature corresponds to the limit $\beta \to \infty$ , and we have $$ \lim_{\beta\to 0 } p_{i0} ( \beta ) = \delta_{i0} $$ in other words , at zero temperature , every member of the ensemble must be in the ground energy level ; the probability that a system in the ensemble will have any other energy becomes vanishingly small compared to the probability that a member of the ensemble has the lowest energy .
stevin 's loop ( or the " epitaph of stevinus " ) gives us the answer to the first part . that is , the loop will not move if its two sides span the same vertical distance down the triangle . so its center of mass lies at the $y$-axis point one-half of the way down the vertical chain length ( as well as one-half of the way down the slanted chain length -- the numbers are the same . ) so from the top corner of the triangle , the center of mass is $l/6$ meters down while the chain is in the starting position . after it slides so that the chain is totally vertical , the center of mass is $l/2$ meters down from the top corner of the triangle . we have assumed the triangle to be a frictionless surface so that no energy is lost . let us define the zero of potential energy to be the top corner of the triangle . then conservation of energy tells us that $$pe_i+ke_i=pe_f+ke_f$$ $$ ( 0-mgl/6 ) +0= ( 0-mgl/2 ) +mv^2/2$$ $$\sqrt{2gl/3} =v . $$
let $g$ be a group , e.g. a finite group or a lie group . then there exists the notion of a group action $g\times x\to x$ , where $x$ is a set . the set $x$ does not necessarily have to be a vector space . it could e.g. be a manifold . and even if $x$ has vector-space structure , the group action could be non-linearly realized , i.e. , a group element $g\in g$ is represented by a non-linear operator $t_g:x\to x$ . non-linear realizations pop up all over the place in modern physics . for instance , in nonlinear realization of supersymmetry , or in nonlinear realization of the conformal group . example : let the lie group $g=gl ( 2 , \mathbb{c} ) $ of invertible $2\times2$ matrices $$\tag{1} a~=~\begin{pmatrix}a and b\\c and d \end{pmatrix} , \qquad \det ( a ) \neq 0 , $$ act on the complex plane $\mathbb{c}$ ( which , by the way , is a vector space ) as $$\tag{2} a . z ~:=~\frac{az+b}{cz+d} , \qquad ( ab ) . z ~=~ a . ( b . z ) ~ . $$ in this way , matrices get non-linearly represented as meromorphic functions . the subgroup $sl ( 2 , c ) $ is the global conformal group in two space-time dimensions , which e.g. plays a fundamental role in the world-sheet description of string theory . finally , let us mention that in mathematics there exists a generalization of the notion of a $\mathbb{f}$-vector space , where the field $\mathbb{f}$ is replaced by a ring $r$ . it is known as an $r$- module .
i can not find any tabulation . you can do a sellmeier equation fit from visible data and extrapolate it to near-ir . but , the visible index is determined by the uv absorptions , so the extrapolation will only be reliable as long as you are much closer to the uv absorption bands than to the ir absorption bands . since these solvents all have pretty strong high-frequency ir bands ( ch stretch and oh stretch around 3um ) , you probably should not extrapolate past 1um or so . ( that is just a guess . ) i can not really say any more unless i know how accurate you need ( what application ) and what wavelength range you are interested in . you can figure everything out with an ftir , but it can be a bit complicated . addendum : if you calculate the index of refraction in the range 850nm-2.5um by extrapolating from the visible , you will overestimate it . the longer the wavelength , the more severe the overestimation . that is because the effect of a resonance , e.g. the ch stretch resonance , is to increase the refractive index for wavelengths longer than the resonance wavelength , and decrease the refractive index for wavelengths shorter than the resonance wavelength . since your extrapolation from the visible would ignore the ir resonances , it would be an overestimate .
if you believe general relativity , and specifically if you believe the assumptions used to derive the flrw model are justified , then the expansion of the universe is solely the result of the expansion of spacetime . those " ifs " may seem a bit excessive , but it is important to emphasise that gr is a mathematical model that seems to work when we compare it with experiment . we may discover new things that modify the model . for example the flrw metric did not include dark energy , though if dark energy can be described by a cosmological constant the flrw metric does include it . as the wiki article mentions , most physicists believe the flrw metric is a good description of the universe . to answer your second question requires a bit of background . if you take two non-interacting particles some fixed distance apart and sit back and wait for some significant fraction of a hubble time you will see the particles moving apart . for the sorts of distances we see every day this effect is tiny ; it is only significant at intergalactic distances . because the expansion is so small at small distances , it can be counteracted by even the tiniest of forces . this means that if our two test particles are interacting , the interaction will probably completely swamp the expansion . this is why the expansion of the universe is not making you expand . the forces between the atoms in your body are hugely greater than the expansion effect . even on galactic scales this is the case . galaxies do not expand because the gravitational forces between the stars in them overcomes the expansion . it is not until we get to the scale of galaxy clusters that expansion wins . the last part of your question is hard to answer without things getting exceedingly technical . in gr you choose some convenient set of co-ordinates to state the metric . the flrw metric uses co-moving co-ordinates . by definition , in these co-ordinates the time co-ordinate is the same as the proper time , which is an invarient , because the proper time is the time experienced by a freely falling observer . that means that time is not curved . however there is no special distinction between space and time co-ordinates , and there will be other co-ordinate systems in which time is curved .
a black hole will not form . the reason why is that the boosted particle is equivalent by a boost to a reference frame where there is no black hole , and the presence/abscence of a black hole is coordinate-independent . while the energy of , say , an object with earth 's density profile can be made arbitrarily large through a boost , the boosted earth will still have a distinguishable stress-energy tensor from a highly compact object that is not boosted . and , it will be distinguishable from the stress-energy tensor of a boosted black hole , which can be defined by noting that the kerr metric can be written in the form : $$g_{ab} = \eta_{ab} + c \ell_{a}\ell_{b}$$ where $\ell_{a}$ is a particular null vector relative to both $\eta_{ab}$ and $g_{ab}$ and $c$ is an exactly specified function . in this form , we can define boosts relative to the background minkowski metric , and find out what the spacetime of moving black holes is . which is a long way of saying that the math encodes the difference between an earth boosted to $ . 999999999c$ and something that is natively super-dense and is moving that fast . you really have to consider the whole stress-energy tensor , which does not just include energy density , but includes momentum and all of the internal pressures in the object .
indeed , the slater-type orbitals ( radial wave functions ) are not orthonormal  they are not even orthogonal to each other . the $\delta_{n , n'}$ kronecker delta symbol does not appear in the inner product and it can not because the $r$-dependent integrand is positively definite and there is no room for cancellation . their not being orthogonal physically means that the orbitals for different $n$ are not mutually exclusive . for a given molecule , one is supposed to use one value of $n$ only .
since bob can not receive information from eve ( which would mean eve giving their game away ) , bob effectively receives the qubits in the reduced state $$\rho_\text{bob}=\text{tr}_\text{eve}\left [ \left ( \alpha|\mathbf{00}\rangle+\beta|\mathbf{11}\rangle\right ) \left ( \alpha^\ast\langle\mathbf{00}|+\beta^\ast\langle\mathbf{11}|\right ) \right ] =|\alpha|^2|\mathbf{0}\rangle\langle\mathbf{0}|+|\beta|^2|\mathbf{1}\rangle\langle\mathbf{1}| . $$ notice that the intervention of eve has completely killed the off-diagonal elements in this density matrix . for the case of $\alpha=\pm\beta$ this state is the completely mixed state , and bob can extract no information from it . thus , even if alice sends $|+\rangle$ , bob might measure $|-\rangle$ , and when they compare results they will find that an unacceptable fractionof their measurements do not match .
taking your question literally , you can see a single barium ion : the trip group has achieved capturing a single barium ion in a paul trap . the images show coulomb crystals formed by a decreasing number of laser-cooled ions as detected with an emccd camera . this forms an important step towards the planned experiments on single radium ions to measure atomic parity violation and build an ultra-stable optical clock . they are in traps like this one : also , warren nagourney from washingtong university took a picture of a single barium atom scattering light from a laser : single trapped atom , glowing blue photo credit : warren nagourney at the university of washington , c . 2000 what is this ? believe it or not , this is a color photograph of a single trapped barium ion held in a radio-frequency paul trap . resonant blue and red lasers enter from the left and are focused to the center of the trap , where the single ion is constrained to orbit a region of space about 1 millionth of a meter in size . what is the red/blue mess on the sides ? low level out-of-focus laser scatter off of metal trap electrodes and accessories ( atom ovens , electron filaments , etc . ) as seen in this photo . how do we know the dot really is an atom ? when one turns off the red laser , the blue dot vanishes . this is because the scattering process requires both laser colors due to a metastable state in the barium ion . if the blue dot stayed around with the red laser off , we might excuse it as being additional laser scatter off some surface . how was the photo taken ? this is a scanned photo ; the camera was a 35mm nikon ( i believe ) with a wide open 50mm f/1.8 lens . the exposure time was two minutes . several shots were taken at different camera positions and this one caught the ion in the very narrow depth of field . is this how you normally " view " the ion ? no , we use a 50 mm f/1.8 camera lens to image the blue dot onto a photomultiplier tube . we do not require the focus to be so good when using the pmt . where can i see more ? lots of ccd images of one and several trapped ions are found on the monroe group site . only two minutes exposure time , so probably in a dark enough room , someone with good sensitivity could actually see it .
this is the distinction between continuous and discrete spectrum , but only considering the low energy excitations . for an hamiltonian with gapped spectrum , the lowest eigenvalue is separated by a gap from $e=0$ . for example dispersion relation of the form $e=|k|$ is an example of gapless spectrum , and $e=|k+m|$ is an example of gapped one , where $k$ is the wave vector ( which can be any real number ) and $m$ is the mass ( = gap ) . this distinction leads to qualitative difference in the physics - for example the difference between a material being a conductor or an insulator . many times also , the gap is generated by interesting physics ( like the mass gap in yang-mills theory , or the gap in bcs superconductivity ) .
there are numerous distance indicators used for within the galaxy . the most common way is by using intrinsic magnitude . by knowing how bright an object would be if we were close , we can determine how far away it is by how dim it is . there are many types of stars where we have a rough idea of how bright they should be due to characteristics of the star : cephied variables : the original type of variable star that was used by hubble to determine the distance to the andromeda galaxy . rr lyrae variable : like the cephied variable , but usually dimmer . type 1a supernova : these guys , unlike the first two , are cataclismic variables . essentially a binary white dwarf slowly accretes matter from its binary till it reaches the chandrashankar limit , after which point it explodes in a very characteristic way ( since the mass at the time of explosion is roughly constant ) . main sequence stars : generally less accurate than the first 3 , there are some types of main sequence stars which are used to find distances in a similar way . there are a few other ways we can measure distances : perpendicular movement : for example there is a " light echo " from sn 1987a which is essentially light from the supernova interacting with dust around the old star . since this echo should be expanding at the speed of light , we can tell how far away the nova is by the angular velocity of the light . relative velocity in a moving cluster : ( see dmckee 's answer ) tulley-fisher relation : a relationship between the luminosity of the galaxy and it is apparent width . can be used as a decent distance calculator . faber-jackson relation : similar to tulley-fisher , relates luminosity with radial velocity dispersion rate . edit : some more information about redshifts . the whole relationship between redshift and distance was in fact established by hubble by relating distance to cephied variables ( i believe ) with redshift . later on it was made more precise using supernova , which are brighter and can be seen from much father away ( i think recent supernova can be occasionally seen around z=2 , while cephieds are all z&lt ; 1 ) . within a galaxy , redshift cannot be used directs since the " peculiar velocity , " the velocity within the galaxy , completely overshadows the effects of universe expansion on which hubble 's law is based . redshift within the galaxy is useful for certain other techniques . edit : corrected a few minor errors .
in the ideal case where the collision is instantaneous and there is only a single point of contact , the forces experienced by each object can only be along the line that connects the centers and passes through the contact point . actually , the " force " will be infinite , but it will impart a finite impulse ( i.e. . change in momentum ) during the infinitesimal interval of time during which the collision occurs . there can be no component of impulse orthogonal to this , because that would amount to a " force " tangential to the circle . with any finite coefficient of friction , you cannot affect a circle by pushing tangent to it at just a point . this constraint , together with conservation of momentum in two directions and conservation of energy , is enough to determine the motion of both circles given any initial conditions ( which needs 4 scalars to be defined fully ) . if the target circle is sitting still before the collision , its direction of motion afterward is easy enough to find : it is exactly along the line connecting the two circles ' centers at the moment of contact . ( make sure you go the right way along this line - only one of the directions is sensible . ) the new direction of the circle that was originally moving will simply be given by the direction of the vector sum of its old momentum and the momentum imparted to it in the collision . if you want numbers , say the moving circle makes contact when its center is at $ ( x_1 , y_1 ) $ , and the other center is at $ ( x_2 , y_2 ) $ . then the angle at which 2 moves away from the collision is $$ \theta = \frac{180^\circ}{\pi} \tan^{-1}\left ( \frac{y_2-y_1}{x_2-x_1}\right ) . $$ by the way , it might be easier to just use $x$ and $y$ components of velocity , rather than speeds and angles . of course , if you want to take this to the next level of realism , you may need to account for the following effects : kinetic energy is lost ( perhaps by a certain percentage ) with each collision . the collision could result in torques , transferring angular momentum . spinning objects may curve while sliding . this is a complicated effect though , and you probably have to model it with a kinetic coefficient of friction that varies with velocity . finally , for numerical accuracy , i suggest keeping all numbers as floats in the calculations and only rounding to the nearest integers when rendering .
this is really the same as adam 's answer but phrased differently . suppose you have a single wire and you connect it to a battery . electrons start to flow , but as they do so the resistance to their flow ( i.e. . the resistance of the wire ) generates a potential difference . the electron flow rate , i.e. the current , builds up until the potential difference is equal to the battery voltage , and at that point the current becomes constant . all this happens at about the speed of light . now take your example of having let 's say two wires ( a and b ) with different resistances connected between the wires - lets say $r_a \gt r_b$ . the first few electrons to flow will be randomly distributed between the two wires , a and b , but because wire a has a greater resistance the potential difference along it will build up faster . the electrons feel this potential difference so fewer electrons will flow through a and more electrons will flow through wire b . in turn the potential along wire b will build up and eventually the potential difference along both wires will be equal to the battery . as above this happens extremely rapidly . so the electrons do not know in advance what path has the least resistance , and indeed the first few electrons to flow will choose random paths . however once the current has stabilised electron flow is restricted by the electron flowing ahead , and these are restricted by the resistance of the paths . to make an analogy , imagine there are two doors leading out of a theatre , one small door and one big door . the first person to leave after the show will pick a door at random , but as the queues build up more people will pick the larger door because the queue moves faster .
given that there has not been any acceleration to cause these velocities , [ . . . ] as a side issue , even in newtonian mechanics , accelerations do not cause velocities . accelerations are just a measure of how rapidly velocities are changing . what you are running into here is the fact that general relativity does not have any notion of how to measure the motion of object a relative to a distant object b . it is neither true nor false that a and b gain relative velocity due to cosmological expansion . it is neither true nor false that a and b have nonzero accelerations relative to one another . frames of reference in gr are local , not global . it is valid to say that distant galaxies are moving away from us at some velocity . it is also valid to say that everything is standing still , but the space between us and the distant galaxy is expanding . [ . . . ] are there still relativistic effects in play ? that is , is there time dilation between the two frames ? kinematic time dilation is well defined in sr , which means that in gr it is only defined locally . gravitational time dilation is only well defined in gr in the case of a static spacetime , but cosmological spacetimes are not static . so it is neither true nor false that there is time dilation between us and a distant galaxy . concretely , you could measure doppler shifts . if you feel like interpreting these shifts in purely kinematic terms , you can assign a velocity to the distant galaxy relative to us . but this is not mandatory and actually does not really work very well , in the sense that the velocity you get is usually several times smaller than the rate at which the proper distance between the galaxies is increasing . ( proper distance is defined as the distance you would measure with a chain of rulers , each at rest relative to the cmb , at a moment in time defined according to a notion of simultaneity defined by cosmological conditions such as the temperature of the cmb . ) in particular , there are galaxies that we can observe that are now and always have been receding from us at $v&gt ; c$ , if you define $v$ as the rate of change of proper distance . the fact that we can observe them tells us that their doppler shifts are finite and correspond to $v&lt ; c$ . here is a nice popular-level article that explains a lot of this kind of stuff : davis and lineweaver , " misconceptions about the big bang , " http://www.scientificamerican.com/article.cfm?id=misconceptions-about-the-2005-03 it is paywalled , but there are lots of copyright-violating copies floating around on the web . the following is a presentation of the same material at a higher level : davis and lineweaver , " expanding confusion : common misconceptions of cosmological horizons and the superluminal expansion of the universe , " http://arxiv.org/abs/astro-ph/0310808
this is a gravitational phenomenon known as tidal lock . it is closely related to the phenomenon of tides on earth , hence the name . tidal locking is an effect caused by the gravitational gradient from the near side to the far side of the moon . ( that is , the continuous variation of the gravitational field strength across the moon . ) the end result is that the moon rotates around its own axis with the same period as which it rotates around the earth , causing the face of one hemisphere always to point towards the earth .
the article you refer to is about the electrolytic splitting of water . a 100% efficient electrolytic cell would require a voltage of about 1.23v to split water , but for various reasons a simple electrolytic cell requires about 1.48v . the difference between the voltages is called the overpotential , and it increases the amount of power needed to split the water because the power required per unit of hydrogen produced is proportional to the cell voltage . the excess power goes into heating the hydrogen and oxygen produced , and in this case it means that simple cells are about 83% efficient at converting electricity into hydrogen . catalysts can be used to increase the efficiency , and indeed platinum based catalysts can be used to reduce the overpotential and make cells with near 100% efficiency . the problem is that platinum is expensive . the result from the stanford team is that a much cheaper nickel based catalyst can achieve the same efficiency as platinum . the paper is here , but note that it is behind a paywall . if the catalyst proves to be stable enough then it will be useful for electrolytic production of hydrogen , but the improvement in efficiency is not going to change the world overnight . it still takes a lot of power to electrolyse water so it is only feasible when cheap electricity is available .
for good doping you need two things : ( 1 ) get enough dopant in to be useful in changing carrier concentrations , and ( 2 ) having an energy level close to a band edge to generate electrons ( holes ) in the band , rather than making a mid-level recombination center . the below is assuming you are trying to dope silicon . data is generally from sze 's excellent ' physics of semiconductor devices ' text . bismuth certainly has a donor level not much lower than as , satisfying ( 2 ) . however , the solid solubility of bi is roughly 3 orders of magnitude less than as , peaking at below $10^{18}/cm^3$ . this limits the utility of bi in current device technology - you just can not get enough in . nitrogen has very low solubility in silicon . i cannot find quickly any info on energy levels in the gap , but oxygen has several levels , all pretty much near gap . i would say nitrogen loses out on both ( 1 ) and ( 2 ) .
olber 's paradox assume infinite and static universe ( infinite life of universe and stars too ) . cosmological redshift due to universe 's expansion shift visible light to infrared or microwave region of electromagnetic spectrum . the cmb radiation is the most clear effect . observable universe is finite ( more or less 93 billion light years ) . this limit the number of galaxies from which we receive radiation . stars have finite life . in addition also some part of universe with recession velocity greater than c could enter ( in the future ) in the event horizon , but they will red shifted at z more than 1.8 , away from visible region . reference : arxiv:astro-ph/0310808v2 13 nov 2003 peacock cosmological physics cambridge press 2010 ( section 12.1 ) davis linewear misconceptions about big bang scientific american march 2005
" before gravity stopped holding it together " is the same ( pretty much ) as " so that apparent gravity at the surface is zero " . this means that $$m \omega^2 r = \frac{gm_{moon}m}{r^2}$$ with $g=6.7\cdot 10^{-11}$ , $m_{moon}=7.3\cdot 10^{22} kg$ , $r_{moon}=1740 km$ , we find $$\omega=\sqrt{\frac{gm_{moon}}{r^3}}=0.0092 rev/min = 0.55 rev/hour$$ it is interesting to see that the result has the ratio of mass and the third power of the radius - in other words it depends on the density and not the size . whether the moon consists of loose dust that will just fly apart when you reach this speed is another question - not one you asked . you asked for rev/minute but that is a hard number to grok . note that one lap per two hours is what you would have to do in order to remain " in orbit " at the surface of the moon . interestingly , the apollo 11 command module ( piloted by michael collins , the least known of the three astronauts that formed the crew of apollo 11 ) was orbiting at about 60 nautical miles , and going around once in just about 2 hours . pretty similar . . . and that was no coincidence . also note that this speed would start the moon ripping apart at the equator - where the centrifugal force appears strongest . at other points along the surface , you have a force of gravity pointing towards the center ; the component of gravity that counters the centrifugal force ( normal to the axis of rotation ) scales with $\cos ( \text{latitude} ) $ which is also how the centrifugal force scales . this means that matter will start drifting from higher latitudes towards the equator . i have not even begun addressing the other questions you had - about the impact on earth if the moon was spinning at half that speed ( 4 hours / revolution ) . i guess we would get to see the back of it , and watching the moon at night would be a less peaceful experience - but i can not think of any real physical effects ( tides etc ) on earth that would affect life . chances are great that if the moon is being hit by asteroids to make it spin faster , that this would affect its orbit around the earth : that would impact tides , and that would really wreck coastal ecology and possibly climates ; but i am assuming that the moon would accelerate from glancing impacts with " twin asteroids " coming from opposite directions , leaving no net linear momentum , and no net mass increase / decrease - just make it spin faster . the impact on a moon base would be more substantial . every point on the moon would experience days and nights on a four hour cycle , with the temperatures cycling violently . communication with earth would be disturbed - you would want to place relay transmitters on the poles to maintain contact . and of course gravity would be halved again - instead of 1/6th of the earth 's pull , you would appear to have 1/12th . coriolis forces would be wicked too - although you could drive a golf ball a very long way with such low gravity , you had get a horrible hook / slice , depending on whether you are on the northern or southern hemisphere of the moon ( and whether you are left- or right-handed ) . finally a word on the energy of the moon if it spun that fast . we know that $$e = \frac12 i \omega^2$$ and $$i = 2/5 m r^2 = 8.3\cdot 10^{28} j$$ if you took all the power of the sunlight that hits the earth ( assuming 1 kw/m^2 over the side that is lit ) for 20,000 years - you would have the amount of energy needed to give the moon that kind of energy . i imagine that if a sufficient density of meteorites came near enough to the moon to impart that kind of energy , we would not be worrying on earth about building moon bases . . . footnote about collins ' orbit . it was said that while he was on the far side of the moon , he was the " loneliest human since adam " - because he was literally further from any human beings ( and with no means of contacting them ) than any other human , anywhere on earth ( this was before major tom , of course ) . the nasa log of the flight contains the tidbits needed to reconstruct his orbital speed : hidden by the moon for 47 minutes per orbit ( july 21 , 9:44 am ) altitude of 60 nautical miles orbital velocity was 5329 ft/sec ( july 22 , 12:56 am ) after converting to sensible units ( really - they put a man on the moon with feet and nautical miles ? what are we doing , trying to force si units on a nation that was capable of such a feat , with those units ) and drawing a little diagram , i convince myself that the fraction of the orbit when the command module was " dark " is computed as $$f = \frac{\pi - 2 \cos^{-1}\left ( \frac{r_m}{r_m + h}\right ) }{2\pi} \approx 0.35$$ and if that took 47 minutes , then the orbit took 121 minutes .
when i am confused about voltages , i always remember how do you measure a voltage . you take two " needles " , you stick them in any part of the circuit and you measure the potential difference . in this case $v_1$ gets measured by sticking the needles around $d_1$ and similarly with $v_2$ and $d_2$ . $v$ is measured by sticking these needles in front of $d_1$ and behind $d_2$ . in circuits we pretend nothing happens in the wires , when we stick a needle $j$ in front of $d_1$ , $k$ behind it , $l$ in front of $d_2$ , and $m$ behind , there will be no potential difference between $k$ and $l$ , because there is just a wire between them . so what is the potential difference $v$ between $j$ and $m$ ?
i think that the ' agent based ' approach is good for your purpose . i have already used mason to play a little with ' a sort of ' gravity . ( you will have to know java - easier if you work on top of the examples- and the documentation is very good ) mason is a fast discrete-event multiagent simulation library core in java , designed to be the foundation for large custom-purpose java simulations , and also to provide more than enough functionality for many lightweight simulation needs . mason contains both a model library and an optional suite of visualization tools in 2d and 3d . see the examples : bouncing particles - is a tutorial demo of simple particles bouncing and interacting with one another . balls and bands ( tutorial 5 ) ( mass springs between objects ) simulates hooke 's law with balls connected by rubber bands of different strengths . heatbugs -- shown in wireframe 3d . you can also explore the breve package . breve a 3d simulation environment for multi-agent simulations and artificial life ( ia ) objects , joints , gravity .
let me see if i understand the question correctly : in general our solutions for a system will involve for every wavevector k a set of frequencies $\omega ( k ) $ . when these curves giving $\omega ( k ) $ do not cross , there is in obvious sense in which we can separate our solutions into different modes . but when they do cross how do we assign modes ? i believe the answer is in general we do not . ( or we do but we should not . ) unless the solutions have different symmetry properties , which is common enough , our label do not have meaning except convenience . i do not think this is not entirely about definitions . consider adiabatic motion : we think that if we perturb our solution slowly enough it will just change its frequency and wavenumber , but stay on the same mode . but in the vicinity of a crossing of two modes our solution will evolve to have components in both modes , barring symmetry . so is there is no sense to think of them as separate modes . ( the only case i can think where the labeling of modes actually matters is in topological insulators , a case which requires no [ bulk ] band touching . other than that it is just a convenience or it labels symmetry properties ) apologies if i misunderstood your question
you can solve this problem by using energy conservation . it holds \begin{equation} -\frac{gmm}{p}+\frac{1}{2}mv^2 = - \frac{gmm}{2a} , \end{equation} where $a$ is the semimajor axis and related to $p$ by $p=a ( 1-\epsilon ) $ . you can understand the rhs from virial 's theorem , for instance . you may also want to check out the vis-viva equation . psm .
yes , the dot would travel at a speed faster than light . firstly , note that this would not be immediate . if $r$ is the radius of the arc , it would take $\frac r c$ time before the dot started moving ( in the meantime , the beam of light , if " frozen " in time 1 , would look bent from most reference frames ) this does not violate causality . why ? because the dot cannot relay information along the arc . the motion of the dot is controlled by the person at the center , and he is the only one who can send information by moving it . since there is a delay of $\frac r c$ seconds before the dot moves , this is perfectly in line with causality . remember , the dot is not a physical object . it is a certain area having some properties ( point of intersection of distant object and laser beam ) a simple argument to justify that the dot will go ftl : at time $t_0$ , i shine my laser pointer at point a , on the left of the arc . i sweep it around , and at time $t_1$ it points towards b on the opposite end of the arc . note that in my reference frame , $t_0$ and $t_1$ can be quite close , while $r$ can be arbitrarily large . in my reference frame , the photons from my initial point reach a ( remember , a photon does not change its direction in special relativity ) at time $t_0+\frac r c$ , and the photons from my final position reach b at $t_1+\frac r c$ so now , the dot has crossed a distance of $r\theta$ in a time $t_1-t_0$ . note that $r$ can be arbitrarily large , and $t_1-t_0$ is only constrained by the rate of production of photons . which means that we can cover an arbitrary distance in a fixed time interval , so there will be some $r$ for which $\frac{r\theta}{t_1-t_0}&gt ; c$ . note that since the events of the dot reaching the two points are spacelike separated ( assuming we choose $r$ such that $\frac{r\theta}{t_1-t_0}&gt ; c$ ) in my reference frame , they are spacelike separated in all reference frames , and the dot moves faster than light for all . 1 . to specify " frozen in time " rigorously : let there be an array of photon detectors . they are synchronized such that they will all record the existence of photons simultaneously in some given reference frame ( the frame of the laser pointer or the frame of somebody on some given point on the arc ) . now , if that person checks which detectors have recorded a photon , he will get a curved locus .
this is a very good question . einstein himself , in a 1907 review ( available in translation as am . j . phys . 45 , 512 ( 1977 ) , e.g. here ) , and planck , one year later , assumed the first and second law of thermodynamics to be covariant , and derived from that the following transformation rule for the temperature : $$ t ' = t/\gamma , \quad \gamma = \sqrt{1/ ( 1-v^2/c^2 ) } . $$ so , an observer would see a system in relativistic motion " cooler " than if he were in its rest frame . however , in 1963 ott ( z . phys . 175 no . 1 ( 1963 ) 70 ) proposed as the appropriate transformation $$ t ' = \gamma t $$ suggesting that a moving body appears " relatively " warmer . later on landsberg ( nature 213 ( 1966 ) 571 and 214 ( 1967 ) 903 ) argued that the thermodynamic quantities that are statistical in nature , such as temperature , entropy and internal energy , should not be expected to change for an observer who sees the center of mass of the system moving uniformly . this approach , leads to the conclusion that some thermodynamic relationships such as the second law are not covariant and results in the transformation rule : $$ t ' = t $$ so far it seems there is not a general consensus on which is the appropriate transformation , but i may be not aware of some " breakthrough " experiment on the topic . main reference : m . khaleghy , f . qassemi . relativistic temperature transformation revisited , one hundred years after relativity theory ( 2005 ) . arxiv:physics/0506214 .
gravitational monopoles are forbidden by the positive mass theorem--- any configuration of gr has positive mass , and therefore is an ordinary " pole " in the analogy with electromagnetism . the analogy is not very good , because the energy is always positive , unlike charge . the reason magnetic monopoles make sense in em is because of electric-magnetic duality , a parity violating symmetry between the electric and magnetic charge in the free maxwell equations that is obviously broken by the fact that sources do not have magnetic charge . the gravitational field has many magnetic field analogs , the field is the levi-civita connection , which decomposes into lots of different nonrelativistic things in different components , but none of these can appear without an ordinary gravitational field at long distances . but there are ways of embedding electromagnetism into gr , as kaluza klein showed , and then there are special solutions of gr that can be interpreted as magnetic monopoles of the electromagnetic reduction . there is also a whole industry of finding self-dual solutions to gr , where the duality property can be thought of as analogous to electric magnetic duality , because it involves the epsilon tensor . self dual gravitational fields , and the decomposition of the so ( 3,1 ) holonomy in gr into two su ( 2 ) 's are both extremely important and fascinating topics which contain many exact solutions , and the inspiration for loop quantum gravity . if you would like some insight into self-dual solutions , this is a better question than asking for gravitational analogs to magnetic monopoles , because these analogs are not there .
dear leandro , it is because the pauli matrices , together with the $2\times 2$ identity matrix , form a full real basis of all the hermitian $2\times 2$ matrices ( note that $2\times 2$ hermitian matrices depend on four real parameters ) , and the identity matrix is irrelevant in a hamiltonian because it is just a conventional energy shift that acts on all vectors equally and does not create any subtleties such as line crossing so one may omit it in any discussion of interesting physical effects . so any $2\times 2$ hermitian matrix that " matters " - and yes , hamiltonians have to be hermitian operators - is a real linear combination of the three pauli matrices . they do not have to be interpreted as a spin of any kind . but they are still a more natural basis than any other basis of the hermitean matrices because the product of any pair of the matrices generates a multiple of another matrix in the basis , thus simplifying all the calculations . ( analogy with the conventional $so ( 3 ) $ generators is a more transparent way to see the power of this basis . ) but even if you chose a less clever basis than the pauli matrices , you could derive the same results . the equations could just become a bit more cumbersome . line crossing etc . is first observed at degeneracy 2 . however , one may also study $3\times 3$ or $n\times n$ matrices . conventional bases of the hermitian matrices are called the gell-mann matrices in the case of $su ( 3 ) $ - because gell-mann generalized the pauli matrices when he studied the strong force where $su ( 3 ) $ enters in two ways . the $n\times n$ hermitian matrices depend on $n^2$ real parameters . one usually takes the identity matrix to be one of the basis vectors ; the remaining $n^2-1$ vectors are " generators of $su ( n ) $" .
i could be wrong , but in my understanding , you are describing and justifying steady state , not detailed balance . in thermal equilibrium , steady state is true always , and detailed balance is true sometimes . detailed balance means that the rate $x \rightarrow y$ is always the same as the rate $y \rightarrow x$ . if a system is both in thermal equilibrium and has time-reversal symmetry , you will have detailed balance . if time-reversal symmetry is broken , for example an electrolyte in an external magnetic field , you can have thermal equilibrium but you will not necessarily have detailed balance . the process $x \rightarrow y$ might be balanced by $y \rightarrow z \rightarrow x$ , instead of being balanced by $y \rightarrow x$ . as an explanation of steady-state , your " continuity equation " explanation is fine . but in my opinion you can say the same thing more clearly without using the words " continuity equation " or writing down any math . if you just say that the number of times per second that the system enters state y equals the number of times per second that the system leaves state y , i think that is intuitively sensible .
the argument that one can only detect positions precisely using high-energy photons is very often used to justify the uncertainty principle but it its not its ultimate justification . the argument 's place in physics is curious : it essentially states that , even if quantum mechanics were wrong , these kind of observer effects would also prevent us from measuring position and momentum simultaneously . the real ground the uncertainty principle stands on is de broglie 's postulate : particles of matter are also waves ( of some sort ) and these waves have wavelength $$\lambda=\frac hp . $$ if you substitute this expression for the particle 's momentum into the uncertainty principle , you get it in the form $\delta x \cdot\delta\frac1\lambda\geq\frac1{2\pi}$ , or in terms of the wavenumber $k=2\pi/\lambda$ , $$\delta x \cdot\delta k\geq1 . $$ this now expresses a fundamental fact of wave physics : you can have a localized wavepacket , but you can only localize it to - roughly - a region the size of the wavelength . more precisely , the more you want to localize a wavepacket , the more waves with different wavelengths you will need to superpose to create it .
short answer . the relation between newtons and kilograms , with respect to work&nbsp ; /&nbsp ; kinetic energy , is actually just through newton 's second law ! here we are not mixing up newtons and kilograms ( forces and masses ) . the mass plays a role in the kinetic energy equation precisely because mass plays a role in how much force it takes to accelerate an object from rest to a speed v . example . consider for instance an object which has a mass of , say , 0.1019&nbsp ; kg . the gravitational force that this body experiences at the surface of the earth is &minus ; 1&nbsp ; n ( that is , a downward force ) , if we take g &nbsp ; =&nbsp ; &minus ; 9.81&nbsp ; m/s&sup2 ; . if we hold this object from a height of one meter , and allow it to drop , gravity performs work on this object , exerting a force of &minus ; 1&nbsp ; n over a ( downward ) displacement of &minus ; 1&nbsp ; m . as a result , just before impacting the ground , the object will have 1 joule of kinetic energy , as ( &minus ; 1&nbsp ; n ) &nbsp ; &middot ; &nbsp ; ( &minus ; 1&nbsp ; m ) = +1&nbsp ; j ; gravity will have done 1 joule worth of work on the object , which causes it to move with 1 joule worth of kinetic energy . in the examples you give , the correspondence that you are looking for is between mass and the gravitational force that it exerts . when you say that you take "1 newton " and lift it one meter , the ' newton ' you are referring is one newton of force exerted downward by an object with some mass &mdash ; or more to the point , the one newton which would be the minimum necessary to raise it steadily by opposing gravity . derivation we can actually show directly how mass comes into the kinetic energy formula , and pinpoint the reason that it is there . suppose that we exert a net force f on an object with mass m , over some displacement d in the same direction as the force , where the object starts at rest . the amount of work done on the object will be $$ w \ ; =\ ; \mathbf f \cdot \mathbf d \ ; =\ ; fd , $$ taking f to be the magnitude of the force f , and d to be the length of the displacement d . because we are doing net work on the object starting from rest , this work will go directly towards the kinertic energy of the object . the acceleration of the object under this force is $$ \mathbf a \ ; =\ ; \mathbf f / m . $$ if t is the time that it takes for the object to be moved by a displacement of d , we have $$ \mathbf d \ ; =\ ; \tfrac{1}{2}\mathbf a \ , t^2 \ , ; \qquad\implies\qquad t \ ; =\ ; \sqrt{2d/a\ ; } \ ; =\ ; \sqrt{2dm/f\ ; } . $$ taking the magnitude a &nbsp ; =&nbsp ; ||&nbsp ; a &nbsp ; || . the speed that the object is travelling after that amount of time is just $$\begin{align*} v \ ; =\ ; a t \ ; and =\ ; \bigl ( f/m\bigr ) \sqrt{2dm/f\ ; } \\ [ 1ex ] and = \sqrt{2fd/m\ ; } , \end{align*}$$ applying newton 's second law again for the acceleration , and cancelling factors of f and m under the square-root . we may then re-express this equation as $$ v^2 \ ; =\ ; 2fd/m \qquad\implies\qquad \tfrac{1}{2}m\ , v^2 = fd = w , $$ where we just applied the formula for the work done on the object at the end . because the net work is the same as the kinetic energy in this case , it follows that $k = \tfrac{1}{2}m\ , v^2$ . in the derivation above , the only ways we used mass was with newton 's law , a &nbsp ; =&nbsp ; f / m . so , the fact that it occurs in the formula for kinetic energy is not because we are confusing mass with force , but because of the relationship between mass and force .
charge is a fundamental conserved property of particles . it is , if you like , a measure of how much a particle interacts with electromagnetic fields . a particle with charge can produce and be affected by electromagnetic fields . this is what we mean when we say a particle has charge . its a simple quantised way to measure the coupling strength of particles with the appropriate force . e . g : charge for em force , color charge to strong force , etc .
not only is it possible to reduce the speed of light , but it is impossible to measure the speed of light unless it has been reduced because light moving though a medium other than space will not be traveling at c - but will be traveling slower because of interference with matter ( not necessarily molecules ) . however your particular question might be rooted in the fact that light will always be measured as moving at v ( reduced speed of light ) no matter the velocity of the observer -- even if the observer is moving at . 9v . so in a sense light cannot be measured at moving slower that v in any given medium -- but that velocity is most likely not as fast as c . there are equations for calculating the speed of light in different mediums ( such as air ) . v=c/n where n is the refractive index of the medium . so , in air ( which has a refractive index very close to 1 ) , light is nearly moving at 3.00x10^8 m/s . however , in water ( which has a refractive index of 1.33 ) the speed of light would be closer to 2.25x10^8 m/s . there is a famous experiment where physicists measured the speed of light in bose-einstein condensate . light travels at 38 mph in bose-einstein condensate , slower than a vehicle on the freeway . you say : but i have read in special theory of relativity that we can make light take more time to travel a distance . however , as far as a photon is concerned it does not experience time . a photon can travel anywhere in the universe in an instant . i recommend you look at the relativistic velocity equations . you should find , if you solve the equation for time , with velocity=c , that the time will be undefined . the trick with relativity is understanding that 1-you will never be able to understand it completely , and 2-the frame of reference can make all the difference , understanding a problem and all the frames involved can be the hardest part of solving it .
yes , the expansion of space itself is allowed to exceed the speed-of-light limit because the speed-of-light limit only applies to regions where special relativity  a description of the spacetime as a flat geometry  applies . in the context of cosmology , especially a very fast expansion , special relativity does not apply because the curvature of the spacetime is large and essential . the expansion of space makes the relative speed between two places/galaxies scale like $v=hd$ where $h$ is the hubble constant and $d$ is the distance . when this $v$ exceeds $c$ , it means that the two places/galaxies are " behind the horizons of one another " so they can not observer each other anytime soon . but they are still allowed to exist . in quantum gravity i.e. string theory , there may exist limits on the acceleration of the expansion but the relevant maximum acceleration is extreme  planckian  and does not invalidate any process we know , not even those in cosmic inflation .
to perform canonical quantization of a fermion field , we write the field in creation and annihilation operators . writing the hamiltonian in those creation and annihilation operators , we find that the energy of a field is unbounded from below ( it can be as negative as you like ) . this would be a disaster ; a field could forever decay to lower energy states by e.g. the emission of a photon . that is not what we see . if , however , we insist that the field obeys an anti-commutation rule ( the pauli exclusion principle ) , the energy is bounded from below ( cannot be as small as you like ) . the situation is saved . to summarise : physically , fermions must obey the pauli exclusion principle , because if they did not , they could forever decay to lower energy states . for detail and the mathematics , see any introductory book on quantum field theory .
what the question refers to as " band intensity " is also referred to a " line strength " $s$ . to calculate an absorption coefficient $k$ from $s$ , a line shape function $f ( \nu - \nu_0 ) $ , where $\nu_0$ is the center of the line . $$k = sf ( \nu - \nu_0 ) $$ then " optical depth " = $ku$ , where $u$ is called " path length " but is really a measure of the absorbing substance in the path . see pages 15 and 16 of this lecture for more information : http://irina.eas.gatech.edu/eas8803_fall2009/lec5.pdf and also : http://nit.colorado.edu/atoc5560/week4.pdf
$$\delta y = v_0 \sin ( \theta ) t - \frac12 gt^2$$ this is a projectile , so it will hit its max at $t_{max}=\frac12t$ , where $t$ is the total time in which the projectile flies . the total time is when , as you know , $\delta y=0$ . hence we have got : $$0=v_0 \sin ( \theta ) t - \frac12 gt^2$$ $$\require{cancel}\frac12 gt^\cancel{2}=v_0 \sin ( \theta ) \cancel{t}$$ $$t=\frac{v_0 \sin ( \theta ) }{\frac12 g}=\frac{2v_0 \sin ( \theta ) }{g}$$ that is the total time of the projectile motion . take half of that to get the time of the maximum height : $$\frac12 t = t_{max} = \frac{v_0 \sin ( \theta ) }{g}$$ now plug it in back to $\delta y$ and you will get the maximum $y$ displacement : $$\delta y_{max} = v_0 \sin ( \theta ) t_{max} - \frac12 gt_{max}^2$$ $$\require{cancel}\delta y_{max}= v_0 \sin ( \theta ) \left ( \frac{v_0 \sin ( \theta ) }{g}\right ) - \frac12 g\left ( \frac{v_0 \sin ( \theta ) }{g}\right ) ^2=\frac{v_0^2 \sin^2 ( \theta ) }{g} - \frac12 \cancel{g}\left ( \frac{v_0^2 \sin^2 ( \theta ) }{g^\cancel{2}}\right ) =\frac{v_0^2 \sin^2 ( \theta ) }{2g}$$ just for bonus , here 's a mathematical proof as to why it is at maximum height halfway through : $$y ( t ) =v_0 \sin ( \theta ) t - \frac12 gt^2$$ $$\frac{dy}{dt}=y' ( t ) =v_0 \sin ( \theta ) - gt$$ $$0=v_0 \sin ( \theta ) - gt\rightarrow gt=v_0 \sin ( \theta ) \rightarrow t=\frac{v_0 \sin ( \theta ) }{g}$$ which is , again , halfway . of course it is a maximum because : $$\frac{d^2y}{dt^2}=y'' ( t ) =-g$$ cheers ! -shahar
an observation is an act by which one finds some information  the value of a physical observable ( quantity ) . observables are associated with linear hermitian operators . the previous sentences tautologically imply that an observation is what " collapses " the wave function . the " collapse " of the wave function is not a material process in any classical sense much like the wave function itself is neither a quantum observable nor a classical wave ; the wave function is the quantum generalization of a probabilistic distribution and its " collapse " is a change of our knowledge  probabilistic distribution for various options  and the first sentence exactly says that the observation is what makes our knowledge more complete or sharper . ( that is also why the collapse may proceed faster than light without violating any rules of relativity ; what is collapsing is a gedanken object , a probabilistic distribution , living in someone 's mind , not a material object , so it may change instantaneously . ) now , you may want to ask how one determines whether a physical process found some information about the value of an observable . my treatment suggests that whether the observation has occurred is a " subjective " question . it suggests it because this is exactly how nature works . there are conditions for conceivable " consistent histories " which constrain what questions about " observations " one may be asking but they do not " force " the observer , whoever or whatever it is , to ask such questions . that is why one is not " forced " to " collapse " the wave function at any point . for example , a cat in the box may think that it observes something else . but an external observer has not observed the cat yet , so he may continue to describe it as a linear superposition of macroscopically distinct states . in fact , he is recommended to do so as long as possible because the macroscopically distinct states still have a chance to " recohere " and " interfere " and change the predictions . a premature " collapse " is always a source of mistakes . according to the cat , some observation has already taken place but according to the more careful external observer , it has not . it is an example of a situation showing that the " collapse " is a subjective process  it depends on the subject . because of the consistency condition , one may effectively observe only quantities that have " decohered " and imprinted the information about themselves into many degrees of freedom of the environment . but one is never " forced " to admit that there has been a collapse . if you are trying to find a mechanism or exact rule about the moments when a collapse occurs , you will not find anything because there is not any objective rule or any objective collapse , for that matter . whether a collapse occurred is always a subjective matter because what is collapsing is subjective , too : it is the wave function that encodes the observer 's knowledge about the physical system . the wave function is a quantum , complex-number-powered generalization of probabilistic distributions in classical physics  and both of them encode the probabilistic knowledge of an observer . there are no gears and wheels inside the wave function ; the probabilistic subjective knowledge is the fundamental information that the laws of nature  quantum mechanical laws  deal with . in a few days , i will write a blog entry about the fundamentally subjective nature of the observation in qm : http://motls.blogspot.com/2012/11/why-subjective-quantum-mechanics-allows.html?m=1
i did not do more than read newton , and a few commentators , so my insight on this is probably meager . but i am sure that you are right that the inertial frame interpretation of the first law is only a modern ex-post-facto justification for making it separate from the second law . newton certainly never used the first law to define an inertial frame , he just assumed you had one in mind , since inertial frames were not the focus of his investigation . i think that the statements of the laws of motion are unfortunately following aristotle more than euclid . since physics is no longer regarded as philosophy , we value independence of axioms over clarity of philosophical expounding , and this makes the first law redundant . but if you are stating a philosophical position--- that things maintain their state of motion unless acted upon--- newton 's first law is a neat summary of the foundation of the world-system . note that newton does not state it as " a body in linear motion continues moving linearly " . he includes rotational motion too , even though this is a different idea . i think he conflates the two to fix in mind the philosophical position that uniform motion is the natural state of all objects . in aristotle , the natural state of massive stuff like " earth " is to be down at the center of universe , and of light stuff like " fire " to be up in the heavens , leading to gravity and levity . newton is replacing this notion with a different notion of natural state . then the second law talks about deviations from the natural state , and is a separate philosophical idea ( although not a separate axiom in the mathematical sense ) . the influence of aristotle has ( thankfully ) declined through the centuries , making newtons laws a little anachronistic . i think that we do not have to be so slavish to newton nowadays . newton was aware of the importance of linear momentum and angular momentum conservation . one other way of understanding and his first law can be thought of as making the conservation laws primary . this point of view is both closer to newton 's thinking ( it is what makes his " natural states " natural ) , and it is also a better fit with modern understanding . so it might be nice to restate the first law as " linear momentum and angular momentum are conserved " . all this is based on personal speculation , not on sound historical research , so take with a grain of salt .
after you fix the closed string vertex operator , the remaining group of isometries is one dimensional and its volume is finite . for example if the vertex operator is fixed at the center of the disk , the remaining isometries are rotations . the volume of that group in some units is $2\pi$ . figuring out the precise constants involved is a bit messy though . there is another subtlety with this calculation - the on-shell graviton is pure gauge and strictly speaking this amplitude is zero . but with appropriate limiting procedure you can extract the tension as the proportionality constant involved in a slightly off-shell amplitude ( which is slightly ill-defined ) . this is another reason why the annulus calculation is cleaner .
we can , and have , taken " pictures " of individual atoms in materials . one example is this image taken at ibm : each dot is a single atom that was placed on the substrate . you can find similar images taken by ibm here . this image was made using a technique called " scanning tunneling microscopy " ( stm ) which relies on quantum tunneling . it is not an optical technique , like the microscopes we use in school . but it uses a probe ( which is just a sharp tip ) which is at some voltage with respect to the sample being imaged , and because of the voltage difference between the probe and the sample , electrons tunnel through the space in between and a " tunneling current " is observed . this tunneling current can be a function of several different things , like the applied voltage difference , and the height of the sample from the tip etc . the resolution of this technique is ~ $0.1 \text{ nm}$ , and can resolve individual atoms . a similar technique is atomic force microscopy ( afm ) , which relies on measuring the force between the tip and the sample . the elaborate ( and extensive ! ) details are in the wikipedia page that i linked to , but the principle of operation is still very similar . if the force between the sample and the tip is kept constant , then the height between them will change . and if you can keep track of the height , you get an " image " of the sample . this technique is also capable of resolving individual atoms , but i think not quite as well as stm , but i could be wrong about that .
if you want an overview over string theory without any ! mathematical background , i can still recommend brain greens " the elegant universe " . a real book on string theory is " barton zwiebach- a first course in string theory " it starts very slow and develops the math behind the theory from the very beginning . even if you have a strong mathematical background , i think it is worth to read it , because it is completly adapted to what you need in string theory . i did not finish it but i liked it .
gravitational waves are transverse but their possible polarizations are described not by a transverse vector but by a transverse tensor . electromagnetic waves moving in the $z$ direction may have two possible polarization vectors $x$ or $y$ , or their ( complex ) linear combinations  vectors perpendicular to the $z$ axis . gravitational waves in 3+1 dimensions also have two polarizations that may be described by the components of a tensor $h_{xx}=-h_{yy}$ and by $h_{xy}$ . one may again consider complex linear combinations of these polarizations , e.g. circular polarizations not too different from the electromagnetic case . in the gravitational case , we need a tensor with two indices  that is why we also say that the gravitons have spin $j=2$ , unlike photons ' $j=1$ . the tensor $h_{\mu\nu}$ in general is symmetric so it has 10 components to start with . however , they have to obey $k^\mu h_{\mu\nu} = 0$ which reduces the number of polarizations to six . this vanishing of the " inner product " is the transverse condition which is why it is right to say that the waves are transverse even though the polarizations are tensor-like . the polarizations of the form $h_{\mu\nu} = k_\mu\lambda_\nu+\lambda_mu k_\nu$ are " pure gauge " , resulting from diffeomorphisms , so they are unphysical . the reduces the 6 candidate polarizations to 3 . finally , there is a traceless condition $h^\mu{}_\mu = 0$ which reduces the number of independent polarizations to 2 , just like for the electromagnetic waves .
what you observe is gibbs ' paradox . the resolution comes about by postulating that the particles are indistinguishable ( and thereby introducing a factor $1/n ! $ ) . then the entropy and the free energy becomes extensive ( in the thermodynamic limit $n , v\to\infty$ , $n/v=\text{const}$ ) .
burning wood is three processes : gasification - under heat and little oxygen , the wood is turned into combustible gases ( mostly carbon monoxide , hydrogen and gaseous tar ) and charcoal combustion of charcoal combustion of gasses the optimum conditions for these are not exactly the same ( i will dig out my thesis to look up the particulars ) . a good stove will have one area where char burns and the gasification happens . then , additional air is added to the ( combustible ) exhaust to burn it more or less completely . an additional complication is that thegaseous tar tends to ( partly ) polymerize in the flame and form soot that usually does not burn - the yellow in the flame you see is glowing soot . so , in summary , burning wood completely is sometimes possible but hard , but good approximations exist .
are photons electromagnetic waves , quantum waves , or both ? a great ensemble of photons build up the electromagnetic wave . if i subdivide an electromagnetic field into smaller electromagnetic fields , should i eventually find an electromagnetic wave of a photon ? this experiment has been done with lasers bringing down to individual photon strength in this double slit experiment : the movie shows the diffraction of individual photon from a double slit recorded by a single photon imaging camera ( image intensifier + ccd camera ) . the single particle events pile up to yield the familiar smooth diffraction pattern of light waves as more and more frames are superposed ( recording by a . weis , university of fribourg ) . you ask : how can individual quantum waves combine to form the macroscopic observable of an electromagnetic field ? it needs some strong math background , but handwaving : both the classical electromagnetic wave and the quantum photon rely on solutions of maxwell 's equations . the individual photons carry information about the frequency ( e=h*nu ) and the spin and electromagnetic potential in the equation , since the quantum mechanical wavefunction of the photon ( which gives the probability distribution of the photon ) and the classical wave depend on the same equations . there is a coherent synergy and the zillions of photons add up to give the classical wave .
yes , in fact one of the comments made to a question mentions this . if you stick to newtonian gravity it is not obvious how a photon acts as a source of gravity , but then photons are inherently relativistic so it is not surprising a non-relativistic approximation does not describe them well . if you use general relativity instead you will find that photons make a contribution to the stress energy tensor , and therefore to the curvature of space . see the wikipedia article on em stress energy tensor for info on the photon contribution to the stress energy tensor , though i do not think that is a terribly well written article .
all we know about the size of quarks is that they are smaller than the resolution of any measuring instrument we have been able to use . in other words , they have never been shown to have any size at all . most physicists suspect that they are not actually points , but we do not know how small they are . the same goes for electrons , by the way . ( protons and neutrons do have a known size , around $1\text{ fm}$ across . )
consider a theory of fields $\phi:m\to t$ where $m$ is a manifold , and $t$ is a set . in physics , $t$ is often either a vector space or a manifold . we call $m$ the domain of the theory , and we call $t$ the target space . of the theory . we call a function from $m$ to $t$ a field configuration , and the set of all field configurations is denoted $\mathcal f$ . let 's consider two situations : case 1 . let groups $g_m$ and $g_t$ be given . let $\rho_m$ be an action of $g_m$ on $m$ , and let $\rho_t$ be an action of $g_t$ on $t$ , then there is a " natural " action of $\rho_\mathcal f$ of $g_m\times g_t$ on $\mathcal f$ given by \begin{align} \rho_\mathcal f ( g_m , g_t ) ( \phi ) ( x ) = \rho_t ( g_t ) \big ( \phi\big ( \rho_m ( g_m ) ^{-1} ( x ) \big ) \big ) \end{align} i will leave it to you to prove that this is a group action . case 2 . let a groups $g$ be given . let $\rho_m$ be an action of $g$ on $m$ , and let $\rho_t$ be an action of $g$ on $t$ , then there is a " natural " action of $\rho_\mathcal f$ of $g$ on $\mathcal f$ given by \begin{align} \rho_\mathcal f ( g ) ( \phi ) ( x ) = \rho_t ( g ) \big ( \phi\big ( \rho_m ( g ) ^{-1} ( x ) \big ) \big ) \end{align} i will leave it to you to prove that this is in fact a group action . now , let us phrase your question in the following way : in either of the above cases , is their a sense in which $\rho_m$ induces $\rho_t$ ? the answer , as far as i am aware , is that it depends on the context and on what you mean by " induced . " let 's consider the example you give in your statement of the question . example . an $\mathrm{o} ( 2 ) $ vector field on $\mathbb r^{3,1}$ . we have \begin{align} m = \mathbb r^{3,1} , \qquad t = \mathbb r^2 , \qquad g_m = \mathrm{p} ( 3,1 ) , \qquad g_t = \mathrm{o} ( 2 ) \end{align} where $\mathrm{p} ( 3,1 ) $ is the poincare group in four dimensions . in this case , one often takes $\rho_m$ and $\rho_t$ to be \begin{align} \rho_m ( \lambda , a ) ( x ) = \lambda x+a , \qquad \rho_t ( r ) ( v ) = rv \end{align} notice that this falls under case 1 above . in this case , there is no " canonical " relationship ( as far as i am aware ) between the poincare group and $\mathrm{o} ( 2 ) $ , so there is no canonical sense in which $\rho_m$ induces $\rho_t$ . however , consider the following example : example . an $\mathrm{so} ( 3 ) $ $2$-tensor field on $\mathbb r^{3}$ . we have \begin{align} m = \mathbb r^{3} , \qquad t = t_2 ( \mathbb r^3 ) , \qquad g = \mathrm{so} ( 3 ) \end{align} where $t_2 ( \mathbb r^3 ) $ is the vector space of $2$-tensors on $\mathbb r^3$ . then there is a natural action of $g$ on $m$ given by \begin{align} \rho_m ( r ) x = rx \end{align} since $2$-tensors on $\mathbb r^3$ can be described as bilinear maps functions $s:\mathbb r^3\times \mathbb r^3 \to \mathbb r$ , there is also a natural action of $g$ on $t$ given by \begin{align} \rho_t ( s ) ( x_1 , x_2 ) = s ( r^{-1}x_1 , r^{-1} x_2 ) \end{align} which can also be written as \begin{align} \rho_t ( s ) ( x_1 , x_2 ) = s ( \rho_m ( r ) ^{-1}x_1 , \rho_m ( r ) ^{-1} x_2 ) \end{align} so , in this case , there is a sense in which $\rho_m$ has induced $\rho_t$ .
the small changes in sunrise and sunset are caused by the tilt of the world , and the changes in light the earth gets causes winter and summer . it is been going on for millions of years , so there is no real harm there . one might also note things like the effect of these on thunderstorms , because the thunderstorms come ultimately from the earth 's magnetic field , and this can be influenced by the solar wind and the moon . nasa keeps an eye out for ' solar flares ' which , while not part of weather , can cause a nasty shock to society chance one comes our way .
to answer your question let me start with the most basic constituents of the universe , i.e. elementary particles . standard model of particle physics contains matter particles ( quarks and leptons ) , force carriers ( w/z , photon , gluon ) and the higgs particle . photon and gluon are massless , and the rest of the particles have non-zero masses . in a non-interacting situation , all those particles have their masses fixed , independent of their speed ( so no relativistic mass which is an old way of thinking about relativistic kinematics anyways ) . when there are interactions between particles and enough energy to produce new particles ( through $e=mc^2$ ) , weird things happen . at this point we need quantum field theory to fully understand what is going on . without going much into details , the way we think about the interactions is through force carrying particles i mentioned in the previous paragraph . when two particles interact , they can simply scatter off each other or the interaction can create a complete different set of particles . we understand the process of going from an " initial " set of particles to a " final " set of particles in terms of all the " paths " connecting them with the interactions allowed by the standard model . this situation is very similar to what happens in the double slit experiment . during this process virtual particles ( from the above set i mentioned ) are created . these intermediate particles have the exact same properties as the real ones , except their mass ( or invariant mass ) can be different than their actual mass . as in the double slit experiment , nature takes all the allowed paths between initial and final states . at this point we can talk about the energy of our particles . according to special relativity there is a rest frame for any massive particle and in this special frame even though our particle is at rest it will have an energy given by $e_0 = mc^2$ . so for a massless particle like photon there is no rest frame and hence no rest energy . but all the particles ( massive or massless ) have momentum , and a total energy given by $e = \sqrt{ ( mc^2 ) ^2 + ( pc ) ^2}$ . note that this reduces to $e = pc$ for a massless particle like photon . if you think in terms quantum physics , an electromagnetic wave contains photons . so the energy of the electromagnetic wave comes from the energy of the photons i.e. the momentum carried by the photons ( which is also related to their frequency or wavelength ) . so far we talked about mass and energy of fundamental particles . what happens when we have bound states like protons/neutrons or nuclei or atoms or molecules ? the mass of these compound objects depend on the bounding energy ( or potential energy ) that keeps them together . so for example the mass of a proton is not equal to the total mass of the quarks that makes up the proton ( two up , one down quark ) but is mostly created by the interaction between these quarks explained by quantum chromodynamics ( qcd ) . since there is more than one fundamental particle making up our bound states , there can also be excited states with slightly different masses . an atom for example can absorb a photon and switch to an excited state for a very brief time which has a different mass . this mass difference is usually very small . this only happens when the energy of the photon matches the difference between the discrete energy levels of the atom . most of the time they will only scatter like billiard balls . for an even larger bound system like a solid object , an absorbed electromagnetic wave usually changes the temperature of the object . in this larger system , we can think of atoms organized in a geometric pattern . on the average they sit at fixed points on a 3d lattice but they all individually vibrate around their equilibrium points . so an electromagnetic wave absorbed by the object usually changes the amplitude of these vibrations or the average vibration energy i.e. the temperature of the object .
if you write $f = -b ( \mathrm{d}x/\mathrm{d}t ) ^2$ as $f = -bv^2$ instead , newton 's second law becomes $$m\frac{\mathrm{d}v}{\mathrm{d}t} = -bv^2$$ which should look a lot more manageable .
reaching the speed of light requires infinite energy . let a proton of mass $m_p$ have a velocity $v$ . then by the energy-mass equivalence : $$e = \frac{m_pc^2}{\sqrt{1 - \frac{v^2}{c^2}}}$$ which goes to infinity as $v$ approaches $c$ . since you can not supply infinite energy to the proton , reaching $c$ is impossible . you can get close to $c$ as the lhc does but you will never ever reach $c$ .
this was suggested by asaph hall in 1894 , in an attempt to explain the anomalies in the orbit of mercury . i retrieved the original article in http://adsabs.harvard.edu/full/1894aj.....14...49h interestingly , he mentions in the introduction that newton himself had already considered in the principia what happens if the exponent is not exactly 2 , and had concluded that the observations available to him strongly supported the exact power 2 ! the story is retold , e.g. , on p . 356 of n.r. hanson , isis 53 ( 1962 ) , 359-378 . see also section 2 of http://adsabs.harvard.edu/full/2005mnras.358.1273v
it is about 0.003 gev , see just to be sure , the measured width of the bumps is due to experimental errors and other things that depend on the situation , not because of the higgs ' intrinsic width . the width dramatically increases with the mass . as one approaches a tev , the width would be almost exactly equal to the mass itself . for the sake of completeness , this is the graph of the branching ratios ( proportions of the decays ending with a given final state ) : the 125 higgs decays to ( virtual or real ) $b\bar b$ ( messy final state , a bit hard to isolate from the background ) in 65% of cases , $ww$ ( neutrinos from the decayed $w$ are missing energy ) in 20% , $gg$ ( messy ) in 7% of cases , $\tau\tau$ in 6% of cases , $zz$ in 3% of cases , $c\bar c$ in 2% of cases , $\gamma\gamma$ in 0.2% , $\gamma z$ in 0.15% of cases . the numbers were estimated by looking at the graph above so they do not quite add to 100 percent , sorry .
thanks for the links above . after pursuing my research i found this quite exhaustive bibliography linking to online resources : http://www.personal.uni-jena.de/~p5thul2/notes/adscft.html i hope it will not be useful only to me ; )
the most natural and numerous contributions of string theory to other fields are string theory 's contributions to the most closely adjacent fields , mathematics and quantum field theory ( both its conceptual understanding as well as model building ) . mathematics string theory is the framework in which mirror symmetry was discovered ; mathematicians have converted it into a subdiscipline of algebraic geometry that is studied by rigorous mathematical tools these days . quite generally , much of algebraic geometry , especially topics related to calabi-yau manifolds , topology of higher-dimensional manifolds , k-theory , are using lots of tools that were first invented with the help of string theory  including topological string theory ; the mechanisms of stringy tachyon condensation ( k-theory ) , non-commutative geometry ( which is naturally realized as string theory with a nonzero b-field ) and others . quantum field theory : formal and conceptual part in the context of the western civilization , supersymmetry was born in the context of string theory  when pierre ramond was incorporating fermions into the old bosonic string theory . ( russians independently discovered supersymmetry purely by analyzing possible symmetries from a mathematical viewpoint . ) string theory reasoning has also been important to discover many properties of the supersymmetric quantum field theories such as the $n=2$ gauge theories ( seiberg-witten ) , and others . because of the ads/cft and other insights , people learned what physical phenomena dominate the strongly coupled limits of many quantum field theories . quantum gravity these days , we consider the terms " quantum gravity " and " string theory " to be more or less synonyma because string theory is the only known , and quite likely the only mathematically possible , consistent quantum theory of gravity in dimensions 3+1 and higher . however , we may also view quantum gravity as a separate subject . if we do so , we may view string theory as a toolkit that has brought us many new important insights and answers to old questions . for example , the information is preserved when the black holes evaporate ; topology of spacetime may change , and so on , and so on . quantum field theory : model building model builders propose possible new particles , forces , and phenomena that could be discovered by future particle physics experiments if they discover anything at all . the internal consistency and the consistency with the known facts about particle physics are the only constraints so there is a lot of room for model builders ' imagination . however , some of the most conceptually new and interesting possibilities were invented either because they were directly inspired by string-theoretical research , or the research of similar topics was continuing simultaneously in string theory and quantum field theory and the quantum field theory insights were found to be very naturally embedded in string theory  and string theory often provided physicists with new insights . various models of large extra dimensions and warped extra dimensions fall into both of these groups much like deconstruction and other quantum field theories with some new conceptual interpretations and behavior . ads/cmt and ads/anything the application of string theory tools to many other , unexpected disciplines exploded after the 1997 discovery of the ads/cft correspondence by juan maldacena . conformal ( scale-invariant ) field theories are more or less omnipresent in physics and in very many of them , it is been proposed that the physics is equivalent to the physics of a quantum gravitational theory , string theory , in a particular curved anti de sitter ( or similar ) spacetime background with a dimensionality higher by one . in this method , many physical objects that seemed to have nothing to do with fundamental physics were studied as manifestations of higher-dimensional ads black holes . the examples include fluids  most famously , string theory naturally calculated the bound on the viscosity-entropy ratio ( although " non-stringy " arguments for the same value were found later , too )  quark gluon plasma ( comparisons with the rhic collider ) , fermi and non-fermi liquids , superconductors , hydrodynamics etc . various applications in this list are established to different extents . the list of applications above is surely not exhaustive and i hope that other users will add complementary information .
there are two problems with seeing things a long way away . firstly there is the sheer distance - the brightness of a star falls off as the inverse square of distance so after 13 billion light years anything is going to be pretty dim . secondly there is the red shift . at 13 billion light years the red shift is about $z = 7$ i.e. the wavelength we see is a factor of eight $ ( z + 1 ) $ longer than the light originally emitted by the star . the longest wavelength we can see is about 700nm , so to see the star it must be emitting at a wavelength of less than about 100nm , which is in the hard ultra-violet . we can do a rough calculation using the distant galaxy z8_gnd_5296 as an example . i do not know the spectrum of z8_gnd_5296 , but we can do a rough calculation . wikipedia gives the apparent magnitude as 25.6 , and the sun has an apparent magnitude of 26.74 , so there are 52.3 orders of magnitude between the two . one magnitude is a brightness difference of $100^{1/5} \approx 2.512$ , so 52.3 magnitudes is a factor of about $10^{21}$ i.e. your eye receives $10^{21}$ fewer photons from z8_gnd_5296 than it does from the sun . a quick back of the envelope calculation tells me that the number of photons from the sun hitting the earth 's surface is about $3 \times 10^{21}$ per square metre per second , so the factor of $10^{21}$ neatly cancels out and we conclude that the number of photons from z8_gnd_5296 hitting the earth is about 3 per square metre per second . this is a very approximate calculation , so you should regard this as an order of magnitude estimate at best . according to wikipedia the diameter of the pupil in your eye is around 9mm ( assuming it is dark ) , and this is about $8 \times 10^{-5} m^2$ , so the number of photons entering one eye from z8_gnd_5296 is around 0.00025 per second . you would have to watch z8_gnd_5296 for about 4,000 seconds for a single photon to enter your eye . this is not quite what you asked of course , because z8_gnd_5296 is just one galaxy out of many at a distance of around 13 billion light years . i have absolutely no idea how many such galaxies you are looking at when you stare out of you bathroom window but there must be at least 4,000 and probably vastly more than this , so i would say it is a fair bet that every second at least one photon and probably many more enter your eye from galaxies 13 billion lioght years away .
metric expansion works ( for now ) on large scales only . this is because matter becomes clumped together by gravitation , which fights the expansion . these clumps of matter may continue expanding more slowly , or they may begin to contract , depending on the balance of gravity to cosmological expansion . the gravitational interactions inside the local group of galaxies are large enough that there is no measurable cosmological expansion going on . for example , the andromeda galaxy is within gravitational distance of the milky way , so the two galaxies are actually falling together , not expanding away . further out , between galaxy clusters , is where the metric expansion is really significant . so , no--the galaxies and solar systems do not follow the metric expansion . ( not that they could not expand for some other reason . ) see http://en.wikipedia.org/wiki/metric_expansion_of_space for a lot more info .
another way to look at it $$ e^x = \sum_n \frac{x^{n}}{n ! } = 1 + x +\frac{x^2}{2} + \dots $$ which comes down to adding quantities with different dimension , which you have already accepted makes no sense . this is why you can not exponentiate values with units . and we can do a similar thing with most transcendental functions .
light is an oscillating electric and magnetic field , so it is electrical and magnetic . later : re the edit to your question , i think there are two issues . firstly the interaction with electric charge and secondly the interaction with magnets . light does not carry any charge itself , so it does not attract or repel charged particles like electrons . instead light is an oscillating electric and magnetic field . if you take an electron and put it in a static electric field ( e . g . around a van der graaff generator ) then the electron feels a force due to the field and will move . this happens when an electron interacts with a light wave , but because the light wave is an oscillating field the electron moves to and fro and there is no net motion . if you could watch an electron as light passes by you had see it start oscillating to and fro , but it is net position would not change . this is exactly what happens in your tv aerial . the light ( i.e. . radio frequency em ) causes electrons in the tv aerial to oscillate and this oscillation generates an oscillating electric current . the voltage this generates is amplified by your tv . at the tv transmitter the same happens in reverse : an oscillating voltage is applied to the tv transmitter , the electrons oscillate in response and the oscillation generates an electromagnetic wave . so the process is oscillating electrons -> light -> oscillating electrons . i am not entirely sure what you mean by there is no transfer of electric charge/electrons ( as there is in ac/dc current in space ) . if the above does not satisfactorily explain what is going on maybe you could expand on your question . and finally on to the interaction with magnets . the big difference between electric and magnetic fields is that ( as far as we know ) there are no isolated magnetic charges . if there were isolated magnetic charges e.g. if you could watch a magnetic monopole as a light wave passed by then you had see similar behaviour to an electron . but there are not , so you do not .
the differential is used to specify that the number is for a " differential range " , which is a way to remind you that the notions involved are somewhat fuzzy . let me give a purely mathematical example . suppose i tell you that i am going to pick an arbitrary real number between 0 and 10 , with the likely hood of a number being picked being proportional to the number itself . what is the probability that i pick the number 7 ? the answer is 0 , because there are infinitely many numbers between 0 and 10 . but what if i ask : what is the probability that the number i picked is between $7$ and $7+\delta$ ? then the probability of course depends on how big $\delta$ is , but if $\delta$ is very small , then all numbers between $7$ and $7+\delta$ are roughly equally likely to be chosen , so we can say that the probability , for $\delta$ very small , is roughly linear in $\delta$ . and so we can say that the probability of choosing a number between $7$ and $7+\delta$ is equal to $q ( 7 ) \cdot\delta$ , where $q ( x ) $ is a function that specifies the " probability density " at the number $x$ . similarly , the salpeter function gives a distribution of the number of stars of a given mass related to the mass of the stars . but one should not state it as " the number of stars at a given mass " ! because if one expects , let us say , one star at each possible mass , since there are infinite number of allowed masses , there will then be infinitely ( in fact uncountably infinite ) many stars overall ! what the salpeter function gives you is that the number of stars between $m$ and $m+\mathrm{d}m$ , for $\mathrm{d}m$ very small , the number of stars can be described by some function $\psi ( m ) \mathrm{d}m$ , while the expected number of stars at exactly mass $m$ should be zero . in other words , the differential serves to remind you that you are dealing with a sort of a density , rather than a straight-up function .
conservation of particle current is nothing but the statement that a theory has to be unitary . in other words the scattering matrix $s$ has to obey $ss^\dagger=1$ defining $s=1+it$ i.e. rewriting the scattering matrix as a trivial part plus interactions ( encoded in $t$ which corresponds to your $f$ ) one finds from the unitarity condition : $itt^\dagger=t-t^\dagger=2im ( t ) $ $tt^\dagger$ is nothing but the crosssection ( i suppressed some integral signs here for brevity ) the optical theorem is right there . hence one finds $\sigma\sim im ( t ) $
it is actually very simple . the general lorentz transformation can be rewritten as $$\left ( \begin{matrix} 1 and 0 \\ 0 and h^\textrm{t} \end{matrix}\right ) \ , \left ( \begin{matrix} ct \\ x \\ y \\ z \end{matrix}\right ) = l_u \ , \left ( \begin{matrix} 1 and 0 \\ 0 and k^\textrm{t} \end{matrix}\right ) \ , \left ( \begin{matrix} ct^\prime \\ x^\prime \\ y^\prime \\ z^\prime \end{matrix}\right ) \ , . $$ this corresponds to aligning the $x$ and $x^\prime$ axes with the direction of the relative velocity , and then applying the standard lorentz transformation .
first let me start by saying that the $n$-body problem in classical mechanics is not computationally difficult to approximate a solution to . it is simply that in general there is not a closed form analytic solution , which is why we must rely on numerics . for quantum mechanics , however , the problem is much harder . this is because in quantum mechanics , the state space required to represent the system must be able to represent all possible superpositions of particles . while the number of orthogonal states is exponential in the size of the system , each has an associated phase and amplitude , which even with the most coarse grain discretization will lead to a double exponential in the number of possible states required to represent it . thus in quantum systems you need $o ( 2^{2^n} ) $ variables to reasonable approximate any possible state of the system , versus only $o ( 2^n ) $ required to represent an analogous classical system . since we can represent $2^m$ states with $m$ bits , to represent the classical state space we need only $o ( n ) $ bits , versus $o ( 2^n ) $ bits required to directly represent the quantum system . this is why it is believed to be impossible to simulate a quantum computer in polynomial time , but newtonian physics can be simulated in polynomial time . calculating ground states is even harder than simulating the systems . indeed , in general finding the ground state of a classical hamiltonian is np-complete , while finding the ground state of a quantum hamiltonian is qma-complete .
after much investigation , simulation and a deep literature search , i have figured out the true answer . you perceive a chirp because you are being hit with the echos of the sharp noise that generated the sound . the times between the arrival of those echos is decreasing inversely with time , so it sounds as if it were a tone with a fundamental frequency increasing linearly in time , hence the chirp . to get a feel for the phenomenon , consider a simulation : above you see a slowed down version of the simulated pressure wave inside a 2d racquetball court . i threw up the generated sound on soundcloud . if you watch the simulation , pick a particular point and watch the reflected sounds go by , you will notice the different instances of the multiple echos arrive faster and faster as time goes on . you can clearly hear the chirps in the generated sound , and if you listen closely you can hear secondary chirps as well . these are also visible in the spectrogram : this phenomenon was studied and published recently by kenji kiyohara , ken'ichi furuya , and yutaka kaneda : " sweeping echoes perceived in a regularly shaped reverberation room , " j . acoust . soc . am . vol . 111 , no . 2 , 925-930 ( 2002 ) . more info in particular , they explain not only the main sweep , but the appearance of the secondary sweeps using some number theory . worth reading in full . this suggests that for the best sweep one should both stand and listen in the center of the room , though they should be generic at any location . simple geometric argument following the paper , we can give a simple geometric argument . if you imagine standing in the middle of a standard racquetball court , which is twice as long as it is tall or wide , and clap , your clap will start propagating and reflecting off the walls . a simple way to study the arrival times is with the method of images , so you imagine other claps generated by reflecting your clap across the walls , and then reflections of those claps and so on . this will generate a whole set of " image " claps , located at positions $$ ( m , l , 2k ) l $$ where $m , l , k$ are integers and $l$ is 20 feet for a racquetball court , the time for any particular clap to reach you is $t = d/c$ and so we have $$ t = \sqrt{m^2 + l^2 + 4k^2} \frac{l}{c} $$ for our arrival times . if we look at how these distribute in time : it becomes clear why we perceive a chirp . the various sets of missing bars , which themselves are spaced like a chirp , give rise to our perceived subchirps . details of the 2d simulation for the simulation , i numerically solved the wave equation : $$ \frac{\partial^2 p}{dt^2} = c^2 \nabla^2 p $$ and used impedance boundary conditions on the walls $$ \nabla p \cdot \hat n = -c \eta \frac{\partial p}{\partial t} $$ i used a collocation method spatially , with a chebyshev basis of order 64 in the short axis and 128 on the long axis . and used rk4 for the time integration . i modeled the room as 20 feet by 40 feet and started it of with a gaussian pressure pulse in one corner of the room . i listened near the back wall towards the top corner . i put up an ipython notebook of my code , with the embedded audio and video . i recommend playing with it yourself . on my desktop it takes about minute to do a full simulation of the sound . effect of listening location i have updated the code to generate sound at multiple locations , and generate their sounds . i can not seem to embed audio on stackexchange , but if you click through to the ipython notebook view , you can listen to all of the generated sounds . but what i can do here is show the spectrograms : these are laid out in roughly their locations inside of the room . here the noise was generated in the lower left , but the chirps should be generic for any listening and generation location .
i would suggest that you continue with halliday and resnick for the present . and do not read courant for calculus now . not that its a bad book , but i do not think it is good introductory book . for a good introduction to calculus try stewart or thomas and finney . and as for physics , you could probably read the classic feynman lectures as well .
something i posted on reddit answers this question quite well , i think : " rational " and " irrational " are properties of numbers . quantities with units are not numbers , so they are neither rational nor irrational . a quantity with units is the product of a number and something else ( the unit ) that is not a number . by choosing the unit you use to express a quantity , you can arrange for the numeric part of the quantity to be pretty much any number you want ( though switching units will not let you change its sign or direction ) . in particular , it can be rational or irrational . and choices of units are a human convention , so it would not make any sense to extend the idea of rationality or irrationality to the quantity itself . you can use a natural unit system , where certain physical quantities are represented by pure numbers . for example , if you use the same units to measure time and space , $c = 1$ . in such a unit system , it does make sense to say the speed of light is rational , but that is kind of a special case . that reasoning does not really work with other physical quantities . and you really do have to be using natural units . ( technically , you could make a natural unit system where $c = \pi$ , but it would have very complicated and perhaps even inconsistent behavior under lorentz transforms , so nobody does that . ) by the way , empirical measurements always have some uncertainty associated with them , so they are not really numbers either and are also neither rational nor irrational . a measurement is probably better thought of as a range ( or better yet , a probability distribution ) which will necessarily include both rational and irrational numbers .
wet wood crackles . dry wood does not . water in the wood boils . the steam builds up pressure because it is trapped inside . the wood explodes , releasing the steam and flying pieces .
the way you convert between units is really just multiplying by several factors of 1 . but it is 1 written in a slightly unusual way . think about this : you are probably familiar with conversion factors in the form $$ ( \text{number} ) ( \text{unit} ) = ( \text{other number} ) ( \text{other unit} ) $$ but of course , you can divide both sides of any equation by the same thing , and the equation will continue to hold . so you can also write the conversion factor as $$\frac{ ( \text{number} ) ( \text{unit} ) }{ ( \text{other number} ) ( \text{other unit} ) } = 1$$ or $$\frac{ ( \text{other number} ) ( \text{other unit} ) }{ ( \text{number} ) ( \text{unit} ) } = 1$$ same equation , just rearranged a bit . now let 's see what happens when you have some value , expressed in $ ( \text{unit} ) $ , that you want to convert to $ ( \text{other unit} ) $ . as you know , you can always multiply anything by 1 , and that does not change the value at all . so , you can go look up the conversion relationship between $ ( \text{unit} ) $ and $ ( \text{other unit} ) $ , change it into one of the forms above , and use it like this : $$ ( \text{given value} ) ( \text{unit} ) \times 1 = ( \text{given value} ) ( \text{unit} ) \times \frac{ ( \text{other number} ) ( \text{other unit} ) }{ ( \text{number} ) ( \text{unit} ) }$$ since you have $ ( \text{unit} ) $ in both the numerator and denominator , you can cancel those out . note that it is up to you to pick the right conversion factor to use so that you can cancel out units ! anyway , you will be left with $$\frac{ ( \text{given value} ) ( \text{other number} ) }{ ( \text{number} ) } ( \text{other unit} ) $$ everything except for $ ( \text{other unit} ) $ is just a number , so now you can actually figure out the numerical value ( e . g . plug things into your calculator if necessary ) . if your given value has a compound unit ( one made as a combination of other units , as in your examples ) , then you will need to use more than one unit conversion factor - basically , you will have to multiply by 1 more than once , using a different conversion factor each time . in your example , you would have to multiply by a conversion factor to go from $\mathrm{hm}$ to $\mathrm{km}$ , and another one to go from $\mathrm{min . }$ to $\mathrm{hr . }$ here 's how that applies to your example , assuming mark eichenlaub is right about the mistake : $ ( \text{given value} ) = 5.66$ for one part of the unit conversion : $ ( \text{unit} ) = \mathrm{hm}$ $ ( \text{other unit} ) = \mathrm{km}$ the conversion factor is $10\text{ hm} = 1\text{ km}$ for the other part of the unit conversion : $ ( \text{unit} ) = \mathrm{min . }$ $ ( \text{other unit} ) = \mathrm{hr . }$ the conversion factor is $60\text{ min . } = 1\text{ hr . }$ note that the time units are in the denominator ! but that does not really change anything - you still just need to arrange the conversion factor in the way that makes the original units cancel out .
another thing that would be changed by a varying fine structure constant would be that it would alter almost every electromagnetically mediated phenomenon . all of the spectra of atoms would change . what would also change would be the temperature at which atoms can no longer hold onto their electrons , since the strength of attraction between electrons and the nucleus would change . this would then change the redshift at which the universe becomes transparent . the end result would be that the cosmic background radiation would be coming from a different time in the universe 's history than otherwise thought . this would have consequences for the values of cosmological parameters . once you alter phenomena in this stage of the universe 's history , though , you have to be quite careful to not disrupt the current predictions for how much hydrogen , helium , and heavy elements there are in the universe ( while creating nuclei depends mainly on the strong interaction , electromagnetism does have something to do with determining the final energies of the nuclei , and so can not be completely neglected--changing the fine structure constant changes these cross-sections ) . current theory predicts these things with great accuracy , and changing things around , particularly particle physics parameters that govern the length of the neucleosyntheis era ( which overlaps with , but is a subset of the time at which the universe is opaque ) potentially make these observations not agree with theory .
the general expression for calculating kinetic energy is $$ke = \frac{m v^2}{2} + \frac{i \omega^2}{2}$$ however , $v$ means the velocity of the center of mass and $\omega$ is rotational velocity around the center of mass . $i$ is moment of inertia about center of mass . you cannot do the above expression just for arbitrary point of the body . as for the second question and pulses of rotational movement : i think during the collisions both pucks roll against each other or against the wall for a very small fraction of time . when rolling you have static friction forces , which have great and temporal effects on speed , rotational speed and their relation .
rather than attempting to capture the energy of lightning , think about capturing the energy of the separated charge before lighting discharges the stored energy . find a way to convert the electrical energy of naturally separated charge in the atmosphere into useful work . . . galt 's motor if you will allow .
the book by wess and bagger is in my opinion the best book on supersymmetry . it is written in a style that is meant for application in qft , meaning , it is not too abstract and has alot of exercises . most papers i have read are written using the notation of wess and bagger . this might be due to the fact that wess ( together with zumino ) is one of the first people to actually use susy in a 4d qft context . http://www.amazon.com/supersymmetry-supergravity-julius-wess/dp/0691025304
quantum computers can process information in a different way then classical computers the main isuues is how to use classical input to " convince " the qbit to process what we are asking and then give an answer that we can use in a positive way . research is progressing towards a quantum level interface that can account for decoherence between the information we give and a understandable answer coming out . in other words the language barrier between classical people and quantum interpretation . there is no technical limitation to qc 's at this time only a limit of our ability to minipulate qbit s the do what we want . http://www.cra.org/ccc/docs/init/quantum_computing.pdf is a pdf document link to the currently imposed limitations of quantum computers .
what does orbits with definite energy mean ? it means that each orbit has an amount of energy associated with it . if you move between 2 orbits , it requires a certain amount of energy . it will be the same amount of energy for the same transition in the same atoms . different transitions have different amounts of energy and different atoms have different orbits ( resulting in different energies ) . why do the electron in their ground state not emit radiation ? the ground state is defined as when all the electrons are in the lowest energy configuration for the atom ( or ion ) . electrons emit radiation ( photons ) when they drop down in energy levels in the atom . if the electrons are in the ground state , they cannot drop lower in energy , thus emitting a photon .
it was not a black hole because the density was not sufficiently high . the density was lower than what is needed for a black hole because the volume was larger . the volume was larger because the atoms ( mostly hydrogen ) were kept away from each other by the pressure produced by the fusion processes . once the fusion processes stop , this source of repulsion between the atoms disappears , the volume shrinks , the density goes up , and the black hole threshold may be surpassed .
in the equation $$ c_v = at+ bt^3\ln t$$ the logarithmic term may be explained by paramagnons , i.e. fluctuations in which the adjacent atoms are demanded to be aligned . such fluctuations are long-lived . this explanation of the non-analytic term was found by doniach and engelsberg as well as berk and schrieffer . both articles are in prl 1966 . http://prl.aps.org/abstract/prl/v17/i14/p750_1 http://prl.aps.org/abstract/prl/v17/i8/p433_1 a full text of berk and schrieffer : http://books.google.cz/books?hl=enlr=id=yqu2bjfykrgcoi=fndpg=pa90dq=berk+schriefferots=vcx7dizdxgsig=hcfjbki-54txrpywjj9of7ovuqeredir_esc=y#v=onepageq=berk%20schriefferf=false brinkman and engelsberg discuss some limitations of the applicability of the log term in 1968: http://prola.aps.org/abstract/pr/v169/i2/p417_1 pethic and carneiro with their fermi liquid explanation came later . 1966 was before the renormalization group but it is not really needed for the calculations . the logarithmic corrections do arise from one-loop processes and similar corrections have been known in condensed matter physics and particle physics long before we knew about the right philosophical words linked to the renormalization group from the 1970s .
the reason lies in the nature of gravity : under the influence of gravity , all bodies , no matter what their mass is , accelerate at the same rate . this is also true for bodies attached to a rope , resulting in a pendulum . the force driving the pendulum is gravity , and hence , its rate of acceleration ( which effectively determines the period ) is the same for all masses .
the hamiltonian $$h=aj_z^2=a\left ( \sum_j s_z^{ ( j ) }\right ) ^2\tag1$$ is a function of the individual $z$ spin projections $s_z^{ ( j ) }$ , and all of those commute . therefore , the eigenstates will be product states of the form $$|\psi\rangle=|a_1\rangle\otimes\cdots\otimes|a_n\rangle , \tag2$$ where each $|a_j\rangle$ is either $|\ ! \uparrow\rangle$ or $|\ ! \downarrow\rangle$ . as such , the system is easily solvable , and you simply need to phrase your questions correctly . the expectation value $\langle j_x^2\rangle$ , for example , is constant and equal to $n$ for all eigenstates of $h$ of the form ( 2 ) ( though there are degeneracies and superpositions of such eigenstates may yet be squeezed ) . edit : ok , i think i know what is confusing you . in particular , from your question edit : starting with an initial ( q- ) distribution in phase space , the distribution evolves with precession frequency proportional to $j_z$ . but from there i do not see how the variance along $\hat z$ would change . the second paper starts off the atoms in a spin-coherent-state cloud along the $+x$ pole on the bloch sphere , and then lets them evolve according to the hamiltonian ( 1 ) . this means that points closer to the $+z$ pole have more positive energy , accumulate more phase on their up components than their down ones , and therefore rotate towards the right . similarly , points closer to the $-z$ pole have more negative energy , accumulate more phase on their down components , and rotate towards the left . the net effect of this is to shear the cloud on the bloch sphere , whilst preserving its height . in particular , the $z$ marginal , i.e. the distribution of ups and downs in a $z$ measurement , is not affected , as it should . as a consequence , the variance in $z$ is not ( yet ) affected . you can see , of course , that the cloud is now longer ( in that its variance in $y$ is now much greater ) , and also thinner , in a slightly off-diagonal direction . thus , to obtain squeezing in the $z$ direction , you need to rotate slightly about the $x$ direction . by how much , of course , is a question of exactly what the circumstances are , and it is a function of the product of $a$ and the interaction time ; if you want the details , you should first give kitagawa and ueda 's paper a thorough read , i think . this is quite a popular protocol for obtaining spin-squeezed states , i think : prepare a spin coherent state , shear it using interactions , and finally rotate it to the measurement direction . the third step is crucial , because the shear does not affect the width along the interaction direction , which is what i was nagging you about in the first part of this answer . once you rotate it , though , you can get reduced ( or vastly increased ) variances in any component .
notation $w^{-} , w^{+}$ may confuse in a sense that it may seem that here are two different particles which are not connected by charge conjugation . but of course , $w^{+}$ is only $ ( w^{-} ) ^{\dagger}$ , so it is an antiparticle to $w^{-}$ . so term $ ( w^{-} \cdot w^{+} ) $ is simple $|w|^{2}$ ( which is standard for the mass-term ) , and , of course , both of particle and antiparticle have the equal masses . also before making substitution $$ \tag 1 w^{\pm}_{\mu} = \frac{1}{\sqrt{2}} ( w_{\mu}^{1} \mp iw_{\mu}^{2} ) $$ you can see that both of fields $w^{1} , w^{2}$ have equal masses . so of course that their linear combinations $ ( 1 ) $ also have equal masses .
a personal point of view is that you may consider that lorentz transformations apply primarily on momenta , and not primarily on ( infinitesimal or not ) space-time coordinates . this is , of course , a " strong " postulate . if you assume ( some additional postulates are needed there ) that transformations are linear , and that there is a rotation invariance , you are going to study " boost " transformations : $\begin{pmatrix} p'_z\\e'\end{pmatrix} = a ( v ) \begin{pmatrix} p_z\\e\end{pmatrix}$ . you may show that , because $a ( v ) a ( -v ) =1$ , $det a ( v ) =1$ . supposing a group structure , you finally are looking at the one-dimensional subgroups of $sl ( 2 , \mathbb r ) $ , which are : $$\begin{pmatrix} \lambda and \\ and \lambda^{-1}\end{pmatrix}\quad \begin{pmatrix} 1 and v\\ and 1\end{pmatrix}\quad \begin{pmatrix} 1 and \\v and 1\end{pmatrix}\quad \begin{pmatrix} \cos \theta and -\sin \theta\\\sin \theta and \cos \theta\end{pmatrix}\quad \begin{pmatrix} \cosh \theta and \sinh \theta\\\sinh \theta and \cosh \theta\end{pmatrix}$$ if you add additional postulates that , in a boost transformation , energy and momentum must change , that there exist a transformation which puts the momentum to zero , and that , if the energy is positive for an observer , energy will be positive for all observers , the first $4$ one-dimensional subgroups of $sl ( 2 , \mathbb r ) $ are excluded , and the last dimensional subgoup corresponds to a lorentz transformation .
in minkowski spacetime the one way light travel time to a galaxy at proper distance $\chi$ is just : $$ t = \frac{\chi}{c} $$ so : $$ \chi = ct $$ as you say . however in an frw universe the travel time is given by a different equation so the proper distance is not simply $ct$ . let 's assume all motion is in the $x$ direction , so the metric simplifies to : $$ c^2ds^2 = -c^2dt^2 + a^2 ( t ) dx^2 \tag{1} $$ we will take our position to be $ ( 0 , 0 ) $ and the galaxy to be at $ ( 0 , \chi ) $ , and we will adopt the usual convention that $a = 1$ at the current time . to get the proper distance we integrate $ds$ , and since $dt = 0$ and $a = 1$ the proper distance is just : $$ \delta s = \int_0^\chi dx = \chi $$ now let 's calculate the time it takes the light beam to get from the galaxy back to us ( i.e. . one half of the journey ) . light travels on a null geodesic so $ds = 0$ and putting this into the metric ( 1 ) and rearranging we get : $$ \frac{dx}{dt} = \frac{c}{a ( t ) } $$ if the universe is static $a ( t ) = 1$ for all $t$ , and we get $x = ct$ so you would be correct that the proper distance is equal to half the total travel time times $c$ . but then with $a = 1$ we just have minkowski spacetime so that is hardly surprising . to calculate the trajectory of the light we need to assume a form for $a ( t ) $ so let 's make the approximation : $$ a ( t ) = 1 + ht $$ where $h$ is the current value of the hubble constant . then we get : $$ \frac{dx}{dt} = \frac{c}{1 + ht} $$ and this integrates to give us : $$ \chi = \frac{c}{h} \log ( h\tau + 1 ) $$ or rearranging this to get the travel time : $$ \tau = \frac{exp ( \frac{\chi h}{c} ) -1}{h} \tag{2} $$ so the proper distance $\chi$ is not simply the travel time times $c$ . just to reassure ourselves that we get the correct result in the limit of $h \rightarrow 0$ , i.e. minkowski spacetime , note that for small $h$: $$ exp ( \frac{\chi h}{c} ) \approx 1 + \frac{\chi h}{c} $$ put this back into equation ( 2 ) and we get : $$ \tau \approx \frac{\chi}{c} $$ which is where we came in .
this is from the physics faq article that i wrote 15 years ago : if shorter wavelengths are scattered most strongly , then there is a puzzle as to why the sky does not appear violet , the colour with the shortest visible wavelength . the spectrum of light emission from the sun is not constant at all wavelengths , and additionally is absorbed by the high atmosphere , so there is less violet in the light . our eyes are also less sensitive to violet . that is part of the answer ; yet a rainbow shows that there remains a significant amount of visible light coloured indigo and violet beyond the blue . the rest of the answer to this puzzle lies in the way our vision works . we have three types of colour receptors , or cones , in our retina . they are called red , blue and green because they respond most strongly to light at those wavelengths . as they are stimulated in different proportions , our visual system constructs the colours we see . when we look up at the sky , the red cones respond to the small amount of scattered red light , but also less strongly to orange and yellow wavelengths . the green cones respond to yellow and the more strongly scattered green and green-blue wavelengths . the blue cones are stimulated by colours near blue wavelengths , which are very strongly scattered . if there were no indigo and violet in the spectrum , the sky would appear blue with a slight green tinge . however , the most strongly scattered indigo and violet wavelengths stimulate the red cones slightly as well as the blue , which is why these colours appear blue with an added red tinge . the net effect is that the red and green cones are stimulated about equally by the light from the sky , while the blue is stimulated more strongly . this combination accounts for the pale sky blue colour . it may not be a coincidence that our vision is adjusted to see the sky as a pure hue . we have evolved to fit in with our environment ; and the ability to separate natural colours most clearly is probably a survival advantage .
the short answer , as karsus ren says , is that it is electromagnetic induction that generates the electricity : the relative movement of a conductor and a magnetic field . as vladimir kalitvianski points out , strictly speaking , windmills do not generate electricity - wind turbines generate electricity , and windmills mill grain . still , it is become reasonably common to refer to wind turbines as windmills . as crowley says , a wind turbine takes the wind 's horizontal motion , turns it into rotary motion , which in some ( but not all ) turbines then goes through a gearbox . the motion then ( depending on the nacelle design ) either : a ) rotates either a conducting coil ( typically copper ) through a magnetic field ; or b ) rotates magnets around a coil ; either way , this generates electricity through electromagnetic induction . the magnets may be permanent magnets ( typically a neodymium alloy ) or electromagnets . the theoretical limit on how much of the wind 's energy can be captured by the blades is called the betz limit , and is an efficiency of ~59.3% ( 16/27 ) . the danish wind-turbine manufacturer vestas has some very accessible stuff on their e-learning site at http://www.vestaselearning.com/ it might seem banal at times , but do not be fooled - there is a lot of decent material in there .
coriolis force is not an actual force , but rather an effect observed in rotating frame of reference . the light path is not actually bent , so it does not matter that the photon has no mass , the earth 's rotation will have an affect on the photon 's apparent path . this does not contradict your calculation of $f_{coriolis}=0$ , because you have to put this force in $f=ma$ , where $m$ is , again , zero . however , you can take the limit of both sides for $m\rightarrow 0$ and calculate $a$ .
yes . this is a perfect case of a so called perpetuum mobile ( see here ) . it would respresent a perfect ( ideal ) non-dissipative system where entropy production $d_is/dt=0$ , in accordance with the 2nd law of thermodynamics . indeed the first law of thermodynamics ( energy conservation ) does not say much about this , except that no term for energy loss included . however , the system must be really non-dissipative , that means no type of friction or dissipative loss of energy in any way , such as friction in the elements of the pendulum etc .
as it turns out , this is not so much a physics question as it is a psychological one . if you use a ruler or some such held at a fixed distance from your eyes , you will find that , as demonstrated in the repetitive shot image you linked , the moon has approximately the same apparent size across its entire path across the sky . the optical illusion arises from the lack of reference points in the sky . basically , when near the horizon , there are terrestrial objects ( trees , hills/mountains , houses , etc . ) whose size we recognize as large . the moon , looking larger than these objects , appears even bigger by comparison . when high in the sky , however , the only thing to compare the moon 's apparent size to is the sky itself , which is much larger than the moon . the result is that the moon appears smaller .
you look at the distance between two infinitesimally different points . let the two coordinate systems be x and y , where x is four numbers and y is four numbers . consider an infinitesimal displacement from y to y+dy . you know this distance in the x coordinates , so you find the two endpoints of the displacement $$x ( y ) $$ $$x^i ( y + dy ) = x^i ( x&#39 ; ) + {\partial x^i \over \partial y^j} dy^j $$ this is using the einstein summation convention--- repeated upper/lower indices are summed automatically , and an upper index in the denominator of a differential expression becomes a lower index , and vice-versa . the distance between these two infinitesimally separated points is : $$ g_{ij} ( x ) {\partial x^i \over \partial y^k} {\partial x^j \over \partial y^l} dy^j dy^l $$ and from this , you read off the metric tensor coefficients--- since this is the quadratic expression for the distance between y and y+dy . $$ g&#39 ; _{kl} ( y ) = g_{ij} ( x ( y ) ) {\partial x^i \over \partial y^k} {\partial x^j \over \partial y^l}$$ this is a special case of the tensor transformation law--- every lower index transforms by getting contracted with a jacobian inverse , and every upper index by getting contracted with a jacobian .
about negative energies : they set no problem : on this context , only energy differences have significance . negative energy appears because when you have made the integration , you have set one point where you set your energy to 0 . in this case , you have chosen that $pe_1 = 0$ for $r = \infty$ . if you have set $pe_1 = 1000$ at $r = \infty$ , the energy was positive for some r . however , the minus sign is important , as it is telling you that the test particle is losing potential energy when moving to $r = 0$ , this is true because it is accelerating , causing an increase in $ke$: let 's calculate the $\delta pe_1$ for a particle moving in direction of $r = 0$: $r_i = 10$ and $r_f = 1$: $\delta pe_1 = pe_f - pe_i = gm ( -1 - ( -0.1 ) ) = -gm\times0.9 &lt ; 0$ as expected : we lose $pe$ and win $ke$ . second bullet : yes , you are right . however , it is only true if they are point particles : has they normally have a definite radius , they collide when $r = r_1 + r_2$ , causing an elastic or inelastic collision . third bullet : you are right with $pe_2 = mgh$ , however , again , you are choosing a given referential : you are assuming $pe_2 = 0$ for $y = 0$ , which , on the previous notation , means that you were setting $pe_1 = 0$ for $ r = r_{earth}$ . the most important difference now is that you are saying that an increase in h is moving farther in r ( if you are higher , you are farther from the earth center ) . by making the analogy to the previous problem , imagine you want to obtain the $\delta pe_2$ . in this case , you begin at $h_i = 10$ and you want move to $h_f = 1$ ( moving in direction to earth center , like $\delta pe_1$: $\delta pe_2 = pe_{f} - pe_{i} = 1mg - 10mg = -9mg &lt ; 0$ . as expected , because we are falling , we are losing $pe$ and winning $ke$ , the same result has $pe_1$ fourth bullet : they both represent the same thing . the difference is that $gh$ is the first term in the taylor series of the expansion of $pe_1$ near $r = r_{earth}$ . as exercise , try to expand $pe_1 ( r ) $ in a taylor series , and show that the linear term is : $pe_1 = a + \frac{gm ( r-r_{earth} ) }{r_{earth}^2}$ . them numerically calculate $gm/r_{earth}^2$ ( remember that $m=m_{earth}$ ) . if you have not made this already , i guess you will be surprised . so , from what i understood , your logic is totally correct , apart from two key points : energy is defined apart of a constant value . in the $pe_1$ , increase r means decrease $1/r$ , which means increase $pe_2 = -gm/r$ . in $pe_2$ , increase h means increase $pe_2=mgh$ .
there is such a diagram--- it is the flux of energy in the linearized gravitational field ( if you use full gr , you get complications with defining the energy ) . unlike the electromagnetic case , where the electric field carry the bulk of the energy and the momentum of the charge carriers is negligible , in the gravitational case , it is the opposite . you can also imagine electromagnetic circuits in which you accelerate very massive spheres which are very lightly charged , and use these as current carriers , and in this case , the momentum of the current carriers will not be negligible . edit : to clarify , there are gravitational fields created by moving water which surround the pipe , like the electric and magnetic fields surround the current-carrying wire . there is an energy flow in these gravitational fields , which carries energy , just like the poynting flux does . these effects are negligible for ordinary materials at ordinary density . nearly all the energy flux ( all but the tiny negligible fraction in the gravitational field ) is carried by the water in the pipe , but the momentum in the water is not analogous to the poynting vector , it is analogous to the electron momentum , which also carries a small amount of energy in a current carrying wire .
the mass is generally a relevant parameter in the renormalization group sense , so that tuning the mass to zero is going to a special point . when you see a parameter tuned to a special point , you have to ask why is it so ? the planck scale is very closely analogous to the atomic scale of space and time . for atomic systems , you can ask nearly the same question about the correlation length . if i give you a generic material and you make a density perturbation of small size , you remove a few atoms in a certain microscopic region , you can ask , how far before the density perturbation dies away and the material looks unperturbed ? for a generic solid , the answer is that the perturbations decay exponentially within a few atomic radii . this is generically true , it holds because the atoms set the distance scale in the solid , so the decay rate is just one atomic radius by dimensional analysis , times a coefficient which is generically order 1 , unless there is a reason for it to be zero . but if you tune your material to a critical point , say you make high pressure water at just the right temperature , then the correlations decay as a power law , the mass parameter of the effective theory is tuned to zero , and the fluid has density perturbations on all scales , and it looks milky white because there are wavelength-of-visible-light size perturbations . this is not an idle analogy : the critical limit of a material is the massless limit of the statistical theory describing its fluctuations , and statistical theories are related to quantum field theories by an analytic continuation in time in the path integral . so whatever sets the atomic scale of space-time , this is the quantity that you expect to set the correlation length , the inverse mass , of the particles . but our universe is filled with particles of inverse mass much larger than the graininess scale , much large than the planck length . when you find a solid tuned to a critical point , you have to ask why it is tuned to be so . it would be strange to find a material whose density is critical without fine-tuning . typical scalars to see more clearly how this works , consider the ising model as a model of a typical scalar field . the average spin in a region is the value of the field , and if you look at the correlations between these average spins , they are described by the following statistical probability $$ p ( h ( x ) ) = e^{-\int h^2} $$ this probability distribution is a sum in the exponent ( an integral is a sum , really ) , so it is a product of independent factors at each point x , and this means that h ( x ) is independent at each point . a field like this , whose value at any point is completely independent from the values at any other point is called " ultralocal " . ultralocal means that the field correlations decay instantly . in our universe , space and time are not really well defined at the planck length , so we expect that the analog of the lattice scale will be set by the planck length . now if you tune the ising model to the critical point , the higher derivative terms in the probability distribution suddenly become visible : $$ p ( h ) = e^{-\int z|\nabla h|^2 + t h^2 + \lambda \lambda h^4 }$$ in the limit that t is tuned to the critical value ( which is some large negative number , it is not at zero ) , the effective mass of the field theory becomes tiny , the correlation scale goes to infinity , and you have a massless limit . although the massless limit is defined by a free theory in which the parameter correpsonding to t is zero , the presence of the self-interaction $\lambda$ shifts the critical value away from t=0 to some other value . this makes it doubly mysterious , because to get the mass to be zero , the more fundamental t parameter has to be tuned to some absurdly small distance away from a random looking number . typical uncharged fermions for fermionic field theories , there is a natural mass scale . if you have one fermionic representation of the lorentz group , the lagrangian is : $$ \int \bar{\psi}^\dot\alpha \sigma^\mu_{\dot\alpha\beta} \partial_\mu \psi^{\beta} + m \epsilon_{\alpha\beta}\psi^\alpha\psi^\beta + m \epsilon_{\dot{\alpha}\dot{\beta}}\bar\psi^\dot{\alpha}\bar\psi^\dot{\beta} $$ in two-component notation . this is massive with mass m . in order to have a massless fermion , you need to tune the parameter m . i wrote this down ( schematically--- you should choose a good two-index convention and check it ) because people often wrongly claim that weyl fermions can not get a mass . this mass is called a " majorana mass " , because you can rewrite two component complex spinors in 4d as real 4 component spinors , and then the mass term looks like a dirac equation mass . naturally massless scalars in a solid , you are surprised when the statistical fluctuations of the density are massless . but you are not surprised that the sound-modes ( the transverse movements of the atoms ) have power law correlations . the sound waves are naturally massless , because the solid breaks the translation symmetry . the naturally massless scalar fields are the goldstone bosons . these are the analog of sound waves . these are described in the wikipedia article on goldstone bosons . naturally massless fermions although fermions can have masses , the mass term mixes the particle with the antiparticle . if you give the field $\psi$ a phase , $\psi\psi$ is not invariant , only $\bar\psi\psi$ is . this is why the majorana mass term is often ignored . in order to give a charged fermion a mass , you must have a partner fermion of opposite helicity and the same charge to couple it to . the standard model is entirely built from fermions which do not have a partner with the same charges , so these fermions are naturally massless . natural small-mass scales from gauge fields the last ingredient you are allowed to naturally use is gauge fields , and these can have couplings which are small , but not small masses . but a confining gauge field in 4d has a confinement length which is exponentially large in the inverse coupling . this means that you can make extremely small masses by starting even with not-so-small couplings at the planck scale . the running of the qcd coupling in our universe sets the mass scale for the proton , and for atoms . it is independent of the higgs mechanism , which is believed to set the mass for the quarks and leptons .
we have to be careful in stating exactly what we are going to allow ourselves to assume here . we need some sort of principle of relativity -- that the laws are the same for both observers . but we do not want to assume anything else a priori , right ? for instance , we do not want to assume at first that rulers have the same length for both observers -- we need to prove that . let 's work in one dimension for simplicity . suppose that observer b is moving at constant velocity v relative to observer a . suppose some object is moving along with some speed $u_b$ as measured by b . we want to show that the speed as measured by observer a is $u_a=u_b+v$ . consider the position of the object at two different times , separated by a small amount $dt$ . since time is absolute ( all observers use the same $dt$ ) , what we want to show is equivalent to $$ dx_a = dx_b+v\ , dt $$ ( multiplying the original equation through by $dt$ ) . here $dx_a$ means $x_a ( t+dt ) -x_a ( t ) $ , that is , the change in the position of the object at the two times , as measured by a , and similarly for b . here 's a useful fact : if both observers measure the distance between two points at an instant of time $t$ , they must get the same answer . the reason is symmetry . if the two disagreed , then one would have to get a bigger answer than the other . but for a measurement of this sort , there is nothing to break the symmetry between a and b -- that is , we can just change the sign of $v$ , and consider b to be stationary and a to be moving , and that should not affect the answer . i think that is enough to get us there . suppose that observer b sets of a firecracker at his location at time $t$ , and another at time $t+dt$ . the two observers must agree on the distance from b to the object at the time the first firecracker went off , and they must agree on the distance from b to the observer at the time the second firecracker went off . the difference between these these two numbers is $dx_b$ . but the difference between these two numbers is also $dx_a-v\ , dt$ , since observer a knows that observer b traveled a distance $v\ , dt$ during that time interval . the conclusion follows .
it would have been easier if you had phrased your question better , with line breaks and clearer language . i am answering my interpretation of your question here , let me know if you wanted something else ( use the comment button below ) . well , first thing let 's explain this concept of " time travel to future " . you are already doing it . now . just by surviving , you are moving forward in time and thus into the future . the main issue is , can we travel through time faster ? now , time is relative . what you measure as a time interval between two events depends on the position and velocity of the events . if you ( or the event-generating object ) move with some velocity , you will disagree on time intervals with someone who is at rest with respect to the event generating object . this can be thought of as coming from your example of going-close-to-speed-of-light-in-a-fast-train example . infact , you will disagree on many things , including lengths and simultaneity , bet that is uneccessary here . now , if you go at any speed with respect to the earth , assuming you start from rest and come back to rest , you will age less , i.e. , while people outside notice that one year has passed , you might feel that it is only a month . this is probably what you mean by " travelling to the future " . this is not a hypothetical situation , and you do not need to go near the speed of light . it will happen even if you walk very slowly , but then the time difference will be negligible , so we do not notice this . it does come into play when considering gps satellites , though ( actually their time difference comes from gravity and not their speeds , but the underlying concept is the same ) . they would not work with accuracy if this dilation of time/contraction of space was not taken into account . similarly , it is observed daily in particle accelerators . particles have a fixed decay time , but particles accelerated close to $c$ live a bit longer . now , your question . " does it make sense ? " . that is for you to decide . at first , it may feel absurd that time behaves this way , but what is the big deal with time anyways ? think for a moment , why should not you be able to move faster through time ? after some thought ( and maybe studying of special relativity ) , it does not feel absurd anymore ( to me , that is ) . you may want to look at the twin paradox for more info : short and simple version ; more detailed with explanations
relativistic mass is not a useful concept . it is the same thing as total energy , just in different units . relativistic momentum on the other hand is useful , because there is no such duplicity . an object composed of fast moving constituents does have higher rest mass , because there is more energy in this object . but you do not need relativistic mass to calculate the rest mass of such object . total energy is enough .
actually in electrostatics energy density of e-field is not a physical observable . as you say , only when charges move will there be any work done . while the two ways of calculating total energy end the same , you cannot distinguish whether energy is stored on the charges or in the field . even e-field itself is more of an abstract mathematical entity , without which everything can be calculated in terms of coulomb law . the physical reality of e and b fields ( and the energy density associated ) becomes apparent only in non-static cases . for example , in electromagnetic radiation , fields can propagate in free space without associating with charges and currents , and the radiation may do work on non-charges ( for example , light pressure ) . because from maxwell equations we can derive a general formula of energy density $$\rho = \frac{\epsilon_0}{2} |\vec e|^2 + \frac{1}{2\mu_0} |\vec b|^2$$ which coincides with the electrostatic case , we deduce that even in electrostatics energy is indeed stored in the fields .
basically from the frame of observation as your car : the fly was inside your car , so its speed with respect to the car is zero . its just as much inside the car as you are . both are travelling at 120 with respect to any observer on the road . ut with respect to anyone inside car you both are just sitting inside the car . so the speed of fly with respect to you is $v=0\ , \frac{m}{s}$ , with respect to some observer on the road is $120\ , $km/h . its no more than a tissue paper you might keep near the steering wheel , in front of you .
you can not integrate the right hand side because $f=f ( x_i ) $ and you have got a differential on $t$ . as for a ) , if you rearrange terms , you can verify that $$m\frac{d\dot{x_i}}{f ( x_i ) }=g ( t ) \ , dt$$ so that now you can not integrate the left hand side because $f$ depends un $x_i$ and you have got a differential on $\dot{x}_i$ .
the equation you are mentioning is the gravitation force derived by newton . this force does not apply to particles such as photons for two reasons : photons are too small , and you can not use newtonian physics to describe their properties . photons travel too fast ( their velocity is the speed of light ) and at such a velocity newtonian mechanics cannot be applied . newton 's gravitation law is really useful to understand the motion of planets around the sun for example , or the motion of a pendulum . but as it comes to light and space one has to look at einstein 's theory of relativity in order to fully understand phenomena . einstein 's general relativity theory is a way to explain gravitation ( and newton 's gravitation law is another ) . the main idea is that the space-time is curved by the presence of mass . what we do know ( and that is always true ) is that photons travel in a straight line in a vacuum . a big mass , such as a black hole , may curve space-time so much that a straight line in space-time is not straight anymore . when we look at photons in space , they seem to bend in a curve through space . to summarize : light can form a curve if it travels near a big mass . you are right , photons do not have mass . you are also right , photons does not follow newton 's gravitation law . photons can be pulled by gravity not because of their mass ( they have none ) but because gravity bends space-time .
from a fundamental ( i.e. . , statistical mechanics ) point of view , the physically relevant parameter is coldness = inverse temperature $\beta=1/k_bt$ . this changes continuously . if it passes from a positive value through zero to a negative value , the temperature changes from very large positive to infinite ( with indefinite sign ) to very large negative . therefore systems with negative temperature have a smaller coldness and hence are hotter than systems with positive temperature . some references : d . montgomery and g . joyce . statistical mechanics of negative temperature states . phys . fluids , 17:11391145 , 1974 . http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19730013937_1973013937.pdf e.m. purcell and r.v. pound . a nuclear spin system at negative temperature . phys . rev . , 81:279280 , 1951 . http://prola.aps.org/abstract/pr/v81/i2/p279_1 section 73 of landau and e.m. lifshits . statistical physics : part 1 , example 9.2.5 in my online book classical and quantum mechanics via lie algebras .
we have not ironed out all the details about how planets form , but they almost certainly form from a disk of material around a young star . because the disk lies in a single plane , the planets are broadly in that plane too . but i am just deferring the question . why should a disk form around a young star ? while the star is forming , there is a lot of gas and dust falling onto it . this material has angular momentum , so it swirls around the central object ( i.e. . the star ) and the flow collides with itself . the collisions cancel out the angular momentum in what becomes the vertical direction and smear the material out in the horizontal direction , leading to a disk . eventually , this disk fragments and forms planets . like i said , the details are not well understood , but we are pretty sure about the disk part , and that is why the planets are co-planar .
this sort of structured surface for friction reduction is an area of current research in phyiscs/engineering so its good to hear someone is actually using it and they work . generally these surface are designed with indentations rather than protusions , more like a golf ball surface , precisely to reduce wear . similarly changing the size/shape of the features will have a strong effect . a more conventional approach would be to use some sort of lubricant between the surfaces such as oil or water to reduce friction , but i guess there may be reasons why you are not doing that anyway . if you want something more exotic , anything that reduces the apparent load should help . think repelling magnets or firing air through holes in one surface . unfortunately i expect these more " cool " ideas are unlikely to very useful in practice .
the half in the non-relativistic kinetic energy can be traced back to the work-energy theorem$^1$ . of course , if one is only interested in solving an elastic collision problem for an isolated system of point particles using momentum and kinetic energy conservation , no harm is done by multiplying the energy conservation equation with a factor 2 on both sides of the equation . -- $^1$ here we assume that the standard formulas for work $w=\int {\bf f}\cdot d{\bf r}$ , newton 's 2nd law ${\bf f}=m{\bf a}$ , acceleration ${\bf a}=\dot{\bf v}$ , velocity ${\bf v}=\dot{\bf r}$ , etc , hold without unconventional normalization factors .
unfortunately gravitational waves have not been detected yet . there is a number of earth-bound detectors planned and already in operation ( e . g . ligo , geo 600 , virgo , nanograv and others ) . as for space-borne detectors , esa works on next gravitational-wave observatory after nasa pulled out of lisa project in april 2011 due to funding problems . joint nasa/esa mission , lisa pathfinder will launch in june 2013 testing technologies to be used by ngo . keep an eye on the pages and blogs of these projects if you had like to stay up to date on their progress . also , if gravitational waves are detected , the discovery will no doubt be announced here and even here .
solar system started out as a nebula of gas that was contracting under its own gravity . it is highly unlikely that all angular momenta of its particles around its center of mass would have summed up to exact zero , so it had some non-zero total angular momentum . due to the conservation of total angular momentum , as the nebula contracted , the speed of its particles , now closer and closer to nebula 's center of gravity become larger and larger . in collisions which gave birth to larger dust particles , asteroids and eventually planets , total angular momentum is preserved as well . this way you end up with a planetary system in which planets and asteroids orbit around the central star . as for venus 's retrograde rotation , it is currently thought it may be an outcome of perturbations and tidal forces exerted on it by the sun and earth .
the cosmic microwave background provides a convenient reference frame for measuring motion ( called the co-moving frame ) . if you are moving relative to the cmb then the doppler shift means the cmb looks slightly hotter in the direction you are moving ( the dipole anisotropy ) , and slightly cooler in the other direction . this motion is called the peculiar motion . if we measure the cmb from earth we find that in fact we are moving relative to the cmb , but at only 368 km/s ( you might think 368 km/s is pretty fast , but it is peanuts compared to galactic red shifts ) so we conclude the earth is roughly stationary wrt the cmb . general relativity , or more precisely the solution called the flrw metric , predicts that every star/galaxy/whatever finds itself to be roughly stationary wrt the cmb , so the fact that the earth appears to be stationary is no surprise . but if the red shift is caused by galactic motions then those galaxies would not be stationary wrt the cmb . you had have to conclude the earth is stationary and all the galaxies are moving away from it i.e. have non-zero peculiar motions , and you had have to conclude that this peculiar motion increases with distance from the earth . this would make the earth a very special place , which seems improbably because the sun seems to be a fairly standard star , in a fairly standard galaxy , in a fairly standard cluster , and so on . it is much more likely that the red shift is due to the expansion of the universe and not peculiar motions of galaxies . so given that it is extremely unlikely that the earth just happens , by chance , to be at the centre of the universe , and that we have a well tested theory ( general relativity ) that predicts the expansion of space , few of us doubt that the expansion of space is the cause of the red shift .
sounds like you are getting at the " coefficient of elasticity , " which is a value in [ 0,1 ] which represents what percent of the pre-collision kinetic energy is found after the collision . in homogeneous materials , the remainder of the energy is typically lost to deformation or heat ( phonons ) as you suggest . you could imagine , for the sake of argument , a steel ball hitting an object which has a spring with a retention device ( a locking lever of some sort ) . in this specialized case , the steel ball does in fact transfer a decent amount of recoverable potential energy into compressing the spring . it takes some other action , i.e. releasing the spring , to return that portion of the potential energy to kinetic . to be clear : suppose the steel ball has n joules of kinetic energy but the spring bottoms out at m
a charged black hole does produce an electric field . in fact , at great distances ( much larger than the horizon ) , the field strength is $q/ ( 4\pi\epsilon_0 r^2 ) $ , just like any other point charge . so measuring the charge is easy . as for how the electric field gets out of the horizon , the best answer is that it does not : it was never in the horizon to begin with ! a charged black hole formed out of charged matter . before the black hole formed , the matter that would eventually form it had its own electric field lines . even after the material collapses to form a black hole , the field lines are still there , a relic of the material that formed the black hole . a long time ago , back when the american journal of physics had a question-and-answer section , someone posed the question of how the electric field gets out of a charged black hole . matt mcirvin and i wrote an answer , which appeared in the journal . it pretty much says the same thing as the above , but a bit more formally and carefully . actually , i just noticed a mistake in what matt and i wrote . we say that the green function has support only on the past light cone . that is actually not true in curved spacetime : the green function has support in the interior of the light cone as well . but fortunately that does not affect the main point , which is that there is no support outside the light cone .
higher maths for beginners is ana amzing little book on all the subjects you mentioned , written by one of the fathers of soviet nuclear bomb , and theoretical phsyicists . on math physics , the best introductory test is elements of applied math physics , it has dufferential equations and complex analysis and other cool topics . unfortunately , it may not have english version . the comprehensive analysis text is fundamentals mathematical analysis . it is a russian textbook , but it is old school , i.e. very readable . another must have book is differential equations and calculus variations . the best reference on pdes is pde by bitsadze , i consult it all the time , it is very thin , and chapters are mostly self-contained . all these books were used by physics students , i can guarantee that .
i will not get into theoretical details -- lubo ad marek did that better than i am able to . let me give an example instead : suppose that we need to calculate this integral : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3}$ here $y_{lm}$ -- are spherical harmonics and we integrate over the sphere $d\omega=\sin\theta d\theta d\phi$ . this kind of integrals appear over and over in , say , spectroscopy problems . let us calculate it for $m_1=m_2=m_3=0$: $\int d\omega ( y_{30} ) ^*y_{20}y_{10} = \frac{\sqrt{105}}{32\sqrt{\pi^3}}\int d\omega \cos\theta\ , ( 1-3\cos^2\theta ) ( 3\cos\theta-5\cos^3\theta ) =$ $ = \frac{\sqrt{105}}{32\sqrt{\pi^3}}\cdot 2\pi \int d\theta\ , \left ( 3\cos^2\theta\sin\theta-14\cos^4\theta\sin\theta+15\cos^6\theta\sin\theta\right ) =\frac{3}{2}\sqrt{\frac{3}{35\pi}}$ hard work , huh ? the problem is that we usually need to evaluate this for all values of $m_i$ . that is 7*5*3 = 105 integrals . so instead of doing all of them we got to exploit their symmetry . and that is exactly where the wigner-eckart theorem is useful : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3} = \langle l=3 , m_1| y_{2m_2} | l=1 , m_3\rangle = c_{m_1m_2m_3}^{3\ , 2\ , 1} ( 3||y_2||1 ) $ $c_{m_1m_2m_3}^{j_1j_2j_3}$ -- are the clebsch-gordan coefficients $ ( 3||y_2||1 ) $ -- is the reduced matrix element which we can derive from our expression for $m_1=m_2=m_3=0$: $\frac{3}{2}\sqrt{\frac{3}{35\pi}} = c_{0\ , 0\ , 0}^{3\ , 2\ , 1} ( 3||y_2||1 ) \quad \rightarrow \quad ( 3||y_2||1 ) =\frac{1}{2}\sqrt{\frac{3}{\pi}}$ so the final answer for our integral is : $\int d\omega ( y_{3m_1} ) ^*y_{2m_2}y_{1m_3}=\sqrt{\frac{3}{4\pi}}c_{m_1m_2m_3}^{3\ , 2\ , 1}$ it is reduced to calculation of the clebsch-gordan coefficient and there are a lot of , tables , programs , reduction and summation formulae to work with them .
dmckee is right ( in the comments ) , the distinction between " theory " and " law " is quite subjective and varies a lot depending on who you ask and the context in which they are used . sometimes they can be nearly synonymous . i would advise you to take any information you get about the difference between these two terms ( including the remainder of this post ) with a grain of salt , and never be afraid to ask someone to clarify what they mean by " theory " or " law " if it matters to your conversation . the one difference that i think most people will agree upon is that in order for something to be called a " law , " there must ( or at least should ) be experimental evidence supporting it . there is no such requirement to be called a " theory . " so it is possible for a theory to be " upgraded " to a law , once there is enough experimental evidence to make it seem true . however , even when that happens , it does not mean people are going to stop calling it a theory ; for example , many people still use the terms " theory of gravity " and " theory of relativity " ( and many others ) even though both those theories have been confirmed by many , many experiments and have unquestionably achieved " law " status . one other difference that i think is common is that " law " often ( but not necessarily always ) refers to a single principle , typically something that can be expressed as a single equation or a set of closely related equations . a " theory " can be more broad . for example , when someone says " law of gravity , " they are probably talking about the equation $$\mathbf{f} = -\frac{g m_1 m_2}{r^2}\hat{\mathbf{r}}$$ or a related equation . but i generally do not hear people using the term " law of relativity , " possibly because special relativity involves several equations and a set of related concepts .
the higgs mechanism is no different from superconductivity , except the condensate responsible for superconductivity is a relativistically invariant scalar field . if you have a bosonic field , its particles can be in a bose-einstein condensate . when this condensate is charged , you call it a superconductor . a photon in a superconductor gets a mass , and this is the higgs mechanism . for a relativistic boson described by a scalar field , you give the field a constant nonzero value to make a condensate . when the field has charge , this makes a superconducting condensate which gives the gauge boson a mass . the whole effect is described in detail on the wikipedia page on the higgs mechanism , starting from a nonrelativistic superconductivity model of bosonic particles , and continuing analogously to relativistic condensates .
the function $u = u ( x_1 , x_2 , \dots x_k , t ) $ would be an example of a potential energy function explicitly dependent on time . in your case , you have the function $u = u ( x_1 , x_2 , \dots x_k ) $ , where it is understood that for each $x_i , \ , i \in \left\{1 , \dots k\right\}$ , we have an implicit dependence $x_i = x_i ( t ) $ . the total derivative of $u$ is $$du = \sum_i \frac{\partial u}{\partial x_i}d x_i + \frac{\partial u}{\partial t}dt$$ and furthermore , $$\frac{du}{dt} = \sum_i \frac{\partial u}{\partial x_i}\frac{d x_i}{dt} + \frac{\partial u}{\partial t} . $$
your observation is linked to the " optical window in biological tissue " . like you already suspected , the absorption of blue light in tissue is higher than the absorption for red light . best read the related wikipedia article , where all relevant effects are nicely illustrated . http://en.wikipedia.org/wiki/optical_window_in_biological_tissue
it is best to always stick to the rigorous formulation of the second law , which comes in two parts . quote from the book " fundamentals of statistical and thermal physics " by f . reif : 1 ) in any process in which a thermally isolated system goes from one macrostate to another , the entropy tends to increase , i.e. , $$\delta s\geq 0$$ 2 ) if the system is not isolated and undergoes a quasi-static infinitesimal process in which it absorbs heat $dq$ , then $$ds = \frac{dq}{t}$$ combining 1 ) and 2 ) in the form of statements such as $ds\geq\frac{dq}{t}$ are weaker than the second law as formulated above , as you need to make additional assumptions that would make this valid . it is best to never use such statements directly and always stick to the modern rigorous formulation of the second law that does not assume that temperature is definable during any non quasi-static change . the paradox you consider is resolved when you consider that the system is not in thermal equilibrium . note that you are in principle free to define the thermodynamic description of a given physical system . so , the physical system is described exactly by specifying its microstate , and the thermodynamic description is a course grained description of it where you keep only a few macroscopic parameters . now , what then can happen is that a slightly less course grained description would have allowed equilibrium thermodynamics to be applicable to the system . this is the case here . so , you have two choice . you if you describe the system as one isolated system , then temperature is not defined and you can only use the first part of the second law which doesn ; t invoke a notion of temperature . however , if you take a sligly less course graiend look at the system , you see that you have two subsystems , the changes in entropy of the system can be described by the second part of the second law , provided the changes are quasi-static .
we have all learned that the earth is getting heat up because of the co2 and co molecules absorbing heat . @benjohn has given you the correct answer . here is my take . the ultimate heat provider of the earth ( except a small percentage of heat from the magma at the center of the earth ) is the sun . it pours down at the surface about 1.2 kilowatts of energy per meter square ( which btw is directly used by solar panels ) . the same energy falls on the surface of the moon whose surface burns up during its daytime and freezes by black body radiation at night . the earth is fortunate to have a gas atmosphere which mitigates the extremes of the possible temperatures that the ground would reach otherwise . an example of mitigation is what happens at the sea floor . most of the energy is picked up by the water and the floor is kept at a steady temperature with small changes day and night in the first meters from the surface , depending on the season , radiating away with the black body radiation , but the body of water has such large heat capacity that variations are small . the gas atmosphere is a more temperamental " blanket " , its heat capacity depends on several gases , called green house gases from the bad impression that agricultural green houses work that way ( they do not , they work by inhibiting heat exchange by convection but that is another story , on which there is no controversy ) . the main green house gas is water , h2o . it is worth contemplating this figure : solar irradiance spectrum above atmosphere and at surface . extreme uv and x-rays are produced ( at left of wavelength range shown ) but comprise very small amounts of the sun 's total output power . we see that h2o has the most absorption spectrum for infrared wavelengths , ( which are the wavelengths of heat ) and then comes co2 . green house gases absorb both incoming and reflected from the surface of the earth infrared , and as most of the reflected wavelengths are in the infrared they act as a slowing down of the black body radiation that would finally leave the earth . as a blanket keeps a person warmer green house gases by playing ball with infrared radiation ( the wavelengths where heat is really transferred ) keep the surface of the earth into a reasonable temperature for life , lucky us . however , how is heat actually kept in those molecules . when photons heat them up , their electron gets excited and goes to a higher energy level ; however , we know that atoms want to remain at a low energy state and they quickly drop down to a lower energy state ( correct me if i am wrong here ) . heat is kept collectively when kept , it is not a one atom thing but emerges statistically by the response of zillions of atoms which keep on exciting and deexciting by collisions and vibrations etc as described in the other answers . if that is happening , then how can heat be actually kept in a carbon-monoxide or carbon-dioxide molecule ? heat is not kept in an individual molecule but in the gas ensemble but in a sense the level of green house gases have a delaying action in the radiation of the earth to the atmpsphere , by reflecting back and forth with the surface . this keeps the temperature close to the surface from fluctuating enormously between daytime and night ( as on the moon ) , it is a buffer similar to the buffer of water for the floor of the ocean . is it because during the day , they get heat up and remain heated because they require more time to get to a lower energy state ? no , it is a collective emergent thermodynamic phenomenon as i said . no need to invoke atoms and quantum mechanics at the level of heat . now going back to the figure , the reason so much stress has been put on anthropogenic co2 is because of computer modeling of the dynamics of the atmosphere . the atmosphere is not a static phenomenon , it has winds , interacts with ocean surfaces , has storms etc . the models assume that co2 increases act as a trigger for the land and atmosphere to release more h2o , in a feedback mechanism , and thus be pivotal in contributing to the small increase in temperature since the middle of last century , but this is another story .
we need to be a bit cautious about making over simplified statements in this area because it is a minefield for the unwary . however it is true that as observed by observers far from the star , the gravitional field does affect the elapsed time for objects within it and it also affects the distances travelled by those objects . the net result is that the speed of light does vary with distance from the star . for more details you might want to look at the questions photons emitted at the event horizon ? and speed of light originating from a star with gravitational pull close to black-hole strength ? for a more detailed discussion of this . the time dilation is a real effect and has been experimentally measured . in 1971 hafele and keating measured the affect of the earth 's gravity on atomic clocks . you ask about this in your comment to user3138766 's answer , and the answer is that the time dilation slows everything . if you hovered near a black hole for a while then returned to earth you would find you were younger than your contemporaries on earth . gravitational lensing is due to the curvature of spacetime . the change in the speed of light in effect changes the refractive index of the space it is travelling through , and this bends the light in an analogous way to the bending of light by a lens .
when you rub fur with ebonite rod , the chemical bond is formed between some parts of the the two surface . rod has its electron more tightly bound than fur . so , fur donates electrons to the rod while electrons from rod will not come out . due to excess of electrons in rod , it becomes negatively charged while the fur with lost electrons , becomes deficient of electrons . thus , fur becomes positively charged .
you had freeze to death faster in the atlantic ocean . space has essentially no thermal conductivity . all the heat you lose will be radiated away . according to the stefan-boltzman law , $w = \sigma t^4$ , you would lose at most 500 watts per square meter of body surface area . by contrast , the convective heat transfer coefficient in water is about 12,500 watts/square meter / degree kelvin temperature difference . so , i think freezing would be the least of your concerns .
higgs mechanism is not the universal mass-responsible detail , but the ultimate . other mechanisms could give you large quantities of mass - and in fact they do - but there is still some part which they are unable to explain . and that is why the higgs mechanism is needed . numbers for you : for the atom of hydrogen : total mass - about 1 gev electromagnetic field - several ev ( billionth parts ) nuclear force field - none masses of electron and quarks - by higgs mechanism - about 20 mev the rest is due to gluons ( and virtual quarks ) tension and motion . for other atoms : total mass - about 1 gev per nucleon ( proton or neutron ) electromagnetic field - up to kev per proton ( not the field inside the nucleus itself ) nuclear force field - up to several mev per nucleon electromagnetic field inside the nucleus - up to the same as nuclear force field 's part masses of electrons and quarks - by higgs mechanism - about 20 mev per nucleon the rest is due to . . . see above .
op wrote ( v4 ) : [ . . . ] strings in string theory also seem to possess a rather complicated and certainly non-trivial suite materials-like properties such as length , rigidity , tension , and i am sure others ( e . g . some analog of angular momentum ? ) . [ . . . ] well , the relativistic string should not be confused with the non-relativistic material string , compare e.g. chapter 6 and 4 in ref . 1 , respectively . in contrast , the relativistic string is e.g. required to be world-sheet reparametrization-invariant , i.e. the world-sheet coordinates are no longer physical/material labels of the string , but merely unphysical gauge degree of freedom . moreover , in principle , all dimensionless continuous constants in string theory may be calculated from any stabilized string vacuum , see e.g. this phys . se answer by lubos motl . op wrote ( v1 ) : what are strings made of ? one answer is that it is only meaningful to answer this question if the answer has physical consequences . popularly speaking , string theory is supposed to be the innermost russian doll of modern physics , and there are no more dolls inside that we can explain it in terms . however , we may be able to find equivalent formulations . for instance , thorn has proposed in ref . 2 that strings are made of point-like objects that he calls string bits . more precisely , he has shown that this string bit formulation is mathematically equivalent to the light-cone formulation of string theory ; first in the bosonic string and later in the superstring . the corresponding formulas are indeed quadratic a la harmonic oscillators ( cf . a comment by anna v ) with the twist that the " newtonian mass " of the string bit oscillators are given by light-cone $p^+$ momentum . thorn was inspired by fishnet feynman diagrams ( think triangularized world-sheets ) , which were discussed in refs . 3 and 4 . however , string bit formulation does not really answer the question what are strings made of ? ; it merely adds a dual description . references : b . zwiebach , a first course in string theory . c.b. thorn , reformulating string theory with the 1/n expansion , in sakharov memorial lectures in physics , ed . l . v . keldysh and v . ya . fainberg , nova science publishers inc . , commack , new york , 1992 ; arxiv:hep-th/9405069 . h.b. nielsen and p . olesen , phys . lett . 32b ( 1970 ) 203 . b . sakita and m.a. virasoro , phys . rev . lett . 24 ( 1970 ) 1146 .
the key thing is that there is no electric field within the perfect wire . so , there is no force acting on the electron , and thus no work done on it ( while it is in the perfect wire ) . this goes back to the definition of a perfect conductor ( which the perfect wire is ) . within a perfect conductor , there is no electric field . instead , the charges ( which have infinite mobility ) rearrange themselves on the surfaces of the conductor in such a way as to perfectly cancel out any internal field . so , the only fields in your circuit would be 1 ) in the battery , and 2 ) in the resistor . i should also add that this is due to the approximation of the wire as ' perfect ' . a real wire has some resistance , or equivalently , its charges do not perfectly reorder so as to perfectly cancel an internal field .
the flaw in your reasoning seems to be that $c$ is not in fact heat capacity . in newton 's law of cooling , the proportionality constant would be related inversely to the heat capacity of the two heated liquids/gasses/materials , and directly to the heat conductance of the object separating the two materials . a material with a higher heat capacity would have a smaller temperature change for a given temperature difference , and a thin piece of metal separating the materials would result in a much larger $\frac{dt}{dt}$ than a thick piece of styrofoam would . when bringing $c_1$ to $\infty$ , you are actually decreasing the heat capacity and increasing the conductance , both of which would cause $t_2$ to drop quickly as you observed .
evaporation is a different process to boiling . the first is a surface effect that happens at any time , while the latter is a bulk transformation that only happens when the conditions are correct . technically the water is not turning into a gas , but random movement of the surface molecules allows some of them enough energy to escape from the surface into the air . the rate at which they leave the surface depends on a number of factors - for instance the temperature of both air and water , the humidity of the air , and the size of the surface exposed . when the bridge is ' steaming': the wood is marginally warmer than the air ( due to the sun shine ) , the air is very humid ( it has just been raining ) and the water is spread out to expose a very large surface area . in fact , since the air is cooler and almost saturated with water , the molecules of water are almost immediately condensing into micro-droplets in the air - which is why you can see them . btw - steam is completely transparent . if you can see it then it is water vapour . consider a kettle boiling - the white plume only occurs a short distance above the spout . below that it is steam , above it has cooled into vapour .
electromagnetic effects do lead to a curvature of spacetime , as gravitation couples to any quantity in the stress-energy tensor , as dictated by the einstein field equations . specifically , the tensor is given by , $$t^{ab}=-\frac{1}{\mu_0}\left ( f^{ac} f_{c}^b +\frac{1}{4}g^{ab}f_{cd}f^{cd}\right ) $$ where $f$ is the field-strength of the electromagnetic $4$-potential $a$ , which the electric and magnetic fields depend on . the corresponding field equations are , $$r^{ab}-\frac{1}{2}g^{ab}r + g^{ab}\lambda = \frac{8\pi g}{\mu_0}\left ( f^{ac} f_{c}^b +\frac{1}{4}g^{ab}f_{cd}f^{cd} \right ) $$ the theory is often referred to as ' einstein-maxwell theory . ' analytic and approximate black hole solutions to the theory are known , c.f. spherically symmetric black hole solutions to einstein-maxwell theory with a gauss-bonnet term by d.l. wiltshire . from their abstract : the only spherically symmetric solutions of the theory are shown to be generalisations of the reissner-nordstrom and robinson-bertotti solutions . the reissner-nordstrom solutions have asymptotically flat and asymptotically anti-de sitter branches , however , the latter are unstable .
a couple of point that might help clarify the situation . when you say " the action of constant force . . . " you really mean the action of a force that is being intentionally harnessed for it is motive power . that is you have given only a partial description of the situation . essential all travel on earth occurs in fluid ( liquid or gas ) environments and much of it occurs in contact with solid surfaces . these facts have physical consequences in the form of dissipative processes ( friction in many guises ) . friction in a force , and it always acts to oppose motion . so what does this imply about the full description of the situation ?
quadrupole magnet quadrupole magnets are mostly used for beam focusing . http://en.wikipedia.org/wiki/quadrupole_magnet
to find out what frequencies a signal contains you fourier transform it . if you have an infinite plane wave , $\psi$ , then when you fourier transform it you get a delta function centred on the wave frequency i.e. it contains a single frequency . however a wave packet of the type shown is the product of an infinite plane wave , $\psi$ , and some envelope function , $e$ , e.g. a gaussian . when you fourier transform this product the frequency distribution you get is the fourier transform of $\psi$ convolved with the fourier transform of $e$ . $\psi$ tranforms to a delta function but , assuming it is a gaussian , $e$ transforms to another gaussian . the convolution of the two gives a gaussian i.e. the packet contains a range of frequencies not just a single frequency . it is because the packet contains a distribution of frequencies that you can not assign it a definite frequency/wavelength . the best you can do is assign an average frequency . re the uncertainty in position : the position is given by the envelope function , $e$ , as described above . in effect $e$ tells you the probability of finding the particle at some position/time . to get a precise position you need to shrink $e$ to a delta function , but if you do that you find it is fourier transform goes to a constant i.e. the packet now has an infinitely uncertain frequency and therefore momentum .
the clebsch-gordan coefficients appear in the representation theory of the [ lie ] group of rotations $so ( 3 ) $ [ and its fundamental cover $su ( 2 ) $ ] . when expressing the tensor product of two irreducible representations of this group [ itself being a reducible representation ] as a direct sum of irreducible representations , the normalized coefficients of the expansion are the clebsch-gordan coefficients . they express the multiplicity of each irreducible representation in the decomposition . the clebsch-gordan coefficients are themselves orthonormal , with orthonormality relation $\sum_{|m_1|\leq j_1 , |m_2| \leq j_2} c ( j_1 , j_2 , m_1 , m_2|j , m ) c ( j_1 , j_2 , m_1 , m_2|j&#39 ; , m&#39 ; ) = \delta_{j , j&#39 ; } \delta_{m , m&#39 ; }$ $\sum_{j=|j_1-j_2|}^{j_1+j_2} \sum_{|m|\leq j} c ( j_1 , j_2 , m_1 , m_2|j , m ) c ( j_1 , j_2 , m_1&#39 ; , m_2&#39 ; |j , m ) = \delta_{m_1 , m_1&#39 ; } \delta_{m_2 , m_2&#39 ; }$ and as exposed above appear when decomposing reducible representations into sums of irreducible representation . in terms of angular momentum states $|j_1 , m_1 \rangle \bigotimes|j_2 , m_2\rangle=\sum_{j , m} c ( j_1 , j_2 , m_1 , m_2|j , m ) |j , m\rangle$ where $c ( j_1 , j_2 , m_1 , m_2|j , m ) =\langle j , m|j_1 , j_2 , m_1 , m_2\rangle$ the clebsch-gordan coefficients appears also , in the expansion of the product of two spherical harmonics in terms of spherical harmonics themselves . the derivation of the formula is a bit cumbersome and the result looks like this $y_{l_1}^{m_1} ( \theta , \varphi ) y_{l_2}^{m_2} ( \theta , \varphi ) =\sum_{l , m} \ \sqrt{\dfrac{ ( 2l_1+1 ) ( 2l_2+1 ) }{4 \pi ( 2l+1 ) }} \\ \times c ( l_1 , l_2 , m_1 , m_2|l , m ) c ( l_1 , l_2,0,0|l , m ) y_{l}^m ( \theta , \varphi ) $ they are also related to other more complicated structures like the wigner 3-j symbols or the racah coefficients . in addition , i may add that there exists a closed formula for them in $3$ dimensions ( derived by racah ) and that this formula is not known for arbitrary dimensions .
i have written an answer to mathoverflow in which explicit formulas for the classical and quantum hamiltonians of a spin system ( generators of $su ( 2 ) ) $ were written explicitely . the classical hamiltonians are given by means of functions on the two sphere and the quantum hamiltonians by means of holomorphic differential operators ( which act on the sections of the quantum line bundle ) . for many spin system with a linear hamiltonian in each spin , one just has a distinct one particle hamiltonian per spin . sorry for referring to my own work , but it is by no means original .
the earth is not a perfect sphere ( or even a perfect oblate spheroid ) so its gravitational field is not axially symmetric . you have probably seen the geoid measured by the goce and grace satellites . as the earth rotates the asymmetries in its gravitational field rotate with it , and any satellite whose orbital period is a ratio of one day can build up a resonance with the daily variations in the earth 's gravity . this is essentially the same physics as the resonances seen in , for example , the moons of jupiter . i had a quick google and found this paper that gives a fairly detailed analysis of the phenomenon . see in particular section 1.4 . the galileo satellites orbit 17 times every 10 days . this is sufficiently far from a simple ratio that resonances do not build up .
formula 1 is the contribution to the magnetic field from the small segment of the coil of length $dl$ shown in the lower portion of figure 1 . that piece of coil has current $di = n i\ , dl/l$ flowing through it where $n$ is the number of turns in the coil , $i$ is the current per turn , and $l$ is the length of the coil . from the hyperphysics site ( or by integrating the biot-savart law ) , the axial field from this loop is : $$b_z = \frac{\mu_0}{4\pi} ( 2\pi r ) di\frac{ r}{ ( r^2+z^2 ) ^{3/2}} = \frac{\mu_0 r^2 di}{2 y^3}$$ where $y= ( r^2+z^2 ) ^{1/2}$ is the distance from the measurement point to the rim of the loop and $r$ is the coil radius . substituting the formula for $di$ gives you the first equation . the second equation is the integral of the above over the length $l$ . the integral is first converted to an integral over angle $\theta$ via $\sin \theta = r/y$ and $\sin\theta\ , dl = y\ , d\theta$: $$b = \int\frac{\mu_0 r^2 di}{2 l y^3} = \frac{\mu_0 n i}{2 l} \int \frac{r^2 dl}{y^3} = \frac{\mu_0 n i}{2 l} \int_{\theta_1}^{\theta_2} \sin \theta \ , d\theta$$
the answer is no . the simplest proof is just the principle of relativity : the laws of physics are the same in all reference frames . so you can look at that 1-kg mass in a reference frame that is moving along with it . in that frame , it is just the same 1-kg mass it always was ; it is not a black hole .
to be consistent with notation , i use the $x'$ for the transformation to the new system and $\tilde{x}$ for the rotation . thus , as you defined $$x'_i=t_{ij}x_j , $$ $$\tilde{x}_i=r_{ij}x_j . $$ we know that $$\tilde{x}'_i=t_{ij}\tilde{x}_j=t_{ij}r_{jk}x_k . \tag{1}$$ you are looking for the transformation matrix $q_{ij}$ , such that $$\tilde{x}'_i=q_{ij}x'_j , $$ or $$\tilde{x}'_i=q_{ij}x'_j=q_{ij}t_{jk}x_k\tag{2}$$ naively , one could now write from ( 1 ) and ( 2 ) $$t r = q t , $$ $$q=t r t^{-1} . $$ however , $t$ is not a square matrix , and does not invertable . in other words , such a matrix $q$ can not be determined uniquely . or , looking at it as $$t_{ij}r_{jk} = q_{pq}t_{qr}$$ you know that these are twelve equations , because both procut matrices are $4\times3$ . $q_{ij}$ is a $4\times4$ matrix , with , thus , $16$ unknowns . in other words , there are infinitely many possibilities . unless , of course , you add constraints .
this might be more of a math question . this is a peculiar thing about three-dimensional space . note that in three dimensions , an area such as a plane is a two dimensional subspace . on a sheet of paper you only need two numbers to unambiguously denote a point . now imagine standing on the sheet of paper , the direction your head points to will always be a way to know how this plane is oriented in space . this is called the " normal " vector to this plane , it is at a right angle to the plane . if you now choos the convention to have the length of this vector ( "the norm" ) equal to the area of this surface , you get a complete description of the two dimensional plane , its orientation in three dimensional space ( the vector part ) and how big this plane is ( the length of this vector ) . mathematically , you can express this by the " cross product " $$\vec c=\vec a\times\vec b$$ whose magnitude is defined as $|c| = |a||b|sin\theta$ which is equal to the area of the parallelogram those to vectors ( which really define a plane ) span . to steal this picture from wikipedia 's article on the cross product : as i said in the beginning this is a very special thing for three dimensions , in higher dimensions , it does not work as neatly for various reasons . if you want to learn more about this topic a keyword would be " exterior algebra " update : as for the physical significance of this concept , prominent examples are vector fields flowing through surfaces . take a circular wire . this circle can be oriented in various ways in 3d . if you have an external magnetic field , you might know that this can induce an electric current , proportional to the rate of change of the amount flowing through the circle ( think of this as how much the arrows perforate the area ) . if the magnetic field vectors are parallel to the circle ( and thus orthogonal to its normal vector ) they do not " perforate " the area at all , so the flow through this area is zero . on the other hand , if the field vectors are orthogonal to the plane ( i.e. . parallel to the normal ) , the maximally " perforate " this area and the flow is maximal . if you change the orientation of between those two states you can get electrical current .
the short answer is that it can , if $m = 1 = m^{-1}$ . in this way of looking at it , all quantities in planck units are pure numbers . the longer answer is that there are two different ways of thinking about natural unit systems . natural unit systems in terms of standard units one of them , and perhaps the easier one to understand , is that you are still working in a " traditional " unit system in which distinct units for all quantities exist , but the units are chosen such that the numerical values of certain constants are equal to 1 . for example , if you want to set $c = 1$ , you are not literally setting $c = 1$ , you are actually setting $c = 1\ , \frac{\text{length unit}}{\text{time unit}}$ . length and time do not actually have the same units in this interpretation ; they are equivalent up to a multiplication by factors of $c$ . in other words , it is understood that to convert from , say , a time unit to a length unit you multiply by $c$ , and so that is left implicit . in order to do this , of course , you have to choose a length unit and time unit which are compatible with this equation . so you could not use meters as your length unit and seconds as your time unit , but you could use light-seconds and seconds , respectively . if you want to set multiple constants to have numerical values of 1 , that constrains your possible choices of units even further . for example , suppose you are setting $c$ and $g$ to have numerical values of 1 . that means your units have to satisfy both the constraints $$\begin{align} c and = \frac{\text{length unit}}{\text{time unit}} = \frac{\ell_g}{t_g} and g and = \frac{ ( \text{length unit} ) ^3}{ ( \text{mass unit} ) ( \text{time unit} ) ^2} = \frac{\ell_g^3}{m_gt_g^2} \end{align}$$ where i have introduced $\ell_g$ , $t_g$ , and $m_g$ to stand for the length , time , and mass units in this system , respectively . you can then invert these equations to solve for $\ell_g$ , $t_g$ , and $m_g$ in terms of $c$ and $g$ - but as you can probably tell , the system of equations is underdetermined . it still gives you the freedom to choose one unit to be part of your unit system , such as $$\text{kilogram} = \text{mass unit} = m_g$$ having made that choice , you can now solve for $m_g$ , $\ell_g$ , and $t_g$ in terms of $c$ , $g$ , and $\text{kilogram}$ ( or whatever other choice you might have made ; each choice gives you a different unit system ) . running through the math for this gets you $$\begin{align} m_g and = 1\text{ kg} and \ell_g and = \frac{g ( 1\text{ kg} ) }{c^2} and t_g and = \frac{\ell_g}{c} = \frac{g ( 1\text{ kg} ) }{c^3} \end{align}$$ now you can plug in values of $g$ and $c$ in , say , si units , and get conversions from si ( or whatever ) to this unit system . note that , as i said , length does not literally have the same units as time or mass , but you can convert between the length unit , time unit , and mass unit by multiplying by factors of $g$ and $c$ , constants which have numerical values of 1 . in a sense , you can consider this multiplication by $g^ic^j$ as analogous to a gauge transformation , i.e. a transformation that has no effect on the numerical value of a quantity , and the units of length , time , and mass are mapped on to each other by this transformation just as gauge-equivalent states are mapped on to each other by a gauge transformation in qft . so it is more proper to say $l \sim t \sim m$ ; the dimensions are not equal , just equivalent under some transformation . if you do the same thing but setting $c = \hbar = 1$ instead , remember what you are really doing is specifying that your units must satisfy the constraints $$\begin{align} c and = \frac{\text{length unit}}{\text{time unit}} = \frac{\ell_q}{t_q} and \hbar and = \frac{ ( \text{length unit} ) ^2 ( \text{mass unit} ) }{ ( \text{time unit} ) } = \frac{\ell_q^2m_q}{t_q} \end{align}$$ ( $q$ is for " quantum " because these are typical qft units ) , and then running through the math , again with $m_q = 1\text{ kg}$ , you get $$\begin{align} m_q and = 1\text{ kg} and \ell_q and = \frac{\hbar}{ ( 1\text{ kg} ) c} and t_q and = \frac{\ell_q}{c} = \frac{\hbar}{ ( 1\text{ kg} ) c^2} \end{align}$$ again , the units are not literally identical , but $\ell_q \sim t_q \sim m_q^{-1}$ under multiplication by factors of $\hbar$ and $c$ . of course , your third constraint does not have to be a choice of one of the fundamental units . you can also choose a third physical constant to have a numerical value of 1 . to obtain planck units , for example , you would specify $$\begin{align} c and = \frac{\text{length unit}}{\text{time unit}} = \frac{\ell_p}{t_p} \\ \hbar and = \frac{ ( \text{length unit} ) ^2 ( \text{mass unit} ) }{ ( \text{time unit} ) } = \frac{\ell_p^2m_p}{t_p} \\ g and = \frac{ ( \text{length unit} ) ^3}{ ( \text{mass unit} ) ( \text{time unit} ) ^2} = \frac{\ell_p^3}{m_pt_p^2} \end{align}$$ you can tell that this is no longer an underdetermined system of equations . solving it gives you $$\begin{align} m_p and = \sqrt{\frac{\hbar c}{g}} and \ell_p and = \sqrt{\frac{\hbar g}{c^3}} and t_p and = \sqrt{\frac{\hbar g}{c^5}} \end{align}$$ here , since you have set three constants to have numerical values of 1 , your three fundamental planck units will be equivalent up to multiplications by factors of those three constants , $g$ , $\hbar$ , and $c$ . in other words , multiplication by any factor of the form $g^i\hbar^jc^k$ is the equivalent to the gauge transformation i mentioned earlier . you can tell that all these units are equivalent under such a transformation , but more than that , all powers of them are equivalent ! in particular , you can convert between $m$ and $m^{-1}$ by multiplying by constants whose numerical value in this unit system is equal to 1 , and thus it is not a problem that $m \sim m^{-1}$ here . unit systems as vector spaces another way of understanding unit systems , which is kind of a logical extension of the previous section , is to think of them as a vector space . elements of this vector space correspond to dimensions of quantities , and the basis vectors can be chosen to correspond to the fundamental dimensions $l$ , $t$ , and $m$ . ( of course you could just as well choose another basis , but this one suits my purposes . ) you might represent $$\begin{align} l and \leftrightarrow ( 1,0,0 ) and t and \leftrightarrow ( 0,1,0 ) and m and \leftrightarrow ( 0,0,1 ) \end{align}$$ addition of vectors corresponds to multiplication of the corresponding dimensions . derived dimensions correspond to other vectors , like $$\begin{align} [ c ] = lt^{-1} and \leftrightarrow ( 1 , -1,0 ) \\ [ g ] = l^3m^{-1}t^{-2} and \leftrightarrow ( 3 , -2 , -1 ) \\ [ \hbar ] = l^2mt^{-1} and \leftrightarrow ( 2 , -1,1 ) \end{align}$$ in this view , setting a constant to have a numerical value of 1 corresponds to projecting the vector space onto a subspace orthogonal to the vector corresponding to that constant . for example , if you want to set $c = 1$ , you project the 3d vector space on to the 2d space orthogonal to $ ( 1 , -1,0 ) $ . any two vectors in the original space which differ by a multiple of $ ( 1 , -1,0 ) $ correspond to the same point in the subspace - just like how , in the previous section , any two dimensions which could be converted into each other by multiplying by factors of $c$ could be considered equivalent . but in this view , you can actually think of the two dimensions as becoming the same , so that e.g. length and time are actually measured in the same unit . since in planck units you set three constants to have a numerical value of one , in the dimensions-as-vector-space picture , you need to perform three projections to get to planck units . performing three projections on a 3d vector space leaves you with a 0d vector space - the entire space has been reduced to just a point . all the units are mapped to that one point , and are the same . so again , $m$ and $m^{-1}$ are identical , and there is no conflict .
this is not as easy as it may sound : in every analogy one has to make a choice between rigor and ' poetic license ' . personally , the one i like better is higgs for waldegrave : where a crowd-analogy is given . but , as they say , your milage may very . if you had like , you can think in terms of a ' caramel pool ' , milky way simply caramel : pool : when we say that a particle ' couples ' to the higgs field , we mean to say that this particle ' sees ' this ' caramel pool ' , and this makes it " harder " for it to move , which we measure as this particle 's " mass " . and , as you can imagine , there are particles that do not couple to the higgs field , meaning to say they do not " see " it : therefore , they move much more easily . ; - ) but , take all this with a grain of salt
you can not jump very quickly . the average vertical jump of nba players is 28 inches . how fast are they going at max ? $$v^2 = 2 a x$$ $$v = \sqrt{2 g ( 0.71m ) }$$ $$ v = \sqrt{13.95m^2/s^2}$$ $$ v = 3.7 m/s$$ that is not very fast . it means that at a maximum you can remove less than 4m/s of your impact speed . since a plane crash may be 100m/s , that does not help much .
there are a number of high level mathematicians who are working on giving a more mathematically precise description of perturbative qft and the renormalization procedure . for example there is a recent paper by borcherds http://arxiv.org/pdf/1008.0129 , work of connes and kreimer on hopf algebras and the work of bloch and kreimer on mixed hodge structures and renormalization http://www.math.uchicago.edu/~bloch/monodromy.pdf just to name a few . to be honest , i am not mathematically sophisticated enough to judge what has been accomplished in these papers , but i think there are some problems in qft which will probably involve some rather high-powered mathematics of the type being developed in these papers . for example , the current attempt to reformulate n=4 sym in terms of grassmannians apparently has some connection to rather deep mathematical objects called motives . results on the degree of transcendentality which show up in perturbative n=4 sym amplitudes also seem beyond what physicists really understand and i believe the presence of transcendental objects ( like $\zeta ( 3 ) $ ) in qft amplitudes provides some of the motivation for the work of bloch and kreimer . i am not an expert on this stuff , so perhaps someone else will chime in with a more complete explanation and additional references . edit : one more reference which is closer to the spirit of the original question is a book in progress by costello on perturbative quantum field theory treated from the wilsonian , effective field theory point of view . notes are available online at http://www.math.northwestern.edu/~costello/renormalization
the equation comes from newton 's second law : $$ f = ma $$ galileo did not know calculus ( because newton and leibniz had not discovered it yet ) so he could not derive the equation mathematically . since we do know calculus we know that acceleration is the variation of velocity with time : $$ a = \frac{dv}{dt} $$ and also the gravitational force $f$ is equal to $mg$ . if we substitute for $a$ and $f$ in newton 's second law we get ( after a slight rearrangement ) : $$ \frac{dv}{dt} = g $$ knowing calculus we can integrate both sides to get : $$ v = gt + c $$ where $c$ is the constant of integration . to find $c$ we note that when $t = 0$ we find $v = c$ , so $c$ is the velocity when we start timing . in the particular case of dropping an object the initial velocity is zero , but for now let 's keep the initial velocity and call it $u$ , which is the usual symbol for it , and our equation becomes : $$ v = u + gt $$ the next step is to note that the velocity $v$ is the variation of distance with time : $$ v = \frac{ds}{dt} = u + gt $$ and we can integrate this to get : $$ s = ut + \tfrac{1}{2}gt^2 + d $$ where once again $d$ is a constant of integration . this time we note that when $t = 0$ the distance $s = 0$ , so the constant $d$ is zero and our final equation is : $$ s = ut + \tfrac{1}{2}gt^2 $$ the equation you give the the special case of an object starting at rest , i.e. $u = 0$ , in which case the equation simplifies to : $$ s = \tfrac{1}{2}gt^2 $$ there are a number of related equations derived in a similar way , and they are generically referred to as the suvat equations .
physics 253: quantum field theory lectures by sidney r . coleman . recorded in 1975-1976 . it is rare that you can find on such advanced topics whole-semester courses that are video recorded , with the exception to coleman 's and minwalla 's lectures . what is usually available are at most 5 video lectures per topic that were given at different summer schools . here is an example : qcd for postgraduates , from cern documet server , by giulia zanderighi , 2010
some definitions might be useful : potential : the potential energy per unit charge , $v = \frac{u}{q}$ . potential depends only on the environment and the location , not on what is placed at that location . voltage : a difference in potential , $\delta v$ , between two points in the same environment . you can think of this as the change in potential energy per unit charge for a test charge moving between the two points . so the first answer i would give you is that potential energy depends on the test charge ( $+q$ in your example ) , but voltage does not , because it is per unit charge . but i think what you really mean to ask is , why is not the potential energy of a capacitor , $\frac{1}{2}cv^2$ , the same as the potential energy of a charge moving across the capacitor , $qv$ ? that is because the potential energy of a capacitor represents the energy that had to be put in to move all the charges that are already in the capacitor . the first charge $q$ to be moved from one plate to the other did not need any energy to do it , because at the beginning , the plates were uncharged , and thus at the same potential . after one charge had been moved , there was a potential difference $v_1 = q/c$ . in order to overcome that potential difference , the next charge needed energy $qv_1 = q^2/c$ . after the second charge had been moved , there was a potential difference $v_2 = 2q/c$ . so the third charge needed energy $qv_2 = 2q^2/c$ . . . . and so on . adding all these up gives $$\frac{q^2}{c} + \frac{2q^2}{c} + \frac{3q^2}{c} + \cdots + \frac{nq^2}{c} = \frac{ ( n^2 + n ) q^2}{2c} \approx \frac{ ( nq ) ^2}{2c}$$ where $nq$ is the total charge on the capacitor . ( $n$ particles , each of charge $q$ . ) of course in practice , we consider infinitesimal elements of charge , and do an integral instead : $$\int_0^{q_\text{total}} \frac{q}{c}\mathrm{d}q = \frac{q_\text{total}^2}{2c}$$
" physics for the inquiring mind : the methods , nature , and philosophy of physical science " by eric m . rogers is a superb book for a teenager . it does not overwhelm the reader with esoteric mathematics and provides a very solid foundation which is often missing in so many books . expensive at about 60 dollars , i found a copy for 15 dollars , so seek young man and ye shall find ! on amazon : physics for the inquiring mind similarities in physics - john northrup shive and robert weber is another great book for showing the interconnectedness in physics and basic ideas ranging from simple harmonic motion , heat , noise etc . actually i think this is one of the great and unknown pedagogical books for physics and it is free here . you really can not go wrong reading that one . if you like a laugh and the kooky then try macschrdinger 's cat - jim o'brien also at amazon , but do not read this one for insight or learning , read this one just to stretch your mind .
resistance can be taken as $$r = \rho \frac{l}{a}$$ where $rho$ is resistivity , $l$ is length of resistor and $a$ is cross sectional area of the resistor . the above is asking you for a ratio of $r_1:r_2$ and you are given all the numbers in the question . shapes within a circuit diagram can typically be ignored , the diagrams are only symbolic to inform you of how the pieces are connected .
the free electron model is surprisingly good at predicting the properties of electrons in metals , and this implies that the electrons really are nearly free . however when you look more closely there is of course an interaction with the lattice . this is modelled using the ( rather predictably named ) nearly free electron model . the conduction electrons are delocalised , so you should not think of them as little balls bouncing off the ion cores . the spatial extent of their wavefunction is typically far greater than the lattice repeat , hence the relatively weak interactions . however interactions with the lattice are responsible for electrical resistance and thermal conductivity , and at very low temperatures for superconductivity . however note that these are not interactions between a single electron and a single ion core , but rather interactions between electron waves and lattice waves ( phonons ) .
because liquids , water in particular have a high surface tension . for this reason blobs of water tend to become spherical . now , given that it starts as an elongated stream ( say because it is pushed out of a bottle ) , the stream breaks up in different pseudo-spherical bubbles ; if an astronaut were to pour water very , very slowly and carefully he could create a single spherical blob . the concept is not dissimilar to drops , but without the gravity .
can someone explain the atomic process , if it even exists , of how this would work to convert energy in to matter , and what form of energy was initially present , and what is required to cause this change ? it is not an atomic process , it is an elementary particle process , atoms are made up of elementary particles in a non trivial way . at the level of elementary particles we are talking of special relativity and mass and energy are interchangeable depending on the process under consideration . in the macro dimensions we live in one may consider light as carrying energy without being massive , but at the elementary particle level the light beam is made up of a huge number of photons , particles with zero mass . a photon photon scatter can create particles , as shown in this link even a gamma gamma collider is discussed to study these higher order interactions in creating particles . now at the very early times of the big bang there are models , essentially extrapolated expecting similar diagrams to hold in the very small dimensions , of elementary particles scattering off elementary particles and generating more of the same , possibly to start with very exotic particles not ( yet ? ) known in our labs . they will be a soup of energy to matter and matter to energy . the big bang theory postulates that as the system expands the universe cools and the energy levels evolve in this soup to become a " photon quark gluon " soup and with the further expansion finally baryons and electrons appear and photons can escape binding in the soup and give us currently the cosmic microwave background snap shot .
if you look closely at the crocodiles ' tails you will see that they wave their tails from side to side to provide propulsion for the jump . compare this to a fish swimming : the side to side motion of the fish 's tail propels it forward , and the crocodiles are using exactly the same sort of side to side motion to propel themselves upwards .
in addition to fresnel equations , and in response to your question regarding the " . . . relation between the amplitude of the transmitted/reflected rays and the original ray": $$t_{\parallel}=\frac{2n_{1}\cos\theta_{i}}{n_{2}\cos\theta_{i}+n_{1}\cos\theta_{t}}a_{\parallel}$$ $$t_{\perp}=\frac{2n_{1}\cos\theta_{i}}{n_{1}\cos\theta_{i}+n_{2}\cos\theta_{t}}a_{\perp}$$ $$r_{\parallel}=\frac{n_{2}\cos\theta_{i}-n_{1}\cos\theta_{t}}{n_{2}\cos\theta_{i}+n_{1}\cos\theta_{t}}a_{\parallel}$$ $$r_{\perp}=\frac{n_{1}\cos\theta_{i}-n_{2}\cos\theta_{t}}{n_{1}\cos\theta_{i}+n_{2}\cos\theta_{t}}a_{\perp}$$ where $a_{\parallel}$ and $a_{\perp}$ is the parallel and perpendicular component of the amplitude of the electric field for the incident wave , respectively . accordingly for the $t$ ( transmitted wave ) and $r$ ( reflected wave ) . i think the notation is straightforward to understand . this set of equations are also called fresnel equations ( there are three or four representations ) .
the brownian motion $x ( t ) $ is non-differentiable , so a particular trajectory $x ( t ) $ can not extremize an action $s$ which would be a functional of $x ( t ) $ and its derivative , $\dot x ( t ) $ , because the derivative is not even well-defined and any expression of the type $\int [ \dot x ( t ) ] ^2 dt$ , the usual kinetic term in the action , diverges . ( see e.g. middle of page 2 of this paper to see the statement that there is no lagrangian , too . the paper does its best to construct something that is " as close as possible " to the normal lagrangian formulation . ) however , when you mention field theory , it is interesting to point out that the typical trajectories $x ( t ) $ that contribute to feynman 's path integral computation of ordinary quantum mechanics do resemble the brownian trajectories very closely . but the amount of zigzag motion is determined by the uncertainty principle and planck 's constant , not by adjustable collisions with the molecules of a liquid etc . there are many other differences in the physical interpretation , too .
( 1 ) the first law is written in form of differentials themselves , so i think there may be no escape from using differential equations . ( 2 ) the way most commonly the first law is written is , $du=dq-dw_{\text{work done by the system}}$ . here dw is work done by the system . however , in subjects other than physics , more important quantity is the work done by the experimenter . ( this is quite common in chemistry ) as the process in thermodynamics are most " quasistatic" ( http://en.wikipedia.org/wiki/quasistatic_process ) , the container/piston is always in equilibrium . so , $\vec{f_{ext}}=-\vec{f_{int}}$ ( they are equal and opposite ) , then we have , $dw_{\text{by the system}}=-dw_{\text{on the system}}$ . and so the first law can be written as : $du=dq+dw_{\text{work done on the system}}$ . here dw is work done on the system . as the two $dw$s have different meanings we will not a different answer . ( 3 ) the internal energy of a gas is state variable/state function ( http://en.wikipedia.org/wiki/functions_of_state ) . $u$ depends only on the final and initial states of the system and not on what process was used to get from initial to the final state . so we can use a constant volume process to get $du$ which then can be used in any process without any modification .
your calculations are correct , provided the cylinder is indeed ohmic . the constant $e$ you are getting is the difference in electric field between both terminals . as for the current flowing from inside to outside , as you said the cross sectional area will be different , and so will the length . the length $l=r_b-r_a$ , but the cross sectional area is not uniform , because at the beginning of the wire ( the interior ) , $a=2\pi r_al$ , and at the " end " of the wire ( the exterior ) , $a=2\pi r_bl$ . so you will have to treat each portion of the wire as its own infinitesimal resistor $dr$ , and the total resistance is the series combination of them : $$dr=\rho\frac{dl}{2\pi ll}$$ $$r=\int_{r_a}^{r_b}\rho\frac{1}{2\pi ll}dl$$
there are very important differences between this two approaches , that can be summarized by noting that the lippmann-schwinger is the ( formal ) solution of a one-body problem ( scattering of a particle by an external potential ) whereas the dyson equation gives the solution of a many-body problem . i focus here on the non-relativistic many-body case ( it is also the case of the scattering problem ) . it is only in the case where the system is in an external potential and non-interacting ( or empty , assuming conservation of the number of particle ) that the two approaches are equivalent ( or more precisely the dyson equation gives back the lippmann-schwinger equation ) .
my understanding of graduate level is an overlap between jd jackson 's classical electrodynamics , landau 's electrodynamics of continuous media and landau 's classical theory of fields . unfortunately , there isnt much video material out there , which is justifiable because there is no great pedagogical need here . if you understand griffith 's level electrodynamics the jackson 's book is an advanced methods to solve sophisticated problems book . which is best learnt by doing problems . although i am not greatly impressed , this is a set of video lectures that treats landau and jackson as textbooks . http://vubeam.pa.msu.edu/lectures/phy962/962d/electrodynamics/ it might be worthwhile to have a look at leonard susskind 's lecture on classical electrodynamics and classical theory of fields in the special relativity module . http://www.cosmolearning.com/video-lectures/electrodynamics/ if you are looking for companion notes , then these lecture slides would help you a lot more specifically with understanding the material presented in jackson , i found it really helpful . http://physics.gmu.edu/~joe/phys685/
as you stated , the degree of green is directly dependent on the thickness of glass you stare at ( beer-lambert law ) . it actually comes from the absorption of the other wavelengths by the glass . due to refraction , even when you look at the glass from a grazing angle in the air , the light rays bend to a higher angle in the glass which makes the light path through the glass shorter ( figure 2 ) . on the contrary , when you stare at the glass from the edge , total internal reflection makes the light rays travel through the whole length of the glass to your eye ( figure 3 ) .
no , if you only know the distance between the objects and the relative orbital velocity of the planet , you cannot determine its mass . in fact , if you only know the distance and velocity at one particular moment , you do not have enough information to determine the orbit . suppose we know the distance $r$ and the relative orbital velocity $\vec{v}= ( v_r , v_t ) $ of a planet at a given moment . here , $v_r=\dot{r}$ is the radial velocity component , and $v_t$ the tangential component . the orbit of the planet has two constants of motion : the specific orbital energy $e$ and the specific relative angular momentum $h$: $$ \begin{align} e and = \frac{1}{2}v_{r}^2 + \frac{1}{2}v_{t}^2 - \frac{\mu}{r}= -\frac{\mu}{2a} , \\ h^2 and = r^2\ , v^2_{t} = \mu a ( 1-e^2 ) , \end{align} $$ where $a$ is the semi-major axis of the orbit , $e$ is the orbital eccentricity , and $\mu = g ( m_\text{p} + m_\text{s} ) $ , with $m_\text{p}$ the mass of the planet and $m_\text{s}$ the mass of the star . so we have two equations and three unknowns $ ( \mu , a , e ) $ , in other words we need additional information to solve them . for instance , if we assume that the orbit is circular , then $e=0$ , and we can solve for $\mu$ and $a$ . another possibility is that we know the distance and velocity at two instances $t_1$ and $t_2$ , then $$ \frac{1}{2}v_{r}^2 ( t_1 ) + \frac{1}{2}v_{t}^2 ( t_1 ) - \frac{\mu}{r ( t_1 ) }=\frac{1}{2}v_{r}^2 ( t_2 ) + \frac{1}{2}v_{t}^2 ( t_2 ) - \frac{\mu}{r ( t_2 ) } , $$ and we can solve for $\mu$ , and , from the first two equations , we know $a$ and $e$ . a third possibility is that we know the orbital period $t$ of the planet . in that case , we can use kepler 's third law : $$ t^2 = ( 2\pi ) ^2\frac{a^3}{\mu} , $$ which , in combination with the first two equations , yields $ ( \mu , a , e ) $ . in any case though , we can only derive $\mu = g ( m_\text{p} + m_\text{s} ) $ , i.e. we only know the sum of the masses ( which will be dominated by the mass of the star ) . if you want to derive the mass of the planet , you need to know the motion of the planet and the star with respect to their common centre of mass . if $ ( r_\text{p} , v_{r , \text{p}} , v_{t , \text{p}} ) $ and $ ( r_\text{s} , v_{r , \text{s}} , v_{t , \text{s}} ) $ are the position and velocity of the planet and the star with respect to their common centre of mass , then $$ \begin{align} e_\text{p} and = \frac{1}{2}v_{r , \text{p}}^2 + \frac{1}{2}v_{t , \text{p}}^2 - \frac{\mu_\text{p}}{r_\text{p}}= -\frac{\mu_\text{p}}{2a_\text{p}} , \\ h^2_\text{p} and = r^2_\text{p}\ , v^2_{t , \text{p}} = \mu_\text{p} a_\text{p} ( 1-e^2 ) , \\ e_\text{s} and = \frac{1}{2}v_{r , \text{s}}^2 + \frac{1}{2}v_{t , \text{s}}^2 - \frac{\mu_\text{s}}{r_\text{s}}= -\frac{\mu_\text{s}}{2a_\text{s}} , \\ h^2_\text{s} and = r^2_\text{s}\ , v^2_{t , \text{s}} = \mu_\text{s} a_\text{s} ( 1-e^2 ) , \end{align} $$ and in addition , if we know $t$ , $$ t^2 = ( 2\pi ) ^2\frac{a^3}{\mu} = ( 2\pi ) ^2\frac{a^3_\text{p}}{\mu_\text{p}} = ( 2\pi ) ^2\frac{a^3_\text{s}}{\mu_\text{s}} . $$ in principle , these are five equations with five unknowns $ ( \mu_\text{p} , \mu_\text{s} , a_\text{p} , a_\text{s} , e ) $ ( actually , these are six equations , but they are not independent ; also note that the eccentricities of the orbits are the same ) . also $$ \begin{align} r and = r_\text{p} + r_\text{s} , \\ a and = a_\text{p} + a_\text{s} , \\ m_\text{p}r_\text{p} and = m_\text{s}r_\text{s} , \\ m_\text{p}a_\text{p} and = m_\text{s}a_\text{s} , \\ \mu_\text{p} and = \frac{gm_\text{s}^3}{ ( m_\text{p} + m_\text{s} ) ^2} , \\ \mu_\text{s} and = \frac{gm_\text{p}^3}{ ( m_\text{p} + m_\text{s} ) ^2} . \end{align} $$ once the equations are solved , you can derive $a$ and $\mu$ ( using kepler 's third law again ) . so you have $m_\text{p}/m_\text{s}$ and $m_\text{p} + m_\text{s}$ , so that you can derive $m_\text{p}$ and $m_\text{s}$ separately . in practice , it is usually too difficult to measure $r_\text{p}$ and $r_\text{s}$ , because the centre of mass will be very close to the centre of the star . but by measuring the velocities at two instances $t_1$ and $t_2$ , we can treat $r_\text{p}$ and $r_\text{s}$ as additional unknowns and calculate them as well .
the surface charge compensates the outer electrical field . if you change that field by moving charges around in the surounding air then the surface charge of the conductor will change .
the first one is electrostatic potential energy , the second on is electric field . you can tell they are supposed to represent different physical quantities because they have different units . i am pretty sure that the way it is presented in your textbook , the second equation for the electric field is to be seen as justified by experiment and you will derive other things , for example , the first equation , from it .
the simple answer is because it becomes more stable by emitting alpha particle ( or any other kind of decay ) . nuclei become more stable once their proton and neutron count is close to " stability islands " which are just " magic " numbers that atoms are more stable when their proton and neutron number gets close to them . the reason behind why these islands exists can not be explained easily , and to some extent they arise from different theoretical models of the nucleus , which in general are almost impossible to be modeled accurately ( this info is kinda outdated and from an old book , maybe today with fast computers the situation is different ) .
as far as i know there has been no experimental evidence that light curves spacetime . we know that if gr is correct it must do , and all the experiments we have done have ( so far ) confirmed the predictions made by gr , so it seems very likely that light does indeed curve spacetime . the trouble is that spacetime is exceedingly hard to curve by any significant amount . curving it is no problem if you have an astronomical body to hand , but measuring the curvature due to lab scale masses requires very fine measurements . bearing in mind that mass is a very concentrated form of energy ( by a factor of $c^2$ ) it is hard to see how we could ever get an intense enough source of light to create measurable curvature . there might be some indirect measurement possible , but none springs immediately to mind .
no , the distribution does change . if the ' central ' person measures both the qubits in the $ |0\rangle$ , $ |1\rangle$ basis , say , then the observers do not see the same distribution in general . only their measurements along the z-axis still have the same distribution , but by tilting their measurement axis , they can conclude whether the qubit was measured or not . more concretely , let the state being prepared be $$ |\psi \rangle = \frac{|01\rangle + |10 \rangle}{\sqrt{2}} $$ then the density matrix corresponding to this ( pure ) state is $$ \rho = \frac{1}{2} \left ( \begin{array}{ccc} 0 and 0 and 0 and 0 \\ 0 and 1 and 1 and 0 \\ 0 and 1 and 1 and 0 \\ 0 and 0 and 0 and 0 \\ \end{array} \right ) , $$ whereas the state of the system after measurement by the central person would become $$ \rho_m = \frac{1}{2} \left ( \begin{array}{ccc} 0 and 0 and 0 and 0 \\ 0 and 1 and 0 and 0 \\ 0 and 0 and 1 and 0 \\ 0 and 0 and 0 and 0 \\ \end{array} \right ) , $$ which is not a pure state and is distinguishable from the previous state .
you have already mentioned the correct reason---the lagrangian is manifestly lorentz-invariant whereas the hamiltonian is not . since a relativistic field theory must be build of lorentz-invariant quantities only the lagrangian approach is good . compare for example the expressions for a free real scalar field $\phi$ $$ \mathcal{l}=\frac{1}{2}\partial_\mu\phi\partial^\mu\phi $$ this is lorentz-invariant , because the lorentz index $\mu$ is contracted in this way . the hamiltonian for this theory is $$ h=\frac{1}{2}\dot\phi^2+\frac{1}{2} ( \vec\nabla\phi ) ^2 $$ which is not manifestly lorentz-invariant . is there any other reason ?
let 's speak about 1d particles for simplicity . what should be understood first of all is that for indistinguishable particles configuration space is not the same as for distinguishable ones . for two distinguishable spinless 1d particles configuration space is a square : one side is for $x_1$ , another for $x_2$ . but if the particles appear indistinguishable , then half of the space is redundant : states obtained by exchange of particle coordinates are identical . so , for two spinless particles configuration space is really a triangle : here lower purple triangle is the config space . now , schrdinger equation for two indistinguishable spinless 1d particles becomes trivially to obtain from that for distinguishable particles : we just have to impose boundary conditions for $x_1=x_2$ line . for bosons wavefunction must be symmetric , and this implies homogeneous neumann conditions . for fermions wavefunction must be antisymmetric , so it has a node at the line of interparticle collision , and this means homogeneous dirichlet conditions . now for fermions this automatically gives us the correct eigenstates , which indeed obey pauli exclusion principle  they just can not have nonzero values on $x_1=x_2$ line . here're first several 1 : if we now want to return to our full ( square ) configuration space , we should just append zeroes to missing triangle , and then subtract the same wavefunction but with exchanged particles from it . it will automatically appear differentiable at $x_1=x_2$ line : now let 's include spin in our picture . thanks to spin being a discrete degree of freedom with finite range of values , for single particle we can just concatenate its configuration space parts corresponding to different spin values . for example , a state of a spin-$\frac12$ particle in infinite box in state $\left|1\uparrow\right\rangle+\left|3\downarrow\right\rangle$ could look like this : here left part corresponds to state of spin-up component of wavefunction , and right one is for spin-down component . note that the hamiltonian should not try to differentiate the wavefunction at the joint  as far as we neglect spin-dependent interactions , its matrix should just be a diagonal block matrix of two spinless hamiltonians . now for two indistinguishable particles nothing changes except we now use 4 times bigger full configuration space ( than in case without spin ) , or 2 times bigger correct triangle-shaped space . here 's how full configuration space would look : here spin states are denoted as $\left|s_1\right\rangle\left|s_2\right\rangle$ where $s_i$ is spin of particle $i$ . lines of interparticle collisions are denoted by slight dark color . now the truncated symmetrized configuration space : the upper-left rectangle is now merged with lower-right one . how  it will depend on the state . also note that there is still possibility of interparticle collision  inside this lower-right rectangle  which we have not symmetrized , and in fact must not do ( there is no reason to do it in this case ) . now it is easy to see that the states with identical spins are forced to have antisymmetric orbital  their orbital parts of configuration space are triangular . the states with non-equal spins may be symmetric or antisymmetric ( or asymmetric at all ) . this corresponds to known classification of two-particle states into spin-singlets and spin-triplets . just looking at the wavefunction , one could immediately say what type of state we have : triplet states will have a node along interparticle collision line  this means the orbital is antisymmetric . let 's now look at first several states : here we can see : spin-singlet states  numbers 1,2,6 , and triplets  numbers 3,4,5 . for more than two particles it seems quite straightforward to generalize : just truncate the configuration space so that it is no longer possible to exchange particles , and impose the correct boundary conditions on the hypersurfaces which appear after these cuts . for many particles this may even give some computational resource savings : hypervolume of configuration space would reduce by $n ! $ where $n$ is number of particles , as compared to space for distinguishable particles . 1: i used simplified way of computing these states , just putting very large potential barrier in the upper-left triangle . to decouple different spin parts of wavefunctions i also added thin barriers . in real computation one should , of course , use the opportunity to remove extra data from processing and more correctly define the hamiltonian matrix .
in general relativity gravity propagates at c . the rate of orbital decay of binary pulsars is , among other factors , dependent on the speed of gravity . the in-spiral rate of one binary pulsar system has been measured and found to agree with the rate predicted by general relativity to within a 0.2% margin of error . gravitational waves have not been directly measured yet though , so there is no direct confirmation . with multiple detectors currently in operation if a signal is detected and able to be tied to a specific location in space the timing delays between when its received between the two primary ligo detectors ( livingston , la , usa and hanford , wa , usa ) and the virgo detector ( pisa italy ) should allow for estimating a propagation speed . advanced ligo , expected to begin collecting data in 2014 is expected to be able to detect a number of signals so hopefully the question will be settled in a few years . however , there might not be any published results for a while after it goes active . the search for pulsar spindown signals with ligo data is done via the einstien@home distributed computing project ; and in prior runs several years passed between when the first part of the data set was collected and when papers on it were finally published .
if you go down further into the article , you will find this : uplift , called such as it tends to lift the dam upward , a condition which although many designers and builders of dams had become aware of by the late 1890s to early 1900s , was still not generally well understood or appreciated . in essence , the dam is lifted by the water seeping under it from the reservoir .
sound traveling through an aircraft propagates at a speed relative to the aircraft , so no , it can not go so fast that the atoms can not communicate via vibrations any more . if sound moves through the aircraft at 1000 m/s and the plane is flying past you at 1500 m/s , you will observe a sound wave to travel from back to front at 2500 m/s . when an jet accelerates , it does indeed compress a bit , since it takes a short time ( hundredths of a second ) for the front of the plane to know the back is accelerating . however , the plane feels only acceleration , not velocity . the plane cannot in principle detect its own velocity due to the principle of relativity . if the plane accelerated hard enough , yes it could buckle up on itself . you may also be interested to know that shocks frequently travel through materials faster than their sound speeds , and that you could in principle send a message along a pole faster than the speed of sound in the pole . it is not a fundamental limit like the speed of light . also , when people talk about supersonic travel , they mean the craft is traveling through the air faster than the speed of sound in the air , not that it is traveling through air faster than the speed of sound in its own material , which would not be very physically meaningful , and also never really happens since the speed of sound in metals is very fast .
because you have $d$ coordinates , so you get $d$ choices for choosing coordinates -- i.e. , i choose eastern standard time , and cylindrical coordinates centered on the sun , measured in kilometers and radians . this would make the solar system metric go from having ten independent components to six .
it is not often that dmckee and i differ ( mainly because he is usually right :- ) but we differ on this on . or at least we differ if i have correctly understood what you are asking . in a hydrogen atom the 1s , 2s , etc wavefunctions are ( subject to various approximations ) good descriptions of the single electron and have well defined angular momentums . in multielectron atoms it is convenient to think of electrons populating successive 1s , 2s , etc levels , but this is only a conceptual model and not an accurate representation . you are quite correct that while there is a well defined angular momentum for the whole atom , you cannot define the angular momentum of individual electrons . in the old days ( maybe it is still done ) we had calculate atomic structure using a hartree-fock method with individual electron wavefunctions as the basis , and as dmckee points out , atoms have spectral lines that can often be approximately thought of as exciting a specific electron between individual electron wavefunctions . however what you are really doing is labelling the whole atom as an $l , m$ state and not an individual electron .
i do not really follow the text underneath the title , but the answer to this question is most certainly , yes . ( for example , we witness electromagnetic waves traveling through curved space . ) here is how electromagnetism is described mathematically ( i fear this answer is slightly beyond the level of the questioner -- sorry -- but perhaps not of other readers ) : maxwell 's equations are best expressed in terms of the field strength tensor . that tensor is the curvature of a connection on a circle bundle ( over spacetime ) . the connection is the four-vector electromagnetic potential , in physics terms . in this set up , two of maxwell 's equations are automatic ( for example , saying that the magnetic field is locally the curl of a 3-vector is saying that it is divergence-free ) . the other two are equations for the " divergence " of this tensor . this way of phrasing the problem makes sense in any metric , i.e. on curved spacetimes .
the physics of a gliding airplane are simple . there is potential energy , proportional to height above the ground . there is also kinetic energy , proportional to speed squared . first , understand the speed . if the plane is not slightly nose-heavy , it will fly a scalloped up-down cycle . if it does that , add a little weight to the nose , or distribute the wing area more toward the rear . assuming you have done that , you control the speed by turning up the trailing edges . the more they are turned up , increasing the angle of attack , the slower it flies . ( up to a maximum angle of attack , at which the wings stop working , or " stall " . ) back to energy . if there were no drag , the plane would never come down . since there is drag , the drag tends to slow the plane down , decreasing its kinetic energy . countering that is the plane 's tendency to maintain constant speed and kinetic energy , so it descends , turning potential energy into kinetic energy , just like a ball rolling down a slope . so the more drag , the more quickly it descends , the less drag , the more slowly it descends . a way to minimize drag is to minimize speed , because drag force is proportional to speed squared . ( therefore the sink rate is roughly proportional to speed squared . ) so the speed you trim it for depends on what you want to maximize : to maximize gliding range , you trim for a speed which is slow enough to have low drag , but not so slow that you do not cover much ground . to maximize time aloft , you trim for an even slower speed which has even lower drag , thus minimizing the sink rate . this speed is roughly half way between the speed for maximum range and the even slower stall speed $v_s$ . check these links : v-speeds , and gliding flight .
i wrote a blog post about this some time ago . the answer is yes , but by a tiny amount that you would never be able to measure : something like $10^{-14}\text{ g}$ ( roughly ) for a typical ~1tb hard drive . that value comes from the formula for the potential energy of a pair of magnetic dipoles , $$e = \frac{\mu_0}{4\pi}\frac{\mu_1 \mu_2 \cos\theta}{r^3}$$ in my post , i estimate that a hard drive might contain $10^{23}$ electrons total , split into $10^{12}$ magnetic domains which are spaced around $0.1\ \mathrm{\mu m}$ apart . that means the magnetic moment of each of these domains is $10^{11}\mu_b$ , with $\mu_b = \frac{e\hbar}{2m_e}$ being the bohr magneton . if you plug this into the formula above , and multiply by 4 under the assumption that each magnetic domain interacts with 4 nearest neighbors , you wind up finding that the total energy is no more than $5\text{ j}$ , depending on the value of $\cos\theta$ . that corresponds , via $e = mc^2$ , to an equivalent mass of around $10^{-14}\text{ g}$ . admittedly all of these numbers are rough order-of-magnitude estimates , and there are various other effects that contribute little bits to the energy , but any corrections are not going to shift this by more than a couple of orders of magnitude one way or another . given that the equivalent mass of the energy stored in the magnets is a full 17 orders of magnitude less than the mass of the hard drive itself , it is safe to say that the difference is undetectable . incidentally , i also tried out the equivalent calculation for flash memory in another blog post .
the easiest ( and roughest ) way to to do it would be to convert your running " work " into a vo2 score . the american college of sports med 's equation is vo2= resting component + horizontal component + vertical component or vo2= 3.5 + ( 0.2 x speed ) + ( 0.9 x speed x elevation gain ) so , using your example of 8.67 mph ( speed in the equation is in meters per min ) 3.5 + ( . 2*232.67 ) + ( 0.9*232.67* . 045 ) = 59.5 thus running on flat ground should give you a speed of 280 m/min or 10.44 mph 5 min 44 sec per mile ( i am an exercise scientist , not a physicist )
1 ) we give a proof by contradiction . 2 ) assume there is a time-independent holonomic constraint of the form $$ f ( x , y , \theta , \phi ) ~=~0 . $$ 3 ) now , by performing a small loop of various sizes , we can ( by all the time obeying the rolling condition ) make the disc return to the same values of $x$ , $y$ and $\theta$ , but with an arbitrary value of $\phi$ . we conclude that the $f$ constraint cannot depend on $\phi$ , i.e. , $$ f ( x , y , \theta ) ~=~0 . $$ 4 ) by performing a small loop , we can return the disc to point in an arbitrary new direction $\theta$ . we conclude that the $f$ constraint cannot depend on $\theta$ either , i.e. , $$ f ( x , y ) ~=~0 . $$ 5 ) this constrains where the disc can touch the table , which is absurd . hence $f$ does not exist .
i think this is a combination of both a convention and a physical problem . you are equating the energy eigenvalue ( ie , the total energy ) to an expression that contains only $x_{zpf}$ , and does not contain $p$ at all . in other words , you are equating the total energy to a potential energy . this would be analogous to equating $e_\mathrm{total} = \frac{1}{2}ka^2$ to find the amplitude $a$ of a classical harmonic oscillator . the result is that you are using $x_{zpf}$ to mean the " amplitude " of the zero-point fluctuation . the true result , as ondrej cernotik 's answer derives , uses the rms value $x_{zpf} = \sqrt{\langle\hat x^2\rangle}$ . so that is the sense in which it is a convention . the sense in which it is a real physical problem is that the " amplitude " of a quantum oscillator is not really a well-defined , measurable thing . the quantum oscillator has a non-zero probability amplitude going all the way out to infinity . the rms value is well-defined and easy to measure . so that is the preferred definition .
classical electrodynamics generally makes a continuity approximation for bulk materials . we are not interested on variation in the fields at the scale of the distances between atoms , so we just average them away . 1 with that approximation , the conduction electrons are acted on by the mean field . they also contribute to the mean field but we treat the two bits separately because there are so many conduction electrons that we can pretend that they are a continuous fluid instead of a set of discrete parts . 2 finally the " zero field in a conductor " argument ( and related ones like " field perpendicular to the surface " ) are contingent on having reached a quiescent state ( sometimes called the " steady state " ) . anytime things change there will be a ( usually brief ) period when this is not true . that is referred to as the " transient behavior " . 1 there are situation in which that trick is not appropriate , but until you have a strong foundation you do not have the tools to look closely at them . 2 this is often a good approximation even at the microscopic scale because at that scale the electrons need to be treated with the ( quantum ) methods of condensed matter physics .
i am not an expert at thermography per se , but i have designed and built a commercial microbolometer ir camera and looked at several other cameras ( for thermography ) and as far as i know , they do not have any " funny business " going on . commercial handheld thermography cameras require you to input the surface emissivity , air temperature and transmittance parameters etc to get an accurate reading , so obviously in practice for random measurements you will not get anywhere near +/- 1k accuracy , but the noise ( precision ) can be as low as dozens of mk like quoted . in industrial ( repeatable ) applications you can probably calibrate this very accurately though .
this multi-photon process are possible , but of higher order ( in the couplage $\alpha$ between the photons and the electrons ) , and therefore highly improbable . furthermore , you need that the frequency of the photons $\nu$ is such that $n \nu=w$ , with $w$ the energy needed to eject an electron from the metal , et $n$ an integer . but in principle , you are right , there should be a ( very small ) photoelectric current scaling as $\alpha^n$ when $n\nu= w$ . i do not know if people have tried to measure it . edit : 5 minutes of googling shows that there are plenty of references about the multi-photons photoelectric effect . here is a random selection : one experiment ( free access ) , another .
your physical intuition is correct , it is indeed sum over all admissible paths . there is a problem with viewing $dx ( t ) $ as a measure on the function space , because it is not well defined ( e . g . infinite at some ' points ' $x ( t ) $ ) . this is one of the big ( and as far as i know open problems ) in the mathematical formulation of path integrals . often one absorbs the kinetic part of the hamiltonian in the measure to get an exponential dumping factor , e.g. $d x ( t ) exp ( -\frac{i}{\hbar} \int t [ x ( t ) ] $ . salmhofer considers in renormalization : an introduction many of the mathematical questions . ( if you know other good sources , i would be happy to hear about them ! ) path integrals are a nice way to ' visualize ' many calculations ( e . g ' i sum xyz over all possible paths ) , but are hard to compute . indeed , the only calculations i know are based on breaking the path in linear segments ( and even this gets clumsy ) . often one performs some kind of taylor expansion and only considers the first orders . there are then rules how to calculate often recurring terms ( see feymann diagrams ) .
it would crash into the earth because the earth 's gravitational field is not uniform and , even if said ring were to be perfectly positioned , ignoring the effects of wind , strikes from cosmic debris ( not a lot that low in the atmosphere ) , change in mass of the ring ( e . g . corrosion ) , change in shape of the ring ( due to e.g. gravitational forces , heat deformation from sunlight ) , etc . , the earth 's gravitational field distribution changes over time and the ring would eventually fall out of its perfect position and crash into the surface .
starting at ${position}_z$ = $z$ = 0 and $v ( z ) = 0$ and by tracking multiple acceleration values either with a time interval or at fixed intervals , $t$ , then you can get the position . . . . somewhat . it will drift over time . also , your device cannot rotate whatsoever , or else you need a gyroscope to track that and then use trigonometry to properly orient the x y and z values from the accelerometer . assuming it is always oriented such that the $a ( z ) $ is always perfect vertical acceleration ( if you are in a vehicle that is always flat , in which case z does not matter , or you are on a vertical guide rail ) , $$p ( z ) = \int_0^t v ( z ) ~dt = \iint_0^t a ( z ) ~dt $$ also , from here : short answer : forget about it . longer answer : unless you are on a perfectly straight rail , you will not achieve what you want to do without ( a ) a set of gyros ; and ( b ) far more accurate sensors than what you have . accelerometers measure acceleration in the body fixed reference frame , whereas you need some displacement in an earth-fixed frame . therefore , you need not only to integrate the accelerometers , but rotate them into the earth-fixed frame before doing the integration . this is assuming perfect sensors . mems sensors are far from perfect - i have written up a post on some of the errors here . consider two errors : 1 . a bias on the accelerometer . 2 . an initial attitude ( tilt ) error . in addition to whatever acceleration signal there is , integrate a bias and you get a ramp error with time . integrate the ramp and you get a quadratically increasing error with time . this will add up really , really quickly . consider a tilt error . you will now be measuring some of the gravity vector in the forward ( or whatever ) direction . integrate this error twice and you will have the same problem as the bias . so , my advice again is do not ! find another method . also , check this book out for more detailed designs , or use whatever sensors and algorithm these guys are on : http://www.youtube.com/watch?v=6ijarke8vku if you still want to give this a shot , use the trapezoidal method in excel , it is pretty easy . there is an explanation page here with a sample , but here 's a more complete way :
you will see it the same , regardless of the refraction index of your medium . the reason is as simple as that , when the light hits your retina , it will be travelling through the interior of your eye , so the only refractive index that matters is that of the eye . what is what we actually detect , wavelength of frequency ? frequency is the one related to energy , so my feeling is that that should be the one influencing chemical reactions , that is , at the end , the way cones can detect light . indeed , the vitreous humour ( the interior filling of the eyeballs ) looses water with age , to the point of getting deatached from the retina , something very common among old people ( wikipedia says 75% of > 65 ) . the main consequences are visual artifacts , but no one has claimed colours suddenly look different . physics books quote wavelengths because those are usually what one measures in the lab in the optical range . plus , the numerical values are ( and this is subjective ) more convenient .
light does not slow down during a reflection . light is a signal disturbance in electric and magnetic fields . these disturbances propagate through space at a fixed speed $c$ in vacuum . the situation is completely analogous , in a mathematical sense , to a wave pulse that is sent along a string . when the pulse encounters a boundary , it flips direction , and may or may not change phase depending on the type of boundary encountered . for good graphical depictions of this phenomenon , visit this page . if you emit a pulse of light at a distance of 1 meter from a plane mirror , and measure the amount of time it takes for the signal to return , you will find that it is 2 meters / $c$ , neglecting refractive effects of the air . in this sense , we say that the light has not slowed down , even though it has changed direction in the middle of its journey .
i think the formula you show is not the travel time for a refracted ray . it is t0 , the intercept of the time-distance curve with the x-axis for the refracted ray . see this figure , and associated formula , and the difference should be apparent . perhaps your text is in error or unclear . keep in mind that multiple ray paths can reach the receiver location - including a direct ray , reflected ray , and refracted ray . which ray is the first arrival depends upon the underlying structure and distance from the source . the refracted ray is the first arrival at distances greater than xcross1 .
you already have several excellent answers , but from your question i suspect you are hoping for an answer that is a bit less mathematical and more about just how the transition between quantum physics and ordinary large-scale physics takes place . for most everyday phenomena , including chemistry and the way behaves and interacts with matter light ( radio , light , x-rays , gamma rays ) , quantum mechanics already predicts classical phenomena with exquisite precision , as best we can tell . qed the " as best we can tell " qualifier is because the computational cost of doing so is horrendous and grows larger so quickly that you have to start approximating very early in the process . the theory that provides this level of precision -- arguably the most precise predictive theory ever developed -- is something with the ungainly moniker of quantum electrodynamics , or qed for short . the variant of qed that richard feynman co-received a nobel prize for is the one that led to feynman diagrams , those little stick-figure diagrams that show electrons as arrows and light particles ( photons ) as squiggles . qed also shows beautifully how such odd physics can lead to our ordinary world : through probabilities . many very strange things are possible in quantum mechanics , including for example light moving around in random loops and turns instead of in a straight line . qed even allows you to calculate the probability of such things , and then build experiments to very that such oddities really do exist ! reasonable probabilities but what happens on the scale of ordinary humans is that these oddly quantum scenarios quickly become so incredibly minute that they simply do not happen , at least not within the life span of the universe . so light , for example , seems to go in straight lines for the most part ( even on our scale that is not entirely true ) because the probabilities for all those odd paths , or even paths slightly off from straight , become vanishingly small . added all together , this collection of " reasonable probabilities " for many , many atoms and particles of light becomes what we think of as ordinary or classical physics . surprisingly , the transition between the two via probabilities is quite smooth and not abrupt in any way . we just do not notice that transition much because , except for certain large-scale phenomena such as metallic mirrors that seem " classical " only because we have simple rules to approximate how they work , all of these transitions from strange quantum physics to the comparative simplicity of classical physics take place at very small scales , typically near the size and mass scale of atoms and their constituent particles . qcd : going nuclear for nuclear phenomena , a similar theory that by intentional analogy called qcd ( for quantum chromodynamics ) does a pretty good job of predicting why particles like protons and neutrons -- and many other less common particles -- behave like they do . that theory is even more difficult to compute than qed , however . the standard model beyond that is the standard model , an intensely quantum-based model that ties together the entire zoo of particles that we see coming out of particle colliders . although the standard model addresses only a very narrow and exotic range of predictions of phenomena that we never encounter directly in everyday physics , it nonetheless is critical for explaining much of how the large-scale structure of the universe emerges . through this roles in defining the universe in which we exist , the standard model also helps explain how quantum phenomena lead ( much less directly ! ) to what we call the classical world . gravity so what is missing ? gravity ! gravity remains ornery and uncooperative in terms of its quantum description providing precise predictability . that is not from any lack of trying ! speculations on quantum gravity are in fact the darling of many popular programs and ideas about physics . but because the original theory of general relativity by a fellow named einstein was a purely geometric theory without a shred of quantum anything to it , it remains to this day a theory that is difficult to fold into the kind of pure-quantum framework exemplified by the spectacular success of qed . that might not be a problem if general relativity was an inaccurate or approximate theory , but that is not the case : like qed for its domain of electrons and light , general relativity for its domain of the overall structure of a universe with gravity remains spectacularly effective and predictive for what we can see . conclusion so , for much of the world , and all of the parts of it that we see and interact with on a daily basis , the transition between quantum physics and ordinary physics is already surprisingly well understood from a mathematical perspective , even if philosophical views are far from being in agreement : it is all just a matter of probabilities , with very small things allowing more ( and more strange ) things to go on , and classical physics just being the sum of probabilities that begin to get very specific and very selective at larger scales . there are holes still , sure , but it seems likely that even when those holes are someday filled , that theme of higher probabilities providing the transition will remain .
it is not the falling that is fatal , it is the deceleration at the end that kills you . something like water or concrete does this on a sub-meter distance ( which requires extremely high forces ) . on the other hand a gas is much less dense , so it cannot decelerate a falling object nearly as quick . sometimes inflatable cushions are used as safety nets ( think : stunts/someone jumping off a building scenario ) . if it is too inflated then the deceleration distance will not be great enough and it can still cause injury or even death . it seems that a sudden deceleration of ~100g is fatal ; that is about 80kn for an average male ( 80kg ) . we need the drag formula : $f_d = \frac{1}{2}\rho v^2c_da$ . plugging in typical values : $f_d = 80*10^3n$ as asserted above , the density of air humans experience is typically $\rho = 1 \frac{kg}{m^3}$ . $a$ , the frontal surface of a human seems to be hidden behind pay walls ; let 's go with $a = 0.5 m^2$ $c_d$ , the drag coefficient , is not so straightforward , but we will go with $1.3$ ( man , ski jumper example given on the wikipedia drag coefficient page ) . $80*10^3n=\frac{1}{2}*1*v^2*1.3*0.5 $ . . . . . . results in a speed of about $500 m/s$ , or 1800 km/h . this does not mean that falling at that speed is lethal . this scenario assumes you suddenly transition form no resistance into dense air .
saying that a splitting varies over the moduli space is not completely well defined : you have to say how to identify the total spaces at different points of the moduli i.e. to specify a flat connection on the bundle of total spaces . in the b-model , if you take the gauss-manin connection as the flat connection then the hodge splitting varies over the moduli space ( because the gauss-manin connection does not preserve the splitting in general ) . in the a-model , if you take the trivial connection as the flat connection then the splitting does not vary over the moduli space ( the trivial connection preserves the degree decomposition ) . but it is not the trivial connection which appears in mirror symmetry on the a-model side but a flat connexion which is the trivial one corrected by contributions of holomorphic world-sheet instantons ( i.e. . gromov-witten invariants ) and this connection does not preserve the degree decomposition in general . about the vacuum line bundle . on the a-model sigma model side , 1 in h^{0} gives a natural trivialization . but the sigma model description is generally only valid in some limit of the moduli space , some cusp which is topologically a punctured polydisk . in particular , any complex line bundle is trivial in restriction to this domain and this is also the case for the vacuum line bundle of the b-model . deep inside the moduli space , the topology can be complicated and the vacuum bundle of the b-model can be non-trivial but it is also the case for the a-model which has no longer a sigma model description and so no longer a "1" to trivialize $\mathcal{l}$ . ( remark : the genus g string amplitude is a section of $\mathcal{l}^{2-2g}$ and not $\mathcal{l}$ . )
a theory is a collection of concepts , laws , and equations in science that is meant to explain some particular subset of observations . it is also used for theories describing gedanken worlds that differ from ours . there is also a related word " model " that differs by a theory by being really specific while a " theory " may leave some details adjustable , and " framework " which is on the contrary less specific than a theory and fully determines the general methods , and type of objects and arguments that are allowed in research . the boundary between the terms is not quite sharply delineated . a theorem is a mathematical proposition , usually a hard one to prove or disprove , and usually a far-reaching and general enough , that has been proven to hold by rigorous mathematical methods . a lemma is a less important version of a theorem , usually one that is used as a step to prove full-fledged theorems . " a rule " is usually used for some prescriptions that should be memorized and that chooses the right answer from a usually small list of possibilities . in particular , there are right hand rules that determine the sign of some quantities and/or its relationship to directions in space ( this direction or opposite direction ) . some of the rules define conventions  claims that could have also been chosen in the opposite way but people must understand each other so they agreed to use one particular sign etc . hund 's rules determine which angular momentum is chosen  again , the number of candidates is usually limited . " a law " is a more substantive insight about nature , a building block of our understand how nature ( or the society ) works . it may be equivalent to one equation , one identity for continuous quantities , like coulomb 's law for the attraction or ohm 's law $u=ri$ etc . how nature works is described by the " laws of physics " . " a principle " is more general than a law and it is usually a statement of a general type or a philosophy or a condition that good laws ( explained in the previous paragraph ) are supposed to satisfy . a principle is therefore an extra criterion that has been induced from the observations and the known laws and that is imposed on the new candidate laws . the principle of relativity is an example . principles usually require more verbal , words-based descriptions to be fully formulated , as opposed to a single particular equation that may define a " law " .
some more misconceptions : the chemguide website quoted above might be a useful reference for " uk-based exam purposes " as stated there , but it certainly does not help in solving the question . the arguments given above that followed the comments on chemguide are inaccurate . a simple quantum chemistry calculation of gold in its ground state will give you that the electron in the s orbital ( a1g ) is the most energetic in this atom . hence , ionization will most easily be accomplished by removal of this electron , and not of d electrons , and this is easily proved by another computation for ionized gold , which will show you that the 5d orbitals will remain filled while the 6s orbital is no longer occupied . actually , it is known that if the most external d shell is filled , the energies of these orbitals will be effectively lowered , and there is a very high probability that the ionized electron will not come from it , but from more energetic s or p orbitals . ( i have just done a few of these calculations in order to make sure this point is right ) in order to analyze why some metals are more inert than others , various effects come into play . relativistic effects , such as the contraction of s orbitals , for example , are a major factor in making gold less reactive than silver , and in lowering the oxidation potential of gold . so , in addition to looking at chemical potentials when discussing the inertness of metals in different environments , it is better not to reduce the arguments to simple electron configuration trends which usually work quite well for main group elements , since , though they might generate insights for the understanding of the behavior of metals , these insights might be either right or wrong .
dear humble , the cosmic nucleosynthesis ( first three minutes ) only produced hydrogen , helium , lithium , and beryllium . all heavier elements came from slow fusion inside living stars and , especially the heaviest ones , originate from dying stars . regular fusion-related processes inside stars ( see the list at the bottom of the answer ) only produce elements up to $z=70$ or so : i included the periodic table for your convenience . the even heavier elements , especially gold and uranium , were produced by three extra processes s-process r-process rp-process the s-process depends on the existence of elements in the iron group . an extra neutron may be absorbed ( probably coming from reactions inside red giants ) , increasing $a$ by one , and if an unstable element is produced in this way , a neutron in the nucleus beta-decays by emitting an electron . additional neutrons may be absorbed and the process may continue . nuclei in the " valley of beta stability " can be produced in this way . " s " stands for " slow " . on the contrary , the r-process is " rapid " . the neutrons are absorbed in a similar way but in the cores of supernovae . the seed nucleus is usually ni-56 . the rp-process , which may occur in the neutron stars and elsewhere , is also rapid , " r " , but the particle is that absorbed is a proton , therefore " p " in " rp " . logically , unlike the previous two , it produces nuclei on the " proton-rich side " of the stable valley . the uranium we observe on the earth probably comes from all these processes - and from many stars - there is arguably no " the star " that preceded the sun . in particular , our earth has not orbited any other star before the sun because it is as old as the sun , at least this is what is believed . the hydrogen used by our sun could not have been " recycled " and it began to burn soon after the sufficient collapse - it could not have been recycled from elsewhere . the heavier elements were recycled from many places . there was probably no " permanent region " that was inheriting the brand " solar system " . these issues were discussed yesterday : how many times has the stuff of the sun been recycled ? how many times has the " stuff " in our solar system been recycled from previous stars ? let me mention that it is not a problem for the heavy material to spread across large distances of the cosmos . for example , an exploding supernova shoots most or all the matter by the speed 1% of the speed of light . so in 400 years , the material from the sun - if it went supernova ( it will not ) - reaches proxima centauri and in less than 100 million years , it may reach almost any point in the milky way . even the solar system is moving at speed of 0.1% of the speed of light which is enough to move matter by light years in thousands of years . it is silly to imagine that the material had to wait on the same place from an " ancestor star " , being saved for some humans on some earth . it may be useful to list all processes of stellar nucleosynethesis , not only those linked to uranium : pp-chain/ cno cycle/  process/ triple-/ carbon burning/ ne burning/ o burning/ si burning/ r-process/ s-process/ p-process/ rp-process
" relativity " is actually a misleading word that einstein did not like . it does not mean " every vantage point is equivalent and it is all relative " . it really means only inertial , non-accelerating vantage points are equivalent . you could think of it as , prior to relativity , people believed that there was an absolute position/speed to the universe . special relativity shows that there is not , but rather , there is an absolute acceleration to the universe . this is illustrated by the famous rotating bucket thought experiment . you put a bucket out in the middle of empty space and spin it , and the water in it starts flowing towards the edges . but if all vantage points were the same , could not you also think of it as the universe spinning and the bucket stationary ? but one vantage point is obviously more correct than the other , because only one involves the water flowing towards the edges . thus there is a " universal " state of zero acceleration that is unambiguous . it is interesting to note that einstein 's original idea for his theory of special relativity was a theory of invariance ( of the speed of light )
you measure the velocity ( relative the earth frame ) by observing the red- or blue-shift of easily identified lines in the spectrum , once you have more than a full cycle ( preferably several cycles ) you : subtract off the contributions from the earth ( known ) orbital velocity around the sun find the period of the orbit just by looking at the peak-to-peak time of the cycle remove the mean relative velocity , and find longitudinal velocity in the remote system center of mass frame as a function of time . you can also get the longitudinal size of the orbit by integrating the velocity compute the reduced mass and reduced radius of the orbit from kepler 's laws in the limit of one massive and one light body the reduced mass is the mass of the heavy partner . if you can make these observations for both bodies you can get both masses .
if the cage is completely closed , it does not make a difference if the bird is hovering inside it or if it sits on the ground . when flying , the bird pushes air to the ground which will exert a downward force on the cage exactly equal to the weight of the bird . this is a direct consequence of the conservation of momentum and newton 's second and third law . since no additional external force is acting on the cage-bird-system when the bird is flying as compared to when it is not , the acceleration on the cage can be no different . the effect due to the flying bird only concerns the internal forces and since action=reaction , they cancel . however , if the cage would not be closed , some of the ' wind ' due to the bird could escape the cage and would become an external force , making the cage-bird-system lighter .
the problem is with your first calculation and also with the somewhat misleading equation that you have found . it is true that $$\frac{i_2}{i_1}=\left ( \frac{d_1}{d_2}\right ) ^2$$ but units are important here . in that formula , $i_1$ and $i_2$ would properly be expressed as power values . to compute with decibels , which are logarithmic quantities , one would instead use $$i_1 + 10\log\left ( \frac{d_1}{d_2}\right ) ^2 = i_2$$ or equivalently , $$i_1 + 20\log\left ( \frac{d_1}{d_2}\right ) = i_2$$ , where $i_1$ and $i_2$ are decibels and $d_1$ and $d_2$ are in identical linear units ( feet or meters , for example ) . with your particular numbers we get $$\begin{eqnarray} i_2 and = and 213\text{ db} + 20 \log\left ( \frac{75}{45000}\right ) \\ and = and 213\text{ db} + 20\log\left ( \frac{1}{600}\right ) \\ and \approx and 213\text{ db} + 20 ( -2.78 ) \\ and \approx and 213\text{ db} - 55.56 \\ and \approx and 157.4\text{ db} \end{eqnarray}$$ estimating manually you have correctly remembered that -3db is half the power . that is , $$\frac{1}{2}p = -3\text{db}$$ . another easily remembered fact is $$\frac{1}{10}p = -10\text{db}$$ . both are very commonly used in engineering for rough estimations . so in this case , because it is an inverse square law , we have $$\begin{eqnarray} \left ( \frac{75}{45000}\right ) ^2 and = and \frac{1}{600^2} \\ and = and \frac{1}{360000} \\ and \approx and \frac{1}{400000} \\ and \approx and \frac{1}{2^2\cdot 10^5} \\ and \approx and -6\text{db} - 50\text{db} \\ and \approx and -56\text{db} \end{eqnarray}$$ so this would give $213\text{db} - 56\text{db} = 157\text{db}$
the wave function itself can never be discontinuous . it is the derivative what it is discontinuous at $x=0$ , and that discontinuity can be calculated integrating the schrdinger equation between $ ( +\epsilon , -\epsilon ) $ and taking the limit $\epsilon \to 0$ . all terms but the proportional to the delta vanish , giving you $$\left . \frac{d\psi ( x ) }{dx}\right|_{\epsilon=0^+} -\left . \frac{d\psi ( x ) }{dx} \right|_{\epsilon=0^-}=-2\hbar \alpha\psi ( 0 ) $$
power - your wifi router puts out about 0.1 - 1.0 w , your microwave oven puts out 1000w . it would take a lot of wifi routers to cook a turkey - more than you think because the antennea on the router is designed to spread the power evenly around the room rather than concentrate it on the center of the oven . there is a danger of being ' cooked ' from being close to very high power transmitters such as some warship 's radar while they are operating . ps . it is the same reason your laser pointer can not be used to cut steel plates ( or james bond ) in half !
if you ignore friction in the drive train even the smallest torque will start the obect moving . your calculation of force is correct , and from newton 's first law the acceleration will be the force divided by the mass . so even the smallest force ( i.e. . torque ) will cause the object to accelerate , albeit very slowly . however experience suggests that if you apply a small force to your car it will just sit there and smile at you . this is because there is static friction in the gearbox , axle etc and you need to apply a force great enough to overcome this static friction . i do not know any easy way to calculate what the static friction will be ; i think you will have to measure it .
i understand your confusion , but here 's why people often feel that quantum entanglement is rather strange . let 's first consider the following statement you make : 2 things have some properties set in correlation to each other at the point of entanglement , they are separated , measured , and found to have these properties a classical ( non-quantum ) version of this statement would go something like this . imagine that you take two marbles and paint one of them black , and one of them white . then , you put each in its own opaque box and to send the white marble to los angeles , and the black marble to new york . next , you arrange for person l in los angeles and person n in new york to open each box at precisely 5:00 pm and record the color of the ball in his box . if you tell each of person l and person n how you have prepared the marbles , then they will know that when they open their respective boxes , there will be a 50% chance of having a white marble , and a 50% chance of having a black marble , but they do not know which is in the box until they make the measurement . moreover , once they see what color they have , they know instantaneously what the other person must have measured because of the way the system of marbles was initially prepared . however , because you painted the marbles , you know with certainty that person l will have the white marble , and person n will have the black marble . in the case of quantum entanglement , the state preparation procedure is analogous . instead of marbles , we imagine having electrons which have two possible spin states which we will call " up " denoted $|1\rangle$ and " down " denoted $|0\rangle$ . we imagine preparing a two-electron system in such a way that the state $|\psi\rangle$ of the composite system is in what is called a superposition of the states " up-down " and " down-up " by which i mean $$ |\psi\rangle = \frac{1}{\sqrt 2}|1\rangle|0\rangle + \frac{1}{\sqrt{2}}|0\rangle|1\rangle $$ all this mathematical expression means is that if we were to make a measurement of the spin state of the composite system , then there is a 50% probability of finding electron a in the spin up state and electron b in the spin down state and a 50% probability of finding the reverse . now me imagine sending electron $a$ to los angeles and electron b to new york , and we tell people in los angeles and new york to measure and record the spin state of his electron at the same time and to record his measurement , just as in the case of the marbles . then , just as in the case of the marbles , these observers will only know the probability ( 50% ) of finding either a spin up or a spin down electron after the measurement . in addition , because of the state preparation procedure , the observers can be sure of what the other observer will record once he makes his own observation , but there is a crucial difference between this case and the marbles . in electron case , even the person who prepared the state will not know what the outcome of the measurement will be . in fact , no one can know with certainty what the outcome will be ; there is an inherent probabilistic nature to the outcome of the measurement that is built into the state of the system . it is not as though there is someone who can have some hidden knowledge , like in the case of the marbles , about what the spin states of the electrons " actually " are . given this fact , i think most people find it strange that once one observer makes his measurement , he knows with certainty what the other observer will measure . in the case of the marbles , there is no analogous strangeness because each marble was either white or black , and certainly no communication was necessary for each observed to know what the other would see upon measurement . but in the case of the electrons , there is a sort of intrinsic probability to the nature of the state of the electron . the electron truly has not " decided " on a state until right when the measurement happens , so how is it possible that the electrons always " choose " to be in opposite states given that they did not make this " decision " right until the moment of measurement . how will they " know " what the other electron picked ? strangely enough , they do , in fact , somehow " know . " addendum . certainly , as lubos points out in his comment , there is nothing actually physically paradoxical or contradictory in entanglement , and it is just a form of correlation , but i personally think it is fair to call it a " strange " or " unintuitive " form of correlation . important disclaimer i put a lot of things in quotes because i wanted to convey the intuition behind the strangeness of entanglement by using analogies ; these descriptions are not meant to be scientifically precise . in particular , any anthropomorphisations of electrons should be taken with a large grain of conceptual salt .
it is not a problem because two of the eight equations are constraints and they are not quite independent from the remaining six . the constraint equations are the scalar ones , $$ {\rm div}\ , \ , \vec d = \rho , \qquad {\rm div}\ , \ , \vec b = 0$$ imagine $\vec d=\epsilon_0\vec e$ and $\vec b=\mu_0\vec h$ everywhere for the sake of simplicity . if these equations are satisfied in the initial state , they will immediately be satisfied at all times . that is because the time derivatives of these non-dynamical equations ( "non-dynamical " means that they are not designed to determine time derivatives of fields themselves ; they do not really contain any time derivatives ) may be calculated from the remaining 6 equations . just apply ${\rm div}$ on the remaining 6 component equations , $$ {\rm curl}\ , \ , \vec e+ \frac{\partial\vec b}{\partial t} = 0 , \qquad {\rm curl}\ , \ , \vec h- \frac{\partial\vec d}{\partial t} = \vec j . $$ when you apply ${\rm div}$ , the curl terms disappear because ${\rm div}\ , \ , {\rm curl} \ , \ , \vec v\equiv 0$ is an identity and you get $$\frac{\partial ( {\rm div}\ , \ , \vec b ) }{\partial t} =0 , \qquad \frac{\partial ( {\rm div}\ , \ , \vec d ) }{\partial t} =-{\rm div}\ , \ , \vec j . $$ the first equation implies that ${\rm div}\ , \ , \vec b$ remains zero if it were zero in the initial state . the second equation may be rewritten using the continuity equation for $\vec j$ , $$ \frac{\partial \rho}{\partial t}+{\rm div}\ , \ , \vec j = 0$$ ( i.e. . we are assuming this holds for the sources ) to get $$ \frac{\partial ( {\rm div}\ , \ , \vec d-\rho ) }{\partial t} = 0 $$ so ${\rm div}\ , \ , \vec d-\rho$ also remains zero at all times if it is zero in the initial state . let me mention that among the 6+2 component maxwell 's equations , 4 of them , those involving $\vec e , \vec b$ , may be solved by writing $\vec e , \vec b$ in terms of four components $\phi , \vec a$ . in this language , we are left with the remaining 4 maxwell 's equations only . however , only 3 of them are really independent at each time , as shown above . that is also ok because the four components of $\phi , \vec a$ are not quite determined : one of these components ( or one function ) may be changed by the 1-parameter $u ( 1 ) $ gauge invariance .
the reflection could viewed as a two step process . the incident wave causes the electrons in the silver to vibrate like in an antenna . though by vibrating they also emit the same light . so it is the electrons at the surface of the silver that reflects the incoming wave . as you mentioned the wave is part electric and part magnetic , but these cannot be taken apart they are each others cause and effect without one the other would not be there either , and therefore it must reflect both parts . that silver ( and all metals ) do not distort is due to that they are also very good conductors . this prevents the electromagnetic waves from entering the object . the boundary conditions which must hold ( from being an conductor ) result in the perfect reflection and that the resulting angle is equal to the incident angle . similar boundary conditions are there for non-conducting materials like plastic and glass . these similar conditions result in reflection of glass and the shine/reflection on other smooth surfaces ( though there can be other causes too ) . also snell 's law would follow from these boundary conditions . in contrast to conducting materials it is possible to for electromagnetic waves to enter non-conduction objects . as a consequence part of the incoming wave is transmitted into the material . the propagation or dampening of the wave through the material is largely depended on the properties of the material . some material like glass hardly dampen the wave and you can see through them , while others like most plastics dampen them and thus are opaque .
charge does curve spacetime . the metric for a charged black hole is different to an uncharged black hole . charged ( non-spinning ) black holes are described by the reissnernordstrm metric . this has some fascinating features , including acting as a portal to other universes , though sadly these are unlikely to be physically relevant . there is some discussion of this in the answers to the question do objects have energy because of their charge ? , though it is not a duplicate . anything that appears in the stress-energy tensor will curve spacetime . spin also has an effect , though i have to confess i am out of my comfort zone here . to take spin into account we have to extend gr to einstein-cartan theory . however on the large scale the net spin is effectively zero , and we would not expect spin to have any significant effect until we get down to quantum length scales .
the concave mirror does not necessary leads to an inverted image . it depends how far you are from the mirror . when you are placed between the focal plane and mirror 's surface ( see fig . 1 ) you see a non-inverted image . when you are exactly at the focal plane you see nothing ( fig . 2 ) . and when you are located behind the focal plane , that is usual case , you see an inversion ( fig . 3 , 4 ) . changing mirrors shape can be considered in this context as an equivalent moving of the object relative to the focal plane . when the object is crossing the focal plane , the image transoms from a very small to a very large abruptly . but you are not able to notice this abruptness since it is very hard to recognize the image approaching the focal plane from the both sides . notice that changing the curvature of the mirror is characterized by some critical radius when one observe this effect and this is not the case when the mirror is flat . when the mirror becomes flat the image changes smoothly changing just a zoom .
what i did wrong here was switching order of integration . let 's integrate from $0$ to $\beta'$ , preserving the variable $\beta=\frac s w$ i introduced in the op , and priming it to avoid confusion with integration variable . the rhs integral will then look as $$\int_0^{\beta'}\text{d}\beta\int_0^{\beta^2}\frac{\chi' ( x ) \text{d}x}{\sqrt{\beta^2-x}}=\mathcal i . $$ its domain of integration looks like : here we integrate over $x$ from $0$ to the curve , then over $\beta$ from $0$ to the top horizontal line . so , to switch order of integration , we should integrate from the curve to the top over $\beta$ , then over $x$ from left to right . so , we get : $$\mathcal i=\int_0^{x ( \beta' ) = ( \beta' ) ^2}\text{d}x\int_{\beta ( x ) =\sqrt x}^{\beta'}\frac{\chi' ( x ) \text{d}x}{\sqrt{\beta^2-x}}=\\ =\int_0^{ ( \beta' ) ^2}\text{d}x\chi' ( x ) \left . \left ( \cosh^{-1}\frac \beta {\sqrt x}\right ) \right|_\sqrt x^{\beta'}=\\ =\int_0^{ ( \beta' ) ^2}\text{d}x\chi' ( x ) \cosh^{-1}\frac{\beta'}{\sqrt x} . $$ after substitution of the previously introduced variables , the result in landau and lifshitz is easy to get .
which particular observation , made us think that it could be the other way around retrograde motion must be a prime candidate . as seen from earth against star background , mars occasionally slows down and goes backwards . our moon does not . it probably became clear to people constructing orreries that heliocentric models were enormously simpler and more convincing . they also tied in with simple inverse square laws of gravitation and planetary motion . the discovery by galileo galilei of jupiter 's moons also provided firm evidence of the existence of heavenly objects that , perversely , did not orbit the earth . photo : thomas bresson ( galileo probably did not have a nikon / mobile phone handy ) luckily , he had available a corner of a napkin , a goose and some soot ( or equivalents )
there are two separate issues here :  " they seem to have a lot of flare coming out of the planet like an x where the planet is at the centre . " this is normal for all newtonian reflectors , so much so that most of us do not even notice them . they are diffraction spikes caused by the spider which supports your diagonal mirror . they are inherent in the telescope 's design , and the only way to eliminate them is by going to a different design : a refractor or a schmidt or maksutov design . but , do not worry about them , as they actually have absolutely no effect on the main image , except for a slight loss of contrast .  " they seem slightly blurred ( almost impossible to get a sharp focus ) " this is a different issue , and springs from two different causes . first , the telescope may not be collimated properly . collimation is the process of lining up the various optical elements in a telescope . collimation is a normal part of the maintenance of all telescopes , and is not difficult if approached systematically . the process is described well here : http://www.backyardastronomy.com/backyard_astronomy/chapter_15__polar_alignment,_collimation,_cleaning_and_testing_of_telescopes_files/appendix%20b-collimation.pdf the second factor is in the images themselves . at present , both venus and mars are far away and , as a result , show very small disks , 22 and 13 arc seconds respectively , as compared to jupiter , 34 arc seconds . this has two effects . first , any detail on these planets is vey much smaller in size than the detail on jupiter . in fact , no detail is ever visible on venus except for its phase ( slightly more than half ) . on mars you may see a tiny polar cap and a faint smudge or two on the rest of the disk . secondly , their small size makes them more subject to the degradation of " seeing , " turbulence in the earth 's atmosphere . as a result of these two factors , seeing detail on mars is a challenge even in much larger telescopes than yours ! finally , there is the question of your eyepieces . planetary observing is probably the most challenging aspect of visual astronomy , because the planets are so small . the planets require much more magnification than any other object you are likely to look at , except for very close double stars . your eyepieces give you 26x and 65x , whereas serious planetary observing begins at around 150x , and is mostly carried out at 200x to 300x . the short focal length of your telescope , while providing fine wide-field views of deep sky objects , is not well suited for high magnifications . the shortest focal length eyepiece commonly used , 4mm , will only get you 162x , which is only barely adequate for planetary observing . even then , the small aperture of your telescope may preclude using this high a magnification . do not waste time or money on filters . they serve no useful purpose on a telescope as small as yours . you would be better off spending the money on better quality eyepieces than those which came with your telescope .
if you are not well-acquainted with special relativity , there is no way to truly explain this phenomenon . the best one could do is give you rules steeped in esoteric ideas like " electromagnetic field " and " lorentz invariance . " of course , this is not what you are after , and rightly so , since physics should never be about accepting rules handed down from on high without justification . the fact is , magnetism is nothing more than electrostatics combined with special relativity . unfortunately , you will not find many books explaining this - either the authors mistakenly believe maxwell 's equations have no justification and must be accepted on faith , or they are too mired in their own esoteric notation to pause to consider what it is they are saying . the only book i know of that treats the topic correctly is purcell 's electricity and magnetism , which was recently re-released in a third edition . ( the second edition works just fine if you can find a copy . ) a brief , heuristic outline of the idea is as follows . suppose there is a line of positive charges moving along the $z$-axis in the positive direction - a current . consider a positive charge $q$ located at $ ( x , y , z ) = ( 1,0,0 ) $ , moving in the negative $z$-direction . we can see that there will be some electrostatic force on $q$ due to all those charges . but let 's try something crazy - let 's slip into $q$ 's frame of reference . after all , the laws of physics had better hold for all points of view . clearly the charges constituting the current will be moving faster in this frame . but that does not do much , since after all the coulomb force clearly does not care about the velocity of the charges , only on their separation . but special relativity tells us something else . it says the current charges will appear closer together . if they were spaced apart by intervals $\delta z$ in the original frame , then in this new frame they will have a spacing $\delta z \sqrt{1-v^2/c^2}$ , where $v$ is $q$ 's speed in the original frame . this is the famous length contraction predicted by special relativity . if the current charges appear closer together , then clearly $q$ will feel a larger electrostatic force from the $z$-axis as a whole . it will experience an additional force in the positive $x$-direction , away from the axis , over and above what we would have predicted from just sitting in the lab frame . basically , coulomb 's law is the only force law acting on a charge , but only the charge 's rest frame is valid for using this law to determine what force the charge feels . rather than constantly transforming back and forth between frames , we invent the magnetic field as a mathematical device that accomplishes the same thing . if defined properly , it will entirely account for this anomalous force seemingly experienced by the charge when we are observing it not in its own rest frame . in the example i just went through , the right-hand rule tells you we should ascribe a magnetic field to the current circling around the $z$-axis such that it is pointing in the positive $y$-direction at the location of $q$ . the velocity of the charge is in the negative $z$-direction , and so $q \vec{v} \times \vec{b}$ points in the positive $x$-direction , just as we learned from changing reference frames .
as kyle kanos stated , the convention is largely due to ben franklin 's work on electricity , and his theory of a " single-fluid model " . physicists ( such as they were in the 1700s ) followed the " corpuscularian " atomic model that we call the " plum pudding " model today ; the particles that formed matter were spaced relatively evenly apart and evenly distributed , forming a loose " fluid " , and different types of matter had different types of particles and at different densities , which gave them their properties such as weight , phase , malleability , and yes , electrical conductivity . most people who worked with electricity thought that different types of particles carried opposing charges , and electrical current thus involved a " two-fluid transfer" ; positively-charged particles moved from surplus to deficit , and same with negatively-charged particles , creating an equilibrium . ol ' ben thought a little differently ; he saw a propagation of current from only one end of a connection between charges , through experiments conducted with a leyden jar to store a static charge gathered either with friction or from lightning , and then discharging it through matter with different electrical resistance , including , as the stories go , his party guests . the people at the far end of a chain of people holding hands reacted last ( and least ) to the discharge from a leyden jar , instead of those in the middle as would be expected from the prevalent two-fluid model . so , he proposed that while oppositely-charged potentials did seek to equalize , only one " charge carrier " was an actual moving " fluid " in this circumstance , and the other potential was simply caused by a deficit of this fluid charge carrier , creating a relative surplus of a " fixed " charge carrier distributed evenly through the material . it was a genius proposal at the time . there was only one problem ; he could not devise an experiment or instrument that could detect which of the opposing charges , positive or negative , was being transported fluidly by the charge carrier . he had to guess , and as luck would have it he guessed wrong ; he documented the fluid charge carrier as being " positive " , describing the charge as flowing from positive to negative . since lightning is often observed traveling from the clouds to the ground , the negative pole of a dc circuit came to be called the " ground side " for this reason . in the following years , hans christian oersted discovered , purely by accident while demonstrating resistive heating of a metal coil , that the wire through which a current passed caused the needle of a nearby compass to deflect from true north , thereby demonstrating electromagnetism . his work was duplicated , leading to experiments and mathematical models predicting the force vectors of magnetic fields based on the direction and strength of current in the wire ( biot-savart law , maxwell 's equations and the lorentz force law ) . around the same time , sir william crookes was experimenting with vacuum tubes , passing a strong electrical charge through them , causing the glass of the tube ( used as the insulator and for visibility ) to phosphoresce . it was not until 1897 that j.j. thomson , while experimenting with these " cathode ray tubes " , connected the dots ; using a thin sheet of mica placed within the tube , he showed , based on the " shadow " the cross forms on the wall of the tube , that what is passing through the tube is some sort of particle , which is being reflected by the mica . he then showed that these particles had to be negatively charged , because they were reflected by the mica sheet on the side of the negative pole , and were affected by the magnetic field of a permanent magnet as a negatively-charged particle would be , in accordance with the biot-savart law and maxwell 's equations . he reasoned that this negative charge carrier must be of lower mass than any other particle that makes up matter , otherwise some other particle would be moving to carry the charge ( creating a more detectable change in mass ; in fact this difference is detectable , but the ratio between charge carrier masses is over 1800:1 ) . he named this particle the " electron " and asserted that it , and not any positive charge carrier , was most directly responsible for electromagnetism . however , it was far too late . the convention that current flows from the positive to the negative of charged dipoles had been in common use for almost 150 years , and a lot of the work that ended up disproving it was , ironically , documented using it . nowadays , we recognize that the movement of electrons is from the negative charge to the positive , but we diagram the movement of current in the opposite direction , as the propagation of a " positive charge " , even though we now know better . that is why the positive lead or terminal is the red one , even though the source of the electrical charge is actually the negative " ground " , while the actual ground in a lightning strike has a relative positive potential . it is only with alternating current that we regain some sanity , because in practical terms no one wire is " positive " or " negative" ; the black or red wire ( in u.s. home wiring codes ) is the " hot " side , on which the potential change is being actively driven by the generator ; the white wire is the " neutral " or undriven side ; and bare ( or green ) is a safety " ground" ; u.s. codes typically state that the ground wire goes to the same terminator on the service panel as the neutral instead of directly to the actual earth , but either way it provides an easier path for a short circuit than through a person .
i know that this does not directly answer your question about purcell 's reasoning ( see addendum i wrote after reading purcell 's argument ) , but here 's how a uniqueness proof would go . suppose that two fields $\mathbf b_1$ and $\mathbf b_2$ both satisfy the magnetostatics equations $$ \nabla\times\mathbf b_i = \mu_0\mathbf j , \qquad \nabla\cdot\mathbf b_i = 0 , \qquad i = 1,2 $$ let $$ \mathbf d= \mathbf b_2 - \mathbf b_1 $$ then the curl and divergence conditions give $$ \nabla\times \mathbf d= 0 , \qquad \nabla\cdot\mathbf d = 0 $$ now your question reduces to what we can say about the vector field $\mathbf d$ . take the curl of both sides of the first equation and use the following vector calculus identity : $$ ( \nabla\times ( \nabla\times \mathbf d ) ) _i = \partial_i ( \nabla\cdot \mathbf d ) - \nabla^2d_i $$ along with the zero divergence condition , to obtain $$ \nabla^2 d_i = 0 $$ in other words , each component $d_i$ satisfies laplace 's equation . now , if we assume that the magnetic field components vanish at infinity ( as would be the case for a bounded current distribution ) then we recall that the only solution to laplace 's equation that vanishes at infinity is the zero solution . this gives $\mathbf d = 0$ and thus $\mathbf b_1 = \mathbf b_2$ . addendum . after having read purcell 's argument , i am fairly confident that he also is making the physical assumption that bounded current distributions produce fields that vanish at infinity . such a boundary condition cannot be derived from the equations themselves . purcell is rather vague when he specifies this boundary condition ; what he says before the argument is " we do not consider sources that are infinitely remote and infinitely strong . "
i am not someone who inderstands the physics well , so i am not 100% sure of this . comments on this will be greatly appreciated . ( update : this approach seems to be correct ) f=dp/dt works as well as $v^2/r$ . as long as you want to spin it at a constant velocity , your lorentz factor will be constant , and since $p=\gamma m_0v$ , your force will become $m_0\gamma d\vec{v}/dt$ . then you can solve it normally using vectors ( exact same proof a the classical one for cpf ) . your end result will be $\gamma m_0 v^2/r$ . when dealing with accelerations due to forces , again use $f=d ( \gamma m_0 v ) /dt$ . if $m_0$ is constant , then we get $f=\frac{m_0a}{ ( 1-\frac{v^2}{c^2} ) ^\frac{3}{2}}=\gamma^3m_0v$ . since we still have $\gamma$ , the accelerations will be very much different . so if a proton has twice the energy of another , the proton has twice the $\gamma$ ( from $e=\gamma m_0c^2$ ) . so , the acceleration will be eight times less .
yes , in some cases , for some applications , depending on your definition of " beating the diffraction limit . " in the normal sense of imaging resolution , you can not beat the diffraction limit with a normal free-space optical system . but in lithography , what you need is not really the same as resolution in the imaging sense . because you are using the light to etch a material to a certain depth , all you need is a sufficient contrast between the bright part of your focal spot and the rest of it . if that contrast is sufficient , then your etched pattern may only become deep over a region smaller than the typical spot size estimate given by $2.44 \lambda \mathcal{f}/\#$ . to illustrate , think of the focal spot due to a perfect lens system with a circular aperture . it takes the form ( ignoring scale factors and such ) of : $$ \left ( j_1 ( r ) \over r \right ) ^2$$ where $j_1$ is the bessel function of the first kind . optics people call this a " jinc " function , due to similarity to the sinc ( ) function . it is squared to give intensity . an image of this spot looks like this : $$ \left ( j_1 ( 2\pi r ) \over r \right ) ^2 $$ an annular aperture can be considered the difference of two circular apertures of slightly different size , so by superposition the field at the focus of a system with an annular aperture will be the difference of two jink functions with different sizes . such a function looks like this : $$ \left ( \frac{j_1 ( 2\pi r ) }{r} - \frac{j_1 ( 2.1\pi r ) }{r}\right ) ^2 $$ you can see that the spot is considerably more spread out in general , which would be a problem for an imaging system ; but the central bright spot is significantly smaller . if the intensity is chosen properly , the outer rings will not affect the lithographic material , while the small central lobe will . this is even more effective if the material is chosen such that it does not absorb light except for rare two-photon events . in this case the intensity dependance of the absorption goes like the square of the intensity , making the contrast between the central spot and the lobes even better . i am not sure how common this is in industry yet .
alright i will throw my hat into the ring with an answer . the idea that it is an unsolved problem is totally bogus . when you start to fall to one side or another if you turn the wheel slightly in the direction you are falling the bicycle starts to follow a curved path . there is a force due to friction that deflects the rider 's path into a curve : the frictional force pushes against the base of the bike and acts to stand the bike back up vertically . the inward directed friction is what is providing the centripetal acceleration . this answer becomes obvious when you ask yourself questions like : what happens if you are riding quickly when you cross a long patch of ice ? the answer is that you slip and fall and no gyroscopic force of the wheels or anything else prevents this . what happens if you ride quickly through loose sand ? the answer is that if you try to turn ( lean ) even a little bit too much the sand flows under the bike wheel and does not provide enough friction to keep the bike up . you fall over . if you want to lean more in a turn do you need to go faster or slower ? the answer is faster and it is because you need a greater inward directed force to make up for the lower normal force . the faster an object is going when it is deflected the more force is needed to deflect it which means there is more force available to fight gravity . while searching for an image for my answer i found this source which explains it the same way : http://electron6.phys.utk.edu/101/ch4/dynamic_stability.htm so , when you are going slowly on a bicycle the curved path you had need to follow in order to provide enough friction to stay upright is too tight for the bicycle to turn . without enough friction to provide the counterbalancing centripetal acceleration the bike will fall . this is also why when you are riding very slowly and trying to stay stable and upright you end up taking very sharp weaving exaggerated turns but when you are traveling quickly you go in almost a straight line . only the very sharp turns provide enough frictional force to stand the rider back up .
assuming a perfectly spherically symmetric earth , you would just float there . remember that the total force acting on you is given by the vector sum of all partial forces . now if you are at the center of a spherically symmetric mass distribution , each force that would pull you in a certain direction is exactly cancelled by one pulling you in the other direction . hence , you would not move anywhere .
as you can see a ) and c ) are different because in a ) the spring is part of the rope and in c ) it is part of the support . the tension in a ) and b ) is the same as the systems are in static equilibrium .
the speed of light is always constant . the speed does not change , but the distance it travels might change . for example the speed of light " decreases " with about 35% when traveling in optical fiber . this happens because light does not go straight trough the fiber , it bounces in all directions . it is like putting a lot of mirrors . so the distance that we measure ( the length of the optical fiber ) is not the same is the distance light travels so if you would have vacuum then the distance light travels would be the same as the distance you " can " measure , but if you do not have vacuum light will bounce from one atom to another . the photon will be absorbed by the atom , the atom 's energy will rise for a few moments , and then it will fall back again to his original state , releasing the photon . this creates first of all a different wave length ( a different color ) and a longer path for light to travel . because the photon does not go in a straight line from one atom to another .
this question can be answered in the simple framework of non-relativistic quantum mechanics . the electron 's electromagnetic charge 's density and current  which are the source of the classical electromagnetic field  are given by the electron 's probability density and current distributions $$\rho ( t , x ) =\psi^* ( t , x ) \ , \psi ( t , x ) \ , $$ $$j ( t , x ) \propto \psi^* ( t , x ) \ , \nabla\psi ( t , x ) -\psi ( t , x ) \ , \nabla\psi^* ( t , x ) \ , . $$ as in a stationary state $\psi ( t , x ) =e^{-i\omega\ , t}\ , \phi ( x ) $ , neither the density nor the current depend on time and therefore they do not emit electromagnetic energy , according to maxwell equations with $\rho$ and $j$ as sources . however , when one takes into account the quantum nature of the electromagnetic field , the probability of radiating a photon ( quantum of the electromagnetic field ) by an atom in a stationary state is different from zero due to the phenomenon of spontaneous emission .
another way of thinking that might be helpful to you is to take heed that $c$ is not primarily the speed of light . it comes indirectly to mean the observed speed by any observer of any massless particle , and because , as far as we know , light is massless , it comes indirectly to mean the speed of light . but , in its most fundamental form , $c$ is only a parameter that happens to have the dimensions of speed . it does not primarily refer to a speed : here 's how we go about defining it . think about the intuitive galilean addition of velocities . the combination law is linear . so , assuming a linear combination law , there are some basic symmetries and characterisics of this everyday law you might like to think about . the following might look a bit daunting at first but it really is intuitive and we are not talking about at first anything that gainsays everyday galilean relativity , so i would urge you to think about applying these ideas to the simple problem where we have three frames : $f_1$ , the street , $f_2$ a bus driving along the street and $f_3$ a person walking down the aisle of the moving bus . in the following , let us call the shift from one frame to another , uniformly relatively moving frame a boost : ( linearity ) if i transform from frame $f_1$ to a frame $f_2$ moving at a constant speed $v_{1,2}$ in some direction then my distance and time co-ordinates $ ( x , t ) $ are transformed by some $2\times2$ matrix $t ( v_{1,2} ) $ , i.e. $x=\left ( \begin{array}{c}x\\t\end{array}\right ) \mapsto t ( v_{1,2} ) x$ ; ( transitivity and associativity ) : if i then transform to a third frame $f_3$ , one moving at velocity $v_{2,3}$ in the same ( original ) direction relative to the transformed frame $f_2$ ( using the matrix $t ( v_{2,3} ) $ , this has to be equivalent to a single transformation $t ( v_{1,3} ) $ from the first to the third frame with some relative velocity $v_{1,3}$ . or , with our " boost " word : a boost combined with another boost in the same direction is still the same as a boost with some relative speed : transformations in the same direction do not change their character by dent of their being composed of boosts or indeed how ( our of an infinite number of ways ) they might be composed of boosts . if i walk at some speed along a bus itself moving along the road , then my motion should be describable as my moving along the road at some relative speed , forgetting about the bus ; ( symmetry of description ) in particular , if frame $f_3$ is moving relative to frame $f_2$ at velocity $-v$ , then frames $f_1$ and $f_3$ have to be the same and $t ( v ) t ( -v ) = i$ ( here $i$ = identity transformation - my running away from you at velocity $v$ should seem the same as your running away from me at the same speed in the opposite direction ) . this symmetry arises from a basic " homogeneity " ( space and time are the " same " in some sense everywhere ) and the copernican notion that there is no special frame . think carefully about these and you will see that the galillean transformation fulfills all these intuitive symmetries . now for the killer question : do the conditions 1 through 3 fully define a galilean transformation ? or , more mundanely , what is the most general form of the matrix $t ( v ) $ that fulfils conditions 1 through 3 ? it turns out that , not only does the galilean law $v_{1,2}+v_{2,3} = v_{1,3}$ fulfill all the above axioms , but there are a whole family of possible transformations , each parameterised by a parameter $c$ , with the galilean law being the transformation law we get as $c\to\infty$ . such laws are the lorentz transformations . see the section " from group postulates " in the " derivations of the lorentz transformations " wikipedia page . notice how one has not assumed that $v_{1,2}+v_{2,3} = v_{1,3}$ , aside from in the special case of when $v_{1,2} = -v_{2,3}$ . it seems likely that ignatowsky ( see wikipedia page ) was one of the first to understand that one could derive relativity from these assumptions alone in 1911 , although einstein actually mentions the group structure of the lorentz transformations in his famous 1905 paper " on the electrodynamics of moving bodies " . so imagine we had carefully reviewed galilean relativity as above but we did not know anything about special relativity . this might well have been how science might have progressed in the late nineteenth century were it not for the michelson-morley experiment . we would now understand that our everyday galilean looking laws might actually arise from a universe wherein we have this weird $c$ parameter that is not infinite but simply very big : this would still be consistent with our everyday addition of velocity laws with a big enough $c$ . at this point , we had only know the form of the lorentz transformation and that there were a $c$ parameter ( maybe infinite ) with dimensions of velocity , so we had like to come up with some experiment to measure whether our universe had a finite $c$ value . it would not be apparent straight away that this velocity parameter were the velocity of anything in particular or indeed whether it even could be the velocity of anything . but , now we say to ourselves , what if something were going at this velocity relative to us ? a simple study of the lorentz transformation would show us that : the speed of this body $c$ would be measured to be the same in all inertial reference frames . moreover , so as to enforce this invariance of $c$ , there would be a peculiar addition rule for velocities not quite the same as the parallelogram rule ; no material object can go faster than $c$ and indeed something can travel at speed $c$ only if it has a rest mass of nought . so now the michelson moreley experiment can be thought of not so much as validating relativity , but rather of showing that light , if made of particles , must be made of massless particles . the michelson morely experiment found something whose speed transforms precisely as foreseen by the general lorentz transformation with a finite $c$ , so it would then be a strong hunch ( not a proof ) that our universe indeed has a finite $c$ and that light is something that travels at this speed . in this context , a positive result of the michelson morley experiment ( i.e. one showing a dependence of lightspeed on frame ) could be thought of either as ( i ) detecting an aether ( medium for light ) but equally well ( ii ) it could be thought of saying that there is no aether but that the light particle has a small mass . neither result would gainsay our newly found relativity laws . of course , many other experiments have since confirmed everything that a relativity grounded on a finite $c$ with $c$ set to the speed of light would foretell , so its quite reasonable to speak of $c$ as the speed of light in relativity . but i hope i have shown that this is not its primary meaning . footnote : unfortunately these ideas do not quite work in more than one dimension . in one dimension , two boosts indeed compose to a boost , but a sequence of boosts in different directions in general compose to one boost together with a rotation . this rotation is called thompson precession . so we speak of the lorentz group as the smallest group of all transformations that can be gotten from a sequence of rotations and boosts , but there is no multidimensional group of boosts , only the " one parameter " one dimensional group of boosts .
first , it is important to properly understand the equations of rotational motion . rather than $f = ma , $ the operative equation of motion is $\tau = i \alpha$ , where $\tau$ is the torque , $i$ is the moment of inertia and $\alpha$ is the angular acceleration . this problem also requires you to know the definition of the radius of gyration in the form of $r_g \equiv \sqrt\frac{i}{m}$ . with a proper understanding of the definitions of $\tau$ and $\alpha$ , you then have all the information that you need to solve the problem . edit : the above answer referred solely to the rotational acceleration of the disk . the translational acceleration of the center of mass of the disk must be worked out separately using $f = ma$ .
you are very nearly there . you are correct to say that $power = fv$ , and that the velocity at the moment the train reaches the top of the slope is given by $v_{top} = 350/ ( 6 + 150gsin2 ) $ so the force at the top of the slope is $f_{top} = 1000 ( 6 + 150gsin2 ) $ . but the acceleration is the net force divided by the mass , and the net force is $f_{top}$ minus the 6kn frictional force i.e. $$ a = \frac{f_{top} - 6000}{m} = \frac {1000 ( 6 + 150gsin2 ) - 6000}{150000} = g sin2$$ which using $g = 9.81m/sec^2$ i get as 0.342 .
yes , by examining the statistical distribution of distances between molecules and angles separating two nearby about a third molecule . in general , correlations of 2nd and higher order of the positions of molecules relative to each other . for a gas , there are few molecules close together , but some due to molecules colliding and almost colliding . at far distances , it'll be more or less uniform . there would be nothing of interest in angular correlations . for a liquid , there would be none closer than about the size of a molecule , but at that distance many . there would be mushy peaks in the distribution of distances , and just uniform mush beyond a few molecule-sizes away . there would be strong angular correlations as nearby molecules try to pack tightly , with fleeting gatherings of several molecules in an approximate crystal , but always jiggling making the angular distribution mushy . for a crystalline solid , every molecule near or far is at a precise distance , and at precise angles with respect to any reference directions . the distance and angular distributions would look like bunches of dirac functions , slightly smoothed out due to thermal motion , phonons , impurites and so on . radial distributions explained by professors at oxford , with plots comparison of radial distribution functions , with plots
simply because our final goal is a set of laws of physics that describes any part of the universe equally well . let 's say a physicist jumped into a black hole and saw that the interior of the black hole was composed entirely of john lennon clones . his last thoughts before getting spaghettified would be " why ? " . from his perspective , physics is incomplete . sure , we probably can not use it to predict anything -- but modern physics is much less about predictions and much more about having a beautiful , mathematically rigorous model of the universe . mathematical models with discontinuities usually are not " beautiful " , and john lennon in black holes counts as a discontinuity if we take general relativity as our mathematical model of the universe . which would mean that we will eventually have to replace our model ( which is why knowing as much as we can about the inside is important ) . besides , if our current theories partially fail inside a black hole , we need to patch that up .
in general , a dynamical equation of motion or evolution equation is a ( hyperbolic ) second order in time differential equation . they determine the evolution of the system . $\partial_{\mu}f^{i\mu}$ is a dynamical equation . however , a constraint is a condition that must be verified at every time and , in particular , the initial conditions have to verify the constraints . since equations of motion are of order two in time , constraints have to be at most order one . the gauss law $\partial_{\mu}f^{0\mu}$ is a constraint because it only involves a first derivative in time in configuration space , i.e. , when $\bf e$ it is expressed in function of $a_0$ and $\bf a$ . furthermore , the gauss law is the generator of gauge transformations . in the quantum theory , only states which are annihilated by the gauss law are physical states . both dynamical equations and constraints may be called equations of motion or euler-lagrange equations of a given action functional . or , one may keep the term equation of motion for dynamical equations . it is a matter of semantic . the important distinction is between constraints and evolution equations . conservation laws follow mainly from symmetries and from noether theorem . often but not always , equations of motion follow from conservation laws . whether one considerers one more fundamental is a matter of personal taste . dirac equation relates several components of a dirac spinor . each component verifies the klein-gordon equation which is an evolution equation of order two .
i have not read that book , but i did read feynman 's discussion of ( sounds like ) exactly the same thing . easy : tell the aliens how to build a telescope , then describe the configuration of some galaxies near them . ok ok , but suppose we rule that out : we can not see any objects in common . easy : send them circularly-polarized radio waves ( thanks @anonymous coward ) . ok ok , let 's say our radio waves must be linearly polarized . easy : tell them to look at almost any phenomenon related to the weak force , for example the beta-decay of cobalt-60 in a magnetic field . but then there is one more catch--what if the aliens are made of antimatter and they actually were watching the beta-decay of antimatter-cobalt-60 ? in feynman 's discussion ( if i recall correctly ) , that is where it ends : there is no way to be really sure that the aliens understand right and left correctly , because they may be made of antimatter . but since 1964 , when cp-violation was observed , we can even eliminate that possibility : we tell the aliens how to watch kaons decay ( for example ) and then the aliens can figure out whether they are made of ( what we call ) matter or antimatter , and therefore they can figure out which way is left and right without any more ambiguity . so i guess that aspect is a slight update from pre-1964 descriptions . watching atoms decay in a magnetic field is a pretty simple thing to do by the standards of particle-physics experiments . i do not know of any parity-violating experiments that are much simpler than that . it just has to involve the weak force .
in free space , this would be impossible , but there is friction , so if you impulse the chair forward , by moving yourself a little back ( by conservation of momentum the chair must move forward ) , and then you get back to you initial position , the chair will move back again , but there is friction and energy is being lost during the process so the distances will be smaller and the chair , at the end , will be a little bit moved forward .
you are on the right track . hints : recall that under a diffeomorphism $f$ , the tensor transformation law tells us that the metric transforms as $g\to g_f$ where \begin{align} ( g_f ) _{\mu\nu} ( f ( x ) ) = g_{\alpha\beta} ( x ) \partial_\mu ( f^{-1} ) ^\alpha ( f ( x ) ) \partial_\nu ( f^{-1} ) ^\beta ( f ( x ) ) \end{align} which , sending $x\to f^{-1} ( x ) $ can be re-written as \begin{align} ( g_f ) _{\mu\nu} ( x ) = g_{\alpha\beta} ( f^{-1} ( x ) ) \partial_\mu ( f^{-1} ) ^\alpha ( x ) \partial_\nu ( f^{-1} ) ^\beta ( x ) . \tag{$\star$} \end{align} consider an infinitesimal diffeomorphism ( physics speak for a smooth , one-parameter family of diffeomorphisms that starts at the identity ) \begin{align} f ( x ) =x - \xi ( x ) + o ( \xi^2 ) \end{align} notice that \begin{align} f^{-1} ( x ) = x+\xi ( x ) +o ( \xi^2 ) \end{align} plug this into the right hand side of $ ( \star ) $ and taylor expand about $\xi=0$ to first order . recall that $\delta g = g_f - g + o ( \xi^2 ) $ , and compare to the expression you wrote down . addendum . ( 2 april 2014 ) notice that the first transformation law i wrote down is a more mathematically explicit version of \begin{align} ( g' ) _{\mu\nu} ( x' ) = g_{\alpha\beta} ( x ) \frac{\partial x^\alpha}{\partial x'^\mu} ( x' ) \frac{\partial x^\beta}{\partial x'^\nu} ( x' ) \end{align} since if we write $x ' = f ( x ) $ then $x = f^{-1} ( x' ) $ so in particular \begin{align} \frac{\partial x^\alpha}{\partial x'^\mu} ( x' ) = \frac{\partial ( f^{-1} ) ^\alpha}{\partial x'^\mu} ( f ( x ) ) = \partial_\mu ( f^{-1} ) ^\alpha ( f ( x ) ) \end{align} where in the last equality , i have simply suppressed the prime in the derivative notation ; a derivative $\partial_0$ for example simply means " take the derivative with respect to the $0^\mathrm{th}$ argument of the function . " while we usually label the zeroth argument of $f^{-1}$ with the letter $x'^0$ because we are thinking of $f^{-1}$ as the transformation that maps us from the " primed " coordinates to the " unprimed " coordinates , but this is just a dummy label , and we do not strictly need it as long as the derivative tells us which argument of the function we are differentiating with respect to .
this creates a point of extremely focused energy at the middle point where the bubble collapses . in theory , this point focuses enough energy to trigger nuclear fusion . it is not currently accepted mainstream science to say that collapsing bubbles focus energy enough to cause nuclear fusion . temperatures over 10,000k can be acheived , but are still well below the millions of degrees needed for fusion . see the extensive review article single-bubble sonoluminescence for detailed information .
is that correct that there are no nuclear warheads in service made of u-235 , as plutonium ones are much smaller and much more efficient maybe . who knows precisely what people use . but publicly available information from us warheads is that they moved away from uranium to plutonium fission devices soon after world war ii . is that correct , that the only current military uses for 80%+ u-235 are naval nuclear reactors and i would replace ' naval reactors ' with ' any small yet powerful reactor` . in most cases that might come down to the same thing , but it also allows the use of them in space probes etc . in rare occurrences - case and/or x-ray " reflector " in tellerulam configuration as it is slightly better neutron breeder compared to more commonly used u-238 and allows to slightly reduce mass of " high-tech " plutonium charge at the same yield ? then you would replace the cheap , bountiful u238 with a more efficient but very expensive to separate u-235 ? and with a lot of u-235 , which would be sensitive to external neutron flux , thus forcing you to add extra shielding ( e . g with an extra layer of boron ) . i am not a bomb design expert , but thus sounds uneconomical . even if you save some plutonium .
my understanding of the question is that it is about minimizing the rate at which rain hits the car . that makes it different from this question , which assumes you want to minimize the total amount of water that hits you before you get to a certain destination . first let 's assume the rain is perpendicular to the road and the car is a sphere . then by the following argument , more rain hits the moving car . we have $\mathbf{v}_{cr}=\mathbf{v}_{ce}+\mathbf{v}_{er}$ , where $\mathbf{v}_{cr}$ is the car 's velocity relative to the rain , $\mathbf{v}_{ce}$ is the car 's velocity relative to the earth , and $\mathbf{v}_{er}$ is the earth 's velocity relative to the rain . let the car have cross-sectional area $a$ . in the rain 's frame of reference , the car is moving at $\mathbf{v}_{cr}$ , and in time $t$ it sweeps out a volume $v=at|\mathbf{v}_{cr}|$ . this volume is maximized by maximizing $|\mathbf{v}_{cr}|$ , and if $\mathbf{v}_{ce}$ is perpendicular to $\mathbf{v}_{er}$ , then this is always maximized by mazimizing $|\mathbf{v}_{ce}|$ . in reality , the car is not a sphere , so the cross-sectional area $a$ presented to the rain is a variable . in some cases , this could allow the car to hit less rain while moving . as an unrealistic example , suppose the car is a pancake tilted at an angle of 45 degrees , and the rain is falling at 10 km/hr . then the car can minimize how much rain hits it by driving at 10 km/hr . if the rain is not perpendicular to the road , but the car is a sphere , then $|\mathbf{v}_{cr}|$ may be minimized for some nonzero value of $\mathbf{v}_{ce}$ . these examples show that in general , the result depends on both the shape of the car and the angle of the rain with respect to the road .
lorentz came with a nice model for light matter interaction that describes dispersion quite effectively . if we assume that an electron oscillates around some equilibrium position and is driven by an external electric field $\mathbf{e}$ ( i.e. . , light ) , its movement can be described by the equation $$ m\frac{\mathrm{d}^2\mathbf{x}}{\mathrm{d}t^2}+m\gamma\frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}+k\mathbf{x} = e\mathbf{e} . $$ the first and third terms on the lhs describe a classical harmonic oscillator , the second term adds damping , and the rhs gives the driving force . if we assume that the incoming light is monochromatic , $\mathbf{e} = \mathbf{e}_0e^{-i\omega t}$ and we assume a similar response $\xi$ , we get $$ \xi = \frac{e}{m}\mathbf{e}_0\frac{e^{-i\omega t}}{\omega^2-\omega^2-i\gamma\omega} , $$ where $\omega^2 = k/m$ . now we can play with this a bit , using the fact that for dielectric polarization we have $\mathbf{p} = \epsilon_0\chi\mathbf{e} = ne\xi$ and for index of refraction we have $n^2 = 1+\chi$ to find out that $$ n^2 = 1+\frac{ne^2}{\epsilon_0 m}\frac{\omega^2-\omega^2+i\gamma\omega}{ ( \omega^2-\omega^2 ) ^2+\gamma^2\omega^2} . $$ clearly , the refractive index is frequency dependent . moreover , this dependence comes from the friction in the electron movement ; if we assumed that there is no damping of the electron movement , $\gamma = 0$ , there would be no frequency dependence . there is another possible approach to this , using impulse method , that assumes that the dielectric polarization is given by convolution $$ \mathbf{p} ( t ) = \epsilon_0\int_{-\infty}^t\chi ( t-t' ) \mathbf{e} ( t' ) \mathrm{d}t ' . $$ using fourier transform , we have $\mathbf{p} ( \omega ) = \epsilon_0\chi ( \omega ) \mathbf{e} ( \omega ) $ . if the susceptibility $\chi$ is given by a dirac-$\delta$-function , its fourier transform is constant and does not depend on frequency . in reality , however , the medium has a finite response time and the susceptibility has a finite width . therefore , its fourier transform is not a constant but depends on frequency .
this is very easy to understand why centeraltap transformer is needed in a full wave rectifier . let us assume that we have a simple transformer , and there are two diodes and the central wire coming out from the transformer is not present there which is obvious since we are not using centeraltap transformer . so now see the figure in first case let a be at lower potential i.e. negative and b be at higher potential i.e. positive . so due to induction 1 will become positive and 2 will become negative . now since 1 is at higher potential so d1 will be forward biased and current will flow through it but since 2 is at lower potential d2 will be reversed biased and no current will flow through it so the circuit will not work because in one case d2 will be reverse biased and in other d1 will be reverse biased . now if we take a centeraltap trnsformer as shown in figure there will be following sign convention in case 1 and just opposite in case 2 . so in case 1 as you can see d1 is forward biased so current will flow through d1 and r and will go to the central low potential point and the circuit will be completed for case 1 and in case 2 the current will flow through d2 and r and again will go to the centeral low potential point and the circuit will again be completed and it will rectifier the full wave that is it is negative as well as positive half cycle in a positive variable dc current . so that is the reason a center tap transformer is used in a full wave rectifier because you need to rectify full wave . hope it helped you .
the non-mathematical answer is that , in our universe , it turns out that velocities do not add by simple ( vector ) addition . in your two-cars-scenario , your measured speed from one car ( of the other car ) is only approximately 120 km/h , although it is a very good approximation . john rennie 's possible duplicate in the comments of the op gives the actual formula . when you get near the speed of light , the error in the " simple addition " method of summing velocities becomes large enough to be noticed . as to your second question , it does not matter how fast the train is going ( it'll have to be slightly less than light speed , but that is not important for our purposes ) , you can still walk forward . in the frame of the train ( which your traveler is in ) , the train is not moving at all . someone outside the train will measure the velocity of the person walking on the train to be less than the speed of light , no matter what , because the correct formula for measuring velocities applies equally well in this case .
the answer is no . the pole would bend/wobble and the effect at the other end would still be delayed . the reason is that the force which binds the atoms of the pole together - the electro-magnetic force - needs to be transmitted from one end of the pole to the other . the transmitter of the em-force is light , and thus the signal cannot travel faster than the speed of light ; instead the pole will bend , because the close end will have moved , and the far end will not yet have received intelligence of the move . edit : a simpler reason . in order to move the whole pole , you need to move every atom of the pole . you might like to think of atoms as next door neighbours if one of them decides to move , he sends out a messenger to all his closest neighbours telling them he is moving . then they all decide to move as well , so they each send out messengers to to their closest neighbours to let them know they are moving ; and so it continues , until the message to move has travelled all the way to the end . no atom will move until he has received the message to do so , and the message will not travel any faster than all the messengers can run ; and the messengers can not run faster than the speed of light . /b2s
assuming a typical computer with cpu processing power ~1 ghz . it means that it can generate output byte sequence at ~$10^9$ byte/s , which is about ~$10^{-13}$ j/k in terms of von neumann entropy . also , the power consumption of a typical cpu is ~100 w , which gives entropy ~0.3 j/k at room temperature . so the ( minimum s ) / ( actual s ) ~ $10^{-14}$ this calculation is not quite right because it is hard to determine what is the actual output of a computer . in most case , the previous output will be used as input later . the above calculation has also made the assumption that all output is continuously written in some external device . a better point of view is that each gates taking two inputs and one output , such as and , or , nand , . . . , must drop one bit to the surrounding as heat . this is the minimum energy $w$ required to process information in a classical computer . in this sense , we may define the efficiency as $e = w/q$ , where $q$ is the actual heat generation per second . the efficiency depends on how many such logical gates that will be used , but i guess it is less than thousand in a typical clock rate , so $e \approx 10^{-11}$ . it means that our computer is very low efficiency in terms of information processing , but probably good as a heater . this theoretical minimum energy requirement is also hard to verified by experiment because of the high accuracy required .
dear andrew , despite moshe is expectations , i fully agree with him , but let me say it differently . in qft , we are talking about " first quantization " - this is not yet a quantum field theory but either a classical field theory or quantum mechanics for 1 particle . those two have different interpretations - but a similar description . when it is " second-quantized " , we arrive to qft . feynman diagrams in qft may be derived from " sums over histories " of quantum fields in spacetime ; for example , the vertices come from the interaction terms in the lagrangian , and the propagators arise from wick contractions of quantum fields . this is the " second-quantized " interpretation of the feynman diagrams . there is also a first quantized interpretation . you may literally think that the propagators are amplitudes for an individual particle to get from $x$ to $y$ , and the vertices allow you to split or merge particles . you may think in terms of particles instead of fields . in qft , this is an awkward approach because most particles have spins and it is confusing to write a 1-particle schrdinger equation for a relativistic spin-one photon , for example . however , in string theory , spin is derived and the first-quantized interpretation is very natural . so the cylindrical world sheet describes the history of a closed string much like a world line describes the history of a particle . and it is enough to change the topology of the world sheet to get the interactions as well . so in string theory , one may produce the amplitudes " directly " from the first-quantized approach because the changed topology of the world sheet , which we sum over , knows all about multi-particle states and their interactions , too . we say that the interactions are already determined by the behavior of a single string . needless to say , like any feynman diagrams , these sums over topologies are just perturbative in their reach . now , you may also write down string theory as a string field theory , in terms of quantized string fields in spacetime . somewhat non-trivially , an appropriate interaction term - that " knows " about the merging and splitting of strings - may be constructed in terms of a " star-product " ( a generalization of noncommutative geometry ) . in this way , string theory becomes formally equivalent to a quantum field theory with infinitely many fields in spacetime - for every possible internal vibration of the string , there is one string field in spacetime . it used to be believed that this formalism would tell us much more than the perturbative expansions because , for example , lattice qcd in principle can be used to define the theory completely , beyond perturbative expansions . however , this belief has been showed largely untrue . at least so far . it is been shown that string field theory indeed offers an equivalent way to calculate all the amplitudes of perturbative string theory - especially for bosonic strings with external open strings ( closed strings are possible , and surely appear as internal resonances , but they are awkward to include directly as external states ; superstrings are probably possible but require a substantially heavier formalism ) . also , string field theory has been very useful to explicitly verify various conjectures about the tachyon potential in bosonic string theory ( or , equivalently , about the fate of unstable d-branes which emerge as classical solutions in string field theory ) . these investigations , started by ashoke sen , led to some nice mathematical identities that had to work - because string theory works in all legitimate descriptions - but that were still surprising from a mathematical viewpoint . but all the physical insights confirmed by string field theory had already been known from more direct calculations in string theory . so because string field theory is widely believed not to tell us anything really new about physics , only a dozen of string theorists in the world dedicate most of their time to string field theory . moshe is surely no exception in thinking that it is not too important to work on sft . still , it is conceivable that sometime in the future , a more universal definition of string theory will be a refinement of string field theory we know today . however , it is also possible that this will never occur because it is not true : string field theory seems too tightly connected with a particular spacetime and with particular objects ( strings ) while we know that the true string theory finds it much easier to switch to another spacetime and other objects by dualities . cheers lm
it is true that you can not talk about intensity without referring to some specific frequency range , but the reason is the definition of intensity itself , not the inherent limits of our measuring devices . intensity , also called irradiance , is defined as power per unit area . typically it takes into account radiation at all frequencies ( or wavelengths ) , but you can also talk about the intensity within a particular band like $400\text{ nm}-700\text{ nm}$ . if you wanted to create a graph of intensity versus frequency , you could certainly do so by dividing the frequency axis into bins and plotting one point for each bin representing the total intensity detected within that frequency range . obviously , the smaller you can make the bins , the more detailed information you get about the spectrum , but the intensity values become correspondingly smaller ( and harder to measure accurately ) as the bins get smaller . besides , if everyone did that , different plots from different experiments would be very difficult to compare unless they all decided on a standard bin size . in order to make data from different experiments comparable , you normalize the intensities by dividing each intensity measurement by the width of the bin . in the limit as the bin width goes to zero , this is just the spectral irradiance .
because of the rotation of the planet . e.g. our galaxy rotates so it has also a stubble elliptical appearance . it is the same with planets orbiting sun but on a larger scale . btw . if the planet did not rotate at all ( which is in reality impossible in space ) the rocks/ring material would be distributed " everywhere equally " . however , it is impossible in real space , where everything is rotating in some way because of other forces of larger objects and the material must come from somewhere to the planet etc .
there is a construction on finite-dimensional , real inner product spaces relating rank 2 tensors and linear operators that i think answers your question . the construction essentially shows how to obtain a rank two tensor from a linear operator . since $\mathbb r^3$ with the standard inner product is a real inner product space , and since the general construction is not so hard , we might as well be more general . inner product space construction . let a finite-dimensional real inner product space $ ( v , \langle\cdot , \cdot\rangle ) $ be given , then we claim that there is a way to associate a tensor of type $ ( 0,2 ) $ to every linear transformation on $v$ . in addition , if we let $t^k_l ( v ) $ denote the vector space of all tensors of type $ ( k , l ) $ on $v$ , then one has the following isomorphisms : \begin{align} t^2_0 ( v ) \cong t^1_1 ( v ) \cong t^0_2 ( v ) \end{align} in fact , in index notation , the isomorphisms written here just correspond to appropriately raising and lower indices . combining these observations , we see that each linear operator on $v$ gives rise to three tensors of types $ ( 2,0 ) $ , $ ( 1,1 ) $ , and $ ( 0,2 ) $ , and they are all essentially the same object . here 's the nitty gritty of how you associate a tensor of type $ ( 0,2 ) $ to a linear operator . for each linear operator $l$ on $v$ , we can define a mapping $t:v\times v\to \mathbb r$ as follows : \begin{align} t_l ( v , w ) = \langle v , l ( w ) \rangle \end{align} i claim that $t$ is a type $ ( 0,2 ) $ tensor . recall that a type $ ( k , l ) $ tensor $s$ is a multilinear function $s : ( v^* ) ^k\times v^l\to\mathbb r$ . therefore , it suffices to show that the function $t_l$ defined above is bilinear . linearity in its first argument follows immediately from linearity in the first argument of the inner product , while linearity in its second argument follows from linearity in the second argument of the inner product combined with linearity of $l$ . to see that this coincides with the index notation we often use as physicists , let $\{e_1 , e_2 , \dots , e_n\}$ denote an orthonormal basis for $v$ , then the components of the tensor $t_l$ are \begin{align} ( t_l ) _{ij} = t_l ( e_i , e_j ) = \langle e_i , t ( e_j ) \rangle = \langle e_i , l_{kj}e_k\rangle = l_{kj}\delta_{ik} = l_{ij} \end{align} we see that the components of the tensor that we defined correspond precisely to those of the linear transformation . contact with your notation . if we denote the linear operator that you refer to as the stress tensor with the letter $l$ , and if we define the corresponding bilinear map $t_l$ , as above , then using your notation for the vectors , the equation i used to define $t_l$ would be written as follows : \begin{align} t_l ( \mathbf m , \mathbf n ) = \mathbf m^t l ( \mathbf n ) \end{align} the right hand side is the same as $\langle \mathbf m , l ( \mathbf n ) \rangle$ since the standard inner product on $\mathbb r^3$ is simply given by \begin{align} \langle \mathbf v , \mathbf w\rangle = \mathbf v^t\mathbf w \end{align} incidentally , in light of this fact notice that transposition effectively associates a dual vector to each vector in $\mathbb r^3$ .
i am not sure how one can know that the half maximum corresponds to $kt/\epsilon \approx 1/3$ without resorting to the formula for the heat capacity . still , notice that there are only two energy scales , i.e. , $\epsilon$ and $kt$ , in the problem . then , whatever ( dimensionless ) number that determines whether the equipartition holds or fails has to be the ratio $x\equiv kt/\epsilon$ . the limits $x \rightarrow 0$ and $x\rightarrow \infty$ should correspond to the two cases , and $x$ would be of order one in the crossover region . this consideration at least lets you make an order-of-magnitude estimation of $\epsilon$ .
the relation $e=mc^2$ only works for particles at rest , which is evidently not the case for photons . in the general case , the relation is $$e^2=m^2c^4+p^2c^2$$ for a particle with momentum $p$ . ( note , though that the momentum is not necessarily $p=mv$ as in the newtonian case ! see for instance if photons have no mass , how can they have momentum ? ) for a massless particle , then , $e=pc$ .
the acceleration is a vector $\mathbf{g}$ throughout the motion , and $\mathbf{g}$ is always pointing downward . since you choose positive $x$ to be vertically downward , so $\mathbf{g}$ is along positive $x$ if we draw out the cartesian coordinate , then $\mathbf{g}$ must have positive value , $\mathbf{g}=g\ , \hat{\mathbf{x}}$ . if you choose vertically upward to be $x&gt ; 0$ , then acceleration $\mathbf{g}$ has negative sign , $\mathbf{g}=-g\ , \hat{\mathbf{x}}$ . it is just the matter how you choose the $x&gt ; 0 , y&gt ; 0$ directions . draw a diagram of $v$ , $g$ , force on the ball with $ ( x , y ) $ coordinates .
a unit length of a cable will have some resistance $r$ ; some conductance $g$ by which charge can leak through the dielectric ; some inductance $l$ ; and some capacitance $c$: these quantities combine to give the cable 's impedance at a given signal frequency $\omega$ , $$ z_0 = \sqrt\frac{r+j\omega l}{g+j\omega c} \approx \sqrt\frac lc $$ most common coaxial cables have impedances of 50 or 75 . other common transmission lines have comparable impedances ; 100 is common . note that if you are using the cable to carry dc or very low frequency currents , the impedance becomes $z \approx \sqrt{r/g}$ and the relationship between current and voltage in the cable will ( hopefully ) be dominated by the impedance of the load . you can find the fields in the dielectric from the capacitance and inductance . a cylindrical capacitor with inner , outer radius $a , b$ , permittivity $\epsilon$ , and voltage difference $v$ has charge per unit length $\lambda$ , where $$ \lambda = \frac{ 2\pi \epsilon v}{\ln ( b/a ) } $$ and field strength $$e = \frac{\lambda}{2\pi\epsilon r} = \frac{vc}{2\pi\epsilon r} . $$ remember that $c$ is the capacitance per unit length , for dimensional consistency . the magnetic field will simply be the biot-savart field due to the current on the central wire ( in the limit where the bend radius for the cable is long compared with the cable 's radius ) .
i am sorry to crush your joy , but conservation of energy is not violated . however , scientific american does rape science , that i can assure you . just ask yourself : how did they get the information in the first place ? did not that require energy ? how much ? you will see that the answer to these questions rules out any violation of the laws of thermodynamics . saying that energy conservation is violated in this case is about as deep as saying that it is violated in the extraction of energy from petroleum .
there is no such thing as anti-time in physics . ( neither is there anti-space or anti-gravity . ) antimatter is a very specific term , namely for particles that have the same properties but opposite quantum numbers ( charges ) as the " regular " particles . sometimes , antimatter refers only to molecules build from anti-particles . if you just slap " anti-" on a random physical concept , it does not have to have a meaning :- ) .
in principle , the gravitational potential energy should be included into total internal energy , but in practice , most often it is not . i know of two reasons . because for systems that are discussed in thermodynamics , it is believed that gravitational energy is negligible compared to electromagnetic potential energy of the constituting particles ; because it is difficult to include $1/r^2$ forces such as electromagnetic or gravitational force to calculations based on standard statistical physics in a unique and convincing way .
yes . higher frequencies are attenuated more over distance than lower frequencies are , which has a rounding effect on the square wave as the upper harmonics are reduced . reference do low frequency sounds really carry longer distances ?
time moving forward or backward is a human convention . by convention we count the flow of time in the positive axis , but you can chose a different convention . imagine a clock where the figures are interchanged , and the motion of the hands is 9-> 8-> 7-> 6-> 5 . . . nothing changes in nature . in particular the electro-chemical reactions in your brain/body do not care about the conventions that you use . your appeal to the milk in the cup seems to suggest that you are really asking about irreversibility . this is sometimes named the arrow of time , but the name is misleading because time flows in the same direction for a given process a--> b and for its inverse b--> a . as explained in quantum mechanics textbooks , the evolution of the universe is not deterministic except as approximation . universe is stochastic and " future is not given " .
the right way to think about this is that , over 5,730 years , each single carbon-14 atom has a 50% chance of decaying . since a typical sample has a huge number of atoms 1 , and since they decay more or less independently 2 , we can statistically say , with a very high accuracy , that after 5,730 years half of all the original carbon-14 atoms will have decayed , while the rest still remain . to answer your next natural question , no , this does not mean that the remaining carbon-14 atoms would be " just about to decay " . generally speaking , atomic nuclei do not have a memory 3 : as long as it has not decayed , a carbon-14 nucleus created yesterday is exactly identical to one created a year ago or 10,000 years ago or even a million years ago . all those nuclei , if they are still around today , have the same 50% probability of decaying within the next 5,730 years . if you like , you could imagine each carbon-14 nucleus repeatedly tossing a very biased imaginary coin very fast ( faster than we could possibly measure ) : on each toss , with a very , very tiny chance , the coin comes up heads and the nucleus decays ; otherwise , it comes up tails , and the nucleus stays together for now . over a period of , say , a second or a day , the odds of any of the coin tosses coming up heads are still tiny &mdash ; but , over 5,730 years , the many , many tiny odds gradually add up to a cumulative decay probability of about 50% . 1 a gram of carbon contains about 0.08 moles , or about 5 &times ; 10 22 atoms . in a typical natural sample , about one in a trillion ( 1 / 10 12 ) of these will be carbon-14 , giving us about 50 billion ( 5 &times ; 10 10 ) carbon-14 atoms in each gram of carbon . 2 induced radioactive decay does occur , most notably in fission chain reactions . carbon-14 , however , undergoes spontaneous &beta ; &minus ; decay , whose rate is not normally affected by external influences to any significant degree . 3 nuclear isomers and other excited nuclear states do exist , so it is not quite right to say that all nuclei of a given isotope are always identical . still , even these can , in practice , be effectively modeled as discrete states , with spontaneous transitions between different states occurring randomly with a fixed rate over time , just as nuclear decay events do .
i think the first few sentences of landau 's mechanics puts it elegantly : one of the fundamental concepts of mechanics is that of a particle . by this we mean a body whose dimensions may be neglected in describing its motion . the possibility of so doing depends , of course , on the conditions of the problem concerned . for example , the planets may be regarded as particles in considering their motion about the sun , but not in considering their rotation about their axes . this speaks to the notion newton and his contemporaries had in mind . newton in particular seems to be fairly vague about his notion of bodies that appears throughout the principia , but corollary iv gives a hint to his thinking : the common centre of gravity of two or more bodies does not alter its state of motion or rest by the actions of the bodies among themselves ; . . . and therefore the same law takes place in a system consisting of many bodies as in one single body , with regard to their persevering in their state of motion or of rest . for the progressive motion , whether of one single body , or of a whole system of bodies , is always to be estimated from the motion of the centre of gravity . but in general , it seems newton was not too explicit in some of the subtleties . according to essays in the history of mechanics - clifford truesdell , it was euler in his mechanica that pointed out some of the subtlety . . . . while newton has used the word ' body ' vaguely and in at least three different meanings , euler realized that the statements of newton are generally correct only when applied to masses concentrated at isolated points ; he introduced the precise concept of mass-point and this is the first treatise devoted expressly and exclusively to it . in particular , the relevant parts seem to be in volume i , chapter 2 of mechanica . here i try to pull out relevant parts , where he builds up the idea of replacing a body composed of many parts by a single point at its center of mass , and a set of forces of restitution imagined to be infinite elastic forces keeping the various parts of the body joined together . 174 . . . it appears possible to determine the motion of a small body , acted on by any kind of forces . . . . 175 . the force of restitution is that imaginary infinite force , which restores the separate parts of the body again to their previous state . . . . 177 . . . . the restoring force must be considered as provided by an infinite elastic force . . . 182 . therefore although the force of restitution is imaginary and only exists in the form of thoughts , yet the effect of this follows the real laws of motion . . . . 184 . . . . that bodies separated into any number of parts can be brought together at the common centre of gravity . so , not to put words in the mouths of the likes of newton and euler , but it would appear as though the talk of point masses going as far back as the beginning is much in line with the quote of landau 's i opened with . it was considered a useful simplification of problems where the extent of the body was small compared to its motion more generally . for gravity in particular , newton ( and euler ) spent great efforts demonstrating that for an extended body , one could replace its individual pieces with a point at the centre of mass , without affecting the analysis . and while at the time there was not a precise theory of any forces that could be keeping those bodies together , preventing their collapse under gravity alone , they had no difficulty imagining those forces as infinite elastic forces between the individual pieces .
i think you have your geometry wrong . you need to set up the speed in three dimensions : $\dot{\bf x} = ( \dot x , \dot y , \dot z ) $ . then $\dot{\bf x}^2 = \dot x^2 + \dot y^2 + \dot z^2$ . convert that into spherical coordinates $ ( r , \theta , \phi ) $ , with $\theta$ as the angle down from the z-axis towards the xy-plane , and $\phi$$ as the angle around the z-axis , starting from the x-axis . the z-axis passes through a diameter of the ring , and the ring rotates about the z-axis . $\omega = \mathrm{d}\phi/\mathrm{d}t$ ; the $\omega^2\sin^2 \theta$ term comes from the rotation of the hoop , and the $\dot\theta^2$ term comes from the motion of the particle along the hoop . after you plug in the definitions of $ ( r , \theta , \phi ) $ in terms of $ ( x , y , z ) $ and apply the restriction that $a^2 = x^2 + y^2 + z^2$ , the rest is algebra . most of the terms cancel and/or simplify down to those two terms .
try this experiment : take a small mirror ( so you can look over the top of it ) and put it vertically on a piece of paper . looking in the mirror in one position , try drawing a dot behind the mirror where you " see " the spot . because the mirror is small you should be able to see where your pen is pointing . now shift where you stand ( without moving mirror or paper ) and repeat . look behind the mirror . your two dots will be in the same place ( within the error of your accuracy ) . but if you draw a dot on the mirror , this dot will move with respect to the image as you move your head . this is called the parallax error . in the image below i try to illustrate this . two points of view : in both cases the virtual image is in the same place , but the smudge on the mirror aligns in only one position .
being a mathematician , i feel that i should point out a common misconception here . do not feel too bad , plenty of mathematicians ( including myself ) have fallen into this trap . basically , if you want to fit data to power law using least squares methods , then you should not fit a straight line in log log space . you should fit a power law using non-linear least squares to the original data in linear space without any kind of transformation . the basic intuition behind this is this . least squares method assume that the error distribution is normal meaning that there is one true ( unknown ) value which you are trying to measure and the measurements you are taking are above/below the true value staying close to the true value so the distribution is gaussian . the problem is that log is not a linear function so when you take log log of the data , numbers bigger than one are pushed together and numbers less than one are spread apart and namely , the error distribution is not normal any more in log log space . . . . meaning that least squares is not guaranteed to converge to the true values any more . in fact it has been proven that fitting in log-log space consistently gives you biased results . fitting a straight line in log-log was fashionable back in the day before ghz processors . now whatever you are using to do the computation , most likely has the ability to do non-linear least squares power law fit to the original data so that is the one you should do . since power-law is so prevalent in science , there are many packages and techniques for doing them efficiently , correctly , and fast . i refer you to these published results and a rant here . this is the my question which sparked this discussion . addendum : so by op 's request here is an actual example . the actual function is $y=f ( x ) =2x^{-4}=ax^b$ . i take a bunch of points from $x\in [ 1,2 ] $ and $x=10$ . note that $f ( 10 ) =0.0002$ and i change this $y$-value to $y=0.00002$ while keeping all others the same and then fitting them using least squares to estimate $a$ and $b$ . using non-linear least squares fit to the functional form $y=ax^b$ gives us \begin{eqnarray} a and = and 2\\ b and = and -4\\ r^2 and = and 0.9999 . \end{eqnarray} using linear least squares fit to the functional form $y=mx+n$ in log-log space gives us \begin{eqnarray} a and = and exp ( n ) = 2.46\\ b and = and m = -4.57\\ r^2 and = and 0.9831 . \end{eqnarray} the $95\%$ bounds for $b$ are $ ( -4.689 , -4.451 ) $ which does not even include the true value for $b$ . basically log pushes numbers bigger than one together and spreads apart numbers less than one . so in linear space changing $y=0.0001$ to $y=0.00001$ is inconsequential . the algorithm does not care too much about it . the change in the residual is very tiny . in log-log space the difference is rather huge ( an entire order of magnitude ) and the point has become an " outlier " from the nice linear trend . the change in residual is now large . since the algorithm is trying to minimize the sum of the squares , the algorithm can not ignore the deviation and must try to accommodate the outlier point so the line is skewed messing up the slope and the intercept . here is the matlab code and the graphs . you can easily reproduce this and play with it yourself . blue are the data points . black is the power fit in linear space . red is the fitted line from log-log least squares fitting . the top panel shows all three in linear space and the bottom panel shows all three in log-log space to emphasize the differences . the exponent is off by more than a half between the two methods . but how see how good the second fit it ( $r^2=0.98$ ) and the confidence interval is nowhere near the true value of $b$ . the algorithm is very confident that $b\neq-4$ . the same effect will work in reverse with large changes in large numbers . for example , changing a data point from 10000 to 20000 in linear space will cause a huge change but in log-log space the change is not a big deal at all so the algorithm will again give misleading results . second addendum : does non linear least squares manage a set of points with experimental/numerical errors ? i am assuming by manage you mean if nllsf will give us an unbiased estimate of the parameters . the answer to that is , least squares fitting works if and only if the errors are gaussian . so if the errors are random errors ( with the expected value and arithmetic mean being zero ) then least squares gives you an unbiased estimate of the true parameter values . if you have any type of systematic error then least squares is not guaranteed to work . we often assume/want-to-think that we only have random error unless there is good evidence to the contrary . and also does give the uncertainties in both a and b based on those parameters ? again assuming errors being normally distrubuted , there are standard methods for estimating the error deviation . the mean is already assumed to be zero . we estimate the standard error for each parameter and then we use it to compute the confidence intervals . so yes , if i only have experimental data and i have no idea what the real values of the parameters are , then i can still compute the confidence intervals . now an introductory bibliographic reference other than wikipedia would be very helpful . i do not know of a good stats/regression book i can recommend . i would say just pick your favorite intro to stats book . and there are those papers by clauset that i linked to both available on arxiv for free . tl ; dr physicist think everything is a power law . it is not . even if the data looks like its power law , it probably is not . if you do legitimately have a power law , do not fit a straight line in log-log space . just use nllsf to fit a power law on the original data . if there is still any doubt , i can easily make up a numerical example and show you concretely how different the two answers can be .
yes , what you want to do is to bring down $\psi_1 ( x ) $ and/or $\psi_2 ( x ) $ and you do that by multiplying them by independent currents and integrating over space-time , which then allows you to take the functional derivative w.r.t. either of the currents independently . this is completely analogous to $\int j ( x ) \psi ( x ) d^4 x$ , you now have a discrete index in addition to the continuous variable $x$ .
perfect state distinguishability and computational speedups with postselected closed timelike curves http://rd.springer.com/article/10.1007/s10701-011-9601-0 http://arxiv.org/pdf/1008.0433.pdf . . . . postselection of quantum teleportation in this fashion implies that an entangled state efectively creates a noiseless quantum channel into the past . . . . .
these guys seem to be doing just what you asked about . have a look here : http://www.liquidcrystaltechnologies.com/products/lcdshutters_2.htm
alternatively , and qualitatively , think about the components of velocity ( in the x y directions ) have changed . along the x axis , velocity has reduced , so the re has been a force in the -x direction . in the y axis , velocity has changed sign , so there must have been a force in the -y direction . hence the total force is down and to the left , ie quadrant iii .
" the gas would slowly radiate its heat through the glass to the ambient container housing the vacuum , and solar panels lining this surface could feasibly collect this energy . " no . if we assume the gas inside and the cells outside are both at temperature $t$ , then no ( thermal ) energy can be extracted . they will be in thermal equilibrium . whatever mechanism you want to come up with will be unable to extract energy . if we assume some pathway that interacts with the ir radiation to complete a chemical reaction , then since the material is at that temperature , the reaction is just as likely to run in reverse and return the same radiation to the interior .
yes , since it is the maximal set of compatible observables , it includes all observables for which $|a\rangle$ , $|b\rangle$ , $|c\rangle$ , etc . are the eigenvectors ( i will use the notation $|\psi_1\rangle$ , $|\psi_2\rangle$ , $|\psi_3\rangle$ etc instead ) . hence this includes the observable $d = \sum_k k |\psi_k\rangle \langle \psi_k|$ . however $d$ has a unique set of eigenvectors , and hence so does the any compatible set of observables which contains $d$ .
the group manifold $u ( 1 ) \times su ( 2 ) \times su ( 3 ) $ is $1+3+8=12$-dimensional , not 7-dimensional . you probably meant the dimension of a manifold that may have this group as its isometry group . but one may show that no such low-dimensional manifold can be interpreted as the extra dimensions of string theory to produce a realistic model . the oldest kaluza-klein theory had an extra circular dimension whose isometry is $u ( 1 ) $ . more generally , one may have more complicated manifolds with the isometry group $g$ ( isometry is a map of the manifold onto itself , or a diffeomorphism , that preserves the metric at each point , the true " symmetry " of the manifold ) . the isometry group always becomes the gauge group in the lower-dimensional description . these facts about the kaluza-klein theory are fully reproduced as a low-energy feature of some string compactifications . but as i have mentioned , realistic models with a large enough gauge group to include the standard model which would come purely from the original kaluza-klein mechanism do not exist in string theory . that is why realistic stringy vacua have a different origin of the gauge symmetries . for example , a stack of $n$ branes has a $u ( n ) $ gauge group which may become orthogonal or symplectic at the orientifold planes . m-theory and f-theory admit extra gauge groups from singularities . heterotic string theory or hoava-witten heterotic m-theory contain extra $e_8$ gauge groups , already in the maximum dimension ( or codimension one boundary , in the m-theory case ) that are simply inherited ( and partially broken ) in four dimensions . all these possibilities are related by various dualities ( non-obvious but exact equivalences ) in string theory . and in some sense , all of them are stringy generalizations of the original kaluza-klein theory . for example , the $e_8\times e_8$ or $so ( 32 ) $ gauge group of the heterotic string comes from 16 chiral " purely left-moving " spacetime dimensions in the spacetime where the heterotic string may live . in some stringy sense , the gauge group may still be interpreted as the isometry of the manifold . well , $u ( 1 ) ^{16}$ arises as the standard isometry of the torus and the remaining generators of the gauge group have a " stringy origin " which may be interpreted as the " string-generalized geometry " .
this question first posed to me by a friend of mine . for the subtleties involved , i love this question . :- ) the " flaw " is that you are not counting the dimension carefully . as other answers have pointed out , $\delta$-functions are not valid $\mathcal{l}^2 ( \mathbb{r} ) $ functions , so we need to define a kosher function which gives the $\delta$-function as a limiting case . this is essentially done by considering a uv regulator for your wavefunctions in space . let 's solve the simpler " particle in a box " problem , on a lattice . the answer for the harmonic oscillator will conceptually be the same . also note that solving the problem on a lattice of size $a$ is akin to considering rectangular functions of width $a$ and unit area , as regulated versions of $\delta$-functions . the uv-cutoff ( smallest position resolution ) becomes the maximum momentum possible for the particle 's wavefunction and the ir-cutoff ( roughly max width of wavefunction which will correspond to the size of the box ) gives the minimum momentum quantum and hence the difference between levels . now you can see that the number of states ( finite ) is the same in position basis and momentum basis . the subtlety is when you take the limit of small lattice spacing . then the max momentum goes to " infinity " while the position resolution goes to zero -- but the position basis states are still countable ! in the harmonic oscillator case , the spread of the ground state ( maximum spread ) should correspond to the momentum quantum i.e. the lattice size in momentum space . the physical intuition when we consider the set of possible wavefunctions , we need them to be reasonably behaved i.e. only a countable number of discontinuities . in effect , such functions have only a countable number of degrees of freedom ( unlike functions which can be very badly behaved ) . iirc , this is one of the necessary conditions for a function to be fourier transformable .
it will never reach such a high velocity . the moon is drifting further from the earth due to tidal acceleration . this process is , at the same time , slowing the rotation of the earth . once the earth 's rotational period matches the moon 's orbital period , the earth-moon system will be tidally locked to each other ( note : the moon is already tidally locked to the earth ) , and the acceleration will cease . to briefly explain the mechanism , the gravitational pull between the earth and the moon causes tidal " bulges " to extend out on both bodies ( just like the ocean tides , except that the entire surface moves slightly , not just the water ) . since the earth rotates faster than the moon completes one orbit , the bulge on the earth lies slightly ahead of the earth-moon line , because the earth is rotating so quickly . this bulge gravitationally pulls on the moon , speeding it up , while at the same time the moon pulls on the bulge , creating a torque on the earth and slowing down its rotation .
when you look at the dynamics in the rotating reference frame , there are 4 forces acting on the particle : the two gravitational pulls from the massive bodies , the centrifugal push away from the center of rotation ( located between the massive objects ) and the coriolis force . the first three forces depend on the position of the particle , and can be derived from a potential ( that also depends on the position ) , whose level curves are shown in the picture presented with the question . this potential has local maxima at l4 and l5 . the coriolis force depends on the velocity of the particle : it is perpendicular to it , contained in the plane of motion and proportional to the speed . it curves the motion of the particle to the right ( if the massive bodies and the reference system rotate counterclockwise , which is what you see in our solar system if you stand on the north pole of the earth ) . if a particle placed at l4 tries to leave the point with a mild speed , the coriolis force curves its trajectory . the trajectory is too curly to get anywhere . see the animation at http://demonstrations.wolfram.com/orbitsaroundthelagrangepointl4/. of course this does not prove that the particle will stay near l4 forever . i do not know a proof . i have seen some computations that show that the dynamical equation linearised at l4 is stable if the mass ratio of the massive objects is sufficiently large , but this also is not enough to prove stability in the non-linearised problem . i would be convinced that the equilibrium is stable if i were shown that there exists a conserved quantity ( depending on the position and speed ) that has a strict local extremum at that point of phase space ( position=l4 , speed=0 ) . the " energy " ( potential discussed above + kinetic energy measured in our non-inertial reference system ) is conserved , because the coriolis force is perpendicular to the trajectory , so it does not perform work ( in fact , in lagrangian mechanics it is derived from a potential that depends on the position and speed of the particle ) . but this quantity does not have an extremum at our equilibrium point , because the potential has a local maximum at l4 and the kinetic term is minimum when the speed is 0 . so i can not prove that the equilibrium is stable .
you have a differential equation that says \begin{equation} a ( x ) = -0.01*w = \frac{d w}{d t} \end{equation} what you did with the change of variables is correct , so $w$ cancels on either side . otherwise you have a first order differential equation to solve .
you are quite correct that motion is relative . i suspect that what is catching you out is that you think time must behave differently in frame a and frame b i.e. there is some absolute sense in which it is slower in the moving frame and faster in the stationary frame . suppose you are sitting in frame a watching me in frame b . as far as you are concerned you are stationary so time moves at the normal rate . however you see me moving , so you see time moving slowly for me . but now look at it from my point of view . as far as i am concerned i am the stationary one , and you are the one who is moving . that means my time moves at the normal rate and i see your time running slowly . this means the situation is symmetrical . each of us sees our time move as normal and the other person 's time slow down . actually it has to be this way because if the situation was not symmetrical there would be a way to assign absolute motion i.e. one frame would be different to the other . the third observer ( i assume you mean he sees us both moving with equal but opposite velocities ) see both your and my time slow down . likewise we both see his time slow down . in general you need to very careful with intuitive arguments about relativity , because it is unintuitive . the only safe way to work out what is going on is to use the lorentz tranformation ( http://en.wikipedia.org/wiki/lorentz_transformation ) to work out the difference between different frames .
firstly , let 's drop the factors of $\hbar$ for the sake of simplicity , as is often done ( see also emilio pisanty 's answer ) . by the vector coupling model for the combination of angular momenta , we know that the combination of two angular momentum quantum numbers $j_1$ and $j_2$ will yield allowed total angular momentum quantum numbers $j$ determined by $|j_1-j_2| \leq j \leq j_1+j_2$ . $^1$ in this case , we have $$|l_1-l_2| \leq l \leq l_1+l_2$$ $$|2-1| \leq l \leq 2+1$$ $$1 \leq l \leq 3$$ and similarly $$|s_1-s_2| \leq s \leq s_1+s_2$$ $$|1/2-1/2| \leq s \leq 1/2+1/2$$ $$0 \leq s \leq 1$$ $^1$ the way to derive this triangle inequality is by a process of elimination , starting from the condition that $m = m_1+m_2$ where the $m$ 's are the eigenvalues ( quantum numbers ) of the $z$-components of the respective angular momenta . this condition can , in turn , be derived by expressing the coupled basis as linear combinations of the product vector basis and letting $j_z = j_{z1}+j_{z2}$ act on both sides of the equality ( for more detail , see e.g. the wikipedia page on clebsch-gordan coefficients ) . now , we know that the largest possible value of $m$ is $j$ . we also know that the largest possible $m$ is obtained if ( and only if ) $m_1$ and $m_2$ are also as large as possible , i.e. $m_i = j_i , i=1,2$ . so we know that $m = j_1+j_2$ is the largest possible value for $m$ and therefore $j = j_1+j_2$ must exist . there are $2j+1$ values for $m$ associated with this value of $j$ , given by $m = -j , \cdots , j$ . the second largest value of $m$ is $j_1+j_2-1$ , which can be obtained in two ways . these are $ ( m_1 = j_1 , m_2 = j_2-1 ) $ and $ ( m_1 = j_1-1 , m_2 = j_2 ) $ . however , one of them corresponds to the previous value of $j$ ( $j_1+j_2$ ) . this still means we have one possibility left to get $m = j_1+j_2-1$ which does not correspond to an earlier value of $j$ . therefore , $j = j_1+j_2-1$ must also exist . we continue this reasoning until we get to the smallest allowed value of $j$ . we can say that this value is reached after $n$ steps if $j_k-n = -j_k$ , where $j_k = \min{\{j_1 , j_2\}}$ . ( think about why this is true ) say $j_k = j_2$ , then $n = 2j_2$ and the smallest allowed value of $j$ , $j_{min}$ is found to be $$j_{min} = j_1+j_2-n = j_1-j_2 , $$ or in the general case ( taking into account the possibility of $j_k=j_1$ ) : $$j_{min} = |j_1-j_2| . $$ summarizing , we have found that all the allowed values for $j$ are : $$j_1+j_2 , j_1+j_2-1 , j_1+j_2-2 , \cdots , |j_1-j_2|$$ which is often written as $$|j_1-j_2| \leq j \leq j_1+j_2 . $$
arxiv:1305.6917 has a nice graph comparing different proton accelerators with respect to beam intensity ( in milliampere , $6.241\cdot10^{15}$ protons per second ) vs . beam energy ( i assume you mean man-made particle sources . . . ) : the 590 mev ( kinetic energy ) , 2.2 ma proton beam at psi is operational , i am not sure about the higher intensity ( but lower energy ) leda and iphi beams . also , the graph does not contain the proposed proton source for daealus . some of these proton beams are also used ( e . g . the psi beam ) to produce neutrons ( for example using a spallation target ) . since neutrons are neutral , there are not any neutron accelerators .
annihilation can happen when all the quantum numbers of two colliding particles add up to zero . it might be electron on positron , proton on antiproton , neutron on antineutron , quark on antiquark etc . the force responsible depends on the possible interactions of the annihilating particles . in the case of electron positron annihilation it is primarily the electromagnetic force that is involved and so one gets two photons as an output , usually , in order to conserve quantum numbers and momentum . ( a single photon would not conserve spin also as the spin of the electron positron system is even ) . an annihilation into four photons is very much suppressed by the 1/137 coupling constant entering each photon vertex . in the case of proton antiproton the main force is the strong force and the products are various hadrons , mesons which conserve quantum numbers , as it is the quarks and antiquarks that disappear and rearange into mesons . annihilation does not require the presence of other fields . pair production by a single photon needs an external field in order to conserve momentum as @karsusren states in his answer . the interaction is electromagnetic . one can think of this as photon photon scattering , where one of the photons is virtual and comes out of the field of the nucleus . gluons are not free so one cannot observe free creation of antiproton proton pairs , but the diagrams exist . here is an interesting measurement of off shell gammma gamma collisions , both photons off shell , generating a proton antiproton pair , which shows how far one can go with the concept of pair production and annihilation ,
free energy refers to the energy in a system that is free to do work i.e. the internal energy minus any energy that is unavailable to perform work . internal energy accounts for the total energy of the system . it is normally called the gibbs energy more recently , though at my uni it is often been refered to as the ' gibbs free energy ' . i believe the same is true of the helmhotz energy .
the question really boils down to the dynamics of event horizons when black holes merge . it turns out that there are some great simulations that explore these dynamics . if one scrolls down to the bottom of this black-holes . org page one can see a video of the merging of two different sized black holes . one can review the underlying paper and see that the actual development of the simulation was very extensive . the actual event horizons do move and oscillate , so the question is whether the spaceship itself has become some sort of physical element of the black hole after it has crossed the event horizon . since it is argued in most cases that space craft can cross the event horizon in large black holes without witnessing any sort of significant effect , although the spacecraft 's mass must be considered part of the black hole 's mass after crossing the event horizon , it still has some freedom of movement . we can see from the simulation that the geodesics that define the event horizon fluctuate when the holes merge . so if the geodesics fluctuate is it possible that the spacecraft would find itself on a geodesic that suddenly allows for an escape ? the answer should be no . the geodesics defining the horizon require trajectories with velocities greater than the speed of light . the spacecraft can not exceed the speed of light . so while the geodesic its on might distort during the merger of the black hole , it is the underlying space itself that is distorting , which is not going to impart some ability to defy local laws of physics to the space craft . as such , it will stay inside the blackhole horizon since its geodesic , while distorted , will still remain inside the horizon .
questions 1,2: an observable is an element that is obtained from experiments . you can take this as the definition of an observable . the fact that we make an operator and give it some properties does not change/influence the outcome of an experiment . it just so happens that the theory we have ascribes linear , hermitian operators to explain experiments . with this in mind , it is easy to say that not all linear , hermitian operators we cook up describe observables . question 3 initially , the classical-quantum correspondence was used , but people quickly realized that it was of limited use . the modern view is that nature can be described by group theory ( especially the poincare group ) and everything that is observed follows from there . with this in mind , you do not have to guess about the existence of the spin operator , it comes up naturally . what is more important though , is the representations of the operator . when you relate theory and experiments , remember that you are dealing with the representations of an operator . an operator cannot be measured and is useless by itself unless you specify the basis . question 4 i do not know the answer to this , but i can tell you that we never measure spin by itself , but the interaction of a spin with something else . why ? in my view , that is the definition of a measurement .
" realistic " black holes , by which i mean black holes of the sort for whose existence we have significant evidence , are almost certainly very close to electrically neutral , pretty much for the reasons you suggest . the two main types of black holes that seem to exist in nature are those of stellar-mass and supermassive ones at galactic centers . both of these almost certainly formed out of large amounts of matter that was , to an excellent approximation , electrically neutral . you could get small amounts of charge separation , but i do not think anyone expects that to occur at any measurable level . of course it is always possible that this is wrong : either the black holes we know about could be charged for some reason , or there could be whole other populations of charged black holes . so we will have to do observations and experiments to check . people are always trying to improve our observations of black holes , not specifically to check for charge but to measure other interesting things such as angular momentum . i would guess that , if these black holes had a large charge , such observations would reveal it , but i have never looked into details .
the situation depends on the specification of " system " and " environment " , and the detailed form of the interaction hamiltonian . clearly , if your environment has only a few degrees of freedom then the poincare recurrence time is finite , and the assertion that the coupling results in decoherence is false at certain points in time , when revivals are seen in the system coherences . usually , one is interested in models where the environment has many more degrees of freedom than the system , in which case the hypothesis holds for many physically reasonable system-environment interaction models ( see the answer by trimok ) . if the interaction couples each state of the system differently to many-body states of the environment hilbert space , over time one expects these environment states will evolve to become orthogonal due to the interaction , implying complete decoherence of the system . however , it is not difficult to find reasonable models where this is not true , for example two qubits interacting linearly with a common bosonic bath but not with each other . this situation arises quite naturally when considering , for example quantum dots interacting with lattice phonons or trapped atomic impurities immersed in a bec . the bath cutoff frequency $\omega_c$ and signal speed $c$ define a length scale $c/\omega_c$ . if the distance between qubits is much smaller than this length scale , the decoherence between certain states of the two-qubit system can be almost perfectly suppressed . this allows one to preserve entanglement in such decoherence-free subspaces . see palma et al . , proc . roy . soc . lond . a452 ( 1996 ) 567-584 .
in qm , a " wave " is not what we normally imagine : something that moves up and down and moves in one direction , like water . it is just a function that evolves with time and has a ( in general ) different value at every point in space . see this applet for some examples of atomic orbitals which are infact electron wavefunctions ( the applet actually shows the absolute value squared $|\psi|^2$ of the wavefunction ; or the probability density ) . the wave does not " exist " per se in physical space . it can be drawn ( superimposed ) on physical space , but that just means that it has a value at every point there . the wave associated with an electron shows the probability of finding it at a particular point in space . if an electron is moving , it will have a " hump " in its vicinity , which shows it is probability at every point in time . this hump will move just like the electron does . for more info on this ( though you may have read stuff like this before ) , see the " why do not they need to be close " section of this answer . when you observe the electron , you collapse the hump to a peak . this peak is still a wave , just narrowly confined so it looks like a particle . your issue is that you are trying to look at the " electron " and " wave " simultaneously . this is not exactly possible . the wave is the particle . you can look at it as if you exploded the electron into millions of fragments and spread it out over the hump . there is a fraction of an electron at every point . the fraction corresponds to the probability of finding it there . at this point , there is no electron-particle . so there is nothing that is " waving " . of course , we never see a fraction of an electron , so these fellows clump together the minute you try to make an observation . edit by op -- this is the section referenced above that i found most helpful quantum mechanics has a nice concept called wave particle duality . any particle can be expressed as a wave . in fact , both are equivalent . exactly what sort of wave is this ? its a probability wave . by this , i mean that it tracks probabilities . i will give an example . lets say you have a friend , a . now at this moment , you do not know where a is . he could be at home or at work . alternatively , he could be somewhere else , but with lesser probability . so , you draw a 3d graph . the x and y axes correspond to location ( so you can draw a map on the x-y plane ) , and the z axis corresponds to probability . your graph will be a smooth surface , that looks sort of like sand dunes in a desert . you will have " humps " or dunes at a 's home and at a 's workplace , as there is the maximum probability that he is there . you could have smaller humps on other places he frequents . there will be tiny , but finite probabilities , that he is elsewhere ( say , a different country ) . now , lets say you call him and ask him where he is . he says that he is on his way home from work . so , your graph will be reconfigured , so that it has " ridges " along all the roads he will most probably take . now , he calls you when he reaches home . now , since you know exactly where he is , there will be a " peak " with probability 1 at his house ( assuming his house is point-size , otherwise ther'll be a tall hump ) . five minutes later , you decide to redraw the graph . now you are almost certain that he is at home , but he may have gone out . he can not go far in 5 minutes , so you draw a hump centered at his house , with slopes outside . as time progresses , this hump will gradually flatten . so what have i described here ? it is a wavefunction , or the " wave " nature of a particle . the wavefunction can reconfigure and also " collapse " to a " peak " , depending on what data you receive . now , everything has a wavefunction . you , me , a house , and particles . you and me have a very restricted wavefunction ( due to tiny wavelength , but let 's not go into that ) , and we rarely ( read:never ) have to take wave nature into account at normal scales . but , for particles , wave nature becomes an integral part of their behavior . --manishearth feb 14 , 2012
regarding your second question , it is intimately linked to the feature of scale invariance of critical phase transitions . essentially what goes on is that a condensing vapour , within the coexistence region , forms droplets of some particular size ( more precisely , a distribution of sizes with some particular scale ) , which depends on the temperature . as you approach the critical point within the coexistence region , this characteristic size increases without bound , and the result is that you will have droplets of all possible sizes from the very smallest to the largest allowed by the container . this causes the blurring of the liquid/vapour meniscus . this phenomenon is of a particularly universal character , and is ubiquitous in nature . for an illustration in a weird place look up for instance the theory of percolation .
one needs to spend more energy to accelerate a faster object by $\delta v$ because of your derivation based on kinetic energies ( which go like the velocity squared , and that is more quickly increasing than proportionality $v$ ) or , equivalently , because the work per unit time ( power ) is $$ \frac{de}{dt} = \vec f\cdot \vec v$$ the force $\vec f = m\vec a$ is the same , regardless of the velocity , for a fixed acceleration . however , when the object is already faster , it travels a longer distance per unit time , and $de = \vec f\cdot d\vec x$ where $d\vec x = \vec v\cdot dt$ . because of the extra factor $\vec v$ in the displayed formula above , the energy spent per unit time grows with the velocity if the acceleration is constant . here , it may be useful to stress that the work/energy goes like $de=\vec f\cdot d\vec x$ and not , for example , $de\neq \vec f\cdot dt$ . it is not hard to see why the latter has to be wrong . it is , for example , because $dt$ is a scalar so its multiplication by $\vec f$ would yield a vector , not a scalar as required for its being $de$ . also , one knows $de=f\ , ds$ from the everyday life experience , e.g. from elevators . if you want to lift an elevator of some weight $f=mg$ , the work is proportional to the distance ( height difference ) and independent of time . it does not matter how quickly you do the work . ( using muscles , we may have the misleading impression that one needs to spend energy even for holding an object , but this useless work may be saved using a pedestal or counterweight , in the elevator example . ) in a later part of the answer , you may be implicitly referring to the galilean invariance of the laws of physics : they do not really change if you switch to a different inertial system , one that is moving by motion with the constant direction and velocity relatively to the original one . however , when we are switching to a different inertial system , the energy is not preserved . it is not hard to intuitively understand why the energy has to change if we switch to a different inertial system . after all , even for one object , the energy contains the kinetic energy that does depend on the velocity , so it must also depend on the frame because the velocity does depend on the frame . paradoxically enough , the explicit description what happens with energy is simpler in special relativity where the energy is simply mixing with the momentum if we switch to a different frame ; however , $e=mc^2$ must be included in the total energy . in non-relativistic physics , the transformation of the energy induced by a switch to a new inertial frame is a bit more awkward . when the energy is $e$ in one frame , then in the frame that is moving by velocity $\vec v$ relatively to the original one , the energy is $$ e ' = e + \vec p\cdot \vec v + \frac 12 m |\vec v|^2 $$ where $\vec p$ is the total momentum and $m$ is the total mass of everything . note that both $\vec p$ and $m$ are conserved so the conserved energy $e$ just gets mixed with two other additional conserved quantities under the galilean transformation ( switching to a differently moving coordinate system ) . in relativity , $e=mc^2$ so the conserved quantities $e , m$ are not independent . so if you consider an object that is already fast and you switch to its rest frame , the energy will be increasing very slowly for a constant acceleration . however , in the original frame , the energy has the extra term $\vec p\cdot \vec v$ which is increasing with $\vec p$ in a way that grows faster with $\vec v$ . it is this middle term that yields the same outcome as the derivations above : the power has to be larger for a fixed acceleration if the velocity is already higher . the term $m|\vec v|^2/2$ does not play a role here because we are considering fixed inertial frames only and $\vec v$ is , much like this whole term , constant . only $\vec p$ is changing as the object keeps on accelerating .
i am not sure where that picture is coming from , but it is misleading at best and here 's why . let 's say that the rocket expels some stuff ( like the flaming gases in the picture ) , then the force of the rocket on that stuff will be $-f$ , say . by newton 's third law , the force of that stuff on the rocket will be $f$ . now let 's consider the system consisting of the rocket plus the box . the net external force on this combined system is $f$ because there is nothing external to the system exerting a force on either the rocket or the box besides the gases . assuming the rocket and the box are in rigid contact , the acceleration of each object equals the acceleration of the whole system which is given by newton 's second law as $$ a = \frac{f}{m_\mathrm{rocket} + m_\mathrm{box}} $$ now consider the system consisting of only the box . the only external force on this system is the force $f$ of the rocket on the box , so that acceleration of the box must also satisfy $$ a = \frac{f}{m_\mathrm{box}} $$ combining these results gives $$ f = \frac{m_\mathrm{box}}{m_\mathrm{rocket}+m_\mathrm{box}}f $$ and therefore $$ f &lt ; f $$ in other words , the contact force between the rocket and the box is less than the contact force between the gaseous exhaust and the rocket !
quantum-mechanics predicts particle-like together with the wave-like behavior of quanta very well , there is no need for another theory . furthermore there can not be defined a new " class " of particles since there is no clear border between particles which show only classical " particle-behavior " ( see for example double slit experiemnt with fullerenes ) and if you like a name for this particles : call them quanta :- )
the central charge counts the number of degrees of freedom only for matter fields living on a flat manifold ( or supermanifold in the case of superstrings ) . an example where this counting argument fails for matter fields is the case of strings moving on a group manifold $g$ whose central charge is given by the gepner-witten formula : $c = \frac{k\mathrm{dim} ( g ) }{k+\kappa ( g ) }$ where $k$ is the level and $\kappa$ is the dual coxeter number . please see the following article by juoko mickelsson . one of the best ways to understand this fact ( and in addition the ghost sector central extension ) is to follow the bowick-rajeev approach described in a series of papers , please see for example the following scanned preprint . i will try to explain their apprach in a few words . bowick and rajeev use the geometric quantization approach . they show that the virasoro central charges are curvatures of line bundles over $diff ( s^1 ) /s^1$ called the vacuum bundles . bowick and rajeev quantize the space of loops living on the matter field manifold . this is an infinite dimensional kaehler manifold . one way to think about it is as a collection of the fourier modes of the string , the fourier modes corresponding to positive frequencies are the holomorphic coordinates and vice versa . in addition , in order to define an energy operator ( laplacian ) on this manifold one needs a metric ( this causes the distinction between the flat and curved metric cases where the dimension counting is valid or not . the reason that the counting argument works in the flat case is that the laplacian in this case has constant coefficients ) . the quantization of a given loop results fock space in which all the negative frequency modes are under the dirac sea . however this fock space is not invariant under a reparametrization of the loop . one can imagine that over each point of $diff ( s^1 ) /s^1$ , there is a fock space labeled by this point . this is the fock bundle whose collection of vacuum vectors is a line bundle called the vacuum bundle . bowick and rajeev proved that the central charge is exactly the curvature of this line bundle . the situation for the ghosts is different . please see the bowick-rajeev refence above . their contribution to the central charge is equal to the curvature of the canonical bundle . this bundle appears in geometric quantization due to the noninvariance of the path integral measure on $diff ( s^1 ) /s^1$ .
the answer is positive . this is due to the fact that the equations describing how currents generate the field are linear . the solution is obtained by a suitable inverse of the linear operator associating currents to fields . it is fundamental to observe that this inverse operator is linear because the boundary conditions satisfy the superposition principle ( this is not obvious but it is true in this case where the boundary conditions are those in the vacuum ) . therefore there is a relation like this $$\vec{b} ( \vec{x} ) = l_{\vec {x}} i$$ where $l_{\vec {x}}$ is a linear operator depending on the point $\vec{x}$ where the filed is evaluated and $i$ the constant current generating the field . you see that if $i$ is replaced by $ci$ the field becomes $c\vec{b} ( \vec{x} ) $ for every constant $c\in \mathbb r$ .
you can try reading zwiebach ' a first course in string theory ' which is roughly at your level right now . its very handwavy , but well thats the best you can hope for at this level . otherwise , to really learn the subject you will absolutely need 1 ) grad level gr 2 ) quantum ii , + 2 semester long courses in qft and then you can start thinking about it . personally i find the above level a little loose , so to make it more comprehensive and less opaque i would recommend in addition to the above , to have some experience in semiclassical gravity ( wald or birrel and davies ) , conformal field theory ( di francesco ) and supersymmetry ( weinberg or wess and bagger )
dear dissonance , as discussed e.g. in these two questions calculate the electric field of a moving infinite magnet , without boosting what&#39 ; s a good reference for the electrodynamics of moving media ? the laws of electrodynamics in moving materials may be a bit subtle ( but they may be determined ) . however , whenever the motion is uniform , it is straightforward to transform maxwell 's equations back to the rest frame of the moving objects . in particular , a conductor will naturally have $\vec e =0$ in its rest frame while $\vec b$ is arbitrary . however , these two propositions get modified in a frame that is moving because the values of $\vec e , \vec b$ have to be transformed and mixed into one another if one changes the inertial frame . see http://en.wikipedia.org/wiki/classical_electromagnetism_and_special_relativity#joules-bernoulli_equation_for_fields_and_forces approximately , neglecting the terms of order $ ( v/c ) ^2$ , we have $$ \vec e&#39 ; = \vec e + \vec v \times \vec b$$ $$ \vec b&#39 ; = \vec b - \frac{\vec v}{c^2} \times \vec e $$ note that even if $\vec e=0$ , it does not mean that the value $\vec e&#39 ; $ in the moving frame is zero . instead , it will be approximately $\vec v \times \vec b$ . in the case of faraday 's effect , this electric field will have all the usual consequences and it will move the electrons just like you expect , proving that changing magnetic fields do have an impact on the flows of electrons , whether you view the situation from the viewpoint of the wires or the magnets .
in addition to what dmckee said , another hint at ( "large" ) extra dimensions would be the detection of kaluza-klein particles at the lhc for example . kaluza-klein particles are in principle nothing but the known standard model particles which can propagate into the extra dimensions if these are large enough . it can be shown that the angular momentum in these extra dimensions is quantized . this leads to the effect that particles propagating into the extra dimensions would be observed as heavier versions of the known standard model particles due to the additional momentum in the otherwise not directly visible dimensions . the energy ( or mass squared ) spectrum of the corresponding expected particle tower would have a step size proportional to 1/r ( where r is the radius of the extra dimension ) . as prof . strassler explains here , to determine the shape and extent of such large extra dimensions it would be necessary to measure the whole mass spectrum using more than one kk particle . up to now no kk particles have shown up at the lhc so far ( which was run only at 7tev and now continues at 8 tev ) . but note that even if there could be such large extra dimensions leaving hints at themselves at the " lhc scale " ( up to 14 tev ) , this does not have to be the case for st to work ; the " large " extra dimensions are only a feature of certain ( phenomenologica ) models . . .
this is actually a common question . many websites have been setup to try to explain this . i like this one for instance . i shall attempt to do my own layman explanation . first of all , in order to have a black hole , you need to have a place for it to be in . since there was no such thing as a universe , there is not a place for the black hole to actually exist in . it is like asking , what is north of the north pole . there is no reference for an answer . secondly , gravity and other fundamental forces did not act the way we are used to them acting . all four fundamental forces were combined in one basic force . therefore there was no such thing as gravity to actually act on the mass as it existed . and then there is the problem that there was no actual mass . it was energy , which was creating the density of the universe . i know it is kind of counter-intuitive to how we are used to understanding these terms . actually , brian greene has a good explanation of all this in his book elegant universe . suffice it to say , at the start of the universe , the fundamental forces really acted very differently from how we see them now . as gravity was separating itself from the other forces , it actually had a repulsive effect as opposed to attraction . finally , the big bang was not really an explosion or a bang as one would think of it . it is actually the rapid expansion of space itself . instead of things themselves speeding away from each other in a fixed space , the motion is actually caused by the space between the objects themselves getting bigger . visualizing this is usually presented as a balloon expanding , and seeing how two dots on the surface of the balloon get further away from each other without actually moving on the surface . of course , it is harder to visualize this in three dimensions ( or actually four ) . while we are limited to the speed of light for any objects , the expansion of space itself is not limited by this ( brian greene 's book also has info on this ) . the bottom line is that anything that you think you know should probably be discarded when thinking about the big bang . the reason that there is " string theory " and many other things is that classical quantum physics and relativity physics break down at the plank epoch which is at the heart of the big bang . i hope that helps .
in the region between two spherical plates , we have , $$\begin{array}{l} \nabla \times e = 0\ , \ , \ , ( 1 ) \\ \nabla . \left ( {\varepsilon \left ( \theta \right ) e} \right ) = 0\ , \ , \ , ( 2 ) \end{array}$$ the first equation leads to the definition of scalar potential $\phi$ , i.e. $e = - \nabla \phi $ . therefore , from eq . ( 2 ) we have , $$\nabla . \left ( {\varepsilon \left ( \theta \right ) \nabla \phi } \right ) = \nabla \varepsilon \left ( \theta \right ) . \nabla \phi + \varepsilon \left ( \theta \right ) {\nabla ^2}\phi = 0 \ , \ , \ , ( 3 ) $$ since the problem is invariant under $\varphi$-rotation , $\phi=\phi ( r , \theta ) $ . using the separation of variable method , $\phi ( r , \theta ) =r ( r ) \theta ( \theta ) $ . by substituting this function into the eq . ( 3 ) , after some straightforward calculations ( in spherical coordinate system ) , one can easily obtain the following equation , $$-\frac{1}{{r ( r ) }}\frac{d}{{dr}}\left ( {{r^2}\frac{dr ( r ) }{dr}} \right ) = {\frac{1}{{\varepsilon \left ( \theta \right ) \theta \left ( \theta \right ) }}\frac{{d\varepsilon \left ( \theta \right ) }}{{d\theta }}\frac{{d\theta \left ( \theta \right ) }}{{d\theta }} + \frac{1}{{\sin \left ( \theta \right ) \theta \left ( \theta \right ) }}\frac{d}{{d\theta }}\left ( {\sin \left ( \theta \right ) \frac{{d\theta \left ( \theta \right ) }}{{d\theta }}} \right ) } $$ one side of this equation is r-dependent and the other side is $\theta$-dependent . therefore both side must be a constant value , namely $\lambda$ , $$\frac{d}{{dr}}\left ( {{r^2}\frac{dr ( r ) }{dr}} \right ) + \lambda r ( r ) = 0\ , \ , \ , ( 4 ) $$ $$\lambda = \frac{1}{{\varepsilon \left ( \theta \right ) \theta \left ( \theta \right ) }}\frac{{d\varepsilon \left ( \theta \right ) }}{{d\theta }}\frac{{d\theta \left ( \theta \right ) }}{{d\theta }} + \frac{1}{{\sin \left ( \theta \right ) \theta \left ( \theta \right ) }}\frac{d}{{d\theta }}\left ( {\sin \left ( \theta \right ) \frac{{d\theta \left ( \theta \right ) }}{{d\theta }}} \right ) \ , \ , \ , ( 5 ) $$ on the other hand , we know that a conductor has an equipotential surface , i.e. the functions $\phi ( r=r_1 , \theta ) $ and $\phi ( r=r_2 , \theta ) $ must be independent of $\theta$ . therefore , using $\phi ( r , \theta ) =r ( r ) \theta ( \theta ) $ , we conclude that $\theta ( \theta ) $ must be constant , namely one , and therefore $$\phi ( r , \theta ) =r ( r ) \theta ( \theta ) =r ( r ) $$ is independent of $\theta$ for all values of $r$ . this is the desired result . in addition , in this case , $\frac{d\theta ( \theta ) }{d\theta}=0$ , from eq . ( 5 ) , we find $\lambda=0$ , and therefore by using eq . ( 4 ) , we have $$\frac{d}{{dr}}\left ( {{r^2}\frac{{dr ( r ) }}{{dr}}} \right ) = 0$$ with the solution , $$r ( r ) = {c_1} + \frac{{{c_2}}}{r} \ , \ , \ , ( 6 ) $$ where , $c_1$ and $c_2$ can be determined from the boundary conditions . ( here the total charge at $r=r_1$ is $q$ and the total charge at $r=r_2$ is $-q$ ) . by setting the reference point at infinity , i.e. by assuming $\phi ( r=\infty ) =0$ , we find $c_1=0$ . for calculation of $c_2$ we use this fact that the total charge at $r=r_1$ is $q$ ( or , equivalently , the total charge at $r=r_2$ is $-q$ ) , $$\int {\sigma \left ( \theta \right ) da} = \int {\sigma \left ( \theta \right ) r_1^2\sin } \left ( \theta \right ) d\theta d\varphi = q \ , \ , \ , ( 7 ) $$ consider the general boundary condition $$\left ( {{\varepsilon _2}{e_2} - {\varepsilon _1}{e_1}} \right ) . {\hat n_{12}} = \sigma$$ where $\hat n_{12}$ is a unit vector from region 1 to region 2 . in this problem the region 1 is inside the spherical plate at $r=r_1$ and the region 2 is its outside surface . since the electric field inside a conductor is zero , we have $$\sigma ( \theta ) = {\left . {\varepsilon \left ( \theta \right ) {e_r}} \right|_{r = {r_1}}} = - \varepsilon \left ( \theta \right ) {\left . {\frac{{d\phi \left ( r \right ) }}{{dr}}} \right|_{r = {r_1}}} = - \varepsilon \left ( \theta \right ) {\left . {\frac{{dr\left ( r \right ) }}{{dr}}} \right|_{r = {r_1}}} = \frac{{{c_2}}}{{{r_1}^2}}\varepsilon \left ( \theta \right ) \ , \ , \ , ( 8 ) $$ substituting eq . ( 8 ) in eq . ( 7 ) , gives , $${c_2} = \frac{q}{{\int {\varepsilon \left ( \theta \right ) \sin } \left ( \theta \right ) d\theta d\varphi }} = \frac{q}{{2\pi \int_0^\pi {\varepsilon \left ( \theta \right ) \sin \left ( \theta \right ) d\theta } }}\ , \ , \ , ( 9 ) $$ which can be obtained easily . another solution : if , at beginning , we know that the electric potential is independent of $\theta$ , we can use the gauss 's law , $$\int {\varepsilon \left ( \theta \right ) e . \hat nda = q} \ , \ , \ , ( 10 ) $$ in this case since $\phi$ is independent of $\theta$ , $e = - \nabla \phi = - \frac{{d\phi ( r ) }}{{dr}}\hat r = e ( r ) \hat r$ is also independent of $\theta$ and radial . therefore , using eq . ( 10 ) , we have , $$e ( r ) {r^2}\int {\varepsilon \left ( \theta \right ) \sin \left ( \theta \right ) d\theta d\varphi = q} $$ and therefore ( see eq . ( 9 ) ) , $$e ( r ) = \frac{q}{{{r^2}\int {\varepsilon \left ( \theta \right ) \sin \left ( \theta \right ) d\theta d\varphi } }} = \frac{q}{{2\pi {r^2}\int_0^\pi {\varepsilon \left ( \theta \right ) \sin \left ( \theta \right ) d\theta } }} = \frac{{{c_2}}}{{{r^2}}}\ , \ , \ , ( 11 ) $$ integrating the differential equation $- \frac{{d\phi ( r ) }}{{dr}} = e ( r ) = \frac{{{c_2}}}{{{r^2}}}$ , one obtain , $$\phi ( r ) = {c_1} + \frac{{{c_2}}}{r}$$ which is exactly the previous result ( $c_1=0$ if we set the reference point at infinity ) .
the main point that you have to always keep in mind is that relevant/irrelevant coupling constants are defined with respect to a fixed point . the standard/naive power counting is done assuming that the fixed point controlling the rg flow is the gaussian . this is true for massless qed and $\phi^4$ theories in four dimensions in the infrared , and for qcd in the uv . however , this is not true in the opposite limits ! qed and $\phi^4$ do not have a uv fixed point , meaning the the theories are not asymptotically safe , and all ' irrelevant ' coupling constants grow in the uv . on the other hand , qcd flows to strong coupling in the ir , though it seems that it still flows toward another fixed point . all in all , what it means is that if the theory flows to a fixed point in the ir/uv , only the few relevant operators have to be fixed in order to describe all the physics at lower/higher energy . in the case of $\phi^4$ in $d=4$ , the mass is a relevant operator at the gaussian fixed point . it means that as one integrates more and more degrees of freedom , the mass drives the system away from the gaussian fixed point ( it introduces a length scale in the system ) . but in order for the system to approach the gaussian fixed point , the mass needs to be small ( otherwise , the system will go away from the gaussian fixed point before it even approaches it ) . this means that under the rg transformation , it will take a long time to have the mass grow up to the cut-off scale ( at which point the rg flow stops ) . the best reference i know that discusses the relationship between the perturbative rg ( as done in qed ) and the wilsonian point of view is given in this review on the ( non perturbative ) rg , and in particular the section 2.6 " perturbative renormalizability , rg ows , continuum limit , asymptotic freedom and all that " .
you can project a real image onto a screen or wall , and everybody in the room can look at it . a virtual image can only be seen by looking into the optics and can not be projected . as a concrete example , you can project a view of the other side of the room using a convex lens , and can not do so with a concave lens . i will steal some image from wikipedia to help here : first consider the line optics of real images ( from http://en.wikipedia.org/wiki/real_image ) : notice that the lines that converge to form the image point are all drawn solid . this means that there are actual rays , composed of photon originating at the source objects . if you put a screen in the focal plane , light reflected from the object will converge on the screen and you will get a luminous image ( as in a cinema or a overhead projector ) . next examine the situation for virtual images ( from http://en.wikipedia.org/wiki/virtual_image ) : notice here that the image is formed by a one or more dashed lines ( possibly with some solid lines ) . the dashed lines are draw off the back of solid lines and represent the apparent path of light rays from the image to the optical surface , but no light from the object ever moves along those paths . this light energy from the object is dispersed , not collected and can not be projected onto a screen . there is still a " image " there , because those dispersed rays all appear to be coming from the image . thus , a suitable detector ( like your eye ) can " see " the image , but it can not be projected onto a screen .
you do not add matter when you store information . unless you do so by plugging in a drive . additionally , since there are about the same amount of 1 's and 0 's running around in a computer when it is drive is empty as when it is full , and since the states of the electrons in the physical drive ( ie how info is stored ) makes a lack of information weigh the same as a quantity of information , the weight of a computer does not change . not in any figure-out-able way . but it does change between off and on states
using that ideal white source on the wiki link , that states 251 lm/w . the insolation level is $2.61kwh/m^2d = \frac{2610}{24}w/m^2$ which across $65cm^2 ( = 0.0065m^2 ) $ gives 0.7w . if everything were 100% efficient , then you had have $0.7 \times 251 lm = 178 lm$ now we derate on some maximum theoretical efficiencies . the biggest theoretical derating will be on the pv , and that will completely dwarf any loss on the maximum theoretical efficiency of a round trip into storage and back . for a single-junction n-p pv cell in unconcentrated sunlight , the maximum efficiency ( shockleyqueisser , doi:10.1063/1.1736034 ) is 30% , giving you about 53 lm . by layering multiple junctions , you could theoretically get 42% ( 2 junctions , 74 lm ) , 49% ( 3 junctions , 87 lm ) , tending to 68% ( $n\to\inf$ , 121 lm ) ( doi : 10.1088/0022-3727/13/5/018 ) now , if you are allowed to put a concentrating lens onto the $65 cm^2$ cell , so that the sun jar harvests light from a much larger area , then we can really go to town . from the second link , the maximum efficiencies for concentrating pv , as the number of junctions tends to infinity , is 86.8% . so then you have got to find out what the maximum concentration could be without the whole lot bursting into flames . . . that 86.8% is based on a concentration factor of 45 900 , so it is a pv cell made of pure unobtainium , and i will stop right there .
first , some terms : the surfaces you are drawing are called " wavefronts " , which are surfaces of constant phase . we usually refer to your |||| waves as " plane waves " , meaning that the wavefronts are nice planes . similarly , we call your ) ) ) ) waves " spherical waves " because the wavefronts are spheres . so those are the terms i will use . also , the basic ideas are exactly the same for sound and light , so i will try to answer both parts of your question at the same time . if the source of a spherical wave is really far away , the sphere is really large by the time it gets to you . but you will only be dealing with a small part of that sphere , so it looks pretty flat to you . that is , you can approximate the sphere as a plane ( in a small region ) . for example , you might imagine a star as a source of perfect spherical light waves -- the star is roughly spherical , so the waves can be spherical . ( a very crude model , but let 's just go with it . ) but that star is so far away that by the time the light reaches us , each sphere is enormous . and since our eyes or any telescope or whatever that we use will be tiny compared to that enormous distance , the sphere is basically flat to us ; the waves are basically plane waves . you can only have perfect plane waves if the source of the waves is infinite and acting in perfect harmony -- or if it is just infinitely far away . since we never really have infinite sources , we never really have plane waves . but some times we have a really large source , so we can approximate the waves as plane waves . so in the real world , you never have perfect plane waves or spherical waves , but they are frequently good approximations . and those approximations are easy to deal with ( for calculations ) , so we use them a lot . but in reality , every wave we get is imperfect , and some sort of wiggly wave front . it may be helpful for you to read about huygens ' principle .
actually , mass and charge are only superficially similar . yes , they both appear in inverse square force laws , namely newton 's law of gravitation and coulomb 's law of electrostatic force , but both of those are approximations . coulomb 's law ignores quantum effects , which is a very slight approximation , but newton 's law ignores all of relativity , which makes a huge difference under certain circumstances . the true underlying theories , quantum electrodynamics and general relativity , are almost completely different . now , to address your questions directly ( though admittedly , this would be a lot easier to explain with the math ) : why did higgs need to introduce concept of universe-wide higgs field to define mass based on interactions with it ? and , no body cared about charge of electron ( for example ) which is also basic attribute and constant ? think about this : in either newtonian gravity or general relativity , mass is a property that you just assume an object has . neither of those theories makes any attempt to explain where the mass of an object comes from ; the mass is just something you plug into the equation to calculate a trajectory or a force . the standard model is more ambitious than that , though : it wants to actually explain things , not just have them put into the theory by hand . it all starts with a principle called local gauge invariance . by going through the math , we find that the consequences of this principle correspond to many of the same properties we know particles to have . for example , one consequence of local gauge invariance is the fact that some particles have electric charge , and the existence of the electromagnetic force . another consequence is that particles have " color charge " which leads to the existence of the strong force . it predicts the existence of antiparticles and the correct conservation laws that govern which elementary particle reactions can and cannot occur in nature . but before the higgs mechanism was discovered , the one thing the standard model did not predict was mass . in fact , all the particles it predicted to exist , which in almost all other respects matched known particles exactly , would be massless ! sure , we could tweak the standard model to force the particles to have mass , but there was no particular reason to do that ( other than the fact that we know the particles have mass in real life ) . there was no simple principle that would require the theory to include mass the way local gauge invariance requires the theory to include electric charge , color charge , etc . what higgs and other scientists ( anderson , brout , englert , guralnik , hagen , higgs , and kibble ) discovered is that the principle of spontaneous symmetry breaking does exactly that : it enables , and in fact requires , the particles of the standard model to have mass . the neat thing is that it only does this in combination with local gauge invariance , but that is kind of beside the point here . the important thing is that when you add spontaneous symmetry breaking in to the standard model , you get particles with mass , where before you had massless particles . in order to add spontaneous symmetry breaking , you need to add a field whose symmetry can be broken . that is where the higgs field comes from .
entanglement is being presented as an " active link " only because most people - including authors of popular ( and sometimes even unpopular , using the very words of sidney coleman ) books and articles - do not understand quantum mechanics . and they do not understand quantum mechanics because they do not want to believe that it is fundamentally correct : they always want to imagine that there is some classical physics beneath all the observations . but there is none . you are absolutely correct that there is nothing active about the connection between the entangled particles . entanglement is just a correlation - one that can potentially affect all combinations of quantities ( that are expressed as operators , so the room for the size and types of correlations is greater than in classical physics ) . in all cases in the real world , however , the correlation between the particles originated from their common origin - some proximity that existed in the past . people often say that there is something " active " because they imagine that there exists a real process known as the " collapse of the wave function " . the measurement of one particle in the pair " causes " the wave function to collapse , which " actively " influences the other particle , too . the first observer who measures the first particle manages to " collapse " the other particle , too . this picture is , of course , flawed . the wave function is not a real wave . it is just a collection of numbers whose only ability is to predict the probability of a phenomenon that may happen at some point in the future . the wave function remembers all the correlations - because for every combination of measurements of the entangled particles , quantum mechanics predicts some probability . but all these probabilities exist a moment before the measurement , too . when things are measured , one of the outcomes is just realized . to simplify our reasoning , we may forget about the possibilities that will no longer happen because we already know what happened with the first particle . but this step , in which the original overall probabilities for the second particle were replaced by the conditional probabilities that take the known outcome involving the first particle into account , is just a change of our knowledge - not a remote influence of one particle on the other . no information may ever be answered faster than light using entangled particles . quantum field theory makes it easy to prove that the information cannot spread over spacelike separations - faster than light . an important fact in this reasoning is that the results of the correlated measurements are still random - we can not force the other particle to be measured " up " or " down " ( and transmit information in this way ) because we do not have this control even over our own particle ( not even in principle : there are no hidden variables , the outcome is genuinely random according to the qm-predicted probabilities ) . i recommend late sidney coleman 's excellent lecture quantum mechanics in your face who discussed this and other conceptual issues of quantum mechanics and the question why people keep on saying silly things about it : http://motls.blogspot.com/2010/11/sidney-coleman-quantum-mechanics-in.html
i think there is no general answer to that . just try and include all important symmetry directions you can think of . for example , in the case of the brillouin zone shown in your left picture , how about the path k'-k-gamma-k'-m-k-gamma-m ? ( i know , the path gamma-k now is included twice , but i could not think of any other possibility . )
zwiebach explains the inequality $p^{\pm}&gt ; 0$ e.g. in section 2.5 of his book a first course in string theory : $$\sqrt{2} p^{\pm}~=~p^0\pm p^1~=~\sqrt{\vec{p}\cdot\vec{p}+ ( mc ) ^2}\pm p^1~&gt ; ~|\vec{p}|\pm p^1~\geq~0~ $$ if $m&gt ; 0$ .
this question is closely related to the question " if photon energies are continuous and atomic energy levels are discrete , how can atoms absorb photons ? " . while this technically is not an exact duplicate of the link above , a similar conceptual explanation applies , namely that for complicated strongly-coupled quantum systems ( like a hot chunk of metal ) , the energy spectrum is so complicated that for all intents and purposes it gets " smeared out " into a near-continuum , which ( along with the fact that systems do not need to be completely on-resonance with external em light to be able to absorb or emit ) means gives an intuitive flavor of why continuous spectra happen . meanwhile , for relatively simple things like gas-phase atoms , the energy spectrum is simple/sparse enough that you can actually resolve individual spectral lines ( unless the coupling to the surroundings becomes non-negligible , such as in the case of pressure broadening , which is partly responsible for the comparatively broad-spectrum appearance of high-pressure sodium vapor arc lighting used on streets and major highways ) . so in essence : simple things ( like individual atoms ) have simple , sparse spectra ; complicated things ( like chunks of hot metal ) have complicated , near-continuous spectra . also , you had be surprised at how quickly the spectra of quantum systems devolve into an absolute chaotic mayhem : in acetylene , which is a 4-atom molecule , there are entire binders hundreds of pages long which contain tens of thousands of spectral lines , which for all intents and purposes means that the molecule has a near-continuous rovibronic spectra . for systems with 5 atoms or more , it is completely insane , and for a chunk of metal ( with trillions of atoms or more ) it is not hard to see why you might expect to see a continuum component to its emission spectrum .
wikipedia actually has a very nice graphic with this information ( which roughly agrees with what i remember hearing from people " in the know" ) : the point is that there are both lower and upper bounds on the mass of the higgs boson . the lhc should be able to cover pretty much the entire range that has not yet been searched , so if it does not find the higgs , we can be fairly confident that something is wrong with the standard model . now , the question is , what could be wrong ? well , there are various possibilities . at the simple end , it is possible that there is more than one higgs boson . the simplest possible model has only one higgs boson , and for obvious reasons that is the model that many people are hoping is correct , but it is perfectly possible that there could be a multiplet of several higgs particles instead . if there is more than one , i am not sure how exactly that would change the lower and upper bounds on the mass range , but i believe that there is some possibility that if there is a higgs multiplet , all the particles could have higher masses than we would be able to detect . ( i used to know more about this but it is been a little while ) at the other extreme , it could be that the whole theoretical framework of the standard model is incorrect . that seems pretty unlikely , since pretty much every prediction the sm has made has turned out to be spot on ( except for the presence of the higgs , of course , but that is still an open question ) . there are definitely alternate theories waiting in the wings that will be receiving quite a bit more attention if the higgs is not found .
strictly speaking light can not be accelerated . viewed from a local frame it always travels at a speed of $c$ and in a straight line . since the acceleration is always zero the jerk is also always zero . light can be bent by gravitational fields , i.e. in curved space-time , and therefore it is accelerated in the sense that it is velocity changes direction so i suppose the jerk is non-zero . however the bending of light we see is just the result of the curvature of spacetime . viewed locally the light travels in a straight line at constant velocity , so it is not clear to me that jerk is an especially useful concept in calculating the trajectory of a light beam .
according to quantum electrodynamics ( qed ) , light can be thought of as going along all paths . however , the only paths that do not experience destructive interference are those in the neighbourhood of paths with stationary ( e . g . , minimal ) action ( time ) , which , in your case , is the " equal angles " path . i strongly recommend reading feynman 's qed : the strange theory of light and matter . in the link you will also find a link to video . so , with qed in hand , anthropomorphically , photons do not need to know where to go , because they go everywhere . : )
in nilsson and riedels textbook : electric circuits , it is actually stated on page 28 that " when represented in a circuit diagram , copper or aluminum wiring is not usually modeled as a resistor ; the resistance of the wire is so small compared to the resistance of the other elements in the circuit that we can neglect the wiring resistance to simplify the diagram " resitors are poorly conducting , while the wiring in circuit diagrams is typically modeled as a perfect conductor ( an equipotential ) , but of course you are correct , and you can calculate the resistance of a wire using pouillet 's law as long as you have information about the resistivity of the wire . the formula is : $$r=\rho\frac{\ell}{a}$$ in introduction to electrodynamics by griffiths the resistivity of copper at room temperature is given as $1.68*10^{-8}$ ohm-m . using this information you can form your own conclusions . ( note that $\rho$ varies with temperature )
this is a so called feynman diagram you see on the board . it is a suggestive way to write the formula written below the diagram . each aspect of the diagram directly translates to part of the formula via the so called " feynman rules " with feynman diagrams you can calculate the " amplitude " ( that is related to the quantum mechanical probability for a process to happen ) . in this case the process it electron muon scattering . as you can see initially there is a electron and a muon ( the lower part of the diagram where the time arrow starts ) . the electron is on the left and carries momentum $p_1$ and spin $s_1$ . this translated to a so called spinor $u^{ ( s_1 ) } ( p_1 ) $ in the formula . the initial muon carries momentum $p_2$ and spin $s_2$ therefore its spinor is $u^{ ( s_2 ) } ( p_2 ) $ . now since electrons and muons are electrically charged ( both are have negative charge ) , they will repel each other via the electromagnetic force . the " force carrier " , also called gauge boson of the em foce is the photon $\gamma$ , the quantum of the em vector field $a_\mu$ . the electron and muon exchange a photon carrying momentum $q$ and represened by the term $\frac{-ig_{\mu\nu}}{q^2}$ . the outgoing electron with momentum $p_3$ and spin $s_3$ corresponds to the term $\bar{u}^{ ( s_3 ) } ( p_3 ) $ and likewise the outgoing muon to $\bar{u}^{ ( s_4 ) } ( p_4 ) $ . the strength with which the electrons and muons couple to the em field is given by $-ie\gamma^\mu$ , or as written here using the fine structure constant $\alpha$: $-i\sqrt{4\pi\alpha}\gamma^\mu$ . from these parts you construct an electron " current " $\bar{u}^{ ( s_3 ) } ( p_3 ) ( -i\sqrt{4\pi\alpha}\gamma^\mu ) u^{ ( s_1 ) } ( p_1 ) $ and muon current $\bar{u}^{ ( s_4 ) } ( p_4 ) ( -i\sqrt{4\pi\alpha}\gamma^\nu ) u^{ ( s_2 ) } ( p_2 ) $ which get couple by the photon . i.e. $\bar{u}^{ ( s_3 ) } ( p_3 ) i\sqrt{4\pi\alpha}\gamma^\mu u^{ ( s_1 ) } ( p_1 ) \frac{-ig_{\mu\nu}}{q^2} \bar{u}^{ ( s_4 ) } ( p_4 ) i\sqrt{4\pi\alpha}\gamma^\nu u^{ ( s_2 ) } ( p_2 ) $ since the exchanged momentum can be anything you have to integrate over all possible momenta $\int d^4q$ but make sure to conserve moentum via dirac delta functions $\delta^{ ( 4 ) } ( p_1-p_3-q ) \delta^{ ( 4 ) } ( p_2+q-p_4 ) $ which is why you get $\int {d^4q \bar{u}^{ ( s_3 ) } ( p_3 ) i\sqrt{4\pi\alpha}\gamma^\mu u^{ ( s_1 ) } ( p_1 ) \frac{-ig_{\mu\nu}}{q^2} \bar{u}^{ ( s_4 ) } ( p_4 ) i\sqrt{4\pi\alpha}\gamma^\nu u^{ ( s_2 ) } ( p_2 ) \delta^{ ( 4 ) } ( p_1-p_3-q ) \delta^{ ( 4 ) } ( p_2+q-p_4 ) }$ the lorentz indices and sign of the photon propagator in the screenshot are wrong btw .
the $\partial x^\mu/\partial y^{\mu'}$ are just the components of the jacobian matrix , and the jacobian of an inverse transformation is equal to the inverse of the original jacobian . find the jacobian matrix that underlies $y = y ( x ) $ , invert it , and you should have the correct components .
probably this is a simple geometric effect . the voltage is proportional to the width times the transverse electric field . the electric field is proportional to the current density . the current density is equal to current over ( thickness times width ) . so when all is said and done the hall voltage is inversely proportional to the thickness for a fixed current . presumably the maximum current hall could achieve was fixed , so by reducing the thickness he could increase the effect .
your first inclination is correct . it is a matter of scale . jupiter is huge . i will resist quoting douglas adams here , but the human brain just is not equipped to deal with those sort of dimensions . add to that the distance from which that photograph was taken . you are actually beyond the orbit of the moon io in that picture , so scale will be even more set off than usual . in looking at the specific photograph you posted , there may have been some post processing effects as well in terms of colour and clarity ( due to some pixelation effects on the limb ) , although in this case i may have to claim some ignorance . however , there have been experiments done where the galileo probe actually watched a star as it passed the limb of jupiter , and measured the atmospheric extent . i can not find that exact experiment , but here is a uv shot that again appears pretty defined , until you realize the distances you are dealing with . image source : nasa the images , which show the limb between 60.6 degrees and 62.2 degrees north latitude ( planetographic ) and near 295 degrees west longitude , were obtained on december 20 , 1996 universal time . the spacecraft was about 1,561,000 km ( 21.8 jovian radii ) from the limb of jupiter and the resolution is about 16 kilometers per picture element . this picture does show some apparent haze , but i only have a few details on it ( i.e. . new horizon 's image courtesy nasa as posted on the boston globe ) . i have no idea the wavelength or any other parameters . so again , our scale of perspective is thrown off because of the huge size of the planet .
the commutators in the above expressions are sued to change the order of the hamiltonian and annihilation or creation operators . i will show you the first one in some detail , the second one should not give you problems afterwards . we start from $\hat{h}\hat{a}\psi_n$ . using the commutator $ [ \hat{h} , \hat{a} ] = \hat{h}\hat{a}-\hat{a}\hat{h} = -\hbar\omega\hat{a}$ , we can write $\hat{h}\hat{a}\psi_n = ( \hat{a}\hat{h}-\hbar\omega\hat{a} ) \psi_n$ . because we have $\hat{h}\psi_n = w_n\psi_n$ , we get $ ( \hat{a}\hat{h}-\hbar\omega\hat{a} ) \psi_n = ( \hat{a}w_n-\hbar\omega\hat{a} ) \psi_n = ( w_n-\hbar\omega ) \hat{a}\psi_n$ ( note that we can change the order of the annihilation operator and c-numbers $w_n$ and $\hbar\omega$ ) . therefore , we have $\hat{h}\hat{a}\psi_n = ( w_n-\hbar\omega ) \hat{a}\psi_n$ and we conclude that $\hat{a}\psi_n$ is an eigenstate of the hamiltonian with eigenvalue $w_n-\hbar\omega$ .
start with force ; force does work when it acts on a body . this displaces the body from its original position . a force does work when it results in this body moving . unit wise , it is joules [ j ] or newton-meters [ nm ] or [ n-m ] . this would imply $w=f \cdot d$ where $d$ is the distance . i prefer this definition ; the work done by some force on an object travels along some curve s is given by a line integral $$w=\int_{s} \vec{f} \cdot d\vec{x}$$ when path dependent . when path independent , one obtains instead that $$w=u ( d_{1} ) -u ( d_{2} ) $$ where $u$ is called the potential energy , measured in joules . $d$ is a point at which the potential is evaluated ; this can be time dependent . hope this helps !
based on my one day lab practice experience , i will cover the general part of your question . color of hologram compared to a conventional photo a hologram additionaly saves phase ( of light wave ) information . depending on the viewport the wave reconstruction allows the observer the experience color 3d pictures . however this reconstruction has a flaw : the color changes like a rainbow ( color hologram on wiki ) as can be seen in holograms on a credit card . holograms demand the same wavelength for reconstruction than creation . i do not know whether there is a procedure to capture real color holograms using 3 capturing wavelengths , better than the above shown mice from wiki . cost of elements for a holographic recorder cost of elements for a hologram recorder : using laser of long coherence length ( depth of focus ) , stable mechanics and environment , beam splitter , beam expander , a photobox full o sand and holographic recording media , this is easiliy done below 30k dollar . scale this up from my $5\cdot5\ , $cm$^2$ using stronger beam expander . your answer is yes but the question of the quality of the hologram remains . a proof is the initial linked picuture of a mice , that is small compared to a water melon .
the only two " somewhat nontrivial " facts that the trick relies upon is : a ) the independence of de facto closed contour integrals of holomorphic functions on the paths in the complex space ( i say " de facto closed " because infinite curves combined with functions that quickly , e.g. in a gaussian way , decrease at infinity allows us to consider the curve closed at infinity , too ; this method also applies to two-dimensional integrals as long as they are correctly rewritten or rethought as contour integrals of contour integrals ) ; b ) the fact that the contour integrals over the real axes are the same thing as the integrals of a real function of a real variable . to see the latter , just realize that $\int dz\ , f ( z ) $ may literally be interpreted in the riemannian way , as the sum of many infinitesimal products $\delta z\cdot f ( z ) $ , and this definition may be used whether $\delta z$ is complex or real . when it is real , it is just a special case . in all such real-vs-complex considerations , the right way to think about it is that you allow all variables to be complex for as long a time as possible , and you only impose the reality conditions such as $x=x^*$ at the very end . in a wide variety of contexts and variables , it is right to consider the complex variables to be more fundamental than the real ones . only with this attitude , one can " internalize " the analytic continuation that is so natural in so many contexts in modern physics . it seems to me that your general concerns boil down to your efforts not to make this transition to the " thinking in complex numbers " and to keep on demonizing and tabooing the methods based on complexification , so it is a self-inflicted wound . your comment slightly sounds as " i will not admit that complex numbers play a key role , anyway " which would not be a good starting point to understand modern physics . concerning the variational problem , the complexification of the variables may at most produce additional solutions , i.e. additional stationary points of the action  because we are dealing with a " larger space " that contains the original " physical space " as a subspace . the solutions of the " real problem " are among the solutions to the " complex problem " as long as the action is analytic ( both in the real sense and the complex sense ) near these solutions . at the end , the toy model for this statement is simple and based on this assertion : if $s' ( x ) =0$ for a real $x$ in the real sense , then also $s' ( x ) =0$ for the same real value of complex $x$ in the complex sense . on the contrary , even if additional solutions exist in the complex plane , they have physical consequences , so it is good to know about them . ( it is analogous to a key result here , the fundamental theorem of algebra , that says that in complex numbers , the $n$-th order polynomials simply allow one to find all $n$ roots ; those that are not real  and complex roots often exist even for real polynomials  should not really be discriminated against . ) for a simpler example of this point , note that the scattering amplitudes $a ( k ) $ as functions of the momentum $k$ often have singularities for complex values of $k$ and they know about the energies of the bound states . why it is so  and why many other statements dependent on analytic continuation are right  may look like " miraculous coincidences " or " magic behind the scenes " at the beginning but these insights are clearly so important that one should better learn and exercise sufficiently so that this " magic " will stop to look mysterious and will become a mundane part of the physicist 's thinking . this part depends on some " somewhat advanced " mathematics and an intuitively , emotionally based person interested in physics has probably never expected that he would have to learn such mathematical things to understand the behavior of physical systems . nevertheless , it is true . because the mathematical facts about the analytic continuation are so useful and universally applicable , one should better get rid of any negative emotions and hostility towards them as quickly as possible . i have personally considered complex numbers to be more fundamental than the real numbers since my 8th birthday or so  but even if it is not your case , it is never too late . one must ultimately make this transition to understand modern physics and especially to be able to do calculations . as one moves towards more complex theories , especially quantum field theory and string theory , the importance of various analytical continuations increases . complex numbers are " truly fundamental " in some physical contexts and at least " very useful tricks " in others . for example , the probability amplitudes ( and matrix elements etc . ) everywhere in quantum mechanics have to be complex for many reasons . the wick rotation is a way to switch to the euclidean spacetime where the integrals relevant for the calculation of green 's functions and probability amplitudes are more well-behaved . the correct calculations in quantum field theory involve various $i\epsilon$ prescriptions in the propagators and the right choice of contours in the complex plane  even if one is trying to stay in the infinitesimal vicinity of the real axes . metastable states have energies $e-i\gamma/2$ ; the imaginary part is related to the decay rate so instabilities and wide resonances " genuinely " move some singularities that existed on the real axis into the complex plane ( by a finite , nonzero amount ) . thermal calculations in quantum mechanics may be converted to the evolution by an imaginary time  so they admit a path integral calculation with a periodic , euclidean time . also , representations of groups  in physics as well as mathematics  should be thought of as complex representations . it is the default type . when they are real , they should be viewed as complex representations with an extra structure that may be imposed , a " structure map " , something that allows one to impose reality projections . a different kind of structure maps creates quaternionic or pseudoreal representations  so quaternions are , much like real numbers , less fundamental than complex numbers ( despite their being " larger" ) . one can not do modern physics without complex representations of groups : most of the representations appearing in quantum physics are complex . on the other hand , in classical electromagnetism , the complex amplitudes for various electromagnetic waves and oscillating circuits are just a " useful tool " that does not have to be considered fundamental or real . but they are still useful . already classical physicists had to learn to think in terms of $\exp ( \pm ix ) $ rather than $\cos x$ and $\sin x$ because the exponentials are simpler : $\exp ( a+b ) =\exp ( a ) \exp ( b ) $ is simpler than the formulae for $\cos ( a+b ) $ etc . and i could continue for a while . the overall message is that the insights of modern physics have shown the we have thought about these matters in a wrong way when we were thinking that the real numbers were fundamental . complex numbers are not just a " trick " to find various things ; the analytic continuations of many things into the complex realm is a natural setup to fully describe many physical situations and the restriction to real values is often just an ad hoc procedure that hides most of the relevant structures hiding behind the laws of physics . and some objects in physics can not be required to be real at all .
check the derivation of the boltzmann distribution from the microcanonical ensemble on the wikipedia page " maxwell-boltzmann statistics " . we suppose that the " wealth classes " of individuals are discretised , so that , for example , we find the number of individuals with $m_1 = \$500$ , the number with $m_2 = \$10000$ and so forth : as an approximation we restrict the wealth classes to discrete values . we do not even have to have the wealth classes equispaced . now , look at the derivation on the wiki page , and there will be a precise analogy between that derivation of the boltzmann distribution from the microcanonical ensemble , and the derivation of tom carter 's formula . " amount of money held " $m_i$ forms a precise analogy with " energy " and " number of particles in each state " with " number of individuals in each wealth class " , call it $n_i$ . what you will get is that the number of arrangements , helped by stirling 's formula , is : $$\log\omega \approx n\log ( n ) -n - \sum_j \left ( n_j\ , \log n_j-n_j\right ) = n\log ( n ) - \sum_j n_j\ , \log n_j$$ and it is constrained by the same two constraints : the total number of individuals is constant : $$\sum_j n_j = n = const$$ and the total amount of money is constant : $$\sum_j n_j m_j = m = const$$ so that you will get , in analogy with the bd : $$p_i = \mathcal{z}^{-1} e^{-\beta\ , m_j}$$ where the partition function $\mathcal{z}$ and $\beta$ come from the lagrange multipliers for the two constraints . the underlying assumption is that all the arrangements of allocating shares of money to indivuals are equally likely , something i highly doubt as wealthier people tend to rig the rules and the conditions to make themselves wealthier ! however , let 's assume it is true . this formula is general for general , unevenly spread wealth classes . we now need to assume what these wealth classes are . let 's now assume they are evenly spaced : $m_j = j\ , \delta$ . then we must calculate $\mathcal{z}$ to make all the $p_j$ sum up to unity . the result is : $$p_j = ( 1-e^{-\delta\ , \beta} ) e^{-j\ , \beta\ , \delta}\qquad ( 1 ) $$ the mean amount of money $m/n$ held by each individual is $$\frac{m}{n}=\sum_{j=0}^\infty j \ , \delta\ , p_j = \frac{1}{e^{\beta\ , \delta}-1}\qquad ( 2 ) $$ and if we make $\delta$ very small to give us many different money classes , then the ( 2 ) approximates to $n/m = \beta \delta$ , and ( 1 ) approximates to $p_j = \delta\ , \beta\ , e^{-j\ , \beta\ , \delta}$ , thus we get : $$p_j \approx \frac{n}{m} e^{-j\frac{n}{m}}$$ which is your formula .
there is a more precise sense in which the question is ill-posed ( at least mathematically ) ; namely , it is a fundamental assertion of relativity ( special and general ) that the time ' measured ' ( counted , experienced , observed . . . ) by an observer between two events occurring on her worldline is the length of her worldline-segment joining the two events ( that is how we connect the physical notion of ( personal ) time with the mathematics of the theory ) . the way she determines motion depends on this notion of time . equivalently , proper time is measured by the arc-length parameter of the observer . now , since null curves have zero length ( hence no arc-length parameter ) the concept of proper time is not defined for null observers . hence neither is ( proper ) relative motion ( i.e. `from the photon 's perspective' ) . also , the relation you describe between timelike and null ( instantaneous ) observers is not reflexive at all ( whereas it is for the timelike ones , via the `lorentz boosts' ) : no isometry of minkowski space can take a timelike vector to a null one . although the question does not make sense , in this strict sense , mathematically , perhaps there are other physical or mathematical tricks for interpreting it ?
energy and matter are not the same . matter is a type of thing , whereas energy is a property of a thing , like velocity or volume . so your premise is flawed . in particular : there is no such thing as " a solid state of energy " - hopefully it makes sense that a property of something does not have states energy is not represented by waves , though it is a property of a wave . it is also a property of a particle ( which , in quantum field theory , is really just a tightly bunched wave ) . note that mass can be converted to energy , because mass actually is energy . it is one of various types of energy : kinetic energy , potential energy , mass energy , and so on . different types of energy get converted into each other all the time . i would suggest looking at several of the questions under the " related " heading at the right for more information about this . ( i actually thought this had been asked here before , but i did not find an exact duplicate . )
i do not think i agree with your first sentence . our simplest theoretical models , based on classical general relativity , say that there was a singularity in the past , but few if any cosmologists take that as a reason to believe that there actually was such a singularity . rather , the most likely possibility is that those classical models are wrong at early times . the truth is that we have no idea what happened " at the big bang , " or even if that phrase is meaningful . there are some theories in which the universe has existed for infinite time ( google " eternal inflation " for instance ) , and others in which time started at a finite point in the past . conservation of energy arguments do not really help here . for one thing , conservation of energy in the expanding universe is more complicated than you might initially expect -- there is really no such thing as the total energy of the universe . for another thing , conservation of energy , all by itself , would not answer the question of whether there was a beginning . ( global ) conservation of energy says that the energy at any one time equals the energy at any other time . it does not say anything about whether there was an initial time .
the semiclassical limit you are describing says that the amplitude for a particle to get from here to there in a set time is equal to the exponential of the classical action for the corresponding classical trajectory . in symbols this reads $$\langle x_b|u ( t ) |x_a\rangle=\int \mathcal{d}\varphi e^{is [ \phi ] /\hbar} \approx e^{{i}s [ \varphi_\textrm{cl} ( x_a , x_b , t ) ] /\hbar} . $$ in a general quantum state , however , particles are not " here " and do not end up " there": they have an initial probability amplitude $\langle x|\psi ( 0 ) \rangle$ for being at each position $x$ at time $t=0$ and will have a final probability amplitude $\langle x|\psi ( t ) \rangle$ for being at position $x$ at time $t$ . to apply the approximation , you pull out the propagator and insert a resolution of the identity : $$\langle x|\psi ( t ) \rangle=\int dy\langle x|u ( t ) |y\rangle\langle y|\psi ( 0 ) \rangle=\int dye^{{i}s [ \varphi_\textrm{cl} ( y , x , t ) ] /\hbar}\langle y|\psi ( 0 ) \rangle . $$ to get a full semiclassical limit , you also need a semiclassical initial state ( since otherwise you have obviously got no hope ! ) . you take , then , a state with ( relatively ) sharply defined position and momentum ( of course , the state will occupy some finite region of phase space but you usually can assume , in these circumstances , that it is small enough ) , and this will make the amplitudes for points outside the classical trajectory interfere destructively and vanish . edit so how does this happen ? for one , $y$ must be close to the initial position , $y_0$ in order to contribute to the integral . for small displacements of the endpoints , then , the action along the classical trajectory varies as $$\delta s=p_{\varphi , x}\delta x-p_{\varphi , y}\delta y$$ ( cf . lanczos , the variational principles of mechanics , 4th edition , dover , eqs 53.3 and 68.1 , or simply do the standard integration by parts and set $\int\delta l dx=0$ along the classical trajectory ) . the main contribution of the initial state to the phase is of the form $e^{ip_\textrm{cl}y}$ , which means that the integral has more or less the form , up to a phase , $$\langle x|\psi ( t ) \rangle\approx \int_{y_0-\delta x/2}^{y_0+\delta x/2}e^{i ( p_{\varphi , y}-p_\textrm{cl} ) y/\hbar}dy . $$ here the momentum $p_{\varphi , y}$ is determined by $x$ and ( to leading order ) $y_0$ , since there is a unique classical path that connects them . this momentum must match ( to precision $\delta p\approx \hbar/\delta x$ , which we assume negligible in this semiclassical limit ) the classical momentum of the initial state , $p_\textrm{cl}$ , and therefore only those $x$ 's on the trajectory determined by the initial state will have nonzero amplitudes .
an electron is not a spinning ball of charge and the intrinsic spin of particles cannot be understood in such terms . not only is it difficult to make sense of what it means for a pointlike particle to spin , but also when treating the electron as a spinning ball of charge one finds a value of the ratio between the magnetic moment and the angular momentum that is a factor $2$ too small . to understand why a rotating charged ball generates a magnetic field , note that every charge on the ball will move in a circle , so there is in fact a current , and that current will generate a magnetic field .
in short : nuking jupiter will not do much . as you correctly noted , there is definitely not enough oxygen in jupiter 's atmosphere to support combustion , so the bomb would not ignite the hydrogen . " flammable " only applies in an oxygen-rich environment , or in an environment where oxygen can be liberated from one 's surroundings . no luck there . hydrogen bombs initiate fusion in hydrogen that is stored inside the weapon , using their initial explosion to create a symmetrical high-pressure situation . they would not initiate fusion outside the weapon . even jupiter 's hydrogen-rich atmosphere would not start undergoing fusion if you detonated a bomb there . if you were watching from earth , you would likely see a small flash of light , and that would be it . take a look for videos of comets and meteors striking jupiter ; there are some great examples where much more energy is released than in even the largest nuclear bombs .
because of the current orientation of the plane of the satellites orbits , only io 's shadow falls on the red spot . since io is the fastest moving of jupiter 's major moons , it is shadow must fall on the red spot fairly often . i would scan the various archives of jupiter images to look for images that meet this criterion : http://atmos.nmsu.edu/jupiter/jupiter.html http://www.arksky.org/alpo/index.php http://www.damianpeach.com/jupiter.htm interesting idea !
for ( 1 ) , there is a theorem of holevo that implies you cannot extract more than one bit of information from one qubit . you can indeed encode one bit of information , since the two inputs $| 0 \rangle$ and $| 1 \rangle$ ( or any two orthogonal states ) are distinguishable . if the sender and receiver share an entangled state , they can use superdense coding to send two bits using one qubit . for ( 2 ) , if the wavelength is exactly 500 nm ( or at least as close as you can get in one hour ) , then by the uncertainty principle the photon must consist of an hour-long wave train which is a sine wave whose frequency gives 500 nm , and which ( aside from its polarization ) carries no information . you need non-zero bandwidth to transmit information . if you have a non-zero bandwidth , you can transmit information based on the frequency of the photon . the amount of information you can transmit depends on the bandwidth . the timescale of one hour limits the precision with which you can determine frequency , and this will give you the amount of information you can send . note that if you use frequency to transmit the information , alice 's random delay does not hurt you . if you did not have a random delay , then you could use the non-zero bandwidth to create wave packets that are localized in time , and use timing to transmit the same amount of information . let 's do some calculations . let 's suppose the frequency is 500 nm $\pm$ . 5 nm ; then the frequency is 6*10$^{14}$ $\pm$ 6*10$^{11}$ hz , and the bandwidth is 1.2*10$^{12}$ hz . now , we have $te \geq \hbar$ , and $e = h \nu$ , so $t\nu \geq \frac{1}{2\pi}$ . if $t$ is 3600 sec , we can distinguish frequencies to accuracy 4.4*10$^{-5}$ hz , so we get 2.7*10$^{16}$ different frequencies we can distinguish . taking the log ( base 2 ) , this is around 55 bits . i suspect this is the most information that can be sent with one photon with the given bandwidth , but i do not know a proof of this . this is all theoretical ; doing this in practice would require a ridiculously accurate frequency filter . if you did not have the random delay , using wave packets localized in time and a very accurate clock would work better . even with the random delay , i would not be surprised if there were clever , experimentally more feasible , ways of communicating with one photon .
you are absolutely correct , the electric field does fall off with distance from the battery . however , this is only true during the transient state ( the state of the field when the battery is first connected ) . in fact not only are the magnitudes inconsistent , but so is the direction of the field . the field does not always point in the direction of the wire . the entire field is inconsistent in direction in magnitude and direction . the image below illustrates this : ignore the green arrows . the yellow arrows indicate the direction and the magnitude of the field at that point . see how there totally wrong ? lets see what happens next . note the field right before and after the " right bend " . the field " going in " is greater in magnitude that the field " going out " and the field going out is pointing in the wrong direction ! because of this electrons start building up at the " right bend " ( since more electrons are going in than going out ) . the build up of electrons creates a new field , which results in the fields before and after the right bend changing . in fact this happens everywhere the field is not consistent in magnitude and direction . electrons start building up which generates a new field , that alters the original field until everything points in the right direction and is of equal magnitude , preventing further electron build up . so during the transient state , electrons build up on certain places of the wire generating new fields until the field is consistent in magnitude and direction . you end up with something like this :  all images were taken from matter and interactions . great question , unfortunately most physics books choose to completely skip over this , quite fundamental , concept . hope that helps !
you need to know the spectral distribution of the source , then fit that curve to the measured reading at the given wavelength . then the total power output will be the integral under this curve , per solid angle . that is you also need to take into account the total spread of the light through a some surface in space . and then integrate over this surface as well . i suggest this is done via some appropriate software , matlab would be my first choice , but you might have a different preference .
you need to master algebraic geometry ( in particular derived functor cohomology of sheaves , say ega 1-3 ) , algebraic topology ( characteristic classes , say the entire book of spanier ) , commutative algebra and algebra in general ( say lang 's book and a/m ) , and also homological algebra ( the entire book of weibel ) .
one 's naive expectation would be that as the object moves through the medium , it collides with molecules at a rate proportional to $v$ . the volume swept out in time $t$ is $a v t$ , where $a$ is the cross-sectional area , so the mass with which it collides is $\rho a v t$ . the impulse in each collision is proportional to $v$ , and therefore the drag force should be proportional to $\rho a v^2$ , with a constant of proportionality $c_d$ ( the drag coefficient ) of order unity . in reality , this is only true for a certain range of reynolds numbers , and even in the range of reynolds numbers for which it is true , the independent-collision picture above is not what really happens . at low reynolds numbers you get laminar flow and $c_d\propto 1/v$ , while at higher reynolds numbers there is turbulence , and you get $c_d$ roughly constant .
the de broglie wavelength formula is valid to a non-fundamental ( many body ) object . the reason is that for a translation invariant system of interacting particles , the center of mass dynamics can be separated from the internal dynamics . consequently , the solution of the schrdinger equation can be obtained by separation of variables and the center of mass component of the wave funtion just satisfies a free schrdinger equation ( with the total mass parameter ) . here are some details : consider a many body nonrelativistic system whose dynamics is governed by the hamiltonian : $\hat{h} = \hat{k} + \hat{v} = \sum_i \frac{\hat{p}_i^2}{2m_i} + v ( x_i ) $ ( $k$ is the kinetic and $v$ the potential energies respectively ) . in the translation invariant case , the potential $v$ is a function of relative displacements of the individual bodies and not on their absolute values . in this case , the center of mass dynamics can be separated from the relative motion since the kinetic term can be written as : $ \hat{k} = \frac{\hat{p}^2}{2m} + \hat{k'} $ where $p$ is the total momentum and $m$ is the total mass . $k'$ is the reduced kinetic term . in the case of a two-body problem , for example the hydrogen atom $k'$ has a nice formula in terms of the reduced mass , for larger number of particles , the $k'$ formula is less nice , but the essential point is that it depends on the relative momenta only . for this type of hamiltonian ( with no external forces ) , the schrdinger equation can be solved by separation of variables : $\psi ( x_i ) = \psi ( x ) \psi' ( \rho_i ) $ where $x$ is the center of mass coordinate , and $\rho_i$ is a collection of the relative coordinates . after the separation of variables , the center of mass wave function satisfies the free schrdinger equation : $ -\frac{\hbar^2}{2m}\nabla_x^2\psi ( x ) = e \psi ( x ) $ whose solution ( corresponding to the energy $e = \frac{p^2}{2m}$ ) has the form : $\psi ( x ) \sim exp ( i \frac{p x}{\hbar} ) $ from which the de broglie wave length can be read $ \lambda = \frac{2 \pi \hbar}{p}$
what seems to be happening is that capillary effects in the presence of gravity create a situation in which the cork being maximally decentralized in the glass corresponds to a minimum energy configuration . my guess is that the cork is non-wetting , and therefore surrounded by a water surface that bends down in the proximity of the cork , thereby creating a minute overall increase of water level in the glass . with the cork resting against the edge of the glass , this water level increase gets minimized . if all of this is right , the effect should disappear if the cork is replaced by a floating material that is water wet .
essentially , you have a manifold comprising two manifolds , say $m = n \times r$ , and the metric is written as $m_{ab} = n_{ab} + r_{ab}$ with $n_{ab}$ defined on $n$ and $r_{ab}$ on $r$ . since the $n$ , $r$ are disjoint , if you define a tensor in , say , $n$ , its local coordinates corresponding to $r$ are zero , hence the ricci tensor etc are all block diagonal .
a satellite can stay in pretty much any orbit , leo satellites are pretty common . the lower the orbit the more air particles slowing the satellite down and the more need for readjusting it either by its own engines or by a spacecraft . now for a human skydiving , there is really no limit . it is just the matter of drawing a line between a suit and a capsule ; if the suit can provide arbitrary amount of air , shield from vacuum , dissipate extreme heat of passing through the atmosphere and so on , there is no limit . if it can not provide arbitrary degree of that , there will always be a slightly better suit allowing for a slightly higher jump . no " top height " as such . current 39km is the result of a golden middle between marketing value of the jump and cost of equipment and research necessary to perform it . on orbital altitudes , the vehicle the jump is performed from would have to move in such a direction and speed as to provide optimal entry curve instead of just dropping the person into the orbit and leaving them there to orbit earth forever . but other than that , there is no reason why a man could not be lobbed from behind jupiter , make a slow-down loop around the moon , then spiral down to earth . . . given some marvelous suit that will withstand the atmospheric entry .
your question has to do with addition of velocities in special relativity . for objects moving at low speeds , your intuition is correct : say the bus move at speed $v$ relative to earth , and you run at speed $u$ on the bus , then the combined speed is simply $u+v$ . but , when objects start to move fast , this is not quite the way things work . the reason is that time measurements start depending on the observer as well , so the way you measure time is just a bit different from the way it is measured on the bus , or on earth . taking this into account , your speed compared to the earth will be $\frac{u+v}{1+ uv/c^2}$ . where $c$ is the speed of light . this formula is derived from special relativity . some comments on this formula provide direct answer to your question : if both speeds are small compared with the speed of light , they approximately add up as your intuition tells you . if one of the speeds is the speed of light $c$ , you can see that adding any other speed to it does not in fact change it : the speed of light is the same in all reference frames . if you add up any two speeds below $c$ , you end up still below the speed of light . so , any material object which has a mass ( unlike light , which does not ) , moves at a speed less than $c$ . adding to it according to the correct rule makes it closer to the speed of light , but you can never exceed it , or in fact not even reach it . i would recommend wheeler and taylor 's " spacetime physics " to read about this . unlike the reputation of the subject it is actually pretty intuitive ( i learned that formula in high school ) .
assuming that $m_1$ and $m_2$ take up a finite amount of space ( e . g . , two spheres of mass with radius $r_0$ ) , that equation is not even valid for $r &lt ; r_0$ , so there is no inconsistency . the derivation follows from gauss ' law ; it is analogous to the application of gauss ' law in electrostatics ; the $m_1$ and $m_2$ are the mass enclosed at some distance $r$ .
the best way to learn supersymmetry in my opinion is to watch lectures on it . fernando quevedo has a terrific lecture series available online here . he also has accompanied lecture notes with homework exercise which are crucial to do in order to get a good grasp of the material . there are also many other free lecture notes on line which you can find by googling supersymmetry lecture notes .
$|q\rangle \langle q|$ is an operator that maps the state $\alpha$ to $$|\alpha\rangle \mapsto |q\rangle \langle q|\alpha\rangle $$ you may easily calculate what the new state on the right hand side is . because $\langle q|\alpha\rangle$ is a simple inner product  it is equal to $\alpha ( q ) $ if you express the state $\alpha$ as a wave function in the position representation  you see that the right hand side is simply , in the position representation , $\delta ( x-q ) \alpha ( q ) $ . so it is an operator that maps any vector to a multiple of the delta-function wave function located at $x=q$ , i.e. $\delta ( x-q ) $ , multiplied by a constant given by $\alpha ( q ) $ . because i told you how the operator acts on any state , i have fully defined the operator . similarly , $|\psi\rangle \langle \psi|$ is an operator that maps the state $\alpha$ to $$|\alpha\rangle \mapsto |\psi\rangle \langle \psi|\alpha\rangle $$ which may also be calculated for every pair of states . every state $|\alpha\rangle$ is mapped to a multiple of the state $|\psi\rangle$ where the coefficient is given by the inner product . these operators are multiples of projection operators but they are only projection operators if the state $|s\rangle$ in $|s\rangle \langle s|$ is normalized to one . the state $|q\rangle$ is surely not normalized to one ( its norm is infinite ) so one should not consider $|q\rangle \langle q|$ to be a projection operator .
one or two sentences ? heat is transferred to water , which therefore evaporizes , taking away the heat with it . latent heat is the amount of heat necessary to trigger a phase transition in a substance , and is not spent to increase or decrease the substance 's temperature . so it is incorrect to say tha the water has removed the latent heate . i would rather say that water removed a heat quantity equal to its latent heat .
the spaces you have in mind are in general not vector spaces . but it still makes sense to ask whether there is a useful concept of length around . in particular , phase space ( the 6n-dimensional space of positions and momenta ) is a vector space if the underlying position space of single particle itself is . more generally , the particles would be constrained to move only on some subspace ( e . g . a sphere ) and the phase space would be cotantgent bundle of the $n$-fold product of the single-particle subspace . more generally still , the total position space need not have a product structure ( which can happen if position of some particle depends on position of others ) . again , the full space would be a cotangent bundle to the total position space . now , the phase space does not support a canonical notion of distance . nevertheless , it does contain something else that is special : a symplectic two-form $\omega$ ( the reason it is there is that phase space is always formed as a cotangent bundle ) which turns it into symplectic manifold . using $\omega$ , one can form a $2n$-form ( n here is the dimension of the total position space ) $\omega^n = \underbrace{\omega \wedge \cdots \wedge \omega}_{n \text{ times}}$ . since this is a form of a maximal degree we can use it to measure volumes and this leads to famous liouville 's theorem that states that hamiltonian flows on the symplectic manifold are incompressible ( i.e. . evolution conserves phase volume ) . this is all the structure we get for phase spaces . you can impose a metric on any manifold to turn it into a riemannian manifold but this will not be canonical ( i.e. . there are many different ways to measure distances ) . in case your phase space happens to be a simple $6n$-dimensional vector space , you can use the euclidean notion of a distance but , again , this will not be canonical . in general , there is no canonical way to measure things and you simply should not be expecting to be able to do so unless you are explictly given something nice like euclidean space or a submanifold thereof .
propagating sound is the periodic motion of the molecules passed on through the air . so the kinetic energy of the molecules is also passed on . temperature is merely a measure of the mean kinetic energy of all the molecules in a gas . so temperature rises only locally where the potential energy decreases , because the distance between the molecules decreases . half a wavelength further the distance between the molecules increases , potential energy increases and the temperature will fall . so to answer the question , the energy of the vibrating fork is converted into sound
the countable and uncountable infinities are " different cardinals " according to set theory but in physics , the bases of that size produce equally large hilbert spaces : the hilbert-space is infinite-dimensional and all infinite-dimensional hilbert spaces are isomorphic to each other ( in other words , there is only " one unified kind of infinity " when it comes to the dimension of a hilbert space ) . quantum mechanics offers you infinitely many examples . perhaps the simplest example are fourier expansions . consider a particle in an infinite well so that the wave function $\psi ( x ) $ is only nonzero for $0\lt x \lt +\pi$ . the operator $x$ has a continuous spectrum i.e. an uncountable number of eigenvalues and eigenstates ( the basis of $x$-eigenstates is uncountable ) . on the other hand , the operator $p^2 = -\hbar^2 \partial^2 / \partial x^2$ has a discrete spectrum and a countable set of eigenvalues and eigenstates . the eigenstates are standing waves $\sin ( nx ) $ for positive integer $n$ and the eigenvalues are $n^2$ . nevertheless , every ( "smooth enough " and/or $l^2$-normalizable etc . ) function $\psi ( x ) $ that is nonzero in that interval  every combination of uncountably many wave functions $\psi ( x ) = \delta ( x-x_0 ) $  may also be written as a linear combination of the standing waves , $\sin ( nx ) $ . this fact is what makes the fourier series possible . ( normally , i would talk about periodic functions and complex exponentials but the sines in a well may be more beginner-friendly . ) there is really no contradiction with the different cardinality of the sets because the two sets , the uncountable basis of $x$ eigenstates and the countable basis of $p^2$ eigenstates , are not being identified via a one-to-one map . instead , the map between one basis and the other is a general linear transformation that mixes them , and the different cardinality places no restrictions on such linear transformations of infinite-dimensional vector spaces . quite generally , cardinal numbers ( the science about distinguishing many types ) as well as most other related results in set theory ( i mean especially gdel 's theorems ) are completely inconsequential in physics . they are just some " recreational subtleties " in mathematical logic and physics does not find any of these operations relevant . so a physicist may do state-of-the-art string theory and interpret it in all corners of physics without even " knowing " that the real numbers are uncountable . the uncountability is unphysical . a physicist is generally agnostic about the existence of real numbers that cannot be constructed , about the validity of the axiom of choice , and other problems that cannot be operationally performed by an experiment . a physicist 's typical reaction is that these questions are " philosophy "  they are empirically undecidable ( we know that the axiom of choice is undecidable even in major axiomatic systems of set theory ) so he does not care about the answers .
how about path integrals ? the probability that a system evolves between state $|\phi_1\rangle$ and $|\phi_2\rangle$ is $$\langle \phi_2|\phi_1 \rangle =\int_{\phi_1}^{\phi_2}\mathcal{d}\phi \exp \left ( \frac{i}{\hbar}s ( \phi ) \right ) $$ where the measure $\mathcal{d}\phi$ is suitably defined and the action $s ( \phi ) $ is the integral of the lagrangian ( over whatever the physical coordinates are ) . consider two systems , described by states $|\phi\rangle$ and $|\psi\rangle$ , which are independent . then the action is $$s ( \phi , \psi ) =s ( \phi ) +s ( \psi ) $$ and the probability of evolving between two configurations is \begin{align} \langle\phi_2 , \psi_2|\phi_1 , \psi_1\rangle and =\int_{ ( \phi_1 , \psi_1 ) }^{ ( \phi_2 , \psi_2 ) }\mathcal{d}\phi\mathcal{d}\psi \exp \left ( \frac{i}{\hbar} ( s ( \phi ) +s ( \psi ) ) \right ) \\ and =\int_{\phi_1}^{\phi_2}\mathcal{d}\phi \exp \left ( \frac{i}{\hbar}s ( \phi ) \right ) \int_{\psi_1}^{\psi_2}\mathcal{d}\psi \exp \left ( \frac{i}{\hbar}s ( \psi ) \right ) \\ and =\langle \phi_2|\phi_1\rangle \langle \psi_2|\psi_2\rangle\\ \end{align} so the probability is a product if the systems are independent . i picked specific states here but take $|\phi_i\rangle$ describing a system $s_i$ in configuration $c_{i1}$ and i think this gets what you want .
hubble 's law applies to the expansion of space itself , i.e. , if two objects stationary to each other that had no force between them were left alone the distance between would increase with time because space itself is expanding . this is what hubble 's law addresses . in the case of the milky way and andromeda galaxies ( and all galaxies for that matter ) there is a force between them : gravity . the gravitational force between the milky way and andromeda galaxies has produced an acceleration that is causing the two galaxies to be moving towards each other faster than the space between them is expanding as calculated by hubble 's law . however , the vast majority of galaxies lie far enough away from the milky way that the gravitational force between us and them is small compared to the hubble expansion and hubble 's law dominates . in short , hubble 's law applies throughout the universe , but localized systems may have enough gravitational attraction between them that the gravitational effects dominate .
at a basic level : the universe , in the beginning was very hot . so hot in fact that there were no atoms , only electrons and protons and neutrons and photons flying around . the photons were scatting off of the electrons and protons , as they interacted strongly because the electrons and protons are charged . the universe was much like the plasma you find in plasma balls , but turned up to 11 . it was opaque . you could not see through it . as the universe expanded , it cooled and at around 380,000 years after the big bang , it was cold enough that stable atoms could form . at this point , all of the photons that were flying around suddenly stopped reacting with all of the free electrons and protons , since they started to form atoms that had no net charge , behaving much like a very dilute gas , like the air . the universe became transparent . just as we can see through air , at this point the photons could travel unimpeded . this is referred to as the " surface of last scattering " , but you should not think of it as a surface , you should think of it as a moment in time where the universe went from being opaque to light to being mostly transparent to light . having suddenly nothing to interact with , those photons just starting travelling in straight lines . some of those photons were just the right distance from us and were pointed in just the right direction that they are hitting us just now . in fact , they are hitting us continuously since the entire universe was filled with this photons just before the universe went " transparent " . so , the cmb is not at the edges , its everywhere , its all of the photons that are still to this day flying off in every which direction . occasionally those photons hit something , but since the universe is mostly empty space , the fraction that hit something is completely negligible . it is safe to assume they have not interacted with anything since the " surface of last scattering " nearly 14 billion years ago . nowadays , those photons are long in wavelength , nearly 1 mm , because as the universe has continued to expand , they continue to cool and stretch in wavelength .
have a look at http://en.wikipedia.org/wiki/galilean_invariance. this is not too mathematical and explains what is going on . the basic idea is that there is no such thing as absolute motion . for example , because the earth is rotating as i sit here typing i am moving at about 800 miles per hour . why am i not splattered against my computer screen ? it is because everything around me is moving at the same speed , so relative to where i am sitting i am not moving . in the specific case of the fly , the fly moves by beating it is wings against the air . but the air is stationary with respect to you , otherwise you had be sitting in a 100km/hr wind . that is why you see the fly moving at whatever speed flies normally move at .
sound disturbances always travel at the speed of sound in the medium . the wave equation is usually solved with $$ u ( x , t ) = u \sin \left ( \omega t \pm \frac{\omega\ , x}{c} \right ) $$ where $c$ is the speed of sound and $f=\frac{\omega}{2\pi}$ is the frequency of the disturbance .
sadly we can not do that . what we see of the cmb is a different part of the universe , the galaxy clusters that have evolved from the clumps that we see in the cmb are beyond our capability to see ( they are farther away than it would take light to travel since their creation ) . theoretically we will eventually see those galaxies if we keep observing for a few billion more years ( as well as other parts of the cmb ) . for now our studies relating structure in the cmb to galaxy formation are entirely based on modelling and statistics .
two resistors in series will have the same current flowing through them . the potential difference across each resistor ( $v=ir$ ) will sum together . so , for example if you connect a battery with a given voltage $v$ to the two resistors ( $r_1$ and $r_2$ ) , the voltage drop across the resistors will equal the battery voltage , and then you will know the current from $v = i ( r_1+r_2 ) $ ) .
perhaps the degree of quantization is so small the radiation curves look continuous yes , this is the reason . the correspondence principle says that quantum mechanics has to become classical in the appropriate limit . one way to obtain an appropriate limit is with large numbers of particles . as you increase the number of particles in a material many-body system , you get more and more ways of putting together combinations of states for your material object . the density of states of the object grows very quickly ( roughly exponentially ) with the number of particles . therefore the number of possible transitions between states also grows very rapidly . the number of particles in a tungsten lightbulb filament is something like avogadro 's number . the exponential of avogadro 's number is really , really big .
this is crude . maybe there can be an energy approach . initially the mass has potential energy $t=m g h$ . at the point of peak splash-back lets assume all the energy has been transferred to the water with peak potential energy related to the radial wave height function $y ( r ) = ? $ . a small volume of water a distance $r$ from impact has differential volume ${\rm d}v = y ( r ) 2\pi r {\rm d}r$ . the potential energy of the small volume of water is ${\rm d}t = \rho g \frac{y}{2} {\rm d}v$ where $\rho$ is density of water . the total energy is thus : $$ t = \int_0^\infty \rho g \frac{y ( r ) ^2}{2} 2\pi r {\rm d} r $$ putting a nice smooth wave height function of $$y ( r ) = y \exp ( -\beta\ , r ) \left ( \cos ( \kappa\ , r ) +\frac{\beta}{\kappa} \sin ( \kappa\ , r ) \right ) $$ with $y$ a height coefficient . this has the properties of ${\rm d}y/{\rm d}r=0$ at $r=0$ with $y ( 0 ) =y$ . $$ t = \frac{\pi y^2 g \rho \left ( 9 \beta^4+2 \beta^2 \kappa^2+\kappa^4\right ) }{8 \beta^2 \left ( \beta^2+\kappa^2 \right ) ^2 } = m g h $$ so wave height should be $$ y = \propto \sqrt{ \frac{h m}{\rho g \pi }} $$
after a long time both will cool to the temperature of the freezer , the time taken will depend on the specific heats of water v vodka ( which i suspect are similar ) . this is a consequence of the second law of thermodynamics .
there are theories which are just not going to be disproved , ever , ever , ever . the story people tell about this is a " noble lie " , in the sense that it is designed to get them to distrust authority , and this is good . but the way this is done by drawing a false equivalency between garbage that gets accepted and disseminated , like aristotle , and science that get superseded in certain domains , like newton . aristotle 's stuff was completely wrong , and newton 's stuff is completely right . newton made a few mistakes , but by and large , was honest and accurate scientist . what he discovered does not get undiscovered , it will stay valid forever . it can only be forgotten . the statement that newton 's theory is valid for macroscopic objects of normal atomic density and size less than the sun , moving at less than $10^{-4}$ of the speed of light , to a certain accuracy which is fairly perfect , will never be overthrown . it is just a fact . these types of facts when organized with some mathematics are usually called the " laws " , and newton 's laws continue to be laws of nature in certain restricted domains even after newton 's theory is superseded . newton 's theory was much more overarching than the modest claim above : newton claimed that his laws are true for all regimes--- it aspires to be a complete theory . in this aspiration , it fails , and it gets modified at high velocities , small distances , and high densities . but you never backpeddle on the experimentally established facts . you never unlearn kepler 's laws , you never unlearn that you can make as much heat as you want by drilling a cannon , you never unlearn that aristotle is garbage , and you will never unlearn that quarks exist . these are laws of nature , forever immutable , like the restricted newton 's laws . similarly , relativity will never be superseded in its domains of validity . if you discover that relativity is wrong , it will be in another regime which has not been seen . the way science proceeds is different from the stories . it is a ratchet , with forward motion , but its wobbly , and it takes a long time for the tooth to drop . the design is to ensure that bullshit like aristotle is not made dogma . the design of science was for another time , with literature which was not instantaneous , and no ability for people to cross check everything by themselves . we live in a better media environment , and this will mean that the mechanism of science can relax a little . it is much harder for bullshit like aristotle to get accepted today , because anyone can call it out publically with no effort .
this is just a footnote to crazy buddy 's answer ( which is correct ! :- ) : length contraction is a real phenomenon , and indeed the rhic observes this every day because the nuclei are moving so fast that the collision is between two disks not two spheres . however to see something you need to have light emitted from the object reach your eye , and the light from different parts of the moving sphere takes different times to reach your eye . this distorts the image of the contracted object and has the apparently paradoxical effect of making it look spherical even though it is contracted . so the moving sphere looks spherical even though it is not spherical . the calculation of how light from the object reaches your eye is quite involved , and i am afraid i do not know of a simple analogy to understand it . there are various animations showing this effect on the web . see for example this one .
you can only do the balance of forces between spatially separate points only if you know the slope of the rope at each point , and the weight of the rope between the points only . to get there you use a small section to derive the differential equations and then integrate over the range of $x$ values you want . given a small section of rope spanning the horizontal distance ${\rm d}x$ then the tangential distance is ${\rm d}s = \sqrt{ {\rm d}x^2 + {\rm d}y^2 }$ which leads to the expression ${\rm d} s = \sqrt{1+y'^2}\ , {\rm d}x$ where $y'=\tan \theta$ is the slope . the total weight of the rope in this section is thus ${\rm d}w = \mu g\ , {\rm d} s$ where $\mu$ is the linear density of the rope $\mu = \rho a = \frac{m}{\ell}$ with $\rho$ mass density , $a$ the cross sectional area , $m$ the total hanging mass and $\ell$ the total hanging length . at each point if we split the tangential tension $t$ into horizontal part $h$ and vertical part $v$ such that $\tan \theta = \frac{v}{h}$ and $t=\sqrt{h^2+v^2}$ this leads to the following equations $$ \begin{align} \frac{{\rm d}h}{{\rm d}x} and = 0 \\ h \frac{{\rm d}^2 y}{{\rm d}x^2} and = \mu g \frac{{\rm d}s}{{\rm d}x} = \mu g \sqrt{1+\left ( \frac{{\rm d}y}{{\rm d}x}\right ) ^2} \end{align} $$ in the end you get an equation for the shape of the rope $y ( x ) $ called a catenary . to get the balance of forces between points (1) and (2) above you have $$ \begin{align} -t_1 \cos \theta_1 + t_2 \cos \theta_2 and = 0 \\ -t_1 \sin \theta_1 + t_2 \sin \theta_2 and = \mu g \int_{x_1}^{x_2} \sqrt{1+\left ( \frac{{\rm d}y}{{\rm d}x}\right ) ^2} {\rm d}x \\ \tan \theta_1 and = \frac{{\rm d}y}{{\rm d}x} \vert_{x=x_1} \\ \tan \theta_2 and = \frac{{\rm d}y}{{\rm d}x} \vert_{x=x_2} \\ t_1 and = h \sqrt{1+\left ( \frac{{\rm d}y}{{\rm d}x}\right ) ^2} \vert{x=x_1} \\ t_2 and = h \sqrt{1+\left ( \frac{{\rm d}y}{{\rm d}x}\right ) ^2} \vert{x=x_2} \end{align}$$ ps . i know all this works because i used the above to create a catenary solver :
check  except the the speed depends on the center of rotation , not the choice of the origin . the general rule is that if the origin as velocity $\vec{v}_o$ then a point a located at $\vec{r}_a$ has speed $\vec{v}_a = \vec{v}_o + \vec\omega \times \vec{r}_a$ . it happenstance that your origin does not move . check  . in general , torques and angular momental are related with $\sum \vec{\tau}_a = \frac{{\rm d}}{{\rm d}t} \vec{l}_a$ where a is any point on the rotating frame . check  . angular velocity and acceleration is shared among all points fixed to the rotating frame . acceleration is the time derivative of velocity $\frac{{\rm d}}{{\rm d}t}\vec{\omega} = \vec{\alpha}$ . incorrect  . mass moment of inertia $i$ is a tensor of 6 values represented by a 33 matrix $$i=\begin{pmatrix} i_{xx} and i_{xy} and i_{xz} \\ i_{xy} and i_{yy} and i_{yz} \\ i_{xz} and i_{yz} and i_{zz} \end{pmatrix}$$ some books have the cross terms negative and some positive . there is no convention of what is a positive $i_{xy} = \int x y {\rm d}m$ or $i_{xy} = \int -x y {\rm d}m$ ( ref 1 ) . typically , this tensor ( mmoi ) is described on the center of mass first , along coordinates fixed to the body . so $i_{body}$ is constant . as the body rotates it has a 33 rotation matrix $r$ so the mmoi tensor in fixed coordinates at the center of mass is $$i_c = r i_{body} r^\top$$ with $^\top$ the matrix transpose operator . to move the mmoi tensor to a different location , like where the rotation center is ( or the origin ) one must apply the parallel axis theorem . this is done with $$i_o = i_c - m [ \vec{r}_c ] [ \vec{r}_c ] $$ where [vector] is the 33 skew symmetric cross product operator ( ref 2 ) and $\vec{r}_c$ is the position of the center of mass relative to o in fixed coordinates . incorrect  . angular momentum at any point not at the center of mass has two components . one is the intrinsic momentum at the center of mass $i_c \vec{\omega}$ and the second the moment of linear momentum $\vec{r}_c \times m \vec{v}_c$ where $\vec{v}_c$ is the linear velocity of the center of mass . $$\vec{l}_o = i_c \vec{\omega} + \vec{r}_c \times m \vec{v}_c = i_c \vec{\omega} + \vec{r}_c \times m ( \vec{v}_o + \vec\omega \times \vec{r}_c ) \\ = \left ( i_c \vec{\omega} - m \vec{r}_c \times \vec{r}_c \times \vec\omega \right ) + \vec{r}_c \times m \vec{v}_o = i_o \vec\omega + \vec{r}_c \times m \vec{v}_o$$ do you see how the parallel axis theorem is derived from the angular momentum transformation laws . incorrect  . because mmoi is a tensor and not a scalar the direction of $\vec{l}$ is generally not the same as $\vec\omega$ . check  . torque is vector field ( varying by location ) with the general law of $$\vec{\tau}_o = \vec{\tau}_a + \vec{r}_a \times \vec{f}$$ this is dual to the velocity transformation law ( above on 1 . ) , if re-arranged like $\vec{\tau}_a = \vec{\tau}_o + \vec{f} \times \vec{r}_a $ . the direction of $\vec\tau$ ( at origin ? ) is different from $\vec\omega$ because $i_c$ is not constant ( see below ) . the general equations of motion are $$\boxed{\begin{align} \sum \vec{f} and = \frac{\rm d}{{\rm d}t} \vec{p} = m \frac{\rm d}{{\rm d}t} \vec{v}_c = m \vec{a}_c \\ \sum \vec{\tau}_c and = \frac{\rm d}{{\rm d}t}l_c = i_c \frac{\rm d}{{\rm d}t} \vec\omega + \frac{\rm d}{{\rm d}t}\left ( i_c \right ) \vec\omega = i_c \vec\alpha + \vec\omega \times i_c \vec\omega \end{align} }$$ expressed at the center of mass always ( ref 3 ) . then use the transformation laws to move the torque around ( like knowing that the torque component along the rotation axis is zero ) . in your case $\hat{z}^\top \vec{\tau}_o =0$ and $\vec{\tau}_o =\sum \vec{\tau}_c + \vec{r}_c \times \sum \vec{f}$ . you need to re-work the rest from the information given above .
the balls are entering the water well below the surface . the pressure there is much higher than at the surface . the work needed to push the balls into the water at this depth cancels the work gained when they float back up . we can ignore the gravitational force on the balls since gravity pulls down as much as up as you traverse the loop . mathematically , if the balls enter the water at depth $d$ , the pressure is $g \rho d$ with $g$ gravitational acceleration and $\rho$ the density of water . the work done to submerge the balls is then the pressure times their volume , or $w_{ball} = g \rho v d$ . the force upwards on the ball is the weight of the water they displace which is $g \rho v$ , and the work the water does on the balls is this force times the distance up that they travel , or $w_{water} = g \rho v d$ . the work the ball does on the water is the same as the work the water does on the ball . no free energy .
accelerating a body beyond $c$ with a force to accelerate the body you can use a force $\vec{f}$ and the newton 's equation in your lab frame ( for experts : the very non-covariant form with relativistic mass $m$ ) : $$\vec{f} = \frac{dp}{dt} = m \vec{a} + \frac{dm}{dt} \vec{v}$$ but for a body with rest mass $m_0$ ( i.e. . the one measured at rest ) and velocity $v$ we have $$m = \frac{m_0}{\sqrt{1-\frac{v^2}{c^2}}}$$ so when the speed approaches $c$ , $1-v^2/c^2$ gets very small and $m$ very big . as we get closer and closer to $c$ , the mass $m$ grows into infinity as well as the force $\vec{f}$ needed for further acceleration . i.e. you would need an infinite force ( and infinite energy ) to accelerate a body even to $c$ , so breaching the limit by a usual force is impossible . the more energy you put into a body , the more massive and thus more repulsive to acceleration it will be and you can asymptotically approach $c$ but will never do so . gravitational free fall " beyond $c$" in general relativity you should ask : accelerates beyond the speed of light with respect to who ? if the answer is an observer far away from the gravitating body , the moment corresponding to beyond-speed-of-light transition corresponds to the transition beyond the event horizon of a black hole . there is a simple argument to get an approximate intuition for this . consider a point far away ( "at infinity" ) from the gravitating body from which the massive particle ( or " elevator" ) accelerates just beyond the speed of light at a height $h$ above the body . stopping the particle and reversing the process to send this particle back to infinity would mean we have to accelerate it to beyond speed of light . which is impossible , so no physics leave from $h$ or lower . the observer far away would just see a black void from $h$ and lower . this is an approximate notion of a black hole . but the whole argument is very inexact . it is more accurate to talk about general relativity , " space-like " and " time-like " directions etc . and surprisingly enough , it turns out that such a " censorship " $h$ is universal for all particles shot at any speed from any observer outside the black hole . but it is the same $h$ only in the sense that not even a particle of speed $c$ can escape towards them . so a usual force cannot reach this situation and gravity makes sure it gets censored . in a sense you could say the particle beyond the horizon is moving beyond the speed of light with respect to an observer outside , but there is no observational sense to this . most importantly , special relativity is never locally violated . only when formally comparing far away causally disconnected events we can seem to violate it ( but it is not true ) .
the $\chi^2$ statistic is independent of the number of degrees of freedom . but converting that statistic to some type of $p$-value does depend on the degrees of freedom . that is , you calculate $\chi^2$ , then with that number and the degrees of freedom you look it up the $p$-value in a $\chi^2$ table . as for the " corrected " version of this test , you may find the wp page for yates continuity correction helpful . the extra 0.5 term in the numerator is there to compensate from modeling a discreet distribution using a continuous distribution . some claim it overcompensates in certain circumstances . true , but i have found it gives closer to exact results in nearly all case .
this still does not explain to what the importance of the factor of i is euler 's formula $$e^{i\omega} = \cos \omega + i\sin \omega$$ thus , the real part of $e^{i\omega}$ is $\cos \omega$: $$\cos \omega = \frac{e^{i\omega} + e^{-i\omega}}{2}$$ and the imaginary part of $e^{i\omega}$ is $\sin \omega$: $$\sin \omega = \frac{e^{i\omega} - e^{-i\omega}}{2i}$$ now consider the complex number $$s = i\sigma + \omega$$ which we will call the complex frequency . by the above , we have $$e^{ist} = e^{-\sigma t}e^{i\omega t} = e^{-\sigma t}\left ( \cos \omega t + i\sin \omega t\right ) $$ the real part is $$e^{-\sigma t}\cos \omega t$$ and the imaginary part is $$e^{-\sigma t}\sin \omega t$$ clearly , these are decaying ( damped ) oscillations which , as you might imagine , are very important in describing many physical systems . so , while it is possible to avoid using complex frequency , it is much less convenient . so when i read something about how fields fall into black holes and the modes consequently decay , why are the corresponding frequencies complex ? if the complex frequency is real , $\sigma = 0$ , there is no decay , no dissipation .
i would start by reading about carnot cycles and similar thermodynamic engines . this may help a bit : there is no fast , easy way to cool a vapor to its liquid state . once compressed , however , standard radiator technology is quite efficient at extracting heat from the liquid . in addition , since the liquid was heated during compression , it is at a higher temperature than the ambient atmosphere ( in the vapor state , it was a lot cooler , so heat transfer would have been very slow ) . large temperature differential means fast heat exchange . as to expansion : it is one of those facts of the world that free expansion and vaporization of the liquid absorbs energy from the liquid 's surroundings -- thus making the surroundings cold , which was the point of the whole system :- ) .
the normal force is a constraint force . it is whatever it needs to be to keep an object in contact with a surface . something like the top surface of the table i am at now can only apply a normal force in the " up " direction . if i push down on the laptop , the table will exert a normal force up to cancel my push and keep the laptop in the same place . if i lift up on this laptop , the table will not exert a normal force down to cancel my lift ; instead the laptop will accelerate up off the table . so for this table and laptop , the normal force is up . ( note , however , that this implies there is a normal force from the laptop on the table which points down . ) if i had something like a bead on a wire , the normal force would be the component of the force of the wire on the bead that points perpendicular to the tangent of the wire . that could be in any direction , depending on the geometry of the wire .
yes , of course ! this nevertheless suppose that no time is lost between the two gates , but it is a usual assumption in basic lectures . if the state vector $\left| \psi_0 \right&gt ; $ enters at the left of the circuit , then reach the gate a , represented by matrix $a$ , then the b one represented by matrix $b$ , and end up at the right of the logic circuit as $\left| \psi_1 \right&gt ; $ , then the final state is $\left| \psi_1 \right&gt ; = \left ( b \cdot a \right ) \left| \psi_0 \right&gt ; $ , since a applies first , then b . in your case , i suppose $\left| \psi_0 \right&gt ; = \alpha_{0000} \left| 0 0 0 0 \right&gt ; + \alpha_{0010} \left| 0 0 1 0 \right&gt ; + \alpha_{0100} \left| 0 1 0 0\right&gt ; + . . . $ has 16 entries , since you have 4 wires . each wire usually represents the time evolution of a single q-bit . a toffoli gate is usually a 3 q-bits gate , so you have to correctly generalise the toffoli gate given on the wikipedia page . well , it means that you must put extra diagonal 1 's in front of the untouched q-bit . you also have to change block-wise for the second gate i presume . ( please tell me if this last point is unclear for you , it is simpler to understand on a black-board than on internet actually :- ) ! )
to permanently magnetize them , you need to start with bb 's that are made of a ferromagnetic material , e.g. , iron ( or some kinds of steel ) , nickel or cobalt . this is the challenging part . when i was a kid they made bb 's out of copper . nowadays they probably make them out of some other less expensive but equally non-ferromagnetic material . assuming you can find a ferromagnetic bb , the next step is to subject it to a very strong magnetic field , which you could do with a homemade electromagnet . you can slightly magnetize the right object simply by subjecting it to high shock , for example , by striking it with a hammer ; but a blow strong enough to be of magnetic benefit might squash your bb . edit : was doing a little research to answer a comment , and discovered that those " copper " bbs i shot as a kid were actually copper-coated steel bbs . so , your magnetic bbs may be much easier to come by than i thought .
the attempt of obtaining $$ g^{ [ \mu\nu ] }_{ , \nu} +\frac{1}{2} ( {{\gamma}}^{\rho}_{\rho\nu}-{{\gamma}}^{\rho}_{\nu\rho} ) g^{ ( \mu\nu ) } =0 , $$ was almost right ! the only thing missing was a little care with relabeling indices . we will proceed in three main steps . 1 . ) so when contracting eq . ( 1 ) with respect to $\mu$ and $\rho$ , we get the identity : $$ -\frac{1}{2} g^{\rho \nu } {\gamma} _{a\rho}^{a}-\frac{1}{2} g^{\rho \nu} \gamma _{\rho a}^{a}+g^{a\nu} \gamma _{a\rho}^{\rho}+g^{\rho a} \gamma _{\rho a}^{\nu}+g_{ , \rho}^{\rho\nu}=0 . $$ now by relabeling the dummy indices in the 3rd term as $ a \leftrightarrow \rho$ , we get that the 3rd term can be written as $g^{\rho \nu} \gamma _{\rho a}^{a}$ . moreover , we can see that now the 2nd and 3rd term can be simplified : adding them together gives $+ \tfrac{1}{2}g^{\rho \nu} \gamma _{\rho a}^{a}$ . by a final index label change $\nu \to \mu$ , we get that : $$ -\frac{1}{2} g^{\rho \mu } {\gamma} _{a\rho}^{a}+\frac{1}{2} g^{\rho \mu} \gamma _{\rho a}^{a}+g^{\rho a} \gamma _{\rho a}^{\mu}+g_{ , \rho}^{\rho\mu}=0 . \ ; \ ; \ ; \ ; ( a ) $$ 2 . ) when contracting eq . ( 1 ) with respect to $\nu$ and $\rho$ , we get the identity : $$ -\frac{1}{2} g^{\mu \rho } \gamma _{a\rho}^a-\frac{1}{2} g^{\mu \rho} \gamma _{\rho a}^a+g_{}^{a\rho } \gamma _{a\rho}^{\mu }+g^{\mu a} \gamma _{\rho a}^{\rho }+g_{ , \rho }^{\mu \rho }=0 . $$ let us also rename the dummy indices in the 4th term as $ a \leftrightarrow \rho$ . we can see now that the 4th term is simply $g^{\mu \rho } \gamma _{a\rho}^a$ , and the 1st and 4th term hence together give $\frac{1}{2} g^{\mu \rho } \gamma _{a\rho}^a$ . moreover , let us perform also the $ a \leftrightarrow \rho$ " dummy index relabeling " , yielding $g_{}^{\rho a} \gamma _{\rho a}^{\mu }$ for the 3rd term . after these manipulations our identity reads as $$ \frac{1}{2} g^{\mu \rho } \gamma _{a\rho}^a-\frac{1}{2} g^{\mu \rho} \gamma _{\rho a}^a+g_{}^{\rho a} \gamma _{\rho a}^{\mu }+g_{ , \rho }^{\mu \rho }=0 . \ ; \ ; \ ; \ ; ( b ) $$ 3 . ) now taking ( b ) - ( a ) , we obtain : $$ g_{ , \rho }^{\mu \rho } -g_{ , \rho}^{\rho\mu} + \frac{1}{2}\left ( g^{\mu \rho } + g^{\mu \rho }\right ) \left ( \gamma _{a\rho}^a - \gamma _{\rho a}^a \right ) =0 , $$ which after the $a \to \rho$ and $\rho \to \nu$ relabeling is exactly the same as the desired eq . ( 2 ) .
since graphene exists , there is no limit to how spherical you can ma ke a buckyball by angle-measure . you can just take a flat sheet of graphene with a tiny number of defects ( or just a tensile stress ) and make an enormous sphere , as large as you like . the statement that the icosahedron is the " most spherical " refers to its symmetry group , which is the largest discrete subgroup of so ( 3 ) . any other near-sphere carbon structure would have equal symmetry or less , and the macroscopic graphene balls will have no exact symmetry at all . later edit : as per peter shor 's comment , tensile stress alone can not do it , since the graphine structure embeds in a tiling of the plane by triangles , and you can not tile a sphere by triangles fitting together in 6 's without at least some 5-vertices , because of the euler characteristic triangulation constraints v-e+f=2 / 3f=2e . there must be defects in the lattice , whose local curvature , positive and negative , end up totaling the euler characteristic . this is certainly possible , but it requires special defect points of a certain density to avoid the tensile stress of any region getting too great , and the problem is more involved than noting the planar limit exists .
your what happens in high-energy physics experiments aside partially contradicts your final question but if we restrict the discussion to radioactive decay , fusion in stars , cosmic rays , is everything a lepton , baryon , or a photon ? radioctive decay has as end products photons , leptons and baryons . fusion and cosmic rays are the realm of elementary particle physics , the energies involved much higher than the ones in natural radiaoctivity . the reality of what " everything is made up of " depends on the energy with which you look at " everything " . the answer is that everything is made up by the elementary particles , following the rules of the standard model , nuclear models , atomic models as the energies involved in " looking " at everything diminish . it is a compositeness built up consecutively . you might be interested in this answer to a similar question . these are the elementary particles out of which all matter is formed . every day matter involve mainly the first column and the last column . the two middle ones have been found in cosmic rays to start with and in accelerator experiments that led to the discovery of the standard model . they are particles that cannot come out from nuclear decays or fissions , i.e. " naturally " but need excess energy to materialize .
you are probably dividing by $\alpha$ at some point to eliminate a global phase , leading to your divide by zero in some cases . it would be better to get the phase angles of $\alpha$ and $\beta$ with $\arg$ , and set the relative phase $\phi=\arg ( \beta ) -\arg ( \alpha ) $ . angle $\theta$ is now simply extracted as $\theta = 2\cos^{-1} ( |\alpha| ) $ ( note that the absolute value of $\alpha$ is used ) . this is all assuming that you want to get to $$|\psi\rangle = \cos ( \theta/2 ) |0\rangle + \mathrm{e}^{i\phi}\sin ( \theta/2 ) |1\rangle\ , , $$ which neglects global phase .
the derivative on the $\hat \phi$ 's are taken with respect to $y$ in your formula , so using $$ [ a , b^2 ] = [ a , b ] b+b [ a , b ] $$ and $$ \bigl [ \frac{\partial \hat \phi ( t , \vec y ) }{\partial y^i} , \hat \phi ( t , \vec x ) \bigr ] = \frac{\partial}{\partial y^i}\bigl [ \hat \phi ( t , \vec y ) , \hat \phi ( t , \vec x ) \bigr ] =0 $$ you get your result . edit : let me comment on why does the above identity holds ( the last equality ) . we know that for spacelike separated points $x$ and $y$ , the commutator $\bigl [ \hat \phi ( y ) , \hat \phi ( x ) \bigr ] $ vanishes . this happens in particular if $x^0=y^0=t$ and $\vec x \not = \vec y$ , showing the result in this case . moreover , when $\vec x = \vec y$ , then the result holds as well since the commutator of an operator with itself is always zero , completing the proof . i hope this is clear !
simple way to solve : use distances travelled . so b has already travelled 20km at 15 east of north . that is $20 \times \sin ( 15 ) $km northwards and $20 \times \cos ( 15 ) $ eastwards . now consider a intercepts b at a time $t$ . the equations for the distances travelled by b are : $y_b = 20 \times \sin ( 15 ) + 30 \times t \times \sin ( 40 ) $ in the y direction and $x_b = 20 \times \cos ( 15 ) + 30 \times t \times \cos ( 40 ) $ in the x direction . for a , the distances travelled are : $y_a = 100 \times t \times \sin ( \alpha ) $ in the y direction and $x_a = 100 \times t \times \cos ( \alpha ) $ in the x direction . for the two to intercept , $x_a = x_b$ and $y_a = y_b$ you now have 2 equations , 2 variables , and a whole lot of fun solving them simultaneously .
in physics , temperature and other concepts in " thermodynamics " ( that was known for centuries from macroscopic analyses of the heat engines and similar systems ) is given by a more fundamental theory , the so-called " statistical mechanics " . according to statistical mechanics , the thermal phenomena are explained by the motion of the atoms and various states in which the atoms may be found ( and the number of these states ) . in particular , the probability $p_k$ of a state $k$ ( in classical physics , the state is described e.g. by the location and velocity of each particle ) is given by $$ p_k = c \exp ( -e_k/k_bt ) $$ where $e_k$ is the energy of the state $k$ , $k_b$ is boltzmann 's constant converting kelvins to joules , and $t$ is the absolute temperature in kelvins . the coefficient $c$ is a " normalization factor " that is $k$-independent and chosen so that the sum of $p_k$ over $k$ is equal to one ( the total probability ) . this form makes it clear that $t\lt 0$ is not allowed : the exponential would be growing with $e_k$ and because there are infinitely many states with ever larger values of $e_k$ ( the kinetic energy may grow arbitrarily high , in particular ) , the probabilities would be getting larger and their sum would diverge : it could not be normalized to one . before this statistical explanation involving boltzmann 's constant was known , the temperature was a phenomenological quantity measured by a thermometer . one was actually uncertain about any redefinition $t\to f ( t ) $ where $f ( t ) $ is a monotonically increasing function . in principle , one may relabel $t$ so that zero kelvins gets mapped to $-\infty$ in another convention for the temperature , for example ; try $t_\text{new convention} = \ln ( t ) $ . however , the ideal gases obeyed $pv = nrt$ so at a fixed pressure , the volume of some gas was proportional to the absolute temperature  the same one as one in statistical mechanics , without any redefinition by a function $f$ . so people knew how to measure the " right absolute temperature " even well before statistical mechanics was understood . the usual thermometers relied on the expansion of liquids etc . which are not ideal gases but they are close enough . for ideal gases , where the absolute temperature is proportional to the volume , the statement that $t\gt 0$ is equivalent to the statement that the volume of the ideal gas cannot be negative . you cool it down and it shrinks but it can not shrink below zero . volume is about the " shape " but the underlying reason for the positivity of temperature is not about locations ; it is about the motion . any physical object with quadratic degrees of freedom will carry $k_bt/2$ of kinetic energy per degree of freedom . again , because the quadratic kinetic energy of the type $mv_x^2/2$ can not be negative , the absolute temperature can not be negative , either . in lasers and similar devices , one may formally find negative absolute temperatures when the number of atoms at a higher energy level is greater than the number of atoms at a lower energy level . however , this negative temperature can not be brought to equilibrium with all degrees of freedom in a larger object because the number of high-energy states is always divergent . in lasers , one kind of abuses the fact that that the energy of the " interesting degrees of freedom " is bounded both from below and from above ( we only allow two or few levels for each atom ) .
in the case of ice , the hydrogen and oxygen atoms are actually sharing electrons . they are bonded " covalently " . each oxygen is bonded to two particular hydrogens , and so you can divide the atoms into separate groups : this oxygen is bonded to these hydrogens , and that one to those , and so on . that is not the case in nacl , where really the different atoms are all attracted to each other electromagnetically . no sodium atoms are actually bonded to any particular single chlorine atom , so it makes no sense to speak of one of the molecules in a crystal .
not so much an answer to your question but you might want to check out 2t physics , the work of itzhak bars et . al . , e.g. , http://arxiv.org/abs/hep-th/0610187 http://inspirehep.net/search?ln=enln=enp=find+t+2t+physicsof=hbaction_search=searchsf=so=drm=rg=25sc=0
the formulae from the nasa document and wikipedia are simply in different frames : the ones in wikipedia assume coordinate frame whose axes pass through the center of mass whereas the nasa document assumes arbitrary cartesian coordinate frame . let capital x i , y i and z i denote coordinates of i-th atom in an arbitrary cartesian coordinate frame o and x i , y i and z i denote respective coordinates in the frame o com translated to the center of mass of the molecule . the coordinates of the center of mass of the molecule in the o frame are : \begin{equation} x_{com} = \frac{1}{m}\sum_{i} m_i x_i \\ y_{com} = \frac{1}{m}\sum_{i} m_i y_i \\ z_{com} = \frac{1}{m}\sum_{i} m_i z_i \end{equation} transformation from o to o com is simply translation by [ -x com , -y com , -z com ] : \begin{equation} x_i = x_i - x_{com} = x_i - \frac{1}{m}\sum_{i} m_i x_i\\ y_i = y_i - y_{com} = y_i - \frac{1}{m}\sum_{i} m_i y_i\\ z_i = z_i - z_{com} = z_i - \frac{1}{m}\sum_{i} m_i z_i \end{equation} now , take for example the formula for i xx in the o frame : \begin{equation} i_{xx} = \sum_{i} m_i ( y_i^2 + z_i^2 ) \end{equation} ( note that the two-term sum in the parenthesis is simply the euclidean distance between the i-th atom and the x axis , exactly as one would expect . ) substituting the formulae for y i and z i above we have \begin{equation} i_{xx} = \sum_{i} m_i [ ( y_i - y_{com} ) ^2 + ( z_i - z_{com} ) ^2 ] \\ i_{xx} = \sum_{i} m_i ( y_i^2 + z_i^2 ) + \sum_{i} m_i ( y_{com}^2 + z_{com}^2 ) - 2 \sum_{i} m_i ( y_i y_{com} + z_i z_{com} ) \\ i_{xx} = \sum_{i} m_i ( y_i^2 + z_i^2 ) + m ( y_{com}^2 + z_{com}^2 ) - 2 m ( y_{com}^2 + z_{com}^2 ) \\ i_{xx} = \sum_{i} m_i ( y_i^2 + z_i^2 ) - \frac{1}{m} ( \sum_{i} m_i y_i ) ^2 - \frac{1}{m} ( \sum_{i} m_i z_i ) ^2 \end{equation} which is exactly the formula for i xx in the nasa document . analogously for the other formulae . interestingly , the sum of the last two terms in the final formula is exactly what one would expect from the parallel axis theorem . the conclusion is that you should use whichever formulae fit your coordinate frame . if the center of your coordinate frame coincides with the center of mass of the molecule , then you can use the simpler formulae from wikipedia . otherwise you should use those from the nasa document .
the most obvious interpretation of the phrase blow up the earth is to dismantle it into tiny particles headed off to infinity . if you are prepared to accept this definition then the calculation is easy because it is ( approximately ) the gravitational binding energy for matter with the mass of the earth falling into a sphere the size of the earth . i say approximately because i am ignoring the ellipticity of the earth and the fact it is rotating , and i am assuming it is a uniform density throughout . the gravitational binding energy of a uniform sphere is : $$ u = \frac{3gm^2}{5r} $$ and for the earth this works out as 2.24 $\times$ 10$^{32}$j . according to the wikipedia article on nuclear weapons , the federation of american scientists estimates there are more than 17,000 nuclear warheads in the world as of 2012 , with around 4,300 of them considered " operational " , ready for use . let 's take the average yield to be one megaton ( almost certainly an overestimate ) , which is 4.184 $\times$ 10$^{15}$j . in that case the total energy of all 17,000 bombs is about 7 $\times$ 10$^{19}$j or about a factor of 3 $\times$ 10$^{-13}$j smaller than the gravitational binding energy . i suppose another interpretation of blow up the earth would be to render it uninhabitable . an obvious reference point for this is the meteor collision that caused the extinction of the dinosaurs . wikipedia estimates this as 4.2 $\times$ 10$^{23}$j , or about a factor of ten thousand greater than all the current nuclear bombs .
before this question gets closed , let me quickly post an answer . first of all , i must warn you , " learn it in order . " . you know those popular - science , math - free books ? stay away from them . they give you the wrong impression that physics is math-free , and that by reading that , you will already know a lot , etc . , but that is false/ . popular science will not gett you anywhere . secondly , what do i mean by " learn it in order . " ? i mean , that to learn physics , you must go like this : kinematics newtonian mechanics lagrangian mechanics hamiltonian mechanics newtonian gravity coulomb 's electricity maxwell 's electromagnetism special relativity general relativity old quantum theory heisenberg matrix quantum mechanics schrodinger wave mechanics feynman ( ? ) path integrals klein - gordon equation dirac equation weyl equation spinors variational formulation of quantum mechanics quantum electrodynamics yang - mills theory quantum chromodynamics electro - weak theory higgs mechanism standard model string theory now , what about some good sources to learn all of theseq ? kinematics , newtonian mechanics jewett and serway physics for scientists and engineers with modern physics lagrangian mechanics , hamiltonian mechanics wikipedia , wikipedia newtonian gravity , coulomb 's electricity , maxwell 's electromagnetism , special relativity jewett and serway physics for scientists and engineers with modern physics general relativity ludvigsen general relativity : a geometric approach and wikipedia and also some videos i have made old quantum theory wikipedia , wikipedia heisenberg matrix mechanics iit nptel lectures , wikipedia , some videos of mine schrodinger wave mechanics iit nptel lectures , some videos of mine feynman ( ? ) path integral mechanics feynman , hibbs ( styer ) quantum mechanics and path integrals , some videos of mine . klein - gordon equation , dirac equation , weyl equation , spinors wikipedia , wikipedia , wikipedia , some videos of mine yang - mills theory , quantum chromodynamics , electroweak theory , higgs mechanism , standard model wikipedia , wikipedia , wikipedia , some videos of mine string theory see this . for the math , see this .
a mis-steered beam at cebaf simply cut a hole thought the niobium wall of the klystron and flooded half the accelerator with helium ( super-conducting klystrons need a liquid helium jacket to work . . . ) . we were down for more than a week . that is an electron beam machine , and very high current ( up to 400 micro-amps ! ) , so the details would be rather different than the lhc beam . likewise , about $60\text{ }\mu\text{a}$ of $5.5 \text{ gev}$ beams from that machine partially melted one of my iron targets ( thick enough that about 6% of the beam interacted with the target ) despite a raster spreading the beam over roughly $2\text{ mm}^2$ , because we did not have good enough thermal contact with the water-cooled frame of the target ladder . high energy , high current beams can carry a lot of power . back to the question as asked : treat your hand as water . the particle data booklet puts the energy loss per proton at around $2.5\text{ mev/g/cm}^2} or something like 4--8 mev though your hand , depending on how chubby you are . ( we are only a few orders of magnitude above the minimum ionization energy , so this is not very sensitive to the actual beam energy . ) phillip says $1.2 \times 10^{11}$ protons per beam in the ring , about 11,000 passes per second ( $3.0 \times 10^8\text{ m/s} / 27\text{ km}$ ) , so $1.3 \times 10^{15}$ protons per second is $8 \times 10^{15}\text{ mev/s} = 1300\text{ j/s}$ is a fair bit of heat , and results in heating of about $300\text{ k/s/ ( cubic cm exposed ) }$ . to finish up here we will have to know something about the beam diameter . your reflex time to move your hand is on order of 0.1--0.2 seconds . it is going to hurt : you will get badly burned , and the damage will extend though the whole depth of the exposed flesh , rather than being limited to the surface as with the contact burns we are all familiar with .
yes , your second guess is more or less correct . in gr , perturbing the metric is the usual way of doing perturbation theory . one writes for the true metric $g_{\mu\nu}$ an expansion of the form $$ g_{\mu\nu} = g^{ ( 0 ) }_{\mu\nu}+h_{{\mu\nu}}+o ( h^2 ) , $$ where $g^{ ( 0 ) }_{\mu\nu}$ is known and usually called background metric . one then substitutes this into the einstein equations and find equations for $h_{\mu\nu}$ . solving those then gives you the first order correction to the background metric .
it is a theoretical demand : $$ \begin{pmatrix} \nu_{e}\\ \nu_{\mu}\\ \nu_{\tau} \end{pmatrix} = \begin{pmatrix} u_{e1} and u_{e2} and u_{e3} \\ u_{\mu1} and u_{\mu2} and u_{\mu3} \\ u_{\tau1} and u_{\tau2} and u_{\tau3} \end{pmatrix} \begin{pmatrix} \nu_{1}\\ \nu_{2}\\ \nu_{3} \end{pmatrix} $$ you know that all states are normalized , for example : $\nu_{e}|\nu_{e}=1= ( u_{e1}^{*}\nu_{1}|+u^{*}_{e2}\nu_{2}|+u^{*}_{e3}\nu_{3}| ) ( u_{e1}|\nu_{1}+u_{e2}|\nu_{2}+u_{e3}|\nu_{3} ) $ so $u_{e1}^{*}u_{e1}+u_{e2}^{*}u_{e2}+u_{e3}^{*}u_{e3}=1$ you can do the same for the whole matrix and find $u^{+}u=i$ edit : as dmckee pointed out it is a general feature in quantum mechanics , the matrix you use to change the basis ( here from mass eigenstate to flavour eigenstate ) must be unitary .
since for some reason this question has resurfaced , i would like to point to a similar one posed later than this . observation of change is important to defining a concept of time . if there are no changes , no time can be defined . but it is also true that if space were not changing , no contours , we would not have a concept of space either . a total three dimensional uniformity would not register . our scientific time definition uses the concept of entropy to codify change in space , and entropy tells us that there exists an arrow of time . in special relativity and general relativity time is defined as a fourth coordinate on par with the three space directions , with an extension to imaginary numbers for the mathematical transformations involved . the successful description of nature , particularly by special relativity , confirms the use of time as a coordinate on par with the space coordinates . it is the arrow of time that distinguishes it in behavior from the other coordinates as far as the theoretical description of nature goes .
a couple of things : 1 ) in the way you phrase it , it will almost certainly not be possible to disprove cosmic censorship , since the singularity you could start with could be , for example , a kerr solution with $a&gt ; m$ , in which case , the horizon that would typically appear at $r=m\pm \sqrt{m^{2}-a^{2}}$ would disappear , but there would be no apparent contradiction with einstein 's equation whatsoever , as these solutions are perfectly valid solutions of einstein 's equation . it is the evolution of such a solution at " late " times from " physically reasonable " matter without any black holes or horizons which is the issue . 2 ) there are fine-tunings of physically realistic initial data where cosmic censorship is false . this was originally discovered by numerical studies by choptuik regarding spherically symmetric collapse of klein-gordon fields . modern restatings of cosmic censorship attempt to show the much weaker claim that the complement of the set of initial data that evolves into naked singularities forms a dense subset of the set of all initial data . in other words , the claim is that you have to do an exact , and unstable , fine-tuning of initial data in order to produce a naked singularity .
to consider an example , take the case of exponential decay $$n=n_\circ e^{-\lambda t}$$ we can write this as \begin{eqnarray*} n and = and \frac{n_{\circ}}{e^{\lambda t}}\\ and = and \frac{n_{\circ}}{\underbrace{e\times e\times e\times e\times\ldots \times e}_{\lambda t\text{ times}}} \end{eqnarray*} so $\lambda t$ must be a dimensionless term that is telling how many times we should multiply $e$ by itself . thus , $\lambda t$ must be dimensionless " overall " . individually , $\lambda$ has the dimensions of $ [ t^{-1} ] $ which cancels with $t$ to give a net dimensionless quantity . $\underbrace{e\times e\times e\times \ldots}_{10 \text{ meters times}}$ makes no sense mathematically . we could have taken a dimensional quantity instead of $e$ but the exponent $\lambda t$ would still be dimensionless . eg in the kinematical equation $s=ut + \frac 12 at^2$ , $t^2$ has the dimensions of $ [ t^2 ] $ but the exponent $2$ is dimensionless . the same applies to transcendental functions i.e. logarithmic , trigonometric , etc .
you can think of light as the carrier of the electromagnetic interaction . the particles interact with light , not directly with each other . it is an experimental fact that light does not interact with itself . note that this is not the case with quantum chromodynamics ( the theory of nuclear matter ) . this theory is built along the same lines as quantum electrodynamics ( qed ) but the gauge field is non abelian this time . the field is represented by matrices ( instead of being a vector for qed ) which do not commute with each other . this makes it necessary to consider field-field interactions . we get objects such as glueballs : bound states of field excitations only .
unless you deal with holograms , all displays we are observing follow the laws of ray optics . so the display has some specific location  for example , rectangle between four points with certain coordinates  and all the light rays leaving from the points of the display carry the information about their origin because they are diverging from that point , and if one extrapolates several rays associated with the same " pixel " on the display , they actually intersect in the pixel that created the light . the human eye , when properly focused , changes the direction of these divergent light rays by refraction so that they converge again and intersect on the retina . so a sharp pixel on the display produces a sharp pixel on the retina . when it is not so , because of myopia , astigmatism , or any condition that prevents the eye from accurate focusing  and believe me , i also know something about it  it just means that the light rays emitted by one pixel on the display are attempting to " reconverge " but they do not intersect at one point of the retina . myopia means that the eye works a little too much in trying to " reconverge " the light rays . as a consequence , the intersection of the light rays from one pixel appears inside the liquid in the eye , before the rays reach the retina . and the signal on the retina is inevitably fuzzy  a disk of a sort  even though the source of the light is one pixel . hyperopia works in the other way around ; the light rays try to " reconverge " but the refraction is too weak and the intersection would only occur " behind " the retina ( in the brain ) which does not physically occur because the photons are absorbed before they reach the intersection . astigmatism means that the light rays emitted into different directions horizontally and the light rays emitted into different directions vertically experience different amounts of refraction . so there is no " common intersection " of all the light rays coming from one display pixel at all . if you only consider light rays differing in the " vertical " direction , they intersect at one point ; the light rays differing in the " horizontal " direction intersect at another distance from the retina ( positive or negative ) . so one is more myopic for horizontal lines or more myopic for vertical lines or more farsighted for horizontal lines or . . . and so on . if a display pixel emits light rays according to the laws of geometric optics  and it is true for crt monitors , lcd panels , led , oled , plasma tv , or anything else of this sort  the light rays simply carry the information about the actual distance and this information can not be fooled . the image on the retina is blurred and when it is blurred , it just can not ever be unblurred ( without a modification of the eye 's refraction , by glasses or lens etc . ) . this is a completely general claim ( for generic images ) . blurring is " irreversible " ( in the same sense as the increasing-entropy phenomena in thermodynamics such as heat diffusion ) because one is losing the detailed sharp information . even if one tried to apply some " sharpening " tools , they would have to work within/near the eye , not on the display 's side . what the unfocused eye sees is always " more blurry " ( analogous to " higher entropy" ) and there is no way to produce sources of light from many pixels that would end up as one pixel on the retina , for example . a normal display can not " fake " the actual point where the photons originate  in this respect , it is completely analogous to all other objects we routinely observe with our eyes . however , holograms can do it . you could construct holographic tvs based on the wave optics which can reconstruct the configuration of photons that is equivalent to an arbitrary arrangement of objects in 3d , between the plate and the human eye . note that holography is not some cheap "3d effect " depending on the fact that we are observing things by two eyes , from two different points . instead , holography creates genuine 3d images so the eyes must individually focus at various distances , depending on the distances of parts of the objects seen on the hologram , and one may actually see how the 3d object looks like from all directions ( something certainly impossible for stereographic "3d " technologies depending on having two images only ) . in this way , you could create light rays that apparently originate from points that are closer to the human eye than the plate . this is still insufficient for astigmatism because astigmatism is an " inconsistency " in the required distance of the objects that the eye is able to see sharply . however , in principle , it should be possible to produce holograms that may be seen sharply by an astigmatic eye , at least if the astigmatism is sufficiently " weak " or " perturbative " . some complicated calculation would have to be done to produce the right hologram that looks sharp to a general enough astigmatic eye .
yes , from the transformation law for the four-velocity , we can explicitly derive the transformations of three-velocities parallel to and perpendicular to a given boost . first , we need to talk about what the transformation law is for the four-velocity with respect to a boost . let the four-velocity be $u = ( u^t , u^x , u^y , u^z ) $ . let 's boost this along the x-direction by a speed $\mu c$ . like any four-vector , the four-velocity transforms under lorentz transformations like so : $$\begin{align*} {u'}^t and = w ( u^t - \mu u^x ) \\ {u'}^x and = w ( u^x - \mu u^t ) \\ {u'}^y and = u^y \\ {u'}^z and = u^z\end{align*}$$ where $w = 1/\sqrt{1-\mu^2}$ is the lorentz factor of the boost . now , break down the original four-vector $u$ as $u= \gamma c ( 1 , \beta^x , \beta^y , \beta^z ) $ . we can find the components of $u'$ as $$\begin{align*} {u'}^t and = w\gamma c ( 1 - \mu \beta^x ) \\ {u'}^x and = w \gamma c ( \beta^x - \mu ) \\ {u'}^y and = \gamma c \beta^y \\ {u'}^x and = \gamma c \beta^z\end{align*}$$ you can then find the components of three-velocity in the primed frame by taking ${u'}^i/{u'}^t$ . $$\begin{align*} {v'}^x and = \frac{{u'}^x}{{u'}^t} = \frac{\beta^x - \mu}{1 - \mu \beta^x} \\ {v'}^y and = \frac{{u'}^y}{{u'}^t} = \frac{\beta^y}{w ( 1-\mu \beta^x ) } \\ {v'}^z and = \frac{{u'}^z}{{u'}^t} = \frac{\beta^z}{w ( 1-\mu \beta^x ) } \end{align*}$$ these are algebraically the same as the formulas you posted in ( a ) and ( b ) . to me , this is much simpler than churning through velocity-addition . the transformation laws of four-vectors are simple to learn , and the amount of manipulation needed to find the new three-velocity is minimal . *note : your terminology on parallel vs . perpendicular to the boost seems confused . nevertheless , i believe my results here capture what you intended . the boost velocity is in the direction of the x-axis .
you have asked the $64k question here so it is hard to give a " real answer " here because i do not think anyone knows the answer . you could almost re-phrase your question and ask " is there a " solid " unitary anything ? how can you have something that is not made of something even smaller than itself ? feynman said that nature may be like an onion with an infinite number of layers . you can peel away one layer only to find another layer .
i am a bit rusty on my qed , but i will give this a shot . the simplest case would be described by a diagram similar to : but the $e^--e^--\nu_e$ vertex does not exist ( also note that i can not draw the required arrow on the neutrino ) - the vertices of the standard model ( with the exception of vertices involving the higgs and neutrino oscillations ) are : with these , the closest interaction to what you describe that i can see is : there are virtual neutrinos as you specified , but also virtual $w$ bosons . if you rotate that diagram 90 degrees , there is an $e^--e^+$ scattering mediated by virtual neutrinos and $w$ bosons , but again , not quite what you asked for .
here pressure is hydrostatic pressure . http://en.wikipedia.org/wiki/buoyancy#simplified_model look at the above link of wikipedia . it is well explained there with cartoons . pressure is not vector , it is a scalar quantity . also , have a look here for fluid statics : http://en.wikipedia.org/wiki/fluid_statics#pressure_in_fluids_at_rest
if you want to work as a theoretical physicist , it would be advisable to get a little bit of grounding in experimental physics anyway . so my answer to #3 is , if you want to get into theoretical physics , get a bachelor 's in physics , not mathematics , and take at least one or two experimental courses . i work at a university where students often do shoddy work in the undergraduate experimental courses , because " i came here to study theoretical physics and i am not interested in experiments . " what they do not realize is that most of our professors in theoretical physics are of the opinion that to be an excellent theoretician , you primarily need to be a well-rounded physicist , with both theoretical and experimental skills . these students run into trouble when they are looking for internships or final projects , because no professor will accept them . ( i edited this answer , because i did not mean to imply that you had to be an excellent experimenter to be a good theoretician . if you do not enjoy experimenting , then you do not enjoy experimenting . just do not dismiss it or ignore it altogether . ) from saturday morning breakfast cereal :
is the universe infinite ? or is it finite ? you can relate observational measurements to this question by making an assumption . namely , if we assume that the cosmological principle holds , then the curvature of the universe that we have measured from our earthly vantage point is true throughout the universe . this allows to extrapolate our measurement of the local curvature so as to make predictions about the global shape of the universe . if the curvature is $\omega &gt ; 1$ , then the universe 's spatial shape can be thought of as the equivalent of the surface of a sphere in 4-d minkowski space . in this case it is finite . if we have $\omega &lt ; 1$ then it has a hyperbolic shape , making it infinite . if $\omega = 1$ then it is flat , also making it infinite . i will not go into the subtleties of discussing the shape of the universe in cases where curvature is not constant or not necessarily the same in all directions , suffice it to say that things can get more intricate than this simple 3-way scenario . current observations do not actually allow us to distinguish between these 3 scenarios as we currently roughly measure $\omega = 1 \pm 0.01$ . this value that is very close to flatness is speculated to be due to inflation , but that is another topic for another time . but if it is finite , the idea that we can not go beyond a certain space just creeps me out . this is just plain wrong . if the universe is finite that does not mean that there exists a clear cut " boundary " to it . indeed , that would be quite odd as you rightfully point out . using the example of a finite space given above , you can see how a universe can be finite without running into this problem . imagine the surface of a sphere ( i stress surface and not the space inside it ) , like say the surface of the globe . it is a finite space , in that it has a finite surface , that does not however require it to have a limit point where you would " exit " the space were you to go beyond it . you can easily generalize this to higher dimensions . note that the extra dimension used here ( we imagined a 2-d surface by invoking the surface of a 3-d object ) is merely a pedagogical and does not actually have to exist . imagine instead a 2-d world map to realize that there is no obligation to invoke an extra dimension to describe this space , it just makes it easier to visualize . hope that answers your question and you are not as creeped out anymore .
there exist pictures of positron annihilations and creation of electron positron pairs . here is one : a positron in flight annihilates with an electron into two gammas , which are invisible . one of them materializes at a certain distance from the track stop , resulting in a new electron-positron pair ( marked with green ) these are taken in bubble chambers with a magnetic field perpendicular to the plane . what you are photographing is much more like what is seen in nuclear emulsions . chemical and biological energies are of the order of ev . to create a positron with a mass of ~500 . kev is not possible . from what i see the tems has at most kev energies in the electron beam you may be seeing muons from the continuous muon background at sea level . the flux if i remember correctly is 1 per centimeter square per second ( all energies ) . these could kick off electrons and even have enough energy to create electron positron pairs but it would not be a repeatable phenomenon since the flux is random . also to see how elementary particle tracks would look in your material you would need to calibrate it at some accelerator lab .
the key concept is small oscilation . it is very easy to see this via lagrangian formalism , so we write the lagrangian for our problem $$ l ( x , v ) =\frac{1}{2}mv^2-w ( x ) $$ we expand the potential in series about the equilibrium point $x_0$ so that $x-x_0$ is a small oscillation in some way , say $\displaystyle\frac{x-x0}{x}&lt ; &lt ; 1$ $$ w ( x ) =w ( x_0 ) +w' ( x_0 ) ( x-x_0 ) +\frac{1}{2}w'' ( x-x_0 ) ^2 +o ( ( x-x_0 ) ^3 ) $$ why truncate the series at $ ( x-x_0 ) ^3$ . because $ ( x-x_0 ) ^2$ gives the first non trivial contribution : $w ( x_0 ) $ is a constant , and it does not contribute to the equations of motion , because the motion is governed by the derivatives of the lagrangian . on the other hand $w' ( x_0 ) =0$ by hypothesis so we can write $$ w ( x ) \approx \frac{1}{2}w'' ( x_0 ) ( x-x_0 ) ^2 $$ and hence $$ l ( x , v ) =\frac{1}{2}mv^2-\frac{1}{2}w'' ( x_0 ) ( x-x_0 ) ^2 $$ this is the lagrangian for a spring with constant $w'' ( x_0 ) $ . remember the notation meaning , $w'' ( x_0 ) $ it is a number , the second derivative evaluated at $x_0$ , it is not a function .
i think that there is two levels of answer in this question , whether we talk about an exact scheme ( the rg is one in principle ) , or about the practical implementation/calculation . if one could implement the rg scheme exactly , one would capture emergence , since this is equivalent to solving the problem exactly . so , if you know the correct question , that is , if you know what is the correlation function you need to look at to observe this emergence , then this exact implementation of the rg will give you the correct answer . however , the correct observables might be very different from the one you would guess from your microscopic theory , and it might be very complicated to find what is the correlation function you need to look at . related to that is the practical implementation of the rg , where you expect some non-trivial flow ( most probably non-perturbative ) that you might not be able to take into account . then you must guess what the emergent degrees of freedom are , write a qft for them , and start your rg all over again . however , one should keep in mind that there are non-perturbative implementations of the rg that can do most of what is described here , at least for some specific problem . the perturbative rg is not all that there is ! two examples . 1- the classical xy model in 2d . it is well known that the dominant contribution to the physics are given by the ( un ) -biding of the vortices , which are very unlocal objects . the usual approach is to start from your spin hamiltonian / field theory , do the transformation to the vortices degrees of freedom , which then give a theory which looks like a 2d coulomb gas , and then do the pertubative rg from there . however , by implementing a non-perturbative rg ( nprg ) scheme , which is described only by the hamiltonian of the spins , one can describe most of the qualitative and quantitative feature of the xy model without ever including the vortices by hand ! see for example the review arxiv:0005.122 , as well as arxiv:1004.3651 . 2- in the case of a quantum model , one can describe the bose-mott transition in the bose-hubbard model using symmetries argument to find what are the two universality classes of the transition . one then introduce a low energy action with some unknown effective parameter to describe the low-energy physics close to the critical point . one cannot perturbatively start from the microscopic action to compute the effective action because the system is on the lattice and strongly coupled . however , using the nprg , one can effectively do this calculation and explicitly find the low-energy degrees of freedom as well as the effective parameters starting from the microscopic action , see for example arxiv:1107.1314 . nevertheless , i should tell that in this case , this is not a strong form of emergence ( the low-energy degrees of freedom are not very exotic , the fixed points are well known , etc . ) , but still , one can do a lot with the rg !
it has been engineered , based on observations hair patterns of insects ( droplet slides down when substrate is oriented so that the hairs point downwards , while it was attached in the first two orientations ) in nature , it is useful e.g. for butterflies who need to expel water droplets from their wings : the droplets slide out thanks to the outward pointing hair .
take the ansatz \begin{align} \vec{r} ( \theta ( t ) ) = \begin{pmatrix} \rho_1\cos ( \theta ( t ) ) \\ \rho_2\sin ( \theta ( t ) ) \end{pmatrix} \end{align} with a yet unknown scalar smooth function $\theta$ . solve the ode $$ k \bigg ( \left|\vec{r}\big ( \theta ( t ) \big ) \right|\bigg ) ^{\beta} = |\vec{r}' ( \theta ( t ) ) | \cdot\dot\theta ( t ) $$ for $\theta$ or explicitely $$ \dot\theta ( t ) = \frac{k\cdot\left|\vec{r} ( \theta ( t ) ) \right|^{\beta}}{|\vec{r}' ( \theta ( t ) ) |} $$ then use the found $\theta$ to calculate $$ \vec{a} ( t ) =\frac{d}{dt}\left ( \vec{r}' ( \theta ( t ) ) \dot\theta ( t ) \right ) . $$
the missing mass problems are several sets of observations that could be explained if there were some matter that has mass ( interacts with other matter via gravity ) but does not interact with light . the same distribution of this missing mass would explain all of them . all competitors that have been explored fail to explain at least one . i only partially understood the evidence explained in the wikipedia article , but let me try to summarize the major pieces of evidence that i understood . historically , the first piece of evidence is the galactic rotational curves . if you have an object with a mass distribution , you can predict different parts of it should rotate about the object 's center of mass . when astronomers apply this idea to galaxies , they find that the stars far away from the center move faster than they had expect based on the mass distribution that they can see . gravitational lensing is a result of general relativity . when one massive object is in front of another object , the mass from the front object distorts the light that arrives from the rear object . astronomers infer the mass of the front object by how the appearance of the rear object changes when the front object moves out of the way . general relativity is the only theory that has successfully described this phenomena , and it predicts that the mass must be there . finally , several experiments have measured the cosmic microwave background radiation in quite some detail . this background radiation looks almost , but not exactly , the same in all directions . this variation is called the anisotropy . i do not fully understand why , matter that interacts with light has a different signal in the anisotropy than matter that does not interact with light . so by measuring the anisotropy in enough detail , astronomers can infer how much normal matter there is , and how much dark matter there is . the dark matter community talks about " candidates " for dark matter . these are particles that could be the missing mass . the scientific american article you cite is about one experiment that is looking for a specific candidate . supersymmetric particles are another candidate for dark matter , not a competing theory . both of your other links set off a lot of red flags for me . the electric universe really looks like it is just a crank with a pet theory . it does not seem like it is even trying to address the anomalies that dark matter explains ; it is only trying to argue against black holes . their theory will not explain any of the missing mass anomalies . science 2.0 looks like it has a mix of good and bad content . the particular article you cited is about mond , modified newtonian dynamics . the idea is to modify either newton 's second law ( $f = ma$ ) or newton 's law of universal gravitation ( so that the force due to gravity would fall off as something other than $r^{-2}$ ) . mond was originally invented to explain galactic rotation curves , and it does that . its problem is that it does not explain , for example , gravitational lensing .
the answer is given by the covariant entropy bound ( ceb ) also referred to as the bousso bound after raphael bousso who first suggested it . the ceb sounds very similar to the holographic principle ( hp ) in that both relate the dynamics of a system to what happens on its boundary , but the similarity ends there . the hp suggests that the physics ( specifically supergravity or sugra ) in a d-dimensional spacetime can be mapped to the physics of a conformal field theory living on it d-1 dimensional boundary . the ceb is more along the lines of the bekenstein bound which says that the entropy of a black hole is proportional to the area of its horizon : $$ s = \frac{k a}{4} $$ to cut a long story short the maximum information that you can store in $1 cc = 10^{-6} m^3$ of space is proportional to the area of its boundary . for a uniform spherical volume , that area is : $$ a = v^{2/3} = 10^{-4} m^2 $$ therefore the maximum information ( # of bits ) you can store is approximately given by : $$ s \sim \frac{a}{a_{pl}} $$ where $a_{pl}$ is the planck area $ \sim 10^{-70} m^2 $ . for our $ 1 cc $ volume this gives $ s_{max} \sim 10^{66} $ bits . of course , this is a rough order-of-magnitude estimate , but it lies in the general ballpark and gives you an idea of the limit that you are talking about . as you can see , we still have decades if not centuries before our technology can saturate this bound !  Cheers,  edit : thanks to @mark for pointing out that $1 cc = 10^{-6} m^3$ and not $10^{-9} m^3$ . changes final result by three orders of magnitude . on entropy and planck area in response to @david 's observations in the comments let me elaborate on two issues . planck area : from lqg ( and also string theory ) we know that geometric observables such as the area and volume are quantized in any theory of gravity . this result is at the kinematical level and is independent of what the actual dynamics are . the quantum of area , as one would expect , is of the order of $\sim l_{pl}^2$ where $l_{pl}$ is the planck length . in quantum gravity the dynamical entities are precisely these area elements to which one associates a spin-variable $j$ , where generally $j = \pm 1/2$ ( the lowest rep of su ( 2 ) ) . each spin can carry a single qubit of information . thus it is natural to associate the planck areas with a single unit of information . entropy as a measure of information : there is a great misunderstanding in the physics community regarding the relationship between entropy $s$ - usually described as a measure of disorder - and useful information $i$ such as that stored on a chip , an abacus or any other device . however they are one and the same . i remember being laughed out of a physics chat room once for saying this so i do not expect anyone to take this at face value . but think about this for a second ( or two ) . what is entropy ? $$ s = k_b \ln ( n ) $$ where $k_b$ is boltzmann 's constant and $n$ the number of microscopic degrees of freedom of a system . for a gas in a box , for eg , $n$ corresponds to the number of different ways to distribute the molecules in a given volume . if we were able to actually use a gas chamber as an information storage device , then each one of these configurations would correspond to a unit of memory . or consider a spin-chain with $m$ spins . each spin can take two ( classical ) values $\pm 1/2$ . using a spin to represent a bit , we see that a spin-chain of length $m$ can encode $2^m$ different numbers . what is the corresponding entropy : $ s \sim \ln ( 2^m ) = m \ln ( 2 ) \sim \textrm{number of bits} $ since we have identified each spin with a bit ( more precisely qubit ) . therefore we can safely say that the entropy of a system is proportional to the number of bits required to describe the system and hence to its storage capacity .
the speed of light is indeed constant regardless of the frame of reference of the observer . if you are on a train travelling 1000 m/s and i am on the earth and we both observe the same light wave , we will both measure it is speed to be the same ( 3e8 m/s ) . this is from einstein 's relativity . it also means that you and i view time differently . is this what you were asking ?
at the exact instance above , we can indeed prove that the tangential acceleration on each of the 3 individual carts is zero : cart 2 obviously experiences no tangential acceleration as gravity pushes it in the same direction as the normal force due to the track , which is perpendicular to the cart 's direction of motion . the force due to gravity on cart 3 does have a tangential component though , as gravity acts straight down with a component along the direction of the track . this causes the third card to " want " to speed up due to gravity , as would be expected . however , since the same exact force is also exerted by gravity on cart 1 ( but is in the opposite direction tangentially to that on cart 3 ) , the force of gravity causing cart 1 to " want " to slow down will exactly cancel , and thus we see that the net tangential acceleration across the entire train at this exact instant is zero . since each cart is connected , this proves that the net tangential acceleration on each individual cart is also zero , and the normal acceleration on each cart is the same and as suggested above , $a=\frac{{v}^2}{r}$ . another perhaps simpler way to think of this is that the instant above describes when the train , as a whole , passes the top of the loop . thinking of the train as a smaller mass ( a ball or point mass , whichever ) it then becomes clear that at the top of the loop is the moment when the train moves from slowing down to speeding up , which is to say in this case that the tangential acceleration is zero .
in the particle-physics-oriented part of the theoretical physics community , it was becoming increasingly clear that the dirac bracket is at most a complicated piece of formalism that is not able to solve any real physical problems and make theories well-defined or finite or renormalizable etc . so the people who are playing with such tools applied to quantized gravity are still close to the loop quantum gravity community . the dirac bracket quantization for the ashtekar-barbero form of gravity has been attempted in papers by sergei alexandrov , e.g. http://arxiv.org/abs/gr-qc/0005085 the paper has 77 respectable followups but i think that none of them really uses the results in any meaningful way .
consider a point source with a sphere of arbitrary radius enclosing it . both electric fields and gravitational fields obey the same basic differential equation : $$\nabla \cdot f = \rho$$ for some field $f$ and some source $\rho$ . the divergence theorem tells us that fields obeying this equation also obey $$\int_{v} \rho \ , dv = \oint_{\partial v} f \cdot ds$$ this is a mathematical consequence of obeying the first equation . this integral theorem must hold for all volumes enclosing our point source . that is to say , these integrals are constant--the left hand integral tells us the total charge or mass , and we have only one point source , so as long as the volume contains the point source , the integral must have a constant value regardless of size . on the other hand , we have chosen a sphere for our surface of integration on the right-hand side . by symmetry , we conclude that this integral reduces to $4\pi r^2 |f ( r ) |$ . for this to be constant , $f ( r ) $ must fall off as $1/r^2$ . in different numbers of dimensions , the dropoff will have a different form . for instance , in 1d there is no dropoff at all ( cf . the electric field from an infinite sheet of charge ) .
in relation to anything else that can make such measurements . as the speed of light is universal , nothing can see any other massive field moving at the speed of light ( which is reserved for massless fields ) your 0.51 number suggests that you expect that naive addition of velocities holds when velocities approach the speed of light . this is wrong . here is an article explaining the relativistic velocity addition expression : http://en.wikipedia.org/wiki/velocity-addition_formula
in the presence of massless chiral fermions , a $\theta$ term in can be rotated away by an appropriate chiral transformation of the fermion fields , because due to the chiral anomaly , this transformation induces a contribution to the fermion path integral measure proportional to the $\theta$ term lagrangian . $$\psi_l \rightarrow e^{i\alpha }\psi_l$$ $${\mathcal d}\psi_l {\mathcal d}\overline{\psi_l}\rightarrow {\mathcal d} \psi_l {\mathcal d}\overline{\psi_l} \exp\left ( \frac{i\alpha g n_f}{64 \pi^2}\int f \wedge f\right ) $$ so the transformation changes $\theta$ by $c \alpha g n_f $ ( $g$ is the coupling constant , $n_f$ the number of flavors ) . the gluons have the same coupling to the right and left handed quarks , and a chiral rotation does not leave the mass matrix invariant . thus the qcd $\theta$ term cannot be rotated away . the $su ( 2 ) _l$ fields however , are coupled to the left handed components of the fermions only , thus both the left and right handed components can be rotated with the same angle , rotating away the $\theta$ term without altering the mass matrix .
what does an aperture do ? it " applies " huygens principle to every point within the aperture , and ignores those outside the aperture because they are blocked . there are a couple of things going on when you consider a lens . let 's make sure we understand them . an aperture produces a diffraction pattern in the space of diffraction angles . recall from the simple derivation : the diffracted rays from every point in the aperture are parallel to each other . ( the diagrams accompanying the discussion often have to be fudged so that parallel lines appear to converge , although some authors are careful to include a lens as described below . ) the diffracted intensity is a function of diffraction angle . the diffracted field from the screen comprises sets of parallel rays , each set corresponding to a particular interference condition ( max , min , or in between ) . again , the diffraction pattern is in " angle space " . in order to see the pattern you need to be far enough away from the aperture that the rays from different angles and different points in the aperture no longer cross each other causing a confused pattern . you need your viewing screen to be " at infinity " . now consider what a lens does . any set of parallel rays entering a lens will be focused to a single point in the focal plane . consider placing a lens after an opaque aperture . any parallel rays entering the lens will be focused to a single spot in the focal plane . hence all the rays in each set of parallel rays from the aperture will focus onto the same spot in the focal plane . we have created the usual " fraunhofer pattern at the focal plane " . allow the aperture and lens to approach each other , and you end up with an aperture containing a lens , producing the usual fraunhofer pattern on the focal plane . to finally answer your question : remove the aperture and leave the lens . the rays that hit the lens behave as before , forming the diffraction pattern at the focal plane . the rays that fall outside of the lens are also diffracted , according to huygens principle . but these do not pass through a lens . so the diffraction pattern from these rays stays in " angle space " . they exist , and in fact some of those rays overlap the rays from the lens . but we do not see them because they are spread out and weak , and if you are close to the lens their rays cross in a confused manner . note , however , that those rays outside of the lens do form a legitimate diffraction pattern , which could be viewed " at infinity " . the pattern would be a dark spot surrounded by light , the complement of the pattern for an aperture . look up babinet 's principle for more details .
goals one wants to achieve with those two theories are similar . we know that superstring theory is a potential theory of everything . one may want to ask what is the difference between the string-net-liquid approach and the superstring approach ? our understanding of the superstring theory has been evolving . according to an early understanding of the superstring theory , all the elementary particles correspond to small segments of superstrings . different vibration modes of a small superstring result in different types of elementary particles . this point of view is very different from that of the string-net liquid . according to the string-net picture , everything comes from simple qubits that form the space . no qubits no space . the "1" qubits form string-nets which fill the whole space . the strings can be as long as the size of universe . light ( photons ) correspond to the collective motion of the large string-nets and an electron corresponds to a single end of string . ( see a picture of string-net " vaccum " . see also a talk ) a modern understanding of the superstring theory is still under development . according to witten , one of the most important questions in superstring theory is to understand what is superstring . so at this time , it is impossible to compare the modern understanding of the superstring theory with the string-net theory . in particular it not clear if the superstring theory can be viewed as a local bosonic system ( ie a qubit system ) . the string-net theory is fundamentally a local bosonic system ( ie a qubit system ) . so , if superstring theory is a qubit model ( or a quantum spin model in condensed matter physics ) , then superstring theory and the string-net theory is the same , since the string-net theory is a qubit model ( or a quantum spin model in condensed matter physics ) .
i have used paul falstad 's applets for classes i have taught . he has some very impressive visualizations there including some that you do not see often ( like the magnetic vector potential ) . he always links to the java source code , so if you can program , it'll give you a good base to start with . check out http://www.falstad.com/vector3de/ for an example .
i think your formatting is messing things up . there are two v 's , $v_{x0}$ ( i.e. horizontal velocity ) and $v_{z0}$ ( i.e. . vertical velocity at $t = 0$ ) . you can get these values right out of the problem statement . also , $g$ is 9.8m/s . lastly , $x$ is insignificant ; you only need to find $\delta x$ ( even if you were given $x$ it would not effect your final answer ) .
in the euclidean space $r^3$ you know that $$ r^2 = x^2 + y^2 + z^2 $$ add to this the fact that $$ r = c \cdot t $$ $$ \text{ ( space = velocity} \cdot \text{time ) } $$ and you can easily obtain the relation between $x$ , $y$ , $z$ and $c \cdot t$: $$ r^2 = x^2 + y^2 + z^2 = ( c \cdot t ) ^2 $$ ( do the same thing for the primed system )
i ) well , the existence of the involution $$\{ i_i , i_j \}_{pb} ~=~ 0 , \qquad i , j\in\{1 , \ldots , n\} . \tag{1}$$ is already generically true locally for a hamiltonian system , cf . e.g. this phys . se post . so it is essentially only a global/topological requirement in the definition of ( liouville ) integrability . ii ) another possible answer ( in light of op 's second equation ) is that we would like to write the hamiltonian $h=h ( i_1 , \ldots , i_n ) $ in terms of the action variables $i_1 , \ldots , i_n$ alone [ i.e. . without the angle variables , which in turn should be cyclic/ignorable variables ] . then the involution ( 1 ) implies op 's second condition $$ \dot{i}_i~=~\{ i_i , h \}_{pb} ~=~ 0 , \qquad i\in\{1 , \ldots , n\} , \tag{2}$$ i.e. that the action variables $i_1 , \ldots , i_n$ are constants of motion [ if the system is autonomous and the action variable contain no explicit ( but possibly implicit ) time dependence ] . in other words , a failure of ( 1 ) could jeopardize ( 2 ) . iii ) finally it seems relevant to mention that the involution ( 1 ) is the main assumption that goes into the caratheodoryjacobilie theorem , which guarantee that the $n$ actions variables $i_1 , \ldots , i_n$ locally can be extended into a full set of $2n$ darboux coordinates , aka . canonical coordinates . see also e.g. this related phys . se post .
time dilation ( and also length contraction ) always occurs with respect to an observer in a different frame of reference . you , in your own inertial frame , will not notice any difference . however , when you compare your measurement to that of an external observer , you will see a discrepancy in the results . if you enter a spaceship and go on a journey through the solar system at a very high velocity , and take a watch with you , and then return to your homebase ( where you have left another watch ) , you will see that the watch you took with you has registered less ticks than the one you left back home . the equivalent statement is that less time has passed for you . regarding the twin paradox : naively , one should ask the question as to why there is a different outcome in the measurement of time for both systems , since one could say that if the twin in the spaceship moves relative to earth , earth also moves relative to the spaceship . however , this is not the whole story : the spaceship , in order to make its journey to outer space and back home has to accelerate , and this breaks the symmetry between the systems .
there are two ways to approach your question . the first is to explain what brian greene means , and the second is to point out that the " particles being swallowed " explanation is a metaphor and is not actually how the calculation is done . i will attempt both , but i am outside my comfort zone so if others can expand or correct what follows please jump in ! when a pair of virtual particles are produced there is not a negative energy particle and a positive energy particle . instead the pair form an entangled system where it is impossible to distinguish between them . this entangled system can interact with the black hole and split , and the interaction guarantees that the emerging particle will be the positive one . nb " positive " and " negative " does not mean " particle " and " anti-particle " ( for what it does mean see below ) , and the black hole will radiate equal numbers of particles and anti-particles . now onto the second bit , and i approach this with trepidation . when you quantise a field you get positive frequency and negative frequency parts . you can sort of think of these as representing particles and anti-particles . how the positive and negative frequencies are defined depends on your choice of vacuum , and in quantum field theory the vacuum is unambiguously defined . the problem is that in a curved spacetime , like the region near a black hole , the vacuum changes . that means observers far from the black hole see the vacuum as different from observers near the black hole , and the two observers see different numbers of particles ( and antiparticles ) . a vaccum near the event horizon looks like excess particles to observers far away , and this is the source of the radiation . see the wikipedia article on the bogoliubov transformation for more information , though i must admit i found this article largely incomprehensible . exactly the same maths gives the unruh effect , i.e. the production of particles in an accelerated frame . the fact that the unruh effect also produces particles shows that a black hole is not necessary for the radiation , so it can not simply be virtual particles being swallowed .
this question cannot really be answered because you cannot travel at the speed of light . see accelerating particles to the speed of light if you were massless , you would always travel at the speed of light . however , in that case you would not perceive the passing of time . in relativity , the time that passes for an observer depends on the proper time . the proper time for a light-like trajectory is always zero , so photons themselves do not experience the passage of time . if you travel very near to the speed of light - perhaps 99.9% light speed relative to earth , you would still be able to view yourself normally in a mirror you carried with you . that is ensured by the principle of relativity , which states that all physical processes work the same way at any constant speed .
well , and in the two wave answers nobody considers the quantum mechanical picture . photons compose these classical waves . photons carry momentum equal to energy if we set c=1 , p=hnu when the distance from the source becomes large enough that individual photons can be counted in a counter , there will be a point where gaps will exist and no photons will be counted . taking this solution from yahoo answers one can see that for a given wavelength and intensity , a delta ( x ) between two detected photons can be found where individual photons will be very rare . so the answer depends on the original intensity , which falls with the distance as 1/r**2 , the distance , the wavelength observed and the time available for the detection . if one waited an infinite time , the answer " there is no gap would hold probabilistically . for any reasonable delta ( t ) there will be gaps that cannot be predicted in r , theta , phi because they will depend on the probability function of the photons .
the equation you state is a very general expression related to heat transfer , and basically everything goes into that constant . convection of course is one thing , but what about radiative cooling ( often important ) , diffusive cooling ( might be important ) , and heat resistance , since the temperature of your object is not uniform . all these contributions can be summed up into one heat transfer coefficients . this is very similar to summation of resistances in an electrical circuit . ( tempature difference &lt ; -> voltage , heat flow &lt ; -> current and resistance &lt ; -> $1/h$ ) you are specifically referring to convective cooling . when looking at convective cooling , you cannot live without nusselt numbers $$nu=\dfrac{h l}{k}$$ where $l$ is a typical length scale and $k$ the thermal conductivity of the fluid . why you cannot live without them , is that all fluids in essence behave in the same way , also with respect to cooling . therefore , the nusselt number is a property of your geometry , and studied for a lot of geometries . they are specified in nusselt-relations , normally as a function of the reynolds number and prandtl number . you calculate the nusselt number , which gives you a good approximation of the heat transfer coefficient $h$ . here you have to realize that there is a difference between forced convection ( fluid flow is driven by some external factor ) and natural convection ( i.e. . buoyancy driven flow caused by the temperature difference itself ) .
also i have searched for it in books like carroll 's or lawden 's , but it is given pretty much as if it would be a definition . because it is . no need for differential geometry , linear algebra is sufficient here : at a given point of space-time , the tangent space is just a vector space , the cotangent space its dual ( ie the space of real-valued linear functions ) . given a basis $\{{\bf b}_i\}$ of the vector space , we define the dual basis $\{\beta^i\}$ via $$ \beta^i ( {\bf b}_j ) =\delta^i_j $$ and can expand any vector and dual vector in terms of these ( and implicitly do so in einstein notation , using the basis induced by a coordinate chart ) . as the vector spaces have the same ( finite ) dimension , they are of course isomorphic , eg via the isomorphism $$ {\bf b}_i\mapsto\beta^i $$ the problem is that this is not a canonical isomorphism , ie it depends on the choice of basis : use a different one , and you will generally end up with a different dual vector . so we need another way to specify an isomorphism . using a non-degenerate bilinear form like the metric tensor to ' lower ' indices is such a mapping from vectors to dual vectors , which by definition comes with an inverse ( 'raising ' indices ) .
if you spend some time looking in detail at the arguments that string theory requires supersymmetry , you will find that they are not watertight . ( how could they be , since we still can not say/do not know precisely what string theory is ? ) basically , some string theorists argue that that the usual classification depends too strongly on choosing nearly trivial boundary conditions and backgrounds , and that weirder things ought to be allowed -- like type 0 strings , liouville backgrounds , gravitational duals of randomly-chosen cfts , miscellaneous higher spin gauge theories , bosonic string tachyon condensation , strings above hagedorn temperature . the experts differ on how believable these arguments are , and in whether the various bizarre things you get this way really should be thought of as part of string theory . what this means is that no one can quite answer your question . no one knows for sure that susy is required . no one knows for sure that it is not . this is why it is a very good idea to a ) not believe most things you read in the popular literature , and b ) not believe most things you read in the scientific literature .
almost all astrophysical objects up to the size of galaxy clusters rotate . their angular momentum just has to have some value and why should that be zero ? even when you start of with a big cloud of gas without any angular momentum ( the universe on large scales cannot rotate ) , it may fragment into smaller clouds which torque each other and each obtain some angular momentum , such that their sum is still zero . another important aspect is that angular momentum cannot be simply radiated away , unlike energy . this means that an object cooling ( by radiating photons ) will loose energy but not angular momentum . usually this implies it will shrink ( unless the radiation losses are balanced by some other source of energy , as for the sun which harnesses fusion energy ) and thus obtain larger angular frequencies . the total angular momentum of the solar system , for instance , is dominated by the orbital angular momentum of jupiter even though its rotation frequency is 1/12year while the spin rate of the sun ( which contains less angular momentum ) is about 1/12hours .
dimensional analysis is enough to see that $mc^2$ has the same units as energy while $\gamma mc$ or $mc$ does not . concerning the components of a 4-vector , special relativity unifies the spatial and temporal components . but the 4 components of a 4-vector with " uniform units " do not necessarily enjoy the same normalization as the quantities outside relativity . instead , you must typically multiply the time component by $c$ or $1/c$ to get the usual non-relativistic normalization of the quantity . the position vector is $ ( x , y , z , ct ) $ . note that all of them have the dimension of length . but the real time is $ct$ , the last component , divided by $c$ . similarly , the energy-momentum vector has $m_0\gamma v_x , m_0\gamma v_y , m_0\gamma v_z , m_0\gamma c$ . again , all components have the same units but now you have to multiply the last component $m_0\gamma c$ by $c$ to get the usual normalization for the energy , $e=m_0 \gamma c^2$ . this is just a question of units . nothing guarantees that the simplest identifications and conventions will lead to correct formulae without any extra $c$ with the units used before relativity . when we say " the time component of a vector is a particular quantity known before relativity " , we mean that it contains the same information but sometimes we need to normalize it differently , add a universal factor . before relativity , people used very unnatural units for many quantities and the temporal and spatial components did not have the sam units even though their key information was always a part of the same 4-vector which shows that they may be " rotated into each other " . among physicists who study relativistic phenomena ( e . g . particle physicists ) , this is a complete non-problem and physicists often use units with $c=1$ , anyway . this really means that distances are measured in light seconds and the speed of light is one light second per second , and the difference between a light second and second is suppressed . ( particle physicists typically use units in which one gev , and not one second , and its powers are used for everything , i.e. times , distances , energies , momenta , masses , etc . : they also set $\hbar=1$ so that energy and frequency i.e. inverse time have the same units . )
you have seen biplanes . almost anything can fly if it has enough area , an angle of attack , and is nose-heavy . i have seen a model airplane in the shape of snoopy 's doghouse ! it flew just fine . if you are wondering why nose-heavy it is this . look at a normal plane with a main wing and a tail . it is nose heavy . the main wing holds the weight by lifting up . the tail keeps the nose from dropping by lifting down . so there is a torque moment between the wing and tail that is balanced by the weight of the nose , and it is proportional to speed squared , just as lift is . now if the plane slows down , that torque moment decreases , causing the nose to drop . when the nose drops , the plane starts going downhill , picking up speed . that causes the moment to increase , causing the nose to rise . so that nose-heaviness , and the moment , stabilize the plane 's speed . if the plane is not nose-heavy , it does not seek a natural speed and is very hard to control in level flight . if it is nose-light , it can even nose up and then slide backward . the way normal planes control their speed is by a trim wheel that creates a little more or a little less downward lift on the tail . then if they want to climb they use more throttle , or to descend they use less . so it is counterintuitive , but the throttle does not control speed , except on a very short time scale . it controls the rate of ascent/descent . the trim wheel controls speed . some planes do not have a tail but instead have canards on the nose . it works on the same principle . the canards hold up the nose . so higher speed , nose goes up , plane slows down , nose drops , etc . planes with just a delta wing also have a moment . it is just built into the single wing . edit : bernhard wanted a picture . here 's one :
it is assumed , but not measured that e =hw . see this reference about the difficulty of measuring a single graviton . http://arxiv.org/abs/gr-qc/0601043 it is an interesting exercise to compute the amount and wavelength of gravitons from a falling apple . as a first approximation , one can compute the gravitational waves emitted by an apple orbiting the earth and use the formulas developed for binary black holes and neutron stars by the ligo virgo etc experiments . in low earth orbit , the apple will radiate at a wavelength of approximately 90 light minutes , give or take a factor of two . this is very roughly 10^12 meters . each graviton will then carry about 10^-30 ergs , a very small amount . according to the wikipedia gravitational wave article the sun earth system emits 200 watts of gravitational radiation , but this would typically emit 10^-34 erg gravitons . 200 watts is 2 10^9 erg seconds , so the sun earth system is emitting 10^43 gravitons per second . using the formula from the wikipedia article , the earth-apple system with a one tenth kilogram apple would emit 10^46 times less gravitational wave power , or 10^-42 watts or 10^-35 ergs/second . this implies an average of one graviton every 10^5 seconds , or about once every twenty 5400 second orbits . if your falling apple falls for about one second , it should emit one graviton once out of every one hundred thousand tries . to be more analagous to the orbital picture , your apple should be thrown horizotally like a baseball , rather than falling vertically
the potential barrier problem and solution in quantum mechanics is discussed within the solutions of schrodineger 's equation in which there exist potentials , and the solutions of the equations with the boundary conditions give the wave function of a particle , i.e. an entity with a mass . in addition it is a non relativistic equation . thus in this framework : relativistic velocities are not allowed as a particle cannot be described by a wave function that is a solution of the schrodinger equation . the photon enters as an interpretation of the energy conservation between transitions of bound state energy levels , a hypothesis that has been amply experimentally observed and validated the use of the schrodinger equation in first quantization . the answers to the question what equation describes the wavefunction of a single photon ? , asked here a while ago , cover the way the photon is described in first ( dirac equation ) and second quantization . in this preprint a view is suggested of using maxwell 's equations for the wave function of the photon . one could use these and define a barrier and solve for it to get the behavior of the photon 's probability to pass the barrier , but it is not a simple problem i could tackle . generally when one calculates behaviors of photons on reaching a barrier it is wise to use classical solutions of maxwell 's equations . transmission , reflectance etc describe the behavior of light at barrier . going down to the individual photon is complicated mathematically and not worth the effort since it can be shown that the classical and the quantum description for photons merges naturally .
your guess is basicly right . on the backside of the phosphor in a crt there is a layer of aluminium , which reflects the light emitted backward to the front , blocks positive ions from damaging the phosphor and is used to carry of the electrons after they have done their duty . this aluminium layer is connected to the anode ( "plate" ) terminal . for this reason the beam is not the cause of the electric charge on the front suface ( which you describe as " fuzzy feeling" ) this charge is produced by influence ( "electrostatic induction" ) . the inner wall ( that aluminium ) is kept at some 10 to 30 kv positive during operation . the charge induced from that layer to the outside stays there if t here is no special surface treatment . slowly it can creep away , but when you switch off , a induction of opposite polarity arises . ( this is often accompanied by a rather loud " hiss " ( corona ) . on top of that the glass front can leak small currents from the anode voltage , this complicates analysis further . because the surface charge of crts can " load " dirt particles with charge and those fly then away with some " push " , the crts of pc monitors were treated to be conductive starting from about mid-nineties . ( a lot of people complained about problems with burning/tears in eyes etc . ) for some time one could buy frames with a fine metal mesh to be positioned in front of monitors of the first generation .
there are two unrelated effects at work here . one is the atmospheric pressure , and the other is the surface tension of the water . start with you holding the plate in place , and consider what happens when you release the plate . for the plate to fall down one of two things must happen . either the volume of the water in the glass must increase , to allow the plate to move down , or air must flow into the glass at the contact line between the glass and the plate . consider the first of these . if you pull the plate down slightly ( and no air leaks in ) the volume inside the glass must increase . water has such a high bulk modulus that we can approximate it as incompressible . you would need an immense force pulling down on the plate to stretch the water to any significant degree . in practice the water would boil before its volume increased significantly , but even the lesser force required to boil the water is far greater than the weight of the glass plate . so the only way the plate can move down is for air to leak in through the contact between the glass and the plate . however this means forming small bubbles at the contact line , and small bubbles have a very high pressure due to the surface tension at the air/water interface . this means the bubble formation requires a greater pressure than the weight of the plate can generate , so the plate can not move down this way either . incidentally , the effect of surface tension explains why the plate will not stick if the glass/plate contact is not very good , or if there is a chip in the glass . in both cases there is a relatively large gap where a bubble can form , and large bubbles have a smaller pressure than small bubbles ( the bubble pressure is inversely proportional to the bubble radius ) . the weight of the plate can generate enough pressure to form the large bubbles , and the plate falls off . now we can explain why the plate falls off when you immerse the glass and plate in water . if you do this there is no air/water interface at the contact between the glass and the plate , so there is no surface tension effect . water can leak through even the tiniest gap between the rim of the glass and the plate so the plate falls off ( though it may take a few seconds as the water will not flow in instantly ) . the trick will work with most liquids because most liquids will neither expand nor boil under the weight of the plate . however it would not work with very volatile liquids like ether , because ether boils too easily and vapour bubbles will form in the glass . you had probably also find it would not work if the air/liquid interface has too low a surface tension , because a low surface tension allows air to lean in between the glass and the plate .
for the first problem , yes . because you shoot in the photon from the boundary of the sphere , the trajectory of the photon , using elementary geometry , will stay within a fixed plane ( the plane is defined by the center of the sphere , the point of entry , and the initial direction of the photon ) . so you reduce the problem to an essentially two-dimensional problem . it is well known that the billiards trajectory in a circle is integrable , and the trajectory is either periodic or hits a dense set of points on the boundary . in the former case the photon will quickly exit the sphere , even in the limit as the size of the initial hole tends to zero . in the latter case for every non-zero sized hole , you can find some finite time $t$ such that the photon will be guaranteed to exit the hole after $t$ . but as you shrink the size of the hole , $t$ increases unboundedly . for the second problem , it is possible to set up trapping obstacles inside your sphere . for an example , see this question/answer on mathoverflow . however , in general you can only trap trajectories for a small set of initial angles ; i am inclined to say that you cannot do so for all angles simultaneously , though i do not have a proof for that .
if ( and that is a big if ) tomorrow we had a $70\sigma$ detection in a repeatable experiment of a particle that travelled faster than $c$ , then one of several things would be true . 1 ) we would be forced to conclude that $c$ is not , in fact , the limiting speed of information transfer ; everything based on this assumption would have to be scrapped ( pretty much all of research-level physics ) ; and we would have to start over in developing even the mathematics that allows us to start re-describing the universe . 2 ) we would be forced to conclude that $c$ is not the limiting speed of information transfer ; we would assume that special relativity and everything based on it is the special-case effective theory of much broader physical laws and behaviours ; and we would have to find a way of modifying relativity ( and basically everything that relies on it ) so that it can causally allow for this particle to exist and yet have everything else we see still basically operate under the idea that $c$ is the max speed . 3 ) we find a way to use this particle to communicate with the past and future , travel faster than light , and then we go home every night and laugh at einstein . 4 ) we perform the experiment thousands of times in different laboratories , find the same result , then go back and discover that there was a fundamental flaw with the theory . once the flaw is corrected , we see that we are not actually observing a superluminal particle . my money is on ( 4 ) with ( 2 ) being a close second . note : this answer assumes that the speed $c$ referenced is the assumed maximum speed ; the speed of a massless particle in a vacuum . this is why i did not include a " we determine the photon is not massless and then have to change em " option .
depending on your definition of " object , " neptune is currently the farthest . it was visited by voyager 2 in 1989 . new horizons will be visiting pluto in 2015 if by object you mean cosmic structure though , then voyager 1 is currently in the heliosheath and is expected to reach the heliopause by 2015 . these are regions that describe interactions between the sun 's solar wind and the interstellar medium .
all known spacetime shortcuts , such as wormholes , alcubierre drive and krasnikov tubes , admit some sort of ctc metric variation . as far as i know , it is not really a theorem , just that it applies to all those cases . but those ctc metrics do not necessarily mean much , because they do not specify whether it is possible to evolve the spacetime shortcut ( the term spacetime shortcut is not really standard by the way but krasnikov has used it here ) to the point where it forms a closed timelike curve . there are three main arguments forbidding compactly generated time machines ( that is , where the time machine is not the entire universe ) . the main one is energy conditions , which all spacetime shortcuts already violate , so it will not help here . the second is stability : ctcs that exist tend to collapse if matter tries to cross it . but this issue does not exist for all ctcs . and the big one is that , in semi-classical approximations , the closer to a ctc one gets , the higher the stress energy tensor of the vacuum gets , due to quantum effects , making it ( possibly ) impossible to form . so , if any of these old in all realistic cases ( there are theoretical scenarios that violate all of them ) , you might be able to build spacetime shortcuts without violating causality . trying to build a time machine will just make the whole thing collapse . you may not even need to try to build a time machine : wormholes get time shifted just by being in different gravitational potentials , which might make then naturally get more and unstable as time goes on .
there is an electrostatic repulsion between the protons in the nucleus . however , there is also an attraction due to another kind of force besides electromagnetism , namely the so-called " strong nuclear interaction " . the strong nuclear interaction ultimately boils down to the forces between the " colorful " quarks inside the protons - and neutrons . it is mediated by gluons , much like electromagnetism is mediated by photons , described by quantum chromodynamics ( qcd ) , much like electromagnetism is described by quantum electrodynamics ( qed ) , and it acts ( almost ) equally on protons and neutrons . the attractive strong nuclear interaction inside the nuclei is 1-2 orders of magnitude stronger than the repulsive electrostatic interaction which is what keeps the nuclei together despite the repulsive electrostatic force .
your issue is about moving within a moving referential . to understand it well , let us first consider a galilean referential : assume you live on a flat infinite plane , which is travelling at a constant speed along a horizontal direction . does it take you more energy to move along with the movement of the plane or in the other way ? no , just as walking to the front or rear of a train going at constant speed . it would if you had some contact with the exterior of the train , e.g. the air . now your referential at the surface of earth is not in translation but in rotation , and that is the origin of coriolis force , but that is only a correction to the movement in the referential , which works the same as above .
in one dimensional case you have the oscillation theorem : $n$-th level has $n$-zeroes . as a special case ground state has no zeroes . it does not generalize on the case of several dimensions however there is still a theorem that the ground state is non-degenerate and has no zeroes . thus the observation that the goldstone mode vanishes somewhere means that it is not a ground state . therefore there are some modes with lower ( i.e. . negative ) eigenvalue , hence instability .
both your answers are same . $1.66\times 10^{-27} = ( 6.02\times 10^{26} ) ^{-1}$ hope that helps . . .
the factor $\frac12$ comes in because we are integrating the equation $$ \frac{\mathrm de}{\mathrm dv}=mv $$ once . less abstract and only using basic arithmetics , the story goes like this : when accelerating a body by applying a ( constant ) force $f$ along a distance $\delta s$ , the body gains energy according to $$ \delta e=f\delta s $$ which is just the definition of ( mechanical ) work . according to newton 's second law $f=ma$ . we also have $\delta s \approx v\delta t$ and thus $$ \delta e\approx mav\delta t $$ this relationship is only approximate because during any finite time interval $\delta t$ , the value $v$ changes as the whole point of the exercise was accelerating the body . now , as $a\delta t=\delta v$ we have $$ \delta e \approx mv\delta v $$ but where does the factor $\frac12$ come in ? from basic calculus : $$ \delta ( v^2 ) = ( v+\delta v ) ^2-v^2=2v\delta v+ ( \delta v ) ^2\approx 2v\delta v $$ which yields $$ \delta e\approx\frac12m\delta ( v^2 ) =\delta ( \frac12 mv^2 ) $$ and thus $$ e\approx\frac12 mv^2 + \mathrm{const} $$ if we go from finite to infinitesimal time intervals , the equations become exact and we no longer need to assume a constant force . a short introduction to differential calculus as relevant to this particular example : at time $t = t_0$ the body has a velocity $v ( t_0 ) =v_0$ . after a time $\delta t$ , the body has the velocity $v ( t_0+\delta t ) =v_0 + \delta v$ . the value of $v^2$ at time $t=t_0$ is of course $v^2 ( t_0 ) =v ( t_0 ) ^2=v_0{}^2$ . what is the value of $v^2$ at time $t=t_0+\delta t$ ? $$ v^2 ( t_0 + \delta t ) =v ( t_0+\delta t ) ^2 = ( v_0+\delta v ) ^2 $$ on the other hand , we also have $$ v^2 ( t_0 + \delta t ) = v^2 ( t_0 ) + \delta ( v^2 ) = v_0{}^2 + \delta ( v^2 ) $$ and thus $$ \begin{align*} \delta ( v^2 ) and = ( v_0 + \delta v ) ^2 -v_0^2 \\ and = v_0^2 + 2v_0\delta v + ( \delta v ) ^2 - v_0{}^2 \\ and = 2v_0\delta v + ( \delta v ) ^2 \end{align*} $$ we are interested in the instantaneous values , ie the change as we take the limit $\delta t \rightarrow 0$ . this means that $\delta v$ becomes arbitrarily small as well and we are in particular able to ignore higher powers like $ ( \delta v ) ^2$ and get $$ \delta ( v^2 ) \approx 2v_0\delta v $$ or $$ \frac{\delta ( v^2 ) }{\delta v}\approx 2v_0 $$ this procedure is so useful that it got its own formalism and symbolic notation $$ \frac{\mathrm{d} ( v^2 ) }{\mathrm{d}v}=2v $$ after taking the limit $\delta v\rightarrow 0$ .
reposting comment as an answer and expanding . the answer is yes . you can find an exposition in condensed matter field theory by altland and simons , starting on page 134 in the second edition . the troubles come from that spin can not be described with a hamiltonian that is a function of $q$:s and their conjugate $p$:s . however the more general formulation of hamiltonian mechanics in terms of symplectic manifolds permits a description of spin . altland and simons cite arnold 's mathematical methods of classical mechanics as a reference for this . it is an under-appreciated gem of a book . so , when we construct the path integral we consider the paths to be paths in phase space , $p$:s and $q$:s . i think to understand this in geometrical terms we have to back to " basics " . the lagrange formulation we have coordinates $q$ and velocities $\dot q$ . the coordinates can be coordinates on any manifold -- this is why the lagrangian formalism is so neat for constrained systems -- so the velocities are really tangent vectors . lagrangian mechanics is thus naturally formulated on tangent bundles . but we can take the legendre transform and pass to the hamiltonian formalism with $p$:s and $q$:s . this takes us to the cotangent bundle , for $$p_i = \frac{\partial l}{\partial \dot q^i}$$ is the 1-form that is $\partial l /\partial \dot q^i$ on the vector field $\dot q^i$ and 0 on the other coordinate vector fields . however you can do all of hamiltonian mechanics on any manifold that comes with a symplectic structure . a symplectic structure is 2-form $\omega$ such that $d\omega = 0$ and for every vector $v$ , $\omega ( v , \cdot ) $ is not the zero 1-form . ( you can think of $\omega$ as a sort of anti-symmetric metric . ) this is what arnold does in his book . the cotangent bundle naturally comes with such an $\omega$: $$\omega = dp_1 \wedge dq_1 + \ldots dp_n \wedge dq_n$$ ( this 2-form is independent of your choice of coordinates $q$ ) . the hamiltonian is a function on $m$ , so in the particular case of a cotangent bundle , you can take it to be a function of the $p$:s and $q$:s . now you can couple orbital angular momentum to the vector potential just fine with $p$:s and $q$:s . but how to do it with intrinsic angular momentum , that is spin ? we want a hamiltonian like $$h = \mathbf b \cdot \mathbf s . $$ since the spin of a particle has a definite magnitude , the dynamical part of the spin is its direction . so this hamiltonian is defined on a manifold like $t^* m \times s^2$ ( i put the first factor there because $\mathbf b$ could of course vary in space ) . of course this rests on $s^2$ having an admissible $\omega$ , but you can take the volume form $$\omega = \sin\theta\ ; d\theta \wedge d \varphi$$ with $\theta , \varphi$ the usual coordinates . therefore the paths you use in your construction of the path integral for spin should be paths on the sphere , and you can use $\theta , \varphi$ as coordinates . there is a formal complication in the quantum case since our states also have phases . this means that we should really use paths in $su ( 2 ) $ , since an arbitrary spin state can always be written like $$|g\rangle := g\vert\uparrow \rangle$$ where $\vert\uparrow\rangle$ is some reference state . so the resolution of identity inserted to construct the path integral should be $$\operatorname{id} \int_{su ( 2 ) } \vert g \rangle \langle g\vert$$ ( the integration is with respect to an $su ( 2 ) $-invariant volume form . ) this leads to an extra term , the usual $\partial_\tau$ , in the path integral . however it turns out that the phase is irrelevant to this term , so the path integral is really over paths on the sphere $s^2$ . you can find the detailed calculation in altland and simons .
you should write the indices on the gamma matrices . so your expression is actually \begin{align} \text{tr} [ \gamma^\mu ( \gamma^\alpha k_\alpha + \gamma^\beta p_\beta + \gamma^\delta q_\delta + m ) \gamma_\mu ( \gamma^\rho k_\rho + \gamma^\sigma p_\sigma + m ) ] . \end{align} then you use the trace technology to evaluate the traces . for example , the $m^2$ term has the coefficient 16 because $\text{tr} [ \gamma^\mu \gamma_\mu m^2 ] = 16 m^2$ . the $k^2$ term has $\text{tr} [ \gamma^\mu\gamma^\alpha \gamma_\mu \gamma^\rho ] k_\alpha k_\rho = 4 ( g^{\mu \alpha}\delta_\mu^\rho - g^\mu{}_\mu g^{\alpha \rho} + g^{\mu \rho}\delta^\alpha{}_\mu ) k_\alpha k_\rho = 4 ( k^2 - 4k^2 + k^2 ) = -8k^2$ , and so on .
what is the electrical potential difference and why we have to talk about a difference and not about the electrical potential itself ? mathematically , the reason is that the force is proportional the gradient of a ( not the ) potential function . $$\vec f = -\nabla \phi$$ note that a potential that differs by an additive constant $$\phi ' = \phi + c$$ yields the same force field $$\nabla \phi ' = \nabla \phi$$ since the gradient of a constant is zero $$\nabla c = 0$$ in other words , adding an arbitrary constant to a potential does not change the physics so the absolute value of the potential is physically unmeaningful . but look at the difference in potential between two points : $$\phi' ( \vec x_1 ) - \phi' ( \vec x_2 ) = \phi ( \vec x_1 ) + c - \phi ( \vec x_2 ) - c = \phi ( \vec x_1 ) - \phi ( \vec x_2 ) $$ so , the potential difference is unchanged by the additive constant and thus the difference is physically meaningful . what is the electrical potential difference in practical terms ( can you explain it using concrete example ? ) in practical terms , the electric potential difference between two points is the amount of work associated with moving a 1 coulomb test charge from one of the points to the other .
if one takes $u$ as the dependent variable and $n , v , $ and $s$ as the independent variables , then one has $u = u{s , v , n}$ with the total derivative of u equal to $$du = ( \partial u/\partial s ) _{v , n} ds + ( \partial u/\partial v ) _{s , n} dv + ( \partial u/\partial n ) _{s , v} dn$$ . each of these partial derivatives has a " simple form " , with $t = \partial u/\partial s$ , $\ , -p = \partial u / \partial v$ and $\mu = \partial u/\partial n ) $ . the condition for equilibrium between two systems open to thermal transfer is that the two $t$ 's be equal , the condition for two systems open to pressure change is that the two $p$ 's be equal , and the same for exchange of particles with the two chemical potentials $\mu$ be the same . these intensive variables are fundamental for thermodynamics , but could have been defined in any reasonable way , such as $1/t$ ( $1/k t$ might have been better ) or whatever . so the short of it is that while the derivatives are all important , their actual definition is somewhat arbitrary .
everything does not expand equally because of cosmological expansion . if everything expanded by the same percentage per year , then all our rulers and other distance-measuring devices would expand , and we would not be able to detect any expansion at all . actually , general relativity predicts that cosmological expansion has very little effect on objects that are small and strongly bound . expansion is too weak an effect to detect at any scale below that of distant galaxies . cooperstock et al . have estimated the effect for systems of interest such as the solar system . for example , the predicted general-relativistic effect on the radius of the earth 's orbit since the time of the dinosaurs is calculated to be about as big as the diameter of an atomic nucleus ; if the earth 's orbit had expanded according to the cosmological scaling function $a ( t ) $ , the effect would have been millions of kilometers . to see why the solar-system effect is so small , let 's consider how it can depend on $a ( t ) $ . there is a cosmology called the milne universe , which is just flat , empty spacetime described in silly coordinates ; $a ( t ) $ is chosen to grow at a steady rate , but this has no physical significance , since there is no matter that has to expand like this . the milne universe has $\dot{a}\ne 0$ , i.e. , a nonvanishing value of the hubble constant $h_o$ . this shows that we should not expect any expansion of the solar system due to $\dot{a}\ne 0$ . the lowest-order effect requires $\ddot{a}\ne 0$ . for two test particles released at a distance $\mathbf{r}$ from one another in an frw spacetime , their relative acceleration is given by $ ( \ddot{a}/a ) \mathbf{r}$ . the factor $\ddot{a}/a$ is on the order of the inverse square of the age of the universe , i.e. , $h_o^2\sim 10^{-35}$ s$^{-2}$ . the smallness of this number implies that the relative acceleration is very small . within the solar system , for example , such an effect is swamped by the much larger accelerations due to newtonian gravitational interactions . it is also not necessarily true that the existence of an anomalous acceleration leads to the expansion of circular orbits over time . an anomalous acceleration $ ( \ddot{a}/a ) \mathbf{r}$ just acts like a slight repulsive force , which is equivalent to reducing the strength of the gravitational attraction by some small amount . the actual trend in the radius of the orbit over time , called the secular trend , is proportional to $ ( d/dt ) ( \ddot{a}/a ) $ , and this vanishes , for example , in a cosmology dominated by dark energy , where $\ddot{a}/a$ is constant . thus the undetectably small effect estimated by cooperstock et al . for the solar system is a measure of the extent to which the universe is not yet dominated by dark energy . the sign of the effect can be found from the friedmann equations . assume that dark energy is describable by a cosmological constant $\lambda$ , and that the pressure is negligible compared to $\lambda$ and to the mass-energy density $\rho$ . then differentiation of the friedmann acceleration equation gives $ ( d/dt ) ( \ddot{a}/a ) \propto\dot{\rho}$ , with a negative constant of proportionality . since $\rho$ is currently decreasing , the secular trend is currently an increase in the size of gravitationally bound systems . for a circular orbit of radius $r$ , a straightforward calculation ( see my presentation here ) shows that the secular trend is $\dot{r}/r=\omega^{-2} ( d/dt ) ( \ddot{a}/a ) $ . this produces the undetectably small effect on the solar system referred to above . in " big rip " cosmologies , $\ddot{a}/a$ blows up to infinity at some finite time , so cosmological expansion tears apart all matter at progressively smaller and smaller scales . general relativity can not describe quantum-mechanical systems such as atoms . a nice way of discussing atoms , nuclei , photons , and solar systems all on the same footing is to note that in geometrized units , the units of mass and length are the same . therefore the existence of any fundamental massive particle sets a universal length scale , one that will be known to any intelligent species anywhere in the universe . since photons are massless , they can not be used to set a universal scale in this way ; a photon has a certain mass-energy , but that mass-energy can take on any value . similarly , a solar system sets a length scale , but not a universal one ; the radius of a planet 's orbit can take on any value . a universe without massive fundamental particles would be a universe without length measurement . it would obey the laws of conformal geometry , in which angles and light-cones were the only measures . this is the reason that atoms and nuclei , which are made of massive fundamental particles , do not expand . photons , however , do expand -- or at least , this is one possible way of describing cosmological redshifts . cooperstock , faraoni , and vollick , " the influence of the cosmological expansion on local systems , " http://arxiv.org/abs/astro-ph/9803097v1
like james maslek said , this is just an effect of having an infinitely thin disk--the field is a step function . if you like , you can replace your disk with two disks , each having the same radius and surface charge density $\sigma/2$ . seperate them by some finite distance , so they are located at $z = \pm \epsilon$ for some small $\epsilon$ . then , work out what the field is at some arbitrary point along their common axis . you will find that the field at $z=0$ is zero for all $\epsilon$ and that your solution limits to your original equation for $z \gg \epsilon$ . this way , you can ignore the discontinuous nastiness of the infinitely thin disk . if you want to get yet fancier , and really analyze this situation in the case where the thickness of the disk is important , you can think of a cylinder with uniform volume charge density , radius $r$ and thickness $l$ . you can work out the field at an aribtrary point along the cylinder 's axis , inside or outside . it should be easy enough to show that the field at the center of the cylinder is zero . then , you can replace $\rho$ with $\frac{\sigma}{l}$ and see how it compares with your expression for a disk in the limit $z \gg l$ , which should once again equal your original expression . finally , a third way to see this , but one which involves much harder calculus would be to derive the field due to a disk at an arbitrary point on the plane of the disk , and then see that the field goes to zero at the center of the disk when you are constrained to the plane .
is the 4-dimensional spacetime of general-relativity discrete or continuous ? in the usual definition of general relativity , spacetime is continuous . however , general relativity is a classical theory and does not take quantum effects into account . such effects are expected to show up at very short distances , where your question is relevant . are there experimental evidences of continuity/discreteness ? all the experimental evidence points to continuous space , down to the shortest distances at which we have been able to measure . we do not know what happens at shorter distances . we also do not have any direct experimental evidence that gravity is a quantum theory , with the same caveat . on the other hand , we are quite confident that a complete theory of nature must include quantum gravity and not just classical gravity . and , we have an educated guess of the distance scale at which quantum effects should become measurable : this is the planck length , roughly $10^{-33}$ cm . this is much much shorter than the shortest distance at which we can carry out experiments , so at least we are not surprised that we did not see any such effects so far . before proceeding , one more caveat . there is an interesting and quite recent astrophysical experiment that showed that lorentz symmetry holds even below the planck length . if lorentz symmetry is broken , it generally means that photons with different energies will travel at different velocities . at the experiment , they managed to detect a pair of photons that were created at almost the same time but had very different energies . they reached the detector almost simultaneously , which means their velocities were similar . because the photons travelled an enormous distance before reaching us , they must have had almost the same velocity . so we know that at least lorentz symmetry holds at very short distances , and it seems difficult to reconcile this experimental fact with a discrete spacetime . so at least naively it seems that this is evidence against discreteness . is the spacetime continuous or discrete ? at long distances spacetime can certainly be thought of as continuous . at short distances , the short answer is : we do not know . string theory is the only consistent theory of quantum gravity we know of , where we can actually compute things with some confidence . ( you will probably hear some opinions that contradict this statement , mentioning loop quantum gravity , causal sets , etc . , which are not related to string theory , but what i said is the common view in the community of high-energy theorists . ) string theory is giving us some strong hints that perhaps spacetime at short distances is not continuous or discrete , but something else that we do not understand yet . so the situation is that even theoretically , without talking about actual experiments that check the theory , we do not know what spacetime is like at short distances . perhaps this is why you do not see this question mentioned a lot . my personal guess is that spacetime at short distances is neither continuous nor discrete , but has a different nature that may require new mathematical tools to describe . or better , what if we consider additional dimensions like string theory hypothesizes ? are those compact additional dimensions discrete or continuous ? adding extra dimensions does not change any of the above .
i believe you could use some differential calculus . a helpful formula is a = d(v^2 / 2)/dx , derived via a = dv/dt = dv/dx * dx/dt $$a = {-kx \over m}$$ $$a = {\operatorname{d}\ ! ( {1 \over 2}v^2 ) \over\operatorname{d}\ ! x}$$ $$\therefore -{1 \over 2}v^2 = \int {k \over m} x dx = {k \over 2m}x^2+c$$ when $x=a$ , $v=0$: $\therefore c = +{ka^2 \over 2m}$ $$\therefore v = +\sqrt{{-kx^2+ka^2 \over m}}$$ when $x=0$ , $v=\sqrt{{ka^2 \over m}}$ this is the same result as that you would get using energy . ( i have assumed that the force causing the spring to retract is f = -kx . i have made this assumption based on it being the force required to stretch the string at constant velocity )
this has to do with the probability that the proton would decay into lighter subatomic particles , such as a neutral pion and a positron , which is a calculable number depending on the theories existing at the moment . the limit in your quote is one order of magnitude higher than the latest experimental limit of proton decay up to now . if a proton decays in the nucleus of an atom the nucleus disintegrates because it loses a baryon and the energy released within will destroy it , and then the atom dissolves .
what you are seeing at a distance is not black . it is a darkish shade of gray , rgb gray 85,85,85 . the reason you are not seeing " white " is because each of those three rectangles has an hsv value of only 33% and you are seeing that merged square against a white background . that merged square will appear to be whitish if you make the background black rather than white and view the screen in a very dark setting .
the simplest feynman diagram for an interaction between two particles looks like a letter " h " . the cross-bar is a force-carrier being exchanged . at each vertex , you have a particle either emitting or absorbing a force-carrier . if the force-carrier has a half-integer spin , then you can not emit or absorb it without violating conservation of angular momentum . for example , an electron can not emit a spin-1/2 particle , because you can not couple spin 1/2 and spin 1/2 to make spin 1/2 .
ben hocking is right that the universe we observe is not the same age everywhere : the further away we look , the younger things are , simply because the light from faraway objects takes time to reach us . but you may be wondering about more than that : what if we could remove those light-travel-time delays and consider the universe as it is now , as opposed to the way we see it ( with greater delays for more distant objects ) ? would it all be the same age in that case ? that turns out to be a more subtle question than you might think , because of relativity . relativity says , among other things , that different choices of reference frame lead to different notions of time -- things that are simultaneous in one reference frame are not simultaneous in another . when you consider what a distant object is like now , you have to be careful to specify what reference frame you are talking about , because different frames lead to different " nows . " at each point in spacetime , there is a reference frame that seems like the " most natural " one to use , namely the one in which the expansion of the universe looks roughly the same in all directions . cosmologists tend to use that reference frame to define and synchronize their clocks . that is , the time coordinate at any point in spacetime is , by definition , the amount of time that would have elapsed according to an observer who'd been at rest in that reference frame since the big bang . that time coordinate is often called " cosmic time . " if you use cosmic time as your time coordinate , then it is true that , at any fixed moment of time , all points in the universe have the same age . but it is true pretty much because of the way we defined our time coordinate , so this is kind of a vacuous statement ! update : based on the comments , i realize i should have explained some things explicitly . i am considering here " cosmological " effects , meaning i am ignoring things that are due to small-scale inhomogeneities and thinking about large scales , on which the universe is approximately homogeneous . if , for instance , you hang around near the horizon of a black hole , you will age at a different rate from someone else . even if you avoid such extreme cases , small-scale variations in the gravitational potential will lead to small-amplitude age variations . in fact , the definition of cosmic time only really makes sense in the approximation where we are willing to average over small-scale inhomogeneities . if you are not willing to do that , then different prescriptions for synchronizing your clocks , no one of which is obviously more natural than another , will lead to small differences in ages of things .
small insects and animals have to deal with the viscosity of the air . the motion of the train takes the air with it , the air does not accumulate at the back neither during acceleration or steady velocity . the effect will be small and only during acceleration . the air has the steady velocity of the vehicle and anything suspended in it by buoyancy will have the same velocity it is easier to see with water , which will slosh initially backwards during acceleration , and at steady velocity will have the velocity of the container , which is in contact with the vehicle . we feel the effect of acceleration in a car by being pushed back in our seats at start up , and falling forward at slow down , and the mosquito will feel a bit of this , cushioned by the viscosity of the air . at steady velocities it is as if we are sitting at the table at home and the mosquito will also be motionless with respect to the vehicle and us .
the set of irrational numbers densely fills the number line . even assuming that quantum mechanics does not disable the preimse of your question , the probability that you will randomly pick an irrational number out of a hat of all numbers is roughly $1 - \frac{1}{\infty} \approx 1$ . so the question should be " is it possible to have an object with rational length ?
not quite . but before getting into the math , you had need to fix up the problem a bit : if the string and the ball are both massless , then there will be nothing left to oscillate once the water drains out . it probably makes the most sense to assume that the ball is a uniform spherical shell with a mass $m$ . for simplicity , i will make two more assumptions : the rate at which the water drains is slow , so that the change in mass over each oscillation is negligible ( i.e. . the process is adiabatic ) the momentum/energy of the stream of water escaping is also negligible i am also using the result which georg brought up , that the surface of the water remains perpendicular to the pendulum 's axis even as it swings . first i will compute the volume of water in the ball when it is filled up to a height $h$ , $$v = \int_{-r}^{h-r} \pi ( r^2 - z^2 ) \mathrm{d}z = \pi h^2\biggl ( r - \frac{h}{3}\biggr ) $$ and then the moment of that water , $$m z_{w} = \int_{-r}^{h-r} zd\pi ( r^2 - z^2 ) \mathrm{d}z = \pi d h^2 \biggl ( hr - r^2 - \frac{h^2}{4}\biggr ) $$ to find the overall center of mass , you add this to the moment of the sphere ( which is zero since its center of mass is at $z = 0$ ) and divide by the total mass . since $m = dv$ , we get $$z_{cm} = \frac{\pi d h^2\bigl ( hr - r^2 - h^2/4\bigr ) }{ ( d ) \pi h^2\bigl ( r - h/3\bigr ) + m} = \frac{hr - r^2 - \frac{h^2}{4}}{r - \frac{h}{3} + \frac{m}{\pi h^2 d}}$$ the thing that you are calling effective length in the problem is $l + r - z_{cm}$ . using $m = 1$ , $r = 1$ , and $d = 1$ as sample values ( e . g . if you are using cgs units with lengths in centimeters and masses in grams ) , a plot looks like this : the graph does start and end at zero , which means that at the beginning and at the end , the center of mass is at the center of the sphere at those times , as you said . but in the middle , it never goes below about -0.24 , which means that the radius of the swing of the center of mass never gets longer than $l + 1.24 r$ ( in this example ) . it will not reach all the way out to $l + 2r$ . however , as georg also mentioned , approximating the pendulum as a point mass on a string of varying length is not quite accurate . to do better , we can use the rotational equation $\vec\tau = i\vec\alpha$ to account for the pendulum 's finite extent perpendicular to the axis . the torque is produced by the force of gravity acting downward through the center of mass , and after a little rearranging the equation becomes $$\ddot\theta + \frac{mg r_g}{i}\sin\theta = 0$$ where $i$ is the moment of inertia of the pendulum and $r_g$ is the radius at which the gravitational force acts . notice that the behavior of the pendulum in this model is determined entirely by the value of the constant $\omega^2 \equiv \frac{mg r_g}{i}$ . for a point pendulum of length $l$ , you have $r_g = l$ and $i = ml^2$ , so $\omega^2 = \frac{g}{l}$ . we can therefore define the " effective length " of a more complex pendulum as the length that gives the same value of $\omega^2$ , $$l_\text{eff} = \frac{g}{\omega^2} = \frac{i}{m r_g}$$ for the dripping pendulum , we already have $r_g$: it is just the radius of the swing of the center of mass , which is what you were calling " effective length " in the problem . but the new definition also incorporates the moment of inertia of the pendulum bob around the pivot point . $$i = \bigl ( i_\text{shell , cm} + m ( l + r ) ^2\bigr ) + \bigl ( i_\text{water , cm} + m ( l + r - z_w ) ^2\bigr ) $$ the first term is the moment of inertia of the spherical shell around its own center of mass , $i_\text{shell , cm} = \frac{2}{3}mr^2$ . the third term is the moment of inertia of the water around its own center of mass . it takes the shape of a " chopped " sphere so we have to do the integral to calculate it : $$i_\text{water , cm} = \iiint x^2\ , \mathrm{d}^3 m = \int_{-r}^{h-r}\int_{0}^{2\pi}\int_{0}^{\sqrt{r^2 - z^2}} \bigl ( ( s\cos\phi ) ^2 + ( z - z_w ) ^2\bigr ) d\ , s\ , \mathrm{d}s\mathrm{d}\phi\mathrm{d}z$$ the second and fourth terms in $i$ come from the parallel axis theorem . putting it all together , we get a long but straightforward expression which can be written as : $$l_\text{eff} = l + \frac{12mrl + 20mr^2 +\pi dh^2\bigl [ -\frac{9}{5}h^3 + 24r^2 ( l+2r ) +3h^2 ( l+5r ) -4hr ( 4l+11r ) \bigr ] }{12m ( l+r ) + \pi dh^2\bigl [ 4l ( 3r-h ) +3h^2-16hr+24r^2\bigr ] }$$ ( yes , i used mathematica for the algebra :-p ) the graph for $l=10$ looks like this : the red line is $l_{eff}$ , the blue line is the swing radius of the center of mass ( the same thing that was in the last graph ) just for comparison . notice that for these values they are pretty similar , and as $r$ shrinks relative to $l$ the curves get even closer together . in fact , you can show this from the analytical analysis as well . i will not show the complete details , but if you eliminate $h$ and $r$ in favor of $\epsilon = \frac{h}{r}$ and $\beta = \frac{r}{l}$ , and do a series expansion in $\beta$ , if mathematica is to be trusted , you wind up with $$\frac{l_{eff}}{l} = 1 + \beta + \frac{2}{3} ( \beta^2 - \beta^3 ) + p^4 ( \epsilon ) \mathcal{o} ( \beta^4 ) $$ in other words , the amount by which the sphere is filled does not even matter until you reach the fourth-order correction in $\frac{r}{l}$ .
the link you provided is a page of g t'gooft on an official site and is quite interesting reading . the statement you are wondering about is : consider the scientific facts concerning the standard model . fact is that the w+ , w and the z boson each carry three spin degrees of freedom , whereas the yang-mills field quanta , which describe their interactions correctly in great detail , each carry only two . those remaining modes come from the higgs field . what this means is that three quarters of the field of the higgs have already been found . the fourth is still missing , and if you calculate its properties , it is also clear why it is missing : it is hiding in the form of a particle that is difficult to detect . he is stating that the higgs ' existence is based on the very strong verification of the standard model . there is strong experimental verification of the existence of w+ w- and z the construction of the theory is such that since the gauge bosons exist one has 3/4 of the data needed to confirm the model , of which the existence of the higgs is 1/4 , counting up dimensions , of the necessary evidence/prediction . he is equating the existence of the higgs to the validity of the sm in this statement .
because a proton can decay to a positron . it is an experimental fact that the proton and positron charges are very close . to conclude that they are exactly equal requires an argument . if a proton could theoretically decay to a positron and neutral stuff , this is enough . in qed , charge quantization is equivalent to the statement that the gauge group is compact . this means that there is a gauge transformation by a full $2\pi$ rotation of the fields which is equivalent to nothing at all . under these circumstances you have the following : charge is quantized there are dirac string solutions which have a magnetic flux indistinguishable from no flux ( the magnetic flux is the phase around a loop ) . if you have any sort of ultraviolet regulator , either a gut or gravity , the existence of dirac strings leads to monopoles . if you do not have an ultraviolet regulator , it is consistent to make all the monopoles infinitely massive . so the question is why is the u ( 1 ) of electromagnetism compact . there are two avenues for answering this : a compact u ( 1 ) emerges from a higher gauge group , because all higher gauge groups must be compact for the kinetic terms to have the right sign . breaking a compact group produces a subgroup , which is necessarily compact . it is also true that in any gut theory producing electromagnetism , you get monopoles , so you automatically get charge quantization by dirac 's argument . but even if you have a u ( 1 ) which is not part of a gut , there are constraints from gravity . if you have particle with charge q and a particle with charge q ' , and they are not rational multiples of each other , you can produce a particle with charge $nq - m q'$ by throwing n q particles into a black hole , waiting for m q ' particles to come out , and letting the resulting black hole decay , while throwing back any charge particle that comes out . this means that in a consistent quantum gravity , you need either charge quantization or a spectrum of charges that accumulates near zero . further , in order for the theory to be consistent , a black hole made from the wee charges must be able to naturally decay to wee charged things , and barring a conspiratorial spectrum of charges and masses , this strongly suggests that the mass of the wee charges must be smaller than the charge , meaning that as the charge gets small they become massless . so in quantum gravity , the only alternative to charge quantization is a theory with nearly massless particles with extremely tiny charges , and this has clear experimental signatures . i should point out that if you believe that the standard model matter is complete , then anomaly cancellation requires that the charge of the proton is equal to the charge of the positron , because there is instanton mediated proton decay as discovered by t'hooft , and this is something we might concievable soon observe in accelerators . so in order to make the charge of the proton slightly different from the electron , you can not modify parameters in the standard model , you need to add a heck of a lot of unobserved nearly massless fermions with tiny u ( 1 ) charge . this is enough conspiratorial implausibility , that together with the experimental bound , you can say with certainty that the proton and electron have exactly the same charge .
of course that black holes can swallow and do swallow additional matter and increase their mass in finite time , whatever reasonable coordinates we choose . yes , the schwarzschild radius for the combined system smaller black hole + extra matter ( "food" ) is larger than it is for the original smaller black hole which means that it is enough for the new matter to cross a critical surface that is outside the original event horizon of the smaller black hole . this occurs in finite time , even from the external observer 's viewpoint , and the black hole simply grows in size . it is a routine process we observe at many places . for example , the black hole at the galactic center devours new matter all the time . incidentally , there can not exist any " concentric pairs of black holes " solutions to the 3+1-dimensional general relativity . once we know that it is the empty schwarzschild metric up to the event horizon ( from outside ) , then it is the black hole and there is no way for the metric to " unbecome " a black hole again . everything that is inside the event horizon is , by definition , causally disconnected from infinity  it can not escape to infinity again . so if there is a matter that is even " more inside " , at an even lower value of $r$ , it is clear that it is still inside the black hole in the sense that it can not get to infinity . so it is impossible to claim that there is a new " island " near the center of the black hole that would be " outside the black hole " again . such a thing can not happen , it would be a logical oxymoron , almost by the definition of the event horizon .
the concept of vacuum in physics indeed comes from two different theories . the general relativity vacuum is a space-time model region without matter . in general relativity all of space-time has a " curvature " which relates to the metric which can all have measurable effects , such as the bending of light rays ( in the vacuum ) near a massive object . one may wish to be a little careful of how one conceptualises the vacuum of empty space however since no events occur there since there is no interacting matter there . as soon as we have interacting matter we no longer just have a vacuum . also general relativity has introduced a term , called the cosmological constant $\lambda$ , which could be said to measure the curvature of the vacuum at the cosmological level . in quantum theory there is the concept of the " vacuum state " which is a little different : it is the lowest possible energy state of a given quantum system . this lowest possible energy state has quantum fluctuations consistent with the $\delta e \delta t &gt ; h$ uncertainty principle . thus if we straightforwardly apply the " vacuum state " concept to the space-time vacuum we get a conceptually different model , called the " curved space vacuum " . some calculations show that this combined " curved space vacuum " is rather different from the vacua of the two component theories : general relativity and quantum mechanics . it has some interesting properties , like temperature , that the component theories dont have and some calculations do suggest that the " quantum vacuum energy " is different from the corresponding expected $\lambda$ value by $10^{120}$ . these results depend a little on what , if any , quantum fields uniformly pervade space and this aspect is not yet settled .
you are right that the oscillations of the electromagnetic field need not have any spatial extent . the oscillations , as you point out , are in the strength of the electric and magnetic fields . if i understand your question correctly , you are asking why then can some objects distinguish between the two different polarizations of light . this is because anisotropic materials ( those which do not have exactly the same structure in all directions ) can act differently to electric fields which lie along their different axes . in the image below the material in the middle does not support an electric field in the horizontal direction . when you try to establish an electric field in the horizontal direction , energy is taken out of the source which is trying to establish it ( the light in this case ) . so , this material , which acts as a typical polarizer , only allows light which is polarized in the vertical direction to pass . there are other ways to distinguish between the two different polarizations of light . birefringent crystals , for instance , have a different index of refraction for the two different polarizations of light . they therefore deflect the two different polarizations of light by a different amount .
if there are no real solutions , that means that speed is too small to throw that rock so high ! so the problem was never solved properly by the one who created it . imho , if you also have volume of the rock , you could calculate buoyancy and that would reduce gravitational acceleration and possibly rock could get so high .
it follows from the rule $$\frac{\partial}{\partial x^i} e^{a_j x^j}= a_i e^{a_j x^j} , $$ which by repeated use leads to $$f ( \frac{\partial}{\partial x^i} ) e^{a_j x^j}= f ( a_i ) e^{a_j x^j} , $$ where $f$ is a polynomial . more general classes of functions $f$ can be reached by taking appropriate limits .
the " integration " we refer to when we " integrate heavy fields out " is nothing else than the feynman path integral  a way to calculate amplitudes in a quantum field theory using the sums-over-histories . if you have been thinking about any other kind of " integral " or if you even replaced the integral by contractions or arbitrary different operations , you had to end up with confusing ( or completely wrong ) conclusions . the feynman path integral gives you a formula for green 's functions and other amplitudes $$ a = \int {\mathcal d}\phi_{\rm light} \ , {\mathcal d}\phi_{\rm heavy}\ , \exp ( is ) \prod_{i}v_i$$ where $v_i$ are some insertions in the integral that are chosen according to the choice of the quantity ( amplitude ) we want to quantify . we integrate over all configurations of all fields etc . to integrate out $\phi_{\rm heavy}$ means to divide the integration process to two steps and first integrate over some degrees of freedom , namely $\phi_{\rm heavy}$  which may be many fields  for fixed values of the remaining fields , $\phi_{\rm light}$ . the resulting i.e. remaining integral which still waits to be integrated over the remaining light fields ( without insertions ) is interpreted as $\exp ( is_{\rm effective} ) $ where $s_{\rm effective}$ only depends on the light degrees of freedom $\phi_{\rm light}$ . $$ a_\text{after integrating out} = \int {\mathcal d}\phi_{\rm light} \ , \exp ( is_{\rm effective} ) \prod_{i}v_i , \\ \exp ( is_{\rm effective} [ \phi_{\rm light} ] ) = \int {\mathcal d}\phi_{\rm heavy}\ , \exp ( is ) $$ in this way , we eliminate the heavy degrees of freedom and calculate the effective action that remembers all the loop effects that the now-forgotten heavy fields used to cause . however , this effective field theory with an effective action  a result of integrating out the heavy fields  is only good for asking low-energy questions , of course . the insertions $v_i$ we may insert into the simplified theory that only depends on $\phi_{\rm light}$ cannot depend on the heavy degrees of freedom anymore , of course : they have been disappeared . but in principle , if you calculate the path integral over the heavy degrees of freedom " exactly " , the effective action may give you completely accurate results for the scattering of the light fields , and so on . in practice , we integrate over the heavy degrees of freedom " approximately "  we assume that the effective action only contains some low-dimension operators ( the renormalizable ones and perhaps one or two extra operators that are non-renormalizable ) and we study what happens with their coefficient . if we wanted the effective action to give exactly the same result for observables depending on the remaining light fields as the original action , we would have to include everything and the effective action would contain arbitrarily high-dimension non-renormalizable operators  equivalently , it would be non-local . if you need to avoid the feynman path integral , then you should interpret " integrating out a set of fields " by " finding the dynamics for the remaining fields that produces the same interactions or green 's functions for them as in the original theory that did contain the now-integrated-out fields " . feynman 's approach gives us a straightforward tool to do such a thing ; it could be very hard to derive the right algorithm in a different computational approach to quantum field theory .
you are basically correct . the apple and the earth exert equal but opposite forces on each other but , as they are in contact , neither of them can move . the forces are balanced by the internal pressure in the apple and in the floor . consider the case where the apple is not resting on the floor . in that case the apple will , of course , fall towards the earth . however , at the same time the earth falls upward towards the apple . as the earth has so much more mass than the apple , its acceleration is correspondingly smaller , due to $$a = f/m$$ hence we never talk about the movement of the earth , but it is there . for the same reason , whenever a spacecraft uses a planet for a " slingshot " approach to increase its speed , the speed of the planet around the sun also changes , by a minute amount . when the moon travels around the earth , what really happens is that both travel around their common centre of mass , which is still inside the earth , but not far from the surface .
the following essay by n.p. landsman : " when champions meet : rethinking the bohreinstein debate . " studies in history and philosophy of science part b : studies in history and philosophy of modern physics 37.1 ( 2006 ) : 212-242 contains an extensive bibliography on the debate . in particular the main references containing or discussing the letters are listed in footnote 1 on page 213 .
the big bang was everywhere , because distance did not exist before it , so from one perspective , everywhere may be the centre ( especially as some theorists think , the universe does not have an edge ) the real issue is that the question should not matter , as we can only gain information from distances within our visible radius and once we get to that limit , what we see gets closer to the big bang so it all looks closer to the centre . tricky eh
as i recall , volcanic activity on io deposits about a millimeter of material on the surface every year . this is matter brought up from the interior and spread over the surface . the volcanic activity is caused by tidal stress . io 's orbit around jupiter is not perfectly circular ; its distance varies as it orbits jupiter every 42+ hours . since tide varies with the cube of the distance , and io is the closest of the 4 large moons , it is much more strongly affected than the other 3 large moons . the tidal stress causes it to flex , which generates heat . one millimeter per year does not seem like much , but it works out to a meter every thousand years , a kilometer every million years , and 1000 kilometers in a billion years . since the radius is just over 1800 kilometers , that is more than enough to recycle the entire body of the moon over the 4+ billion year lifetime of the solar system . this assumes that material is brought all the way from the core up to the surface . i do not know ( and i do not know that anyone knows ) just how deep the redistribution of matter goes . io is massive enough that its gravity forces it to retain a spherical shape . it is not perfectly spherical ( it has mountains taller than everest ) , but over time it is very close . it does not significantly affect its orbit or destroy the moon because the eruptions are not powerful enough to eject much matter to escape velocity ; it all falls back on the surface .
for example , the dirac-lorentz equation .
is it a mathematical definition that you truly want ? i am a student myself , and i find the transformation definition to be the most elucidating . it seems the central idea is this : you want the quantities to look a certain way , regardless of the point of view . http://www.grc.nasa.gov/www/k-12/numbers/math/documents/tensors_tm2002211716.pdf this pdf was particularly elucidating , along with boas ' chapter on tensor analysis . succinctly put , " not all matrices are tensors , but all rank-2 tensors are matrices . "
the quantum-mechanical proof is actually pretty much identical to the classical one given in the link . you simply replace the integral over the phase space with a trace over the states in the hilbert space . the equilibrium density operator is $$ \rho = \frac{e^{-\beta h}}{z} , $$ where the partition function is $z = \mathrm{tr} ( e^{-\beta h} ) $ and the inverse temperature is $\beta = 1/k_bt$ . explicitly , the expectation value of the energy is given by $$ \langle h \rangle = \frac{1}{z}\mathrm{tr} ( h e^{-\beta h} ) = -\frac{1}{z} \frac{\partial}{\partial\beta} \mathrm{tr} ( e^{-\beta h} ) = -\frac{1}{z} \frac{\partial z}{\partial\beta} . $$ likewise , you should find $\langle h^2 \rangle = \frac{1}{z}\frac{\partial^2 z}{\partial\beta^2}$ , and $$ c_v = \frac{\partial \langle h\rangle}{\partial t} = -\frac{1}{k_b t^2}\frac{\partial}{\partial\beta} \left [ \frac{1}{z} \mathrm{tr} ( h e^{-\beta h} ) \right ] = \frac{1}{k_bt^2}\left ( \langle h^2 \rangle - \langle h \rangle^2\right ) . $$ you will have to fill in a few steps for yourself to prove the last equality .
from a purely temperature point of view , not human perceived level of hotness , it is better to point the fan outward . this is because the fan motor will dissipate some heat , and when the air is blown outwards , this heat goes outside . this is all assuming the room has enough ventillation cracks and the like that the pressure inside is still effectively the same as the pressure outside regardless of what the fan is doing . human-perceived hotness is quite different because humans are a heat source themselves and have a built-in evaporative cooling system . air flow will help with the cooling process and remove heat from the area around the body . a human sitting in a chair in the room with the fan blowing in will feel cooler than with the fan blowing out due to the higher motion of the air in the room . if the point is to make you in the room feel cooler , blow the air in . the extra power from the fan motor is a miniscule effect in the overall scheme of a normal room in a house and the kind of airflow such a fan would create . worrying about the fan motor power is really nitpicking , but can be significant for things like cooling chassis of electronics . another issue is where the air comes from that enters the room if the fan blows outward . if it is coming from other parts of the same house that are also hot , then that may technically be the most efficient for bringing down the temperature in the whole house , but less useful for just the room in question .
the solution to the confusion lies in the definition of $a$ . it is not the relative atomic weight , which is a dimensionless number . according to the particle data group , it refers to the atomic mass of the absorber , a quantity of unit g mol$^{-1}$ .
this is very simply down to surface tension - a very small amount of beer ( in a relatively thin film ) will adhere strongly to a glass as the percentage of bonds between the water and the glass is high . as the drop size increases ( up to a point ) more and more of the bonds are internal increasing the strength of the drop and decreasing the adhesion to the glass .
most naturally , both sine and cosine  i suppose you meant simply $x=\sin ( t ) $ and $x=\cos ( t ) $ because $\cos ( x ( t ) ) $ is not a particular path , it is a functional of a path  are solutions to the differential equation $$ \frac{d^2}{dt^2} x ( t ) = -x ( t ) $$ which is the equation for a harmonic oscillator ( with a unit spring constant , in this case ) . this equation may also be derived as the euler-lagrange equation from the action for the harmonic oscillator , $$ s = \int dt \left [ \frac 12 \left ( \frac{dx ( t ) }{dt}\right ) ^2 - \frac {x^2}{2} \right ] $$ which is the difference between the kinetic and potential energy ( the lagrangian ) integrated over time ( the action ) .
boy , this was tricky , but the secret is in conservation of momentum . see , you are assuming that , after the collision , the velocity of the ball-elevator ensemble is $u$ , but this is not fully true : it will be $u ' = u + \frac{m}{m+m}\sqrt{2gh}$ , $m$ being the mass of the elevator . of course if $m \to \infty$ that reduces to $u ' = u$ , but when computing the ke , something funny happens : $$\frac{1}{2} ( m+m ) u'^2 = \frac{1}{2} ( m+m ) u^2 + \frac{m^2}{m+m}gh + um\sqrt{2gh}$$ that last term which does not depend on $m$ is the key here . of course the first term , with the $ ( m+m ) $ dominates the others , but it will be cancelled out by identical terms in the ke before the collision . but if you assume that because $m \to \infty$ you can take $u ' = u$ , you will be missing this last term , which exactly cancels out that extra energy . doing the math for a finite elevator mass , and using conservation of momentum to compute the final velocity , you eventually get to energy lost in an inellastic collision to be $\frac{1}{2}\frac{mm}{m+m} ( u-v ) ^2$ , which for $m \to \infty$ reduces to $\frac{1}{2}m ( u-v ) ^2$ , as johannes already pointed out .
no , nasa has not confirmed that . what nasa has confirmed , again , is that it has some rather nutty folks working for it .
the following paper currently has almost 7500 citations . why does it have so many ? because it is amazing ; read it . anti de sitter space and holography , by edward witten the canonical , detailed review is the following paper which currently has nearly 4000 citations ( pretty weak . . . i know ) . large n field theories , string theory and gravity , by magoo everyone i know who works in ads/cft has read witten 's paper above more than once , and at least once with pen and paper in hand to redo his computations . the paper by magoo is the go-to reference in the field .
the wave function of a single photon has several components - much like the components of the dirac field ( or dirac wave function ) - and this wave function is pretty much isomorphic to the electromagnetic field , remembering the complexified values of $e$ and $b$ vectors at each point . the probability density that a photon is found at a particular point is proportional to the energy density $ ( e^2+b^2 ) /2$ at this point . but again , the interpretation of $b , e$ for a single photon has to be changed . so whether the field around an object is electric or magnetic or both is encoded in the " polarization " of the virtual photons . you may imagine that the photon has 6 possible polarizations or so , identified with the components of $e$ and $b$ . well , for a particular direction , it is really just the $e+ib$ combination that acts as the wave function , so there are only three polarizations for a given direction - and one of them ( the longitudinal ) is forbidden , too . ; - ) but the qualitative point that there are many polarizations is correct . however , as emphasized repeatedly , you should not imagine that a virtual photon is a real particle that can be counted . that is a reason why qgr 's answer is pretty much irrelevant for your question because there is no operator counting virtual photons at all - so it makes no sense to ask whether it commutes with other operators . qgr may have thought about real photons but he has not answered your question , anyway . by the way , static fields correspond to a vanishing frequency - because everything with a non-vanishing frequency will go like $\exp ( i\omega t ) $ or $\cos ( \omega t ) $ . so if you want to describe the fields of electric sources and magnets as a collection of virtual photons , you must realize that the static nature of the field implies that the relevant fields will have the energy equal to zero . but the momentum is nonzero because the field depends on space - because of the sources . such virtual photons are very far from being on-shell - they are very virtual , indeed . it is not too helpful to talk about virtual photons with particular frequencies and wavenumbers if there are electric sources in the middle of the region you want to describe . the fourier analysis is only helpful for photons in a pretty much empty space . but you could calculate the probabilities of various outcomes for a charged particle in an external electric or magnetic field , produced e.g. by many spinning electrons , using feynman diagrams - where the virtual photons are the internal lines . the feynman diagrams would be able to calculate the force acting on the probe particle . some terms in the force would not depend on the velocity - the electric forces - while others would depend on the velocity - the magnetic ones . these different terms would always come from the " same type " of virtual photons but all these photons depend on the sources of the field , so you would of course get different results for electric and magnetic fields . all this stuff is confusing and really unnecessary . if you worry that quantum electrodynamics will not reproduce basic properties of electromagnetism - such as the difference between electricity and magnetism ; or the difference between attractive and repulsive forces - then you should not worry . it can be easily demonstrated that in the classical limit - e.g. for strong enough fields with a low enough frequency - the quantum electrodynamics ( and the quantum field ) directly reduces to the right classical limit , the classical electrodynamics ( and the classical fields ) . virtual photons are just a very helpful tool to study all kinds of processes similar to scattering . their maths can be deduced from quantum fields - not the other way around - and these virtual photons do not happen to be useful to describe your kind of highly classical situations . best wishes lubo
this is , as lubos mentioned , an effect of the wave nature of light , and cannot be explained using geometrical optics . what you are seeing is called the point spread function ( psf ) of the imaging system . because stars are so far away that they are effectively point sources of light ( i.e. . they are spatially coherent ) their image will be the psf of the imaging system . up to a scale factor , the psf is the fourier transform of the pupil of the imaging system . for a lens system , the pupil is usually just a circle , so the psf is the 2d fourier transform of a circle : $$ \frac{j_1 ( 2 \pi \rho ) } {2 \pi \rho} $$ where $j_1$ is the order 1 bessel function of the first kind . however , most modern telescopes are built with reflective optics , and there are various obscurations in the pupil due to the structures that support the secondary mirror . this more complicated pupil shape can produce a variety of artifacts in the psf . the starburst pattern in your example images could be due to a simple " plus " shaped structure supporting the secondary mirror , but the effect is so strong that i suspect it was emphasized for creative effect . i am not sure how the hubble psf looks , off the top of my head . in general , an image can be represented by the convolution of the ideal image $g ( x , y ) $ with the psf , usually denoted $h ( x , y ) $ . in the case of a point source ( so $g ( x , y ) $ is a delta function , $\delta ( x , y ) $ ) it is trivial that the image is a copy of the psf : $$h ( x , y ) =\int_{-\infty}^{\infty} \delta ( \xi , \eta ) h ( x-\xi , y-\eta ) d\xi d\eta $$ but in the case of a more complicated object , the convolution by the psf acts to smooth or blur the image . this is why an out of focus camera produces blurry images . although aberrations also degrade the image under a geometrical approximation , this is more accurate . the geometrical case and the wave optics ( diffraction ) result will become closer as the aberrations become large . sometimes this effect is produced intentionally . you can actually buy filters for commercial cameras that have a fine grid of wires to produce this starburst effect for creative purposes . nb : this answer ignores any discussion of phase effects in diffraction ( because i am short on time , i may update later ) . if you would like to learn about diffraction and the wave optics approach to imaging , the leading text on the planet is " introduction to fourier optics " by j . goodman . it is an absolutely spectacular book .
i could collect the sources from various websites and place it here , browse through their content to get what you want are there any good physics podcasts ? physics general interest seminar podcasts physics podcasts podcasting the mysteries of the universe the first link has some good collection . enjoy
yes he feels lighter unless he is on the north or south pole , and on most of the asteroid it would feel like partly sideways gravity . since we can assume the asteroid is rigid , it is distinguished in such a way from my recent [ hydrostatic self-gravitation problems ] [ 1 ] question . yes he can jump higher . . . unless he is standing on a pole . this part is more complicated , i will address more detail below on equator if the astronaut jumps from the equator he leaves the ground from a point that has an apparent gravity ( gravity combined with rotational acceleration ) normal to the surface . for small jumps on a large asteroid , yes , he will make it back to the same spot . but what is the cutoff ? i believe it would be the point at where orbital dynamics started to matter . $$g = \frac{a}{r^2} + b \frac{v_x^2}{r}$$ i believe this would be the relevant equation , since $v_x$ , the horizontal velocity in the cm frame , would be the invariant quantity . say he jumps a distance $d$ upwards , then to the extent that $d ( 2 a / r^3 + b v_x^2/r^2 ) \ll a/r^2+b v_x^2/r $ , he would land about in the same place . if this is not true he would land in a different place . i am not entirely sure about this , but it is my best shot . poles again , if he is on the poles the rotation does not matter , and he can jump to infinity and make it back to the exact same spot as an academic exercise . in reality , the jump speed would have to be very carefully produced to go far and make it back without reaching escape velocity . elsewhere again , the apparent gravity is non-normal . so the astronaut already feels like he is on a slope , and obviously he would have to jump in the direction that felt like " up " for this to make sense at all . but if he did that , similar rules to the equator would apply .
the dimension of the hilbert space of a free particle is countable . to see this , simply note that the hilbert space of a free particle in three dimensions is $l^2 ( \mathbb{r}^3 ) $ . the dimension theorem guarantees that any two bases of of a vector space have the same cardinality , which allows us to define the dimension of a vector space as the cardinality of any basis . the hilbert space $l^2 ( \mathbb r^3 ) $ is separable ; it admits a countable , orthonormal basis ( e . g . hermite functions ) . therefore , by the definition of the dimension of a vector space , it has countable dimension .
when $v\ll c$ , the ratio $\beta = v/c$ is small , so we perform a taylor expansion about $\beta = 0$ ; \begin{align} \frac{1}{\sqrt{1-\beta^2}} = ( 1-\beta^2 ) ^{-1/2} = 1+\frac{1}{2}\beta^2+\frac{3}{8}\beta^4+\cdots \end{align} now plug this into your expression and simplify .
a classic book is " molecular quantum mechanics " by atkins . possibly also " the chemical bond " by murrell , kettle and tedder , though i think atkins is better .
this says it concisely , when describing the effect of tides : gravitational coupling between the moon and the tidal bulge nearest the moon acts as a torque on the earth 's rotation , draining angular momentum and rotational kinetic energy from the earth 's spin . in turn , angular momentum is added to the moon 's orbit , accelerating it , which lifts the moon into a higher orbit with a longer period . as a result , the distance between the earth and moon is increasing , and the earth 's spin slowing down . in fewer words : it is the tides . edit : i am copying from a comment : to show the right sign , one must show that the orbital angular momentum of the moon actually increases with the radius - despite the decreasing velocity as the function of the radius for a $1/r$ potential , $mv^2\propto m/r$ says $v\propto 1/\sqrt{r}$ , so the angular momentum $l=rp=mrv=mr/\sqrt{r}\propto \sqrt{r}$ which increases with $r$ .  lubo motl in addition i found this better link by googling .
as you can see on these absorption spectra for $\textrm{h}_2\textrm{o}$ and $\textrm{c}\textrm{o}_2$ , both molecules have moderate to strong absorbtion in the mid-ir wavelengths , with the absorption of $\textrm{c}\textrm{o}_2$ extending out into the longer wavelengths . other molecules common in the atmosphere do not have such strong absorption at the wavelengths given off by thermal radiation . if you are asking why that is , i am afraid i can not give you a very detailed answer , except to say that the absorption spectra of molecules ( and atoms ) is governed by quantum mechanics . maybe somebody else can explain how they would be calculated from principles , but that is beyond my education .
to understand binding energy and mass defects in nuclei , it helps to understand where the mass of the proton comes from . the news about the recent higgs discovery emphasizes that the higgs mechanism gives mass to elementary particles . this is true for electrons and for quarks which are elementary particles ( as far as we now know ) , but it is not true for protons or neutrons or for nuclei . for example , a proton has a mass of approximately 938 mev/c2 , of which the rest mass of its three valence quarks only contributes about 11 mev/c2 ; much of the remainder can be attributed to the gluons ' quantum chromodynamics binding energy . ( the gluons themselves have zero rest mass . ) so most of the " energy " from the rest mass energy of the universe is actually binding energy of the quarks inside nucleons . when nucleons bind together to create nuclei it is the " leakage " of this quark/gluon binding energy between the nucleons that determines the overall binding energy of the nucleus . as you state , the electrical repulsion between the protons will tend to decrease this binding energy . so , i do not think that it is possible to come up with a simple geometrical model to explain the binding energy of nuclei the way you are attempting with your a ) through o ) rules . for example , your rules do not account for the varying ratios of neutrons to protons in atomic nuclei . it is possible to have the same total number of nucleons as $fe^{56}$ and the binding energies will be quite different the further you move away from $fe^{56}$ and the more unstable the isotope will be . to really understand the binding energy of nuclei it would be necessary to fully solve the many body quantum mechanical nucleus problem . this cannot be done exactly but it can be approached through many approximate and numerical calculations . in the 1930s , bohr did come up with the liquid drop model that can give approximations to the binding energy of nuclei , but it does fail to account for the binding energies at the magic numbers where quantum mechanical filled shells make a significant difference . however , the simple model you are talking about will be incapable of making meaningful predictions . edit : the original poster clarified that the sign of the binding energy seems to be confusing . hopefully this picture will help : this graph shows how the potential energy of the neutron and proton that makes up a deuterium nucleus varies as the distance between the neutron and proton changes . the zero value on the vertical axis represents the potential energy when the neutron and proton are far from each other . so when the neutron and proton are bound in a deuteron , the average potential energy will be negative which is why the binding energy per nucleon is a negative number - that is we can get fusion energy by taking the separate neutron and proton and combining them into a deuteron . note that the binding energy per nucleon of deuterium is -1.1 mev and how that fits comfortably in the dip of this potential energy curve . the statement that $fe^{56}$ has the highest binding energy per nucleon means that lighter nuclei fusing towards $fe$ will generate energy and heavier elements fissioning towards $fe$ will generate energy because the $fe$ ground state has the most negative binding energy per nucleon . hope that makes it clear ( er ) . by the way , this image is from a very helpful article here : http://nuclearenergy.canalblog.com/archives/2012/01/03/23145897.html which should also be helpful for understanding this issue .
i do not think you need a microscopic explanation for the phenomenon . all you need is the fact that the phase of the wave must be continuous across the water-air boundary . if you take any two points at the interface , spaced a distance $w$ apart , then the length of the line segment , $l$ , i have labelled d$\phi$ is just $w \sin i$ , and the phase difference is just $2\pi l/\lambda$ , so the phase difference between the light beams at the two points is is just : $$ d\phi = w \sin i \frac{2\pi}{\lambda} = w \sin i \frac{2\pi f n_i}{c} $$ now we appeal to huygen 's principle and say that we can regard our two points as sources for the light on the other side of the interface , and because the phase is constant across the interface the phase difference for the emitted rays must be the same $d\phi$ that we calculated above : $$ d\phi_r = d\phi_i $$ or : $$ w \sin r \frac{2\pi f n_r}{c} = w \sin i \frac{2\pi f n_i}{c} $$ a quick rearrangement gives snell 's law , and because $\sin r$ can not be bigger than unity we conclude there can not be any transmission for $i$ greater than some critical angle i.e. we get total internal reflection . so the physical origin is the continuity of the electromagnetic field that makes up the light and you do not need to think about what the atoms are doing . i suppose you could say that the electron density around the atoms at the interface will be oscillating in phase with the incoming wave and also in phase with the refracted wave ( which is why the phase across the interface must be constant ) and this would not be possible if any light were transmitted above the critical angle . response to comment : if you consider the atoms/molecules at the interface , these will have electrons in outer orbitals and/or molecular orbitals that are polarisable so the light will induce an oscillating dipole . so you have a layer of oscillating dipoles at the interface , and these will themselves radiate an em wave . the direction ( s ) the radiation is emitted depend on the phase changes as you move within this layer because the dipoles will interfere with each other , constructively in some directions and destructively in others . for example if all the dipoles are in phase the light will be emitted normally . if the phase changes linearly with distance as you move in the layer the light will be emitted at an angle . in the case of total internal reflection there is no direction in the low refractive index side where all the dipoles can interfere constructively , but there is such a direction in the high refractive index side . therefore all radiation from the dipoles will be emitted into the high $n$ side i.e. you have total internal reflection . but be cautious about taking this as anything more than an analogy as you can not treat the light and the dipoles separately . i am not implying that the light is absorbed by the dielectric then re-emitted .
there is some work , pioneered by nader engheta and mario silveirinha . the basic idea is to make a waveguide out of a medium whose permeability $\varepsilon$ is close to zero , which basically allows a sort of " classical tunneling " , since the phase velocity in the medium is very large . say , transporting a light field unchanged from one end of the waveguide to the other . it is been experimentally demonstrated for microwaves , but i doubt that it will be possible with visible wavelengths in the immediate future . here are some papers that you could read : silveirinha and engheta ( 2006 ) . tunneling of electromagnetic energy through subwavelength channels and bends using -near-zero materials . phys . rev . lett . 97 , 157403 . http://prl.aps.org/abstract/prl/v97/i15/e157403 ( the original paper . ) liu , cheng , hand , mock , cui , cummer , smith ( 2008 ) . experimental demonstration of electromagnetic tunneling through an epsilon-near-zero metamaterial at microwave frequencies . phys . rev . lett . 100 , 023903 . http://prl.aps.org/abstract/prl/v100/i2/e023903 ( experimental demonstration for microwaves , using metamaterials , i.e. materials with repeated structures that are smaller than the wavelength . ) edwards , alu , young , silveirinha , engheta ( 2008 ) . experimental verification of epsilon-near-zero metamaterial coupling and energy squeezing using a microwave waveguide . phys . rev . lett . 100 , 033903 . http://prl.aps.org/abstract/prl/v100/i3/e033903 ( published only a week after the above paper , this is another experimental demonstration for microwaves taking a different approach . instead of using metamaterials , they operate the waveguide at its cutoff point . this makes the effect strictly monochromatic , i think , but the waveguide is much simpler and cheaper to make . ) silveirinha and engheta ( 2009 ) . transporting an image through a subwavelength hole . phys . rev . lett . 102 , 103902 . http://prl.aps.org/abstract/prl/v102/i10/e103902 ( theoretical paper that describes transporting what you call a light field . ) silveirinha and engheta ( 2012 ) . sampling and squeezing electromagnetic waves through subwavelength ultranarrow regions or openings . phys . rev . b 85 , 085116 . http://prb.aps.org/abstract/prb/v85/i8/e085116 ( latest updates - i have not kept abreast of any developments after 2009 so i am not quite sure what is new here . )
neither effect can really be calculated in a meaningful way . kinematic time dilation describes the time dilation of one frame of reference relative to another . there is no preferred frame of reference , so there is no way to say what it would mean to remove the effect of kinematic time dilation . gravitational time dilation is a concept that makes sense in a static spacetime , and in that case the time dilation between two different points is given in terms of the difference $\delta\phi$ in gravitational potential as $e^{\delta\phi}$ . here again you have the problem of what to compare to . do you want to compare to interplanetary space ? interstellar space ? space outside our local cluster of galaxies ? as you continue this process , you reach cosmological distances , at which point you run into the problem that cosmological spacetimes are not static , and the whole thing becomes meaningless . this is what relativity is all about . there is no best measure of time . it is all relative .
taste : there are 5 basic tastes that the human tongue can detect . they are sweet , savory , salty , sour and bitter . these are detected by taste receptor cells on our tongue , i will not go deep into the biology part . the basic tastes of sweet , salty and sour have different thresholds , or concentration levels , at which they can be detected . in other words , it is easier to detect some flavors at low concentrations compared with other flavors . taste thresholds can vary from person to person . so just like sight , our sense of taste also has some thresholds or limitations . smell : it is very similar to taste in the sense that there is an odor detecting threshold for every person which is the lowest concentration of a certain odor compound that is perceivable by the human sense of smell . again , the detection is done by receptors in our nose which can detect only certain compounds ( odor compounds ) to give us the perception of smelling something . touch : the somatosensory system is a complex sensory system . it is made up of a number of different receptors , including thermoreceptors , photoreceptors , mechanoreceptors and chemoreceptors . this means that when you touch something , you do not just detect one thing . you detect the heat transfer , rigidity , shape etc . when you say something feels cold , it just means it is transferring heat to your body very quickly ( like metal ) . there is also a minimum threshold of vibrations that your touch can detect . the sense of touch also includes receptors that feel pain ( unlike all the other senses ) which basically work the same way as the other receptors , except one major difference . the receptors detect something and send electromagnetic pulses to the brain at the speed of light which then sends an appropriate signal as a course of action or conclusion . but when you touch a hot plate , your body needs to tell you that you immediately need to stop touching it because of the huge amount of heat being transferred to your body . so , instead of the brain , the pulse goes to the spinal chord which already has the appropriate course of action ' coded ' into it , if you will , and that pulls your hand back immediately . it is involuntary , unlike the others .
i think the current opinion is that d-wave have demonstrated some quantum computing abilities but not all the features that a quantum computer could be capable of . their current system also performs the specific calculation ( a simulated annealing ) slower than a conventional computer can . that said - they are a genuine effort and certainly not cold-fusion/perpetual motion snake oil con . remember when government organisations like nasa and pseudo-government outfits like major defense companies invest in something it can be for more political reasons that the actual deliverables . or less cynically , that they are investing in the future .
well , consider this : the same thing happens with electromagnetic forces . we can describe them as particles responding to the presence of electric and magnetic fields , or we can describe them as resulting from the exchange of virtual photons . those views seem similarly incompatible , but nevertheless both theories ( classical electrodynamics and quantum electrodynamics , respectively ) give excellent predictions . we can not really say that one is more " right " than another ; we just have to accept them both . the situation with gravity is pretty much a direct analogy to electromagnetism . we can describe gravity as particles responding to the presence of spacetime curvature , or we can describe them as resulting from the exchange of virtual gravitons . as with em , these views would correspond to classical gravity and quantum gravity , respectively . but the difference is that , although general relativity fills the role of the classical theory , we do not have a good quantum theory of gravity yet . i would not say that the field/particle duality is one of the problems that impedes the combination of quantum mechanics with gr . after all , we had no problem getting around the dual descriptions of electromagnetism . it is just the peculiar details of quantum gravity that make it a difficult theory to develop .
yes , this is exactly how noise suppression headphones operate , works like charm , http://en.wikipedia.org/wiki/active_noise_control
the answer is that $\{|x\rangle\}$ is not a basis of $l^2 ( \mathbb r ) $ which admits only countable basis . the point is that objects like $|x\rangle$ are not vectors in $l^2 ( \mathbb r ) $ . to provide them with a rigorous mathematical meaning one should enlarge $l^2 ( \mathbb r ) $ into an extended ( non-hilbert ) vector space structure including schwartz distributions or adopt a viewpoint based on the so called direct integral of hilbert spaces . these are structures quite complicated to use with respect to a standard hilbert space . nevertheless the practical use of formal objects like $|x\rangle$ is quite efficient in physics provided one is able to distinguish between problems arising from physics and false problems just due to a naive misuse of the formalism . ( when i was student i wasted time in discussing if identities like $a|\psi\rangle = |a\psi \rangle$ had any sense . )
according to this site , we have about 433 working reactors , 65 under construction , 160 planned and 323 proposed which is too many . . . we are consuming about 67,990 tons per year of u-238 which would probably die out soon within about 75 years . besides fission products , spent fuel rods contain some plutonium produced by the u-238 in breeder reactors by absorbing neutrons . this plutonium and leftover uranium can be separated in a reprocessing plant ( reprocessing involves the removal of any leftover uranium and the plutonium that has been formed ) and could be used as reactor fuel again . in fact , not only u-235 is used as a nuclear fuel . for now , it is the one which is mostly used . but , there are other fuels like pu-239 from breeders which we would use after constructing enough breeder reactors and u-233 obtained from th-232 whose fissile properties are somewhat similar to u-235 but it emits higher levels of radiation compared to pu-239 . it is used as fuel in the kalpakkam mini reactor ( kamini ) which is near my area , at chennai . after the fuel has been in the reactor for about 18 months , much of the uranium has already undergone fission ( we know about half-life , do not we ? ) and a considerable quantity of fission products would've been built up in the fuel . the reactor is then refueled by replacing about 1/3 of the fuel rods . this generally takes one or two months . the fission products are then put in a form for long term storage . a large reactor produces about 1.5 tons of fission products per year . the fission products are originally in a mixture with other substances , so reprocessing is required to get it down to a 1.5 tons . if the waste is incorporated into a glass , the total weight is 15 ton . if the density is 3 times that of water , which means the volume of the waste is 0.5 cubic meters , and the volume of the waste glass is about 5 cubic meters . but one thing , we humans ( especially the government ) will not leave it like that as these facts have already scared us before few years . as breeder reactors have more neutron economy than normal power reactors , we would increase the proposals for them and even we would jump toward integral fast reactors for now . due to these increasing technologies , we would somehow fuse some creepy things to produce uranium in the near future . for now , we will not consider it , ' cause it is available to us as common as tin or zinc . . !
no , the size of the atoms is not changing as the universe keeps on expanding . the bohr radius will always be the same fraction of a nanometer or the same fraction of a wavelength of some light ( of a certain spectral line ) . because the universe is expanding and the size is growing , it literally means that there is " more room " and one can squeeze an increasing number of atoms in the " same " volume , i.e. into the tetrahedron with vertices located at centers of 4 galaxies .
first general relativity is typically taught at a 4th year undergraduate level or sometimes even a graduate level , obviously this presumes a good undergraduate training in mathematics and physics . personally , i am more of the opinion that one should go and learn other physics before tackling general relativity . a solid background in classical mechanics with exposure to hamiltonians , lagrangians , and action principles at least . a course in electromagnetism ( at the level of griffiths ) i think is also a good thing to have . mathematically , i think the pre-reqs are a bit higher and since the question asks about mathematical detail , i will focus on that . i learnt relativity from a very differential geometry centric viewpoint ( i was taught by a mathematician ) and i found that my understanding of differential geometry was very helpful for understanding the physics . i have never been a fan of hartle 's book which i think is greatly lacking on the mathematical details but is good for physical intuition . however having worked in relativity for some time now i think it is better to teach from a more mathematical point of view so you can easily pick up the higher level concepts . additionally , i think you really need to understand what is going on mathematically to understand why we must construct things the way we do . i am going to have to disagree with nibot here and say that you will need more then just linear algebra and college calculus . calculus you must have at least seen up to vector calculus and be familiar with it . linear algebra is something you should have a very good understanding considering that we are dealing with vectors . a good course in more abstract algebra dealing with vector spaces , inner products/orthogonality , and that sort of thing is a must . to my knowledge this is normally taught in a second year linear algebra course and is typically kept out of first year courses . obviously a course in differential equations is required and probably a course in partial differential equations is required as well . i do not think a course in analysis is required , however since the question is more about the mathematical aspect , i would say having a course in analysis up to topological spaces is a huge plus . that way if you are curious about the more mathematical nature of manifolds , you could pick up a book like lee and be off to the races . if you want to study anything at a level higher , say wald , then a course in analysis including topological spaces is a must . you could get away with it but i think it is better to have at the end of the day . i would also say a good course in classical differential geometry ( 2 and 3 dimensional things ) is a good pre-req to build a geometrical idea of what is going on , albeit the methods used in those types of courses do not generalise . of course , there is also the whole bit about mathematical maturity . it is a funny thing that is impossible to quantify . i , despite having the right mathematical background , did not understand immediately the whole idea of introducing a tangent space on each point of a manifold and how $\{\partial_{i}\}$ form a basis for this vector space . it took me a bit longer to figure this out . you can always skip all this and get away with just the physicists classical index gymnastics ( tensors are things that transform this certain way ) however i think if you want to be a serious student of relativity you had learn the more mathematical point of view . edit : on the suggestion of jdm , a course in classical field theory is good as well . there is a nice little dover book appropriately titled classical field theory that gets to general relativity right at the end . however i never took a course and i do not think many universities offer it anyway unfortunately . also a good introduction if you want to go learn quantum field theory .
for a projectile the range is given by $$r=\frac{u^2\sin2\theta}{g}$$ where $u$ is the initial speed of projectile and $\theta$ is the angle of projection . substituting $r=7.5m , g=10 m/{s^2} , \theta = \frac \pi4$ , we get $u$ as $$u=\sqrt{gr}=\sqrt{7.5 * 10} = 5\sqrt3 m/s$$ so the change in momentum of the ball is $$\delta p = mu-0$$ $$\therefore \delta p = 20\sqrt3 kgm/s$$ change in momentum equals impulse applied by the thrower . thus during training he must also practise to apply the same impulse . change in momentum of the dumbbell will be $$\delta p = m_{dumbbell}*1m/s$$ $$\therefore 1*m=20\sqrt3$$ $$m=20\sqrt3kg$$ . note : the equations of projectile assume there is no air resistance , and that the projectile is being launched from ground level . ( a formula can be derived considering it is thrown from an initial height ) . also , during the practise , there will be a lot of other factors involved like decelerating the dumbbell as you do not intend to throw it . i have ignored all those effects .
when thinking about fundamental entities , it is quite easy to ask a question that , upon reflection , is contradictory . the questions of this kind take the form : what is [ some fundamental thing ] made of ? the contradiction here is that there can only be an answer if the fundamental thing is not fundamental ! the electromagnetic field is one such fundamental entity . it is not made of anything else , it just is what it is . in the context of qft , photons ( real and virtual ) are , loosely speaking , " excitations " of this entity . real photons are associated with the long range propagation of energy and momentum , i.e. , electromagnetic waves . virtual photons are associated with the electromagnetic force , i.e. , the lorentz force , as well as evanescent waves , and near field antenna radiation .
they are lorentz scalars . every scalar is lorentz invariant .
galaxies would appear stretched along the line of sight , not jumbled . let 's say a galaxy is ten million light years away and , as you proposed , is 100,000 light years across and we see it nearly edge on . the front of the galaxy will appear to us as it did ten million years ago and the back of the galaxy as it did 10,100,000 years ago . thus , if the galaxy is moving towards us it will appear bigger than it actually is due to the delay between light from the front of the galaxy and the rear of the galaxy reaching us . if the galaxy is moving away from us , it will appear smaller than it actually is ( compressed along the line of sight ) . as far as the effects of this time delay on viewing the rotation of the galaxy , the length of the cosmic year ( the length of time it takes for the sun to travel around the center of the milky way ) is 225 to 250 million years . so , as many other people have already pointed out , the rotation period of a galaxy is small compared to the time it takes for light to travel across the galaxy and there is no jumbling effect .
by conservation of momentum . when the thruster expels high velocity gas in one direction , that gas has momentum . since there is no external force acting on the system , the total momentum of the system ( thruster and expelled gas ) cannot change . thus , the thruster must acquire an opposing momentum such that the total momentum is unchanged . see the wiki article tsiolkovsky rocket equation
the analysis of the phase structure of gauge theories is a whole field . some major breakthroughs were the t'hooft anomaly matching conditions , the banks-zaks theories , seiberg duality , and seiberg witten theory . there is a lot of controversy here , because we do not have experiment or simulation data for most of the space , and there is much more unknown than known . the first thing to note is that when the higgs field vacuum expectation value is zero , the higgs does not touch the low energy physics . you can ignore the higgs at energy scales lower than it is mass , and if this mass is much greater than the proton mass , the result is indistinguishable qualitatively from the higgsless standard model . so i will describe the higgsless standard model . higgsless standard model even without the higgs , electroweak symmetry is broken anyway by qcd condensates . when the higgs vev is zero , the w and z do not become completely massless , although they become much much lighter . the reason is that qcd has a nontrivial vacuum , where quarks antiquark pairs form a q-qbar scalar fluid that breaks the chiral symmetry of the quark fields spontaneously . this phenomenon is robust to the number of light quark flavors , assuming that there are not so many that you deconfine qcd . qcd is still asymptotically free with 6 flavors , and it should be confining even with 6 flavors of quarks . so i have no compunctions about assuming the confinement mechanism still works with 6 flavors , and all 6 are now like the up and down quark . assuming the qualitative vacuum structure is analogous to qcd is plausible and consistent with the anomaly conditions , but if someone were to say " no , the vacuum structure of qcd with 6 light quarks is radically different from the vacuum structure of qcd " , i would not know that this is wrong with certainty , although it would be strange . anyway , assuming that qcd with 6 light quarks produces the same sorts of condensates as qcd with 3 light quarks ( actually 2 light quarks and a semi-light strange quark ) , the vacuum will be filled with a fluid which breaks su ( 6 ) xsu ( 6 ) chiral rotations of quark fields into the diagonal su ( 6 ) subgroup . the su ( 6 ) is exact in the strong interactions and mass terms , it is only broken by electroweak interactions . the electroweak interactions are entirely symmetric between the 3 families , so there is a completely exact su ( 3 ) unbroken to all orders . the su ( 6 ) xsu ( 6 ) breaking makes a collection of massless goldstone bosons , massless pions . the number of massless pions is the number of generators of su ( 6 ) , which is 35 . of these , 8 are exactly massless , while the rest get small masses from electroweak interactions ( but 3 of the remaining 27 go away into w 's and z 's by higgs mechanism , see below ) . the 8 massless scalars give long-range nuclear forces , which are an attractive inverse square force between nuclei , in addition to gravity . the hadrons are all nearly exactly symmetric under flavor su ( 6 ) isospin , and exactly symmetric under the su ( 3 ) subgroup . all the strongly interacting particles fall into representation of su ( 6 ) now , and the mass-breaking is by terms which are classified by the embedding of su ( 3 ) into su ( 6 ) defined by rotating pairs of coordinates together into each other . the pions and the nucleons are stable , the pion stability is ensured by being massless , the nucleon stability by approximate baryon number conservation . at least the lowest energy su ( 3 ) multiplet the condensate order-parameter involved in breaking the chiral su ( 6 ) symmetry of the quarks is $\sum_i \bar{q}_i q_i$ for $q_i$ an indexed list of the quark fields u , d , c , s , t , b . the order parameter is just like a mass term for the quarks , and i have already diagonalized this order parameter to find the mass states . the important thing about this condensate is that the su ( 2 ) gauge group acts only on the left-handed part of the quark fields , and the left-handed and right handed parts have different u ( 1 ) charge . so the condensate breaks the su ( 2 ) xu ( 1 ) gauge symmetry . the breaking preserves a certain unbroken u ( 1 ) subgroup , which you find by acting the su ( 2 ) and u ( 1 ) generators . the left handed quark field has charge 1/6 and makes a doublet , so for the combination $i_3+y/2$ where i is the su ( 2 ) generator and y is the u ( 1 ) generator , you get a transformation of 2/3 and 1/3 on the top and bottom component , which is exactly the same as $i_z + y/2$ on the singlets ( since they have no i ) . so this combination is not chiral , and preserves the vacuum . so the qcd vacuum preserves the ordinary electromagnetic subgroup , which means it makes a higgs , just like the real higgs , which breaks the su ( 2 ) xu ( 1 ) down to u ( 1 ) electromagnetic , with w and z bosons just like in the standard model . this is not really as much of a coincidence as it appears to be--- a large part of this is due to the fact that qcd condensates in our universe are not charged , so that they do not break electromagnetism , because u-bar and u have opposite electromagnetic charge transformation . this means that a u-bar u condensation leaves electromagnetism unbroken , and it is not a surprise that it does not leave any of the rest of su ( 2 ) and u ( 1 ) unbroken , because it is a chiral condensate , and these are chiral gauge transformations . the major difference is that there are 3 separate higgs-like condensates , one for each family , each with an identical vev , all completely symmetric with each other under the global exact su ( 3 ) family symmetry . the w 's and z 's get a mass from an arbitrary one of these 3 , leaving 2 dynamical higgs-like condensates . the main difference is that these scalar condensates do not necessarily have a simple distinguishable higgs-boson-like oscillation , unlike a fundamental scalar higgs . the result of this is that the w 's and z 's acquire qcd-scale masses , so around 100 mev for the w 's and z 's , as opposed to approximately 100 gev in the real world . the ratio of the w and z mass is exactly as in the standard model . behavior of analogs of ordinary objects the low energy spectrum of qcd is modified drastically , due to the large quark number . the 8 massless pions and 24 nearly massless pions ( three of the pions are eaten by the w 's and z 's to become part of the massive vectors ) include all the diquark degrees of freedom that we call the pions , kaons and certain heavy quark mesons . there will still be a single instanton heavy eta-prime from the instanton violated chiral u ( 1 ) part of u ( 6 ) xu ( 6 ) . there should be 35 rho particles splitting into 8 and 27 and 35 a particles splitting into 8 and 27 effectively gauging the flavor symmetry . the 6 quarks could be thought of as getting a mass from their strong interaction with the higgs-like condensates , of order some mevs , but since the mass of a quark is defined at short distances , from the propagator , it might be more correct to say the quarks are massless . some of the particles you see in the data-book , the sigma ( 660 ) , the f0 ( 980 ) should disappear ( as these are weird--- they might be the product of pion interactions making some extremely unstable bound states , something which would not work with massless pions ) the electron and neutrino will be massless except for nonrenormalizable quark-lepton direct coupling , which would couple the electron to the higgslike chiral quark condensate . this effect is dimension 6 , so the compton wavelength of the electron will be comparable to the current radius of the visible universe . the neutrino mass will be even more strongly suppressed , so it might as well be exactly massless . the massless electron will lead the electromagnetic coupling ( the unhiggsed u ( 1 ) left over below the qcd scale ) to logarithmically go to zero at large distances , from the log-running of qed screening . so electromagnetism , although it will be the same subgroup of su ( 2 ) and u ( 1 ) as in the higgsed standard model , will be much weaker at macroscopic distances than it is in our universe . nuclei should form as usual at short distances , although isospin is now a nearly exact su ( 6 ) symmetry broken only by electromagnetism , and not by quark mass , and with an exact su ( 3 ) subgroup . so all nuclei come in su ( 6 ) multiplets slightly split into su ( 3 ) multiplets . the strong force will be longer ranged , and without the log-falloff of the electromagnetic force , because the pions quickly run to a free-field theory , since the pion self-interactions are of a sigma-model type . the pion interactions will look similar to gravity in a newtonian approximation , but scalar mediated , so not obeying the equivalence principle , and disappearing in scattering at velocities comparable to the speed of light . the combination of a long-ranged attractive nuclear force and a log-running screened electromagnetic force might give you nuclear bound galaxies , held at fixed densities by the residual slowly screened electrostatic repulsion . these galaxies will be penetrated by a cloud of massless electrons and positrons constantly pair-producing from the vacuum .
the probability distribution of any observable that is a linear function of fields is gaussian i.e. $$a\exp ( - ( x-x_0 ) ^2/2\delta x^2 ) $$ in the ground state of a gaussian fixed point . also , the path integral of a gaussian fixed point ( free field theory ) is the integral of the exponential of a bilinear function of the fields which is also called gaussian .
excerpt from a report i once wrote on this topic : http://bytepawn.com/pdf/exciting.pdf gravitationally bound compact objects --- such as neutron stars --- cannot have pulsar periods less than 0.3msec . the pulsar period is of course radius and mass dependent , but is bound from below by a neutron star just on the verge of forming a black hole singularity . strange quark stars are hypothetical objects qualitatively dierent from other compact stars such as neutron stars . first , they are composed of strange quark matter , a hypothetical form of matter consisting of equal part u , d and s quarks . this phase must be of lower energy than hadronic matter for a strange star to be stable to quark fusion . second , unlike neutrons stars which are held together by gravity , strange stars are bound by the strong interaction . as strange stars are not bound by gravity , they are free to occupy the sub-millisecond pulsar region . edit : the limiting angular velocity for a gravitationally bound star goes as $ ( m/r^3 ) ^{1/2}$ . this comes from newtonian arguments ( centrifugal vs . gravitation force ) . the gr version turns out to have the same form , just a different multiplicative constant ( "accurate to better than 10%" according to glendenning ) . so , to maximise the angular velocity , you maximise m and minimize r . but , if you do that , you make the start more compact , and eventually it forms a black hole ! quantitatively , there is your limit . but note that the limit is not one number it is actually a function mass m . to get rid of r , use the fact that according to einstein 's equations of stellar structure ( oppenheimer-volkoff equations for hydrostatic equilibrium ) $m/r &lt ; 4/9$ . in the end the formula for the rotational period is : $p &gt ; 0.167 m / m_{sun} [ msec ] $ for more please check the book compact stars by n . k . glendenning , it is very good .
it would work in a vacuum , so i will simplify by assuming no air resistance . when the ball is hit , it spins in a certain direction , for example to hit the ball so that it would follow this path : requires the ball to be hit along the red arrow , but spinning in the direction of the blue arrow : the red arrow here is pointing the same way as the initial direction of the dashed black arrow on the first picture . as the ball travels forward it is " slipping " on the table . this means that the ball is moving over the table but it is rubbing past , not rolling smoothly , like a car skidding to a stop . the rotation of the ball will slow down as it travels but so will the ball 's speed , so initially it will look like this : $$ f_\mathrm{net} = -\mu_k n $$ where $ \mu_k $ is the coefficient of kinetic friction and is lower than $ \mu_s $ which is the coefficient of static friction . n is the normal force ( in this case equal in magnitude to $mg$ ) the force is in the negative direction , meaning the ball is slowing down ( i have taken positive to be the direction it initially travels in ( the direction of the red arrow ) ) so at some point the ball will stop moving completely . at this point , if the ball is still rotating , there will still be a resultant force on the ball : $$ f_\mathrm{net} = -\mu_k n $$ so the ball will start moving in the negative direction . some short time later the ball will stop slipping and continue to roll , with no resultant force , and therefore no net acceleration , but in a different direction to the direction it was hit . the three graphs below might be useful to understand the motion of the ball further . the first graph shows the position of the ball , the second graph shows the first derivative of position ( velocity ) and the third graph shows the second derivative ( acceleration ) . the time $t'$ denotes the point at which the ball stops slipping and starts rolling smoothly .
the horizantal beam on such scales is intentionally placed below the rotational axis . as long as the weights are in equilibrium the torque is equal on both sides . but as soon as the position changes e.g. tipping the left scale down , the torques differ because only the tangential part of the gravitational force vector in relation to the rotational axis contributes to the torque around it . when tipping down the left scale , torque on the left side gets smaller and torque on the right side gets bigger , therefore the right side moves down again until equilibrium is reached ( besides some swings to accommodate for the temporary impulse energy ) . this effect gets the more pronounced as the distance of the horizontal bar approaches the half length of the bar . this effect would not be if the horizontal bar went exactly through the axis .
hannesh , you are correct that the second law of thermodynamics only describes what is most likely to happen in macroscopic systems , rather than what has to happen . it is true that a system may spontaneously decrease its entropy over some time period , with a small but non-zero probability . however , the probability of this happening over and over again tends to zero over long times , so is completely impossible in the limit of very long times . this is quite different from maxwell 's demon . maxwell 's demon was a significant problem because it seemed that an intelligent being ( or more generally any computer ) capable of making very precise measurements could continuously decrease the entropy of , say , a box containing gas molecules . for anyone who does not know the problem , this entropy decrease could be produced via a partitioning wall with a small window that the demon can open or close with negligible work input . the demon allows only fast-moving molecules to pass one way , and slow-moving ones the other way . this effectively causes heat to flow from a cold body of gas on one side of the partition to a hot body of gas on the other side . since this demon could be a macroscopic system , you then have a closed thermodynamical system that can deterministically decrease its entropy to as little as possible , and maintain it there for as long as it likes . this is a clear violation of the second law , because the system does not ever tend to thermodynamic equilibrium . the resolution , as you may know , is that the demon has to temporarily store information about the gas particles ' positions and velocities in order to perform its fiendish work . if the demon is not infinite , then it must eventually delete this information to make room for more , so it can continue decreasing the entropy of the gas . deleting this information increases the entropy of the system by just enough to counteract the cooling action of the demon , by landauer 's principle . this was first shown by charles bennett , i believe . the point is that even though living beings may appear to temporarily decrease the entropy of the universe , the second law always catches up with you in the end .
the $\sqrt{\frac{\hbar c^2}{v}}$ is there because of the fact that mandl and shaw the quantise the field in a box and not in free space . so when you take the continuum limit you have that $$ \int d^3k \rightarrow \sum_k \sqrt{\frac{\hbar c^2}{v}} $$ to get the right dimensions . the $\frac{1}{\sqrt{2\omega_k}}$ comes from lorentz invariance , particularly from the fact that you want the integration measure ( again in the continuum limit/free space ) to be lorentz invariant . the argument of why this is lorentz invariant is a bit long but you can find a quick explanation in this post for instance . actually note that what makes the measure lorentz invariant is a factor $\frac{1}{2\omega_k}$ and the rest of the factor i.e. a multiplying $\sqrt{2\omega_k}$ goes with the creation operators so that our definite momentum states $$ |k\rangle = \sqrt{2\omega_k} a^\dagger ( k ) |0\rangle $$ have a lorentz invariant measure too , as @hunter explains .
the two equations are unrelated . first equation the first equation is a simple modification of the logistic differential equation , although it is somewhat disguised . the usual logistic equation is $$ {dx\over dt} = x ( 1-x ) $$ or in terms of the derivative of $\log ( x ) $ , it is the equation peter parker writes , with $\beta=1$ . the factor of k in conjunction with g sets the scale for $\phi$ , and it is irrelevant , the qualitative behavior for different $\beta$ at long times is not modified , since the equilibrium position is at $$\phi_\mathrm{eq}={k\over g^{1\over\beta}}$$ values above this go down and values below this go up . further there is a nonzero first derivative of $\phi^\beta$ for all reasonable idea of what $\beta$ is supposed to be , so this is describing a quantity $\phi$ which wants to go up exponentially , but is suppressed by competitive effects . the exponent $\beta$ describes the competitive effects . the logistic equation describes ( say ) bacteria ( or white blood cells ) replicating where two bacteria compete for the same limited resources . in this case , the competition is $\beta$-fold , the bacteria crowd each other out worse than quadraticaly ( or less worse , depending on whether $\beta&lt ; 1$ or $\beta> 1$ ) . this equation is consistent with a biological interpretation that $\phi$ is the concentration of some replicating crowding out agent , like a disease model . second equation the second equation is writing down $\phi$ in a way that depends on g but not on t . it has a k in it , but there is an unrelated expansion of $\phi$ in terms of the $e_n$ 's , so it is not the expression for the equilibrium value or the relaxation to this equilibrium value . further , you can massage the form by exponentiating , expanding the denominator in a power-series , and performing the sum on j , to produce a second infinite series , but only if you assume the hidden log part does not depend on j , but only on the variable " i " which has so far not been used . ${\phi\over k} = \exp ( g ( \sum_{k=1}^{\infty} \delta^k e^{g\delta^k} ) ) \sum_i log ( . . . ) $$ where $\delta= ( 1-e_k ) $ , and from the form , i will assume $0&lt ; \delta&lt ; 1$ , so that $0&lt ; e_k&lt ; 1$ . the $\log$ part makes no sense as a time development either , this is not the development of the logistic equation , or any reasonable asymptotic of this , ( although the symbol that is partly obscured is probably an $\alpha$ which can only appear multiplied by t on dimensional grounds , so you can assume that it is $\log ( \alpha t . . . ) $ , so one can only assume that the movie-makers chose a second equation to look impressive from an unrelated system .
fire is a reaction between molecules in gases . it may look as if a piece of wood is burning , but actually the burning happens in gases given off by the wood as it is heated . burning wood , paper etc is a complicated business , so let 's take a relatively simple system like burning the gas in your cooker ( assuming you use a gas and not electric oven ) . actually even reacting gas ( methane ) with oxygen is a multistep reaction , but basically a methane molecule and oxygen molecules collide , react , and the reaction products split apart with more speed than they started with . the extra speed of the molecules comes from the energy liberated in the reaction . so the products of combustion are gas molecules ( mainly h$_2$o and co$_2$ ) moving at high speeds , and in a gas the speed of the molecules is related to the temperature . high speeds mean high temperatures . in other words a flame is just a hot gas . but when we think of fire we think of the glowing flames . the glow comes from two sources . by far the most common source of the glow is when the molecules containing carbon do not fully burn but leave behind tiny particles of carbon i.e. soot . the particles are heated by the hot gas and they glow just as anything glows when it gets very hot . the yellow/red colour of flames is due to these glowing particles of soot . the other source of the light happens because the energy ( i.e. . the temperature ) of the combustion products is not all the same . at any temperature the energy of gas molecules is distributed according to the maxwell-boltzmann distribution and a small fraction of the gas molecules can have exceedingly high energies . high enough in fact to cause ionisation of other gas molecules , and light emission as they recombine . it is this ionisation that leads some people to talk about a flame as a plasma . however you need to bear in mind that only a tiny tiny fraction of the combustion products are ionised , so it is not a plasma in the sense that the sun contains plasma .
the amplitude of quadrupole radiation is proportional to the source 's value of $d^3 x/dt^3$ . this applies to both electromagnetic radiation and gravitational radiation ( for which quadrupole radiation is the lowest possible multipolarity ) .
it is because most materials have ( many ) natural resonances . i assume you are alright with the phase velocity being different in different media , that is i assume you are alright with something of the form $$ \frac{ \omega }{ k } = \frac{c}{n} = \frac{ c}{\sqrt{ \mu \epsilon }} \sim \frac{c}{\sqrt \epsilon} $$ where $k$ is the wavenumber of a wave , $\omega$ its frequency , $c$ the speed of light , $n$ is the index of refraction , $\mu$ the magnetic permeability of a substance and $\epsilon$ its dielectric constant . since most materials have a magnetic permeability close to that of the vacuum , your question now becomes , why do most materials have a frequency dependent dielectric constant ? this is because most materials have some internal intrinsic frequencies . we can understand this better with some simple models . the dielectric constant measures the ratio of the electric field in a material versus what the electric field in vacuum would be , so what is important is the induced dipole field $ e ' = e - 4\pi p$ . if we assume the material is made up of a constant density $n$ of simple dipoles , consisting of charge $q$ separated by distance $x$ , we have for out dielectric constant $$ n^2 = \epsilon = \frac{ e + 4\pi p}{e} = 1 + 4\pi n q x $$ to proceed , we need a simple model for the response of the dipoles in our material . for that , let 's just model them as simple driven damped harmonic oscillators $$ \ddot x + \frac{ \omega_0 }{q} \dot x + \omega_0^2 x = \frac{qe_0}{m} e^{i\omega t} $$ where $q$ is the quality factor , $\omega_0$ is the resonant frequency , $q$ is the charge , $e_0$ is the amplitude of our incoming wave , $m$ is the mass of our charge and $\omega$ is the driving frequency . for now , if we assume we are far from resonance , we can ignore the damping term and look for the amplitude of our oscillations at the driving frequency , so taking the ansatz $$ x = a e^{i \omega t} $$ we obtain $$ a = \frac{ q e_0 }{m } \frac{ 1 }{ \omega_0^2 - \omega^2 } $$ giving us $$ n^2 = \epsilon = 1 + \frac{ 4 \pi n q^2 }{m } \frac{ 1 }{ \omega_0^2 - \omega^2 } $$ so , we can see the frequency dependence right here . as long as our material is made up of things like dipoles ( which they are ) , that have some kind of natural frequency ( which they do ) , we will get a frequency dependent dielectric constant , which will mean we have a frequency dependent phase velocity , which means dispersion . now , typically , molecules will have a whole bunch of resonant frequencies given by their slew of excitation frequencies , and each of these will contribute , but on the whole , the effect of all of these different frequencies will amount to something like the above with $\omega_0$ now to be interpreted as an " average " resonant frequency . for something like glass , this " average " frequency will be in the ultraviolet , meaning for optical phenomenon , $\omega &lt ; \omega_0$ , so that $n &gt ; 1$ and in particular , the index of refraction for blue light should be higher than the index of refraction for red light . this is " normal " dispersion , where the index of refraction increases with increasing frequency . this is why the blue coming out of a prism is bent more than the red . if we had some kind of material with a particularly low average resonance , or we were looking at high frequency waves ( like x-rays ) , we would have " anomalous " dispersion , where the index of refraction decreases with increasing frequency .
the situation is a bit difficult if you want to take the strong interaction effects and so on into account , because it is a question what you permit to be " geometrical " . the fundamental forces involving charges are decribed by qunatum field theories , gauge theories of the the yangmills type and as far as this whole connection -business goes , it is actually a pretty geometric setting . so from a mathematical side , there are several similarities to general relativity . these theories are not concerned with a curved base manifold and the tangent space in the exact same sense as riemann-geometry , but there is also a notiond of curvature . i do not know if these will help you but here are two more links regarding a united mathematical perspective . for starter , lets keep this pre quantum . the obvious thing that comes to mind here is kaluza klein theory . however , as you will read in the very first paragraph , this is a theory in five dimensions . if you do not mind this , then i would push you further in a stringly direction , although this also differs pretty much from conventional general relativity . it is not unlikely that some people here might write an answer from that perspective . the incorporation of a force like electromagnetism in a classical four dimensional pseudo-riemann geometry type theory like general relativity turns out to be problematic . without going into computations involving features like spacetime metrics , christoffel symbols and the possibility to choose a local inertial frame , here is an argument involving the related equivalence principle . i quote the weak , einsteinian and strong version from wikipedia here and highlight some words : weak : " all test particles at the alike spacetime point in a given gravitational field will undergo the same acceleration , independent of their properties , including their rest mass . " einsteinian : " the outcome of any local non-gravitational experiment in a freely falling laboratory is independent of the velocity of the laboratory and its location in spacetime . " strong : " the gravitational motion of a small test body depends only on its initial position in spacetime and velocity , and not on its constitution and the outcome of any local experiment ( gravitational or not ) in a freely falling laboratory is independent of the velocity of the laboratory and its location in spacetime . " let 's consider a situation in general relativity where you have a big electrically negative charged object $x$ and the corresponding spacetime curvature is governed by the reissnernordstrm metric . you are sitting in your cosy laboratory , falling through space and you have optionally two to four particles $a , b , c$ and $d$ of charge $q_a=0 , q_b=-1 , q_c=+1$ and $q_d=-9001$ . lets say at the beginning of your experiments they are never moving relative to you . you and the uncharged particle will fall freely ( along a geodesic produces by the reissnernordstrm metric ) towards $x$ , while the other particles are never free falling , interact electromagnetically with $x$ and therefore move in different ways , depending on their charge . they also might interact with each other in various ways . there is only one particle mass , but netral , positive and negative electric charges so that different species are affected differently by the surroundings . given the fact that the physical observations so far agree with the results of general relativity : if you incorporate elecromagnetism in your four dimensional curved spacetime of your geometrical theory , how would you realize the equivalence principle ?
$$v=\frac{dx}{dt}=a+2bt-3ct^2$$ $$acc=\frac{dv}{dt}=2b-6ct=0$$ this gives $$t=b/3c$$ put this in $v$ to get the answer $$v=a+\frac{b^2}{3c}$$
in op 's eq . ( 1 ) it is assume that the matrix $m$ is antisymmetric $$\tag{a} m^t~=~-m . $$ [ a symmetric part in eq . ( 1 ) would not contribute to the integrand ( 1 ) . ] the term $\eta^tm^{-1}\eta$ in eq . ( 2 ) does in general not vanish $$ ( \psi - m^{-1}\eta ) ^t m ( \psi - m^{-1}\eta ) ~=~ ( \psi^t + \eta^t m^{-1} ) m ( \psi - m^{-1}\eta ) $$ $$~=~\psi^t m\psi+\eta^t\psi -\psi^t\eta-\eta^tm^{-1}\eta . \tag{b}$$
this is actually a question for mathematical forum - solving linear non-homogenous differential equations . you have to find solutions for homogenous ( $c=0$ ) and non-homogenous ( $c\ne0$ ) equation . non-homogenous solution is obviously $$x_\text{n} = \frac{c}{a} , $$ while homogenous solution in $$x_\text{h} = x_0 \exp [ -\frac{a}{b}t ] . $$ the whole solution of the differential equation is the sum of both solutions $x = x_\text{n} + x_\text{h}$ . the point is that for $t \rightarrow \infty$ $x_\text{h} \rightarrow 0$ , therefore robot is exponentially getting closer to non-homogenous solution $x_\text{n}$ . i would call $x_\text{n} = \frac{c}{a}$ steady state , while half time $\tau$ can be obtained by equation $$x_0 \exp [ -\frac{a}{b}\tau ] = \frac{x_0}{2} . $$
update : as zassounotsukushi correctly points out in the comments , my original answer was wrong . i said before that objects move out across our horizon , but they do not . sorry about that . i hope i have fixed things now . it is best to avoid phrases like " the universe is expanding faster than the speed of light . " in general relativity , notions like distances and speeds of faraway objects become hard to define precisely , with the result that sentences like that have no clear meaning . but if we leave that terminological point aside , your questions are perfectly well-posed and physically meaningful . it is true that we can only see out to a finite distance , due to the universe 's finite age , and that this is an explanation of olbers 's paradox , which is the name for the old puzzle of why the night sky is dark . as the universe expands , more objects pop into view , since that " horizon distance " is continually getting bigger , at least in principle . in fact , though , that is a very small effect and would not lead to the night sky becoming brighter in practice . first , we should note that we would not expect to see stars popping into view as our horizon expands . the reason is that objects right near the edge of the horizon are so far away that we would see them as they were long ago , around the time of the big bang . in practice , we can not see all the way back to $t=0$ , because the early universe was opaque , but still , we can see back in time to long before there were discrete objects like stars . when we look back near the horizon , we see a nearly-uniform plasma . but there is a much more important point . the further away we try to look , the more redshifted the light from a given object is . even if we could see an object near our horizon , the radiation from it would be shifted to extremely long wavelengths , which also means that it would carry extremely little energy . in practice , this just means that things near our horizon become unobservably faint . in a practical sense , our ability to see faraway objects actually decreases with time : although in principle our horizon grows , the redshift causes any given object to become unobservably faint much faster than the rate at which new stuff is brought in across the horizon . lawrence krauss has written a bunch about this stuff . the details are in this paper , and he has a scientific american article ( paywalled ) . dennis overbye wrote about this stuff in the ny times a while back too . ( if you read the pop stuff , tread carefully . some of it seems to be saying the incorrect thing i said before , namely that things that are currently inside our horizon move outside of it . the technical article is correct , but the nontechnical ones can be misleading . that is my excuse for messing things up in my original answer , but it is not a very good excuse , because i should've known better . )
let us define the inertial mass , gravitational mass and rest mass of a particle . inertial mass : - to every particle in nature we can associate a real number with it so that the value of the number gives the measure of inertia ( the amount of resistance of the particle to accelerate for a definite force applied on it ) of the particle . using newton 's laws of motion , $m_i = f/a$ gravitational mass - ( this is defined using newton 's law of universal gravitation i.e. the gravitational force between any two particle a definite distance apart is proportional the product of the gravitational masses of the two particles . ) to every particle in nature we can associate a real number with it so that the value of the number gives the measure of the response of the particle to the gravitational force . $f = \frac{gm_{g1}m_{g2}}{r^2}$ all experiments carried out till date have shown that $m_g = m_i$ this is the reason why the acceleration due to gravity is independent of the inertial or gravitational mass of the particle . $m_ia = \frac{gm_{g1}m_{g2}}{r^2}$ if $m_{g1} = m_i$ then $a = \frac{gm_{g2}}{r^2}$ that is acceleration due to gravity of the particle is independent of its inertial or gravitational mass . rest mass - this is simply called the mass and is defined as the inertial mass of a particle as measured by an observer , with respect to whom , the particle is at rest . there was an obsolete term called relativistic mass which is the inertial mass as measured by an observer , with respect to whom , the particle is at motion . the relation between the rest mass and the relativistic mass is given as $m = \frac{m_0}{\sqrt{1-v^2/c^2}}$ where $v$ is the speed of the particle and $c$ is the speed of light , $m$ is the relativistic mass and $m_0$ is the rest mass .
ok , so since your reasoning seems to be ok , the real question is : " how can one measure experimentally a laser spot width ? " . in your case , since you are using a gaussian beam , an equivalent is " how to measure the waist of my laser ? " if you are working in a wealthy lab , the simpliest way seems to buy and use a ccd camera . otherwise , an other " easy peasy lemon squeezy " ( and cheaper ) method is possible . for this , you will need : a linear translation stage ( for optic tables ) with a graduate spanner a photodiode a piece of black cardboard step 1 : align the photodiode with the laser . at this point , be sure to get the maximum power from your laser . step 2 : attach the piece of cardboard on the translation stage . this blocks the laser light so that it can not reach the photodiode . step 3 : place the all thing between your fiber opening and the photodiode . step 4 : move the translation stage with the spanner and measure the power collected by the photodiode in respect of the displacement ( using the graduation ) . since your beam is gaussian , what you expect is an error function because the power that you are measuring is simply the integral of a gaussian . . . $$\mathcal{p}\sim\int \exp \left [ - \left ( \frac{2 ( x-x_0 ) }{w}\right ) ^2 \right ] \mathrm{d}x\ ; \sim \mathrm{erf}\left ( \frac{x-x_0}{w/\sqrt 2}\right ) $$ where $w$ is the waist . step 5 : use your favorite software and do a fit . then you have $w$ . note that this method is not that precise . but it gives a good order of the waist , thought .
the nist atomic spectra database is a decent source for general-purpose lookup and identification of spectral transitions and levels . you will probably be more interested in their spectral line info .
pluto is now classified as a dwarf planet . the main difference between a planet and a dwarf planet has to do with the requirement that a planet clear out the material in and near its orbit . planets do this , dwarf planets do not . the reclassification was triggered by the discovery of many additional object ( the edgeworth-kuiper belt ) out beyond the orbit of neptune . some of the objects are nearly as big as ( and is a few cases , possibly bigger than ) pluto and in very similar orbits . thus it was realized that pluto was just the largest of a large number of objects in the outer solar system . this is simply science at work . at the local university , we have an astronomy textbook from the 1800 's that lists the 12 planets : mercury , venus , earth , mars , ceres , pallas , juno , vesta , jupiter , saturn , uranus , and neptune . however , as more objects were detected between mars and jupiter , it was realized this was a new class of object and the middle four were downgraded from planet status to asteroids . it is the same process at work today out in the outer solar system .
you need at least a rudimentary understanding of classical physics in order to even understand why quantum physics is useful/important . that being said , you can learn them both at the same time . if you are interested in physics , pick up whatever book interests you the most and start learning . when you run into concepts from other areas of physics that are necessary , you will have to take a break in order to learn them . do not get discouraged along the way ; learning physics takes a lot of effort and dedication , but the payoff is well worth it !
gravity can quite easily be repulsive due excessive negative pressure as mentioned by @stan liou . let us work this out . we will need two equations - einstein 's equation $$ r_{\mu\nu} = 8 \pi g \left ( t_{\mu\nu} - \frac{1}{2} t g_{\mu\nu} \right ) $$ where $t = g^{\mu\nu} t_{\mu\nu}$ . this equation describes how matter affects the curvature of space-time . in particular , we will use two forms of $t_{\mu\nu}$ . one for a perfect fluid of density $\rho$ and isotropic pressure $p$ $$ t_{\mu\nu}^{fluid} = \left ( \rho + p \right ) u_\mu u_\nu + p g_{\mu\nu} $$ where $u_\mu$ is the four velocity of the fluid itself . the other em tensor that will be used is the one that contributes to a non-zero vacuum energy , or a cosmological constant , defined by $$ t^{cm}_{\mu\nu} = - \rho_{vac} g_{\mu\nu} $$ raychaudhari 's equation $$ \frac{d \theta}{d \tau} = 2 \omega^2 - 2 \sigma^2 - \frac{1}{3} \theta^3 - r_{\mu\nu} u^\mu u^\nu $$ this equation is a purely geometric one . it is simply a statement about the behaviour of geodesics on a curved manifold . here $\theta$ is the " expansion parameter " and its value describes the affect of gravity on a ball . $\theta&gt ; 0$ implies that the ball grows and $\theta&lt ; 0$ implies that it shrinks . here $u^\mu$ denotes the 4-velocity of particles moving in the geodesic . we can now work out the effect of gravity for the simplest possible situation . we consider a set of particles that are initial at rest w.r. t each other in a small region ( so that we can approximate $g_{\mu\nu} = \eta_{\mu\nu}$ ) of spacetime ( take this time to be $t=0$ ) , i.e. $u^\mu = ( 1,0,0,0 ) $ . at $t=0$ , $\omega = \sigma = \theta = 0$ ( these quantities are like the stress , shear and expansion . since the particles in the ball are not moving at all , all of the above are zero ) . thus , at this time , the raychaudhuri 's equation takes the form $$ \frac{d\theta}{d\tau} = - r_{00} $$ we can also treat ( in an approximation ) the set of particles as a perfect fluid , in which case $$ t_{00} = \rho , ~~ t = g^{\mu\nu} t_{\mu\nu} = -\rho + 3 p $$ we can then use einstein 's equation to derive $r_{00}$ for the system . we then get $$ \frac{d\theta}{d\tau} = - r_{00} = - 4 \pi g \left ( \rho + 3 p \right ) $$ what we are really interested is the sign of $\frac{d \theta}{d \tau}$ . usually energy densities are positive . for any positive pressure , $\frac{d \theta}{d \tau} &lt ; 0$ and the ball shrinks ( doing exactly what we expect of gravity ) . in fact , even for negative pressure with magnitude as big as $|p|_{max} = \frac{\rho}{3}$ gravity always attracts . however , for any negative pressure with larger magnitude , we find that gravity indeed becomes a repulsive force ! great ! now , what does this mean about the relation to the vacuum energy ? note that $t_{\mu\nu}^{cm} = t_{\mu\nu}^{fluid} ( p \to - \rho_{vac} ) $ . we then find $$ \frac{d\theta}{d\tau} = - r_{00} = 8 \pi g \rho_{vac} $$ we then find that any positive energy density will cause gravity to act repulsively ! the usual example of a spacetime with positive energy density is de sitter space . note , however that the vacuum energy explanation of the expansion of the universe gives us a theoretical value $\rho_{vac} \sim 10^{112} erg/cm^3$ . the measured value is $10^{-8} erg/cm^3$ thereby giving us a discrepancy of $10^{120}$ . one might argue that our theoretical argument is flawed , but that would require some special symmetry that one does not see in spacetime . this is one of the biggest open problems in theoretical physics today and is called the " cosmological constant problem "
i could not understand how the static electricity could cause the shock from the car ? although the car looks like it should be insulated by the rubber tires they are actually quite good conductors so touching the car is a lot like touching any other grounded metal object . and how could be the source of this is shoes ? shoes , especially with synthetic soles that are being rubbed along the ground can build up a static charge . and how holding the key could prevent someone from this shock ? the key is metal and is held firmly in your hand , so the current flows smoothly from your hand to the key . the spark then jumps from the end of the key to the metal object . it is the very high field where the spark forms that hurts if it comes directly from your finger .
the energies of electrons in tightly bound states ( some of which might be referred to as " core states " , depending on circumstances ) are very nearly the same as the energies of the free molecule . these electrons do not interact very much with their neighbors , so their properties are nearly as if they are isolated . in such cases , with little or no overlap between states of adjacent molecules ( little contribution to bonding ) , the corresponding energy states on each molecule are the same , that is they are degenerate , and there is no interaction to break the degeneracy . in the extreme case of no interaction whatsoever , the energy eigenstates can be taken to be either the localized molecular states themselves , or extended bloch states , but in either case the result is the same : the states all have the same energy . in the bloch picture , that means a perfectly flat band with no dispersion . if there is some interaction , the bloch states are the energy eigenstates , not the localized states . the interaction will break the energy degeneracy ; the states no longer all have the same energy . to the extent that there is an interaction , there will be some dispersion in the band : the more interaction , the more dispersion . the greater the covalent interaction , the greater the dispersion . i can not say more than that without knowing more about your crystal , for example is 3 ev large or small ? do we expect the 2p oxygen levels to participate in bonding ? and so on .
if the electric and magnetic fields are steady , $\oint \mathbf{e}\cdot\mathrm{d}\mathbf{l} = 0$ is a law of nature . even if you change the material across the wire , the electric field should still respect $\oint \mathbf{e}\cdot\mathrm{d}\mathbf{l} = 0$ and therefore must be uniform . in fact , this is one of the boundary conditions at dielectric interfaces , if i recall correctly . however , by ohm 's law , $\mathbf{j} = \sigma \mathbf{e}$ , the current density is no-longer uniform . total current must be conserved , so if i suddenly create an interface with a different conductor of equal thickness , the electric field $\mathbf{e}$ will have to change , but it will do so uniformly , so that $\oint \mathbf{e}\cdot\mathrm{d}\mathbf{l} = 0$ remains .
relativistic effects like time dilation only depend on how close you are to the speed of light in vacuum . this may make it sound like relativistic effects have something to do with the light itself , but that is not the case . what it means is that there is a speed , c , to which no mass can be accelerate , no matter how much energy you dump into it . light happens to travel at c because photons have no mass . light slows down when it interacts with a medium , but that does not affect time dilation in a moving reference frame at all . in fact , matter particles can and do travel faster than light in media , causing a phenomenon called cherenkov raditaion
the effect you describe is called engine torque or propeller torque . all that happens is that the pilot opposes this torque with aileron in the opposite direction . because the wings have a much a larger lever arm than the propeller , usually only very small deflections of aileron are needed to counteract the propeller torque . most times the pilot will not even be conscious of it . there are exceptions , when one is flying a very powerfull plane at low speeds and one suddenly increases power , the engine torque can overcome the ability of ailerons to counteract it . this is called an involuntary torque roll . this was known to have killed many inexperienced pilots with the corsair fighter in wwii , for example , because it had such a powerful engine with high torque . another example was the japanese zero , which had such high torque that it could not roll very effectively in the counter torque direction , giving american pilots an advantage .
the gate g1 is there to help pull electrons off the cathode , increasing the overall current . this exploits the so called schottky effect to lower the effective surface barrier ( "work function" ) of the metal cathode . electrons are emitted by thermionic emission , which is exponentially sensitive to the barrier because the probability of an electron to have a given energy $e$ inside the metal due to the temperature $t$ is proportional to $\exp ( -e/k_b t ) $ . so anything you do to reduce the barrier to emission helps increase the overall current greatly . with an applied field to increase emission people speak of " field enhanced thermionic emission . "
this is because how much ever force the magnet is applying on the iron ( not any metal ) is opposed by an equal force applied by the iron on the magnet ! and since the magnet is a part of the vehicle , this force will cancel out the force on iron by the magnet . magnetic force occurs both ways ! therefore , there is no net force on the whole vehicle , which is why it will not move . what will happen is that the magnet and iron will be pulled to each other , and stick . if the rod connecting the vehicle to the magnet is rigid , and can withstand the magnetic force , nothing will happen . you should remember internal forces never can produce a change in momentum of the system as a whole . another example is if you try to push a car while sitting in it . no doubt , the car will recieve a force pushing it in a direction , but that force will be canceled out by the force you are unknowingly applying on the car in the exact backward direction . this force is applied by your feet while stopping themselves from slipping :
charge is only the most familiar of the properties that are inverted between a particle and its antiparticle , but it is not the only one . so you should not consider " same mass and opposite electric charge " to be a definition of what an antiparticle is ; it is merely a plain-english explanation . a list of properties in which particles and antiparticles differ can be found on the wikipedia page for flavor . in particular , they include each of the six quark flavor quantum numbers ( upness , downness , strangeness , charmness , bottomness , topness ) isospin , which is like a combination of upness and downness baryon number , which is like a combination of all six quark flavor numbers each of the three lepton flavor quantum numbers ( electron number , muon number , tau number ) lepton number , which is like a combination of all three lepton flavors weak isospin electric charge hypercharge , which is like a combination of weak isospin and electric charge parity chirality . . . sort of ( let 's just say that one 's complicated ) as i have mentioned , some of these are just combinations of others , so you could not make a complete list of all the quantum numbers in which particles and their antiparticles differ , but you could list all the " fundamental " ones ( the basis of the vector space of quantum number operators ) . i am not sure i got them all here , but i can not think of any others off the top of my head .
the main important idea of feynman wheeler theory is to use propagators which are non-causal , that can go forward and backward in time . this makes no sense in the hamiltonian framework , since the backward in time business requires a formalism that is not rigidly stepping from timestep to timestep . once you give up on a hamiltonian , you can also ask that the formalism be manifestly relativistically invariant . this led feynman to the lagrangian formalism , and the path integral . the only reason the feynman wheeler idea does not work is simply because of the arbitrary idea that an electron does not act on itself , and this is silly . why can not an electron emit and later absorb the same photon ? forbidding this is ridiculous , and creates a nonsense theory . this is why feynman says he abandons the theory . but this was the motivating idea--- to get rid of the classical infinity by forbidding self-interaction . but the result was much deeper than the motivating idea . feynman never abandons the non-causal propagator , this is essential to the invariant particle picture that he creates later . but later , he makes a similar non-causal propagator for electrons , and figures out how to couple the quantum electrons to the photon without using local fields explicitly , beyond getting the classical limit right . this is a major tour-de-force , since he is essentially deriving qed from the requirement of relativistic invariance , unitarity , the spin of the photon and electron , plus gauge-invariance/minimal coupling ( what we would call today the requirement of renormalizability ) . these arguments have been streamlined and extended since by weiberg , you derive a quantum field theory from unitarity , relativistic invariance , plus a postulate on a small number of fundamental particles with a given spin&lt ; 1 . in feynman 's full modern formalism , the propagators still go forward and backward in time just like the photon in wheeler-feynman , the antiparticle goes backward , and the particle forward ( the photon is its own antiparticle ) . the original motivation for these discoveries is glossed over by feynman a little , they come from wheeler 's focus on the s-matrix as the correct physical observable . wheeler discovered the s-matrix in 1938 , and always emphasized s-matrix centered computations . feynman never was so gung-ho on s-matrix , and became an advocate of schwinger style local fields , once he understood that the particle and field picture are complementary . he felt that the focus on s-matrix made him work much harder than he had to , he could have gotten the same results much easier ( as schwinger and dyson did ) using the extra physics of local fields . so the only part of wheeler-feynman that feynman abandoned is the idea that particles do not interact with themselves . other than that , the feynman formalism for qed is pretty much mathematically identical to the wheeler-feynman formalism for classical electrodynamics , except greatly expanded and correctly quantum . if feynman had not started with backward in time propagation , it is not clear the rest would have been so easy to formulate . the mathematical mucking around with non-causal propagators did produce the requisite breakthrough . it must be noted that schwinger also had the same non-causal propagators , which he explicitly parametrized by the particle proper time . he arrived at it by a different path , from local fields . however they were both scooped by stueckelberg , who was the true father of the modern methods , and who was neglected for no good reason . stueckelberg was also working with local fields . it was only feynman , following wheeler , who derived this essentially from a pure s-matrix picture , and the equivalence of the result to local fields made him and many others sure that s-matrix and local fields are simply two complementary ways to describe relativistic quantum physics . this is not true , as string theory shows . there are pure s-matrix theories that are not equivalent to local quantum fields . feynman was skeptical of strings , because they were s-matrix , and he did not like s-matrix , having been burned by it in this way .
a material object may hit the ( spacelike ) singularity ( like one inside the schwarzschild black hole ) at any speed smaller than $c$ , if measured from the frame in which the singularity itself is described by $t={\rm const}$ . in other words , the angle in the penrose causal diagram between the incoming trajectory of the doomed massive object and the horizontal line of the singularity may be an arbitrary angle between $-\pi/4$ and $+\pi/4$ . there is no other restriction on speeds . the op 's main statement on the page behind the words " like here " is invalid . a massive object crossing the event horizon moves by a speed $v\lt c$ ( locally ) with respect to any local inertial frame . it cannot be otherwise . this statement in no way contradicts the fact that the escape speed from the black hole is really or formally $c$ ( or larger ) . the reason why there is no contradiction is that there is absolutely no symmetry between the inward motion and the outward motion . the former is possible and maximally supported ; the latter is classically prohibited . the time goes up , the bottom is the past and the top is the future . the grey region is the black hole interior . you see that it is very easy to get in but , because the restriction $v\lt c$ , one can not get out . the time-reverted ( upside-down rotated ) penrose diagram would describe a " white hole " . at the level of the microstates , the states describing a white hole would be the very same states as the black hole states , so one should not double count them . however , only the evolution described by the black hole spacetime diagram is allowed ( e . g . a black hole is swallowing matter , not spitting it out ) because the " white hole spacetime diagrams " would violate the second law of thermodynamics ( the entropy would macroscopically decrease which is not allowed ) .
the confusion possibly comes from the casual notation , for example the last term in equation ( 3 ) in its full form ought to be $j_i\epsilon^a t^a_{ij}\phi_j$ , which is just a number ; while in the original notation $j \epsilon^a t^a \phi$ it might lead you to think it is a matrix because of the presence of $t^a$ . one quick way to check the mistake is to notice that this term is added to the action $s$ in equation ( 2 ) , and action must be scalar , hence the term also has to be scalar .
it is always virtual particles that " mediate " the interactions , not real ones . you imply real gravitons , not virtual ones ; this is a hole in your reasoning .
from sciencemuseum : apollo 10 holds the record as the fastest manned vehicle , reaching speeds of almost 40,000 km per hour ( 11.08 km/s or 24,791 mph to be exact ) during its return to earth on 26th may 1969 . using the formula ( as above ) . after traveling for 40 years , you would be a little over 0.86 seconds younger . added : i did some calculating and determined that if your radial velocity directly away from the sun is 11.08 km/s , you would still be inside the solar system after 20 years ( just beyond neptune ) .
the credit should actually go to slaviks for the wise suggestion of keywords to look for . tunable porosity gives the following article : doi : 10.1002/ange . 201201686 prashant tyagi et al . dynamic interactive membranes with pressure-driven tunable porosity and self-healing ability . angewandte chemie , 2012 . aba triblock copolymer ( pictured above ) poly ( styrene-co-acrylonitrile ) -b-poly ( ethylene oxide ) -b-poly ( styrene-co-acrylonitrile ) ( psan-b-peo-b-psan ) has been used to generate flower-like micelles . porosity tuning is done by changing water pressure which re-arranges the micelles .
cycles of anomaly are lappings of a planet by the earth . if there are $ca$ of them and the earth orbited the sun $n$ times during the time , then one may determine that the planet has orbited $n\pm ca$ times . revolutions ( not cycles ) in longitude are periods between two adjacent nodes , i.e. moments when a planet 's trajectory crosses the ecliptic . so it is roughly a revolution of a planet around the sun but it should be measured by a particular method involving the nodes . those concepts are perfectly suited for heliocentric ( the modern ) system ; they are just not being used too often these days .
the various theoretical options are very different in nature , and the answer to this question almost defines the option . 1 ) relativity is wrong , there is objective absolute time , lorentz symmetry is emergent ( as in electrodynamics before einstein ) , and going faster than light does not create any time-loop paradoxes . 2 ) relativity is valid , but neutrinos , not photons , travel at the actual relativistic speed-limit ; photons have a small mass , or photons are confined to a braneworld while neutrinos can take shortcuts through hyperspace . 3 ) relativity is valid , neutrinos are tachyons ( or the opera result manifests some other sort of spacelike correlation ) , and something ( deutsch 's multiverse , hawking 's ctc instability , weakness and/or uncontrollability of the effect , qm itself actually comes from ctcs . . . ) prevents it from creating a time-loop paradox . it should be extremely difficult to turn any of these 3 options into a fully working theory capable of reproducing the standard model . the odds are still heavily against opera having actually observed ftl neutrinos .
in hindsight , here is a short proof . the metric $g_{\mu\nu}$ is the flat constant metric $\eta_{\mu\nu}$ in both coordinate systems . therefore , the corresponding ( uniquely defined ) levi-civita christoffel symbols $$ \gamma^{\lambda}_{\mu\nu}~=~0$$ are zero in both coordinate systems . it is well-known that the christoffel symbol does not transform as a tensor under a local coordinate transformation $x^{\mu} \to y^{\rho}=y^{\rho} ( x ) $ , but rather with an inhomogeneous term , which is built from the second derivative of the coordinate transformation , $$\frac{\partial y^{\tau}}{\partial x^{\lambda}} \gamma^{ ( x ) \lambda}_{\mu\nu} ~=~\frac{\partial y^{\rho}}{\partial x^{\mu}}\ , \frac{\partial y^{\sigma}}{\partial x^{\nu}}\ , \gamma^{ ( y ) \tau}_{\rho\sigma}+ \frac{\partial^2 y^{\tau}}{\partial x^{\mu} \partial x^{\nu}} . $$ hence all the second derivatives are zero , $$ \frac{\partial^2 y^{\tau}}{\partial x^{\mu} \partial x^{\nu}}~=~0 , $$ i.e. the transformation $x^{\mu} \to y^{\rho}=y^{\rho} ( x ) $ is affine .
you need to read this paper by jaynes . i can not explain it as well as him , but i will try to summarise the main points below . the first thing is to realise that the entropy is observer-dependent : it depends on what information you have access to about the system . a finite temperature means that you do not have access to all the information about the state of the system ; in particular , you cannot keep track of the ( infinite ) degrees of freedom of the bath . however , suppose that some demon could keep track of all the degrees of freedom of the system and bath : he/she sees zero entropy . for the demon , it looks a bit like the total system is at zero temperature ( although really it is better to say that temperature is ill-defined for the demon ) . given that you are ignorant ( sorry , but at least i am not calling you a demon ) , you need to find a consistent prescription for assigning probabilities to the different microstates . the prescription must be ' honest ' about what you do or do not know . the entropy is in some sense a unique measure of ignorance , as proved by shannon . therefore you should ' maximise your ignorance ' , subject to the constraint that you do know certain macroscopic observables , e.g. average energy or average particle number if the system is open , etc . maximising the entropy of the system is the most logical way to assign probabilities to the microstates of the system , given access only to a limited subset of observables . the same ' maxent ' principle is quite general and applies to all statistical analysis , not only physics . the lagrange multiplier $\beta$ is identified with inverse temperature by comparing the outcome of this abstract procedure to the experimental facts of phenomenological thermodynamics . if you are interested in the actual dynamics of equilibration , there has been a lot of literature on this recently , especially in mesoscopic systems . particular focus is laid on the integrability of the system : non-integrable ( chaotic ) systems do thermalise , whereas there is a fair bit of evidence that integrable systems do not thermalise properly . intuitively , this is because integrable systems have a maximal set of locally conserved quantities so that , even when in contact with a heat bath , the memory of the initial conditions is never quite lost . see , for example : dynamics of thermalisation in small hubbard-model systems and thermalization and ergodicity in many-body open quantum systems , if you search ' thermalization ' ( sic ) on arxiv then you will find many more .
about 73% of the energy of the universe is stored in dark energy - the cosmological constant , most likely - which has a negative pressure numerically equal to the energy density . because it is a cosmological " constant " , this portion of the mass of the universe cannot be really compressed . so it is a problematic idea to include the dark energy into the " mass that you want to compress " . only dark matter and visible matter - whose pressure is nearly zero - can really be " compressed " . so that would be about $1\times 10^{54}$ for the visible universe . $2gm/c^2$ for this mass $m$ produces about 150 billion light years which is about 3 times larger than the radius of the observable universe . we obtained the result that even if we only count the " particulate " component of the mass of the universe , we find out that the universe is actually smaller than the black hole . it is no contradiction because the universe is not a static system . it would be impossible for the universe to be kept this small . however , the galaxies are receding from each other which prevents the formation of a nearby event horizon . instead , the closest horizon in the reality is the " cosmic horizon " - how far we can actually see because of the finite speed of light and finite age of the universe . it depends on the observer but effectively makes everything beyond this horizon unphysical . our universe , dominated by the dark energy , is already rather close to an empty de sitter space which is , from many viewpoints , analogous to a black hole except that the interior of the visible part of the de sitter space is analogous to the exterior of a normal black hole , and the analogy of the interior of a black hole is everything that is behind the cosmic horizon - where we do not see . it is misleading to create the analogy with the static black holes directly because our universe is not static in the normal cosmological coordinates .
you have a few longer answers which were already updated , but here is a concise statement of the situation in mid-2014: an independent measurement by the icarus collaboration , also using neutrinos traveling from cern to gran sasso but using independent detector and timing hardware , found detection times " compatible with the simultaneous arrival of all events with equal speed , the one of light . " in an edited press release ( and probably in the peer-reviewed literature as well ) , all four of the neutrino experiments at gran sasso report results consistent with relativity . the mumblings that begin a few months after the initial report , that a loose cable caused a timing chain error , have been accepted by the experimenters . frdric grosshans links to a nice discussion by matt strassler which includes this image : you can clearly see that the timing offset was introduced in mid-2008 and not corrected until the end of 2011 . it is important to remember the scale of the problem here . in vacuum , the speed of light is one foot per nanosecond . in copper/poly coaxial cable it is slower , about six inches per nanosecond , and in optical fiber it is comparable . a bad cable connector can take a beautiful digital logic signal and reflect part of it back to the emitter , in a time-dependent way , turning the received signal into an analog mess with a complicated shape . and a cable can go bad if somebody hits it the wrong way with their butt while they are working in the electronics room . ( i actually had something similar happen to me on an experiment : i had an analog signal splitter " upstairs " that sent a signal echo back to my detectors " downstairs " , and a runty little echoed pulse came back upstairs after about a microsecond and got processed like another event . i wound up spending several thousand dollars on signal terminators to swallow the echo downstairs . it was an unusual configuration and needed unusual termination hardware and i must have answered the question " but could not you just " a hundred times . ) gran sasso is an underground facility for low-background experiments  the detectors can not see gps satellites directly , because there is a mountain in the way , and their access to the surface is via a tunnel whose main purpose is to carry traffic for a major italian motorway . i am quite impressed that they had ~100ns timing resolution between the two laboratories ; the " discovery " came about because they were trying to do ten times better than that . as an experimentalist i do not begrudge the opera guys their error at all . i am sure they spent an entire year shitting pineapples because they could not identify the problem . when they finally did release their result , they had the courage to report it at face value . the community was properly incredulous and the wide interest prompted a large number of other checks they could make . independent measurements were performed . an explanation was found . science at its best .
you are right , the planetary model of the atom does not make sense when one considers the electromagnetic forces involved . the electron in an orbit is accelerating continuously and would thus radiate away its energy and fall into the nucleus . one of the reasons for " inventing " quantum mechanics was exactly this conundrum . the bohr model was proposed to solve this , by stipulating that the orbits were closed and quantized and no energy could be lost while the electron was in orbit , thus creating the stability of the atom necessary to form solids and liquids . it also explained the lines observed in the spectra from excited atoms as transitions between orbits . if you study further into physics you will learn about quantum mechanics and the axioms and postulates that form the equations whose solutions give exact numbers for what was a first guess at a model of the atom . quantum mechanics is accepted as the underlying level of all physical forces at the microscopic level , and sometimes it can be seen macroscopically , as with superconductivity , for example . macroscopic forces are limiting cases of the real forces which reign microsopically .
you need to distinguish between retardation and radiation . retardation usual refers to the effect of limited propogation seeds on interactions . that is we replace $$a_g = g\frac{m}{r^2}$$ with $$a_{g , \mathrm{ret}} = g\frac{m}{r ( t - \frac{r}{c} ) ^2} . $$ this only matters if the ration of velocities in the system to propogation speeds are significant , or the effect is constantly in one direction in other cases ( as in retarded newtonian gravity in the solar system ) it simply results in a constant correction to some effective parameter of the system ( reduced mass in our example ) . in your case , you have a static field which means that $\mathcal{e} ( t ) = \mathcal{e}$ and the problem reduced to original case . there is no effect due to retardation . i am taking " radiation " to mean the loss of energy by accelerating charges known as bremsstrahlung . everything you really want to know is in the wikipedia link . you propose a linear acceleration case , so we can use $$ p_{a\parallel v} = \frac{q^2 a^2 e^6}{6 \pi^2 \epsilon_0 m^6 c^{15}}$$ for the power lost to radiation . here i have used $e = \gamma m c^2$ for the total energy of the electron ( including it is mass ) . as i noted above , the energies involved are such that the electron remains largely non-relativistic , so we will stick to the physics 101 for of the kinematic equations . the average velocity is about 1% of the speed of light , so the time to cover the distance is about 600 ns , and the acceleration is $a = \frac{0.02 * 3\times10^8\text{ m/s}}{6\times10^{-7}\text{ s}} = 10^{13}\text{ m/s}^2$ . because the electron 's energy only changes by about 2% over the whole range i am not going to bother with a proper integration of the power , and instead just multiply ( i make an error on order of 6% by doing so ) . the energy lost to radiation is about $$e_l = p_{a \parallel v} t = \frac{q^2 a^2 \left ( 1.01*mc^2\right ) ^6}{6 \pi^2 \epsilon_0 m^6 c^{15}} t = \left ( 1.01\right ) ^6 \frac{q^2 a^2}{6 \pi^2 \epsilon_0 c^3} t$$ and ( after furiously checking the units ) i just start plugging in $q = 1.6 \times 10^{-16}\text{ c}$ $\epsilon_0 = 8.8 \times 10^{-12} \text{ m}^{-3} \text{ kg}^{-1} \text{ s}^2 \text{ c}^2$ $c = 3\times10^8\text{ m/s}$ $t = 6\times10^{-6}\text{ s}$ $$e_l \approx 1.2\times 10^{-28}\text{ j} = 7 \times 10^{-10}\text{ ev}$$ . so the losses are trivial . this is not surprising since a typical crt runs at 10+ kev over a few centimeters and does not spew lots of x-rays around the room .
you seem to have trouble to understand the basic approach . actually there is a systematic way to solve the schrdinger equation for picewise constant potentials . maybe this will give you some basic idea how to solve your problem : let be the potential given by $$v ( z ) = \begin{cases} \infty and z &lt ; z_1 \\ v_1 and z_1 &lt ; = z &lt ; z_2 \\ v_2 and z_2 &lt ; = z &lt ; z_3 \\ . . . \end{cases}$$ for the above potential the wavefunction for energy eigenvalue $e_n$ is given by $$\psi_n ( z ) = \begin{cases} 0 and z &lt ; z_1 \\ a_1\exp ( -i k_1 z ) + b_1\exp ( +i k_1 z ) and z_1 &lt ; = z &lt ; z_2 \\ a_2\exp ( -i k_2 z ) + b_2\exp ( +i k_2 z ) and z_2 &lt ; = z &lt ; z_3 \\ . . . \end{cases}$$ with $k_i = 2\pi/h \sqrt{2 m e ( e_n-v_i ) }$ and some ( yet to be determined ) constants $a_i$ and $b_i$ . this is easily verified by plugging in . ( in fact each " segment " is the solution to the schrdinger equation with constant potential ) . note that the $k_i$ can be real or imaginary , in which case the wavefunction in the respective segment is either sinusoidal or exponential . as required by physics the wavefunction must be continuous and continuously differentiable everywhere . hence the constants $a_i$ and $b_i$ must be chosen so that this is fulfilled at each point where this possibly is violated ( i.e. . the points $z_i$ ) . the above results in a linear equation system for the $a_i$ and $b_i$ . this equation system now only contains the energy $e_n$ as remaining unknown . if you do it correctly the equation system contains as many unknowns as equations . now you compute the determinant of the equation system and set it to zero to find the $e_n$ values for which it is solvable . this is the transcendetal equation for the eigenvalues . this equation has in your case infinitely many discrete solutions $e_n$ ( each solution denoted by the running index $n$ ) . for each $e_n$ there are sets of $a_i$ and $b_i$ ( which solve the equation system ) which give you the wavefunction . in case there is more than one set of linearly independent $a_i$ and $b_i$ , you have more than one wavefunction to the same eigenvalue $e_n$ . in that case the state is degenerate . ( you have degenerate states in your problem ! ) . regarding symmetry : the wavefunctions do not need to have the same symmetry as the potential . of course if you have a solution wavefunction , then the mirrored wavefunction must be a solution as well ( if the potential is symmetric as in your case ) . it needs to belong to the same energy eigenvalue . regarding the single bound state : once you have calculated the $e_n$ you will see that there are conditions where $e_1 &lt ; v_0$ and $e_2 &gt ; v_0$ ( $e_2$ the second largest eigenvalue ) . this depends on the geometry , i.e. width of your barrier and well . generally speaking the energy states have higher spacing , if the well is smaller . so probably the single bound state condition will display itself as range specification for $a$ and $b$ .
a physical electron is pointlike , which is significantly different from being a point . the reason that it is not a point are the radiative corrections acquired in the renormalization procedure . ( a ''bare electron'' is a point particle , but it has no reasonable physical properties as everything of interest turns out to be infinite . renormalization is essential for getting meaningful answers . ) according to qed , the physical ( renormalized ) electron has nontrivial form factors ( generating the anomalous magnetic moment and the lamb shift ) , which proves that it is not a point particle . the electromagnetic form factors have a classical meaning , as they give the response of the electron to a classical external electromagnetic field . for more details , see section ''are electrons pointlike/structureless ? '' in chapter b2: photons and electrons of my theoretical physics faq . on the other hand , the shape of an electron is determined by its charge density , which is a function of its state , hence not an invariant property of the electron . see the section ''the shape of photons and electrons'' in the same chapter .
$l_r = \frac{d^2\phi}{d\omega \space da\cos{\theta}}$ i can separate out the derivatives in the above equation to get : $l_r = \frac{d}{d\omega} ( \frac{d\phi}{ \space da\cos{\theta}} ) $ [ note , alternatively i could have done $l_r = \frac{d}{da} ( \frac{d\phi}{ \space d\omega\cos{\theta}} ) $ , but i believe the former will allow for a more intuitive explanation of the formula . ] let us examine the portion in brackets , that is , $\frac{d\phi}{ \space da\cos{\theta}}$ . firstly , note that $\theta$ represents the angle between the surface normal and the specified direction from which $l_r$ is measured , so we can think of $a cos\theta$ as being the area perpendicular to the direction l is measured . this term is thus calculating how much power there is in the light , per the component of unit area that is perpendicular to the light , at a particular point . the reason for the derivative , rather than simply a " divide by " is because we must consider each " infintisimally small " surface element separately , because each surface element could have a different orientation , and thus a different value of $\cos{\theta}$ , since $\theta$ depends upon the orientation of the surface element . a simple " divide by " over a large area would thus not work , as there would be no value of $\theta$ applicable to the entire area . let us now consider the $\frac{d}{d\omega}$ component . this operator acts on the quantity " power per perpendicular unit area " , and thus evaluates to " power per perpendicular unit area , per unit frequency " . the power per unit area could be due to a contribution from a variety of frequencies , and this quantity $l_r$ can thus tell us how the power contributions change as frequency changes . ultimately the quantity could be used to evaluate the power contributions in a certain frequency range , by computing the integral $\int_{\omega_0}^{\omega_0 + \delta} l_r d\omega$ .
this web page has a nice discussion on it : http://archive.ncsa.illinois.edu/cyberia/numrel/einsteintest.html basically the orbit is eccentricity would precess around the sun . classical stellar mechanics ( or newtonian gravity ) could not account for all of that . it basically had to do with ( and forgive my crude wording ) the sun dragging the fabric of space-time around with it . or as the web page says : mercury 's changing orbit in a second test , the theory explained slight alterations in mercury 's orbit around the sun . daisy petal effect of precession since almost two centuries earlier astronomers had been aware of a small flaw in mercury 's orbit around the sun , as predicted by newton 's laws . as the closest planet to the sun , mercury orbits a region in the solar system where spacetime is disturbed by t he sun 's mass . mercury 's elliptical path around the sun shifts slightly with each orbit such that its closest point to the sun ( or " perihelion" ) shifts forward with each pass . newton 's theory had predicted an advance only half as large as the one actually observed . einstein 's predictions exactly matched the observation . for more detail that goes beyond a simple layman answer , you can check this page out and even download an app that let 's you play with the phenomenon : http://www.fourmilab.ch/gravitation/orbits/ and of course , the ever handy wikipedia has this covered as well : http://en.wikipedia.org/wiki/tests_of_general_relativity#perihelion_precession_of_mercury although , truth be told , i think i said it better ( i.e. . more elegantly ) than the wiki page does . but then i may be biased .
short answer is yes . but if you want to nit pick , i could argue that when a star collapses to form a bh , it first forms a horizon before the singularity forms ( cannot form a " naked singularity" ) . and since time inside the horizon is essentially frozen with respect to that of an observer outside , the singularity never forms . yet from the point of view of the collapsing star , the singularity forms in about a millisecond after the horizon .
it is just easier , i.e. less expensive , to build and maintain them that way . there exist alternative designs that are more efficient but also more difficult ( = more expensive ) to build , put up and maintain . you can check those out via this link .
that is a myth . he actually discovered gravity while trying to explain the motion of planets . he extrapolated the theory used to explain planetary motion to the " now " well known fact that " every body attracts every other body . . . " this enabled him to explain the falling of apples and lot more . many scientists before him were also trying to explain planetary motion , but as far as my knowledge of scientific history goes ( and it was also said so in cosmos ) that he first made an accurate theory .
ashcroft and mermin are occasionally not too careful about notation , and so the resolution is , as marek mentioned in a comment , related to the fact that $$ u_k = \sum_g c_{k-g}e^{igr} $$ admits several solutions -- every set of coefficients $\{c\}$ corresponds to some function $u_k$ , but we are only interested in the $n$ solutions that solve the schroedinger equation . every delocalized band arises because of the hybridization of quantum states between unit cells . if there are $n$ states that the electron can occupy , their will be $n$ ( possibly degenerate , depending on the symmetry of the $g$-space point ) bands . i say this to clarify that there is no sense in which the band index corresponds to the set reciprocal lattice vectors . however , to answer your last question , you are absolutely right that reduction to the first brillouin zone does give you multiple values of $e$ for a given $k \in ibz$ . if i remember correctly a and m introduce band theory by starting with the nearly free electron model and should show explicitly that this is the case . in the tight binding picture ( sometimes called linear combination of atomic orbitals ) the energies for $k$ outside the ibz fall back onto their values in the ibz . continued : specifically , i wanted to point out that $e_n ( k ) =e ( k+g_n ) $ simply does not make any sense . where did you read that ? however the point is that the eigenvalues that make up a particular band are periodic in $g$ , so it is true that $e_n ( k ) =e_n ( k+g ) $ . now , what this means is that a description with any more than the first brillouin zone is redundant . the fact that you have multiple bands does not come from the reduction , though it can look that way at first , as a and m present it . i suggest skipping ahead to the chapter on tight binding to understand how multiple bands arise . continued again : okay , i did not address this as clearly as i would hoped . the reason a new quantum number appears at all is that we know from experience that $e ( k ) $ is not single-valued . the reason for this is that each atom has a set of discrete orbitals , and mathematically we can describe the motion of an electron in a lattice as tunnelings between these orbitals . . . however , these orbitals have very different symmetries , and we can not , for example , hop from an s-orbital on site-1 to a p-orbital on site-2 . thus each flavor of orbital symmetry will basically correspond to a new band ( with a lot of technical complications that will only muddy the point ) . lastly , you are right about $e_n$ being bounded everywhere , and your reasoning is right as well . what was the question ?
as i understand it , it is the speed of light . as to what gravity actually is , oh boy , now that is a question . physicists are still trying to work it into the standard model .
there is a definine velocity and momentum , we just do not know it . nope . there is no definite velocity--this was the older interpretation . the particle has all ( possible ) velocities at once ; it is in a wavefunction , a superposition of all of these states . this can actually be verified by stuff like the double-slit experiment with one photon--we cannot explain single-photon-fringes unless we accept the fact that the photon is in " both slits at once " . so , it is not a knowledge limit . the particle really has no definite position/whatever . is not that equivalent to saying because we have not seen star x , it does not exist ? it is limiting the definition of the universe to the limits of our observation ! no , it is equivalent to saying " because we have not gotten any evidence of star x , it may or may not exist --it is existence is not definite " technically , an undetected object does exist as a wavefunction . though it gets slightly philosophical and boils down to " if a tree falls in a forest and no one is around to hear it , does it make a sound ? "
the speed of light is a cosmic speed limit of sorts , so if the universe is expanding such that neither celestial object is moving faster than the speed of light , but the distance between the two is growing faster than the speed of light , then we have a situation where basically the star will be invisible because the photons simply can not cover the distance while abiding by the cosmic speed limit ( speed of light ) . so the short answer is no , there are cases where we would never be able to detect stars . their photons will simply be lost traveling in space , never reaching a destination ( this is why faster than light travel is necessary if we ever hope to even reach the outskirts of our own galaxy , let alone others ) . to go one step further , we know for a fact that as galaxies continue to expand away from one another , we will eventually reach a point where the galaxies we can now easily see , will slowly start dimming and eventually disappearing from the night sky , until the only stars we can see are ones specifically within our galaxy . we also can not know if there are galaxies that this has already occurred with , which is why people generally talk about the universe in terms of our observable universe , since there could be plenty of things out there that we simply cannot see . extras : the alcubierre drive is a speculative faster-than-light engine that relies on expanding and contracting space-time to achieve a faster-than-light traveling speed between objects , even though the space craft would never actually travel faster than light . similar to how the earth and star could move ( relatively speaking ) faster than the speed of light away from each other , except this work to close the distance faster than the speed of light . another quick example is if a train is moving at nearly the speed of light , and someone starts running forward in the train , the train 's on-board time will slow down to ensure the cosmic speed limit is enforced and that the person cannot move faster than the speed of light . physics blows my mind . and if i made any mistakes , please correct me ! thanks !
suppose you take two helium atoms and try to push them together . there will be a short distance repulsion and we had normally describe this as the pauli exclusion force , so it does indeed sound as if the exclusion principle generates a fundamental force . however suppose one of the he atoms has both electrons excited to the $2s$ orbital ( not physically likely , but this is just a thought experiment ) . this will clearly affect the exclusion force because the overlap of the he $1s$ and $2s$ orbitals is different to the overlap of two $1s$ orbitals . indeed , is you push the two he atoms into the same space the overlap integral would go to zero and the exclusion force would disappear . admittedly , there is an egregious amount of hand waving going on here but you get the basic idea . so the force exists because the exclusion principle requires excitation of electrons out of the ground state as they are pushed together , and this costs energy . but it only costs energy because of the electromagnetic force responsible for binding the electrons into the he atoms in the first place . if you weakened the electromagnetic force then the exclusion force would also weaken and in fact disappear in the limit of the em force going to zero . this is why the exclusion force is not a fundamental force . when you push the he atoms together the work you do is going into excitations of the em field not excitations of some " pauli field " . the exclusion principle is a fundamental principle , but the fact that work is required is down to the electromagnetic force .
i think the solution has more to do with the tennis racket effect ( see : stackexchange-url let me clarify the disk with hole in it has two stable axes of rotation and one unstable one . the unstable one is through the hole and the stable one is across ( below in green ) and normal to the disk . i have confirmed that without friction ( and from the videos in the link above ) when the disk is spun on the unstable axis , it will perdiodically flip . this is what caused it to flip when the disk was falling without friction . the additional nuiance here is the once it is on the " upsidedown " oriention and friction is present then the unstable axis becomes stable . if the hole spans from the center to the edge of the disk , then the center of gravity is at $$ \vec{c} = ( 0 , -\frac{r}{6} , 0 ) $$ where $r$ is the outside radius of the disk . the principal moments of inertia about the center of mass are $$ \begin{aligned} i_{xx} and = m \left ( \frac{\ell^2}{12} + \frac{29 r^2}{144} \right ) \approx 0.2 m r^2 \\ i_{yy} and = m \left ( \frac{\ell^2}{12} + \frac{5 r^2}{16} \right ) \approx 0.31 m r^2 \\ i_{zz} and = m \left ( \frac{37 r^2}{72} \right ) \approx 0.51 m r^2 \end{aligned} $$ where $\ell$ is the thickness of the disk . since $i_{yy}-i_{xx} = \frac{8}{37} i_{zz}$ this means that the y direction is the medium inertia value , x the minimum and z the maximum . hence the instability about the y axis according to the tennis racket effect . i am working to qualify the above statement and i am going to update this post with my findings .
if i was on a bus at 60 km/h , and i started walking on the bus at a steady pace of 5 km/h , then i would technically be moving at 65 km/h , right ? not exactly right . you would be correct if the galilean transformation correctly described the relationship between moving frames of reference but , it does not . instead , the empirical evidence is that the lorentz transformation must be used and , by that transformation , your speed with respect to the ground would be slightly less than 65 km/h . according to the lorentz velocity addition formula , your speed with respect to the ground is given by : $$\dfrac{60 + 5}{1 + \dfrac{60 \cdot 5}{c^2}} = \dfrac{65}{1 + 3.333 \cdot 10^{-15}} \text{km}/\text h \approx 64.9999999999998\ \text{km}/\text h$$ sure , that is only very slightly less than 65 km/h but this is important to your main question because , when we calculate the speed of the light relative to the ground we get : $$\dfrac{60 + c}{1 + \dfrac{60 \cdot c}{c^2}} = c$$ the speed of light , relative to the ground remains c !
yes , it is possible . consider a mirror in the shape of a closed hemisphere - that is , half of a concave spherical mirror combined with a flat circular mirror passing through its center . if you are inside this hemispherical mirror , you will see a real image of yourself rotated 180 degrees around the axis of the hemisphere . here 's one way to show this is true : the flat mirror creates a virtual image of you which is your reflection in the plane ( duh ) , and the spherical mirror takes that as an object and creates a real image which is the inversion of that through the center of the sphere . the composition of the reflection and the inversion is a rotation . now , for practical reasons you do not want to be entirely enclosed by the mirror ( it would be hard to get in and out , and you had have to have your own light source . . . ) , so you could build just a part of the hemisphere . but the part you built would have to contain both some of the spherical surface and some of the flat surface for it to work . if you have trouble understanding the orientation of everything , here is a concrete description : the flat mirror is a circle in the $z=0$ ( horizontal ) plane , $x^2+y^2\le1$ . the spherical mirror is a hemisphere described by $z&gt ; 0$ , $x^2+y^2+z^2=1$ . you are standing inside so your hand is at a point near the axis , for example , ( 0.01 , 0 , 0.2 ) . what the mirrors do is this : for an object at a point $ ( x , y , z ) $ , the flat mirror creates an image at $ ( x , y , -z ) $ ( this image has opposite handedness because one coordinate is flipped ) . then the spherical mirror takes that image at $ ( x , y , -z ) $ and creates another image at $ ( -x , -y , z ) $ . this image has the same handedness as the original object . it is also " right side up " because the z coordinate is not flipped . the image of your right hand at ( 0.01 , 0 , 0.2 ) will appear as another right hand at ( -0.01 , 0 , 0.2 ) which is in the appropriate orientation for you to shake hands .
during the flight , you need to get up to use the restroom . there is one 10 rows in front of you , and another 10 rows behind you . does it take longer to walk to the one that is moving away from you at 600 mph than the one that is moving towards you at 600 mph ? no , because you are moving at 600 mph right along with it -- in the ground-based frame of reference . in the frame of reference of the airplane , everything is stationary . similarly , the airplane is already moving along with the surface of the earth before it takes off . the rotation of the earth has no direct significant effect on flight times in either direction . that is to a first order approximation . as others have already said , since the earth 's surface is ( very nearly ) spherical and is rotating rather than moving linearly , coriolis effects can be significant . but prevailing winds ( which themselves are caused by coriolis and other effects ) are more significant that any direct coriolis effect on the airplane .
are these really elementary , or they can be expressed in terms of more elementary entities ? well , consider that , for example , according to the standard model , electric charge is a particular mixture of weak isospin and weak hypercharge : from wiki : electric charge , $q$ , is related to weak isospin , $t_3$ , and weak hypercharge , $y_w$ , by $q = t_3 + \frac{y_\mathrm{w}}{2}$ i am not sure that " entity " is correct here though . perhaps " property " ?
i think it really depends on what you mean by depolarized , but a spherical particle like this scatters light with a complicated , but wholly foretellable ( by mie theory ) and reproducible polarization dependence on scattering direction . so you imagine a farfield radiation diagram for the scattered light . to get the full picture , a " radiation diagram " would be a complex jones vector field on the surface of a unit sphere . each point on the unit sphere represents the direction defined by the ray joining the sphere 's centre and the surface point in question . at each point , two complex quantities define the relative intensity and polarisation of the scattered wave . this will be a complicated object : as shown in born and wolf " principles of optics " section 14.5 ( "diffraction by a conducting sphere : theory of mie ) , this polarization field can be exquisitely sensitive to position on our unit sphere . but there is one , consistent , reproducible and only one such polarization state for each point on the unit sphere sphere . you do not get the polarization state randomly varying with time if the mie scatterer is lit by coherent , polarized light . so some people call this complicated polarization scrambling " depolarization " and what you have drawn is a fairly accurate intuitive guide to how this polarization scrambling arises . true depolarization arises from thes phenomenons : the scattering properties of your mie object to fluctuate randomly with time : these properties can be ( i ) orientation , if the object is not spherically symmetric ( e . g . tumbling polar molecule ) , ( ii ) length dimensions if a micro- as opposed to molecular sized object vibrates , ( iii ) relative positions of scatterers if more than one scatterer is involved . the mixing of many such scatterings as you have drawn above by objects whose relative positions and orientations fluctuate randomly with time . the relationship between polarizations of incoming and scattered light is also fundamentally bound up with the exchange of angular momentum between the light and the medium it interacts with . for further explanation of this statement , see chapter 18 of the third volume of the " feynman lectures on physics " . this chapter is called " angular momentum " . there is a simple way to summarise all these complicated mechanisms : these are the interactions of light with a thermalized system of scatterers .
short answer - frequency ( $\nu$ ) stays constant since it is a characteristic property of the source . using the relation $v = \lambda \nu$ in the definition of the refractive index , $n = c/v$ , your required answer follows .
here 's a video of physicist richard feynman discussing this question . imagine a blue dot and a red dot . they are in front of you , and the blue dot is on the right . behind them is a mirror , and you can see their image in the mirror . the image of the blue dot is still on the right in the mirror . what is different is that in the mirror , there is also a reflection of you . from that reflection 's point of view , the blue dot is on the left . what the mirror really does is flip the order of things in the direction perpendicular to its surface . going on a line from behind you to in front of you , the order in real space is your back your front dots mirror the order in the image space is mirror dots your front your back although left and right are not reversed , the blue dot , which in reality is lined up with your right eye , is lined up with your left eye in the image . the key is that you are roughly left/right symmetric . the eye the blue dot is lined up with is still your right eye , even in the image . imagine instead that two-face was looking in the mirror . ( this is a fictional character whose left and right side of his face look different . his image on wikipedia looks like this : ) if two-face looked in the mirror , he would instantly see that it was not himself looking back ! if he had an identical twin and looked right at the identical twin , the " normal " sides of their face would be opposite each other . two-face 's good side is the right . when he looked at his twin , the twin 's good side would be to the original two-face 's left . instead , the mirror two-face 's good side is also to the right . here is an illustration : so two-face would not be confused by the dots . if the blue dot is lined up with two-face 's good side , it is still lined up with his good side in the mirror . here it is with the dots : two-face would recognize that left and right have not been flipped so much as forward and backward , creating a different version of himself that cannot be rotated around to fit on top the original .
i will try an answer , even if i am not sure to understand fully what the question means . using the second quantization formalism in qm is actually sometimes useful , but it is in some sense just a trick . suppose you have $n$ identical $1$-dimensional bosons interacting by means of a two-body potential . the hilbert space of the theory is $l^2_s ( \mathbb{r}^n ) $ , the space of square integrable symmetric functions . the hamiltonian is usually something like this : $$h_n=-\sum_{j=1}^n \delta_j +\frac{1}{n}\sum_{i&lt ; j} v ( x_i-x_j ) $$ where $v$ is symmetric and real . you can see this operator as the restriction to the $n$-particle sector of an operator in a symmetric fock space ( second quantization formalism ) . the fock space is $\gamma_s ( l^2 ) =\bigoplus_{n=0}^\infty l^2_s ( \mathbb{r}^n ) $ ( with the convention $l^2_s ( \mathbb{r}^0 ) =\mathbb{c}$ ) and the operator is , using the usual annihilation and creation operators , $$h=\int_{\mathbb{r}} ( \nabla a ) ^* ( x ) \nabla a ( x ) dx +\frac{1}{2}\int_{\mathbb{r}^2}v ( x-y ) a^* ( x ) a^* ( y ) a ( x ) a ( y ) dxdy\ ; . $$ it is easy to see that $h\rvert_{l^2_s ( \mathbb{r}^n ) }=h_n$ . in many body theory you have a state with fixed number of particles , i.e. $\psi_n\in l^2_s ( \mathbb{r}^n ) $ , and the evolution $u_n ( t ) $ generated by $h_n$ ( equivalently $h$ ) maps $l^2_s ( \mathbb{r}^n ) $ on itself ( preserves the number of particles ) . so even if you have these two ways of writing the hamiltonian of your system , they are completely equivalent . in some sense , you have a direct sum of " theories " that do not interact , each one corresponding to a different number $n$ of particles : not only the fock space is a direct sum , but also $h=\bigoplus_{n=0}^\infty h_n$ ; once the number $n=n$ is fixed and it is finite , your system will never leave the subspace $l^2_s ( \mathbb{r}^n ) $ . in real relativistic qfts , the situation is radically different . the hamiltonian does not preserve the number of particles , and has terms with an uneven number of creation and annihilation operators . so it is impossible to give a closed description of the dynamics in the subspace $l^2_s ( \mathbb{r}^n ) $: even if you start with a state in this space , the evolution will create and destroy particles , and you will end up with a state that spreads in general on the whole fock space . it is therefore of no use to search for a " first quantization " ( i.e. . in the subspace $l^2_s ( \mathbb{r}^n ) $ ) description of qfts .
your confusion comes from the oft-made , but not strictly true , assumption that global potential energy minima must be deep local minima . stability is a function of the local depth of a potential energy well in a potential curve , and excited states in atoms/molecules have different potential curves , above those of their ground states . it is however possible for the potential energy curves in the excited state to have deeper wells than in the ground state , like so . the additional detail with excited states is that they can decay by de-excitation as well as decomposition . if the excited state is to be long-lived , then not only must its potential well be deep , but once the atom/molecule is excited , de-excitation must be somehow suppressed . it could be a process forbidden by some selection rule , for example a spin-forbidden transition .
ah , good question . the radian is actually a " fake unit . " what i mean by that is that the radian is defined as the ratio of distance around a circle ( arclength ) to the radius of a circle - in other words , it is a ratio of one distance to another distance . for an angle of one radian specifically , the arclength $s$ is equal to the radius $r$ , so you get $$1\text{ rad} = \frac{s}{r} = \frac{r}{r} = 1$$ the units of distance ( meters or whatever ) cancel out , and it turns out that " radian " is just a fancy name for 1 ! incidentally , this also implies that " degree " is just a fancy name for the number $\frac{\pi}{180}$ , and " rotation " is just a fancy name for the number $2\pi$ . this actually addresses the edit to your question . suppose that you had some object oscillating at $\omega = \pi/4\frac{\mathrm{rad}}{\mathrm{s}} = 0.785\frac{\mathrm{rad}}{\mathrm{s}}$ , and you wanted to evaluate its position after 10 seconds . to get the cosine term , you would plug the numbers in , getting $$\cos\bigl ( 0.785\tfrac{\mathrm{rad}}{\mathrm{s}}\times 10\mathrm{s}\bigr ) = \cos ( 7.85\text{ rad} ) = \cos ( 7.85 ) $$ and then you would go to a trig table in radians ( or your calculator in radian mode ) and look up 7.85 . however , suppose that you were measuring $\omega_0$ in degrees per second instead of radians per second . you would instead have $$\cos ( 45^\circ/\mathrm{s}\times 10\mathrm{s} ) = \cos ( 450^\circ ) $$ if you go look this up in a trig table given in degrees , you will get the same answer as $\cos ( 7.85 ) $ . why ? well , remember that the unit " degree " is just code for $\pi/180$ , so this is actually equal to $$\cos\bigl ( 450\times\tfrac{\pi}{180}\bigr ) $$ and $450\times\frac{\pi}{180} = 7.85$ , which is just $450^\circ$ converted to radians . so now you have the same value in the cosine , $\cos ( 7.85 ) $ . trig tables listed in degrees already have this extra factor of $\frac{\pi}{180}$ built into them as a convenience for you ; basically , if you look up any number $\theta$ in a table that uses degrees , what you get is actually the cosine ( or sine , or whatever ) of $\theta\times\frac{\pi}{180}$ .
there are roughly $10^{28}$ particles in the human body . if we assume that each particle can be described by a single double-precision number , then it would take $$ 10^{28}{\rm atoms}\cdot\frac{{\rm numbers}}{{\rm atoms}}\cdot\frac{8{\rm bytes}}{{\rm numbers}}\simeq10^{29}\ , {\rm bytes}\sim10^{17}{\rm terabytes} $$ which is way more memory than what is thought to be made in the world currently . so there is one limitation . i know some space based observatories can handle the rejection/acceptance of particles at about a million per second . if we assume we can do a few orders of magnitude better in determining particles with some non-existent earth-based scanning device ( another limitation ) , then $$ t_{scan}=\frac{10^{28}\ , {\rm particles}}{10^{12}{\rm particles/sec}}=10^{16}\ , {\rm sec}\lesssim t_{age\ , of\ , universe}=4\times10^{17}{\rm sec} $$ so there is your third limitation . there would likely be a time limitation with rebuilding the stored information similar to the one above , but i am not sure any type of numerics on it . seems that we are really far away from being able to do this , if it could ever be done ( which i am still doubtful of ) .
we start with $$\tag{1} [ b , b^{\dagger} ] ~=~{\bf 1} , $$ and two numbers $w$ and $x$ . the shift and bogoliubov transformation are encoded by $$\tag{2}s_1 ~:=~ w ( b-b^{\dagger} ) , $$ and $$\tag{3}s_2 ~:=~ \frac{x}{2} ( b^2 - ( b^{\dagger} ) ^2 ) , $$ respectively , so that $$\gamma ~:=~\cosh ( x ) b + \sinh ( x ) b^\dagger + w{\bf 1} ~=~ e^{s_2} ( b+w{\bf 1} ) e^{-s_2} $$ $$\tag{4}~=~ e^{s_2}e^{s_1} b e^{-s_1}e^{-s_2}~=~ e^{s_3} b e^{-s_3} . $$ here we have used that $$\tag{5} [ s_2 , b ] ~=~x b^{\dagger} , \qquad [ s_2 , b^{\dagger} ] ~=~x b , $$ so that $$ e^{s_2} b e^{-s_2}~=~e^{ [ s_2 , \cdot ] }b ~=~\cosh ( [ s_2 , \cdot ] ) b + \sinh ( [ s_2 , \cdot ] ) b$$ $$\tag{6}~=~\cosh ( x ) b + \sinh ( x ) b^{\dagger} . $$ note that $$\tag{7} [ s_1 , s_2 ] ~=~xs_1 . $$ therefore $s_3$ in eq . ( 4 ) is given by the baker-campbell-hausdorff formula $$\tag{8} s_3~=~\ln ( e^{s_2}e^{s_1} ) ~=~bch ( s_2 , s_1 ) ~=~s_2+b ( x ) s_1~=~\underline{\underline{s_2+\frac{x}{e^x-1}s_1}} , $$ where $$\tag{9} b ( x ) :=\frac{x}{e^x-1}$$ is the generating function of bernoulli numbers .
you are understanding correctly . in the massless up/down quark limit , chiral symmetry is restored , and the pion becomes massless but quarks are still confined , and baryons have about the same mass as they do now . this is exactly why the idea that the pion is made of quarks is nonsense . in the 1980s , many in the new generation sought to undo the progress of the 1960s , and willfully ignored the revolutionary work of nambu , sakurai , skyrme , and others , dismissing it as pre-quark nonsense . they decided that a pion is made up of two nonrelativistic quark-objects , they called these objects " constituent quarks " , and they made up force laws for these to reproduce the hadron spectrum . georgi and glashow even went so far as to invent a quark-quark coupling force which was designed to lower the mass of the pion by interquark interactions ! this work is a little embarassing to read . the proper model of the pion was the much earlier one due to nambu and weinberg , and this is now verified thanks to numerical lattice qcd , where the mass of the quark can be tuned at will . when you tune the mass of the quarks to zero , the pion mass vanishes according to the laws of chiral peturbation theory . the pion is a mode of oscillation of the quark chiral condensate , a material filling all of space . it is made out of quarks which are created by the independent fluctuations of the gluon field . the gluon field completely randomizes on a baryon scale , meaning that a quark going in a closed path larger than a proton circumference will get a completely random pick from su ( 3 ) as its holonomy . a random gauge field will create large numbers of objects whose mass scale is much lower than this randomization scale , and in this case , the objects it creates are the light up and down quarks , and to a lesser extend strange quarks . these quarks condense in pairs in the vacuum , making a condensate whose order parameter is much like a mass term in the dirac equation : $m \bar\psi \psi$ . this condensate is not invariant under rotations of the left and right-handed quarks into each other , but the lagrangian is ( more or less , except for the negligible quark mass ) . the goldstone modes of the broken symmetry are waves in this condensate , and these are the pions . the goldstone mode is due to oscillations where the left and right part of the condensate slosh in phase in opposite directions , and these are collective excitations of quarks . the pion is made of quarks to the same extent that a sound wave is made of atoms . that the pions are goldstone bosons was not only theoretically predicted by nambu , it explains their strange derivative couplings at low energy , and this was spectacularly extended to a full theory by weinberg 's soft-pion theorems , and chiral perturbation theory . the condensates were further used to give nonperturbative corrections to qcd particle propagation at intermediate distances in the shifman-vainshtein-zakharov sum rules . so really , everyone should have known better than constituent quarks . it is not clear that the notion of " constituent quark " actually has any form of real meaning , or whether it is just a figment of the imagination . the only partial evidence in it is favor that i think is not easy to explain in other way is that the total cross sections for pions are about 2/3 the total cross section for protons , as if the pomeron hits 2 quarks instead of three . i do not know if this approximate equality is not just a coincidence .
so , you are correct , that it is not quite as simple as that . have you studied special relativity ( sr ) yet ? one of the most fundamental ideas is the limiting speed of light . nothing moves faster than c , and this means that velocities must add differently . think of the reverse , if you were on the rocket-ship traveling at $0.5c$ and shot a laser-beam at $c$ , you certainly will not get the laser traveling at $1.5c$ ! see here http://en.wikipedia.org/wiki/relativistic_velocities#composition_of_velocities so , we need $$ u ' = \frac{u-v}{1- \frac{uv}{c^2}}$$ in our case , we get $$ u ' = \frac{0.5c-0.3c}{1- \frac{0.5c \times 0.3c}{c^2}} = \frac{0.2c}{1-0.15} = 0.235c$$
there are a bunch of questions here . let me try to take them in order : is it possible that our universe has the feature that if you travel far enough you return to where you started ? yes . the standard big-bang cosmological model is based on the idea that the universe is homogeneous and isotropic . one sort of homogeneous spacetime has the geometry of a 3-sphere ( like a regular sphere , but with one more dimension ) . in these cosmological models , if you travel far enough you get back to where you started . however , the best available data seem to indicate that the universe is very nearly spatially flat . this means that , if we do live in a 3-sphere universe , the radius of the sphere is very large , and the distance you had have to travel is much larger than the size of the observable universe . even if that were not true , the fact that the universe is expanding would make it hard or impossible to circumnavigate the universe in practice : no matter how fast you went ( short of the speed of light ) , you might never make it all the way around . nonetheless , 3-sphere universes , with the geometrical property you describe , are definitely viable cosmological models . does this give rise to a symmetry by noether 's theorem ? not really . noether 's theorem is generally applied to continuous symmetries ( i.e. . , ones that can be applied infinitesimally ) , not discrete symmetries like this . the fact that space is homogeneous gives rise to a symmetry , namely momentum conservation , whether or not space has the 3-sphere geometry , but the symmetry you are talking about here does not give rise to anything extra . would small curled up dimensions have the same sort of symmetry ? i will leave this for someone else , i think . not my thing . is it known exactly what the geomtrical shape of the universe is ? no , and do not let anyone tell you otherwise ! sometimes , especially in pop-science writing , people imply that we know a lot more about the global properties of the universe than we do . we often assume things like homogeneity to make our lives simpler , but in fact we have precisely no idea what things are like outside of our horizon volume . how to describe the " size " of a dimension ? if the universe 's geometry has enough symmetries , it makes sense to define an overall time coordinate everywhere . then it makes sense to imagine a " slice " through spacetime that represents the universe at an instant of time . if some of those slices have the geometrical property you are talking about , that traveling a distance r in a certain direction gets you back to your starting point , then it makes sense to call r the " size " of the corresponding dimension . if you can travel forever , then we say the size in that dimension is infinite . is it possible to describe to a layman the shape of the universe without resorting to inept analogies ? all analogies are imperfect . i think the best you can do is use a bunch of them and try to convey the limitations of each .
recall $ \hat{h} = \left ( \hat{n} + \frac{1}{2} \right ) $ and $ \left [ \hat{a} , \hat{a}^\dagger \right ] = 1 $ ( dropping $\hbar$ and $\omega$ ) . assume the ground state $\left|0\right&gt ; $ is non-degenerate . you can prove this by solving $\left&lt ; x\right|\hat{a}\left|0\right&gt ; =0$ in position representation , but i do not know how to do it algebraically . the rest of the proof is algebraic . let the first excited state be $k$-fold degenerate : $\left|1i\right&gt ; $ , $i=1 , \ldots , k$ , where $\left|1i\right&gt ; $ orthonormal . then , by the algebra we have $$ \hat{a} \left|1i\right&gt ; = \left|0\right&gt ; $$ and $$ \hat{a}^\dagger \left|0\right&gt ; = \sum_i c_i \left|1i\right&gt ; $$ where $ \sum_i c_i^\star c_i = 1 $ . now , for these states to be eigenstates of $\hat{h}$ with energy $\frac{3}{2}$ they must be eigenvalues of $\hat{n}$ with eigenvalue 1 . this requires $$ \begin{matrix} \hat{n}\left|1i\right&gt ; and = and \hat{a}^\dagger \hat{a}\left|1i\right&gt ; \\ and = and \hat{a}^\dagger \left|0\right&gt ; \\ \left|1i\right&gt ; and = and \sum_j c_j \left|1j\right&gt ; \end{matrix}$$ this must hold for all $i$ , which leads to an immediate contradiction ( no solution for the $c_i$ ) unless $k=1$ . induction proves non-degeneracy for the higher states .
only those photons that travel in the same spatial mode are used , and these are located at the intersection of the cones in which the down-converted photons can be found . as you can see , momentum and energy conservation imply that the colors at the intersection points are equal . quoting : " along the intersections of the cones of the same wavelength ( in our photograph the green circles ) polarization-entangled photon states can be observed . "
the electron is an elementary particle , bound by the rules of quantum mechanics . it is not a particle like a tiny billiard ball , it is a quantum mechanical entity which sometimes does have mass and a position in spacetime ( x , y , z , t ) and sometimes it manifests as a probability wave in dimensions commensurate to h , the planck constant . the boundary conditions of the problem define which will be the manifestation , particle or probability wave , when a measurement is made of a particular electron . this means that when an electron is free , not in a potential well , measurements will show a track defining a classical trajectory of its motion . bubble chamber photograph of an electron knocked out of a hydrogen atom the electron you see knocked off before the knock was in an orbital around the proton nucleus of the hydrogen atom . an orbital , not an orbit , because its location is probable , described by a probability wave given by a mathematical formula , the square of the wavefunction which is a solution of the potential problem " electron and proton in the field of each other " . the shapes of the first five atomic orbitals : 1s , 2s , 2px , 2py , and 2pz . the colors show the wave function phase . these are graphs of  ( x , y , z ) functions which depend on the coordinates of one electron . to see the elongated shape of  ( x , y , z ) 2 functions that show probability density more directly , see the graphs of d-orbitals below . so the electron in its previous life was not orbiting the proton , the way the moon orbits the earth , but it had a probability of being in a particular ( x , y , z ) when probed . so in the image , the electron moves because another particle kicked it and transferred momentum enough to free it from the proton of the hydrogen atom . when in a orbital it is in a steady state situation , except its position is undefined within the limits given by the probability derived from the wavefunction , which depends on the potential in the problem .
optical diodes ( aka isolators ) are nothing new http://en.wikipedia.org/wiki/optical_isolator recent work like the one you cite is not about demonstrating that optical diodes are possible , it is about making optical diodes that are compatible with silicon-based integrated circuits . here is more about that . a solar sail does not need to be a special material , because sunlight only comes from one direction . therefore i assume you are imagining propelling a spacecraft through deep space off the cosmic microwave background radiation . that radiation is about 3k , so if the spaceship is also at 3k , the second law of thermodynamics forbids its acceleration , no matter what it is made of , unless it is burning power . even if it is made of optical diodes , it will also have its own blackbody radiation that will exactly cancel out the effects of the cmb . the passive acceleration you are hoping for is impossible . on the other hand , if the ship is hotter than 3k , it can certainly be propelled . but you do not need any special kind of material to make that happen , you just need one side to have higher emissivity than the other side . ( cf . pioneer anomoly . ) this can be easily done in practice , and should be simple to model in theory .
your worry is not necessary . in the usual experiment the detector measures the distribution of the time between the muon stopping in the detector and the time of it is decay . then an exponential curve is fit to the data and the lifetime taken from the fit parameters muon decay is a processes analogous to radioactive decay , and ( like all exponential processes ) it has the special property that the lifetime from the fit does not depend on the absolute start time . an important thing here is what the " lifetime " means . and what it does not mean . it does not mean that muons tend to decay $2.2 \ , \mathrm{\mu s}$ after they are created ( which would imply that you are looking for a peak and that the absolute start time matters ) . it does mean that given a collection of muons , if you wait $2.2 \ , \mathrm{\mu s}$ about 67% of them will have decayed . and if you wait another lifetime about 67% of the one that were left will have decayed . and so on . this means that anytime you fit an exponential to the decay time distribution you will always measure the same lifetime . a kind of old school way to see this is to note that $y = a \exp \left ( -t/\tau \right ) $ implies that ( writing $a = \exp \alpha$ ) $$ \ln y = \alpha -\frac{1}{\tau} t \ , , $$ which is a straight line if you plot $\ln y$ as a function of $t$ . the slope of the line is $-1/\tau$ and does not depend on the range of $t$ over which you measure it .
i will first describe the naive correspondence that is assumed in usual literature and then i will say why it is wrong ( addressing your last question about hidden assumptions ) : ) the postulate of relativity would be completely empty if the inertial frames were not somehow specified . so here there is already hidden an implicit assumption that we are talking only about rotations and translations ( which imply that the universe is isotropic and homogenous ) , boosts and combinations of these . from classical physics we know there are two possible groups that could accomodate these symmetries : the gallilean group and the poincar group ( there is a catch here i mentioned ; i will describe it at the end of the post ) . constancy of speed of light then implies that the group of automorphisms must be the poincar group and consequently , the geometry must be minkowskian . [ sidenote : how to obtain geometry from a group ? you look at its biggest normal subgroup and factor by it ; what you are left with is a homogeneous space that is acted upon by the original group . examples : $e ( 2 ) $ ( symmetries of the euclidean plane ) has the group of ( improper ) rotations $o ( 2 ) $ as the normal subgroup and $e ( 2 ) / o ( 2 ) $ gives ${\mathbb r}^2$ . similarly $o ( 1,3 ) \ltimes {\mathbb r}^4 / o ( 1,3 ) $ gives us minkowski space . ] the converse direction is trivial because it is easy to check that the minkowski space satisfies both of einstein postulates . now to address the catch : there are actually not two but eight kinematical groups that describe isotropic and uniform universes and are also consistent with quantum mechanics . they have classified in the bacry , lvy-leblond . the relations among them is described in the dyson 's missed opportunities ( p . 9 ) . e.g. , there is a group that has absolute space ( instead of absolute time that we have in classical physics ) but this is ruled out by the postulate of constant speed of light . in fact , only two groups remain after einstein 's postulate have been taken into account : besides the poincar group , we have the group of symmetries of the de sitter space ( and in terms of the above geometric program it is $o ( 1,4 ) / o ( 1,3 ) $ ) . actually , one could also drop the above mentioned restriction to groups that make sense in quantum mechanics and then we could also have an anti de sitter space ( $o ( 2,3 ) / o ( 1,3 ) $ ) . in fact , this should not be surprising as general relativity is a natural generalization of the special relativity so that the einstein 's postulates are actually weak enough that they describe maximally symmetric lorentzian manifolds ( which probably was not what einstein intented originally ) .
in drawing your triangle , you assume that the object has already moved in the direction of your force ( maybe just a little bit ) . in doing so , you forget that the force then stops being perpendicular . what you have ( re ) discovered is simply that a force which is constant throughout space cannot remain perpendicular to the motion of a free particle if it lasts a finite amount of time : the particle will simply start to move in the direction of the force , picking up velocity as it starts moving in that direction . however , if $\vec{f}$ is not of the same magnitude and direction everywhere , one can produce a force that remains perpendicular . the canonical example is that of a body exerting gravity . the direction of the force is always towards the particle ( i.e. . not the same everywhere ) . it also decreases with distance , but that is really not an essential point here . now , if a test particle at a distance $d$ moves past the other body with the right velocity , this force will always remain perpendicular . can you guess what kind of shape the trajectory of the test particle will trace out ? hint : think about the symmetry of the problem .
that is easy . hamiltonian mechanics describes reversible dynamics . just introduce irreversibility in your system . like friction , dissipation , viscosity etc . can you answer the question now ?
one example of such an approach is ambjorn and loll 's causal dynamical triangulations , which is very similar in many ways to the very old idea of regge calculus , whereby spacetime is discretized . at small scales , non integer dimensions can emerge . for an introductory article , see jan ambjrn , jerzy jurkiewicz and renate loll . the self-organizing quantum universe . scientific american ( july 2008 ) , 299 , pp . 42-49 . doi:10.1038/scientificamerican0708-42 , available here .
firstly , remember what it means for space to be expanding - it does not mean that space is some rubbery fabric that gets pulled , it means that the metric of space is expanding . that is , the scale factor ( which essentially represents the relative distance of objects ) is growing in time . so , if all galaxies were separated by some distance , this distance would then be larger at a later time . so , how could we tell that the expansion is really metric expansion , and not relative velocity ? well , the first reason is hubble 's law . since the universe is undergoing metric expansion , it appears that galaxies have an apparent velocity , given by $v=h_{0}d$ . so , we see that galaxies that are further away are moving away with a higher velocity . this makes sense - photons travelling from more distant galaxies must traverse more expanding space , and therefore have their wavelengths expanded by a larger amount , so that they are more redshifted . this would not be the case if galaxies were just moving away from us , since we would see a wide variety of redshifts , not a pattern . the second reason is general relativity . gr predicts that certain metrics that contain homogeneous distributions of matter ( i.e. . the galaxies that make up the universe ) will cause metric expansion to occur . since gr is supported by the evidence , so therefore is metric expansion . third is the cosmic microwave background . we observe a uniform microwave radiation that fills the universe , that has a temperature of 2.73 degrees kelvin . satellites that have studied it ( cobe , wmap ) have determined that it has a blackbody curve , and that it has the redshift that dates it back to a time very soon after the big bang . we now know that this represents the first radiation ever emitted , 380,000 years after the big bang at the recombination , when hydrogen atoms formed . another reason is that when a quasar 's spectrum shows absorption lines due to neutral hydrogen clouds , the redshifts of the hydrogen lines are always found to be less than the redshift of the quasar . furthermore , examples have been found in which the absorption spectrum shows a feature called the gunn-peterson trough , which had been predicted earlier as a consequence of the reionization of hydrogen in the early universe . however , your example considers only two galaxies . in that case , there is nothing that they can do to determine if they are moving apart just due to velocity , or expansion . fortunately , we do not live in such a universe . for the evidence for metric expansion ( and the big bang ) see here : http://www.talkorigins.org/faqs/astronomy/bigbang.html
lets define some variables first . lets say that the length of the column of air that you trap in the tube between the water level and your thumb is $h_0$ and it is initially at the same pressure as the surrounding air which we will call $p_0$ . lets also define the cross-sectional area of the tube to be $a$ . as you draw the tube out of the water , the water level in the tube will rise above the surrounding water level , and the pressure at the bottom of this column of water will be given by $$ p_0=p_t+\rho g h_w , $$ where $p_t$ is the pressure of the volume of air trapped by your thumb , $\rho$ is the mass density of the water , $g$ is the gravitational acceleration at the surface of the earth , and $h_w$ is the height difference between the water in the tube and the surrounding water . the second term on the right hand side is a standard equation from fluid statics . you can see from this already that the pressure can not be the same at the top of the tube anymore , as you suspected . we can use the ideal gas law to rewrite the first term on the right hand side $$ p_tv_t=p_0v_0\qquad\rightarrow\qquad p_t=p_0\frac{v_0}{v_t} $$ now the volume of air at the top of the tube before you start to draw it up is given by $v_0=ah_0$ , but as you draw it up the pressure and volume will change to $v_t=ah_t$ . so , putting all of this information into the first equation we get $$ p_0=p_0\frac{h_0}{h_t}+\rho g h_w $$ the final thing we need to take care of is replacing $h_t$ with something we actually know . namely , the height we draw the tube up to , which we will call $h_s$ and is given by $h_s=h_t+h_w$ . sticking this into the above equation yields $$ p_0=p_0\frac{h_0}{ ( h_s-h_w ) }+\rho g h_w $$ solving this equation for $h_w$ gives $$ h_w=\frac{1}{2\rho g}\left ( p_0+\rho g h_s+\sqrt{p_0^2+ ( \rho gh_s ) ^2+p_0\rho g ( 4h_0-h_s ) }\right ) . $$ i have plotted this equation below for differing values of $h_0$ . notice that there is a maximum height to which you can draw water with this method , this is the height at which the pressure in the tube reaches zero . finally , since your question was actually about the pressure in the tube , we can rearrange the above equations to solve for the pressure in the tube under your thumb . $$ p_t=p_0-\rho gh_w=\frac{1}{2}\left ( p_0-\rho g h_s-\sqrt{p_0^2+ ( \rho gh_s ) ^2+p_0\rho g ( 4h_0-h_s ) }\right ) . $$ this equation is plotted below for a number of different values of $h_0$ .
the answers . com page you mentioned uses the following formula : $$l_{planck} = \sqrt{\frac{gh}{2\pi c^3}}$$ note that there is the $2\pi$ factor in the denominator - so $h/2\pi$ may be simplified as the usual $\hbar$ . they probably were not able to type this character , or wanted to avoid terminology and symbols that are only known to physicists . but there is no numerical error on the answers . com page . at any rate , the definition above is equivalent to $$l_{planck} = \sqrt{\frac{g\hbar}{c^3}}$$ which is the usual " unreduced " planck length . see wikipedia for the same formula : http://en.wikipedia.org/wiki/planck_length numerically , it is $1.6 \times 10^{-35}$ meters . ( update : the oxford dictionary of english has a wrong formula - they omitted $2\pi$ and forgot to cross the $h$ , too . but they clearly mean the same planck length . ) sometimes , people also use the " reduced " planck length which is more fancy and " professional " in a sense : $$l_{planck , reduced} = \sqrt{\frac{8\pi g\hbar}{c^3}}$$ note that the $8\pi$ in the numerator may also be merged with the $\hbar$ to get $4h$ back - so the reduced planck length is twice ( because of the square root ) the wrong planck length that you would get by using $h$ instead of $\hbar$ . but what is the real reason why $8\pi$ was added there ? the reason why $8\pi g$ appears instead of $g$ is because in some sense , $8\pi g$ is more natural a constant than $g$: this discussion is analogous to the treatment of $4\pi$ in electrodynamics . the constant $8\pi g$ is natural because the einstein-hilbert action is $$s_{eh} = \int d^d x \frac{1}{16\pi g} r\sqrt{-g} $$ the most natural coefficient would be $1/2$ instead of $1/16\pi g$ which makes it natural to set $8\pi g=1$ . the reduced planck length is somewhat longer ( five times or so ) - less extremely tiny . even more often , particle physicists talk about the planck energy and the reduced planck energy which are close to $10^{19}$ and $10^{18}$ gev , respectively . the convention for the constant $g$ was originally chosen by newton who wanted to write the gravitational force as $gmm/r^2$ . well , it would be more natural to have the factor of $4\pi$ or $8\pi$ in the denominator , $\gamma mm/8\pi r^2$ . you can see that $\gamma$ is simply $\gamma=8\pi g$ , and it would be natural to set $\gamma$ equal to one . i hope that i do not have to explain why $\hbar$ is more natural than $h$ for adult physicists . the " laymen " versions of the formulae may be simpler with $h$ - but they deal with wavelength etc . adult physicists know that the wavelength of the sine is proportional to $2\pi$ . and the most fundamental equations , such as the schrdinger 's equation or the commutators of $ [ x , p ] $ , take a simpler form in terms of $\hbar$ than $h$ , of course . back to $g$: people had to choose the convention how to normalize $g$ in higher dimensions . the usual convention , as implicitly used above , is that the einstein-hilbert action always has the coefficient $1/16\pi g$ . that implies that in $d$ spacetime dimensions , the force will not be $gmm/r^{d-2}$ but it will have some $d$-dependent numerical coefficients in it . best wishes lubos
the twin paradox is generally considered to be an illustration of time dilation , not time travel . to answer your question , the difference between the twins is that twin a was accelerated several times . speed is relative ; if an object is moving at a constant speed , whether the object is considered to be moving or not depends on which inertial frame of reference you choose to measure the object 's speed in . the same is not true of acceleration ; an accelerating object 's acceleration will be measured to be nonzero no matter which inertial frame of reference is used to measure the acceleration in . so all observers will agree that it was twin a instead of twin b who accelerated between different inertial frames of reference .
if i understand you correctly , your two points about apparent slowness of speeds is related to scale , and disappears when you quantify it using a common unit . ie : we think of 10m/s as relatively slow because the average human is 1.8 metres in height , and we can imagine that 10 metres per second , or 36 kilometer/hour as an achievable speed using a machine ( car ) . if we were the size of a proton , obviously , 36 km/h would appear to be quite fast . it is quite fast even if we were the size of ants . but in physics , we quantify everything . to human of our size , and to a human the size of a proton , we will have to agree on scales and units , and thus the speed in itself will not change due to the size consideration alone . as for you final scenario : the relative speed in both cases is 0.999c , and c would be the same unit for both parties , so the relative speed is the same . on the other hand , if we used the height of human and diameter of proton ( assuming hard spherical shape for simplicity ) covered in a second as the unit in each case respectively , then we would have a different numerical value as the units are different . if the numerical values were the same , then the speeds would actually be different because the units are different .
l , angular momentum , is not like linear momentum ( $p$ ) that means that the object is actually moving in that direction . angular momentum and torques are defined via the " cross-product " , which means mathematically that angular momentum and torque vectors point 90$^\circ$ from the radius and force that produce the torque and angular momentum . the radius is the red arrow in the animation while the force is the dark green arrow in the diagram . that is why the angular momentum vector is pointing up and down in the animation ; however , that does not translate to actual linear motion in that direction . it instead represents a direction that the object is spinning , using the right hand rule . if you take your right hand and point your thumb in the direction of the angular momentum vector and curl your fingers , the direction that your fingers point is the direction in which the object will be spinning . to repeat , mathematically the angular momentum vector represents the direction and speed with which an object is spinning ( which direction can be found via the right-hand rule ) . for a less mathematical explanation , if what you claim is true ( that rotating things counter-clockwise will cause them to rise ) then i think it would be difficult to keep tops on the table , since they are able to spin pretty fast and would be able to levitate . i have never seen this happen , nor have i seen a situation where spinning a top increased or decreased it is weight ( based on the direction it is spinning ) . however , spinning the top ( while not allowing it to fly ) does increase its stability by making it precess under the force of gravity rather than topple ( if it is spinning fast enough ) . this is angular momentum and torque at work in a real situation .
there is a youtube video that visualizes the air flow around a propeller for various configurations . i caught a screen shot of a moment that more or less shows what is going on : as you can see , this happens at 2:07 into the clip - this happens to be for a dual rotor configuration ( two counter rotating blades ) but the principle is the same . behind the rotor ( above , in this picture ) the air is moving slowly . air over a wide range of area is drifting towards the rotor , where it is accelerated . i will leave it up to others to describe the mathematics behind this contraction - but i thought visualizing the flow would at least confirm your observation that it is indeed slower behind the fan , and faster in front of it . in other words - it pushes , but does not suck . a better image showing the flow lines around the propeller is given at this article about the mechanics of propellers as the pressure is increased , the flow velocity goes up and the flow lines end up closer together ( because of conservation of mass flow ) . this gives the flow the asymmetry you observed . but it is still more intuitive than rigorous . . . afterthought hot licks made an excellent observation in a comment that i would like to expand on . the air being drawn towards the fan is moving in the pressure differential between the atmosphere at rest , and the lower pressure right in front of the fan blades . the pressure gradient is quite small , so the air cannot flow very fast - and it has to be drawn from a wide region to supply the mass flow . after impact with the blade ( or at least after " interacting " with the blade ) , the air has a lot more momentum that is directed along the axis of the fan ( with a bit of swirl . . . ) . this higher momentum gives the air downstream of the fan its coherence as can be seen in the diagram .
i believe the explanation can be found in manual of harmonic analysis and prediction of tides : in deriving mathematical expressions for the tide-producing forces of the moon and sun , the principal factors to be taken into consideration are the rotation of the earth , the revolution of the moon around the earth , the revolution of the earth around the sun , the inclination of the moon 's orbit to the earth 's equator , and the obliquity of the ecliptic . the key here is the fact that the earth 's axis is at an angle relative to the plane of the sun , and that in general the moon will not be in the same plane . thus , there are two sets of bulges - but they will be not be symmetrical with respect to the equator . what you are seeing then is the fact that a typical point on earth ( away from the equator ) will be closer to one bulge than the other . . . in this picture you can see that for a given latitude away from the equator , you will " see " more of one bulge of the tides than the other . this asymmetry is present for both the lunar and the solar tides ( although to different degree , given that the moon 's orbit is tilted differently ) . the result is a 24 hour component . this is described very well at http://oceanmotion.org/html/background/tides-types.htm - confirming that the tides become more symmetrical when the moon is over the equator , and less so when it moves towards the tropics of cancer or capricorn . quoting from that reference : different types of tides occur when the moon is either north or south of the equator . whereas semidiurnal tides are observed at the equator at all times , most locations north or south of the equator experience two unequal high tides and two unequal low tides per tidal day ; this is called a mixed tide and the difference in height between successive high ( or low ) tides is called the diurnal inequality . when the moon is above the tropic of cancer or tropic of capricorn , the diurnal inequality is at its maximum and the tides are called tropic tides . when the moon is above or nearly above the equator , the diurnal inequality is minimum and the tides are known as equatorial tides . when the moon and its associated tidal bulges are either north or south of the equator , most points at high latitudes in theory would be impacted by one tidal bulge and would experience one high tide and one low tide per tidal day . this so-called diurnal tide has a period of 24 hrs and 50 min . if you are interested in the math , you might want to spend the time decoding this program which implements the equations and shows good agreement with observations . edit i got curious and converted the code at the link above to python ( so i could run it ) . then i ran it for three different cases . the y axis is in microgals ( $1\ gal = 1\ cm/s^{2}$ - the galileo is the common unit in this field ) . the units on the x axis are hours - but the date is wrong ( i had some trouble initially adapting the code properly - i believe these plots might correspond to january 1981 , but i am not certain . the effect , however , is real . ) latitude = 0: latitude = 20: latitude = 40: it is pretty obvious that the asymmetry between the tides is a function of the latitude , just at my picture above would predict , and although there are clear discrepancies between this plot and the one in the original paper , the general shape and magnitude is the same - especially for latitude 40 ( boulder is at 40 degrees latitude ) . i think we have found the culprit . postscript i had some problems getting the output from my program to match the figure ; but i figured it out . here is the overlay of the data with the output of the program for may 2/3/5 1981 for latitude 40 , longitude 105: and here is the python code ( note - i took the basic code and adapted it as little as possible . . . this is not being offered for code review , just for reference ! )
this was beautifully answered theoretically right away at the 1989 aps session in ny , i think by koonin . theoretically , for any sort of fusion one needs to overcome the coulomb repulsion of the relevant nuclei , on the order of mev in order to allow the nuclei to get close enough for their wave functions to overlap and fuse . because of the phenomenom of quantum mechanical tunnelling , this can be reduced to tens to hundreds of kev . so temperatures of > > 10^5 k , or cold muons ( which outweigh electrons by 200x ) are required to reduce the internuclear distance ( as in muon catalyzed cold fusion , a real phenomenon ) , or some other special mechanism is required to allow this close approach . however , for any sort of chemically catalyzed fusion , i.e. via the valence electrons , to take place , the binding energy of the two h atoms to the catalyst would have to be so high , that the particular configuration of the low energy valence electrons , etc . would necessarily be entirely irrelevant to the problem , i.e. whatever their arrangement they could not possibly catalyze the fusionable nuclei to approach close enough to fuse . so no clever packing arrangement , quasiparticles , special adsorbtion , special crystal lattice structures , etc . could ever alter this conclusion . whatever was happening at such low energy scales would appear as a kind of irrelevant fluff compared to the energy scale of the internuclear distance necessary for fusion . therefore valence electron catalyzed cold fusion would violate the fundamental laws of quantum mechanics , nuclear physics , etc . leggett and baym also published an argument like this around the same time ( summarized for free here ) . koonin and nauenberg published an accurate calculation here , showing that if the mass of the electron were 5-10 times larger than it really is , chemically calalyzed fusion could work . note however , that the reaction rate depends on the electron mass very , very strongly , so that this remains impossible in our universe .
we could easily prove that more than four dimensions exist simply by observing a fifth dimension , but proving that only four dimensions exist is much harder and probably impossible . this is an example of the general case that proving something does not exist is usually impossible outside the halls of mathematics . i would not stake my life on no proof being possible . it is conceivable that someone might come up with a potentially observable feature of the universe that could not exist if there were more than four dimensions . however any such argument would almost certainly require assumptions that could be challenged by the multidimensionalists . the blog post you link does not prove anything . it argues that the observable universe is compatible with there being four dimensions , so extra dimensions are not necessary , but it gives no proof that extra dimensions can not exist . some footnotes : as far as i know there are no real ( as opposed to mathematical ) objects with 1 or 2 spatial dimensions . everything has three spatial dimensions . time is not the fourth dimension , or at least not for us physicists because we usually make time the first dimension i.e. we write spacetime coordinates as $ ( t , x , y , z , . . . ) $ .
the thing you throw in the air is also traveling at the same speed you are , in the same direction . when you throw it up , it does not matter that the earth below is moving backwards at speed , nor that the moon is moving past even more quickly , nor that the earth itself is spinning and moving relative to the sun . the ball has a speed and direction and currently that matches your speed and direction . when you throw the ball up , you have added force in a new direction , which alters its speed and direction , but only with respect to your speed and direction . in other words , to you the ball appear to go up and down , but to the earth it is falling like a projectile - forward up and down . since you are traveling forward at the same speed as the projectile , it appears to you that it only goes up , then down , even though during that time you both moved forward . i am not actually going to break out the math , but here 's the short version : you and the object are moving at a speed and in a direction that we will call vector p and b , respectively . currently your two vectors match . relative to some other reference frame you are both moving , but relative to you , since your vectors match , the object appears to be motionless . you apply a force on vector b , which alters its trajectory . now this force results in additional speed and direction described by vector t . the object , therefore , is now moving according to the vector b + t . however , again , since b = p , it appears to you that the object is only moving according to vector t . gravity is applying a force to the object , which will eventually reverse t in the down direction , unless the ball is acted upon by another force , such as your hand catching the ball again . so regardless of what vector you apply to it , it will be in addition to the vector you are already traveling at , and therefore it will appear to you as though it is only traveling along its new vector .
in newton 's law of cooling , the constant $k$ appears in most solutions schematically as , $^\dagger$ $$t ( t ) \sim e^{-kt}$$ clearly , the argument of the exponential must be dimensionless , hence the constant $k$ has dimensions , $$ [ k ] =\frac{1}{ [ \mathrm{time} ] }$$ the constant $k$ is a measure of the rate of cooling , with the same dimensions as a frequency . $\dagger$ we have assumed the simplest formulation of newton 's law of cooling , wherein the system of differential equations are linear , with constant coefficients .
i cannot resist this mother goose quote : what are little boys made of ? what are little boys made of ? frogs and snails , and puppy-dogs ' tails ; that is what little boys are made of . what are little girls made of ? what are little girls made of ? sugar and spice , and all that is nice ; that is what little girls are made of . you state : so atoms are formed from protons and neutrons , which are formed from quarks . and ask : but where do these quarks come from ? what makes them ? how do we know atoms are formed from protons and neutrons ? we have deep inelastic scatterings which showed that the atoms have a hard core , so they are not a uniformly distributed matter . then we have the periodic table of elements which organizes itself well counting protons and neutrons . how do we know that protons and neutrons are formed from quarks ? we have the results from painstaking experiments that showed us once more that deep inelastic scattering shows a hard core inside the protons and neutrons . the study of the interaction products organized the particles and resonances into what is now called the standard model , a grouping in families that have a one to one correspondence with the hypothesis that the hadrons ( protons neutrons resonances ) are composed out of quarks . but not only . they also have gluons which hold the quarks together due to the strong interaction , and the gluons have been seen experimentally , again with scattering experiments . this is where we are now . the lhc is scattering protons on protons , i.e. quarks on quarks at much higher energies then ever before , and we are waiting for results . the theoretical interpretation called the standard model , so successful at lower energies presupposes that the quarks are elementary . due to the gluon exchanges it is hard to see how a hard core might appear in quark quark scattering to take the onion one level lower , i.e. tell us that the quarks have a core . even in neutrino quark scattering the gluons will interfere , if the sm theory is correct at high energies . at the moment there is no experimental indication that the quarks are not elementary . nature though has surprised us before , and might do it again , once high energy lepton quark scattering experiments are designed and carried out in the future . feynman i think had said : " to see what a watch is made of one does not throw one watch on another watch and count the gears flying off . one takes a screw driver " . leptons with their weak interactions are the equivalent of the screw driver .
inglis used complex potential functions to extend kirsch 's work to treat the stress field around a plate containing an elliptical rather than circular hole http://www.saylor.org/site/wp-content/uploads/2012/09/me1023.2.1.pdf
newton 's third law , states that ' to every action there is always an equal and opposite reaction : or the forces of two bodies on each other are always equal and are directed in opposite directions " , which is an assertion of the symmetry of interaction . in principle , changing any one of newton 's laws does not necessarily ' contradict ' the other two laws , since they are independent postulates - there is no necessity of logic which links them all . see : logical connection of newton&#39 ; s third law to the first two however , one must be careful when changing a ' fundamental law ' as any of newton 's laws , since they are so intimately intertwined . newton 's three laws of motion generally go ' hand-in-hand ' and are fundamentally taken as premises in any argument about the interaction of matter in classical mechanics . for an interesting discussion of the nature and implications of newton 's 3rd law , see : deriving newton&#39 ; s third law from homogeneity of space and at : violation of newton&#39 ; s 3rd law and momentum conservation
the supspaces $v_n = span \{ ( a_1^{\dagger} ) ^{n_1} , . . . ( a_d^{\dagger} ) ^{n_d} |0&gt ; \}$ , $n_i \ge 0$ , $ n_1 + . . . n_d = n$ , constitute of invariant subspaces of the operator $ s s^{\dagger}$ action . the dimension of $v_n$ is $ \frac{ ( d+n-1 ) ! }{ ( d-1 ) ! n ! }$ . thus the operator can be represented on each of these subspaces as a square matrix of size $ \frac{ ( d+n-1 ) ! }{ ( d-1 ) ! n ! }$ for which the spectrum can be found by elementary linear algebra . the spectrum on the whole of the fock space is the union of the spectra over $v_n$ , $ n = 0 , 1 , . . . $
1 ) the word classical in this context means $\hbar=0$ . 2 ) in the context of an action principle , the euler-lagrange equations $$ \frac{\delta s}{\delta\phi^{\alpha}}~\approx~0 $$ are often referred to as the ( classical ) equations of motion ( eom ) , cf . comment by jia yiyang . here the $\approx$ symbol means equality modulo eom . let on-shell ( off-shell ) refer to whether eom are satisfied ( not necessarily satisfied ) , respectively . 3 ) in the context of a global continuous ( off-shell ) symmetry of an action , noether 's ( first ) theorem implies an off-shell noether identity $$d_{\mu} j^{\mu} ~\equiv~ - \frac{\delta s}{\delta\phi^{\alpha}} y_0^{\alpha} , $$ where $j^{\mu}$ is the full noether current , and $y_0^{\alpha}$ is a ( vertical ) symmetry generator . this leads to an on-shell conservation law $$d_{\mu} j^{\mu}~\approx~0 . $$
i just discovered this very interesting website through prof wen 's homepage . thanks prof wen for the very interesting question . here is my tentative " answer": the spontaneous symmetry breaking in the ground state of a quantum system can be defined as the long range entanglement between any two far-separated points in this system , in any ground state that preserves the global symmetries of the system . to be more precise , denote $g$ as the symmetry group of the system and $|\psi\rangle$ a ground state that carries a 1d representation of $g$ . for an ising ferromagnet , the ground state will be $|\psi_\pm\rangle =\frac{1}{\sqrt{2}}\left ( |\text{all up}\rangle \pm |\text{all down}\rangle\right ) $ . then consider two points 1 and 2 separated by distance $r$ in the space , and two small balls around points 1 and 2 with radius $r\ll r$ , denoted by $b_1$ and $b_2$ . define $\rho_1$ , $\rho_2$ and $\rho_{12}$ as the reduced density matrices of the region $b_1$ , $b_2$ and $b_1+b_2$ , and correspondingly the entropy $s_{1}=-tr ( \rho_1\log \rho_1 ) $ ( and similarly for $2$ and $12$ ) . the mutual information between the two regions is defined as $i_{12}=s_1+s_2-s_{12}$ . if $i_{12}&gt ; 0$ in the $r\rightarrow \infty$ limit for all symmetric ground states , the system is considered as in a spontaneous symmetry breaking state . in the example of ising fm , $s_{12}=\log 2$ for both ground states $|\psi_\pm\rangle$ . i am afraid it is just a rephrasing of odlro but it might be an alternative way to look at spontaneous symmetry breaking .
no mirror can be perfectly reflective due to quantum tunneling so that already answers your question . but even if it could be done , you would never be able to check the situation because when you look inside , the light almost instantly leaves through the peephole . this also poses a problem for your initiation method , which john m already touched on : you need to be very , very quick to insert the light beam and remove your insertion device/hole before it can reabsorb the light . in any case , let 's consider what would happen if you had been able to pull of this insertion of a light beam . the light would definitely be inside for a short but finite period of time . the problem is that even classically perfect mirrors are impossible with current technology ; i believe the highest reflectivity that we are able to achieve is about 99.999% if the wavelength is just right . alas , even this kind of amazing reflectivity means that a light beam will lose 90% of its intensity in less than a second in a spherical container with a diameter of $1\ , \mathrm{km}$ ! you can do the maths of this yourself : use that the reflectivity $r$ is the factor by which the intensity $i$ of the light beam is reduced when it reflects off a wall . so $$i_{after} = r i_{before}$$ now give the intensity of the light an index corresponding to the number of interactions with the wall . so the original beam has intensity $i_0$ . after one reflection , it still has $i_1 = ri_0$ left . after two reflections , it has $i_2 = ri_1 = r^2i_0$ left . there is a pattern here . after $n$ reflections , it has $i_n = r^ni_0$ left . now , you want to find out for which $n$ the intensity has dropped to 10 percent of the original $i_0$ . so the question is : when is $r^n &lt ; 0.1$ ( with $n$ a natural number ) ? you can probably work that out by yourself . now consider a spherical container of diameter $1\ , \mathrm{km}$ . then the light will always have to travel $1\ , \mathrm{km}$ or less before it hits the wall again . in other words , it has to travel ( at the highest ) $n$ times $1\ , \mathrm{km}$ before it has lost 90 percent of its original intensity . using that the speed of light is approximately $300000\ , \mathrm{km/s}$ you should find that it only takes the light about $0.77\ , \mathrm{s}$ to do this .
you are allowed to treat the argument of $\tan^{-1}$ as $\infty$ at the initial point , provided of course you use the appropriate limit $\tan^{-1} ( \infty ) =\frac{\pi}{2}$ . more formally , change " evaluate the function at $s=r$ to find the constant " to " take the limit $s\rightarrow r$ to find the constant " ( which you should do since the function is indeed formally undefined ) . then the constant is $$c=\lim_{s\rightarrow r^-}\arctan\left ( \sqrt{\frac{r}{r-s}}\right ) =\frac{\pi}{2} . $$
i like your description of this cool bit of unintuitive physics . i find the best balance of $a$ to $b$ to $c$ to cost of the object involved is best for a ( boxed ) pack of playing cards . the mathematical explanation for this ( see also wikipedia ) is that when considered in the principal axis frame ( i.e. . the frame of reference that rotates with the body and whose axes are the principal axes of inertia of the body ) , the motion can be described by the angular velocity $\vec{\omega}$ and the angular momentum $\vec{l}=i\vec{\omega}$ , and in the absence of external torques it must conserve the magnitude of the angular momentum , $$l^2=l_1^2+l_2^2+l_3^2$$ ( though not its direction since the frame is noninertial ) , and the rotational energy , $$e=\frac{l_1^2}{2i_1}+\frac{l_2^2}{2i_2}+\frac{l_3^2}{2i_3} . $$ the motion is then constrained to move along the intersections of an ellipsoid and a sphere : these curves are closed ellipses , or nearly so , close to the axes with the smallest and largest moments of inertia , but they are locally hyperbolae close to the middle one . hence the instability .
i was surprised to see an effect that is actually real , and not a hidden motor or something like that . i believe this experiment can feasibly be repeated . the principles behind it make sense . the driving force comes from the density difference in the cup versus in the pipes . the cup has very few bubbles in it compared to the pipe . why ? because : the bubbles in the cup have an exit strategy - floating to the top . not so in the pipe . the pipe has greater wall area to volume ratio . so it likely has more nucleation sites for the bubbles . if i were recreating this , i would design the pipe to turn horizontal/vertical as close as possible to the bottom of the cup , in order to help maximize the driving pressure . also , make sure to use the soda/beer right after opening it . this experiment shows ( not perpetual motion ) that carbonation contains stored energy in some sense . more specifically , the process of a carbonated drink decaying into un-carbonated liquid and co2 gas liberates extra energy . a small fraction of that energy is harvested here to drive the flow . very good science project . in order to demonstrate that it is not perpetual motion , either allow it to run to its full conclusion , or try it again with soda that has sat out for a day . the data should support the hypothesis that the driving force to power the flow comes from stored energy in the carbonation .
the " trick " is that the cane he is apparently holding is actually firmly attached to the platform . a rigid piece goes up his sleave , then to a harness that holds his whole body up . for more about this type of magic trick device , google " broom suspension " or " aerial suspension harness " . no electric or magnetic fields were abused here . image credit : twentytwowords
the lorentz force $\textbf{f}=q\textbf{v}\times\textbf{b}$ never does work on the particle with charge $q$ . this is not the same thing as saying that the magnetic field never does work . the issue is that not every system can be correctly described as a single isolated point charge for example , a magnetic field does work on a dipole when the dipole 's orientation changes . a nonuniform magnetic field can also do work on a dipole . for example , suppose that an electron , with magnetic dipole moment $\textbf{m}$ oriented along the $z$ axis , is released at rest in a nonuniform magnetic field having a nonvanishing $\partial b_z/\partial z$ . then the electron feels a force $f_z=\pm |\textbf{m}| \partial b_z/\partial z$ . this force accelerates the electron from rest , giving it kinetic energy ; it does work on the electron . for more detail on this scenario , see this question . you can also have composite ( non-fundamental ) systems in which the parts interact through other types of forces . for example , when a current-carrying wire passes through a magnetic field , the field does work on the wire as a whole , but the field does not do work on the electrons . when we say " the field does work on the wire , " that statement is open to some interpretation because the wire is composite rather than fundamental . work is defined as a mechanical transfer of energy , where " mechanical " is meant to distinguish an energy transfer through a macroscopically measurable force from an energy transfer at the microscopic scale , as in heat conduction , which is not considered a form of work . in the example of the wire , any macroscopic measurement will confirm that the field makes a force on the wire , and the force has a component parallel to the motion of the wire . since work is defined operationally in purely macroscopic terms , the field is definitely doing work on the wire . however , at the microscopic scale , what is happening is that the field is exerting a force on the electrons , which the electrons then transmit through electrical forces to the bulk matter of the wire . so as viewed at the macroscopic level ( which is the level at which mechanical work is defined ) , the work is done by the magnetic field , but at the microscopic level it is done by an electrical interaction . it is a similar but more complicated situation when you use a magnet to pick up a paperclip ; the magnet does work on the paperclip in the sense that the macroscopically observable force has a component in the direction of the motion of the paperclip .
in theoretical physics , entropy is typically dimensionless . for example , instead of defining $s=k_b \log w$ , we would define $s=\log w$ . this is precisely what has been done in this equation : $\hbar \cdot c$ has units of $j \cdot m$ , which cancels the units up top . see also : http://www.scholarpedia.org/article/bekenstein_bound
the only materials that can block a magnetic field are those that strongly interact , such as a ferromagnetic material ( iron , steel , etc ) , or a superconductor . since we do not live in a sea of liquid nitrogen , for weaker fields mu-metal is best , but for stronger magnets mu metal loses it is advantage and any iron-based metal is just as good . an unobtanium that blocks magnets but is not affect by them can not exist . if it did , you could make a perpetual motion machine by letting the magnets attract ( which relases energy ) , inserting a plate of said material between them ( you would leave a small gap in between the magnets ) , and pulling the magnets back apart with negligible energy expenditure . iron will not let you do this . the magnets release energy when you let them attract , and the plate releases additional energy when you insert it . however , the presence of an attracting metal plate means it will take more energy to remove the magnets with the plate in between than without , and you end up having to put all the energy back in you got out in the first place ( nature is an accurate banker ) .
assuming the distance $d$ ( $z$ is a formal notation ) is : $$d ( \sigma , \sigma' ) = \int_\sigma^{\sigma'} dz ~~e^{w ( z ) } \tag{1}$$ we have , developping at order 2 , $w ( z ) $: $$d ( \sigma , \sigma' ) = e^{w ( \sigma ) } \int_\sigma^{\sigma'} dz ~ e^{ ( z-\sigma ) . \partial w ( \sigma ) + \frac{1}{2} ( z-\sigma ) ^a ( z-\sigma ) ^b \partial_a \partial_b w ( \sigma ) }\tag{2}$$ that is : $$d ( \sigma , \sigma' ) = e^{w ( \sigma ) } \int_\sigma^{\sigma'} dz ~ ( 1 + ( z-\sigma ) . \partial w ( \sigma ) + \frac{1}{2} ( z-\sigma ) ^a ( z-\sigma ) ^b\partial_a \partial_bw ( \sigma ) +q ( w ) ) \tag{3}$$ where $q ( w ) $ represents quadratic quantities in $\partial_a w \partial_b w$ . because the metrics is diagonal , christophel symbol are of king $\partial_x w$ , so the difference between $\nabla_x \partial_y w$ and $\partial_x \partial_y w$ are precisely these quadratic quantities . we can only choose a inertial frame , so that $\partial_x w=0$ , so we can neglect theses quantities $q$ , and working with standard derivatives ( more details at the end of the answer ) . after formal integration , we get : $$d ( \sigma , \sigma' ) = e^{w ( \sigma ) } |\sigma ' - \sigma| ( 1 + \frac{1}{2} ( \sigma'-\sigma ) . \partial w ( \sigma ) + \frac{1}{6} ( \sigma'-\sigma ) ^a ( \sigma'-\sigma ) ^b\partial_{ab}w ( \sigma ) +q ( w ) ) \tag{4}$$ reexpressing with an exponential , we have : $$d ( \sigma , \sigma' ) = e^{w ( \sigma ) } |\sigma ' - \sigma| e^{\frac{1}{2} ( \sigma'-\sigma ) . \partial w ( \sigma ) + \frac{1}{6} ( \sigma'-\sigma ) ^a ( \sigma'-\sigma ) ^b\partial_a \partial_bw ( \sigma ) +q ( w ) } \tag{5}$$ so , now , we get , with $\delta ( \sigma , \sigma' ) = \frac{\alpha'}{2} \ln d^2 ( \sigma , \sigma' ) $: $$\delta ( \sigma , \sigma' ) =\alpha' ( w ( \sigma ) + \frac{1}{2} ( \sigma'-\sigma ) . \partial w ( \sigma ) + \frac{1}{6} ( \sigma'-\sigma ) ^a ( \sigma'-\sigma ) ^b\partial_a \partial_bw ( \sigma ) +q ( w ) +f ( \sigma , \sigma' ) ) \tag{6}$$ where $f ( \sigma , \sigma' ) $ is a function of $\sigma , \sigma'$ which does not depend on $w$ , so we do not care by looking at variations of $\delta$ relatively to $w$ . choosing a inertial frame means that we do not care about the quadratic quantities $q$ too . we have an other problem , because here $\delta ( \sigma , \sigma' ) $ is not symmetric in $\sigma , \sigma'$ , so we need to symmetrise it , so finally the relevant part of $\delta$ is : $$\delta ( \sigma , \sigma' ) =\alpha' ( \frac{1}{2} [ w ( \sigma ) + w ( \sigma' ) ] + \frac{1}{4} ( \sigma'-\sigma ) . [ \partial w ( \sigma ) - \partial^{'} w ( \sigma' ) ] + \frac{1}{12} ( \sigma'-\sigma ) ^a ( \sigma'-\sigma ) ^b [ \partial_a \partial_bw ( \sigma ) +\partial^{'}_a \partial_b^{'}w ( \sigma' ) ] ) \tag{6'}$$ so , taking variations , and derive relatively to $\sigma'_b$: $$\partial^{'}_b \delta_w \delta ( \sigma , \sigma' ) =\alpha' ( \frac{1}{2}\partial^{'}_b \delta w ( \sigma' ) + \frac{1}{4} [ \partial_b w ( \sigma ) - \partial^{'}_b w ( \sigma' ) ] \\ - \frac{1}{4} ( \sigma ' - \sigma ) ^a \partial'_a \partial_b'w ( \sigma' ) + \frac{1}{6} ( \sigma ' - \sigma ) ^a [ \partial_a \partial_bw ( \sigma ) +\partial^{'}_a \partial_b^{'}w ( \sigma' ) ] + o ( \sigma - \sigma' ) ^2\tag{7}$$ then , we derive relatively to $\sigma_a$: $$\partial_a \partial^{'}_b \delta_w \delta ( \sigma , \sigma' ) =\alpha' ( \frac{1}{4} [ \partial_a \partial_b \delta w ( \sigma ) + \partial^{'}_a \partial^{'}_b \delta w ( \sigma' ) ] - \frac{1}{6} [ \partial_a \partial_b \delta w ( \sigma ) + \partial^{'}_a \partial^{'}_b \delta w ( \sigma' ) ] + o ( \sigma - \sigma' ) \tag{8}$$ so , finally , when $\sigma ' \rightarrow \sigma$ , we have : $$\partial_a \partial^{'}_b \delta_w \delta ( \sigma , \sigma' ) = \alpha ' \frac{1}{6}\partial_a \partial_b\delta w ( \sigma ) \tag{9}$$ this is precisely the expression ( 3.6.15b ) , remembering that we calculate in a intertial frame , so $\nabla_x \partial_y w = \partial_x \partial_y w $ the same method , beginning from $ ( 6' ) $ and applying two derivates $\partial_a \partial_b$ gives $3.16.15c$ ( a $\frac{1}{3}$ term ) [ remark ] the quadratic quantities $q$ could be written : $$ ( \sigma'-\sigma ) ^a ( \sigma'-\sigma ) ^b ( \partial_a w ) ( \partial_b w ) $$ when you derive 2 times this expression , you get : $$ o ( \sigma'-\sigma ) + o ( \partial_a w ) $$ so , when $\sigma ' \rightarrow \sigma$ and when we are in a inertial frame ( $\partial_a w=0$ ) , these quantities are not relevant .
you are right - for isolated galaxies , there is no obvious way of discerning whether they are made of matter or antimatter , since we only observe the light from them . but if there are regions of matter and antimatter in the universe , we would expect to see huge amounts of radiation from annihilation at the edges of these regions . but we do not . you could also make the case that galaxies are well-separated in space , and there is not much interaction between them . but there are plenty of observed galaxy collisions even in our own small region of the universe , and even annihilation between dust and antidust in the intergalactic medium would ( probably ) be observable .
in general yes , everything rotates . it is to do with something called angular moment . gravity is the central force in the universe , because it is the only one which has a significant pull over large distances . when things collapse under their own gravity in space ( i.e. . clouds of gas and dust ) , any small amount of asymmetry in the collapse will be enough start it spinning . even if it spins by a tiny amount , as it collapses , angular momentum conservation will mean it spins more and more quickly - just like an spinning ice-skater pulling their arms into their body and spinning more quickly . this means that all coherent masses are spinning - e.g. asteroids , neutron stars , galaxies , quasars . the universe is a complex place so something may be slowing down ( because the gravity of other objects is putting on the brakes ) or some things may appear not to be rotating ( e . g . the moon rotates but at the same rate as it goes around the earth ) . huge clouds of gas and dust tend not to be spinning as a whole because they are expanding to fill the available volume - like a bad smell in room ! - and not necessarily gravitational bound together . however they might have little pockets which start are turbulent , collapse under their own gravity , spin and form stars .
in the unbroken phase of the standard model , all particles are massless . thus , they can only move with the speed of light , in order to fulfill the mass-energy-relation $$ e^2 = \vec p^2 c^2 + m^2 c^4 = \vec p^2 c^2 \big|_{m = 0} $$ this is true for photons and gluons in our broken world , but would be true for $w^\pm , z^0$ ( or more correctly for the $w^{i} , b$ bosons of $su ( 2 ) \times u ( 1 ) $ as there would be no need to form linear combinations of them ) and for all fermions as well .
dark matter candidates have to interact very weakly with the particles of the standard model in order to have a relic density compatible with the one measured by the plank satellite . the higgs boson cannot be dark matter , because the decay rate for a process like $h\to f\bar{f}$ is very high for a mass around $m_h=126 ~\rm{gev}$ . however , there are still some very interesting possibilities concerning scalar particles . if we want to have the correct relic density without considering extremely heavy dark matter particles , then we have to suppose the existence of a mediator that makes the connections between the standard model and the " dark sector " . two possibilities are : a very light vector boson , so-called " dark photon " ( 0607094 ) , a light pseudoscalar boson ( 0712.0016 ) . in particular , in most extension of the sm there are several " higgs bosons " and maybe one of these particles can be such a mediator . two particular examples are the minimal supersymmetric standard model ( mssm ) and the next-to-minimal supersymmetric standard model ( nmssm ) . in the latter case , it is possible to have a very light cp-odd particle ( pseudoscalar ) in addition to the higgs boson observed at the lhc . usually the latter is identified with the lightest cp-even boson of these models ( cf . 1301.1325 ) . in conclusion , we know very few about dark matter and its interaction , but the possibility of having a new higgs boson that could explain the experimental results like the cmb measurements is not completely ruled out .
this does not seem likely for several reasons . most hospital waste is considered low level waste ( llw ) and not very hazardous and would make for a anticlimactic dirty bomb . in fact llw is usually just incinerated or buried with only minimum precaution . let 's assume they got a hold of something that was not llw waste and was a high level waste ( hlw ) item , like in goiania . something that would actually work in dirty bomb would have to be stored in a shielded container , thus you would not need bananas anyway as bananas radiate very little in comparison to almost everything else ( see below ) . but if the crooks were as ignorant about radiation as the scrap metal recyclers in goiania who spread the glowing powder and dust around and even let a kid rub it on her skin for effect , then perhaps they took it out of the container to transport it . at this point the exposure levels would be so high that bananas would not even compare . so other than providing a snack in between bouts of vomiting from radiation sickness i do not see what bananas would do to mask it . banana radiation does make a nice little plot trivia that helps the story seem more realistic . let 's compare some numbers in banana terms ( bed banana equivalent dose ) : 80 million bed = fatal dose even with treatment 20 million bed = severe radiation poisoning , fatal in some cases 500,000 bed = maximum legal yearly dose for a us radiation worker 40,000 bed = ten years of normal background dose , 85% of which is from natural sources 4000 bed = 1 mammogram 1000 bed = approximate total dose received at fukushima town hall in two weeks following accident 400 bed = 1 flight from london to new york 300 bed = yearly release target for a nuclear power plant 200 bed = chest x-ray 50 bed = dental x-ray 1 bed = eating a banana ( 0.1 sv = 0.0001 msv dosage ) that is a lot of bananas to get close to a dosage that would be fatal , so it would take a lot to cover up an unshielded radiation source . in fact our bodies carry around more bed doses than a banana . we surrounded by naturally-radioactive materials , and are constantly bathed in radiation originating in the rocks and soil , building materials , the sky ( space ) , food and one another . a typical background level of exposure is 2-3 millisieverts per year ( msv/y ) or 20000 to 30000 bed / year . bananas probably would not provide a lot of cover . after all do not they scan a lot of organic products and know how much to expect ? so anything additional would be suspect and hlw would stand out unless it was so well shielded that there would be no point for bananas anyway .
what you think as a particle , the electron for example , is a quantum mechanical entity that behaves as a classical billiard ball in some experiments but collectively displays behaviors that cannot be explained by classical mechanics , one of them is to display a wave nature , i.e. interference phenomena , when studied appropriately . i will repeat some paragraphs from a previous answer . in the quantum mechanical framework , single events/instances can be described by classical trajectories and physics . it is when the statistics are accumulated that the wave behavior appears . the statistical distribution of such scatterings will be a probability distribution given by the quantum mechanical wave equations , and will display the wave nature of the underlying framework . the waves in quantum mechanics are probability waves . many instances must be accumulated in a distribution to manifest the wave nature . in the double slit experiment with single electrons a single electron does not express any wave nature . one can calculate its trajectory classically after the fact . one cannot predict the trajectory unless the probability wave nature of the underlying framework is taken into account . so in the double slit experiment the individual electron appears as a dot on the screen but its trajectory cannot be predicted by classical mechanics , by knowing the momenta and geometries . to summarize for individual measurements the wave nature may not appear at all or cannot be predictive of a trajectory . what the wave nature does is predict a statistical distribution for the particles under consideration . in the double slit experiment with individual electrons the accumulation of single " particle " hits displays an interference pattern , and this is the particle/wave duality . you ask : but does not that just mean that this particle is a particle the whole time , but you can not know it is location without observing it and the wave just presents a range of possible locations where it might be if you do observe it ? in fact you do not know what it is until you observe it , and if it is one event , you can only pull your beard and wonder at its trajectory as a single particle . by accumulating events you observe the wave nature in the distribution . this insight allowed to describe mathematically the microcosm with the quantum mechanical differential equations , which are wave equations , and a system of postulates , the main one " the square of the solution is the probability distribution " . the description has been validated by innumerable experiments .
not sure i understand your question . let me know if the answer does not make sense . when a accelerometer is in a given orientation to the direction of gravity , then the value measured on each axis is the dot product of the gravity vector and the orientation vector of the sensor . imagine that you start with the sensor in the " normal " orientation : x points in ( 1 0 0 ) , y points in ( 0 1 0 ) and z points in ( 0 0 1 ) . the acceleration of gravity points along the z axis : ( 0 0 -g ) . you can see that the dot product of the x and y vectors with the gravity vector are zero - and the z sensor reads $-g$ . now rotate the sensor to an arbitrary orientation . if you rotated 45 degrees about the x axis , your sensor axes would point : X = (1 0 0) Y = (0 r -r) Z = (0 r r)  where $r=\sqrt{2}$ . and now you would see a gravity value on both the y and z axes of the sensor . the total gravity is of course the vector sum of the components in x , y , z . the above should allow you to subtract the gravity vector from a reading , so you can obtain the " other " acceleration of the system . let me know if this is clear enough .
electric current is the rate of flow of electric charges across any cross-sectional area of a conductor . the direction of electric current is taken as the direction of flow of positive ions or opposite to the direction of flow of free electrons . your assumption is not necessary here . . . electrons always flow from negative terminal to positive terminal . $$i=\frac{dq}{dt}$$ when current flows through an electrolytic solution or during the process of electrolysis , the plate towards which positive ions ( cations ) flow is called the cathode and the plate towards which negative ions ( anions ) flow is called the anode . wikipedia says clearly , in an electrochemical cell , the electrode at which electrons leave the cell and oxidation occurs is called anode and the electrode at which electrons enter the cell and reduction occurs is called cathode . each electrode may become either the anode or the cathode depending on the direction of current through the cell . a bipolar electrode is an electrode that functions as the anode of one cell and the cathode of another cell . so , the convention is totally based on our definition of the direction of current flow that it always flows opposite to the direction of electrons ( i.e. ) electrons can be called as cations or anions depending on the usage . and based on this , we dump our thought that cathode should always be negative , etc . . .
i am not entirely sure that i have understood what you are referring to , but it sounds like the following . to figure out the allowed representations for a massive particle in $d$-dimensional space-time dimensions , we can boost to its rest frame . then we have the remaining $so ( d-1 ) $ rotations that leave us in the rest frame of the particle , and so its the respresentation of $so ( d-1 ) $ that determine the properties of the particle . so in $d = 4$ our massive particles are labelled by representations of $so ( 3 ) $ , which is good old angular momentum . on the other hand if we have a massless particle , we can not boost to its rest frame since it does not have a rest frame . so what we have to look at instead are the transformations that leaves it is direction fixed , that is the transformations that fix a null ray . this is $so ( d-2 ) $ ( if we ignore the fact that one of the directions is not compact , i think its really $so ( d-3,1 ) $ or whatever it is called ) , and so it is the representations of $so ( d-2 ) $ that determine the states of massless particle . in $d = 4$ we have that our massless particles are labeled by representations of $so ( 2 ) $ . so when we say the photon has angular momentum 1 we are not talking representation of $so ( 3 ) $ . it does not have a state with $m = 0$ - a longitudinal polarization - even though the $l=1$ represenation of $so ( 3 ) $ has three states $m=-1,0,1$ . i hope this is what you were looking for .
this diagram shows the earth rotating round the sun at it is orbital velocity $v$ . that is the centre of the earth is orbiting around the sun at velocity $v$ . nb the scale is rather fanciful - do not take it literally ! i will also assume the orbit is circular , and for convenience i will ignore the earth 's rotation i.e. assume it is tidally locked . to calculate the orbital velocity at the centre of the earth , $v$ , we just note that the centripetal acceleration must be the same as the gravitational acceleration of the sun so : $$ \frac{v^2}{r} = \frac{gm}{r^2} $$ which gives : $$ v^2 = \frac{gm}{r} \tag{1} $$ which is a well known result . now consider the point on the earth 's surface nearest the sun i.e. the black dot . the acceleration due to earth 's gravity is the usual $9.81 m/s^2$ , but there will be a correction due to the fact the point is $r_e$ metres nearer the sun . let 's calculate that correction . the gravitational acceleration due to the sun at the black dot is : $$ a_g = \frac{gm}{ ( r - r_e ) ^2} $$ the centripetal acceleration due to the motion of the point around the sun is : $$ a_c = \frac{v^2}{r - r_e} $$ where because i have assumed the earth is tidally locked the velocity $v$ is just the earth 's orbital velocity given by equation ( 1 ) . if we substitute for this we get : $$ a_c = \frac{gm}{r ( r - r_e ) } $$ so the correction to the acceleration at the black dot is : $$\begin{align} \delta a and = a_g - a_c \\ and = \frac{gm}{ ( r - r_e ) ^2} - \frac{gm}{r ( r - r_e ) } \\ and = gm \left ( \frac{r_e}{r ( r - r_e ) ^2} \right ) \\ and \approx gm \frac{r_e}{r^3} \end{align}$$ where the last approximation is because $r \gg r_e$ so $r - r_e \approx r$ . putting in the numbers we get : $$ \delta a \approx 2.5 \times 10^{-7} m/s^2 $$ so the fractional change in the weight of an object due to the sun is : $$ \frac{2.5 \times 10^{-7}}{g} \approx 2.6 \times 10^{-8} $$ and the object is 0.0000026% lighter . interestingly if you go through the working for the far side of the earth you get exactly the same result i.e. the object on the far side is also 0.0000026% lighter . in fact this is why the tidal forces of the sun ( and moon of course ) raise a bulge on both the near and far sides of the earth . incidentally , i note that christoph guesstimated a correction of $10^{-7}$ and he was pretty close :- )
both are same . $a^{-1}=\frac{1}{a}$
in thermodynamics , the early 19th century science about heat as a " macroscopic entity " , the second law of thermodynamics was an axiom , a principle that could not be derived from anything deeper . instead , physicists used it as a basic assumption to derive many other things about the thermal phenomena . the axiom was assumed to hold exactly . in the late 19th century , people realized that thermal phenomena are due to the motion of atoms and the amount of chaos in that motion . laws of thermodynamics could suddenly be derived from microscopic considerations . the second law of thermodynamics then holds " almost at all times " , statistically  it does not hold strictly because the entropy may temporarily drop by a small amount . it is unlikely for entropy to drop by too much ; the process ' likelihood goes like $\exp ( \delta s/k_b ) $ , $\delta s \lt 0$ . so for macroscopic decreases of the entropy , you may prove that they are " virtually impossible " . the mathematical proof of the second law of thermodynamics within the axiomatic system of statistical physics is known as the boltzmann 's h-theorem or its variations of various kinds . yes , if you will simulate ( let us assume you are talking about classical , deterministic physics ) many atoms and their positions , you will see that they are evolving into the increasingly disordered states so that the entropy is increasing at almost all times ( unless you extremely finely adjust the initial state  unless you maliciously calculate the very special initial state for which the entropy will happen to decrease , but these states are extremely rare and they do not differ from the majority in any other way than just by the fact that they happen to evolve into lower-entropy states ) .
the standard answer on earth is that meteors are seen only when meteoroids enter the earth 's atmosphere and heat it to incandescence , while comets are bodies in interplanetary space , visible from anywhere in the solar system . on the moon , there is no significant atmosphere , so there are no meteors as such . however , meteors have been observed ( from earth ) on several occasions impacting the surface of the moon , so in fact meteors have been seen on the moon . http://science.nasa.gov/science-news/science-at-nasa/2001/ast30nov_1/
$g^{\alpha\beta}$ is symmetric in $\alpha$ and $\beta$ , while $r_{\alpha\beta\gamma\mu}$ is anti-symmetric in $\alpha$ and $\beta$ , so the contraction $g^{\alpha\beta}r_{\alpha\beta\gamma\mu}$ is necessarily $0$ , and cannot be $r_{\gamma\mu}$ . moreover , it is not correct to say , that if the contraction of $2$ tensors with another tensor ( here the metric tensor ) are equals , then the $2$ tensors are equal . for instance , if you take a tensor $t_1$ , and a tensor $t_2$ , with $t_2-t_1$ anti-symmetric in lower indices $\alpha , \beta$ and contract the tensors $t_1$ and $t_2$ with a tensor symmetric in upper indices $\alpha , \beta$ , you will get the same result .
your question is hypothetical since the universe is almost certainly not closed and will not recollapse ( pace andrei linde ) . however if we assume an flrw metric and a closed universe then all comoving observers will agree on the time between the big bang and the big crunch so the twins will have aged the same amount . if you have in mind a different scenario you will have to be clearer what you want . for example the hubble expansion of space will not separate the twins if they start at the same spacetime point ( unless that point is the big bang ) . also note that if you intend some artifical expansion caused by moving masses around then the twins will almost certainly have aged different amounts even when they are moving freely . this is because in gr multiple geodesics between two spacetime points can have different elapsed proper times .
i think you are confusion is with notation since unfortunately , two notation often used to denote projected spinors . one notation is to write : \begin{equation} \psi \equiv \left ( \begin{array}{c} \psi _l \\ \psi _r \end{array} \right ) \end{equation} in this notation $ \psi _l $ and $ \psi _r $ are two component weyl spinors . however , a second notation is also used where \begin{equation} \psi _l \equiv p _l \psi , \quad \psi _r \equiv p _r \psi \end{equation} and $ p _{l/r } $ are the projection operators . now $ \psi _l $ and $ \psi _r $ are four component spinors with a zero value for two the components . in the first notation ( where $ \psi _{ l/ r } $ are weyl spinors ) the dirac term takes the form , \begin{equation} m \bar{\psi} \psi = m \left ( \psi _r ^\dagger \psi _l + h.c. \right ) \end{equation} and in the second ( where $ \psi _{ l/r} $ are four component objects ) it takes the form , \begin{equation} m \bar{\psi} \psi = m \left ( \overline{ \psi _r} \psi _l + h.c. \right ) \end{equation} its just a matter of notation the end result is the same .
tylerhg : yes it is easy to calculate the density of states . but what i am really asking here is " why . " note that a thin circular ring in $\mathbf{k}$-space of thickness $dk$ has area $da=2\pi k\ , dk$ ( by elementary geometry ) . in $e$-space , since $e\propto k^2$ , that ring corresponds to a patch of width $de=2k\ , dk$ . thus $$\frac{da}{de}=\pi . $$ but $a\propto n$ , so the density of states is constant .
two comments : first , you see a mirror surface on a lake at shallow angles , true , but only because there is light in the atmosphere ( $ n_1$ in your equations ) . for your problem , assume there is no such light source , so only light emanating from the water ( $ n_2$ ) is of concern . next , essentially you are correct that the limiting ray angle exiting the water ( snell window ) can be as close to horizontal as you desire . the problem as stated is not terribly useful unless you also calculate the relative intensity as a function of viewing angle ( or , back to your comment about mirror-surface , the relative intensity of the emanating light as compared to the reflected light from the atmosphere ) .
first of all , the procedure is called dimensional regularization , not dimensional renormalization . regularization is the process by which we make sums and integrals non-singular so that their results are not infinite  the result " infinity " carries no meaningful physical information because the results of measurements in physics are always particular finite numbers . after the regularization , the results of integrals are manageable finite expressions although they may still diverge in the limits we call " physical " . renormalization is another step in which we carefully distinguish bare values of parameters ( in the action ) and the observed values , making sure that the theory with the appropriate values of the parameters agrees with the observations . renormalization is something we would have to do even if the underlying integrals were convergent . it usually follows a regularization procedure but is independent of it . dimensional regularization is just a methodology to evaluate particular integrals  not mentioning what physical quantities or parameters are expressed by these integrals  so it is clearly a regularization technique , not renormalization technique . a broader technique to see these integrals in the loop diagrams and give them the right interpretations for the amplitudes may lead to $\overline{ms}$ , em-es-bar , which is a renormalization scheme ( a renormalization scheme is given by the choice of the renormalization scale as well as the exact definition of physically measurable quantities , i.e. scattering amplitudes of particles with certain energies , that play the role of the coupling constants for taylor expansions etc . ) . but dim . reg . itself is just a regularization technique . now , the new parameter $\lambda$ in dim . reg . is auxiliary , newly added , so we finally expect or want it to drop out of the physical expressions . indeed , $\lambda$ of dim . reg . also drops out of the final physical expressions although it is only after the full calculation of the physics quantities including the renormalization . in particular , various quantities linked to certain scales may depend on $\lambda$ , or the ratio $\mu/\lambda$ involving a new auxiliary scale , but the observed/predicted cross sections are linked to other observed cross sections etc . by formulae that contain neither $\lambda$ nor $\mu$ . scale invariance below equation 7 of the paper you mentioned , they make it rather clear what they mean by the scale invariance . they mean that the expression , the integral in equation 7 , is formally scale-invariant under $k\to kx$ . if we just rescale $y\to ky$ as well , the integration variable , the factors of $k$ cancel between $dy$ and $1/\sqrt{x^2+y^2}$ . the integral itself is divergent but if we were satisfied with this unphysical answer $\infty$ , it would be ok for the scale invariance because $\phi ( x ) =\phi ( kx ) =\infty$ . as they make it clear between equations 7 and 12 , this scale invariance is subtle for divergent integrals because while we may say that $\infty=\infty$ , it is still true that $\infty-\infty$ which appears in physically important quantities ( work that is done ) is an indeterminate form whose value may be any finite ( or infinite ) number . in particular , if you rescale $x , y$ by $k$ , the $\overline{ms}$ renormalized expression changes additively by $ ( \lambda/4\pi\epsilon_0 ) \ln k$ with some sign . it is a purely additive shift that is independent of $x , y , z$ and such an additive shift may be undone by a simple $u ( 1 ) $ gauge transformation whose gauge parameter is something like $ ( \lambda/4\pi\epsilon_0 ) \ln ( k ) \cdot t$ i.e. linear in time ( because we want to eliminate the temporal $a_0$ component ) . so the renormalized value of the potential is not quite scale-invariant because , as you correctly said  and it is easy to verify it by looking at the actual logarithmic expression for the potential  the value of $\lambda$ would have to be rescaled as well . but if $\lambda$ is not rescaled , the change of the potential under $\vec x\to k\vec x$ is just a simple constant additive shift that is equivalent to a gauge transformation so it is still true that all gauge-invariant quantities that may be calculated out of such a potential ( and physical , measurable , observable quantities have to be gauge-invariant ) are scale-invariant . more precisely , they are " covariant " and get rescaled by the right power $k^\delta$ where $\delta$ refers to their dimension . that is why the electric field goes like $1/x$ etc . for this reason , one may use a bit sloppy language and say that the potential itself is scale-invariant . but the extent to which this statement is true for the potential in various forms  formal expression for the integral , the naive result of the integral , or the result in a renormalization scheme  depends on the details in the way sketched above . on the other hand , as you wrote , the translational invariance in the direction along the wire is uncontroversial , manifest , and protected by all forms of the potential or the field strength .
minkowski space is a real affine space of dimension $4$ whose space of translations is equipped with a metric of lorentzian type . a ( real ) affine space is a triple $ ( \mathbb a , v , \vec{} ) $ , where $\mathbb a$ is a set whose elements are said points , $v$ is a ( real ) vector space and $\vec{}$ is a map $\vec{} : \mathbb a \times \mathbb a \to v$ with the following properties , $$\forall q \in \mathbb a\: , \forall v \in v\: , \exists \mbox{ and is unique } p \in \mathbb a\quad \mbox{such that}\quad \vec{qp} = v\: , \tag{1}$$ $$\vec{pq} + \vec{qr} = \vec{pr}\quad \forall p , q , r \in \mathbb a\: . \tag{2}$$ by definition , the dimension of the affine space is that of $v$ , whose elements are said translations . from now on , if $p , q\in \mathbb a$ a $v \in v$ , $$p= q+ v$$ means $$\vec{qp}=v\: . $$ form ( 1 ) this notation is well posed . $q+v$ is the action of the translation $v$ on the point $q$ . this action is transitive and free , its existence physically corresponds to homogeneity of both space and time in special relativity . assuming that $v$ is finite dimensional , if one fixes $o \in \mathbb a$ and a basis $e_1 , \ldots , e_n \in v$ , a cartesian coordinate system on the affine space $\mathbb a$ with origin $o$ and axes $e_1 , \ldots , e_n$ is the bijective map $$\mathbb r^n \ni ( x^1 , \ldots , x^n ) \mapsto o + \sum_{j=1}^n x^je_j \in \mathbb a$$ once again , using ( 1 ) one sees that , in fact , the map above is bijectve and thus identifies $\mathbb a$ with $\mathbb r^n$ . changing $o$ to $o'$ and the basis $e_1 , \ldots , e_n$ to the basis $e'_1 , \ldots , e_n'$ , one obtains a different cartesian coordinate system $x'^1 , \ldots , x'^n$ . it is simply proved that the rule to pass form the latter coordinate system to the former has the form $$x'^a = c^a+ \sum_{j=1}^n {a^a}_j x^j \tag{3}$$ for $n$ constant coefficients $c^j$ and a nonsingular $n\times n$ matrix of coefficients ${a^a}_j$ . the said matrix verifies $$e_k = \sum_{i=1}^n {a^i}_k e'_i\tag{3'}$$ whereas the coefficients $c^k$ are the components of the vector $\vec{oo'}$ . ( as a matter of fact the affine structure gives rise to a natural differentiable real analytic structure on $\mathbb a$ of dimension $n$ . ) a real affine space equipped with a ( pseudo ) scalar product in $v$ is called ( pseudo ) euclidean space . minkowski spacetime $\mathbb m^4$ is a ( real ) four dimensional affine space equipped with a pseudo scalar product $g : v\times v \to \mathbb r$ of lorentzian type . " of lorentzian type " means that there exist bases , $e_0 , e_1 , e_2 , e_3$ , in $v$ such that ( i adopt here the convention $-+++$ ) $$g ( e_0 , e_0 ) =-1 \: , \quad g ( e_i , e_i ) = 1 \mbox{ if $i=1,2,3$}\: , \quad g ( e_i , e_j ) =0 \mbox{ if $i\neq j$ . }\tag{4}$$ these bases are called minkowskian bases . lorentz group $o ( 1,3 ) $ is nothing but the group of matrices $\lambda$ connecting pairs of minkowskian bases . it is therefore defined by $$o ( 1,3 ) := \left\{ \lambda \in m ( 4 , \mathbb r ) \:|\: \lambda \eta \lambda^t = \eta \right\}$$ where $\eta = diag ( -1,1,1,1 ) $ is the matrix representing the metric $g$ in ( 4 ) in every minkowskian basis . a minkowskian coordinate system on $\mathbb m^4$ is a cartesian coordinate system whose axes are a minkowskian basis . lorentz transformations are transformations of coordinates between pairs of minkowskian coordinate systems with the same origin ( so that $c^k=0$ in ( 3 ) ) . thus they have the form $$x'^a = \sum_{j=1}^n {\lambda^a}_j x^j $$ for some $\lambda \in o ( 1,3 ) $ . if we admit different origins we obtain the so-called poincar transformations $$x'^a = c^a+ \sum_{j=1}^n {\lambda^a}_j x^j \: . $$ when viewing lorentz transformations as transformation of coordinates , their formal linearity does not play a relevant physical role , since it only reflects the arbitrary initial choice of the same origin for both reference frames . however , these transformations are also transformations of bases ( 3' ) in the space of translations ( the tangent space ) , in this case linearity is natural because it reflects the natural linear space structure of the translations .
maybe you should start with qed the strange theory of light and matter by feynman . its a layman book . once you get the intuition , get back to path integrals in quantum mechanics . i do not think a simpler book is there .
to add to rory 's answer- the ability to radiate particles in a random , statistical way , is in a deep sense identical to an object having the property we know as " temperature . " so , black holes have a temperature . it has a particular formula that is inversely proportional to the mass of the black hole . if you set that temperature equal to the current temperature of the cosmic microwave background , 2.725 k , then you get a mass of about 4.503 x 10^22 kg , or a little over half the mass of the moon . black holes above this mass will be cooler than the cmb incident upon them , so will gather mass-energy from it . black holes below it will lose energy due to hawking radiation faster than they gain it from the cmb , so will head towards a catastrophic , runaway " pop . " note that the cmb is also getting cooler as time goes on , so the equilibrium mass shifts upwards . no one that i know of has bothered to do any detailed " race " calculations between a black hole 's hawking radiation and the changing temperature of the cmb . another important mass related to hawking radiation is the mass at which the black hole is so cool that it would have emitted negligible radiation even if had been around since the beginning of the universe . this is about 2 x 10^11 kg , roughly comparable to the total mass of all humans . the second mass is less than the first , so if a whole range of black holes had been created at the beginning of the universe , the upshot is that some would be popping right now ! astronomers are on the lookout for these events .
a postulate is an ( usually fundamental ) assumption a writer makes in order to discuss a subject in a coherent fashion . examples of postulates are the born rule in quantum mechanics ( which defines how the wave function is to be interpreted ) , or in classical mechanics the existence of a lagrangian ( which defines the starting point of theoretical mechanics ) . a principle is a more or less universally observed ( usually fundamental ) fact . examples of principles are the second law of thermodynamics ( universal dissipation ) , the principle of relativity ( independence of the reference frame ) , or heisenberg 's uncertainty relation . a hypothesis is a theoretical assumption made to develop a ( usually alternative ) theory . examples are planck 's and einstein 's hypothesis of quantized light , or the existence of supersymmetry . one can turn a principle or hypothesis into a postulate , but not a postulate into a principle . edit2: note that it is possible that a principle is derived from a set of postulates . this reflects the fact that there is is some freedom in setting up the foundations . for example , the second law of thermodynamics can be derived from statistical mechnaics , and the principle of relativity can be derived from the postulate of lorentz invariance .
when the electromagnetic waves propagate without energy losses , e.g. in the vacuum , it is easy to prove that the total energy is conserved . see e.g. section 1.8 here . in fact , not only the total energy is conserved . the energy is conserved locally , via the continuity equation $$\frac{\partial \rho_{\rm energy}}{\partial t}+\nabla\cdot \vec j = 0$$ this says that whenever the energy decreases from a small volume $dv$ , it is accompanied by the flow of the same energy through the boundary of the small volume $dv$ and the current $\vec j$ ensures that the energy will increase elsewhere . the continuity equation above is easily proven if one substitutes the right expressions for the energy density and the poynting vector : $$ \rho_{\rm energy} = \frac{1}{2}\left ( \epsilon_0 e^2+ \frac{b^2}{\mu_0} \right ) , \quad \vec j = \vec e\times \vec h $$ after the substitution , the left hand side of the continuity equation becomes a combination of multiples of maxwell 's equations and their derivatives : it is zero . these considerations work even in the presence of reflective surfaces , e.g. metals one uses to build a double slit experiment . it follows that if an electromagnetic pulse has some energy at the beginning , the total energy obtained as the integral $\int d^3x \rho_{\rm energy}$ will be the same at the end of the experiment regardless of the detailed arrangement of the interference experiment . if there are interference minima , they are always accompanied by interference maxima , too . the conservation law we have proved above guarantees that . in fact , one may trace via the energy density and the current , the poynting vector , how the energy gets transferred from the minima towards the maxima . imagine that at the beginning , we have two packets of a certain cross section area which will be kept fixed and the only nonzero component of $\vec e$ goes like $\exp ( ik_1 x ) $ ( and is localized within a rectangle in the $yz$ plane ) . it interferes with another packet that goes like $\exp ( ik_2 x ) $ . because the absolute value is the same , the energy density proportional $|e|^2$ is $x$-independent in both initial waves . when they interfere , we get $$\exp ( ik_1 x ) +\exp ( ik_2 x ) = \exp ( ik_1 x ) ( 1 + \exp ( i ( k_2-k_1 ) x ) $$ the overall phase is irrelevant . the second term may be written as $$ 1 + \exp ( i ( k_2-k_1 ) x = 2\cos ( ( k_2-k_1 ) x/2 ) \exp ( i ( k_2-k_1 ) x/2 ) $$ the final phase ( exponential ) may be ignored again as it does not affect the absolute value . you see that the interfered wave composed of the two ordinary waves goes like $$ 2\cos ( ( k_2-k_1 ) x/2 ) $$ and its square goes like $4\cos^2 ( \phi ) $ with the same argument . now , the funny thing about the squared cosine is that the average value over the space is $1/2$ because $\cos^2\phi$ harmonically oscillates between $0$ and $1$ . so the average value of $4\cos^2\phi$ is $2$ , exactly what is expected from adding the energy of two initial beams each of which has the unit energy density in the same normalization . ( the total energy should be multiplied by $a_{yz} l_x\epsilon_0/2$: the usual factor of $1/2$ , permittivity , the area in the $yz$-plane , and the length of the packet in the $x$-direction , but these factors are the same for initial and final states . ) finally , let me add a few words intuitively explaining why you can not arrange an experiment that would only have interference minima ( or only interference maxima , if you wanted to double the energy instead of destroying it  which could be more useful ) . to make the interference purely destructive everywhere , the initial interfering beams would have to have highly synchronized phases pretty much at every place of the photographic plate ( or strictly ) . but that is only possible if the beams are coming from nearly the same direction . but if they are coming from ( nearly ) the same direction , they could not have been split just a short moment earlier , so it could not have been an experiment with the interference of two independent beams . the beams could have been independent and separated a longer time before that . but if the beams started a longer time before that , they would still spread to a larger area on the photographic plate and in this larger area , the phases from the two beams would again refuse to be synchronized and somewhere on the plate , you would find both minima and maxima , anyway . the argument from the previous paragraph has simple interpretation in the analogous problem of quantum mechanics . if there are two wave packets of the wave function for the same particle that are spatially isolated and ready to interfere , these two terms $\psi_1 , \psi_2$ in the wave function are orthogonal to each other because their supports are non-overlapping . the evolution of the wave functions in quantum mechanics is " unitary " so it preserves the inner products . so whatever evolves out of $\psi_1 , \psi_2$ will be orthogonal to each other , too , even if the evolved wave packets are no longer spatially non-overlapping . but this orthogonality is exactly the condition for $\int|\psi_1+\psi_2|^2$ to have no mixed terms and be simply equal to $\int|\psi_1|^2+|\psi_2|^2$ . the case of classical maxwell 's equations has a different interpretation  it is the energy density and not the probability density  but it is mathematically analogous . the properly defined " orthogonality " between the two packets is guaranteed by the evolution and it is equivalent to the condition that the total strength of the destructive interference is the same as the total strength of the constructive interference .
i answered a question related to this a few days ago , so i suppose i will try to summarize it here . wave particle duality does not really say that waves are particles . it says that " particles " are not really particles , nor are they really waves , they are just little objects that have some properties of waves and some properties of particles , and there are certain situations where one is more visible than the other . i have heard it said ( in a very rough sense ) that subatomic objects travel like waves , and interact like particles . again , this is a huge simplification , but there is an important intuition , which is that these objects are always a little like waves and a little like particles . we can describe their position by a function that tells you the probability that the object will be at a particle point in space at a particular time ; this function takes the mathematical form of a wave , so we call it a wavefunction , and this is the sense in which particles are like waves . when these objects interact , however , we tend to see them more as particles , like little classical marbles . the double-slit experiment is a good example of this . once more , i emphasize that this is a very big simplification , but just for the purposes of giving you a bit of context , we can imagine that as the electron travels through the slits , its wavelike character is more obvious , and so there are noticeable behaviors we normally attribute to classical waves , like interference . when it collides with the backboard , however , its particle-like character is more obvious , and so we see a single point where the electron collided with the wall . but at all times , the electron had both wave and particle characteristics , and that is the essence of wave-particle duality .
a force is a normal vector and a vector is characterized by its magnitude ( its norm , " lenght " in graphical representation ) and direction .
hints : notice that if we define $\mathbf s_{123} = \mathbf s_1 + \mathbf s_2 + \mathbf s_3$ and $\mathbf s_{12} = \mathbf s_1 + \mathbf s_2$ , then we have \begin{align} \mathbf s_{123}^2 = \mathbf s_{12}^2 + \mathbf s_3^2 + 2\mathbf s_{12}\cdot\mathbf s_3 \end{align} notice that your hamiltonian can be written as follows : \begin{align} h = \frac{a}{2} ( \mathbf s_{12}^2- \mathbf s_1^2 - \mathbf s_2^2 ) + b ( \mathbf s_{12}\cdot\mathbf s_3 ) \end{align} combine the last two hints . recall that the representation theory of the angular momentum algebra ( aka addition of angular momentum ) tells us that a tensor product of three spin-$1/2$ reprsentations splits into a direction sum as follows : \begin{align} \tfrac{1}{2}\otimes\tfrac{1}{2}\otimes\tfrac{1}{2} and = ( 1 \oplus 0 ) \otimes\tfrac{1}{2} \\ and = ( 1\otimes \tfrac{1}{2} ) \oplus ( 0\otimes \tfrac{1}{2} ) \\ and = \underbrace{ ( \tfrac{3}{2}\oplus\tfrac{1}{2} ) }_{s_{12}=1 , s_3 = \frac{1}{2}}\oplus \underbrace{\tfrac{1}{2}}_{s_{12}=0 , s_3=\frac{1}{2}} \end{align} ( addendum ) to clarify some discussion in the comments , the last line in number 4 means that the hilbert space admits an orthonormal basis of states which i will label $|s_{123} , s_{12}\rangle$ for which \begin{align} \mathbf s_{123}^2|s_{123} , m_{123} , s_{12}\rangle and = \hbar^2 s_{123} ( s_{123}+1 ) |s_{123} , m_{123} , s_{12}\rangle \\ s_{123}^z |s_{123} , m_{123} , s_{12}\rangle and = \hbar m_{123}|s_{123} , m_{123} , s_{12}\rangle\\ \mathbf s_{12}^2|s_{123} , m_{123} , s_{12}\rangle and = \hbar^2 s_{12} ( s_{12}+1 ) |s_{123} , m_{123} , s_{12}\rangle \\ \mathbf s_{1}^2|s_{123} , m_{123} , s_{12}\rangle and = \hbar^2 \tfrac{1}{2} ( \tfrac{1}{2}+1 ) |s_{123} , m_{123} , s_{12}\rangle \\ \mathbf s_{2}^2|s_{123} , m_{123} , s_{12}\rangle and = \hbar^2 \tfrac{1}{2} ( \tfrac{1}{2}+1 ) |s_{123} , m_{123} , s_{12}\rangle \\ \mathbf s_{3}^2|s_{123} , m_{123} , s_{12}\rangle and = \hbar^2 \tfrac{1}{2} ( \tfrac{1}{2}+1 ) |s_{123} , m_{123} , s_{12}\rangle \end{align} and explicitly , the basis is as follows : \begin{align} \left . \begin{array}{l} |\tfrac{3}{2} , \tfrac{3}{2} , 1\rangle \\ |\tfrac{3}{2} , \tfrac{1}{2} , 1\rangle \\ |\tfrac{3}{2} , -\tfrac{1}{2} , 1\rangle \\ |\tfrac{3}{2} , -\tfrac{3}{2} , 1\rangle \end{array}\right\} s_{123} = \tfrac{3}{2} , s_{12} = 1 , \dim = 4\\ \left . \begin{array}{l} |\tfrac{1}{2} , \tfrac{1}{2} , 1\rangle \\ |\tfrac{1}{2} , -\tfrac{1}{2} , 1\rangle \end{array}\right\} s_{123} = \tfrac{1}{2} , s_{12} = 1 , \dim = 2\\ \left . \begin{array}{l} |\tfrac{1}{2} , \tfrac{1}{2} , 0\rangle \\ |\tfrac{1}{2} , -\tfrac{1}{2} , 0\rangle \end{array}\right\} s_{123} = \tfrac{1}{2} , s_{12} = 0 , \dim = 2 \end{align} this is a basis for the entire three spin-$1/2$ particle hilbert space ; it is eight-dimensional . there are therefore a total of 8 orthonormal eigenvectors for the hamiltonian .
the magnitude that is denoted as $\bar{\nu}$ is not the frequency , it is the wavenumber defined as follows : $$ \bar{\nu} = \frac{1}{\lambda} $$ the wavenumber used in spectroscopy $\bar{\nu}$ and " usual " wavenumber $k$ are different . the corect conversion of fsr is $$ \mathrm{fsr} ( \nu ) = c \cdot \mathrm{fsr} ( \bar{\nu} ) = \frac{c}{2\pi} \mathrm{fsr} ( k ) $$
old fashioned ( or very cheap ) fluorescent bulbs flicker at twice the mains frequency if they use magnetic ballasts driven directly from the ac . modern fluorescent bulbs use an electric ballast that runs at 5khz - 40khz ( for cfls ) , then the electrons stay excited for longer than the switching frequency and the light output is constant . leds are constant , unless you count the individual electrons moving over the diode junction ! high power leds normally have their own driver chip which carefully controls the current and voltage of the diode so they are less sensitive to the supply power . solid state lasers are essentially the same as leds . edit : some very cheap led lights , eg christmas decoration are driven directly from ac - using the diodes themselves to rectify it - these will flicker at the 1/2 mains frequency .
this paper describes the effect in some detail . stable levitation is caused by a combination of the meissner effect , and of flux pinning in type-ii superconductors . the miessner effect is the property of superconductors which prevents magnetic flux from penetrating the superconducting material ( beyond the penetration depth ) . it occurs because the applied magnetic field induces " shielding " currents in the surface of the superconductor , which generates an opposing magnetic field that acts to exactly cancel the applied field . it is as though you have taken two conventional magnets , with the two north poles pointing each other , and suspended one above the other . this alone is not enough to produce the stable levitation that is shown in the video ; that requires the flux pinning property of type-ii superconductors . flux pinning occurs when a magnetic field of relatively low strength is applied . the flux does partly penetrate the superconductor , but is concentrated around imperfections in the material . the superconductor is held in position rigidly in the magnetic field , and experiences a kind of friction when nudged because flux vortices are forced to move from one pinning site to another . the design of the track allows the superconductor to move freely in one direction : along the length of the track there is no variance in the field , which allows the superconductor to move back and fourth with no energy loss . perpendicular to the length of the track , the bar magnet 's poles are aligned anti-parallel to each other , ( s-n-s ) . this alignment produces a considerably strong gradient [ along the width of the track ] . the variance of magnetic field strength from one side of the track to the other is so great and the pinning so strong in this superconductor that there is not only drag but also a restoring force . if the superconductor is given a small push in attempt to force it from the track , it will oscillate slightly and quickly return to its original position . if there is enough pinning and if the magnetic track is strong enough , then the levitating " train " should remain locked in place even if the track is rotated sideways or is upside down . note that if the applied magnetic field is extremely strong , this can cause the superconductor to loose its superconductivity .
you can make some assumptions like a typical fm receiver needs -110 dbm to work . then assume you have an isotropic antenna in both cases because you did not say anything about the antennas so we will ignore the gain . next take a look at the path loss based on the 910 w ( +59.6 dbm ) power . your path loss can not exceed 59.6 + 110 = 169.6 db ( with loss db and dbm are the same , but db and dbm to watts is different ) . the free space loss model is $l_{fs}=32.45 + 20*log ( d_{km} ) + 20*log ( f_{mhz} ) $ you will need to solve this for $d_{km}$ and set $l_{fs}=169.6$ and $f_{mhz}=89.7$ . there are many other path loss models . nist suggests several free space model ( shown above . . . least accurate ) ccir hata walfisch-ikegami ( wim ) having the antenna height will reduce your path loss some . in the popular wim model in a line of sight ( los ) situation like yours where the base antenna is > 30m high with no obstructions in the direct path then you can use a more realistic estimate : $l_{wim-los}=42.64+26*log ( d_{km} ) +20*log ( f_{mhz} ) $ also the itu has a specification itu 1546 method for point-to-area predictions for terrestrial propagation ( pdf here ) . you can also find matlab solutions and a lot of literature on their technique and field test results for broadcast fm . ( fyi the models are all very similar in structure to nist 's models ) .
technically , the equation $$d = \frac{\mathrm{d}x}{\mathrm{d}t}t + \frac{\mathrm{d}^2x}{\mathrm{d}t^2}\frac{t^2}{2}$$ is not right . instead , for constant acceleration , you need $$d = \left ( \left . \frac{\mathrm{d}x}{\mathrm{d}t}\right|_0\right ) t + \left ( \left . \frac{\mathrm{d}^2x}{\mathrm{d}t^2}\right|_0\right ) \frac{t^2}{2}$$ in other words , a quantity like $\mathrm{d}x/\mathrm{d}t$ changes in time , but you want to use the initial velocity only . i think this is what you probably intended to begin with , though . if you wanted to solve the problem purely kinematically , then you could try to expand the position in a taylor series as you wrote in your answer . however , this only works if the function is equal to its taylor series . for simple functions like exponentials and trig functions this is true , but for a person driving a car it is not . if a function equals its taylor series everywhere , then if you observe its position over any finite interval of time , no matter how short , you can completely determine what the car will do in the future . this is not realistic . instead , you will want some way of determining either the velocity or the acceleration as a function of time or position . in physics , it is common to be able to determine the acceleration as a function of position . the reason is that acceleration comes from the equation $$f=ma$$ so that if you can determine the forces present , you know the acceleration , and higher-order derivatives are not necessary . if you know the velocity as a function of time , you can simply integrate it to find the displacement . $$d ( t ) = \int_{t_0}^t v ( t&#39 ; ) \mathrm{d}t&#39 ; $$ if you know the acceleration as a function of time , you can integrate that too , although this situation is less common . $$d ( t ) = v_0 ( t - t_0 ) + t\int_{t_0}^t a ( t&#39 ; ) \mathrm{d}t&#39 ; - \int_{t_0}^t t&#39 ; a ( t&#39 ; ) \mathrm{d}t&#39 ; $$ i found this expression by looking for something whose derivative with respect to time was the velocity $$v ( t ) = v_0 + \int_{t_0}^t a ( t&#39 ; ) \mathrm{d}t&#39 ; $$ if you know the velocity as a function of position , you have the differential equation $$\frac{\mathrm{d}x}{\mathrm{d}t} = v ( x ) $$ which you can solve by separation of variables . if you know the acceleration as a function of position , you have the differential equation $$\frac{\mathrm{d}^2x}{\mathrm{d}t^2} = a ( x ) $$ which is not always easy to solve . in more realistic scenarios , the acceleration will depend not only on the object 's own position , but also on the positions of the things it is interacting with . this gives coupled differential equations , which can be simplified in a special cases , but frequently can only be solved numerically .
it is not true . to see this , you can try an experiment with some batteries and light bulbs . hook up two bulbs of different wattages ( that is , with different resistances ) in parallel with a single battery : both bulbs will light up , although with different brightnesses . that is , current is flowing through the one with more resistance as well as through the one with less resistance .
i remember that the question in your title was busted in mythbusters episode 72 . a simple google search also gives many other examples . as for single- vs alternate-direction folding , i am guessing that the latter would allow for more folds . it is the thickness vs length along a fold that basically tells you if a fold is possible , since there is always going to be a curvature to the fold . alternate-direction folding uses both flat directions of the paper , so you run out of length slightly slower . this would be a small effect since you have the linear decrease in length vs the exponential increase in thickness . thanks to gerry for the key word ( given in a comment above ) . i can now make my above guess more concrete . the limit on the number of folds ( for a given length ) does follow from the necessary curvature on the fold . the type of image you see for this makes it clear what is going on for a piece of paper with thickness $t$ , the length $l$ needed to make $n$ folds is ( oeis ) $$ l/t = \frac{\pi}6 ( 2^n+4 ) ( 2^n-1 ) \ , . $$ this formula was originally derived by ( the then junior high school student ) britney gallivan in 2001 . i find it amazing that it was not known before that time . . . ( and full credit to britney ) . for alternate folding of a square piece of paper , the corresponding formula is $$ l/t = \pi 2^{3 ( n-1 ) /2} \ , . $$ both formulae give $l=t\ , \pi$ as the minimum length required for a single fold . this is because , assuming the paper does not stretch and the inside of the fold is perfectly flat , a single fold uses up the length of a semicircle with outside diameter equal to the thickness of the paper . so if $l &lt ; t\ , \pi$ then you do not have enough paper to go around the fold . let 's ignore a lot of the subtleties of the linear folding problem and say that each time you fold the paper you halve its length and double its thickness : $ l_i = \tfrac12 l_{i-1} = 2^{-i}l_0 $ and $ t_i = 2 t_{i-1} = 2^{i} t_0 $ , where $l=l_0$ and $t=t_0$ are the original length and thickness respectively . on the final fold ( to make it n folds ) you need $l_{n-1} \leq \pi t_{n-1}$ which implies $l \leq \frac14\pi\ , 2^{2n} t$ . qualitatively this reproduce the linear folding result given above . the difference comes from the fact you lose slightly over half of the length on each fold . these formulae can be inverted and plotted to give the logarithmic graphs where $l$ is measured in units of $t$ . the linear folding is shown in red and the alternate direction folding is given in blue . the boxed area is shown in the inset graphic and details the point where alternate folding permanently gains an extra fold over linear folding . you can see that there exist certain length ranges where you get more folds with alternate than linear folding . after $l/t = 64\pi \approx 201$ you always get one or more extra folds with alternate compared to linear . you can find similar numbers for two or more extra folds , etc . . . looking back on this answer , i really think that i should ditch my theorist tendencies and put some approximate numbers in here . let 's assume that the 8 alternating fold limit for a " normal " piece of paper is correct . normal office paper is approximately 0.1mm thick . this means that a normal piece of paper must be $$ l \approx \pi\ , ( 0.1\text{mm} ) 2^{3\times 7/2} \approx 0.3 \times 2^{10.5}\ , \text{mm} \approx . 3 \times 1000 \ , \text{mm} = 300 \text{mm} \ , . $$ luckily this matches the normal size of office paper , e.g. a4 is 210mm * 297mm . the last range where you get the same number of folds for linear and alternate folding is $l/t \in ( 50\pi , 64\pi ) \approx ( 157,201 ) $ , where both methods yield 4 folds . for a square piece of paper 0.1mm thick , this corresponds to 15cm and 20cm squares respectively . with less giving only three folds for linear and more giving five folds for alternating . some simple experiments show that this is approximately correct .
first , let me emphasize something that is being covered by a thick layer of misinformation in the media these days : it is totally premature to conclude whether the lhc will see susy or not . the major detectors have only collected 45/pb ( and evaluated 35/pb ) of the data . the " slash pb " should be pronounced as " inverse picobarns " . the lhc is designed to collect hundreds or thousands times more data than what it has recorded so far , and it should eventually run at a doubled energy ( 14 tev total energy instead of the current 7 tev total energy ) . each multiplication of the integrated luminosity ( number of collisions ) by 10 corresponds to the access of new particles whose masses are approximately 2 times larger or so . it means that the lhc will be able to decide about the existence of new particles at masses that are 4-16 times higher than the current lower bounds ( 16 also includes the likely upgrade from 2x 3.5 tev to 7 tev ) . there are at least two " mostly independent " parameters with the dimension of mass in susy - i mean $m_0$ and $m_{1/2}$ . so the number from the previous sentence should really be squared , and in some sensible counting and with a reasonable measure , the lhc has only probed about 1/16 - 1/256 of the parameter space that is accessible to the lhc over its lifetime . so the only thing we can say now is that susy was not discovered at an extremely early stage of the experiment - which many people have hoped for but this possibility was never supported by anything else than a wishful thinking . whether the lhc may see susy may remain an open question for several years - unless the lhc will see it much sooner than that . it is an experiment that may continue to 2020 and beyond . we do not really know where the superpartner masses could be - but they may sit at a few tev and this would still mean that they are accessible by the lhc . now , your questions : what susy helps to solve first , susy is a natural - and mostly inevitable - consequence of string theory , the only consistent quantum theory that includes gravity as well as the yang-mills forces as of 2011 . see http://motls.blogspot.com/2010/06/why-string-theory-implies-supersymmetry.html in this context , supersymmetry is needed for the stability of the vacuum and other things , at least at a certain level . for other reasons , to be discussed below , it is natural to expect that susy should be unbroken up to lhc-like energy scales ( i.e. . that it should be visible at the lhc ) - but there is no sharp argument that might calculate the superpartner scale . some string theorists even say that it should be expected that supersymmetry is broken at a very high scale ( near the gut scale or planck scale ) - because this is a " generic behavior " in the stringy landscape ( the " majority " of the minima have a high-scale susy breaking which would make susy unavailable to any doable experiments ) - so these proponents of the anthropic reasoning do not expect susy to be seen at the lhc . however , more phenomenological considerations make it more natural for susy to be accessible by the lhc . why ? there are several main arguments : susy may offer a very natural dark matter particle candidate , namely the lsp ( lightest supersymmetric particle ) , most likely the neutralino ( the superpartner of the photon or z-boson or the higgs bosons , or their mixture ) , that seems to have the right approximate mass , strength of interactions , and other things to play the role of the majority of the dark matter in the universe ( so that the big bang theory with this extra particle ends up with a universe similar to ours after 13.7 billion years ) . see an article about susy and dark matter : http://motls.blogspot.com/2010/07/susy-and-dark-matter.html also , susy with superpartners not far from the tev or lhc energy scale improves the gauge coupling unification so that the strengths of the couplings get unified really nicely near the gut scale ( and maybe incorporated into a single and simple group at a higher energy scale not far from the planck scale ) , see : http://motls.blogspot.com/2010/06/susy-and-gauge-coupling-unification.html the unification in the simplest supersymmetric models is only good if the superpartners are not too far from the tev scale - but if they are around 10 tev , it is still marginally ok . the same comment with the same value 10 tev also holds for the dark matter job of the neutralinos discussed above . finally and most famously , susy with superpartner masses not far from the tev or lhc scale stabilizes the higgs mass - it explains why the higgs mass ( and , consequently , the masses of w-bosons and z-bosons , among other particles ) is not driven towards a huge energy scale such as the planck scale by the quantum corrections ( with loops of particle-antiparticle pairs in the feynman diagrams ) . those otherwise expected quantum corrections get canceled at the tev accuracy if the superpartner masses are near a tev - and the resulting higgs mass may then be naturally in the expected 100 gev - 200 gev window with an extra 10:1 luck ( which is not bad ) . the lighter the superpartner masses are , the more " naturally " susy explains why the higgs mass remains light . but there is no strict argument that the superpartners have to be lighter than 1 tev or 10 tev . it just " sounds strange " if they were much higher than that because a non-negligible portion of the hierarchy problem would remain . see a text on susy and the hierarchy problem : http://motls.blogspot.com/2010/07/susy-and-hierarchy-problem.html one may say that experiments already do disprove 99.999999999+ percent of the natural a priori interval for a conceivable higgs mass in the standard model . susy changes this counting - the probability that the higgs mass ends up being approximately as low as suggested by the electroweak observations becomes comparable to 100 percent according to a susy theory . to agree with other available experiments , susy needs to adjust some other parameters but at good points of the parameter space , none of the adjustments are as extreme as the adjustment of the higgs mass in the non-supersymmetric standard model . can we decide whether susy is there at the lhc ? susy may hide for some time but the lhc is simply scheduled to perform a certain number of collisions at a certain energy , and those collisions may eventually be studied by the most up-to-date methods and the evidence for susy will either be there in the data or not . some phenomenologists often want to stay very modest and they talk about numerous complex ways how susy may keep on hiding - or remain de facto indistinguishable from other models . however , sometimes the very same people are capable of reverse-engineering a randomly constructed man-made model ( fictitiously produced collision data ) within a weekend : these are the games played at the lhc olympics . so i do not really expect too much hiding . with the data , the fate of the lhc-scale susy will ultimately be decided . obviously , if susy is there at the lhc scale , the lhc will eventually be discovering fireworks of new effects ( susy is also the most attractive realistic possibility for the experimenters ) - all the superpartners of the known particles , among other things ( such as an extended higgs sector relatively to the standard model ) . their spins and couplings will have to be checked to ( dis ) agree with those of the known particles , and so on . all the masses may be surprising for us - we do not really know any of them although we have various models of susy breaking which predict various patterns . alternatives in the case of susy non-observation the dark matter may be composed of ad hoc particles that do not require any grand structures - but such alternatives would be justified by nothing else than the simple and single job that they should play . of course that there are many alternatives in the literature but none of them seem to be as justified by other evidence - i.e. not ad hoc - as susy . i think that in the case of no susy at the lhc , the lhc will remain some distance away from " completely disproving " susy particles as the source of dark matter because this role may work up to 10 tev masses or so , and much of this interval will remain inaccessible to the lhc . so the lhc is a great gadget which is stronger than the previous one - but one simply can not guarantee that it has to give definitive answers about all the questions we want to be answered . this fact may be inconvenient ( and many laymen love to be promised that all questions will inevitably be answered for those billions of dollars - whether it is true or not ) but it is simply a fact that the lhc is not a machine to see every face of god . there are various alternatives how to solve the hierarchy problem - the little higgs model , the randall-sundrum models ( which may be disproved at the end of the lhc , too - the lhc is expected to decide about the fate of each solution to the hierarchy problem although they may always remain some uncertainty ) , etc . - but i am convinced that even in the case that susy is not observed at the lhc , superpartners with slightly higher masses than those accessible by the lhc will remain the most well-motivated solution of the problems above . of course , if someone finds some better new models , or some amazing experimental lhc ( or other ) evidence for some existing models , the situation may change . but right now , away from susy , there are really no alternative theories that naturally explain or solve the three problems above at the same moment . this ability of susy to solve many things simultaneously is surely no proof it has to be the right solution of all of them - but it is a big hint . it is the reason why particle physicists think it is the most likely new physics at this point - a conclusion that may change but only if new ( theoretical or experimental ) evidence arrives . while it is clear that the absence of susy at the lhc would weaken the case for susy and all related directions , i am convinced that unless some spectacular new alternatives or spectacular new proofs of other theories are found in the future , susy will still remain the single most serious direction in phenomenology . in formal theory , its key role is pretty much guaranteed to remain paramount regardless of the results of lhc or any conceivably doable experiments . the more formal portions of high-energy theory a theorist studies , obviously , the less dependent his or her work is on the lhc findings . i do not have to explain that the absence of susy at the lhc would mean a sharper splitting of the particle physics community . absence of susy and string theory clearly , if no susy were seen until 2012 or 2015 or 2020 , the critics of string theory would be louder than ever before . within string theory , the anthropic voices and attempts to find a sensible vacuum with the susy breaking at a high-energy scale would strengthen . but nothing would really change qualitatively . the lhc is great but it is just moving the energy frontier of the tevatron at most by 1-1.5 order ( s ) of magnitude or so . if there is some non-susy new physics found at the lhc , most particle physicists will obviously be interested in whatever models that can be relevant for the observations . if the lhc sees no new physics , e.g. if it only sees a single higgs boson , and nothing else ever appears , the current situation will qualitatively continue and the tensions will only get amplified . serious physicists will have to continue their predominantly theoretical and ever more careful studies ( based on the observations that have been incorporated into theories decades ago ) without any guidance about new physics from the available new experiments ( simply because there would not be any new data ! ) - while the not so serious physicists and people around science will strengthen their hostile and utterly irrational claims that physics is no longer science . sociologically , the situation would almost certainly become unpleasant for good physicists and pleasant for populist and uneducated critics of science who are not really interested in the truth about the physical world . but nature works in whatever way she does . she is not obliged to regularly uncover a part of her secrets . a paper with the same question in the title amusingly , there exists a 2-week-old preprint by 8 authors : http://arxiv.org/abs/arxiv:1102.4693 what if the lhc does not find supersymmetry in the sqrt ( s ) =7 tev run ? you may see that the question in their title is almost identical to your question at physics stack exchange . their answer is much like my answer above : if the lhc is not found during the 7 tev run ( that should continue until the end of 2012 ) , susy would still remain an acceptable solution to all the problems i mentioned above ; just our idea about the masses of the strongly interacting superpartners ( gluinos and squarks ) would have to be raised above 1 tev or so . it is pretty natural for those strongly interacting superpartners to be the heaviest ones among the superpartners - which automatically makes them harder to be seen at hadron colliders such as the lhc .
no , it is not feasible to dig a hole to the center of the earth . the deepest such hole appears to be the kola superdeep borehole , which goes down about 12km , or about 0.2% of the way to the center of the earth . the pressure at the center of the earth is so high that rocks are about five or six times denser there than at the crust ; they have literally been crushed down to a fraction of their original size . the temperature there is believed to be around 7000k , hot enough to melt just about any metal . clearly , this is not a reasonable engineering goal . you might be interested in a 2003 paper by dave stevenson proposing to study the deep interior of earth by dropping molten iron down a crack in the crust and allowing it to sink all the way to the core . i can not seem to find the original paper , but here 's a readable summary . the reference is a87 . stevenson , david j . mission to earths core -a modest proposal . nature , 423 , 239-240 , 2003 .
there are working quantum computers right now .
eureka ! as archimedes said , according to legend . in principle , " themachinecharmer 's " answer is feasible , but i would recommend recording the change in the volume of water instead ( if you need an accurate measurement ) , because ( 1 ) it could be difficult to measure the volume of the spilled water , and ( 2 ) it is also a little less accurate to do so . ( some water will be left on the sides of the first container , and inserting the object into the filled container , catching all of the spillover while making sure the water level does not drop below the brim , could be difficult . ) if you only need a rough idea , the other way is fine . if you need a more accurate measurement , try one of these : get the volume of a container by filling it with water ( e . g . from a graduated vessel ) . empty the container , then place the object in it . fill it with water again , measuring how much water you added . subtract this number from the volume of the container to find the volume of your object . ( the order in which you do these does not matter , of course . ) this one could be a little harder , because you need a large graduated vessel . fill the empty vessel with roughly enough water to submerge the object . put the object in and record the change in the water volume .
i will start with current first . . . 1 ) " current flows in a circuit " is the simple answer . in other words - it is the rate of flow of electric charges . other than $i=\frac{dq}{dt}$ , current is also given by $i=naev_d$ which says something that it depends upon the drift velocity of electrons . the drift velocity is the average velocity between two successive collisions . this velocity prevents the electrons from accelerating continuously . ok . let 's consider a circuit with three resistors with resistances in an increasing order $r_1&gt ; r_2&gt ; r_3$ . first , current enters $r_1$ . after some collisions ( causes heat generation ) , it exits the resistor . now , the same current enters and exits $r_2$ and $r_3$ in the same manner . one point is to notice that , the rate of flow of charges is always the same ( the current entered and exited the resistors with same magnitude ) . only the drift velocities vary in different resistors . if the same are connected in parallel ( now , we look into the resistors ) , current flows through $r_3$ easily . because , $r_3$ requires a lesser $v_d$ ( i.e. ) electrons entering $r_3$ would exit within a small period of time relative to the other two ( thereby increasing the rate ) . . . as a result , larger current would be observed . 2 ) voltage is simply the " energy per unit charge " in both electrostatics and current electricity . let 's assume that the electrons have some energy and they pass through the series of resistors . when the electrons encounter collisions within the resistor $r_1$ , they lose their potential ( which is referred to as " voltage drop" ) accordingly with their drift velocity ( i.e. ) number of collisions which evolves as heat ( depends upon the resistance of each ) . the remaining potential is still present in those electrons which drops across other resistors . thus , the sum of each potential drop gives the total potential in that circuit . as in case of parallel circuit , current has divided . now , though the charges are taking different paths , they have the same potential ( energy ) . hence , the potential drop across each resistor would be the same . . . man , it is all the consequence of ohm 's law $v=ir$ . note : and , sorry for that confusion . electrons gain energy only as they pass through the battery . but , they are always continuously accelerated by the electric field . also , resistance is also given by $r=\frac{ml}{nae^2\tau}$ which simon ohm replaced it as a constant ( for a given temperature ) . . .
these scientists are of course not fakers , but many science journalists are overhyping legitimate and valuable but not revolutionary new results sometimes . applying the statistical mechanics definition of temperature , systems that can exhibit a population number inversions ( higher energy levels are over populated compared to lower energy levels ) of their energy states , are characterized by a negative temperatures . however , these facts are not new , lasers and masers are examples of such negative temperature systems that are well known since long time ago . in the new study asked about in this question , they are just using different ( momentum ) degrees of freedom , as is further explained here .
as you say , the above problem stated barely is not solvable . i add that this is in the sense that there are many solutions , based on a thought experiment approach . it is unstated in the problem what the pressures at the three nodes of the network are . if the pressures are all the same , the flow will be for practical purposes zero . if the pressures are different , there will be flow . one aspect of the pressure differences is the geometry relative to the local gravitational field , which again is unstated . if this is flat , with no other pressures acting , no flow , if it is vertical , flow . if the sources you see claiming that this is a solvable problem are knowledgeable , i would assume that they mean " given the relative pressures and geometry " . it is clear also that if the way in which the pipes are joined is nontrivially asymmetric , in a practical setting , that may modify the effective c-values of the various pipes ( in other words , it may not be enough to measure the c-values of the three pipes before the whole is constructed ; knowing whether the effects of the join will have to be taken into account in a given situation is a matter of engineering experience ) . thanks for the links to the hazen william formula and the hardy cross method , which were interesting . needless to say , my answer is ab initio , i do not have experience in pipework . i hope it is helpful , but i guess i am taking a risk , having just downvoted kleingordon .
yes . i am not going to list all the stars here but it is easy to compile such a list . wikipedia has a list of exoplanetary host stars . if you sort the table by ascending apparent magnitude and decide how bright " naked-eye stars " need to be , you can take as many as you like from the list by taking all the stars with smaller magnitudes . i think magnitude 6 is roughly the faintest that is visible with the naked eye but it varies with your location and conditions . for example , the exoplanet hosts with magnitudes less than 4 are pollux , $\alpha$ arietis , $\gamma$ leonis , $\gamma$ cephei , $\iota$ draconis , $\epsilon$ tauri , $\epsilon$ eridani , $\beta$ pictoris and 7 canis majoris .
did he think : i have got to get a negative result , so i choose the limits from $r+a$ to $ra$ ? yep ! well , probably . i do not know exactly what feynman was thinking , of course , but that is a pretty typical way for a physicist to figure out the correct sign for an expression . if you want to be a little more precise , you can use the same sort of physical reasoning earlier in the calculation : in particular , on the mass of the ring . feynman gives the following expression for that mass : $$\mathrm{d}m = 2\pi a\mu\mathrm{d}x$$ now , we know the mass has to be positive . so $\mathrm{d}x$ had better be greater than zero , and that in turn means we have to do the integral in the direction of increasing $x$ . $$\int_{\text{smaller limit}}^{\text{larger limit}}\ldots\mathrm{d}x$$ in figure 13.6 , $x$ increases from left to right , so the lower limit of the integral has to be the left side of the sphere ( where $x = -a$ ) , and the upper limit has to be the right side ( where $x = a$ ) . so you could actually think of the integral like this : $$\int_{\text{left side of sphere}}^{\text{right side of sphere}}\ldots\mathrm{d}x$$ just one catch : feynman actually writes the integral as an integral over $r$ , not $x$ . so he is doing this instead : $$\int_{\text{left side of sphere}}^{\text{right side of sphere}}\ldots\mathrm{d}r$$ it is still the case that the lower limit is at the left side of the sphere , but you just have to pick the value of $r$ ( instead of $x$ ) at that point , which is $r = r + a$ . that is why $r + a$ is the lower limit . and similarly , the upper limit is $r - a$ because $r = r - a$ at the right side of the sphere . as you can tell , this sort of reasoning can be tricky , which is why we like to bypass it and just pick whichever sign gives the correct answer .
ok , i will make my comment into an answer . the wiki article has basic information . sound waves are pressure waves and depend on density so gravity which stratifies the atmospheric density affects sound waves through that . in solids and liquids to the extent that gravity stratifies them it will change the behaviour of sound waves . any configuration that can take density variations should have the possibility of sound waves propagating , so there could be sound waves in neutron stars .
planets made mostly of water almost certainly exist , and at least one may have been detected . however , such bodies will not be made entirely of liquid . in fact they will be mostly solid , even if the temperature is very high . this is because water can form more than one form of ice . the kind we are familiar with forms at low temperatures and is less dense than water . but there are several other kinds that form at very high pressures instead . these " exotic " ices are heavier than liquid water , so will sink to the bottom . you can get a feel for the pressures involved by looking at a phase diagram of water , e.g. here or here . earth 's oceans are not deep enough for such pressures to exist , but if the whole planet were made of water , the planet would be composed mostly of " exotic " ices , covered by a relatively thin layer of liquid ( though this would still be a lot deeper than earth 's oceans ) . in fact , europa , titan and many of the moons of the outer solar system are probably more or less like this , except that they have a layer of normal water ice on top of the liquid ocean , because they are so cold . another answer mentions the possibility that there would be a smooth transition from water vapour to liquid water , rather than a clear ocean surface as there is on earth . whether this is true or not depends on the temperature . the critical point for water is at about $647\ , \mathrm{k}$ , or $374^\circ\mathrm{c}$ . if the temperature is higher than this then there will not be a phase transition between the atmosphere and the liquid , but if it is lower then there will , just as there is on earth . with those details out of the way we can address the question of stability . your original question was what is the largest body of liquid water that can hold itself together under its own gravity . however , i do not think such a body can exist at all . if it did , it would have to be rather small in order to prevent the formation of exotic ices in its interior . but that would mean its gravity would be quite low , so it would not have an atmosphere . the low pressure at the surface would cause the water to evaporate or boil , and the molecules of the resulting gas would easily be moving fast enough to escape the body 's gravitational pull . ( of course , things are different if we allow the surface to be frozen - see the outer solar system moons - but i am assuming you are specifically interested in a body with a liquid surface . ) however , if we allow the interior to be composed of exotic ices rather than liquid , it gets a lot easier . then all we need is for it to be big enough that it is escape velocity is higher than the velocity of water vapour molecules . anything from around earth-sized upwards should do the trick . i do not think there is an upper limit to the size , except that if it is really , really big then nuclei in its core will start to fuse and it will turn into a star . you mention uv radiation causing a loss of the water , and this is an issue for a body the size of earth . uv radiation can split water into hydrogen and oxygen , and the hydrogen atoms have a much higher velocity , making it easier for them to escape into space . on earth we are fortunate to have an oxidising atmosphere , which tends to turn the hydrogen atoms back into molecules before they can escape . if it was not for that , the earth 's oceans would already have disappeared . however , if we start with a pure $\mathrm{h_2o}$ planet that is big enough , eventually it too will have an oxygen atmosphere . not because of photosynthesis , but simply because as the hydrogen escapes it leaves oxygen behind . given sufficient time i would expect this to lead to a protective oxygen atmosphere that prevents further escape of hydrogen . the only detail left is how to get a surface temperature that is in the right range for the surface to be liquid . this depends on the distance from the star , but also on the composition of the atmosphere . on earth , the atmosphere 's composition is mostly due to the action of the biosphere , which keeps the temperature regulated in just the right range for water to be liquid . perhaps it is possible to imagine life on such a water world , in the form of photosynthesising algae-like organisms , which might play a similar role .
idealizing the plane 's wheels as frictionless , the thrust from the propeller accelerates the plane through the air regardless of the treadmill . the thrust comes from the prop , and the wheels , being frictionless , do not hold the plane back in any way . if the treadmill is too short , the plane just runs of the end of it and then continues rolling towards take off . if the treadmill is long enough for a normal takeoff roll , the plane accelerates through the air and rotates off of the treadmill . update : do not take alfred 's word for it . mythbusters has actually done the experiment . update 2: i have been thinking about how the problem is posed ( for now as i am typing this ) and it occurred to me that the constraint " run at the same speed as whatever the planes tyres rotation speed " actually means run such that the plane does not move with respect to the ground . consider a wheel of radius $r$ on a treadmill . the treadmill surface has a linear speed $v_t$ to the right . the center of the wheel has a linear speed $v_p$ to the left . the ccw angular speed of the wheel is : $\omega = \dfrac{v_t + v_p}{r}$ if " run at the same speed as whatever the planes tyres rotation speed " means : $\omega = \dfrac{v_t}{r}$ then the constraint requires $v_p = 0$ . that is , the question , as posed , is : if the treadmill is run such that the plane does not move , will the plane take off ? obviously , the answer is no . the plane must move to take off . looking at mwengler 's long answer , we see what is happening . the rotational speed of the tires and treadmill are not the key , it is the acceleration of the treadmill that imparts a force on the wheel axles ( ignoring friction for simplicity here ) . so , it is in fact the case that it is possible , in principle , ( do not think it is possible in practice though ) to control the treadmill in such a way that it imparts a holding force on the plane , preventing it from moving . but , once again , this force is not proportional to the wheels rotational speed , but to the wheel 's angular acceleration ( note that in the idealized case of massless wheels , it is not even possible in principle as the lower the moment of inertia of the wheels , the greater the required angular acceleration ) .
yes . it does in fact mean that electromagnetic fields can also change the geometry of spacetime . anything with energy and/or momentum affects the geometry of spacetime because , as you point out , the gravitational field equations exhibit a coupling of spacetime geometry to energy-momentum . for more info in the case of electromagnetism coupling to gravity , see this .
you have to run a massively sophisticated supernova simulation to get that kind of data . whole research groups work on them . the biggest unknowns are generally the details of neutrino physics . this is both because neutrino hard data does not come easy , and because solving the radiation field of a supernova is a function of seven or eight variables ( x , y , z positions , two angles for direction of radiation propagation , energy band , polarization , and time ) . trying to obtain decent resolution in all of those variables quickly becomes computationally explosive . try google scholaring " supernova yield " .
what do you take to define " red " light : a wavelength of $650~\mathrm{nm}$ or a frequency of $460~\mathrm{thz}$ ? on the one hand , this borders on being an ill-defined question , but i suppose it can be massaged into something answerable . i would argue that frequency is more fundamental to describing the light . after all , it is the frequency that is constant throughout all this , as you noted . when a photon strikes a receptor in your eye , it does not matter whether it did so after just passing through glass or through vacuum - the biochemical response is dictated by the frequency/energy of the photon . thus it would be more appropriate to say red light stays red , but the wavelength corresponding to red shifts in glass .
i would read richard feynman 's lectures on the subject . specifically the book qed . if you are striving to learn some general concepts , a knowledge of the math is not necessary but is helpful . the extremely basic quantum physics topics use differential equations and complex variables and equations . ( the standard schrodinger equation for instance ) books by stephen hawking speak about quantum mechanics as well , and are great introductions to the concepts .
as far as i understand , if it exists , it must be far away from the " positive " matter because of repelling force , so it explains why there is no observations of such matter . for one , negative mass would still be attracted to positive mass , but the positive mass would be repelled . this would lead to the negative mass " following " the positive mass . why is this ? the force is a repulsive one . but we also have the fact that $\vec f=m\vec a$ . since one of the bodies has negative mass , it will be attracted . if there was a body of negative mass less massive ( talking about absolute values here ) than our positive massed-universe , it would move much faster and eventually " catch up " with ours . if it had the same absolute value of mass , both would keep accelerating and it would never catch up . if it was larger , it would be left behind eventually . the lack of any large quantities of negative mass in our universe excludes the case of a body of negative mass having caught up with us . the lack of any acceleration of the universe signifies that there is not any large body of negative mass with absolute value less than or equal to the mass of the universe . so , if there is any large body of negative mass , it has larger mass than our universe and it is separated from our universe . " all structures that exist mathematically exist also physically " -max tegmark . this is a part of his mathematical universe hypothesis . note that it is just a hypothesis , it is not backed by much concrete evidence yet . despite being completely inconsistent with a common-sense approach and the expected behavior of " normal " matter , negative mass is completely mathematically consistent and introduces no violation of conservation of momentum or energy . this is true--it lets one create energy out of thin air by introducing a body with negative mass to the system , but this does not conflict the principle of conservation of energy as the body has it is own energy become more negative . such matter would violate one or more energy conditions and show some strange properties , stemming from the ambiguity as to whether attraction should refer to force or the oppositely oriented acceleration for negative mass . i am not too sure about the energy conditions , i am not strong in general relativity . but the second half is just about some strangeness attached to it and some confusion regarding terminology ( relating to what i explained near the top of this answer ) . finally , vacuum fluctuations ( which exist ) can have a net negative energy density , so these have " negative mass " , in a way . but these are not easy to harness with current technology .
lets take each case and make some calculation . so , the first case , waiting for 5 minutes than adding some cold water . assume the following values : the initial " hot " temperature of the coffee $t_h=80^{\circ}c$ the temperature of the surrounding medium $t_m=23^{\circ}c$ using newtons cooling law $$\frac{dt}{dt}=-k ( t-t_m ) $$ and after a simple integration we get $$t_c=t_m+ ( t_h-t_m ) \mathrm{e}^{-kt}$$ taking $k=0.05$ we find $t_c$ to be $t_c\simeq63^{\circ}c$ . now we have to add some cold water . lets say we add $1/4^{th}$ of a cup at $t_{cold}=10^{\circ}c$ . $$-q_{coffee}=q_{water}$$ $$-cm_{c}\delta t_{coffe}=cm_{w}\delta t_{water}$$ $$-\frac{3}{4} ( t_f-t_{c} ) =\frac{1}{4} ( t_f-t_{cold} ) $$ and we find $t_f$ to be $t_f\simeq49^{\circ}c$ now lets look at the second method , mixing them from the start . we just have to replace the numerical value for $t_c$ in the above formula with the initial temperature of the coffee $t_h$ . doing this , we find that the temperature after mixing is $t_{f}'\simeq 62^{\circ}c$ . thus , as neuneck already said , the first method is the best one . these are at best some approximate calculations , but even so , the difference is clearly visible . edit : as a response to the comments , if you add hotter and hotter water to your coffee the final temperature after mixing will be higher ( it will grow linearly with the temperature of the water you add . ) here is a plot for it .
the kepler orbit of the earth around the sun is determined by two constants : the specific orbital energy $e$ and the specific relative angular momentum $h$: $$ \begin{align} e and = \frac{1}{2}v_{r , \oplus}^2 + \frac{1}{2}v_{t , \oplus}^2 - \frac{\mu}{r}= -\frac{\mu}{2a} , \\ h^2 and = r^2\ , v^2_{t , \oplus} = \mu a ( 1-e^2 ) , \end{align} $$ where $\mu = g ( m_\odot + m_\oplus ) $ , $r$ is the distance earth-sun ( at the moment of impact ) , $a$ is the semi-major axis , $e$ is the orbital eccentricity , $v_{r , \oplus}$ is the radial orbital velocity of the earth , and $v_{t , \oplus}$ the tangential velocity . now , suppose that a large asteroid collides with the earth , with orbital velocity $ ( v_{t , a} , v_{r , a} ) $ and mass $m_a$ . its relative velocity is then $$ \begin{align} v_{t , a}' and = v_{t , a} - v_{t , \oplus} , \\ v_{r , a}' and = v_{r , a} - v_{r , \oplus} . \end{align} $$ we can express these relative velocities in terms of the total impact velocity $v_\text{i}$ and the impact angle $\theta$: $$ \begin{align} v_{t , a}' and = v_\text{i}\cos\theta , \\ v_{r , a}' and = -v_\text{i}\sin\theta , \end{align} $$ where i defined $\theta$ as in fig . 1 of this article . so we obtain $$ \begin{align} v_{t , a} and = v_{t , \oplus} + v_\text{i}\cos\theta , \\ v_{r , a} and = v_{r , \oplus} - v_\text{i}\sin\theta . \end{align} $$ if we assume that the collision is central , that heat loss is negligible and that the debris remains gravitationally bound to the earth , then conservation of momentum implies $$ \begin{align} m_\oplus\ , v_{t , \oplus} + m_a\ , v_{t , a} and = ( m_\oplus+m_a ) u_{t , \oplus}\\ m_\oplus\ , v_{r , \oplus} + m_a\ , v_{r , a} and = ( m_\oplus+m_a ) u_{r , \oplus} , \end{align} $$ with $ ( u_{t , \oplus} , u_{r , \oplus} ) $ the new orbital velocity of the earth ( and the gravitationally bound debris ) after the impact . we get $$ \begin{align} u_{t , \oplus} and = v_{t , \oplus} + \frac{m_a}{m_\oplus+m_a}v_\text{i}\cos\theta , \\ u_{r , \oplus} and = v_{r , \oplus} - \frac{m_a}{m_\oplus+m_a}v_\text{i}\sin\theta . \end{align} $$ so the orbital energy and angular momentum will have changed into $$ \begin{align} e ' and = \frac{1}{2}u_{r , \oplus}^2 + \frac{1}{2}u_{t , \oplus}^2 - \frac{\mu}{r}= -\frac{\mu}{2a'} , \\ h'^2 and = r^2\ , u^2_{t , \oplus} = \mu a' ( 1-e'^2 ) . \end{align} $$ ( the change in $\mu$ is negligible ) . right , let 's plug in some numbers . suppose we start with a circular orbit , with a radius equal to the present-day semi-major axis : $$ \begin{align} \mu and = 1.32712838\times 10^{11}\ ; \text{km}^3\ , \text{s}^{-2} , \\ r and = a = 1.49598261\times 10^{8}\ ; \text{km} , \\ e and = 0 . \end{align} $$ for a circular orbit , it follows that $$ \begin{align} v_{t , \oplus} and = \sqrt{\frac{\mu}{r}}= 29.785\ ; \text{km}\ , \text{s}^{-1} , \\ v_{r , \oplus} and =0\ ; \text{km}\ , \text{s}^{-1} . \end{align} $$ the impact velocity of an asteroid will always be at least equal to the earth 's escape velocity $11.2\ , \text{km/s}$ , which is the speed it takes up as it falls into the earth 's gravitational potential well . the article that i already linked to states that typical asteroid impact velocities are in the range of $12-20\ , \text{km/s}$ . in theory , the impact velocity can be as large as $72\ , \text{km/s}$ in the case of a head-on collision , when the earth and the asteroid have opposite orbital velocities , thus a relative velocity of ~$60\ , \text{km/s}$ , augmented with the escape velocity as the asteroid falls into our gravitational potential well . this is very unlikely for asteroids , but it is possible for comets . so , let us assume a typical impact velocity $v_\text{i}=16\ , \text{km/s}$ , a mass $m_a = 0.1m_\oplus$ and an impact angle $\theta=45^\circ$ . we find $$ \begin{align} u_{t , \oplus} and = 30.813\ ; \text{km}\ , \text{s}^{-1} , \\ u_{r , \oplus} and = -1.0285\ ; \text{km}\ , \text{s}^{-1} , \\ e ' and = -411.87\ ; \text{km}^2\ , \text{s}^{-2} , \\ h'^2 and = 2.1248\times 10^{19}\ ; \text{km}^4\ , \text{s}^{-2} , \\ a ' = -\frac{\mu}{2e'} and = 1.61109\times 10^8\ ; \text{km} , \\ e ' = \big [ 1- h'^2/ ( \mu a' ) \big ] ^{1/2} and = 0.0788 , \\ r_\text{p} = a' ( 1-e' ) and = 1.48411\times 10^8\ ; \text{km} , \\ r_\text{a} = a' ( 1+e' ) and = 1.73807\times 10^8\ ; \text{km} , \end{align} $$ with $r_\text{p}$ and $r_\text{a}$ perihelion and aphelion . evidently , the influence on the earth 's orbit is substantial . in the case of a direct-from-behind collision , we get $\theta=0^\circ$ , $v_\text{i}=11.2\ , \text{km/s}$ , so that $$ \begin{align} u_{t , \oplus} and = 30.803\ ; \text{km}\ , \text{s}^{-1} , \\ u_{r , \oplus} and = 0\ ; \text{km}\ , \text{s}^{-1} , \\ e ' and = -412.72\ ; \text{km}^2\ , \text{s}^{-2} , \\ h'^2 and = 2.1234\times 10^{19}\ ; \text{km}^4\ , \text{s}^{-2} , \\ a ' = -\frac{\mu}{2e'} and = 1.60778\times 10^8\ ; \text{km} , \\ e ' = \big [ 1- h'^2/ ( \mu a' ) \big ] ^{1/2} and = 0.0695 , \\ r_\text{p} = a' ( 1-e' ) and = 1.49598\times 10^8\ ; \text{km} , \\ r_\text{a} = a' ( 1+e' ) and = 1.71958\times 10^8\ ; \text{km} . \end{align} $$ as expected , the radius at impact has become the perihelion , and the change in eccentricity is lowest . and just for fun , let 's try the worst-case scenario : $\theta=180^\circ$ , $v_\text{i}=72\ , \text{km/s}$: $$ \begin{align} u_{t , \oplus} and = 23.239\ ; \text{km}\ , \text{s}^{-1} , \\ u_{r , \oplus} and = 0\ ; \text{km}\ , \text{s}^{-1} , \\ e ' and = -617.10\ ; \text{km}^2\ , \text{s}^{-2} , \\ h'^2 and = 1.2086\times 10^{19}\ ; \text{km}^4\ , \text{s}^{-2} , \\ a ' = -\frac{\mu}{2e'} and = 1.07530\times 10^8\ ; \text{km} , \\ e ' = \big [ 1- h'^2/ ( \mu a' ) \big ] ^{1/2} and = 0.391 , \\ r_\text{p} = a' ( 1-e' ) and = 0.65462\times 10^8\ ; \text{km} , \\ r_\text{a} = a' ( 1+e' ) and = 1.49598\times 10^8\ ; \text{km} , \end{align} $$ so that the radius at impact has become the aphelion , and the change in eccentricity is highest . although i wonder how much would be left of the earth after such an apocalyptic event . . . if the collision is not central , then part of the energy will be transferred to the axial rotation of the earth , which should reduce the effect on the orbit . but that will be more difficult to quantify .
lubo 's answer is of course perfectly correct . i will try to give you some examples why the straightest line is physically motivated ( besides being mathematically exceptional as an extremal curve ) . image a 2-sphere ( a surface of a ball ) . if an ant lives there and he just walks straight , it should be obvious that he will come back where he came from with his trajectory being a circle . imagine a second ant and suppose he will start to walk from the same point as the first ant and at the same speed but into a different direction . he will also produce circle and the two circles will cross at two points ( you can imagine those circles as meridians and the crossing points as a north resp . south poles ) . now , from the ants ' perspective who are not aware that they are living in a curved space , this will seem that there is a force between them because their distance will be changing in time non-linearly ( think about those meridians again ) . this is one of the effects of the curved space-time on movement on the particles ( these are actually tidal forces ) . you might imagine that if the surface was not a sphere but instead was curved differently , the straight lines would also look different . e.g. for a trampoline you will get ellipses ( well , almost , they do not close completely , leading e.g. to the precession of the perihelion of the mercury ) . so much for the explanation of how curved space-time ( discussion above was just about space ; if you introduce special relativity into the picture , you will get also new effects of mixing of space and time as usual ) . but how does the space-time know it should be curved in the first place ? well , it is because it obeys einstein 's equations ( why does it obey these equations is a separate question though ) . these equations describe precisely how matter affects space-time . they are of course compatible with newtonian gravity in low-velocity , small-mass regime , so e.g. for a sun you will obtain that trampoline curvature and the planets ( which will also produce little dents , catching moons , for example ; but forget about those for a moment because they are not that important for the movement of the planet around the sun ) will follow straight lines , moving in ellipses ( again , almost ellipses ) .
it sometimes works and sometimes does not . making a foam costs energy because it increases the area of the air/water interface , so foams are thermodynamically unstable . the reason a foam develops is because surface active molecules are present , and these adsorb at the air/water interface . the surfactant stabilises the air/water interface because the rate it desorbs from the interface is slow i.e. they create a kinetic barrier to collapse of the bubble walls ( nb it is a kinetic barrier : the foam is still thermodynamically unstable even when a surfactant is present ) . an obvious example of a surface active agent creating a foam is bubble bath , which typically contains the surfactant sodium lauryl ether sulphate . sles is a synthetic compound , but many foods contain natural surfactants like lipids and proteins . this is why boiling pasta creates a foam - if you tried boiling distilled water you would find that it does not foam . so what then is the role of the spoon . well it does two things : firstly remember that the foam is thermodynamically unstable , so if you can puncture the bubble wall the bubble will collapse just like a burst balloon . typically foam bubbles can be punctured by touching them with something hydrophobic . automatic washing powders include silicone oil ( typically dispersed onto silica ) to burst the foam and stop your automatic washing machine from foaming at the mouth . lab demonstrations usually demonstrate the effect using ptfe powder . the mechanism of the bubble bursting is simply that the water contact angle on a hydrophobic surface is very large so the water retreats from the contact . so if the surface of your spoon is hydrophobic it will burst any bubbles that touch it . however if whatever you are boiling has a powerful surfactant the surfactant will simply adsorb onto the wooden spoon and render it hydrophilic , and the bubbles will stop bursting . so how well the spoon bursts bubbles depends on the spoon and on what you are boiling . the second mechanism is a lot simpler . in the foam from boiling water the bubbles are filled with steam , not air , and if the bubble touches anything that is below 100c the steam will immediately condense and this bursts the bubble . you can see this effect by blowing on the boiling foam . assuming your breath is colder than 100c you will see the foam retreat . so the wooden spoon will burst bubbles simply by being cold ( or at least colder than 100c ) . but of course the spoon is above a pan of boiling water so it will eventually heat up and stop bursting the bubbles . once again the spoon may or may not have a big effect depending on how fast it heats up . allegedly metal spoons do not work because they heat up too fast , though i must admit i have never tested this myself .
correct . for a sphere of uniform density , the acceleration drops off linearly . $$g = g_{surface} \frac{r}{r}$$ where $r$ is the location under consideration , $r$ is the radius of the sphere and $r &lt ; r$ . under such a scheme , gravity would be one half that at the surface . the earth is not a uniform sphere though . the outer crust is much less dense than the iron core . approaching this core then allows gravity to increase with depth for a distance before finally decreasing . the gravity of earth wiki page has a graph based on a reference model of the density of the earth with depth . under that model , gravity at half the earth 's radius is just about equal to that at the surface .
$g$ is not necessarily a constant if you consider it as " the gravitational acceleration at point a on earth " , and more so if you consider other planets . $g$ varies around the earth--since the distance from the center of the earth varies . $g$ at mount everest is lesser than $g$ elsewhere . aside from that , $g$ on the moon is approximately one-sixth of $g$ on earth . so $g$ can vary . anyway , one can include relevant dimension-ed constants while doing dimensional analysis . in fact , one has to do so . otherwise , with dimensional analysis , you will get the wrong expression--since by multiplying/dividing by a power of the dimensioned constant ( which you will have to do sooner or later to make it dependant on the constant ) , the dimensions of the result change . aside from this , you may have a two-equations-three-variables moment . a more intuitive reason for why we include dimensioned constants--you can imagine that they changed , and predict the result based on that . in most cases anyway , the constant is not truly a constant , like $g$ . the only " true " constants are $g , c , \hbar , r$ , and parameters of various bodies . and some other stuff i probably forgot .
see the free ( irrotational ) vortex section of the wikipedia article on vortices . the velocity varies linearly with distance not as the square of the distance .
your two questions are not really related , in my thoughts . the first one is about friction of some magnets clutched to a ferromagnetic wall . the second is about failiure of some " wire " . both are strange and unnessecary mixtures of idealized classical mechanics and some real world problem . so , first question is really : does friction ( at rest ) last forever ? and second : does a " wires " stability against rupture last forever ? and answer ( s ) : yes in a surrounding of appropriate idealization , no in real world . georg
deepak 's answer covers the question , except if " simple " means " in terms an english major would understand " . i have been trying to formulate such an answer , fwiw : mechanics covers the macroscopic world , the world we see even with glass microscopes and simple telescopes . it explains the motion and interaction of bodies , from large , to quite small . wave mechanics covers the macroscopic behavior of waves as observed in the sea and the behavior of light . the theories by the beginning of the 20th century were beautifully established so that some physicists thought that real physics was finished , and engineering was all that was left . then came the quanta . they came from various fronts . chemists studying the elements and measuring atomic weights came up with a table of elements that had discrete numbers . the photo electric effect showed that light was not always behaving as a classical wave , because light could hit material and kick out a single electron . then lines were found in the light spectra of elements . this forced physicist to think of energy coming in discrete numbers instead of a continuum , quanta of energy . the first model of an atom , the bohr model , had the nucleus like a miniature sun and the electrons as planets around it . the discrete lines observed though , meant that the orbits had fixed positions . electrons could only occupy certain orbits , get kicked to a higher one and release a photon falling back , with a specific line . classical mechanics could not solve this conundrum : why there were discrete orbits and why the electrons did not fall into the nucleus anyway since the nucleus is positive and the electrons negative and the attraction inevitable classically . physicists were forced to postulate quantum mechanics and developed a whole new set of theories of how the microscopic atomic world worked : in quanta of energy . in this new mathematical theory the electrons stay in orbit around the nucleus because they can only change orbits by quanta of energy . they cannot fall into the positive nucleus because there is a lowest stable ground state , which is the lowest energy an electron can have . an atom can gain a quantum of energy and the electron can jump to a higher energy state ; it can not go lower than the ground state . one can never take a micro photo of the electron , only a probability distribution where it might be , can be computed by the new theories . studying the probability distributions coming out of the solution of quantum mechanical problems , it was found that in the microscopic world particles sometimes behave as waves , and waves ( light ) sometimes behave as particles , depending on the circumstances under study . experiments confirmed all this . quantum mechanics explains beautifully the light spectra of atoms , the periodic table of elements and nuclear interactions and a lot of phenomena , from transistors to lasers . the price payed is a loss of intuitive understanding of " particle " and " motion " , new intuitions have to be developed to understand the predictions of a quantum mechanical world that need long study and perseverance . specifically : quantum physics includes a ) quantum mechanics : solutions of problems with known potential energy using the schrodinger or klein gordon equations . the problems are treated as particles moving in a potential well b ) second quantization , where particles are treated as creation and annihilation operators acting on the vacuum , and c ) quantum strings where one has quantized strings and the particles are energy levels on these strings . d ) whatever new coming down the theoretical pike . ( it is turtles all the way down : ) ) to make any sense of this you have to read further . quantum computers , utilize the knowledge gained by quantum mechanics to create compact computers , and my knowledge is covered by the wikipedia article quantum gravity is an attempt to extend quantum mechanics to general relativity , which is a classical theory . there are computational difficulties in doing this . theorists are aiming at a unified theory of everything , called toe . to get that , one has to quantize gravity , which means that the gravitational field should be coming in quanta , called gravitons . this is ongoing research and connected with string theory research , since , up to now , string theories are the only ones that have come up with both the quantum levels needed to describe particles and also to quantize gravity . deepak 's answer is a good beginning and also sb1 's answer . otherwise start with a quantum mechanics course .
you can find the shortest and easiest derivation of this result in the paper where it was released by einstein himself ( what better reference can you find ? ) in 1905 . it is not the main paper of special relativity , but a short document he added shortly afterwards . a . einstein , ist die trgheit eines krpers von seinem energieinhalt abhngig ? , annalen der physik 18 ( 1905 ) 639 . a pdf file of the english translation does the inertia of a body depend upon its energy-content ? is available here . ( hattip : user53209 . ) it is a delightful document to read . there is no dramatic references to huge power release nor anything similar . he simply states after the derivation " if a body gives the energy away $l$ in form of radiation , then its mass decreases in an amount $l/v^{2}$ ( . . . ) the mass of a body is a measure for its energy content ( . . . ) one can not exclude the possibility that , with the bodies whose energy content changes rapidy , for example radium salts , a proof of the theory will be found ( . . . ) if the theory adjusts to the facts , then the radiation transports inertia between emitters and absorbers . " google for that short paper and see the derivation yourself , it is very easy . the minkowsky four-dimensional spacetime had not yet been incorporated to special relativity , so the equations are formally very simple , easy to follow with little mathematical training .
the standard contact force named the " normal " is only used when the contact forces are small enough not to disturb the rigidity of the object . so basically it is used in mechanics when the conditions allow an object to be treated a rigid . but this is an idealization . if the force is strong enough it can alter the position of the atoms relative to each other , resulting in a deformed or cracked object . the particular force strength ( or rather , pressure is the important variable here ) needed depends on the particular material and there is no simple way to calculate it ( it is easy to measure it though ) .
the time-component ( 0th component ) of the 4-current represents charge . the spatial components represent the 3-vector current .
the highest pressure in the ocean is at the bottom of the mariana trench , where the pressure is 1,086 atmospheres . using the online calculator for the properties on nitrogen at 4c and 1,000 atmospheres the density comes out as 602 kg/m$^3$ , which is still less than water . so a bubble of nitrogen would rise even at the deepest point in the ocean . response to comment : in principle we can continue increasing the pressure and the nitrogen should get denser . however at temperatures above 0c and the sorts of pressures we are talking about nitrogen is a supercritical fluid so it does not obey anything like an ideal gas law . calculating at what point the density would exceed the density of water is far from easy . the effect of pressure on water is straightforward . at sea bottom temperatures ( about 4c ) the density of water increases only slowly with pressure to about 1050 kg/m$^3$ at 6,000 atmospheres , at which point the water freezes to form ice v . so the question is whether the density of nitrogen exceeds 1050 kg/m$^3$ below a pressure of 6000 bar . i can not find any figures for the density of nitrogen at these sorts of pressures and temperatures , though i did find this paper that gives a mie-grneisen type equation relating the density , pressure and temperature . unfortunately the preview only shows two pages and the rest of it is behind a paywall . however using the figures they give and waving my arms around a bit i find the density of nitrogen rises to 1050 kg/m$^3$ at around 4,000 atmospheres . so , it might just be possible to get a nitrogen bubble that is denser than water and will sink instead of floating . but i do not know whether the equation from the paper i cited is accurate at these sorts of pressures and temperatures , and it is possible the nitrogen will solidify before the water does ( though i would guess not ) .
no , not really . the reason why force x distance is a useful quantity is because work is typically defined on a moving particle : force x distance is really a special case of $w = \int f \ , dx$ . in other words , the particle has to have moved for there to be work . because movement is only one dimensional ( distance ) and not 2d ( area ) , there is no clear interpretation of what force*area is . it is obvious what i mean when i say " a particle has moved 3 cm , " but it is nonsensical to say that " a particle has moved 4 square miles . "
indeed , the only thing you can say is that the phase varies by the magnetic flux quantity after one loop . that is all . since the phase must be periodic , you also know that the flux inside a superconducting loop is quantised in integer number of the flux quantum $\phi_{0}=\pi \hbar/e$ . you do not need to know more , since only a phase-difference matters . you can make the phase evolving the way you want along the loop , the only constraint is that after one turn you have $\delta \chi = \chi\left ( 2\pi\right ) - \chi\left ( 0\right ) = 2\pi n \phi / \phi_{0}$ , with $n$ some integer . physical effects are associated with $\delta \chi$ , never with $\chi$ only . in quantum mechanics , it is the same as saying that the wave function $\left\vert\psi\right\rangle$ is unphysical , but only $\left\vert\left\langle x \vert\psi \right\rangle\right\vert^{2}$ can be measured ( for instance , you can replace $\left\langle x\right|$ by whatever bra you prefer ) . in classical physics , this is the same thing as for a voltage drop : you do not need to know the voltage at any point , you just need to know that at the two end of a wire , you have a non-zero voltage drop . so yes , superconductivity is a quantum mechanics problem at the macroscopic scale .
there is no such thing as the " si unit for intercept of a graph " , just like that . the unit will depend on what precisely you are plotting , as the intercept will have the same units as whatever the y-axis ( vertical axis ) of your plot represents . so , ask yourself , what does my vertical axis represent ? what are the si coordinates of that magnitude ? as an example , if you are plotting a graph of speed as a function of time , with time on the x-axis and speed on the y-axis , the si unit of the intercept will be that of speed ( m/s ) .
yes , taking any object and decreasing its volume , while keeping the mass constant , will result in creating a black hole - the question is how long is it going to " live " , because there is such thing as evaporation due to hawking 's radiation . for example , turning earth into a black hole requires squeezing it down into a ball with the radius of about $8\times 10^{-4} $ meters .
the answer to this question is as simple as you seem to think it is . let 's consider the case of two spatial dimensions for simplicity . let 's take our object to be moving inertially without rotation , and we choose a frame co-moving with the object . additionally , lets choose our origin to be at the center of mass of the object . now suppose we apply an impulse $\mathbf{j}$ . if the mass of the object is $m$ , newton 's laws tell us the final center of mass velocity of the object must be $\mathbf{j}/m$ . this tells us the translational part . next we consider the rotation . we know the angular impulse will depend on the place where the force is applied to the object . if the force is applied a position $\mathbf{r}$ and the object has moment of inertia $i$ , then the angular impulse $k$ will be $\mathbf{r} \wedge \mathbf{j}$ . ( the wedge is basically a cross product ) . the resulting angular velocity about the center of mass is then $k/i$ . your last question makes me think your intuition is not that good . you say how much of the force becomes rotation and how much becomes translation . it does not work like that . all of the force goes into translation and all of the force goes into rotation . if you had a fixed amount of impulse you could generate and wanted to make a bigger rotational speed , you would not have to sacrifice translational speed . in fact , you could not sacrifice translational speed---it will always be $j/m$ . the only way you could get more rotational speed is by using a bigger lever arm , which does not affect the translational speed .
expanding on ron 's comment : $$i ( \nu , t ) d\nu =\frac{2h\nu ^3}{c^2}\frac{d\nu }{e^{\frac{h\nu }{kt}}-1}$$ $$\nu \to \frac{c}{\lambda } , \quad d\nu \to c\frac{d\lambda }{\lambda ^2}$$ $$i ( \lambda , t ) d\lambda =\frac{2h}{c^2}\left ( \frac{c}{\lambda }\right ) ^3\frac{1}{e^{\frac{hc}{\lambda kt}}-1}c\frac{d\lambda }{\lambda ^2}=\frac{2hc^2}{\lambda ^5}\frac{d\lambda }{e^{\frac{hc}{\lambda kt}}-1}$$
a moment is the twist as a result of a force at a distance . go try to loosed the lug nuts of a tire and you will notice that the further away you can push on the wrench , the less effort is needed for the same amount of twisting force . a simpler example is to try to open a ( unlocked and unlatched ) door by pushing on it a various distances from the hinge . try it , and let us know if that gave you a more intuitive feel for moments .
by convention we try to set things up so that it is usually a minimum , but we can not make any definition of the action that would make that hold in all cases . in optics , consider a situation in two dimensions where you have an ellipsoidal cavity with reflecting walls . if you release a ray of light from the center , along the major axis , it gets reflected back to the center by following a path of maximum time . if you start a ray from the center , along the minor axis , it comes back after following a path of minimum time . you can choose the action to be the time or minus the time , but no matter what , one of these rays will be a minimum of the action and the other will be a maximum . in special relativity , you can take the action for a particle to be the proper time $s=\int ds$ ( with $ds$ positive ) , or you can take it to be $-s$ or other possibilities such as $-mcs$ ( landau and lifschitz 's choice ) . it does not matter which sign you choose , because the physical predictions are the same either way . in both of these examples ( optics and sr ) , you can make a choice of sign such that the action is minimized for infinitesimally short trajectories in free space . however , the example of the ellipsoidal cavity shows that you cannot in general make it a minimum for all paths of finite length . in relativity , the metric only defines $ds$ in absolute value , $ds^2=g_{ab}dx^a dx^b$ . also , we had like to be able to talk about timelike , lightlike , and spacelike geodesics . if we choose the action for timelike geodesics to be real , then it has to be imaginary for spacelike ones -- or we could define it as $\int\sqrt{|ds^2|}$ , but the absolute value could be a nuisance because it is not a smooth function . for l and l 's discussion of this , see mechanics ( 3rd ed . ) , section 2 ; and classical theory of fields ( 2nd ed . ) , sections 8 , 53 , and 87 .
the answer is that electrons will sometimes be emitted when two photons , each of less than the work function energy , hit the surface . but there is no special significance to this so you should not leap out of your bath and run naked down the street just yet . the electrons in a metal have a natural oscillation frequency called the plasma frequency . even in total darkness lattice vibrations will randomly transfer energy to the plasma oscillations , and especially at high temperatures random redistributions of this energy may concentrate enough energy in a small volume to eject an electron . this is the phenomenon of thermionic emission , and while the probability of emission is only significant at high temperatures there is in principle a finite probability of electron emission at room temperature . incident photons also transfer energy to plasma oscillations , but they transfer a lot of energy to a small volume so it is much more probable the energy can end up ejecting an electron . even so , the quantum yield for photoemission is typically much less than one because in most cases the energy transferred to the plasma oscillations just leaks away into the bulk . a quick google suggests typical quantum yields are around $10^{-6}$ i.e. only one photon in a million ends up ejecting an electron . the point of all this preliminary rambling is that there is a small probability than even a single sub work function photon will eject an electron because you could get a combined photo/thermionic transfer of energy resulting in emission . the probability of this is exceedingly low , but then there are an exceedingly large number of electrons in metals . given the above , it should be obvious that there will also be a finite probability of two sub work function photons ejecting an electron . but is this really photoemission ? you could argue that the first photon just causes local heating and this increases the chance of the second photon ejecting an electron through combined photo/thermionic emission . so the pedant in me would have to concede that two sub work function photons can eject an electron . however the experimental scientist in me would regard an unmeasurably small probability as being zero , and would suggest that the pedant should get out more .
there is no problem in writing down a theory that contains massless charged particles . simple $\mathcal{l} = \partial_{\mu} \phi \partial^{\mu} \phi^*$ for a complex field $\phi$ will do the job . you might run into problems with renormalization but i do not want to get into that here ( mostly because there are better people here who can fill in the details if necessary ) . disregarding theory , those particles would be easy to observe assuming their high enough density . also , as you probably know , particles in standard model compulsorily decay ( sooner or later ) into lighter particles as long as conservation laws ( such as electric charge conservation law ) are satisfied . so assuming massless charged particles exist would immediately make all the charged matter ( in particular electrons ) unstable unless those new particles differed in some other quantum numbers . now , if you did not mention electric charge in particular , the answer would be simpler as we have massless ( color- ) charged gluons in our models . so it is definitely nothing strange to consider massless charged particles . it is up to you whether you consider electric charge more important than color charge . another take on this issue is that standard model particles ( and in particular charged ones ) were massless before electrosymmetric breaking ( at least disregarding other mechanisms of mass generation ) . so at some point in the past , this was actually quite common .
graphical explanations are most welcomed . this is my personal favorite one of those ( from mtw 's " gravitation" ) . for an animation , see , for example this java applet .
since 1998 or earlier , there have been no doubts that the ads/cft correspondence provides us with a full non-perturbative definition of string theory on the ads-like background , including all of ( type iib ) stringy objects and interactions and subtleties that we have ever heard of . an obvious reason why the cft can not be equivalent " just to supergravity " is that the pure supergravity is inconsistent as a quantum theory while the cft is self-evidently consistent . the basic relationship between the parameters on both sides of the duality is $$g_{\rm string} = g_{\rm ym}^2 , \quad \frac{r^4}{\ell_{\rm string}^4} = g_{\rm ym}^2 n \equiv \lambda $$ so at a fixed $n$ , the weak coupling of the yang-mills side coincides with the weak string coupling in the type iib string theory bulk . when $n$ is allowed to scale to infinity as well , the ' t hooft coupling $\lambda\equiv g_{\rm ym}^2 n$ is what decides whether the loop diagrams are actually suppressed . you see that when $\lambda$ is smaller ( or much smaller ) than one , then the yang-mills expansion is weakly coupled and the perturbative gauge-theory diagrams are guaranteed to approximate physics well ( or very well ) . on the contrary , when $\lambda$ is greater ( or much greater ) than one , the ads radius $r$ is greater ( or much greater ) than the string length which means that one may approximate the physics by string theory on a " mildly curved " background . in this limit , when the curvature radius is ( much ) longer than the string length , it is always possible to approximate low-energy physics of string theory by supergravity . in string theory , the sugra approximation means to neglect the $\alpha'$ stringy corrections . in the gauge-theoretical language , it means to focus on the planar limit for large $\lambda$ and neglect $1/n$ nonplanar corrections . however , it is been demonstrated that all the " beyond supergravity " states you expect to see in the type iib background appear on both sides of the ads/cft correspondence , including arbitrary excited strings )  this is particularly clear in the bmn/pp-wave limit ( see also 1,000+ followups )  as well as various wrapped d-branes and , what is critical for the usefulness of the whole ads/cft framework , evaporating quantum black holes .
the first term in the four-momentum is $\gamma m c$ as you have said , but given $c$ is constant we can multiply both sides by $c$ and pull a $c$ into the derivative . this does not really matter anyway as it is not required for a derivation . an easy place to see where the last equation comes from is the following . special relativity gives us the relativistic energy relation $e = \gamma m c^2$ ( mass + kinetic energy ) , where $\gamma = \frac{1}{\sqrt{1-v^2/c^2}}$ . the lhs of your equation is thus the rate of change of relativistic energy with respect to coordinate time , $t$ . what is this ? well we know from the lorentz force law that ( in the absence of a magnetic field ) : $$\vec{f} = q\vec{e}~~~ ( = -q\vec{\nabla}\phi ) $$ at any point along the path travelled by this charged particle , we can find the instantaneous work done on the charged particle due to $\vec{e}$ , in moving an infinitesimal distance tangent to the path , described by $d\vec{x}$ . this is given by $$dw = \vec{f}\cdot d\vec{x} = q\vec{e}\cdot d\vec{x} $$ by energy conservation , this work done ( $dw$ ) is the energy gained by the charged particle ( $de$ ) . so we have $$\frac{d ( \gamma m c^2 ) }{dt} = \frac{de}{dt} = q\vec{e}\cdot \frac{d\vec{x}}{dt} = q\vec{e}\cdot \vec{v} $$ where $\vec{v}$ is the instantaneous velocity of the particle . note that this assumes that all of the energy is going into the motion of the charge particle , it does not take into account radiated energy .
the energy comes from the ice-skater 's muscles ; they have to work to pull their arms in . there is no external work done on the skater - the energy is converted from the chemical potential energy stored in the skater 's body to kinetic energy .
the operation you mentioned $$\delta \hat{a} = \hat{a}-\langle \hat{a}\rangle \hat{i}$$ is just shifting of the annihilation operator . typically people even drop the identity and write $$\hat{a}_{new}=\hat{a}_{old}-\alpha_0 , $$ where $\alpha_0$ is a complex number . in your question the shift is such that $\langle \hat{a}_{new} \rangle = 0$ , which may be convenient for calculations . in particular when the pump beam has many photons , the quantum part ( i.e. . related to $\hat{a}_{new}$ ) may be neglected . additionally , $\hat{a}_{new}$ is an annihilation operator with the same anti-commutation relations as $\hat{a}_{old}$ . from mathematical point of view it is a perfectly legit operation . moreover , both operators have the same domain , and spectrum only shifted by $\alpha_0$ . to see that domain is the same take any $|\psi\rangle \in \text{dom} ( \hat{a}_{old} ) $ . then denoting $|\phi\rangle := \hat{a}_{old} |\psi\rangle$ , we check that $\hat{a}_{new} |\psi\rangle = |\phi\rangle-\alpha_0|\psi\rangle$ is a well-defined vector .
you can not derive the existence or magnitude of diamagnetic properties just from unpaired electrons . on the contrary , diamagnetism mostly comes from the complete shells . they behave as electric current loops that orient themselves in a certain way in the external magnetic field . copper has lots of these complete shells , so the diamagnetic contribution is large . there is also the opposite contribution from the unpaired electron but it is just one electron and the paramagnetic contributions " scale " with the number of these electrons and one is too few . so the diamagnetic terms win .
the idea is that there is no fundamental physical limit to how little energy is required to open and close the door . so you can make it arbitrarily small , and if you make it small enough , the demon still contradicts the second law of thermodynamics . this definitely is not apparent at first sight , but i believe this is one of the first things people checked in attempts to resolve the paradox . if it had been this easy to resolve , it would not have stayed open for as long as it did . the rather non-intuitive resolution of the maxwell demon paradox is that you need $ ( ln 2 ) k t$ energy to erase each bit of information from the demon 's memory . see landauer 's principle and this stack-exchange question .
it had a tangential velocity and its weight was enough to provide the centripetal force for motion .
1 . e . zeidler , quantum field theory i basics in mathematics and physics , springer 2006 . http://www.mis.mpg.de/zeidler/qft.html is a book i highly recommend . it is the first volume of a sequence , of which not all volumes have been published yet . this volume gives an overview over the main mathematical techniques used in quantum physics , in a way that you cannot find anywhere else . it is a mix of rigorous mathematics and intuitive explanation , and tries to build ''a bridge between mathematiciands and physicists'' as the subtitle says . it makes very interesting reading if you know already enough math and physics , and gives plenty of references as entry points to the literature for topics on which your background is meager . as regards to your request for high level mathematics ( in the specific form of pseudo-differential operators , etc . ) , zeidler discusses - as section 12.5 - on 28 ( of 958 total ) pages microlocal analysis and its use , though there is only two pages specifically devoted to pdo ( p . 728-729 ) , but he says there ( and emphasizes ) that ''fourier integral operators play a fundamental role in quantum field theory for describing the propagation of physical effects'' - so you can expect that they play a more prominent role in the volumes to come . but , of course , pdo are implicit in all serious high level mathematical work on quantum mechanics even without mentioning them explicitly , as for example the hamiltonian in the interaction representation , $h_{int}=e^{-ith_0}he^{ith_0}$ , is a pdo . work on wigner transforms is work on pdos , etc . . 2 . other books using pdo , much more specialized : g . b . folland , harmonic analysis in phase space a.l. carey , motives , quantum field theory , and pseudodifferential operators a . juengel , transport equations for semiconductors c . cercignani and e . gabetta , transport phenomena and kinetic theory n.p. landsman , mathematical topics between classical and quantum mechanics m . de gosson , symplectic geometry and quantum mechanics p . zhang , wigner measure and semiclassical limits of nonlinear schroedinger equations 3 . finally , as an example of a book that ''is strictly supported by mathematics ( given a set of mathematically described axioms , the author develops the theory using mathematics as a main tool ) '' , i can offer my own book a . neumaier and d . westra , classical and quantum mechanics via lie algebras .
it expands . take a normal disc , and draw a circle on it . the circle expands . take a new disc , draw a hole , and make a score on that circle . it still expands . repeat with deeper scoring . it still expands . keep making it deeper . it still expands . at one point , your razor ( or whatever ) will start poking through the metal . it still expands . cut out the circle , but leave the cut out piece in the hole . now , does not it seem logical that the circle+hole will expand ? remove the circle , it still expands . so , to answer your question , a hole in a material behaves just like a circle of that same material . it expands on heating . what is actually happening is that if it tries to expand inwards ( contract basically ) , it will have to compress itself , and increase its density . there will be resistance to it , so it tries to expand into a free region , namely the outside . and the inside expands just because it does when there is no hole .
this web page provides a good explanation : http://www.thebigview.com/spacetime/spacetime.html to oversimplify the explanation , you have to understand the curvature of space time around a black hole . the basic principle is that because of the curvature of spacetime around a black hole , the amount of " distance " a beam of light has to cover is greater near a black hole . however , to an observer in that gravitational field , light must appear to always be 300,000 km/sec , time has to slow down for that individual as compared to someone outside that gravitational field as related by the time/distance relationship of speed . or as the web page says : if acceleration is equivalent to gravitation , it follows that the predictions of special relativity must also be valid for very strong gravitational fields . the curvature of spacetime by matter therefore not only stretches or shrinks distances , depending on their direction with respect to the gravitational field , but also appears to slow down the flow of time . this effect is called gravitational time dilation . in most circumstances , such gravitational time dilation is minuscule and hardly observable , but it can become very significant when spacetime is curved by a massive object , such as a black hole . a black hole is the most compact matter imaginable . it is an extremely massive and dense object in space that is thought to be formed by a star collapsing under its own gravity . black holes are black , because nothing , not even light , can escape from its extreme gravity . the existence of black holes is not yet firmly established . major advances in computation are only now enabling scientists to simulate how black holes form , evolve , and interact . they are betting on powerful instruments now under construction to confirm that these exotic objects actually exist . this web page provides a large series of links for further research into the subject : http://casa.colorado.edu/~ajsh/relativity.html
the paradox is , sort of , resolved as follows : the number of photons changes when you switch between non-inertial frames . this is actually a remarkable fact , and holds also for quantum particles , which can be created in pairs-antipairs , and whose number depends on the frame of reference . now , a step back . forget about gravity for a moment , as it is irrelevant here ( we are still in gr , though ) . imagine a point charge , which is accelerating with respect to a flat empty space . if you switch to the rest frame of the charge , you observe a constant electric field . when you switch back to the inertial frame , you see the field changing with time at each point . this naturally corresponds to appearing magnetic fields , and hence radiation . in the presence of gravity the case is absolutely similar . to conclude , switching between non-inertial frames makes a static electric field vary and hence represent a radiation flow . another point might be : when moving with charge , no energy is emitted , but when standing in the lab frame , there is a flux observed . however , there is no contradiction here as well , as the energy as a quantity is not defined for noninertial frames .
there is no way to be 100% sure , but we can put upper limits on the mass . massless particles do not have a rest frame , so it does not make sense to talk about time dilation in the photon 's frame . a massive photon would have a rest frame , so you could eventually catch up to it and move alongside it . list of experimental limits on photon mass more comprehensive list
there is a simple way to look at this that does not involve any maths . suppose the two cars are parked and are stationary , and you accelerate past them in your car . if you are accelerating forwards then from your perspective it looks as if the two cars are accelerating backwards ( at the same rate ) . but the cars are at rest , so the distance between them can not be changing . this means that if two objects accelerate at exactly the same rate the distance between them will not change . it is a simple extension of this to answer your question about velocities . suppose now the two cars are travelling at the same velocity , and again you accelerate past them in your car . once again it appears to you as if the two cars are accelerating backwards . but we know they are just travelling at the same speed , and this means that if two objects are accelerating at exactly the same rate their relative velocity does not change . proving this mathematically is easy for us physics nerds , though possibly less so for non-physicists . shout if you want me to show the maths and i will add it to this answer .
i think you had describe the display in that clip as a volumetric display rather than a hologram . as the wikipedia will tell you , there have been basic examples of such displays created but at the moment nothing like the display in the clip exists . the problem is how to get some medium in your display to emit light . the example in the wikipedia article uses a laser to heat air until it forms a plasma . however this is only monochrome and of limited resolution . try a google image search for " volumetric display " for a selection of current attempts , but be prepared for disappointment !
the pauli exclusion principle can be stated as " two electrons cannot occupy the same energy state " , but this is really only a rough way of stating it . it is more precise to say that the wavefunction of a system is anti-symmetric with respect to exchange of two electrons . the trouble is that now i have to explain to a non-physicist what " anti-symmetric " means and that is hard without going into the maths . i will have a go at doing this below . anyhow , brian cox is being a bit liberal with the truth because i am not sure it makes sense to say the electrons in his bit of diamond and electrons in far away bits of the universe can be described by a single wavefunction . if this is not a good description then the pauli exclusion principle does not have any meaning for the system . suppose you have two electrons in an atom or some other small system . then that system is described by some wavefunction $\psi ( e_1 , e_2 ) $ where i have used $e_1$ and $e_2$ to denote the two electrons . the pauli exclusion principle states : $$\psi ( e_1 , e_2 ) = -\psi ( e_2 , e_1 ) $$ that is if you swap the two electrons $\psi$ changes to $-\psi$ . but suppose the two electrons were exactly the same . in that case swapping the electrons cannot change $\psi$ because they are identical . so we had have : $$\psi ( e_1 , e_2 ) = \psi ( e_2 , e_1 ) $$ but the exclusion principle states : $$\psi ( e_1 , e_2 ) = -\psi ( e_2 , e_1 ) $$ therefore if both are true : $$\psi ( e_2 , e_1 ) = -\psi ( e_2 , e_1 ) $$ ie $$\psi = -\psi$$ the only way you can have $\psi = -\psi$ is if $\psi$ is zero , which means $\psi$ does not exist . this is why if the pauli exclusion is true , two electrons can not be identical i.e. they can not be in the same energy state . but this only applies because i could write down a wavefunction $\psi$ to describe the system . when systems become large , e.g. two footballs in a swimming pool instead of two electrons in an atom , it is not useful to try and write a wavefunction to describe the system and the exclusion principle does not apply . nb this does not mean the exclusion principle is wrong , it just means it does not apply to that system .
in cartesian coordinates , a vector with magnitude 0 will not have a direction . however , in polar/spherical coordinates ( or almost any system with angular coordinates ) , you could arbitrarily create a vector with r=0 but with $\theta$ or $\phi$ being some angle . then it would technically have zero magnitude with an arbitrary direction , but this is both meaningless and , as john previously stated , it is not a point . to reference your example , no , you cannot say point b has a direction toward the +x-axis because it has no component in that direction . the only way you can claim something has a certain direction is if its coordinates are such that everyone would agree on the direction . with your example , you could say point b is in +x direction , but i would say it looks more like the -z direction . since we can not agree , it can not have a direction .
as i talked in the comments . physicists are rarely concerned with " current technology level " and are more interested in ultimate laws of physics . there is no physical law , which places an ultimate limit on the size an weight of an aircraft . you could reduce it to one atom , if you can call one atom an aircraft :p . it all comes down to engineering and not physics . i managed to find this article though : supersonic unmanned aerial vehicles close to becoming reality
1 hp = 745.7 watts . 1 watt is a joule/second , which corresponds to the measure of the rate of energy . 27 hp= 20133 joules/second . the output of a 27 horse power engine is 20133 joules per second . the engine of a car is rated a t 50- 120 kw . this engine can make work 672 light bulbs of 30 w . your analogy is quite right if the pressure of the piston is independent of time .
we are simply treating the lie algebra of the relevant lie group here purely as a vector space and making linear transformations on that linear space . since $\operatorname{tr} ( x\ , y ) = \operatorname{tr} ( y\ , x ) $ is generally true , the matrix of the trace is symmetric . the $l_{a , b}$ are like generalized rotations and , as long as they have nonsingular matrices , keep all the information of the lie algebra . some of georgi 's comments i do not think are general . the form he is defining is the killing form in the adjoint representation and it is not always an inner product . he is assuming that the group concerned ( i ) has a finite centre and ( ii ) is compact , for we have the following remarkable theorem : given that a lie group has a finite centre , the killing form on a group 's lie algebra is negative definite if and only if the group is compact . a good reference for this is : s . helgason " differential geometry lie groups and symmetric spaces " chap . ii , section 6 , prop . 6.6 . i love this theorem - it is really quite spesh when you think about it - telling us as it does something about the group 's global properties from information encoded locally ( in the lie algebra ) . so the killing form is the negative of an inner product for compact groups with finite centres . once we have an inner product , we can of course define orthogonality , orthonormality and unitary transformations of the lie algebra . although i have not seen this before , this is going to be how the diagonalisation you speak of can be done . once you have an inner product , the gramm-schmidt procedure can be worked through , and this is how your $l_{a , b}$ are going to be derived . for the unitary groups ( which i suspect georgi is dealing with - we are talking about prof su ( 5 ) / so ( 10 ) here ! ) , the negative of the killing form is even more readily seen to be an inner product : $$\left&lt ; x , \ , y\right&gt ; = \operatorname{tr} ( x^\dagger\ , y ) = -\operatorname{tr} ( x\ , y ) $$ because of course the lie algebra members are skew-hermitian .
op wrote ( v1 ) : so the torque should not be measured in nm but radnm . would that then be completely consistent ? no , that would not be consistent with the elementary definition of torque $\vec{\tau}=\vec{r} \times \vec{f}$ as a cross-product between a position vector $\vec{r}$ and a force vector $\vec{f}$ . an angle in radians is the ratio between the length of a circle arc and its radius , and is therefore dimensionless . for instance , the angular version $\tau = i \alpha$ of newton 's 2nd law is only true ( without an extra conversion factor ) if the angle behind the angular acceleration $\alpha$ is measured in radians . however , it should be mentioned that due to the formula $$ w~=~\int \tau ~d\theta , $$ for angular work , torque can be viewed as energy per angle , i.e. , the si unit of torque is also joules per radians . see also this wikipedia page and this phys . se question .
more blades give you more cost , but very little increase in efficiency . three blades turns out to be the optimum . with four or more blades , costs are higher , with insufficient extra efficiency to compensate . edit : prompted by a comment , here 's some elucidation - this is more expensive per unit electricity generated , if you go for more , but shorter , blades : if you have 4 shorter blades ( rather than three longer ones ) , the blades are sweeping through a smaller volume of air ( i.e. . an amount of air with a lot less energy ) , swept area being proportional to the square of the radius . and the efficiency is only a few percent higher . you get higher mechanical reliability with three blades than with two : with two blades , the shadowing effect of tower and blade puts a lot of strain on the bearings . so although it costs more to make a three-bladed turbine , they tend to have a longer life , lower maintenance needs , and thus on balance reduce the unit cost of electricity generated , as the increased availability and reduced maintenance costs outweigh the extra cost of the third blade . edit2: for the nitty-gritty of wind-turbine aerodynamics , wikipedia is not a bad place to start : http://en.wikipedia.org/w/index.php?title=wind_turbine_aerodynamicsoldid=426555179
the equations you need are given in john baez 's article on the relativistic rocket . if $t$ is the time measured on the rocket , and $v$ is the velocity measured on the rocket , then the equation you need is : $$ v = c \tanh \left ( \frac{at}{c} \right ) $$ for a rocket accelerating at $g$ the velocity time looks like : so , as you guessed , option ( 1 ) is the correct answer . bear in mind that the crew of the rocket assume they are stationary and that the rest of the universe is accelerating towards them . their situation is as if they were stationary in a constant gravitational field of $g$ and were observing everything else falling towards them in that field . it should come as no surprise that they do not observe any superluminal velocities . if you are interested in taking this further , the coordinate system for a uniformly accelerating observer is called the rindler coordinates . incidentally , one of the few things observers in two frames will agree on is their relative speed ( measured as the two observers meet i.e. when they are at the same point in space ) . you will find that the time dilation and length contraction balance each other out to give both frames the same relative speed .
the pressure at a , b , and c are the same . the pressure in the water in the jug above the outside water surface is below your ambient air pressure . if you had a long pipe instead of a jug and you kept pulling it out of the water , eventually the pressure at the top of the pipe would reach 0 and the water column would stay in place even if more pipe was raised . there would be a near vacuum in the pipe above the water ( it would be at the vapor pressure of the water at whatever temperature it was at the top of the column ) . think of this another way . if the pressure at b were higher than at a and c , then the water at b would flow to a and c . since the water would be still if you waited for the waves to die down , all the water at the same height is at the same pressure .
this thread(physicsforums.com) contains a link to shouryya ray 's poster , in which he presents his results . so the problem is to find the trajectory of a particle under influence of gravity and quadratic air resistance . the governing equations , as they appear on the poster : $$ \dot u ( t ) + \alpha u ( t ) \sqrt{u ( t ) ^2+v ( t ) ^2} = 0 \\ \dot v ( t ) + \alpha v ( t ) \sqrt{u ( t ) ^2 + v ( t ) ^2} = -g\text , $$ subject to initial conditions $v ( 0 ) = v_0 &gt ; 0$ and $u ( 0 ) = u_0 \neq 0$ . thus ( it is easily inferred ) , in his notation , $u ( t ) $ is the horizontal velocity , $v ( t ) $ is the vertical velocity , $g$ is the gravitational acceleration , and $\alpha$ is a drag coefficient . he then writes down the solutions $$ u ( t ) = \frac{u_0}{1 + \alpha v_0 t - \tfrac{1}{2 ! }\alpha gt^2 \sin \theta + \tfrac{1}{3 ! }\left ( \alpha g^2 v_0 \cos^2 \theta - \alpha^2 g v_0 \sin \theta\right ) t^3 + \cdots} \\ v ( t ) = \frac{v_0 - g\left [ t + \tfrac{1}{2 ! } \alpha v_0 t^2 - \tfrac{1}{3 ! } \alpha gt^3 \sin \theta + \tfrac{1}{4 ! }\left ( \alpha g^2 v_0 \cos^2 \theta - \alpha^2 g v_0 \sin \theta\right ) t^4 + \cdots\right ] }{1 + \alpha v_0 t - \tfrac{1}{2 ! }\alpha gt^2 \sin \theta + \tfrac{1}{3 ! }\left ( \alpha g^2 v_0 \cos^2 \theta - \alpha^2 g v_0 \sin \theta\right ) t^3 + \cdots}\text . $$ from the diagram below the photo of newton , one sees that $v_0$ is the inital speed , and $\theta$ is the initial elevation angle . the poster ( or at least the part that is visible ) does not give details on the derivation of the solution . but some things can be seen : he uses , right in the beginning , the substitution $\psi ( t ) = u ( t ) /v ( t ) $ . there is a section called " . . . e der bewegung " . the first word is obscured , but a qualified guess would be " erhaltungsgre der bewegung " , which would translate as " conserved quantity of the motion " . here , the conserved quantity described by david zaslavsky appears , modulo some sign issues . however , this section seems to be a subsection to the bigger section " aus der lsung ablesbare eigenschaften " , or " properties that can seen from the solution " . that seems to imply that the solution implies the conservation law , rather than the solution being derived from the conservation law . the text in that section probably provides some clue , but it is only partly visible , and , well , my german is rusty . i welcome someone else to try to make sense of it . also part of the bigger section are subsections where he derives from his solution ( a ) the trajectory for classical , drag-free projectiles , ( b ) some " lamb-nherung " , or " lamb approximation " . the next section is called " verallgemeneirungen " , or " generalizations " . here , he seems to consider two other problems , with drag of the form $\alpha v^2 + \beta$ , in the presence of altitude-dependent horizontal wind . i am not sure what the results here are . the diagrams to the left seem to demonstrate the accuracy and convergence of his series solution by comparing them to runge-kutta . though the text is kind of blurry , and , again , my german is rusty , so i am not too sure . here 's a rough translation of the first part of the " zusammanfassung und ausblick " ( summary and outlook ) , with suitable disclaimers as to the accuracy : for the first time , a fully analytical solution of a long unsolved problem various excellent properties ; in particular , conserved quantity $\rightarrow$ fundamental [ . . . ] extraction of deep new insights using the complete analytical solutions ( above all [ . . . ] perspectives and approximations are to be gained ) convergence of the solution numerically demonstrated solution sketch for two generalizations edit : two professors at tu dresden , who have seen mr ray 's work , have written some comments : comments on some recent work by shouryya ray there , the questions he solved are unambiguously stated , so that should answer any outstanding questions . edit2: i should add : i do not doubt that shouryya ray is a very intelligent young man . the solution he gave can , perhaps , be obtained using standard methods . i believe , however , that he discovered the solution without being aware that the methods were standard , a very remarkable achievement indeed . i hope that this event has not discouraged him ; no doubt , he will be a successful physicist or mathematician one day , should he choose that path .
the whirl is due to the net angular momentum the water has before it starts draining , which is pretty much random . if the circulation were due to coriolis forces , the water would always drain in the same direction , but i did the experiment with my sink just now and observed the water to spin different directions on different trials . the coriolis force is proportional to the velocity of the water and the angular velocity of earth . earth 's angular velocity is $2\pi/24\ {\rm hours}$ , or about $10^{-4}\ s^{-1}$ . if water 's velocity as it drains is $v$ the coriolis acceleration is about $10^{-4} v\ s^{-1}$ . the water moves about a meter while draining , which takes a time $1\ m/v$ , so the total velocity imparted by coriolis forces could be at most $10^{-4} v\ s^{-1} * 1\ m/v = 10^{-4} \ m/s$ . so the coriolis effect is quite a small effect . but this first-order coriolis effect does not cause the water to rotate . the direction of coriolis force depends on your direction of motion . all the water in your tub is moving the same direction , so the coriolis force pushes it all the same direction . the effect is that if the bathtub starts out perfectly flat and begins draining ( and it points north ) , all the water will get pushed east . the two edges of the tub will have very slightly different depths of water , because the coriolis force is pushing sideways . the coriolis force could create " spinning " on uniformly-moving water , but only as a second-order effect . as you move away from the equator , the coriolis force changes . this change in the coriolis force is because the angle between " north " and the angular velocity vector of earth changes as you move around ; as you go further north ( in the northern hemisphere ) the " north " direction gets closer and closer to making a right angle with the angular velocity vector , so the coriolis force increases in strength . the size of this effect would be proportional to the ratio of the size of your tub to the radius of earth . that ratio is $10^{-7}$ , so this effect is completely negligible . the coriolis force could also create some " spinning " if different parts of the water are moving different speeds . if the tub is draining to the north in the northern hemisphere , and water near the drain is moving faster than water far away , then the water near the drain would be pushed east more than water far away is . if you subtracted out the average effect of the coriolis force , what remained would be an easterly push near the drain and a westerly push far away . this gives a clockwise spin as viewed from above . we have already estimated the typical velocities as $\omega l$ , so the angular momentum per unit mass induced this way would be on the order of $\omega l^2$ ( but maybe smaller by a factor of 10 ) . that is only $10^{-4}\ m^2/s$ . to get an equivalent effect , in a tub of $100\ l$ , you could give just one liter of water on the edge of the pool a velocity of a few cm/s , something you surely do many time over when removing your body from the tub . this effect is too small to affect your bathtub , but it is still observable under the right conditions . according to wikipedia , otto tumlirz conducted several experiments in the early 20th century that demonstrated the effects of the coriolis forces on a draining tub of water . the tub was allowed to settle for 24 hours in a controlled environment before the experiment began . this was enough to damp out the residual angular momentum left over from filling the tub up to the point where coriolis effects were dominant .
it would be exactly the same , ( atleast in newtionian picture ) , no gravitational fields outside planets radius would change . the easiest way to see this i think is to use the gravitational analogoue of gauss law . since we have spherical symmetry in both cases int g da = g*4*pi*r^2 ~ m so g is constant . see http://en.wikipedia.org/wiki/gauss ' s_law
as far as we know the electron is a point particle - this is addressed in the question qmechanic suggested : what is the mass density distribution of an electron ? however an electron is surrounded by a cloud of virtual particles , and the experiments in the links you provided have been studying the distribution of those virtual particles . in particular they have been attempting to measure the electron electric dipole moment , which is determined by the distribution of the virtual particles . in this context the word shape means the shape of the virtual particle cloud not the shape of the electron itself . the standard model predicts that the cloud of virtual particles is spherically symmetric to well below current experimental error . however supersymmetry predicts there are deviations from spherical symmetry that could be measurable . the recent experimentals have found the electric dipole moment to be zero , i.e. the virtual particle cloud spherically symmetric , to an accuracy that is challenging the supersymmetric calculations . however there are many different theories based upon supersymmetry , so the result does not prove supersymmetry does not exist - it just constrains it .
the integration sign in $\delta i$ is there because the integration sign was there in the original $i$ to start with . the term " variation " means the addition of $\delta$ in front of the object . it means we study an infinitesimal differential of the object ; the rules that obey the variation are identical to those for other derivatives including , for example , the leibniz rule for the variation of a product . the only possible way how the integral sign could disappear would be if we were taking the derivative of the function $i$ with respect to $t_2$ or $t_1$ ( the upper or lower limit ; the lower limit would pick a natural minus sign ) . but the variation is not a derivative with respect to a particular variable such as $t_2$ , the upper limit . it is the object that knows about the derivatives of $i$ with respect to everything that can vary . the main thing we want to vary are the values of $\delta q ( t ) $ for any value of $t$ , not just a single upper limit $t_2$ .
yes , it is possible . though , i am sure the engineers of the sears tower took that into account . catastrophic failure of the rod is pretty straightforward . lighting is a ton of electrical current , and the rod has some resistance . current through a resistance makes heat by joule heating , which says that the power is proportional to the resistance times the square of current . a big strike can have currents up to 120 ka . the resistivity of copper is about $1.68 \cdot 10^{8} \omega \mathrm m$ , so the resistance of a typical ground rod used in us residential construction ( 5/8 in in diameter , 8 feet long ) is something like : $$ \frac{1.68 \cdot 10^{8} \omega \mathrm m \cdot 2.5 \mathrm m}{2\cdot 10^{-4} \mathrm m^2} = 0.21 \mathrm m \omega $$ now , lighting is a pretty fast pulse , so a lot of the energy will be high frequency , so our copper rod will be subject to skin effect , which will effectively increase the resistance by some factor dependent on frequency . to keep our estimate brief let 's skip the full fourier analysis on lighting and just say the resistance increases by an order of magnitude . so the resistance of a ground rod is something like $ 2 \mathrm m \omega $ . now according to joule , the power in the rod is : $$ 2\mathrm m \omega ( 120 \mathrm{ka} ) ^2 = 28800 \mathrm{kw}$$ wow ! but , lighting strikes are also really brief . a big strike transfers maybe 350 coulombs of charge . working backwards , if the current were a constant 120ka , then to transfer that much charge would take : $$ \frac{350 \mathrm c}{120 \mathrm{ka}} \approx 3 \mathrm {ms} $$ so all that power for that 3ms means a total energy of : $$ 28800 \mathrm{kw} \cdot 3 \mathrm {ms} = 84 \mathrm{kj} $$ wolfram alpha puts that in perspective as about the energy released by burning two grams of gasoline . now , you would go on to calculate the heat capacity of our grounding rod , and figure out how much hotter this energy would make the rod , and determine if that is enough to melt it . but , just through intuition and experience , i can tell you that with two grams of gasoline you can make a grounding rod pretty hot , but not melt it . of course , there are all kinds of effects for which we are not accounting . lightning strikes are so fast that we must consider the inductance and thermal resistance of everything . the parts of the grounding system that carry most of the current ( the surface of the rod ) will get the hottest fastest . but also , the inductance of the entire grounding system helps to limit the current by spreading it over a longer time ( storing the energy temporarily in the field of the inductance ) . and since power is proportional to the square of the current , moving the same amount of charge over a longer period of time means significantly less joule heating in the conductors . also , sears tower does not have just one little ground rod . it has huge steel piles that go way deeper than 8 feet and are many orders of magnitude more massive . as far as multiple strikes accumulating to cause damage , i doubt it . even in a big storm , the time between strikes is very long relative to the duration of the individual strikes . in this entire time , anything that got hot has time to transfer its heat to less hot things nearby . the bits in the ground have the whole earth as a heat sink . the bits above ground are very massive . this might seem counter-intuitive , since after all , lightning is really powerful , right ? i mean , it is super bright and loud . but consider , most of the energy in the strike goes into the flash of light , and heating the air , making thunder . though a lightning strike does indeed release a ton of energy , most of it is expended working on the air and to electromagnetic radiation . all we need to do is provide a path for the current , and it takes much less energy to move current through a metal rod than it does through air . so , the metal rod need not absorb most of the energy .
it comes down to the change in the speed of sound at the interface . the speed of sound in air is approximately 343 m/s . the speed of sound in a solid object is typically much , much higher because the stiffness is much higher . for example , copper is 3901 m/s , brick is 4176 m/s and there are many other materials you can look at for reference . on the other hand , a near-vacuum has extremely few molecules to transmit the sound . this means sound does not travel very well through it and the speed of sound is much , much lower . contrary to popular belief , it is not zero for all frequencies , but for many sounds they just do not travel very well without molecules to transmit them . so we have two conditions . in the first , the wave is going from one medium to another where the speed of sound is higher ( the solid object ) and the second the wave is going from one medium to another where the speed of sound is lower ( the near vacuum ) . an animation of these two effects can be seen at the bottom of this page . you can see that the transmission to a lower wave speed results in a phase shift ( the wave is weakened and inverts ) while the transmission to the higher wave speed results in just an amplitude weakening . a mathematical treatment of waves crossing the boundaries is performed using maxwell 's equations , but an electromagnetic wave and a sound wave obey the same ( basic ) governing equations . the primary mechanism in both cases is that a wave impacting a surface generates a wave in the second material . that wave will leave at either a higher or lower speed than the incident wave . if the properties of both materials were identical , the wave would pass through and there would be no reflection ( no echo ) .
the depictions you are seeing are correct , the electric and magnetic fields both reach their amplitudes and zeroes in the same locations . rafael 's answer and certain comments on it are completely correct ; energy conservation does not require that the energy density be the same at every point on the electromagnetic wave . the points where there is no field do not carry any energy . but there is never a time when the fields go to zero everywhere . in fact , the wave always maintains the same shape of peaks and valleys ( for an ideal single-frequency wave in a perfect classical vacuum ) , so the same amount of energy is always there . it just moves . to add to rafael 's excellent answer , here 's an explicit example . consider a sinusoidal electromagnetic wave propagating in the $z$ direction . it will have an electric field given by $$\mathbf{e} ( \mathbf{r} , t ) = e_0\hat{\mathbf{x}}\sin ( kz - \omega t ) $$ take the curl of this and you get $$\nabla\times\mathbf{e} ( \mathbf{r} , t ) = \left ( \hat{\mathbf{z}}\frac{\partial}{\partial y} - \hat{\mathbf{y}}\frac{\partial}{\partial z}\right ) e_0\sin ( kz - \omega t ) = -e_0 k\hat{\mathbf{y}}\cos ( kz - \omega t ) $$ using one of maxwell 's equations , $\nabla\times\mathbf{e} = -\frac{\partial \mathbf{b}}{\partial t}$ , you get $$-\frac{\partial\mathbf{b} ( \mathbf{r} , t ) }{\partial t} = -e_0 k\hat{\mathbf{y}}\cos ( kz - \omega t ) $$ integrate this with respect to time to find the magnetic field , $$\mathbf{b} ( \mathbf{r} , t ) = -\frac{e_0 k}{\omega}\hat{\mathbf{y}}\sin ( kz - \omega t ) $$ comparing this with the expression for $\mathbf{e} ( \mathbf{r} , t ) $ , you find that $\mathbf{b}$ is directly proportional to $\mathbf{e}$ . when and where one is zero , the other will also be zero ; when and where one reaches its maximum/minimum , so does the other . for an electromagnetic wave in free space , conservation of energy is expressed by poynting 's theorem , $$\frac{\partial u}{\partial t} = -\nabla\cdot\mathbf{s}$$ the left side of this gives you the rate of change of energy density in time , where $$u = \frac{1}{2}\left ( \epsilon_0 e^2 + \frac{1}{\mu_0}b^2\right ) $$ and the right side tells you the electromagnetic energy flux density , in terms of the poynting vector , $$\mathbf{s} = \frac{1}{\mu_0}\mathbf{e}\times\mathbf{b}$$ poynting 's theorem just says that the rate at which the energy density at a point changes is the opposite of the rate at which energy density flows away from that point . if you plug in the explicit expressions for the wave in my example , after a bit of algebra you find $$\frac{\partial u}{\partial t} = -\omega e_0^2\left ( \epsilon_0 + \frac{k^2}{\mu_0\omega^2}\right ) \sin ( kz - \omega t ) \cos ( kz - \omega t ) = -\epsilon_0\omega e_0^2 \sin\bigl ( 2 ( kz - \omega t ) \bigr ) $$ ( using $c = \omega/k$ ) and $$\nabla\cdot\mathbf{s} = \frac{2}{\mu_0}\frac{k^2}{\omega}e^2 \sin ( kz - \omega t ) \cos ( kz - \omega t ) = \epsilon_0 \omega e_0^2 \sin\bigl ( 2 ( kz - \omega t ) \bigr ) $$ thus confirming that the equality in poynting 's theorem holds , and therefore that em energy is conserved . notice that the expressions for both sides of the equation include the factor $\sin\bigl ( 2 ( kz - \omega t ) \bigr ) $ - they are not constant . this mathematically shows you the structure of the energy in an em wave . it is not just a uniform " column of energy ; " the amount of energy contained in the wave varies sinusoidally from point to point ( $s$ tells you that ) , and as the wave passes a particular point in space , the amount of energy it has at that point varies sinusoidally in time ( $u$ tells you that ) . but those changes in energy with respect to space and time do not just come out of nowhere . they are precisely synchronized in the manner specified by poynting 's theorem , so that the changes in energy at a point are accounted for by the flux to and from neighboring points .
could there be a blob of liquid water in space the size of , say , a planet ? it is pretty unlikely , but yes , theoretically it is possible . could the water being ejected by that black hole ever condense into something of the size i am describing ? that would be one of the very few scenarios where something like this could form . maybe . it depends on the density fluctuations of the water ejecta . you need a condensation center to gather a large mass of water and get the planet started . would it boil over immediately ? a cloud of stuff contracting under its own gravity will definitely heat up . but here 's the thing - it does not matter whether the water is solid , liquid or gas . it would collapse just the same . gravity is stronger . does not matter whether is cold as ice , or boiling like crazy . once it is a small sphere of water in some form , more complex phenomena will take over . see below . would the elements near the middle heat up and maybe fuse ? maybe some heat will be provided by the initial collapse . maybe it does have some radioactive impurities which will heat it up . but anyway , over a very long time it would tend to cool down . in general , the core would be an exotic form of high-pressure ice , no matter what the temperature - water is solid at very high pressure , even if you elevate the temp a lot . above that there will be a layer of liquid water , if the whole planet has enough warmth , or just plain old ice if it is too cold altogether . the surface might be solid again , cold ice ( if the planet is wandering alone in space ) , or liquid ( if it is close to a star ) . there may be a water atmosphere above , either wispy and thin ( cold planet ) or thick ( warm planet ) . http://www.lsbu.ac.uk/water/phase.html roughly how large a body of water could you have that is stable given that it could be protected from the vacuum ? when the escape velocity of water molecules is bigger than the average thermal speed , the body is stable . in other words : warm planet - needs to be bigger to keep the water in . cold planet - it can be smaller . a small chunk of ice could survive for quite some time in outer space before sublimating to nothing . a moon-size blob of ice would probably be stable forever . but if there is warm water on the surface , and a thick water atmosphere , it would probably require an earth-like mass ( and gravity ) to keep the water from escaping . and just for fun , supposing the body of water was " bootstrapped " with primitive life or the ingredients for it , could it potentially support life ? if it is life that does not need dry land and ocean bottom to survive , then yes . there is a type of exoplanet that is similar to what you describe . not identical , but kind of close : http://en.wikipedia.org/wiki/ocean_planet
the problem of extending hamiltonian mechanics to include a time operator , and to interpret a time-energy uncertainty relation , first posited ( without clear formal discussion ) in the early days of quantum mechanics , has a large associated literature ; the survey article p . busch . the time-energy uncertainty relation , in time in quantum mechanics ( j . muga et al . , eds . ) , lecture notes in physics vol . 734 . springer , berlin , 2007 . pp 73-105 . doi:10.1007/978-3-540-73473-4_3 , arxiv:quant-ph/0105049 . carefully reviews the literature up to the year 2000 . ( the book in which busch 's survey appears discusses related topics . ) there is no natural operator solution in a hilbert space setting , as pauli showed in 1958 , w . pauli . die allgemeinen prinzipien der wellenmechanik , in handbuch der physik , vol v/1 , p . 60 . springer , berlin , 1958 . engl . translation : the general principles of quantum mechanics , p . 63 . springer , berlin 1980 . by a simple argument that a self-adjoint time operator densely defined in a hilbert space cannot satisfy a ccr with the hamiltonian , as the ccr would imply that $h$ has as spectrum the whole real line , which is unphysical . time measurements do not need a time operator , but are captured well by a positive operator-valued measure ( povm ) for the time observable modeling properties of the measuring clock .
the first paragraph is basically right , but i would not ascribe the uncertainty principle to particles , just to the universe/physics in general . you can no more get arbitrarily good , simultaneous measurements of position and momentum ( of anything ) than you can construct a function with an arbitrarily narrow peak whose fourier transform is also arbitrarily narrowly peaked . physics tells us position and momentum are related via the fourier transform , mathematics places hard limits on them based on this relation . the second paragraph is used to explain the uncertainty principle all too often , and it is at best misleading , and really more wrong than anything else . to reiterate , uncertainty follows from the mathematical definitions of position and momentum , without consideration for what measurements you might be making . in fact , bell 's theorem tells us that under the hypothesis of locality ( things are influenced only by their immediate surroundings , generally presumed to be true throughout physics ) , you cannot explain quantum mechanics by saying particles have " hidden " properties that can not be measured directly . this takes some getting used to , but quantum mechanics really is a theory of probability distributions for variables , and as such is richer than classical theories where all quantities have definite , fixed , underlying values , observable or not . see also the kochen-specker theorem .
ok , having devoted my lunch hour to this ( the sacrifices i make for physics ! ) i have an answer for you . i am not sure this is the best possible answer , so if anyone can improve on it please jump in . firstly you are absolutely correct to say that the density of your object increases as it lorentz contracts . this is not an illusion : the rhic observes this every day . note how the illustrations on the rhic page i have linked to show the colliding nuclei flattened into disks . however the contracted object can not form a black hole because this violates one of the principles of relativity i.e. the presence or otherwise of the black hole could be used to tell who was moving and who was standing still . so what is going on ? the paradox arises from your assumption that it is the mass/density of the object that determines whether it will be a black hole or not , because this is not true , or rather it is only true in special cases . the einstein equation that gives us the curvature , and therefore whether a black hole will form , is : $$g_{\alpha\beta} = 8\pi t_{\alpha\beta}$$ $g_{\alpha\beta}$ is the einstein tensor that describes the curvature , while $t_{\alpha\beta}$ is the stress-energy tensor . so it is not the mass/density of the object that determines the curvature , it is the stress-energy tensor . there is a shortcut here , because the stress-energy tensor is an invarient i.e. it is the same in all co-ordinate systems . that means the stress-energy tensor we observe is the same as the stress-energy tensor observed in the rest frame of your test object . so if the test object does not form a black hole in it is rest frame it will not form a black hole in any other co-ordinate system , even the one you describe in which the object is moving at almost the speed of light . however it is at this point that i run out of steam a bit , which is why i think there is scope for this answer to be improved . it would be nice to give an intuitive feel for what the stress-energy tensor is , and why it does not change when we see the object moving at almost the speed of light . we normally write the stress-energy tensor as a 4 x 4 matrix , and with a few approximations about your test object the tensor only has one non-zero value , $t_{00}$ , which is indeed the density . if we write the stress-energy tensor in our frame , where the object is moving , our value for $t_{00}$ will increase as the density increases , and if nothing else changed this would eventually form a black hole . however in our frame the other entries in the matrix are no longer zero . the changes in the other entries balance out the change in the density , so when we plug our stress-energy tensor into the einstein equation we get the same curvature as in the test object 's rest frame . no black hole !
photons are absorbed at the same moment they are emitted ( as seen from the point of view of the photon . ) if you think of things conversely , it is only a massless particle that can experience no lapse of time . the photon is only able to move at c because it is not moving at all in time . i believe the answer to this question is unrelated .
the photon sphere is of theoretical interest only- the photon orbits are unstable [ a boulder at the top of a hill , instead of the bottom of a valley ] , so in the real world there is not going to be some huge herd of photons to detect or crash through or anything like that . in the same vein , if you fall through it , you fall through it . nothing special happens to you . other than the fact that you are almost certainly going to have a bad experience very soon , if you are not already . no . the photon is captured if its impact parameter ( that is , the distance " of closest approach " to the black hole ) is any less than the radius of the photon sphere . that is the definition of the photon sphere . in other words , any photon that approaches from distant space will be captured if its path goes inside the photon sphere at all . however , a photon emitted on a radial trajectory from an object between the event horizon and the photon sphere can escape . a photon emitted by an object inside the event horizon has nowhere to go but towards the singularity . neutron stars do not have a photon sphere . the schwarzschild radius of a neutron star is about 3 km , which means you have to compress it within that size to create a black hole . this means that the photon sphere radius would be about 4.5 km , so to have a photon sphere you would need to cram the neutron star smaller than that . [ warning : do not try this at home . you will create a supernova , with a black hole remnant . ] neutron stars are actually more like 10-15 km in radius , so they do not have strong enough gravity to do any of that .
here , you introduce the notation $q ( t ) $ which means ' the amount of chemicals in the the pool at time $t$' . it is an important physical quality to get the equation . in order to get the differential equation , you have to think about what changes with the time goes by , here it is $q ( t ) $ . the reason why $q ( t ) $ changes is the difference between the income and outgo of chemicals . the income of chemicals per unit time $\delta t$ is $300\ gal/h \times 0.01\ g/gal \times \delta t$ , and the outgo of chemicals is $300 \ gal/h \times \frac{q ( t ) }{1,000,000} \times \delta t$ due to the amount water of pond is constant $1,000,000\ gal$ . so , you know $$\delta q ( t ) = q ( t+\delta t ) - q ( t ) = income - outgo = 300\ gal/h \times 0.01\ g/gal \times \delta t - 300 \ gal/h \times \frac{q ( t ) }{1,000,000} \times \delta t$$ , that is the equation $\frac{\delta q ( t ) }{\delta t} = 300 ( \frac{1}{100} - \frac{q ( t ) }{1,000,000} ) $ . the last step is to set the $\delta t\rightarrow 0$ , and $\frac{\delta q ( t ) }{\delta t}\rightarrow\frac{dq ( t ) }{dt}$ . now , you get the differential equation .
here is my understanding of what you are asking ( and i believe it is a little different to lubos 's interpretation , so our answers will differ ) : can you build a computer that uses the interference effects to perform computation , where you use the presence of light in a particular place to represent 1 and no light to represent 0 ? the answer is yes , though with certain caveats . first let me note that there is something already called a quantum computer which exploits quantum effects to outperform normal ( classical ) computers . classical computation is a special case of quantum computation , so a quantum computer can do everything a classical computer can do , but the converse is not true . if you have single photon detectors , you can use the interference effects of a network of beam splitters and phase plates together with the detectors to create a universal quantum computer . this is something called linear-optics quantum computing or loqc . perhaps the best known scheme is the klm proposal , due to knill , laflamme and milburn . now the caveats : you need to have a fixed finite number of photons , and you need to adapt the network based on earlier measurement results . this adaptive feed-forward is quite difficult to achieve in practice , though not impossible , and computation with such a setup has been demonstrated . a further interpretation of the question is whether it is sufficient to use such a linear network , but only make measurements at the end . this is an open question , though there is strong evidence that such a device is not efficiently simulable by a classical computer ( see this paper by scott aaronson ) . it is however not yet known whether you can implement universal classical or quantum computing on such a device .
crt displays have a fairly strong electric charge and this induces a dipole moment in the pollen particle itself causing it to be attracted to the screen . somewhat related , the hairs on the bodies of some bees become electrically charged and this attracts pollen particles . exercise 51 on page 602 of this document http://cyber.gwc.cccd.edu/faculty/kstein/h_r_ch_22.pdf will give you a ball park figure for the possible strength of this attraction . it assumes a neutral pollen particle .
first notice that the generators are $-i\sigma_k/2$ and $-il_k$ , since the groups are real lie groups and thus the structure tensor must be real . the answer to your question is positive . in principle it is enough to take the exponential of the lie algebra isomorphism and a surjective lie group homomorphism arises this way $\phi : su ( 2 ) \to so ( 3 ) $: $$\phi\left ( \exp\left\{-\sum_k t^k i\sigma_k/2\right\}\right ) =\exp\left\{-\sum_k t^k il_k\right\}\: . $$ the point is that one should be sure that the argument in the left-hand side covers the whole group . for the considered case , this is true because $su ( 2 ) $ is compact . if you instead consider no compact lie groups , like $sl ( 2 , \mathbb c ) $ , the exponential does not cover the group . however it is possible to prove that products of exponential do . in that case a product of two exponentials is sufficient , in practice decomposing an element of $sl ( 2 , \mathbb c ) $ by means of the polar decomposition , mathematically speaking , or as a ( unique ) product of a rotation and a boost physically speaking .
would the same apply to a molecule ? that is , given a wavelength , i would think you had just plug that back into the energy equation . . . and get an estimate of the size no . in the hydrogen atom , size is the distance of the electron from the proton . in a molecule , size is primarily the relative locations of the nuclei . especially for large molecules , absorption wavelengths are reflective of local structure , not the overall structure . for example a c=c bond will absorb at the about the same wavelength from one molecule to another . to take an extreme example , consider a protein molecule . the protein may contain 100s or 1000s of amino acid residues . but all protein molecule have strong absorbance in the 280nm range regardless of size . an exception to the wavelength being indicative of only part of the molecule , would be if the entire molecule was a conjugated system have delocalized electrons . in a system with an alternating series of double and single carbon-carbon bonds : $$-c=c-c=c-c=c-c=c-$$ the wavelength of a transition will increase with the length of the series . in the case of conjucated double bonds a particle in a box model is sometimes applied . see conjugated bonding in cyanine dyes : a " particle in a box " model .
veritasium has made a video about this . with this as a solution video . someone also posted this video response , which also has a good explaination . even though they use a disk with a unbalanced center of mass , the same still applies to your ball . the rotating ball will try to rotate around its center of mass . however if this does not align with the point of contact with the floor then the point of contact will be dragged along in a circle . if there is friction between the floor and the ball then this will cause the rotating ball to precess . the reason why this " pushes " the center of mass to the top is better explained in the videos . although it still seems counter intuitive to me , we have to remember that our intuition is not always right .
it is not surprising that you found one of the pauli matrices as the generator of your rotation . let us see how it can be seen algebraically that it is to be expected : observe that 2d rotations embed naturally into 2d unitary matrices , as $\mathrm{so} ( 2 ) \subset \mathrm{su} ( 2 ) $ , corresponding to the subgroup of real matrices . now , as we know , the $\mathrm{su} ( 2 ) $ is generated by the pauli matrices , and lie subgroups must be generated by lie subalgebras . the question is - which lie subalgebra is generating the $\mathrm{so} ( 2 ) $ ? well , the one generating the real matrices of course ! if you examine 1 the pauli matrices , you will see that only the one you found by directly linearizing a 2d rotation is giving real instead of complex matrices when exponentiated . now , how can we be sure that is it ? because 2d rotations are isomorphic to the $\mathrm{u} ( 1 ) $ , the one-dimensional lie group , and so we expect only one generator . 1 observe that $\mathrm{e}^{\mathrm{i}\phi ( \hat n \cdot \vec \sigma ) } = \boldsymbol{1}\mathrm{cos} ( \phi ) + \mathrm{i} ( \hat n \cdot \vec \sigma ) \mathrm{sin} ( \phi ) $ . plugging in the explicit form of the pauli matrices , we find that the summand from $n_y\sigma_y$ is real ( since $\mathrm{i}\sigma_y$ is real ) , while the others are complex .
interesting question . normal forces are normal : perpendicular to the surface . so where does the centripetal acceleration come from ? the only place i can see it coming from is friction between the earth and my shoes . a simple calculation shows that the centripetal acceleration at mid latitudes is about 20 mm/sec${}^2$ . consider a hanging plumb bob . the tension in the string must play two roles : balancing the gravitational force , and applying the centripetal force . so the plumb bob does not point to the center of the ( ideal , rigid , solid , spherical ) earth , but away from the center by about 0.002 radians ( about 0.1 degrees ) at mid latitudes , and negligible for most purposes . one thing disturbs me about this analysis . the force of friction on my shoes would have to be about ( 75 kg ) * ( 20 mm/sec${}^2 ) $ or 1.5 n . i would think that that magnitude of force would be noticeable in certain situations . the force on a 0.1 kg hockey puck or air table puck would be about 0.002 n , and perhaps not noticeable .
this is not paradoxical and it is not necessary for any physical phenomenon to a priori have to obey any particular law . some phenomena do have to obey inverse-square laws ( such as , particularly , the light intensity from a point source ) but they are relatively limited ( more on them below ) . even worse , gravity and electricity do not even follow this in general ! for the latter , it is only point charges in the electrostatic regime that obey an inverse-square law . for more complicated systems you will have magnetic interactions as well as corrections that depend on the shape of the charge distributions . if the systems are ( globally ) neutral , there will still be electrostatic interactions which will fall off as the inverse cube or faster ! the van der waals forces between molecules , for instance , are electrostatic in origin but go down as $1/r^6$ . it is for systems with a conserved flux that the inverse-square law must hold , at least at large distances . if a point light source emits a fixed amount of energy per unit time , then this energy must go through every imaginary spherical surface we think up . since their area goes up as $r^2$ , the power per unit area ( a . k.a. the irradiance ) must go down as $1/r^2$ . in a simplified picture , this is also true for the electrostatic force , where it is the flow of virtual photons that must be conserved .
the three-torus geometry or , more precisely , the $r\times t^3$ spacetime geometry , breaks the lorentz invariance to nothing . the identification $x\sim x+2\pi r_x$ and similarly for $y , z$ needed to define the torus is an identification that does not stay the same when $t , x$ are mixed by the lorentz transformation , so the spacetime is just not lorentz-symmetric and this broken lorentz invariance therefore leaves no constraints on the events in this universe .
usually if someone says a particular qft is not well-defined , they mean that effective qfts written in terms of these physical variables do not have a continuum limit . this does not mean that these field variables are not useful for doing computations , but rather that the computations done using those physical variables can only be entirely meaningful as an effective description/approximation to a different computation done with a different set of variables . ( think of doing an expansion in a basis and only keeping terms that you know make large contributions to the thing you are trying to compute . ) a funny and circular way of saying this is that a qft is well-defined if it can be an effective description of itself at all length scales . you are right that it is slightly perverse to talk about renormalization flow in terms of increasing resolution . but there is a ( n inversely ) related story in the terms of decreasing resolution . suppose you have a set of fields and a lagrangian ( on a lattice for concreteness , but you can cook up similar descriptions for other regularizations ) . you can try to write down the correlation functions and study their long distance limits . your short distance qft is not well defined if it only flows out to a free theory . when people talk about having coefficients blow up , they are imagining that they have chosen a renormalization trajectory which involves the same basic set of fields at all distance scales . flowing down to a non-interacting theory means that -- along this trajectory -- the interactions have to get stronger at shorter distance scales . testing this in practice can be difficult , especially if you insist on a high degree of rigor in your arguments . not all well-defined theories are asymptotically free . conformal field theories are certainly well-defined , but they can be interacting and these ones are not asymptotically free .
this the classic " hurling a stone into a black hole " problem . it is described in detail in sample problem 3 in chapter 3 of exploring black holes by edwin f . taylor and john archibald wheeler . incidentally i strongly recommend this book if you are interested in learning about black holes . it does require some maths , so it is not a book for the general public , but the maths is fairly basic compared to the usual gr textbooks . the answer to your question is that no-one observes the stone ( proton in your example ) to move faster than light , no matter how fast you throw it towards the black hole . i have phrased this carefully because in gr it does not make sense to ask questions like " how fast is the stone " moving unless you specify what observer you are talking about . generally we consider two different types of observer . the schwarzschild observer sits at infinity ( or far enough away to be effectively at infinity ) and the shell observer sits at a fixed distance from the event horizon ( firing the rockets of his spaceship to stay in place ) . these two observers see very different things . for the schwarzschild observer the stone initially accelerates , but then slows to a stop as it meets the horizon . the schwarzschild observer will never see the stone cross the event horizon , or not unless they are prepared to wait an infinite time . the shell observer sees the stone fly past at a velocity less than the speed of light , and the nearer the shell observer gets to the event horizon the faster they see the stone pass . if the shell observer could sit at the event horizon ( they can not without an infinitely powerful rocket ) they had see the stone pass at the speed of light . to calculate the trajectory of a hurled stone you start by calculating the trajectory of a stone falling from rest at infinity . i am not going to repeat all the details from the taylor and wheeler book since they are a bit involved and you can check the book . instead i will simply quote the result : for the schwarzschild observer : $$ \frac{dr}{dt} = - \left ( 1 - \frac{2m}{r} \right ) \left ( \frac{2m}{r} \right ) ^{1/2} $$ for the shell observer : $$ \frac{dr_{shell}}{dt_{shell}} = - \left ( \frac{2m}{r} \right ) ^{1/2} $$ these equations use geometric units so the speed of light is 1 . if you put $r = 2m$ to find the velocities at the event horizon you will find the schwarzschild observer gets $v = 0$ and the ( hypothetical ) shell observer gets $v = 1$ ( i.e. . $c$ ) . but this was for a stone that started at rest from infinity . suppose we give the stone some extra energy by throwing it . this means it corresponds to an object that starts from infinity with a finite velocity $v_\infty$ . we will define $\gamma_\infty$ as the corresponding value of the lorentz factor . again i am only going to give the result , which is : for the schwarzschild observer : $$ \frac{dr}{dt} = - \left ( 1 - \frac{2m}{r} \right ) \left [ 1 - \frac{1}{\gamma_\infty^2}\left ( 1 - \frac{2m}{r} \right ) \right ] ^{1/2} $$ for the shell observer : $$ \frac{dr_{shell}}{dt_{shell}} = - \left [ 1 - \frac{1}{\gamma_\infty^2}\left ( 1 - \frac{2m}{r} \right ) \right ] ^{1/2} $$ maybe it is not obvious from a quick glance at the equations that neither $dr/dt$ nor $dr_{shell}/dt_{shell}$ exceeds infinity , but if you increase your stone 's initial velocity to near $c$ the value of $\gamma_\infty$ goes to $\infty$ and hence 1/$\gamma^2$ goes to zero . in this limit it is easy to see that the velocity never exceeds $c$ . in his comments jerry says several times that the velocity exceeds $c$ only after crossing the event horizon . while jerry knows vaaaaastly more than me about gr i would take him to task for this . it certainly is not true for the schwarzschild observer , and you can not even in principle have a shell observer within the event horizon .
both pendulums are correct in their respective situation . we must remember that newton 's second law dictates that the vector sum of forces on an object must be equal to the mass of the object times the acceleration of the object . \begin{equation} \sum_n f_n = ma \end{equation} in the first pendulum the object is swinging side to side so we know that the acceleration of the object is orthogonal to the arm of the pendulum pointing at an angle $\theta$ below the horizontal towards the center of oscillation of the pendulum . this means that the forces in line with the arm of the pendulum must be equal and opposite since there is no motion in this direction and we see that $t=mg\cos{\theta}$ is true for this pendulum . for the second ( conical ) pendulum the object is moving at the same vertical height in a circular path of radius $r$ . this tells us that the acceleration of the object points horizontally inward at an angle of $\pi/2-\theta$ with respect to the arm of the pendulum . we also know that for circular motion newton 's second law can be rewritten as \begin{equation} \sum_n f_n = \frac{mv^2}{r} \end{equation} since there is no downward acceleration in the conical pendulum the vertical forces must be in equilibrium such that $t\cos{\theta}=mg$ . moral of the story ? your choice of coordinate axes is important and net acceleration must be accounted for when making free-body diagrams .
they have used the schrdinger equation : $$i\hbar\frac{\partial \psi}{\partial t} = \hat{h} \psi$$ and hence : $$\frac{\partial \psi}{\partial t} = \frac{1}{i\hbar}\hat{h}\psi$$ and since $1/i = -i$: $$\frac{\partial \psi}{\partial t} = -\frac{i}{\hbar}\hat{h}\psi$$ similarly for $\psi^*$ we take the complex conjugate : $$\frac{\partial \psi^*}{\partial t} = \frac{i}{\hbar} ( \hat{h}\psi ) ^*$$ plugging these values into the integral gives us the last line you were confused about .
the two main contributions are thought to be from the s-process and the r-process , which are both neutron capture processes that are differentiated by their speed relative beta-decay of neutron rich isotopes . both are end-of-stellar-life processes with the s-process probably happening in the giant stages of very heavy stars and the r-process probably happening in core-collapse supernovas .
this is not a dumb question . note that when you say " two rocket ships that are traveling at 60% the speed of light fly past each other " , their respective velocities are measured by somebody standing still and observing both of them : he sees that ship a travel at $0.6c$ from his left and ship b travel at $0.6c$ from his right , and they are about to fly past each other . however , the situation is different if he , now on ship a , measures the velocity of oncoming ship b . by classical addition of velocity , he will measure the velocity of ship b relative to his ship be $0.6c+0.6c=1.2c$ , which is greater than the speed of light . this is probably what you are thinking about . however , if you read about lorentz velocity transformation : http://hyperphysics.phy-astr.gsu.edu/hbase/relativ/veltran.html ( or you can google it yourself ) . the velocity of ship b , relative to ship a is $\frac{v_{a}+v_{b}}{1+\frac{v_{a}v_{b}}{c^2}}\approx0.882c$ .
consider a couple of cases : if the glass was completely full , no air , there is nothing inside the glass to balance the air pressure outside ( the water can only exert its own vapour pressure , which at room temperature is much less than typical ambient air pressures ) . so , the air outside the glass keeps everything in place . if the glass is partly filled with air , some amount of water may escape ; if air is not allowed in to replace the escaped water , the pressure inside the glass will drop . the height of the water inside the glass , because of its weight , equates to a pressure , which can manifest as a pressure difference between the inside and outside of the glass . at the point the water column pressure plus the air pressure inside the glass balances the air pressure outside the glass , there is no longer any net force to push more water out of the glass . in both cases , what keeps the water in the glass is ambient air pressure . what keeps air from entering the glass ( which would allow water to escape ) is surface tension . if you very carefully lift your inverted glass off the table , you can create a tiny but noticeable gap between the table and the rim of the glass , and everything still stays in place . but if you lift it too far , surface tension is no longer sufficient to keep everything in place , air starts to bubble in and water starts to run out .
in order to keep the density of water at 1kg/litre as you decrease its temperature below 0 centigrade you would need a container that remained rigid at many thousands of atmospheres pressure . the water would then remain liquid until something like -20 degrees centigrade . at still lower temperatures it would become solid , but it would not form crystalline ice which always has a density lower than 1kg/litre . instead it would form a glassy solid known as amorphous ice which has a higher density under such pressures . there are several high density phases of amorphous ice and to get the exact details of which phase it would pass through you need a phase diagram showing density as a function of temperature and pressure extending to very high pressures and low temperatures . you can then follow the contour corresponding to a density of 1kg/litre to see how the pressure and phase would vary as the temperature decreased . i can only find enough information to give the vague answer above and it is possible that the diagram is not known well enough yet to say much more .
historical issues i suppose ; indeed current definition of ampere is rather stupid ( force between two cables in vacuum ) in light of the fact it could be done with number of elementary charges per second . candela is even worse , because it involves properties of average human eye ( so called " luminosity function" ) -- so in principle it changes instantaneously as people get birth , die and their eyes age ( not to mention various eye/brain fractures and treatments ) .
the surface of any fluid has an associated energy-per-unit-area , known as the surface energy , a.k.a. surface tension . this energy is not a property of the fluid alone , but of the fluid and the medium it is in contact with . in your case you would have associated surface energies for the water-air interface , $e_{wa}$ , as well as for the water-paper interface , $e_{wp}$ . the total energy of the fluid in a configuration is the sum of the potential energy , plus the product of the corresponding surface energies by their respective surface areas , $s_{wa}e_{wa} +s_{wp}e_{wp}$ . so if you want to look at it in a purely energy balance point of view , the increase in potential energy of the water climbing up the paper is compensated by a reduction of the total surface energy . when capillary action makes things rise , it is because the liquid-solid energy is lower than the liquid-air energy . by wicking into the porous material , the solid-liquid contact area is increased at the expense of the liquid-air one , resulting in an overall reduction of contact energy , which is what drives the rise in potential energy .
another way of solving such problems is to go to another reference frame , where you obviously do not have enough energy . for example you have got a $5 mev$ photon , so you think that there is plenty of energy to make $e^-e^+$ pair . now you make a boost along the direction of the photon momentum with $v=0.99\ , c$ and you get a $0.35 mev$ photon . that is not enough even for one electron .
the only " known " content of the universe that can act this way is dark energy . of course , ordinary matter , dark matter or radiation can not do this . from acceleration equation in a frlw spacetime , you would need a component which satisfies a equation of state with $p&lt ; -\rho/3$ . for matter , $p=0$ , and for radiation $p=\rho/3$ , both grater than $-\rho/3$ , but for dark energy , $p=-\rho$ , and it acts that way .
all speeds are relative . in this case the 100 mm/h ( have not verified that , taking your word on it ) is relative to the solar system as a whole . therefore , you can " see " this speed from any place where you are in a fixed position relative to the solar system . for example , if you could park yourself 93 mmiles off the predominant plane of the solar system directly perpendicular to the sun , you had see earth going around in a circle spanning about 90&deg ; of your view . the earth would take one year to complete this circle , and it would move at the 100 mm/h speed . it would be rather boring to watch , and just a dot without a telescope anyway .
as you said if two operators commute they share eigenvectors . physically this means that you can have a definite value for both . for example in the hydrogen atom the hamiltonian $h$ , which is the energy , and $j^2$ , the magnitude of angular momentum , commute . a hydrogen atom can be in a state of definite energy and definite angular momentum . however , the position operator $x$ does not commute with $h$ , so in a state of definite energy the electron does not have a well-defined position . then conversely the commutator measures the inability for two quantities to have definite values in the same state . more quantitatively , we have the general heisenberg uncertainty principle $$\delta a \delta b \ge \frac{1}{2} |\langle [ a , b ] \rangle |$$ that is , the product of the uncertainties in $a$ and $b$ is at least half the ( absolute value of the ) expectation value of their commutator . by uncertainty we mean the usual standard deviation , $$\delta a = \sqrt{\langle a^2\rangle - \langle a \rangle^2 } . $$ for the position operator $x$ and the momentum operator $p$ , the commutator is just a scalar , $i\hbar$ ; its expectation value is always $i\hbar$ . we thus get the most famous instance of the heisenberg principle $$\delta x\delta p \ge \frac{\hbar}{2} . $$ now you could ask why should we have $ [ p , x ] = i\hbar$ of all things . well , in the hamiltonian formulation of classical mechanics there is an operation called the poisson bracket , $\{f , g\}$ . the poisson bracket has the same algebraic properties as the commutator ( they are both brackets in a lie algebra ) and satisfies $$\{ x_i , p_i \} = \begin{cases}1 and i = j \\ 0 and i \neq j\end{cases} . $$ so suppose that you know that whatever quantum mechanics is , quantum states are vectors and observables are operators , and you want to figure out how those operators should be related . then it would be tempting to just try $$ [ x , p ] \overset{ ? }{=} 1 . $$ the problem is that $x$ and $p$ should be hermitian ( so that expectation values are always real ) . then $ [ x , p ] $ must be anti-hermitian . but that is not a big problem , you can just multiply by $i$: $$ [ x , p ] \overset{ ? }{=} i . $$ that is okay algebraically , but $x$ has units of length and $p$ has units of momentum , so we need to put a constant there to get the right units too : $$ [ x , p ] \overset{ ? }{=} i\hbar . $$ i still put a question mark there because this is really just an educated guess , but experiments show that this is the correct commutation relation to use . ( well , you also have to measure $\hbar$ somehow . )
if the photon energy $e$ is sufficient to produce a pair ( $e&gt ; 2mc^2$ ) , then a pair can be produced in a collision . it is due to equivalence of mass and energy . the same energy can be " represented " differently - as photons , particles , etc . kind of conversion of one form of energy into another .
hint : the floor goes down at $13 m/s^2$ , the person goes down at $g \approx 9.81 m/s^2$ , so the net acceleration is ?
i am answering your question here , but please provide more information about your goals/experience , as specified by the comments . primarily , i would like to say that i was planning on answering your question much less in depth than i ended up doing . however , while brushing up , i got carried away and figured out some very interesting calculations concerning your question : in order to aptly portray the reason for this phenomenon , i will begin by addressing why the rings form . in addition , i would like to note that i will be using a convex glass lens as an example for the sake of simplicity ( and by your assertions , i am assuming that is what you are familiar with ) . however , most experimental research done with this phenomenon requires far more complex calculations , with changing radii of curvature [ these calculations are sometimes conducted to test the flatness of a glass surface beyond what can be done with a sphereometer ] . the illuminated rings become visible when the change in path between the two interfering waves is : $\bigg\{\cfrac{\lambda}{2} , \cfrac{3\lambda} {2} , \cfrac{5\lambda}{2} , . . . \bigg\}$ thus , the path difference between any pair of adjacent rings is $\pm \lambda$ ( depending on whether the adjacent ring is on the inside or outside relative to its partner ) . the rings themselves are formed via thin film interference , by a minuscule layer of air between a curved glass surface and the flat glass surface where it resides . due to the curvature of the glass surface , the thickness of the air layer does not increase linearly . the farther a point is from the center , the smaller the horizontal distance , which means an increase in vertical thickness of $\lambda$ . thus , the horizontal distance between two neighboring rings decreases as we observe radially outward from the center . here is a visual : $\textbf{mathematical approximation for high order ring separation:}$ this part i believe you will find exceptionally useful , since your objective concerned small separation the higher order fringes . mathematically speaking , if you want to address the change in distance between adjacent rings , it is possible to make an approximation . if we assume that the order number , $\mu&gt ; &gt ; 1$ and that $\triangle\mu = 1$ , we are allowed to safely assume that the distance between a pair of rings ( $\triangle r ) $ is equal to : $\cfrac{ ( dr ) ( \triangle \mu ) }{d\mu}$ ( i am assuming here that you have taken introductory physics , which is typically taken with first semester calculus as a co-requisite , so this should make sense to you ) . due to the fact that we have a standard formula for the radius of a ring of order $\mu$ , [ which i will not derive , but it is explained in a wikipedia link given to you above in the comments ] , we can make the following calculations : $\omega = $ radius of curvature $\mu =$ order $r =$ radius of ring $\triangle\ r =$ separation of rings $\lambda =$ wavelength $r = \big ( ( \mu - \frac12 ) \lambda \omega\big ) ^{1/2}$ and from above we know that $\triangle r =$$\cfrac{ ( dr ) ( \triangle \mu ) }{d\mu}$ and since $\triangle\mu$ = $1$ , $\triangle r$ = $\cfrac{\omega\lambda}{2}\big ( ( \mu - \frac12 ) \lambda \omega\big ) ^{-1/2}$ , or if we square our value and then take the square root , we can achieve a much simpler value of : $\triangle r = \sqrt{\bigg ( \cfrac{\lambda \omega}{4 ( \mu - \frac12 ) }\bigg ) }$ here , i have graphed this function . notice that the fringe distance between a pair of rings has a horizontal asymptote approaching zero as the order number approaches infinity : it worth mentioning here that you can do the same calculation for the dark rings , but you would use $\mu$ , instead of $\mu - \cfrac12$ . let me know if you have any questions .
as the wikipedia article you linked explains , capillary action is due to the " surface tension " ( cohesion within the liquid ) as well as " adhesion " ( attraction between the water and the paper ) . in particular , surface tension http://en.wikipedia.org/wiki/surface_tension is the energy per unit area of the surface of the liquid . the molecules of water inside the liquid are nicely sitting in a potential " hole " induced by the adjacent molecules . however , the molecules of water that are on the surface lack the attractive force to the now non-existent neighbors on the side where the liquid no longer exists . because attractive forces are related to a negative potential energy , the lack of them leads to a positive potential energy . consequently , water will contain some extra positive energy called the " surface tension " . it is a form of potential energy but it can only be interpreted in this way at the molecular level . as hinted in the previous paragraph , the surface tension is a contribution to the energy that is proportional to the surface . that is why the surface tension causes water to take the shape of spherical droplets , among many other things , because the sphere minimizes the surface among shapes of the same volume . similarly , adhesion - between the paper and water in this case - adds a negative energy proportional to the surface of contact . again , it may be explained by the ( now added , not lacking ) negative potential energy between the molecules of the water and the molecules of the paper that attract . to reduce the energy , water tries to touch the paper as much as it can . only the total energy is conserved . the energy needed to lift the water is obtained from the surface tension and adhesion . microscopically , you may literally imagine that the molecules of water at the top were attracted by the nearby ( and so far higher ) molecules of paper .
hint : use ohm 's law $i=\frac{v}{r_p}$ and the formula $\frac{1}{r_p}=\frac{1}{r}+\frac{1}{r_0}$ for parallel resistors to derive a straight line in a $i$-$\frac{1}{r}$ diagram $$ i~=~v \left ( \frac{1}{r}+\frac{1}{r_0}\right ) . $$ here $r$ and $r_0$ are the variable and the fixed resistor , respectively . to answer op 's two questions : the slope is $v=1{\rm volt}$ . the intercept of the straight line on the vertical $i$-axis is $c=\frac{v}{r_0}$ . the intercept of the straight line on the horizontal $\frac{1}{r}$-axis is $\frac{-1}{r_0}$ .
( full disclosure , i did not rtfa , and i do not have time to . ) just to review , $f\left ( x , v , t\right ) dxdv$ is the number of particles with positions between $x$ and $x+dx$ and velocities between $v$ and $v+dv$ . first , why is there no integral over position ? in principle , there should be . however , assuming that $d$ is small , $f\left ( x , v , t\right ) \approx f\left ( x+d , v , t\right ) $ ; i.e. $f$ should not change much over small distances . if that is not the case , you probably should not use the boltzmann transport equation . my guess is that the collision term has the form of " number of collisions " = " number of particles at x , v " * " number of particles those particles can collide with " . the former is just $f\left ( x , v , t\right ) $ . the latter is that integral . say two particles have the same x and v ; then they are moving in parallel and will not collide . ( even if they did collide , it would not change anything . ) suppose their velocities are just a little different . collisions will happen , but they will be infrequent . think about a car chase where one car is moving just a little faster than the other . the faster car will catch the slower one , but it'll take longer , so fewer chases will end in a given time period . if the difference in velocities was large , the catch will happen much more quickly ; particles with very different velocities collide much more often . ( just to be clear , i really do mean velocity and not speed ; direction is important . ) that is what the $\left|v-v^\prime\right|dt$ term encapsulates . if the velocities are the same , it is zero . if the velocities are very different , it is large . you can get that term by rigorous means if you had like . combine that with the number of particles with velocity $v^\prime$ , integrate over all possible $v^\prime$ , and you have the " number of particles those particles can collide with " . i think that is the gist of it anyway .
there is a test of the collapse of a magnetized neutron star ( case 3 ) in this preprint : http://arxiv.org/abs/arxiv:1208.3487 . here is some of the text and figures from that paper that describe how the magnetic field is expelled : magnetized collapse to a black hole our final and most comprehensive test is represented by the collapse to a bh of a magnetized nonrotating star . this is more than a purely numerical test as it simulates a process that is expected to take place in astrophysically realistic conditions , such as those accompanying the merger of a binary system of magnetized neutron stars [ 26 , 27 ] , or of an accreting magnetized neutron star . the interest in this process lays in that the collapse will not only be a strong source of gravitational waves , but also of electromagnetic radiation , that could be potentially detectable ( either directly or as processed signal ) . the magnetized plasma and electromagnetic fields that surround the star , in fact , will react dynamically to the rapidly changing and strong gravitational fields of the collapsing star and respond by emitting electromagnetic radiation . of course , no gravitational waves can be emitted in the case considered here of a nonrotating star , but we can nevertheless explore with unprecedented accuracy the electromagnetic emission and assess , in particular , the efficiency of the process and thus estimate how much of the available binding energy is actually radiated in electromagnetic waves . our setup also allows us to investigate the dynamics of the electromagnetic fields once a bh is formed and hence to assess the validity of the no-hair theorem , which predicts the exponential decay of any electromagnetic field in terms of quasi normal mode ( qnm ) emission from the bh . . . . as the collapse proceeds , the restmass density in the center and the curvature of the spacetime increase until an appararent horizon is found at t = 0.57 ms and is marked with a thin red line in fig . 12 . as the stellar matter is accreted onto the bh ( the rest-mass outside the horizon $m_{b , out} = 0$ is zero by $t \geq 0.62$ ms ) , the external magnetic field which was anchored on the stellar surface becomes disconnected , forming closed magneticfield loopswhich carry away the electromagnetic energy in the form of dipolar radiation . this process , which has been described through a simplified non-relativistic analytical model in ref . [ 52 ] , predicts the presence of regions where |e| > |b| as the toroidal electric field propagates outwards as a wave . this process can be observed very clearly in fig . 13 , which displays the same three bottom panels of fig . 12 on a smaller scale of only 15 km to highlight the dynamics near the horizon . in particular , it is now very clear that a closed set of magnetic field lines is built just outside the horizon at t = 1.0 ms , that is radiated away . note also that our choice of gauges ( which are the same used in [ 61 ] ) allows us to model without problems also the solution inside the apparent horizon . while the left panel of fig . 13 shows thatmost of the rest-mass is dissipated away already by t = 0.65 ms ( see discussion in [ 62 ] about why this happens ) , some of the matter remains on the grid near the singularity , anchoring there the magnetic field which slowly evolves as shown in the middle and right panels .
black holes can also have magnetic charge . from spacetime and geometry : an introduction to general relativity - sean carroll : " stationary , asymptotic flat black hole solutions to general relativity coupled to electromagnetism that are nonsingular outside the eventhorizon are fully characterized by the parameters mass , electric and magnetic charge , and angular momentum . "
the problem is what konstantin tsiolkovsky discovered 100 years ago : as speed increases , the mass required ( in fuel ) increases exponentially . this relation , specifically , is $$ \delta v=v_e\ln\left ( \frac{m_i}{m_f}\right ) $$ where $v_e$ is the exhaust velocity , $m_i$ the initial mass and $m_f$ the final mass . the above can be rearranged to get $$ m_f=m_ie^{-\delta v/v_e}\qquad m_i=m_fe^{\delta v/v_e} $$ or by taking the difference between the two , $$ m_f=1-\frac{m_f}{m_i}=1-e^{-\delta v/v_e} $$ where $m_f$ is the exhaust mass fraction . if we assume we are starting from rest to reach 11.2 km/s ( i.e. . , earth 's escape velocity ) with a constant $v_e=4$ km/s ( typical velocity for nasa rockets ) , we had need $$ m_f=1-e^{-11.2/4}=0.939 $$ which means almost 94% of the mass at launch needs to be fuel ! if we have a 2000 kg craft ( about the size of a car ) , we can not fit nearly 31,000 kg of fuel in a craft that size ! which means we need a bigger craft which means more fuel ! this is why this has been dubbed " the tyranny of the rocket problem " .
the product may be understood geometrically as the projection of one vector onto another , multiplied by the length of the vector that it is projected onto . if one takes the dot product of two vectors $\vec{a}$ and $\vec{b}$ , we can apply this procedure to find the correct formula for the dot product : let 's call the angle between the vectors $\varphi$ . then , the projection of $\vec{a}$ onto $\vec{b}$ is $|\vec{a}|\cos \varphi$ . multiplying by the length of $\vec{b}$ gives us $$\vec{a}\cdot\vec{b}=|\vec{a}||\vec{b}|\cos\varphi$$ this is the correct expression for the dot product . of course , the dot product is symmetric so we might as well picture it as projecting $\vec{b}$ along $\vec{a}$ .
that is correct ; there is no change to the angular momentum here with respect to the center of mass . remember that $l = r \times p$ . if $p$ is constant ( as it is once they both let go of the rope ) , then you can see that $r_\perp$ is constant also , and $r_\parallel \times p$ is guaranteed to be zero .
i do not believe x-ray diffraction machines focus the x-rays . generally you want to put them in along a straight line and detect them undisturbed from the sample . the straight line input beam is just a source an energy selector and a slit ( unless the machines have got a lot more complex ) the wolter mirrors used in telescopes are because you only have very few x-rays photons from a source and so you need to concentrate them on the detector . the optics are not cheap or convenient to use !
check here cosmology today-a brief review chapter thermodynamics in the early universe , page 10 . . . follow the word gas . . . some words here would be fine , but , . . . find for yourself .
superconductors float not just because of the meinssner effect . they float because of quantum locking . very small weak points in a thin superconductor allow magnetic fields to penetrate , locking them in . these are called flux tubes . the meissner effect is only if you make a conductor into a superconductor while the magnet ( or magnetic field ) is near it . placing a magnet near an existing superconductor is an example of electomagnetic induction producing magnetic levitation . most examples of the meissner effect and electromagnetic induction are done with a floating magnet . attempting to float a thick superconductor without quantum locking would quickly cause it to fly off due to unstable magnetic fields . any example of a floating superconductor is done with a thin superconductor exhibiting quantum locking .
dear calvin , if any portion of the world is described by probabilistic wave functions , then the whole world has to be . it is easy to show it . take a decaying nucleus , connect it to a hammer that kills a cat a that also makes the sun explode into 2 pieces . the nucleus is evolving into a linear superposition of " decayed " and " not yet decayed " states . correspondingly , because of the mechanism , the sun has to evolve into a linear superposition of " exploded " and " not yet exploded " . these two states have different gravitational fields . it proves that in general , the evolution produces linear superpositions of states with different gravitational fields , so the gravitational fields - and any other physical properties of the world - have to be described by linear operators just like any other physical property . if some building blocks can only be predicted probabilistically , it is clear that everything else that may be affected by these building blocks can only be predicted probabilistically , too . the whole world , including gravitational fields , interacts with itself , so clearly the gravitational waves have to be ultimately described by quantum physics , too . gravitons are physical particles that are quanta of gravitational waves . they have to exist because gravitational waves exist the energy stored in a frequency $f$ classical state is always a multiple of $e=hf$ the first point was indirectly , but pretty much conclusively , proved by the observation of the binary pulsar that changes its frequency in the right way , as it emits the gravitational waves and loses energy . everything agrees with general relativity beautifully . the 1993 physics nobel prize was given for this confirmation of the gravitational waves . the second point is a trivial consequence of schrdinger 's equation . take a classical gravitational wave of frequency $f$ - e.g. similar to what is emitted by the binary pulsar . divide the corresponding quantum state to energy eigenstate components . they go like $$c_n\exp ( e_n t/i\hbar ) $$ if all classical observables evaluated in this state are periodic with periodicity $1/f$ , it is trivial to see that all the energy differences $e_m-e_n$ must be multiples of $e_0=hf$ , so the energy can only be added to the gravitational waves by quanta , the gravitons . one may also derive the gravitons and their polarizations from the linear approximation of the quantized general relativity . while general relativity has problems at higher-loop level - strongly quantum effects that affect spacetime - this approximation has to work very well , is consistent , and implies the existence of gravitons , spin 2 massless particles , too . in quantum gravity , virtual gravitons are the messengers of the gravitational force in the same way as photons are messengers of the electromagnetic force . gravitons and photons have a different spin but otherwise the analogy between them is - and must be - much tighter than the wording of your question is willing to admit . they are described as quantum fields and the effective quantum field theory with feynman diagrams etc . has to work , with the same interpretation , at least in some approximation . also , it is not true that the non-gravitational forces can not be geometric . the kaluza-klein theory explains the electromagnetic field as " twists " that include an extra dimension of space . whether or not the extra dimensions of space have this simple form , string theory generalizes the kaluza-klein lesson and every field and particle species may be viewed as a component of a generalized geometry field - because the geometry is generalized in such a way that it includes all other fields as well . some of the string scenarios are very close to the kaluza-klein theory , some of them are far from it , but all of them confirm that gravity ultimately comes from the same underlying physics as everything else . the term " string theory " may sound too narrow-minded in this context . there are very many different ways how to describe its physics , including gauge theory ( just an old quantum field theory ! ) that lives on the holographic boundary of the ads space . all of them agree that gravitons have to exist and they are just one particle species among many that share a common origin . so the term " string theory " in all these discussions really means " everything we have ever learned about quantum gravity that has worked " .
you can solve this by picking a simple , specific example and then generalizing it as necessary . lets start with a simple case , where the tempo is such that there are 120 quarter notes per minute . the frequency of quarter notes is then $$ \frac{\text{120 quarter notes}}{\text{1 minute}} \cdot \frac{\text{1 minute}}{\text{60 seconds}} \cdot \frac{\text{1 second}}{\text{1000 milliseconds}} = \frac{\text{0.002 quarter notes}}{\text{millisecond}} . $$ this is the number of quarter notes that occur each millisecond . for a more useful number , we can invert this fraction to see that the number of milliseconds taken by one quarter note is $$ \frac{1}{\frac{\text{0.002 quarter notes}}{\text{millisecond}}} = \frac{\text{1 millisecond}}{\text{0.002 quarter notes}} = \frac{\text{500 milliseconds}}{\text{1 quarter note}} . $$ you can do fancier things involving other notes , tempos , key signatures , etc . by first finding out how many of your note occur in one minute ( 120 quarter notes in this example ) , and then following this same process .
fourier expansion is used as a change of basis method that makes our calculations simpler and more in context . usually we are working with particles with precise momentum . momentum is a far more useful quantity than position when doing experiments . also the feynman rules can be obtained in terms of momentum as well . furthermore as acuriousmind mentioned in the comments above , we obtain nice fourier modes which we interpret as creation and annihilation of particles in some momentum eigenstate .
i have reproduced your calculation . if $\theta$ is the angle from the central vertical axis of the hemisphere to the ball , the tangential downward force is $g \sin \theta = g\frac rr$ the tangential upward force due to rotation is $\omega ^2 r \cos \theta=\omega ^2 r \sqrt {1-\frac {r^2}{r^2}}$ when $\omega \lt \sqrt{\frac gr}$ the downward force is always greater and the ball will sit at the bottom of the bowl . when $\omega \gt \sqrt{\frac gr}$ you get the right answer .
if a dense , spherical star were made of uniformly charged matter , there would be an attractive gravitational force and a repulsive electrical force . these would balance for a very small net charge : $$ df = \frac1{r^2}\left ( - gm_\text{inside} dm + \frac1{4\pi\epsilon_0}q_\text{inside} dq \right ) $$ which balances if $$ \frac{dq}{dm} = \frac{q_\text{inside}}{m_\text{inside}} = \sqrt{g\cdot 4\pi\epsilon_0} \approx 10^{-18} \frac{e}{\mathrm{gev}/c^2} . $$ this is approximately one extra fundamental charge per $10^{18}$ nucleons , or a million extra charges per mole not much . any more charge than this and the star would be unbound and fly apart . what actually happens is that the protons and electrons undergo electron capture to produce neutrons and electron-type neutrinos .
" covariant vectors as expressed in a dual basis " is exactly the same thing that " orthogonal projections to achieve covariant components " choose one generating system $\vec e_1 , \vec e_2$ , non necessary orthogonal , a vector $\vec v$ may be expressed by $\vec v = v^1 \vec e_1 + v^2 \vec e_2$ . the coordinates $v^1 , v^2$ are called contravariant coordinates of the vector $\vec v$ now , let a new generating system $\vec f^j$ be a dual of the generating system $\vec e_i$ . this is expressed by $\vec f^j . \vec e_i = \delta^j_i$ ( this is the definition of " dual" ) this means that $\vec f^2 . \vec e_1 = \vec f^1 . \vec e_2 = 0$ so , this means that $f^2$ is orthogonal to $\vec e_1$ , and that $\vec f^1$ is orthogonal to $\vec e_2$ the coordinates $v_1 , v_2$ of the vector $\vec v$ , relatively to the generating system $\vec f^j$ are the covariant coordinates of $\vec v$ : $\vec v = v_1 \vec f^1 + v_2 \vec f^2$ now , to find $v_1$ for instance , you have to take , from the arrow of the vector , the parrallel to $\vec f^2$ , but we have seen above that $\vec f^2$ is orthogonal to $\vec e_1$ , so " take the parrallel to $\vec f^2$" is the same thing as " take an orthogonal to $\vec e_1$" that is how you get your orthogonal projections to achieve covariants components . update considering covariant dual vectors as linear functions from vector space to $\mathbb r$ , $f^i$ are promoted to functions : $f^i ( \vec e_j ) = \delta ^i_j$ so , you have : $$w ( \vec v ) = ( w_1 f^1 + w_2 f^2 ) ( v^1 \vec e_1 + v^2 \vec e_2 ) = ( w_1 . v^1 + w_2 v^2 ) $$ update 2 now , in a finite dimensional space , there is an isomorphism between the vectors space and the dual space of the vector space . let $\vec w$ be an element of the vector space and $w$ be an element of the dual space of the vector space . this isomorphism is : $$w ( \vec v ) = \vec w . \vec v$$ applying this isomorphism to the generating system , gives an isomorphism between $ \vec f^i$ ( "reciprocal basis" ) and $f^i$ ( basis of the dual space ) : $$f^i ( \vec e_j ) = \vec f^i . \vec e_j= \delta^i_j$$
this is a very difficult problem , i try to explain why . in statistical mechanics , tending to the most probable distribution is a probability event , and for boltzmann ' entropy , $ds\ge 0$ is also a probability event but not an inevitable result . so you cant prove $ds\ge 0$ as an inevitable result from statistical mechanics . if we want to obtain the mathematical proof of the second law from thermodynamics , we must consider the mathematical proof of the entropy first , as a state function . clausius definition $ds=q/t$ cannot be proven in mathematics , as an exact differential , so the definition $ds=q/t$ can only depend on imaginary reversible cycles . on the other hand , in c . caratheodory or m . planck approaches , the expressions of the entropy are the mathematical equations but not the definition of a physical concept because the equation contains the difference of functions ( please see bellow , eulers equation ) , the physical image of the entropy and both the second law are not clear , that is why we cannot explain the physical meaning of entropy according to these approaches . in such case , to prove the second law in mathematics will be very difficult , it is not an isolated problem . the following are the some steps of the link paper , the paper introduce a new approach , and the new statement on the second law can be considered as an axiom . 1 ) according to the fundamental equation of thermodynamics ( eulers equation ) . \begin{align} ds=\frac{du}{t}-\frac{ydx}{t}-\sum_j\frac{\mu_jdn_j}{t}+\frac{pdv}{t} . \end{align} 2 ) define the function that \begin{align}dq=du-ydx-\sum_j\mu_jdn_j . \end{align} here $dq$ can be proven as an exact differential in mathematics , the physical meaning of $q$ is the heat energy within the system . ( but not the heat in transfer $q$ ) 3 ) such that we get \begin{align}ds=\frac{dq}{t}+\frac{pdv}{t} . \end{align} here $ds$ can be proven as an exact differential in mathematics . 4 ) consider an interaction between the two locals , then we can get the total differential of the entropy production . \begin{align}d_is=\nabla \left ( \frac{1}{t}\right ) dq+\frac{1}{t}\nabla ydx+\sum_j\frac{1}{t}\nabla \mu_jdn_j+\nabla \left ( \frac{p}{t}\right ) dv . \end{align} this is a non- equilibrium thermodynamic equation . 5 ) prove the total differential of the entropy production $d_is\ge0$ . the new statement of the second law : " irreversibility root in a fundamental principle : the gradients of the four thermodynamic forces spontaneously tend to zero " . the four gradients of thermodynamic forces are \begin{align} \nabla \left ( \frac{1}{t}\right ) , \ , \ , \ , \ , \nabla y , \ , \ , \ , \ , \nabla \mu_j , \ , \ , \ , \ , \nabla \left ( \frac{p}{t}\right ) . \end{align} the conditions of thermodynamic equilibrium are these four gradients equal to zero . please compare the different statements about the second law , and see which statement can be considered as an axiom . 1 ) -4 ) are quoted from http://en.wikipedia.org/wiki/second_law_of_thermodynamics 1 ) clausius statement heat can never pass from a colder to a warmer body without some other change , connected therewith , occurring at the same time heat can never pass from a colder to a warmer body without some other change , connected therewith , occurring at the same time . 2 ) kelvin statement it is impossible , by means of inanimate material agency , to derive mechanical effect from any portion of matter by cooling it below the temperature of the coldest of the surrounding objects . 3 ) planck 's statement every process occurring in nature proceeds in the sense in which the sum of the entropies of all bodies taking part in the process is increased . in the limit , i.e. for reversible processes , the sum of the entropies remains unchanged . 4 ) principle of carathodory in every neighborhood of any state s of an adiabatically enclosed system there are states inaccessible from s . 5 ) the new statement . the gradients of the four thermodynamic forces spontaneously tend to zero . note statistical mechanics is restricted to the postulate of the equal a priori probability , but this postulate does not need to be considered in thermodynamics , so the valid ranges of the two theories are different . the valid range of h-theorem is less than the second law of thermodynamics . there are some computer simulations for h theorem , the changes in h are not monotonous . please see http://arxiv.org/pdf/1201.4284v5.pdf
it is not possible to perform reliable ab-initio computations of melting phase transitions . simple reason being that such computations need to cover the thermodynamic ( large size , many particle ) limit to access the loss of long rang order . we simply lack the computational resources to do so . what is possible , is to apply semi-empirical rules to estimate melting temperatures ( google " lindemann 's criterion" ) .
in short no . but . . . while light has no mass , it does have non-zero energy . this means light contributes to the stress-energy tensor that in turn determines the gravitational field a.k.a. the shape of space . it takes a large energy to produce a measurable gravitational field , so we likely will not directly measure the bending of space due to light . compare the energy of a gamma-ray photon : $e\approx 10^6\ , \mathrm{ev}$ to the energy of a typical terrestrial object $e=mc^2\approx 1 \mathrm{kg} \cdot c^2 \approx 5\times 10^{35} \mathrm{ev}$ . we have trouble measuring the gravitational field of 1 kg objects , so a single photon is out of the question . there is one place where the energy of photons matters for gravity : cosmology . it is possible to consider a universe filled with nothing but photons , we call it a ' photon gas ' . the energy of the light changes the cosmological dynamics relative to an empty universe . this is called a ' radiation dominated ' cosmology . this model is actually a pretty good approximation for the early universe , which is filled with very fast , very small particles . in that sense by learning about the dynamics of the early universe we can study the gravitational field of light .
giving a precise answer to this is i suspect impossible , as the very notion of physical existence is quite subjective . please therefore treat this answer as subjective - i would not expect all physicists to agree with it . but here goes as a first stab , id be inclined to reason a bit like this : your suggested philosophical definition of having extension in space does correspond to human instinct about what existence means , so lets see if we can refine it to match physics : you need to say , spacetime not space , since time is  so far as we can tell - not separate from space . it may be better to say something like forms part of spacetime rather than in spacetime . thats because most physicists would view spacetime itself as having physical existence . now you have a definition something like  x physically exists if x forms a part of spacetime  . which is quite good , because all the physical objects around us ( like electrons , and photons , and tables , and snazzy new hi-fis ) can be regarded as perturbations of spacetime , and therefore being part of space-time rather than merely existing in it . so this definition does match intuition quite well . there is something more though . physics is ultimately only interested in whether something causes measurable effects  because that is after all the only thing we are capable of discovering about the universe we live in . suppose something existed in the universe  say 2.5m x 1.5 m x 0.5m in size - that had no ability to affect us or any of the objects we can measure in any manner whatsoever ( the invisible pink unicorn ; - ) ) . such an object would be completely undetectable to us and therefore of no interest to physics . indeed , a scientist would probably argue that my words in the previous sentence suppose something existed . . .  are nonsensical , since , if theres no way for us to detect it , then as far as we are concerned , it is completely imaginary , and therefore cannot be seen as existing in any real sense . i suspect that is where physics would differ from philosophy : physics would not consider such an object to have any physical existence , whereas philosophy would consider the theoretical possibility of that object existing ( after all , ive just given its dimension , so it does have extension in space ! ) because of that reasoning , id be inclined to add an additional criteria to a physicists definition of physical existence : x physically exists if x is part of spacetime and is able ( either directly or indirectly ) to influence other objects that are a part of spacetime and are detectable to our senses . i still dont think that definition is quite complete , although it may well be as close as you can reasonably get . the main issue with it is it doesnt account for things like logic or ideas or beauty  clearly , all those things do influence us , and therefore are detectable ( albeit subjective ) , but most people would say they do not exist in any physical sense . im not sure off the top of my head what precisely makes those things not physical , and therefore what else might need to be added to my definition .
i agree mostly with jerry and danu in their comments , in that your proposed definition would make much sense , and indeed the boltzmann constant is unity in natural ( planck ) units . a unitless entropy would have a great deal of appeal , especially given the tight links between the thermodynamic entropy and the information-theoretic shannon entropy - they would be equal in your units for a thermalized system of perfectly uncorrelated ( statistically independent ) constituents - the special case envisaged by boltzmann 's stosszahlansatz ( molecular chaos , although boltzmann 's own word means " collision number hypothesis" ) . your entropy would then be measured in nats : you would have to use units wherein the boltzmann constant were $\log 2$ to get entropy in bits . note , however , that the thermodynamic entropy calculated from marginal distributions $n \sum p_j \log p_j$ is not in general equal to the shannon entropy in these units : one in general has to take into account correlations between particles , which lessens the entropy ( because particle states partially foretell other particle states ) . see , for a good explanation of these ideas e . t . jaynes , " gibbs vs boltzmann entropies " , am . j . phys . 33 , number 5 , pp 391-398 , 1965 as well as many other of his works in this field there is one last point to note , however , and that is that the idea of temperature being proportional to average constituent energy is in general only approximately true . it is true for an ideal gas , as you know . however , the most general definition of thermodynamic temperature is that the efficiency of an ideal reversible heat engine working between two infinite heat reservoirs at different temperatures defines the ratio of these temperatures : it is : $$\frac{t_1}{t_2} = 1 - \eta$$ where $\eta$ is the efficiency of the heat engine , $t_2$ the higher temperature and the temperature of the reservoir which the engine draws heat from and $t_1$ the lower temperature of the other reservoir which the engine dumps waste heat into . once a " standard " unit temperature is defined ( e.g. as something like that of the triple point of water ) , then the full temperature definition follows . this definition can be shown to be equivalent to the definition ( in your units , with $k=1$: $$t^{-1} = \partial_u s$$ i.e. the inverse temperature ( sometimes quaintly called the " perk" ) is how much a given system " thermalizes " in response to the adding of heat to its internal energy $u$ ( how much the system rouses or " perks up" ) . see for a good summary the section " second law of thermodynamics " on the wikipedia page for temperature . so let 's apply this definition to a thermalized system of quantum harmonic oscillators : suppose they are at distinguishable positions . at thermodynamic equilibrium , the boltzmann distribution for the ladder number ( number of photons / phonons in a given oscillator ) is : $$p ( n ) = \left ( e^{\beta\ , \hbar\ , \omega }-1\right ) e^{-\beta\ , \hbar\ , \omega \ , ( n+1 ) }$$ the mean oscillator energy is then : $$\left&lt ; e\right&gt ; = \frac{\hbar\ , \omega}{2}\ , \coth\left ( \frac{1}{2}\ , \beta\ , \hbar\ , \omega \right ) $$ the shannon entropy ( per oscillator ) is then : $$s = -\sum\limits_{n = 0}^\infty p ( n ) \log p ( n ) = \frac{\beta\ , \hbar\ , \omega\ , e^{\beta \ , \hbar\ , \omega}}{e^{\beta\ , \hbar\ , \omega}-1} - \log \left ( e^{\beta\ , \hbar\ , \omega}-1\right ) $$ so the thermodynamic temperature is then given by ( noting that the only way we change this system 's energy is by varying $\beta$ ) : $$t^{-1} = \partial_{\left&lt ; e\right&gt ; } s = \frac{\mathrm{d}_\beta s}{\mathrm{d}_\beta \left&lt ; e\right&gt ; } = \beta$$ but this temperature is not equal to the mean particle energy at very low temperatures ; the mean particle energy is : $$\begin{array}{lcl}\left&lt ; e\right&gt ; and = and \frac{1}{\beta}+\frac{1}{12}\ , \beta\ , \hbar^2\omega^2-\frac{1}{720}\ , \beta^3 \ , \hbar^4\ , \omega^4+\frac{\beta^5\ , \hbar^6\ , \omega^6}{30240}+o\left ( \beta^7\right ) \\ and = and t+\frac{1}{12}\ , t^{-1}\ , \hbar^2\ , \omega^2-\frac{1}{720}\ , t^{-3}\ , \hbar ^4\ , \omega^4+\frac{t^{-5}\ , \hbar^6\ , \omega^6}{30240}+o\left ( t^{-7}\right ) \end{array}$$ so that you can see that the your original definition as the mean particle energy is recovered for $t&gt ; &gt ; \hbar\omega$ , the photon energy .
the reason is simple . with the existence of external magnetic field , the coupling involve the magnetic moment quantum number $m$ and the z-component of angular momentum $l_z$: $$\mathcal{h}\propto \vec{b}\cdot \vec{l_z}$$ however , in quantum mechanics , the magnitude of possible $l_z$ are strictly less than magnitude of total angular momentum $l$: $$|l|=\sqrt{\ell ( \ell+1 ) }\hbar$$ $$|l_z|=m\hbar$$ where $m\le \ell$ . so , how is it possible in the classical sense ? the only possibility is that the angular momentum vector $\vec{l}$ is not align with the external magnetic field . it cant move toward z-axis either , otherwise , it will violate the above principle . mathematically , you can solve the hamiltonian and find the expectation value of all $\langle l_x\rangle$ , $\langle l_y\rangle$ , $\langle l_z\rangle$ and you will see that there is sinusoidal oscillation in x- and y- direction . and there is a angle between $l_z$ and $l$ that is exactly equal to $\cos^{-1} ( l_z/l ) $ , which is exactly the meaning of preccession .
the key to this kind of problem is ( i ) to think of a lens as a fourier transformer and ( ii ) use the principle of linear superposition . take a look at my drawing below ( it is one i drew to train people in the use of infinity conjugate optics , so do not worry about the " tested objective" ) . the key point here is that a point source on the focal plane of a lens transforms into a plane wave ( or an approximation thereto , limited by the system 's aperture ) tilted at an angle $\theta$ to the optical axis given by $\tan\theta= \frac{r}{f}$ , where $r$ is the transverse distance of the point source from the optical axis . you can derive this from a simple ray diagram : draw a ray from the point source through the optical centre and you have got it . the ray , in the wave world , represents the wavevector , whose direction is the direction of propagation of a plane wave . so , we have , roughly , making a paraxial approximation $$\delta ( \vec{x} - \vec{x}_0 ) \ , \leftrightarrow\ , \exp\left ( i\ , \frac{k}{f}\ , \vec{x}\cdot \vec{x}_0\right ) \tag{1}$$ where i write on the left the field distribution on the focal plane and on the right the output field distribution over the transverse plane through the optical centre of the equivalent thin lens , or at the system aperture . so now you simply use linear superposition to calculate the field distribution on the output transverse plane when the input field is $g ( x , \ , y ) $ on the focal plane $\mathscr{f}$ or , re-writing it in terms of an inner product : $$g ( \vec{x} ) = \int_\mathscr{f} \delta ( \vec{x}^\prime - \vec{x} ) \ , g ( \vec{x}^\prime ) \ , \mathrm{d}^2 x^\prime\tag{2}$$ by linear superposition of the " basic response " in ( 1 ) as weighted in ( 2 ) , the transverse distribution at the system output must be : $$g ( \vec{x} ) = \int_\mathscr{f} \exp\left ( i\ , \frac{k}{f}\ , \vec{x}\cdot \vec{x}\right ) \ , g ( \vec{x} ) \ , \mathrm{d}^2 x$$ where $\vec{x}$ is the transverse position in the output plane . this is , of course , the fraunhofer diffraction integral .
say the density of the string is $\mu$ and the tension is $t$ . it is clear that the kinetic energy of an infinitesimal piece of string is $$dt = \frac{1}{2} ( \mu \ , dx ) u_t ( x ) ^2$$ the length of the infinitesimal piece of string from $ ( x , u ( x ) ) $ to $ ( x + dx , u ( x + dx ) ) $ is \begin{align} d\ell and = \sqrt{dx^2 + ( u ( x+dx ) - u ( x ) ) ^2} \\ and = \sqrt{dx^2 + ( u_x ( x ) dx ) ^2} \\ and = dx \sqrt{1 + u_x ( x ) ^2} \end{align} for small displacements , $u_x ( x ) \ll 1$ , and \begin{align} d\ell - dx and = \left ( \sqrt{1 + u_x ( x ) ^2} - 1\right ) \ , dx \\ and \approx \left ( 1 + \frac{1}{2}u_x ( x ) ^2 - 1\right ) \ , dx \\ and = \frac{1}{2} u_x ( x ) ^2 \ , dx \end{align} to extend the string by a small increment requires doing work against the tension force , $dw = t \ , ( d\ell - dx ) $ , so the potential energy of an infinitesimal piece of string from $x$ to $x + dx$ is \begin{equation} dv = \frac{1}{2} t u_x ( x ) ^2 \ , dx \end{equation} and we can write down the action as \begin{equation} s = \iint \mathcal{l} ( x , t ) \ , dx \ , dt \end{equation} with lagrangian density \begin{equation} \mathcal{l} ( x , t ) = \frac{1}{2} \mu u_t ( x , t ) ^2 - \frac{1}{2} t u_x ( x , t ) ^2 \end{equation} the euler--lagrange equation for an action of this form is \begin{equation} \frac{\partial \mathcal{l}}{\partial u} = \frac{\partial}{\partial x}\frac{\partial\mathcal{l}}{\partial u_x} + \frac{\partial}{\partial t}\frac{\partial\mathcal{l}}{\partial u_t} \end{equation} so \begin{gather} 0 =\frac{\partial}{\partial x} ( - t u_x ) + \frac{\partial}{\partial t} ( \mu u_t ) \\ u_{tt} = \frac{t}{\mu} u_{xx} \end{gather} and the speed is $c = \sqrt{\frac{t}{\mu}}$ .
you have an open system , i.e. the heat is removed from gas through its walls . when the gas cools down , it shrinks , so the piston will squeeze into the chamber . since you still apply the force , gas will heat up , and heat will be removed through walls again . while gas shrinks more it'll be more and more difficult to remove heat from walls , because of finite heat transfer of the walls . at some point the gas will shrink so slowly that you may say it simply stopped shrinking at all for all practical purposes .
no , because there is only an attractive force between the magnet and the iron , but the combined system of magnet , iron and pipe has no change in its net momentum , since there is no external force . if the entire system would accelerate , it would gain momentum and kinetic energy without an external force . but since momentum and energy are conserved , this is impossible . let 's look at the individual forces acting*: the magnet acts on the iron with a force $f$ the iron acts on the magnet with a force $-f$ both forces cancel each other out to exactly $0$ . *i assume the pipe to be infinitely rigid .
the special theory of relativity is really enough to see that gravitational signals have to propagate by the speed $c$ which we call " speed of light " because the light is the most commonly understood entity that is moving by this maximum speed . special relativity is ok to describe infinitesimal deformations of spacetime . all other massless particles also have to propagate by the same speed $c$ because this speed $c$ is needed to enhance the vanishing rest mass to a finite total relativistic energy . and gravitons are inevitably massless because they do not pick any preferred reference frame - or , alternatively , because gravity is a long-range force . massive particles could only induce short-range forces ( similar to the weak nuclear force caused by w , z bosons ) . any particle - e.g. neutrino - whose energy is much greater than the rest mass is moving nearly by the speed of light , too . the same thing would hold for massless scalar particles such as the " moduli " ( their quanta ) if they existed . it is an elementary consequence of the formulae of special relativity . the speed of light is the maximum speed that the information and material objects may pick , by causality , and it is also the typical speed that massless ( exactly ) and light ( approximately ) particles actually choose . so the answer to your last question is no , the appearance of the same speed $c$ does not imply any additional dynamical relationship between electromagnetism and gravity - it is a direct and elementary consequence of the special theory of relativity - and its kinematics - that was fully understood in 1905 . the importance of the speed $c$ in the scheme of things - because of special relativity - is so high that adult physicists use units in which $c=1$ and they are never ever surprised when $c$ plays an important role - it is exactly the same degree of " surprise " as if the number $1$ appears somewhere in maths .
overblowing is a phenomenon that exists in all wind instruments . the details of the physics are different from one instrument to the next , but there is a broad similarity , which is that it is the result of a nonlinear interaction between the air column and whatever is driving the air column . the recorder is in fact one of the simpler examples to understand . the mechanism that drives the air column is called an edge tone . the mouthpiece of the recorder contains a knife edge . the stream of air encounters the knife edge , but does not split smoothly onto the two sides . instead , it forms a vortex which carries the energy to one side of the edge . however , a feedback process then causes this pattern of flow to deflect until it flips to the other side of the edge . so this is a highly nonlinear system . it is binary . air either flows to one side of the edge or the other , and if we label the two states 0 and 1 , we get a pattern over time that looks like 0000111100001111 . . . you could graph it as ( approximately ) a square wave . when this edge-tone system is coupled to an air column , it is forced to accomodate its frequency to the resonant frequencies of the column . for example , if you imagine a pulse emitted from the edge , this pulse then travels down the tube , is partially reflected at the open end , returns , and slaps against the air in the edge-tone system , influencing its evolution . there is a tendency for the edge-tone system 's vibrations to become locked in to one of the resonant frequencies of the column . in overblowing , the pattern switches from 0000111100001111 . . . to 00110011 . . . the square wave doubles its frequency from the fundamental frequency $f_0$ to the first harmonic $2f_0$ . the original square wave contained fourier components $f_0$ , $2f_0$ , $3f_0$ , . . . the new one contains $2f_0$ , $4f_0$ , $6f_0$ , . . . as you observed on the oscilloscope , $f_0$ is absent from the overblown spectrum . the ear 's sensation of pitch is based on the frequency of the fundamental , so we hear a jump in pitch . bamboo flutes and whistles also use edge tones , so exactly the same analysis applies . i think the original classic work on this was an analysis of organ-pipe acoustics in a german-language paper by cremer and ising . in general , the edge-tone system could be replaced by a reed , lip reed ( as in brass instruments ) , or air reed ( flute ) . there can be overblowing at the octave , or , in instruments such as the clarinet that have asymmetric boundary conditions , at an octave plus a fifth ( i.e. . , a factor of 1.5 in frequency ) . on the saxophone , for example , a skilled player using a stiff reed can overblow to frequencies corresponding to several higher harmonics beyond the first . references : cremer and ising , " die selbsterregten schwingungen von orgel , " acustica 19 ( 1967 ) 143 . fletcher , " sound production by organ flue pipes , " j acoust soc am 60 ( 1976 ) 926 , http://www.ausgo.unsw.edu.au/music/people/publications/fletcher1976.pdf backus , the acoustical foundations of music , norton , 1969 , pp . 184-186 .
if you can accept schrdinger 's equation , i can give you a motivation of born 's rule . the wave function psi ( x ) completely specifies the system 's state ( let 's talk about an electron ) . therefore , the probability ( here : to find the electron at x ) must be some functional of the wave function . schrdinger 's equation describes the temporal dynamics of the wave function . considering this , you have the requirement that the functional must such that the dynamics does not change the total probability . eventually , the expression should be easy . then , you are left with the wave functions absolute square ( not just the square ! ) . @nervxxx : this is not really true . in maxwell 's electrodynamics you can pretty easily derive a continuity equation ( with source term ) and identify it with the energy . then you get an expression ~ e + b for the energy density . note : you get e , not |e| . the electric field is always a real number . it is convenient to use complex numbers ( including a wave 's phase ) in the calculation and take the real part later . but you have to take the real part before taking the square because the complex/real trick only works for linear operations . so you should write ( re e )  .
just to add another look to the answer of paul . as you know any thermodynamic potential has its own variables . as the function of its variables it achieves a minimum in equilibrium . entropy is one of such potentials that can be used on the equal basis with the others . the only difference is that it achieves minimum rather than maximum . it appeared historically this way , that entropy has been defined as it is . put minus in front of it , and it will be as all the others . ok , its own variables are the internal energy , e , the volume , v , and the number of particles n . the first of these , the internal energy , is very inconvenient in use . one typically cannot measure this parameter independently . it is not often evident , how to calculate it into measurable parameters to compare with experiment . for this reason the entropy is rarely used . in fact the philosophy behind is that you first define the set of variables that is adequate to the problem , and then work with the corresponding potential . the set ( v , t , n ) corresponds to the free energy . use that ! second . you write : [ . . . ] and more disorder means less symmetry in the system . ( or is this statement wrong ? ) you are definitely wrong . below i just give you a counter-example that will clarify the situation : a ) the symmetry group of a crystalline solid is a discrete group with ( i ) translations over a discrete set of vectors , rotation over discrete set of angles , and reflection in a discrete set of planes . eventually the symmetry might be even more complex , but this does not influence my example . b ) introducing a disorder one may transform this crystal into amorphous solid . this has the symmetry group with continuous translations and rotations and the infinite set of the mirror planes . this group is continuous , the so-called , euclidean motion group . any symmetry group of any crystal is its subgroup . thus , by increasing the disorder we increased the symmetry . c ) by further increasing the disorder ( say , by increasing the temperature ) we may bring our solid into the liquid state . here the symmetry is still higher , since it allows all movements with a constant volume . the euclidean motion group is the subgroup of this one . one may also give a number of such examples within the solid state . though it is typical that during phase transitions a high-temperature phase has a higher symmetry , there are also opposite examples . they are too specific and i will not give them here . there are also examples for transitions between solid states with the symmetry groups that have no group-subgroup relations between one-another . in this case one cannot decide which symmetry is higher . to summarize all this one should say that there is no solid rule about the relation between disorder and symmetry , though increasing disorder is often followed by the symmetry increase .
this is one of those problems that take on a much neater solution if you go dimensionless . . . if you take a new time , $\tau$ , horizontal coordinate , $\xi$ , and vertical coordinate , $\eta$ , such that $$\tau = t \gamma , \ \xi = x \frac{\gamma}{v_0 \cos \alpha} , \ \eta = y \frac{\gamma}{v_0 \sin \alpha}$$ you can rewrite your equations as $$\ddot{\xi} + \dot{\xi} = 0$$ $$\ddot{\eta} + \dot{\eta} = -\frac{g}{\gamma v_0 \sin\alpha} = -\lambda$$ with initial conditions at $t=0$ $$\xi = \eta = 0 , \ \dot{\xi} = \dot{\eta} = 1$$ this has solutions $$\xi = 1 -e^{-\tau} , \ \eta = ( 1+\lambda ) ( 1 -e^{-\tau} ) - \lambda \tau$$ and setting $\eta = 0$ , we can get rid of $\tau$ , and the range will be $\xi^*$ that fulfills $$-\frac{1+\lambda}{\lambda}\xi^* = \log ( 1-\xi^* ) $$ the maximum range will happen when $dx^*/d\alpha = 0$ , so it is easy to show that at the maximum range : $$\frac{d}{d\alpha}\frac{1}{\lambda} = \frac{1}{\lambda \tan \alpha}$$ $$\frac{d\xi^*}{d\alpha} = \xi^* \tan \alpha$$ differentiating the equation for $\xi^*$ w.r.t. $\alpha$ using these two last formulas will , after a lot of cancelling and rearranging , take you to $$\xi^* = \frac{1}{1+\lambda \sin^2 \alpha}$$ you can use this relation in the first expression we got for $\xi^*$ to get rid of $\xi^*$ and get a trascendental equation for $\alpha$ that looks something like : $$\frac{\sin \alpha + \lambda^*}{\lambda^*}\frac{1/\lambda^*}{1/\lambda^* + \sin \alpha} = \log ( 1/\lambda^* +\sin\alpha ) - \log ( \sin \alpha ) $$ where $\lambda^* = \lambda \sin \alpha$ and is independent of $\alpha$ . this last equation you would want to solve numerically to get $\alpha^*$ , the angle producing the maximum range . while messy , the solution clearly depends on $\lambda^*$ , so there will not be a single angle that always works . edit just run this last equation through a numerical simulation , and here 's the optimal angle $\alpha^*$ ( actually $\sin\alpha^*$ ) as a function of $\lambda^*$ , showing how it grows ever closer to $\sqrt{2}/2$ for $\lambda^* \to \infty$ .
the photons move at the speed of light in a straight line from the laser to the moon and back . the spot on the moon can move faster than light . there is no law against that . the spot is not a physical object , just an image . when you turn your wrist nothing happens to the photons which are already on the way to the moon - they continue on the same trajectory . but new photons are emitted in the new direction of your laser . it is like waving a garden hose back and forth .
the difference is the direction the light is emitted in . mirrors ' bounce ' light in a predictable direction , white objects scatter light .
i think that your skepticism comes about because you intuitively think that the force of kinetic friction should change gradually to static friction as the ball speeds up , since the relative motion between the ball 's spinning surface and the ground decreases to zero . your textbook assumes that this transition is actually instantaneous , and that the kinetic friction force is exactly the same until there is no relative motion at all , at which point the friction is entirely static . it seems counter-intuitive , but that is actually how it is . if you have ever seen someone play a cello or any stringed instrument with a bow , you can see a little better how this works . a cellist steadily moves his bow across the string , which vibrates as it slips and catches on the sticky bow , switching between kinetic and static friction . if friction had a gradual transition , i imagine that the bow would just push the string to a certain displacement and reach equilibrium , and we had be robbed of a great instrument , and a lot of excellent music . does your problem make more sense now ? the angular acceleration and the torque are constant , because the kinetic friction force is constant until it disappears . once the ball spins with the floor , there is no torque on the ball because it is just rollingto the floor it seems to be still . but as the ball is spinning up ( and the slipping is slowing down ) , the force on it remains the same . friction is just weird like that .
i am quite sure that you have left something . the more their mass , they have got more momentum and of course , inertia . the heavier the body is , the more their body resists their state of motion ( or rest ) . yes . they move faster than skinny guys . if you resolve the forces acting on the skier like his acceleration , gravity , friction ( of snow ) , the normal force and also his air drag , you can find that his velocity does depend on his mass . resolving all the forces , we get $$ma=mg\ cos\theta-\mu\ mg\ sin\theta-\frac{1}{2}\rho av^2c_d$$ $$a+\frac{1}{2m}\rho av^2c_d=g\ cos\theta-\mu\ g\ sin\theta$$ as the mass is in the denominator portion , we can clearly see that it supports his acceleration ( it should increase in order for balancing other forces ) by opposing gravity .
it would depend on damping effects being taken into account or not . invoking newton 's 2nd law of motion , a differential equation for the motion of a damped harmonic oscillator can be written ( including an external , sinusoidal driving force term ) : $m\frac{d^2x}{dt^2}+2m\xi\omega_0\frac{dx}{dt}+m\omega_0^2x=f_0\sin\left ( \omega t\right ) $ where $m$ is the inertial mass of the system , $\omega_0$ is its characteristic frequency , $\xi$ a dimensionless damping factor . . . and , last but not least , where $f_0$ is the amplitude of the driving force and $\omega$ its frequency . the stationary ( $t\rightarrow\infty$ ) solution takes the shape $x\left ( t\right ) =a_0\sin\left ( \omega t-\varphi_0\right ) $ , where $a_0$ is an amplitude factor ( whose particular expression in terms of the particular parameters is not relevant to this question ) and $\varphi_0$ is phase lag , which is this phase difference you are asking about . this phase difference can be calculated to be $\varphi_0=\left|\arctan\left ( \xi\frac{2\omega\omega_0}{\omega^2-\omega_0^2}\right ) \right|$ . it is a phase lag , so with the ( implicitly ) chosen phase convention , it has to be positive . if there was no damping whatsoever in the system , $\xi$ would be zero , and you would be right : $\varphi_0=0$ . the stationary motion of the oscillator would be in phase with the driving force ( regardless of which is the relationship between $\omega$ and $\omega_0$ ) . but in an undamped resonant situation the amplitude $a_0$ diverges , which means that the stationary solution is never reached ( starting from reasonable , finite initial conditions for the system ) . also , in a physical down-to-earth situation , the system would eventually breakdown somewhere , somehow , since energy is being introduced into the system with perfect efficiency ( that is what ' resonance ' is all about ) and without any means to dissipate it . somewhere , sooner or later , something would go boom or crash . that is how nasty undamped resonances are . on the other hand , for a non-zero damping , in the resonant case $\omega=\omega_0$ , the argument of the $\arctan$ function diverges , so the phase difference turns out in this case to be $\frac{\pi}{2}$ . to sum up , the $\frac{\pi}{2}$ phase appears as an effect of damping in the system , and just a little bit of it is enough to offset the oscillatory response from the system . as it happens , every realistic , down-to-earth harmonic system has some kind of damping in its dynamics . even if the damping is so small that the induced dephasing in an out-of-resonance situation is negligible for every purpose that the model has , damping has to be taken into account in resonant and closely resonant motion , otherwise the model yields highly unphysical results .
it is the error created by photons striking on elementary particles it is not . heisenberg 's uncertainty principle actually has nothing to do with any particular experiment , or any particular interaction . it is a purely mathematical statement about waves . its true meaning is explained in detail on the wikipedia page , but the gist is that if you have a wave , you can express it as a function of position , $\psi ( x ) $ , or of momentum , $\phi ( p ) $ . these two functions are fourier transforms of each other . you can then calculate the variance of each function , $\sigma_x^2$ and $\sigma_p^2$ respectively , using formulas given on wikipedia , and you will find that these two quantities obey the relationship $$\sigma_x \sigma_p \ge \frac{\hbar}{2}$$ since $\sigma$ is a measure of how tightly concentrated the wave is around one particular point , this tells you that a wave which is tightly concentrated in position must be fairly spread out in momentum , and vice versa . ( for the proper definitions of " concentrated " and " spread out , " of course . ) the only way this connects to measurement is that , if you make a series of position measurements on objects with the same quantum state , the variance of those measurements will tend to the variance of the wavefunction . and similarly for momentum . so with a large number of measurements of both position and momentum , if you compute their variances , you will find that they have to satisfy that inequality . in a sense , it is a statement about the particle 's state before it gets hit with a photon ( or something else ) , not some effect of the photon hitting it .
here is a representation of the hydrogen atom energy levels . it displays the availabe solutions of the schrodiner equation for an atom composed of a proton in the nucleus and an electron existing in their mutual potential . systems stay in the minimum energy state , and for the single electron of hydrogen the minimum energy state is the n=1 state and the value of that energy is -13.6 ev . it can happen that a photon of 10.2 ev scatters the electron to the n=2 state . this will be an unstable solution because there exists an empty lower energy state and the electron will radiate back to n=1 . the same is true for the higher n states to which the electron can get scattered , and then can cascade down to the ground state . the radiation from these excitations is a spectrum measurable in the lab and it is how we know we have the correct quantum mechanical model of the hydrogen atom . a second electron has no meaning in this solution of the hydrogen atom , which has zero charge as an atom . a second electron will not be attracted because there is no potential atom+ second electron . each atom has as many electrons as there are protons in the nucleus and there will be solutions that will give the energy levels those electrons can occupy . for z=2 and higher the pauli exclusion principle does not allow two electrons in the same energy state . it is worth looking up helium to get an idea of the complexity of the energy levels of multi electron atoms and the role of the pep .
let 's recall basics of classical and quantum mechanics for non-statistical systems . in classical hamiltonian mechanics , one models the non-statistical state of a system as a point in phase space $\mathcal p$ . if the configuration space ( space of spatial positions ) of the system is $n$-dimensional , then the phase space is $2n$ dimensional because the state of the system is described both by its position and its momentum . the time evolution of the system is governed by the hamiltonian $h$ , a real-valued function defined on phase space , in terms of which the dynamical equations of the system , hamilton 's equations , are written . in quantum mechanics , one models a non-statistical state of a system as a point ( vector ) in a hilbert space $\mathcal h$ , a complex vector space that can be infinite-dimensional . the time evolution of the state of the system is government by the hamiltonian $\hat h$ , a self-adjoint operator on $\mathcal h$ in terms of which the dynamical equation of the system , the shrodinger equation , is written . when one moves to statistical mechanics , then the state of a classical mechanical system is no longer modeled as a point in phase space , but rather as a probability density $\rho$ on phase space . this probability density encodes the fact that one is ignorant about the exact states ( positions and momenta ) of individual particles in the system , and can be thought of in terms of ensembles of identically prepared systems , and one becomes primarily concerned with computing statistical quantities like ensemble averages of a given observables for a given phase density . the phase density will take different forms based on the ensemble ( namely based on how the macroscopic state of the system in prepared ) , and for a given ensemble , one can define an object called the partition function which allows one to compute , for example , the ensemble average of any observable for a system in that ensemble . for example , the partition function for $n$ identical particles in the canonical ensemble in classical mechanics will be the following phase space integral : \begin{align} z ( \beta ) = \frac{1}{n ! h^{3n}}\int d^{3n}pd^{3n}q\ , e^{-\beta h ( p , q ) } \end{align} where $\beta = 1/kt$ . in quantum statistical mechanics , the state of the system is again no longer modeled in the same way ( as a vector in hilbert space ) , but rather as a non-negative self-adjoint operator $\hat \rho$ of unit trace called the density operator ( or density matrix ) . as in the classical case , one uses this operator to determine statistical quantities such as ensemble averages of observables . in particular , one can again compute the partition function as in the classical case , but the expression will be different . concretely , it its the hilbert space trace of the density operator ; \begin{align} z = \mathrm{tr}\hat \rho = \sum_i \langle i|\hat \rho|i\rangle \end{align} where $\{|i\rangle\}$ is a basis for the hilbert space . in particular , for $n$ identical particles , one needs to careful to distinguish between the computation one does with fermions , and that performed for bosons . for $n$ identical fermions , the hilbert space of the system will be restricted to the antisymmetric subspace of the $n$ particle hilbert space . on the other hand , if the system consists of identical bosons , then the hilbert space of the system is restricted to be the symmetric subspace of the $n$-particle hilbert space . it follows that , for example , when one determines the partition function for such systems , one needs to trace over the appropriate hilbert space . since the antisymmetric and symmetric hilbert spaces do not generally coincide , the partition functions for fermionic systems will generally be completely different than those of bosonic systems .
the relationship that sakurai seems to be talking about is a mathematical one : both the schr:odinger equation in wave mechanics and models of other physical systems , like small vibrations of a stretched elastic membrane , reduce to solving eigenvalue problems with elliptic differential operators . in the quantum mechanical setting , the operator is the hamiltonian , the eigenvectors are the stationary solutions , and the eigenvalues the energies . in the vibrating membrane context , the operator is the laplacian , the eigenvectors are the normal modes , and the eigenvalues are ( minus the squares of ) the corresponding frequencies of oscillation . the physics is different , and sakurai is not suggesting that the two things are physically related . one problem is ' as straightforward as ' the other because they are mathematically closely related .
you are basically assuming an infinite mean free path for the air molecules , whereas people normally would use the navier-stokes equations which assume an infinitesimal mean free path . you will therefore underestimate the pressure difference . further , instead of solving the full fluid flow problem , people normally simply model a hole as " an impedance to flow" ; flow rate scales with the hole area and with the square root of the pressure difference . from your problem statement , it seems this scale constant was given separately .
there are two manifolds that are involved in string propagation . the spacetime in which the string propagates . the worldsheet of the string itself . the fields $x^\mu$ are embedding coordinates of the worldsheet in the spacetime manifold . this means that for each point $ ( \sigma^1 , \sigma^1 ) $ on the worldsheet , $x^\mu ( \sigma^1 , \sigma^2 ) $ gives the coordinates of that point in the spacetime manifold . in the case you are considering , the spacetime is taken to be minkowski , so the metric is $\eta_{\mu\nu}$ . now we could ask " given that the worldsheet is a two dimensional embedded submanifold of minkowski space , is there some way that this manifold inherits its metric from the metric on the ambient spacetime ? " this question is analogous to " given that the sphere $s^2$ is some two-dimensional embedded submanifold of euclidean space $\mathbb r^3$ , is there some natural sense in which it inherits its metric from $\mathbb r^3$ ? the answer to both of these question is yes , and the metric on the submanifold that does this is precisely the induced metric . the formula expression the induced metric for a two-dimensional submanifold of some ambient manifold with metric $g_{\mu\nu}$ ( not necessarily flat ) in terms of embedding coordinates is $$ \gamma_{ab} ( \sigma ) = g_{\mu\nu} ( x ( \sigma ) ) \partial_ax^\mu ( \sigma ) \partial_b x^\nu ( \sigma ) , \qquad \sigma = ( \sigma^2 , \sigma^2 ) $$ you are right about the derivation of the induced metric , it comes from demanding that the distance measured between points on the embedded submanifold is calculated to be the same number whether you use the ambient metric , or the induced metric . to see that the above expression for the induced metric does this , simply note that the infinitesimal distance between any two points on the embedded submanifold can be written in terms of the ambient metric and the embedding coordinates as \begin{align} g_{\mu\nu} ( x ( \sigma ) ) d ( x^\mu ( \sigma ) ) d ( x^\nu ( \sigma ) ) and = g_{\mu\nu} ( x ( \sigma ) ) \partial_a x^\mu ( \sigma ) \partial_bx^\nu ( \sigma ) d\sigma^ad\sigma^b \\ and = \gamma_{\mu\nu} ( \sigma ) d\sigma^ad\sigma^b \end{align} to get some intuition for all of this , recall that expression for embedding coordinates of $s^2$ in $\mathbb r^3$ is \begin{align} x ( \theta , \phi ) and = \sin\theta\cos\phi\\ y ( \theta , \phi ) and = \sin\theta\sin\phi\\ z ( \theta , \phi ) and = \cos\theta \end{align} and using these embeddings you should be able to show that the metric on the sphere is simply $$ \gamma_{ab} ( \theta , \phi ) = \mathrm{diag} ( 1 , \sin^2\theta ) $$ let me know if that is unclear or if you need more detail !
black holes affect the causal structure of spacetime in such a manner that all future light cones within a black hole lie within the event horizon of it . although photons are massless they have energy and have to obey the geometry of a curved spacetime . since all future lies within the event horizon , photons are trapped inside the black hole .
when a nucleus makes an alpha or beta decay , usually it is left in an excited state . it can make the transition to lower energy state by emitting gamma rays , so in a sense , the atom is more " stable " because the nucleus is in a lower energy state . i am not sure if this answer your question . hope it helps .
i think your derivation is fine . in general we cannot measure the velocity of the secondary directly , but only the " projected " velocity $v_s \sin ( i ) = k_s$ , where $i$ is the inclination of the orbital axis to the line of sight ( $i=90^{\circ}$ is an orbit seen edge-on ) . in those circumstances your derivation becomes $$ \frac{m_{p}^{3} \sin^3 i}{ ( m_p + m_s ) ^2} = \frac{p k_{s}^{3}}{2\pi g} $$ the right hand side contains observable quantities , the left hand side contains the masses and the inclination angle . if only the secondary ( projected ) velocity can be measured this is as far as you can go without making assumptions about the relative masses of the components and the inclination . to obtain the formula you quote then indeed you have to say that $m_p \ll m_s$ and that $v_s$ is the measured projected velocity ( i.e. . that $i=90^{\circ}$ ) .
i interpreted the question as asking whether it is possible to show that a sphere is the minimum energy shape of an object being acted on by its own gravity . i attempted to do this using spherical harmonics , but got stuck part of the way there ; i am posting this anyways just in case someone can figure out how to complete the last bit . the gravitational self-energy of a matter distribution $\rho ( \mathbf{r} ) $ which gives rise to a gravitational potential $v ( \mathbf{r} ) $ is given by $$e=\frac{1}{2}\left\langle v , \rho\right\rangle=\frac{1}{2}\left\langle \nabla^{-2}\rho , \rho\right\rangle$$ where $\langle , \rangle$ represents the inner product and $\nabla^{-2}$ is the inverse laplacian . if there is a change in the matter distribution $\rho=\rho_0+\delta\rho$ , then the self-energy changes to $$e=\frac{1}{2}\left\langle \nabla^{-2} ( \rho_0+\delta\rho ) , \rho_0+\delta\rho\right\rangle=\frac{1}{2}\left\langle \nabla^{-2}\rho_0 , \rho_0\right\rangle+\left\langle \nabla^{-2}\rho_0 , \delta\rho\right\rangle+\frac{1}{2}\left\langle \nabla^{-2}\delta\rho , \delta\rho\right\rangle \\ =e_0+\delta e+o ( \delta^2 ) $$ where the self-adjointness of the laplacian was used in the second to last step . thus it suffices to determine the sign of $\delta e=\left\langle v_0 , \delta\rho\right\rangle$ . since $v_0$ is just the gravitational potential of a uniform-density sphere ( which has simple closed form ) this becomes tractable . let the matter density be $\rho_d$ and assume the planet is incompressible . the radius of the planet $r ( \omega ) $ can be expanded in the real harmonics as $$r ( \omega ) =r_0+\sum_{l=0}^\infty\sum_{m=-l}^l\delta c_{lm}y_{lm} ( \omega ) =r_0+\delta r ( \omega ) . $$ to conserve volume , note that $$v=\frac{4}{3}\pi r_0^3=\frac{1}{3}\int r ( \omega ) ^3\ , d\omega=\frac{1}{3}\int \left ( r_0+\sum_{l=0}^\infty\sum_{m=-l}^l\delta c_{lm}y_{lm} ( \omega ) \right ) ^3\ , d\omega \\ =\frac{4}{3}\pi r_0^3+2\sqrt{\pi}r_0^2\delta c_{00}+r_0\sum_{l=0}^\infty\sum_{m=-l}^l\delta c_{lm}^2+o ( \delta^3 ) \\ \rightarrow \delta c_{00}=-\frac{1}{2\sqrt{\pi}r_0}\sum_{l=1}^\infty\sum_{m=-l}^l\delta c_{lm}^2+o ( \delta^3 ) . $$ since the volume conservation term $\delta c_{00}$ scales as $\delta^2$ let 's rewrite it as $\delta^2c_{00}$ , giving $$r ( \omega ) =r_0+\sum_{l=1}^\infty\sum_{m=-l}^l\delta c_{lm}y_{lm} ( \omega ) +y_{00}\delta^2c_{00}=r_0+\delta r_1+\delta^2 r_2 . $$ with a bit of visualization one can see that $$\left\langle v_0 , \delta\rho\right\rangle=\int d\omega r ( \omega ) ^2\int_{r_0}^{r ( \omega ) }v_0 ( r ) \rho_d\ , dr \\ =\int d\omega r ( \omega ) ^2\left [ -\frac{g m r_1 \rho_d}{r_0}\delta+\frac{gm ( r_1^2-2r_0r_2 ) \rho_d}{2r_0^2}\delta^2+\: . . . \right ] \\ =\int d\omega\left [ -gmr_0\delta r_1\rho_d-\frac{1}{2}gm\rho_d\left ( 3 ( \delta r_1 ) ^2+2r_0\delta^2r_2\right ) \: . . . \right ] $$ where in the last step terms in the integral have been arranged according to their order in $\delta$ . the first term vanishes due to symmetry of the real harmonics , and the second term simplifies due to orthonormality to become $$=-\frac{1}{2}gm\rho_d\left [ 3\sum_{l=1}^\infty\sum_{m=-l}^l\delta c_{lm}^2-2\sum_{l=1}^\infty\sum_{m=-l}^l\delta c_{lm}^2\right ] \\ =-\frac{1}{2}gm\rho_d\sum_{l=1}^\infty\sum_{m=-l}^l\delta c_{lm}^2&lt ; 0 . $$ unfortunately , this predicts that the energy change is always favorable to deformation , rather than always unfavorable ! so i must have an arithmetic error somewhere , but i am too tired to find out where it is ( the units at least work out correctly to joules ) . however , this gives a general idea of how one can proceed down this line of proof .
the total energy in the space does increase , precisely because of the reason you mention . energy is not expected to be conserved , because the metric is not invariant under time translations . what does hold is the first law of thermodynamics , $du = -p dv + \cdots$ . since the pressure in this system is negative , this is one way of seeing the origin of the extra energy as the space grows .
the energy goes nowhere . it needs to go nowhere , since energy conversation only holds for systems which are time translation invariant , and conversation of energy then follows by noether 's theorem . but the universe , as a whole , is not a time-translation invariant system ( or in gr terms , there is no guarantee that we always have the right time-like killing vectors ) , see also this old question . you should not expect energy to be in any form conserved on cosmological scales ( though in sr , and in suitable subsystems , it is ) .
this is a generic phase diagram : i looked up the construction of a demonstration for classes cloud chamber using alcohol . for the air/alcohol gas line 3 , constant pressure , change in temperature is where you want to work for your cloud chamber . cooling it without condensation . the gradient of temperature in your chamber will define the thickness over the iced bottom where supersaturation can be maintained . so your choice of alcohol should depend on how slowly the phase diagram changes with temperature as to have a larger distance from the cold plate where the vapors will be supersaturated and tracks can form . i do not think it is the gradient in the chamber that is decisive , but the one from the triple point to the vapor phase in line 3 . the hot on top is to generate the vapour phase in the chamber . it will all depend on the phase diagram of your specific choice for vapor .
depending on how you define the limits of the virgo supercluster , it is ( probably ) not a gravitationally bound object and the hubble expansion will ( probably ) eventually pull it apart . the virgo cluster is typically taken as the centre of the supercluster , and if you look at the recessional velocities of the galaxies in the virgo cluster they are mostly receding from us at around 1000 km/s ( within an order of magnitude ) . i am not sure if it is known for certain whether the milky way ( or the local group ) is gravitationally bound to the virgo supercluster but i believe the evidence is that it is not . it would be interesting to know how much the mass in the supercluster decreases the expected hubble velocity . the expected recessional velocity is just given by $v = h_0d$ , and with $h_0$ around 70 ( km/s ) /mpc and the centre of the virgo cluster at 16.5 mpc we had expect a recessional velocity for the virgo cluster of 1,150 km/sec . this is about the order of magnitude of the observed recessional velocities , but the trouble is that the velocities of the galaxies that make it up are widely scattered . i do not know if anyone has managed to obtain a meaningful average recessional velocity for the virgo cluster . i have found this article that claims the observed recession is about 10% smaller than expected from hubble 's law . even though the article is on a respected web site ( berkeley ) it gives no detail or references so i would take it with a grain of salt .
mathematics is just a systematic way of stating facts about the world . it is only useful inasmuch as it is internally self-consistent . the latter fact means that there is nothing to " assume " about mathematics . it is a relationship between axioms and conclusions that enables one to succinctly summarize many observations . something like galileo 's gravitational experiments in pisa is necessarily logically prior to a conclusion like " in the absence of air resistance , all objects near to the surface of earth will accelerate downward with an acceleration of $9.8\ , \ , {\rm m/s^{2}}$ . " all the latter statement does is summarize the experiments . and all mathematics gets you is having the power of having that summary turn into an assumption for a later experiment , which can then be checked against other things . in the end , you could , in principle , do physics without mathematics . it would just be infinitely harder to understand any of it . mathematics is just a language that we can use to focus our thinking .
you just do . if a calculation gives you $\pi$ , then you just say it gave you $\pi$ . it is a real number and it is as good as any rational in describing any numeric quantity : $$\text{all physical quantities are described by }\mathbb{real}\text{ numbers} . $$ if you must insist on what real numbers are and how we can even conceive of such a number as $\pi$ , then you need to look carefully at your definition . as far as physics goes , the definition of the real numbers is the real number field $\mathbb r$ is the unique ordered field that is dedekind complete . this is independent of the multiple possible realizations of this set of axioms from a " more basic " set , and of the multiple representations which you might think of using ( such as decimal expansions , continued fractions , dedekind cuts , equivalent classes of cauchy sequences , or what-have-you ) . in describing numeric quantities , we need a structure that allows us to add , substract , multiply and divide as usual , compare one number to another and always conclude that one is greater than ( or equal to ) the other , and take limits , infima , suprema , derivatives , integrals , and so on . the definition above embodies all these requirements . since there is a unique such structure ( up to a natural isomorphism ) , we do not really care what that structure is meant to be . " but , but , but . . . " , i hear you say , " what about rational approximations ? we can never measure an irrational number ! " and yes , that is correct . but we can not measure rational numbers , either . our measurements come with a central value and a finite , nonzero precision : they describe an interval . * if our calculations come out to an irrational number , then all we can say is whether that number is consistent with experiment , and it is exactly the same with a rational number . say that some calculation predicted a value of 4/3 for some quantity : then , in your own words , we can approximate $1.33$ , $1.3333$ , and so forth , increasing the precision , but yet never getting really to $4/3$ . such is life , and such is the mathematical structure of physics . *or a probability distribution , if you insist . same difference , though .
it will help you understand the quantum mechanical picture if you read up on atomic orbitals . these are the loci around the nucleus where the electrons have a probability to be found . you will see that the orbitals have a shape , which depends on the angular momentum of the state . the electrons carry the charge and thus you can interpret the plots as probability of the charge being there . they are not uniformly spherically symmetric except l=0 . l=1 has two lobes and is elongated , and higher spins more lobes . now in the case of an ion , where an electron is missing : when close to an ion , as another atom will be , the shape of the positive charge will be the complement of the hole left by the missing electron , and it will not be uniformly spherically symmetric either , there will be lobes . to assume that the single excess positive charge of the singly charged ion is at the center and be able to apply gauss ' law you have to be far enough that the details of the orbital shapes can be ignored . for milimmeter and micron distances this certainly will be true . for nanometer and smaller it becomes problematic and will depend on the specific problem one wants to address .
velocities are strange beasts . relativity theory tells us rapidities ( the sum of the accelerations experienced by an object ) are more intuitive quantities . rapidities are directly observable quantities ( external observers relate these to blue/redshifts ) . and unlike velocities , rapidities do sum up . velocities depend non-linearly on rapidities , and therefore velocities follow a more complex addition rule . imagine three observers $a$ , $b$ and $c$ all moving along a railway track . observer $a$ measures $b$ to have velocity $v_{ab}$ . from $b$ 's perspective $c$ has a velocity $v_{bc}$ . and to close the circle , from $c$ 's perspective $a$ has velocity $v_{ca}$ . common experience ( encoded in so-called galilean relativity ) tells us these velocities simply add up to zero : $v_{ab} + v_{bc} + v_{ca} = 0$ this is wrong . it ignores a non-linear term that becomes important at speeds approaching the speed of light $c$ . lorentzian relativity tells us the correct equation is : $v_{ab} + v_{bc} + v_{ca} + v_{ab} . v_{bc} . v_{ca} / c^2= 0$ this is all the math you need . give it a try . enter $v_{ab}=0.9 c$ and $v_{bc}=0.9 c$ and see what value you get for $v_{ac} = -v_{ca}$ .
the question seems to be ill-posed . from perspective i must say that mathematics knowledge requires constant improvement . i am doing a phd now . as a student i did msc in physics and separate msc in mathematics . i think i have a good background to study new things , but i have to do that often . for example in quantum mechanics there is a notion of boundedness of operators , whole spectral theory , self-adjoint extension of hamiltonians . i can not imagine a person that learns these in " learn maths " mode . this should be stimulated by physical intuition and done in parallel with learning physics - the life is too short to do it in another way . the best option is to quickly revise calculus , and then to follow a more advanced mathematical analysis course that requires mathematical thinking ( topology , functional analysis , analysis on functional spaces ) . it is not practical to judge one 's level of familiarity with mathematics for physics by checking if one is able to solve problems . calculus is an essential tool , which knowledge is useful , but this is just set of methods to solve standard problems . typically one very soon encounters problems which are not solvable on paper ( equation that require numerical treatment ) . level of mastery of calculus : not so big ( within reason ) . complex analysis ( residues , analytical extensions ) is important . most interesting series are divergent , and mathematicians typically neglect " asymptotic series " . the show goes on . . . as for books try : simon and reed - this gives the overview of useful material , but not many people know this book by heart .
to a good approximation , normal magnets can be treated as dipole magnets , in which case the force between them can be found in this wikipedia article . to avoid link-only answers , here it is : $$\mathbf{f} = \dfrac{3 \mu_0}{4 \pi r^5}\left [ ( \mathbf{m}_1\cdot\mathbf{r} ) \mathbf{m}_2 + ( \mathbf{m}_2\cdot\mathbf{r} ) \mathbf{m}_1 + ( \mathbf{m}_1\cdot\mathbf{m}_2 ) \mathbf{r} - \dfrac{5 ( \mathbf{m}_1\cdot\mathbf{r} ) ( \mathbf{m}_2\cdot\mathbf{r} ) }{r^2}\mathbf{r}\right ] . $$ however , be warned that while equilibrium levitation is possible using magnets , stable equilibria are impossible due to earnshaw 's theorem . magnetic levitation is indeed possible but you need fancier schemes than just two magnets . the wikipedia article on it is a good starting point .
no , due to the redshift the light of stars further away will be shifted more and more towards the infrared ( and beyond ) , becoming invisible to the eye .
no , one does not need to measure the material for years - or even millions or billions of years . it is enough to watch it for a few minutes ( for time $t$ ) and count the number of atoms $\delta n$ ( convention : a positive number ) that have decayed . the lifetime $t$ is calculated from $$ \exp ( -t/t ) = \frac{n - \delta n}{n}$$ where $n$ is the total number of atoms in the sample . this $n$ can be calculated as $$n={\rm mass}_{\rm total} / {\rm mass}_{\rm atom} . $$ if we know that the lifetime is much longer than the time of the measurement , it is legitimate to taylor-expand the exponential above and only keep the first uncancelled term : $$ \frac{t}{t} = \frac{\delta n}{n} . $$ the decay of the material proceeds atom-by-atom and the chances for individual atoms to decay are independent and equal . to get some idea about the number of decays , consider 1 kilogram of uranium 238 . its atomic mass is $3.95\times 10^{-25}$ kilograms and its lifetime is $t=6.45$ billion years . by inverting the atomic mass , one sees that there are $2.53\times 10^{24}$ atoms in one kilogram . so if you take one kilogram of uranium 238 , it will take $2.53\times 10^{24}$ times shorter a time for an average decay , e.g. the typical separation between two decays is $$t_{\rm average} = \frac{6.45\times 10^9\times 365.2422\times 86400}{2.53\times 10^{24}}{\rm seconds} = 8.05\times 10^{-8} {\rm seconds} . $$ so one gets about 12.4 million decays during one second . ( thanks for the factor of 1000 fix . ) these decays may be observed on an individual basis . just to be sure , $t$ was always a lifetime in the text above . the half-life is simply $\ln ( 2 ) t$ , about 69 percent of the lifetime , because of some simple maths ( switching from the base $e$ to the base $2$ and vice versa ) . if we observe $\delta n$ decays , the typical relative statistical error of the number of decays is proportional to $1/ ( \delta n ) ^{1/2}$ . so if you want the accuracy "1 part in 1 thousand " , you need to observe at least 1 million decays , and so on .
as an ex-physicist who now works as a quant in power markets i think it is safe to say the physics of the matter will be swamped by the economics in commodities and how power markets work . two things to note : power prices are set by markets and not by the viability of the technology ( prime mover ) solar is hard to make money with w/o a long term power purchase agreement ( ppa ) and those are super hard to come by . there are many viable technologies available right now that from a physics perspective look very promising but from a market perspective are dead in the water . consider flywheels from beacon power : http://www.beaconpower.com/ they approached my company a year or so ago looking to put together a deal with our origination group . the technology is sound , proven , and is clearly efficient . it is a really sweet idea that has tons of promise . did we do the deal ? no . unfotunately pricing out regulation is hard and the rules that are followed by isos for payouts on regulation services are unclear . the isos are interested in the technology but they are making up the rules as they go and companies that try to make money off the flywheels are currently going back and forth with them to create a market structure that can allow someone who owns this technology to make a reasonable return . by reasonable i mean one that beats out gas , coal , oil , etc . my point is that the market structure ( or lack of one ) is killing the deals for new tech like this . . not the efficiency . same goes for the solar ( and the wind ) . the prime mover that will succeed is the one that is pushed by the companies that understand how best to work the power markets . siting and development ( meaning leaning how to best construct the installation ) are huge factors as well . it is a bad scene when your dev guys do not know how to properly handle the solar modules or manage a solar installation project . it is indeed true that the physics of the technology must create a low-cost solution but the final word on success for a particular tech will be more based on the market acumen of the policy groups involved and the marketing/origination function than anything else . storage tech like flywheels from beacon are a huge part of this puzzle too ' cause the sun never seems to be out exactly when you need it and the wind blows best during off peak hours . : ) you need a good way to store the power so you can release it during favorable times of the day . all in all , figuring out what tech is ' most promising ' is more than just making a choice based on physics . it is a good question tho . ur best bet is to talk to someone in power-markets from a big company like constellation , pg and e , or perhaps tva and ask them about the hurdles those technologies face . then marry that up with a physics assessment on efficiency and you will get a good hint on which tech to help push forward with your studies .
i think the reason is that when you are blowing on an object , you are making lots of air particles collide with it perpendicularly in one direction thus transferring a lot of momentum to the object . when you are sucking air in , the only force that is acting on the object is by the air particles that rush in to fill up the gap that you just created . these particles come in from all kinds of different directions failing to transfer momentum in a consistent way .
this is more or less correct , the intention is clear , but you are saying it in a little bit of a clumsy way . your first statement is that if you have light crossing a certain distance , the distance is the same from the point of view of point a and point b , but the time taken to traverse the distance is different . this is correct in the way you set things up , but it is not best to think of this as the local speed of light being different at a and at b , because the local speed of light is not defined by how far light goes by how much time it takes as measured from a . it is defined by the metric itself--- if the interval-length of the tangent vector to the path of the light ray is zero , then the light ray is by definition moving at the speed of light locally . your metric is purely diagonal . this means that little vectors in the direction defined by the t , x , y , z coordinates are always perpendicular to each other . the surface t=constant is therefore a kind of global notion of time , and when you say " a light beam crossed from a to b in a time $\delta t_a$ as measured at a , you mean that the light crossed from point a at global time slice t , to point b at global time-slice $t+\delta t$ . the time this takes mismatches at a and b , but this is not an indication that the speed of light is different , because both a and b agree that the tangent vector to the light path is still null . but a and b do not agree on the rate at which global slices separate from each other in time . this effect is extremely important , it is the main effect of general relativity . the first approximation to the effect of a gravitational field , is that there is a local variation in the clock rate . this is a metric of the form $$ ds^2 = - ( c^2 - 2\phi ( x , y , z ) ) dt^2 + dx^2 + dy^2 + dz^2$$ with explicit c 's ( usually you set these to 1 ) , where $\phi ( x ) $ is the newtonian gravitational potential ( divided by c^2 ) . this approximation for the metric is not great--- there are spatial metric terms i ignored which are of the same order as the time metric term for the case of light ( this is why einstein 's deflection of light is double newton 's--- this approximation reproduces newton 's value ) , but ignore these . the effect of the newtonian potential is to make the clock rate incompatible at different positions , in exactly the way you describe . for the earth 's gravitational field $\phi ( x , y , z ) = gz$ , where g is $9.8{m\over s^2}$the potential increases with height as g the difference in the transit time for different positions leads a person at a height h , who labels the time slices according to the faster clock at the higher location , considers the light travelling a shorter distance per time slice than a person who labels the time slices according to the slower clock at a lower position . the sychronization failure means that clocks at different altitudes must be configured to run at different rates to stay synchronized , and this is the " general relativistic effect " that people talk about with regard to gps satellites . in addition to their special relativistic time-dilation due to the orbital speed , which correction factor is proportional to $v^2\over 2c^2$ and makes them tick slower , they also have a general relativistic correction factor proportional to $2\phi\over c^2$ , which makes them tick faster than clocks close to the ground . the virial theorem shows that in a kepler orbit , the expected value of the newtonian potential energy is proportional to the expected value of the newtonian kinetic energy $$ v^2/2 = gm/2r $$ $$ \langle {v^2\over 2}\rangle = -{1\over 2}\langle\phi\rangle $$ where the brackets mean average over one orbit . you can see this directly for a circular orbit from setting the required centripetal equal the newtonian gravitational force . $$ {mv^2\over r} = {gmm\over r^2} $$ but the virial theorem says that it is true on average for elliptical orbits too . so the total slowing-down factor for a clock is $$ {1\over c^2} ( {v^2\over 2} + \phi ) = -{1\over 2} {v^2\over 2c^2}$$ and the gr effect is twich as big as the sr effect , and in the opposite direction--- so the orbiting satellite clock is faster than the stationary ground clock by the expected slowing down amount . the general relativistic slowing down of clocks near the earth 's surface is also measured by taking atomic clocks on a plane around the earth , and this is a famous experiment from the 1970s . the effect is equivalent to the redshifting of light as they travel up a gravitational well ( the light loses energy as it climbs up the gravitational well , each photon loses frequency ) . the lost frequency just means that the photon going from a to b keeps its frequency fixed in terms of the clock ticks of a ( or in terms of the clock ticks of b ) but since the clocks do not match , the frequency as measured locally is different . these general relativistic effects are generally known as weak-equivalence principle tests , since they only test that time dilates in the way described by the local metric above . this dilation was derived by einstein by analyzing a uniformly accelerated frame in special relativity , and deriving the relation between the tick-rate of synchronized clocks in the accelerating frame . he assumed that there is no local difference between the uniform earth 's gravity and constant acceleration , so he was able to find the relation between gravitational potential and the tick-rate for clocks that stay synchronized . the derivation is found in elementary gr books ( schutz is good and especially elementary ) , and it is also found on the wikipedia article for gravitational redshift
a localized state of a particle or a field means the same thing as in semi-colloquial context : it means that most of the energy density ( for the case of a field ) or most of the probability ( particle , quantum mechanics ) may be found within a region ( e . g . a ball ) whose linear size is smaller ( or much smaller ) than the total size of the material ( and surely smaller than infinity ) . the opposite term is a delocalized state  e.g. a plane wave  in which the particle may be found anywhere in the material or anywhere in space .
what you refer to is conservation of mass under some assumptions : constant density a steady state flow i will bring us back to your equation by starting with the very fundamental mass accounting for a given fluid flow . to be comprehensive , we need to recognize that velocity is not constant over the entire area , but we will assume that it is . take the flow rate to be $\dot{m}$ . $$\dot{m} = \rho v a$$ now , if we have a steady state flow along a single flow path , then this quantity will be constant over the entire path , $\dot{m}=const$ . water in the cases you are concerned about is sufficiently incompressible so $\rho = const$ . this results in your conclusion that $va$ is constant . gravity may or may not shift the balance from $v$ to $a$ or vice versa . it depends on if there are rigid boundaries to the flow . if you have a flow fall freely in air or flow downward in a trench ( like a river ) then the boundary of the fluid may change freely . if you have a pipe with a given flow area , then the velocity is fully determined from that . anyway , there are laws that conserve other things - like energy . so in a rigid pipe flowing downward ( absent friction ) the pressure will increase as you go down in elevation , which results directly from gravity .
as a supplement to @tpg2114 's answer , it also depends on the " wetness " of an object . as most people should know the evaporation of water requires energy and this lowers the temperature . the lowest temperature a wet object can reach is what is called the " wet-bulb temperature . " this can be several degrees lower than the " dry-bulb temperature , " the amount can vary depending on the humidity ( specifically atmospheric pressure ) . if that wet-bulb temperature gets below 0c , then freezing is possible . in order for said wet object to get close to the wet-bulb temperature , some convection needs to occur in order to take that evaporated water away ( i.e. . wind ) . this is one of the ways wind under a bridge can cause freezing under the right conditions . another possible reason might be heat lost by radiation or the earth via conduction .
there are many materials that can be charged by triboelectric effect . tipicaly you can observe this effect rubbing a material like wool and amber . the phenomenon is quite complex but it is in great part because the different electron affinity of the materials ( one loses easily an electron and the other captures an electron ) .
yes , i agree with david . if somehow , you were able to travel at the speed of light , it would seem that ' your time ' would not have progressed in comparison to your reference time once you returned to ' normal ' speeds . this can be modeled by the lorentz time dilation equation : $$t=\frac{t_0}{\sqrt{1 - ( v^2 / c^2 ) }}$$ when traveling at the speed of light ( $v=c$ ) , left under the radical you would have 0 . this answer would be undefined or infinity if you will ( let 's go with infinity ) . the reference time ( $t_0$ ) divided by infinity would be 0 ; therefore , you could infer that time is ' frozen ' to an object traveling at the speed of light .
the problem is in a misunderstanding of " simultaneous " . " simultaneous " refers to two different events that occur at the same time in some particular reference frame , but you are applying it to the same event in two different frames . so it does not make sense to say " pulse has to occur simultaneously for both boxguy and platgirl . " that is a single event - it can not be simultaneous all by itself , even when observed by two different people . you could , if you want , set the origins of the coordinate systems they are using so platgirl and boxguy assign the same time coordinate to pulse . if you do , they will not assign the same time coordinate to reflect . the time between the events pulse and reflect is different in different frames . additionally , platgirl and boxguy will not agree on the length of the boxcar . your calculation assumes they both measure the length to be $d$ , but actually platgirl will observe the boxcar to be lorentz-contracted . one way to analyze your scenario is to set up coordinate systems $s$ for the boxcar and $s'$ for the platform . we set ( x , t ) = ( 0,0 ) = pulse in both systems . in frame $s$ ( box ) , the coordinates are : pulse : ( 0,0 ) reflect : ( d , d/c ) return : ( 0,2d/c ) in frame $s'$ ( platform ) , the coordinates are : pulse : ( 0,0 ) reflect : $ ( \sqrt{3}d , \sqrt{3}d/c ) $ return : $ ( \frac{2\sqrt{3}}{3} d , \frac{4\sqrt{3}}{3} d/c ) $ you can verify that in both frames , light moves outward at speed $c$ and returns at speed $-c$ in reply to your edit , yes the durations from pulse to reflect and reflect to return are the same for boxguy and different for platgirl . that is just a fact . that is how it is . notice , though , that the spatial separations are also different . for boxguy , these events the same distance apart . for platgirl , they are different distances apart . what is the same between frames is the interval $\delta x^2 - \delta t^2$ .
the wording of the question suggests that the electrons were the first objects or particles whose charge required the people to establish the sign convention . but that is obviously not the case . the electron was discovered by j . j . thomson in 1897 but for much more than a century before that moment , people had already been studying electric ( and magnetic ) phenomena , even quantitatively , and they had already fixed some convention which charged objects or sides of a battery are positive and which of them are negative . because this convention was already established , there was absolutely no freedom in the decision about the sign of the electron 's charge . it was simply measured in the cathode rays etc . and it turned out to be negative . historically , the first man to decide about a sign convention for the electric charge was probably benjamin franklin in the 18th century . his model of electricity assumed that charged objects contain some fluid  it is the electric charge of a continuous type ( the similarity with phlogiston , the fluid that was believed to personify heat , can not be overlooked ) . if there is too much of this fluid , which is naturally identified with the plus sign , he would talk about the positive electric charge and vice versa . up to the moment when the elementary particles were being discovered , there was no way to prove that one of the two sign conventions was better than others . in fact , even today , it is not true that the opposite sign convention would be " better " in any sense . electrons could carry a positive charge in the opposite convention but protons and nuclei ( and up quarks ) that are equally important ( and , in the case of up quarks , equally fundamental ) would be negatively charged while they are nicely positively charged in the world around us . once a convention is fixed for the electric charge , a natural convention emerges for the sign of the current , voltage , and many other electric observables , too . it just happens that in the circuits , the arrows for the current have the opposite direction than the velocities of the electrons but this discrepancy only became visible once people knew that the currents was composed of the negatively charged electrons which was a long time  a century  after benjamin franklin 's setting of the convention . this apparent discrepancy causes no problems as long as we carefully follow it and realize ( and , when necessary , emphasize ) that the arrows represent the current according to the established conventions and not the electrons ' velocity . one should also point out that there exist conductors where the conductivity is guaranteed by positively charged carriers ( or both ) , for example in solutions ( positively charged ions ) or semiconductors ( holes ) . in those conductors , the signs of the current agree with the sign of the velocity of the ( positively charged ) carriers .
first , be proficient in qft and gr . and by this , not just ' undergrad ' versions , but with mathematical rigor -- that includes being fluent in differential geometry and gauge theory . depending on what type of string theory work you do , and a response to your request of " not entirely essential but interesting " , would be to know algebraic topology ( in order to make sense of tqft ) . at this point , you are becoming more of a mathematician -- this is no surprise , it is in essence a mathematical topic . take a look at nakahara 's textbook " geometry , topology , and physics " for a start ( before you begin to think about string theory ) .
gravitational waves are transverse waves but they are not dipole transverse waves like most electromagnetic waves , they are quadrupole waves . they simultaneously squeeze and stretch matter in two perpendicular directions . gravitational waves definitely propagate in a given direction but the effect that they have on matter is completely perpendicular to the direction of motion . below is a picture of what the metric of a passing wave does to space ( the wave traveling is perpendicular to the screen ) . if you imagine a free particle sitting at each grid intersection point , the particle would move sinusoidally right along with the grid : this diagram is from this paper
unfortunately , these problems are widespread on many inexpensive telescopes . notice that i said many ; by choosing carefully , you could have bought a telescope without these problems , or with them greatly diminished , for about the same money . that is why i always recommend good optics combined with a manual dobsonian mount , rather than computerized mounts on tripods . however , it is probably too late to return this scope and exchange it for a better one , so here 's what you can do . vibration . tighten everything on the mount as tight as you can get it . hang a weight from under the tripod head . get a length of chain and hang it on the inside of the tube with half of the chain hanging out ; this is a very effective vibration suppressor . i own a set of vibration pads , but have never noticed that they make much difference . last solution : build a new mount , a simple dobsonian . you will lose the goto , but you will also lose the vibrations ! focusing knob . most rack and pinion focusers , even the cheapest , are adjustable . there should be a couple of screws between the knobs , which allow adjustments . chinese telescopes use a nasty glue-like substance to grease mechanical parts like focusers and mounts . try degreasing them with a solvent and then replacing with lithium snowmobile grease . overexposed planets . this is caused by a very small planet in a large black field of view . all exposure meters try to render everything neutral grey , which is fine for the moon , but not for the planets . if the camera makes the background grey , it grossly overexposes the planets . if your camera allows , try underexposing by two stops . try some of the other settings , perhaps backlight .
the period between the cmb redshift ( $z_\text{cmb}\approx1100$ ) and the current record for highest redshift objects known ( either grbs or galaxies in the hubble ultra-deep field , about $z\approx9$ ) is when the first stars are expected to have formed . they probably start forming at redshifts in the somewhere in the range $15$ to $30$ , maybe larger . it is a big unknown at the moment . either way , these ages correspond to roughly 13.4 to 13.6 gyr lookback time but the exact timing is not precisely known . in this range , we had expect to see the first stars and the first ( proto ) galaxies . the stars ( and associated events like supernova/grbs ) are therefore there to be seen and , since the high-$z$ protogalaxies we know of are already looking quite galaxy-like , there are probably others out there at slightly higher redshifts . at higher redshifts than $30$ , things really do become dark , though . that is why we call this period the " dark ages " . the gas in the universe cools because it is no longer ionized and therefore mostly transparent . it only starts heating up again when self-gravitating structures begin to contract and collide . based on this . . . what is there to see ? up to $z\approx12$ , maybe more extended protogalactic structures , like those in the hubble udf . up to $z\approx30$ , there should be metal-free stars and ( shortly after ) their associated death-throes . at higher redshift the universe is genuinely quite dark , though . what telescopes will see them ? the james webb space telescope is partly designed to see these high-$z$ objects . its wavelength range is roughly 0.6 to 28.5 microns . the first stars are expected to emit mostly ultra-violet light . at redshift $z=14$ , say , a 0.1 micron uv photon would be redshifted to 15 microns , squarely in jwst 's range .
one of the results of special relativity is that a particle moving at the speed of light does not experience time , and thus is unable to make any measurements . in particular , it cannot measure the velocity of another particle passing it . so , strictly speaking , your question is undefined . particle #1 does not have a " point of view , " so to speak . ( more precisely : it does not have a rest frame because there is no lorentz transformation that puts particle #1 at rest , so it makes no sense to talk about the speed it would measure in its rest frame . ) but suppose you had a different situation , where each particle was moving at $0.9999c$ instead , so that that issue i mentioned is not a problem . another result of special relativity is that the relative velocity between two particles is not just given by the difference between their two velocities . instead , the formula ( in one dimension ) is $$v_\text{rel} = \frac{v_1 - v_2}{1 - \frac{v_1v_2}{c^2}}$$ if you plug in $v_1 = 0.9999c$ and $v_2 = -0.9999c$ , you get $$v_\text{rel} = \frac{1.9998c}{1 + 0.9998} = 0.99999999c$$ which is still less than the speed of light .
because even if reactor is not-critical , there are lots of radioactive materials in the fuel which decays and produce some heat . you cannot stop that process . although it gives much less heat than nuclear fission , it is still significant and can cause meltdown . the same reason is why spent nuclear fuel is stored under water for few years - it just selfheat and can melt .
no , unless we find the laws of relativity to be seriously incomplete or incorrect ( not very likely to happen as both sr and gr are well-tested theories . ) " building an instrument " would presumably mean that it has some finite mass and unless that rest mass is 0 , you will be limited by c .
suppose you have a balance that looks like this : where $m_1$ and $m_2$ are the weights you are comparing and $m$ is some large weight fixed to the balance . for simplicity let 's $m_1$ is zero , so one one end of your scales you have some weight $m_2$ and there is nothing else on the other end . you are quite correct that with simple lever scales the lever would tip over until it was vertical with $m_2$ at the bottom . however the scales pictured above would rotate only a small distance because the restoring force from the large weight $m$ would balance out the tilting force from $m_2$ . the result is that the angle increases as you increase $m_2$ , just as you observe in practice . you can work out the angle of rotation just by taking moments around the pivot point , and you get : $$ m_2glcos\theta = mghsin\theta $$ or $$ tan\theta = \frac{m_2 l}{mh} $$ up to about 25$^\circ$ $tan ( \theta ) $ is approximately proportional to $\theta$ so you get : $$ \theta \propto m_2 $$ i.e. for angles less then 25$^\circ$ the angle of tilt is proprotional to the weight on the scales .
one example of a physics problem which is ultimately an optimization problem is that of determining the ground state for a spin glass . the simplest model of a spin glass is an ising model with random bonds between all of the sites . that is the hamiltonian is defined as $$ h = - \sum_{i&lt ; j} j_{ij} s_i s_j - \sum_{i} h_i s_i $$ here $s_i$ denotes the spin on each lattice site , $j_{ij}$ the strengths of the interactions between sites , $h_i$ is an external magnetic field . typically , the spins are said to only take two values $\{ 1 , -1 \}$ , and their interactions might be limited to only occur between nearest neighbors . the problem is to determine the particular set of plus and minus ones that minimizes this hamiltonian . with various choices for what the $h$s or $j$s are allowed to be , you can actually make this problem equivalent to many other known problems in computer science , and has been shown to be np-hard in general . for more information see : ising formulations of many np problems , andrew lucas arxiv/1302.5843 . in fact , finding solutions to this problem was in the news recently , you may remember d-wave . d-wave claimed to have built a quantum annealing computer that could efficiently solve a restricted version of this problem . for instance see one of their papers on the particular problem they studied . this caused some controversy , with people like scott aaronson initially showing a great deal of skepticism , and nature being less pessimistic . as a result , people started collecting benchmarks for classical algorithms to vet some of the numbers coming out of d-wave . in particular , alex selby has a nice rundown , as well as github repository of a canonical set of problems and the timing results for a couple canonical classical algorithms as well as d-wave . the classical algorithms outperform the d-wave results . you could test your optimization routines against those benchmarks .
nmr ( nuclear magnetic resonance ) and mri are both based on the same transitions , transtions between spin states of nuclei in a magnetic field . these states are not limited to up and down . where $i$ is the spin of the nucleus , there will be states $m = -i , -i + 1 , -i + 2 . . . i$ so if the nucleus is $i=0$ one can not do nmr on the nucleus . if $i=1/2$ , then there will be two states , $m = -1/2$ and $m = +1/2$ . most common , nmr is done on $i=1/2$ nuclei such as protons and carbon-13 , in which case spin up and spin down is applicable , but nmr is also done on deuterium and other nuclie that are not $i=1/2$ . esr ( electron spin resonance ) is based on transitions between spin states of electrons in a magnetic field .
if there was no central tapping , what would be the change ? consider this circuit diagram , from the answer you are not satisfied with : there is no " central tapping " so the two diodes are connected in series . in a series connected circuit , the current through each circuit element is identical . now , note that d1 allows only a clockwise current while d2 allows only a counter-clockwise circuit . the only solution that satisfies both constraints is zero current . so , without the center-tap , there is zero current in the secondary circuit and thus , by ohm 's law , zero voltage across the resistor r .
the difficult bit is creating a coherent beam of large objects since the rate of decoherence increases rapidly with the size of the object . if you could create a coherent beam of tennis balls you could diffract it , but in the real world you would not be able to maintain the coherence for longer than the tiniest fraction of a second . as far as i know the largest object that has been diffracted is a buckyball . it is unlikely you will ever be able to prove that objects the size of a tennis ball have wave like properties . however since qm correctly predicts diffraction for sizes ranging from electrons to buckyballs there is no obvious reason why this should break down for larger objects . for more on this subject see the questions : will a football ( soccer ) diffract ? validity of naively computing the de broglie wavelength of a macroscopic object why doesn&#39 ; t a marble rolling on a table ever reflect back at the edge ?
what is going on here ? is swimming through spacetime only possible if the spacetime is curved in some way that breaks the symmetry under lorentz boosts ? or is there some error in my reasoning ? that is precisely the case . no error in your reasoning . in the case of a curved spacetime the " center of mass " of an extended body is no longer well-defined w.r. t external - i.e. located in an asymptotically flat region - observers . in order to " swim " through spacetime one exploits the inhomogeneities of the gravitational field . the presence of these inhomogeneities breaks local lorentz symmetry which is necessary for the mechanism to work . in particular the scale of the swimmer and the inhomogeneities should be comparable . this is one reason why , at present , the construction of an actual swimmer is far beyond our technological means . edit : for those interested on extended body effects in gr there is are classic papers by dixon . more recently abraham harte has done some amazing work along these lines extended-body effects in cosmological spacetimes .
1 ) i gather you mean gravitation potential energy of the test particle . well , any such thing is only useful in so far as it is related to a constant of motion throughout the geodesic--in the case of gravitational potential , being part of the conserved mechanical energy , kinetic + potential . ( another example could be angular momentum . ) in gtr , these constants are given by a killing vector field , which is an infinitesimal generator of an isometry : spacetime " looks the same " in the direction of a killing vector . most spacetimes do not have any , but by definition , a static spacetime has a timelike killing vector field , and can always be put in the following form : $$ds^2 = -\lambda dt^2 + d\sigma^2 , $$ where $d\sigma^2$ is the metric for any spacelike manifold and $\lambda$ is independent of $t$ . the factor $\lambda^{1/2}$ is commonly called the gravitational redshift . for example , for the scwarzschild spacetime in the usual schwarzschild coordinates , $\lambda = \left ( 1-\frac{2gm}{c^2r}\right ) $ , and the following is a constant of motion representing the specific ( per-mass ) energy of the freefalling particle : $$e = \left ( 1-\frac{2gm}{c^2r}\right ) \frac{dt}{d\tau} . $$ this is the natural generalization of the total mechanical energy , including also rest-mass energy ; for the schwarzschild case , the spherical symmetry allows one to build an " effective potential " quite analogous to the newtonian case , but that approach is less useful in general . 2 ) gravitational energy cannot be explicitly included in the einstein field equations because the equivalence principle--there is always a local inertial frame ( the free-falling one ) in which spacetime looks like the ordinary , flat , special-relativistic one . hence if there was a frame-independent local notion of gravitational energy , i.e. , a tensor , that tensor is zero in some local frame , and hence zero in every frame . however , one can think of the non-linearity of the einstein field equation as caused by gravitational energy itself interacting with spacetime . in this sense , gravitational energy is " implicitly " included . another thing one can do is try to build another notion of gravitational energy that is not necessarily both local and frame-independent , e.g. , landau-liftshitz pseudotensor and others .
gravitational mass is a bit of a misnomer , because in general relativity the spacetime curvature is determined ( mostly ) by the energy density . mass is simply treated as equivalent to the amount of energy given by $e = mc^2$ , or conversely energy is just treated as the equivalent amount of mass . so the fact the particles are massless above the electroweak symmetry breaking energy and have a mass below it ( acquired through the higgs mechanism ) makes no difference to gravity .
assume that you have a maximal set $a , b , c , \ldots$ and two states $\phi_1$ and $\phi_2$ with the same set of eigenvalues in that set . then construct the operator $z = |\phi_1\rangle\langle\phi_1|$ . convince yourself that it would distinguish between $\phi_1$ and $\phi_2$ , and that it would commute with all of $a , b , c , \ldots$ --- i.e. your original set was not maximal .
yes . a swing does not depend on friction . the way you start up is by leaning back and pulling hard with your hands placed on the ropes . your hand-hold positions effectively create a compound pendulum , so your mass is offset from null . by timing your shift in weight back to a " sitting-up " position , you essentially add energy to the pendulum system in phase and frequency with the full-length pendulum 's oscillation . i am sure there are some pictures of this somewhere :- ) edit : i should have said , as peter shor did , that this makes use of gravitational force .
if you remove a bit of moon and leave the rest with the same velocity , it will continue to follow the same orbit . since you are carrying away some of the mass , as you carry it away it will exert a gravitational force on the moon , which could change the velocity . you are correct that cutting the mass in half will cut the gravitational force in half , but so will the inertia be cut in half , so the trajectory does not change .
anthony , even after your parlay with georg and mark , i am still not confident i know exactly what you want , but i will give it a stab . . . first , the the velocities of the balls , both before and after collision , are broken into two components : velocities tangential to the point of contact and velocities normal to the point of contact . what happens in each of these orthogonal axes does not affect what happens to the other . this is the first really important concept to grasp : for momentum , what happens in tangential stays in tangential ; and what happens in normal stays in normal . speaking of tangential : notice that the tangential velocity of each ball is exactly the same , both before and after collision . this would be expected if we assume ( and we do ) no friction . another way to look at it : if the tangential velocity of each ball was the only component of velocity , and they barely gave each other a frictionless kiss in passing , would not we expect the velocity of each ball to remain unchanged ? yes . ok , then , we have established that the tangential velocity ( and momentum , by the way ) of each ball remains unchanged throughout the collision . now we go on to the normal component of velocities : we know the velocities prior to collision , and we know the masses throughout the collision . we also know that kinetic energy and momentum are conserved ( since this is an elastic collision ) . furthermore , we also know that the tangential components of velocities have remained unchanged . we are left with a boat load of equations and only two unknowns : normal velocities of the masses ( m1 and m2 ) after the collision ( v1an and v2an , respectively ) . velocities before collision : v1b = sqrt ( ( v1bn^2 ) + ( v1bt^2 ) ) and v2b = sqrt ( ( v2bn^2 ) + ( v2bt^2 ) ) . velocities after collision : v1a = sqrt ( ( v1an^2 ) + ( v1at^2 ) ) and v2a = sqrt ( ( v2an^2 ) + ( v2at^2 ) ) . since momentum is conserved --> ( m1 ) ( v1bn ) + ( m2 ) ( v2bn ) = ( m1 ) ( v1an ) + ( m2 ) ( v2an ) . . . and the normal velocities must remain normal , it is a short step of reasoning to deduce , for m1=m2 , that |v1an| = |v1bn| , but are in the opposite direction , and likewise for |v2an| = |v2bn| . for the more general case , where m1 does not equal m2 and |v1| does not equal |v2| , the equations are : ( v1an ) = ( v1bn ) ( ( ( m1-m2 ) + ( ( 2m2 ) ( v2bn ) ) / ( m1+m2 ) ) ( v2an ) = ( v2bn ) ( ( ( m2-m1 ) + ( ( 2m1 ) ( v1bn ) ) / ( m1+m2 ) )
1 ) let there be given a hilbert space $h$ and a mixed state described by a density operator $\hat{\rho}:h\to h$ , which is a positive operator $\hat{\rho}\geq 0$ , and with trace ${\rm tr} ( \hat{\rho} ) =1$ . 2 ) let $v\subseteq h$ be an eigenspace of states for an hermitian observable $\hat{a}:h\to h$ with eigenvalue $\lambda\in\mathbb{r}$ . ( we will ignore subtleties with unbounded operators , domains , selfadjoint extensions , etc . , in this answer . ) 3 ) let $\hat{p}:h\to h$ be the projection operator onto $v$ . it is the unique operator , such that $\hat{p}$ is hermitian $\hat{p}^{\dagger}=\hat{p}$ ( and hence $\hat{p}$ is diagonalizable in an orthonormal basis ) . an idempotent $\hat{p}^2=\hat{p}$ ( and hence can only have eigenvalues $0$ and $1$ ) . the eigenspace ${\rm ker} ( \hat{p}-1 ) $ for $\hat{p}$ with eigenvalue $1$ is equal to the subspace $v$ . if $ ( \mid\psi_i\rangle ) _{i\in i}$ is an orthonormal basis for $v$ , then $$\hat{p}=\sum_{i\in i}\mid\psi_i\rangle\langle\psi_i\mid . $$ 4 ) then the collapse $\hat{\rho}\longrightarrow \hat{\rho}^{\prime}$ of the density operator , due to the measurement , would be $$ \hat{\rho}^{\prime}~=~ \frac{\hat{p}\hat{\rho}\hat{p}}{{\rm tr} ( \hat{p}\hat{\rho} ) } . $$ 5 ) for a pure state $\hat{\rho}=\mid\psi\rangle\langle\psi\mid $ , the collapse $\mid\psi\rangle\longrightarrow\mid\psi\rangle^{\prime}$becomes $$ \mid\psi\rangle^{\prime}~=~\frac{\hat{p}\mid\psi\rangle}{\sqrt{\langle\psi\mid\hat{p}\mid\psi\rangle}} . $$ so if one starts from a pure state $\mid\psi\rangle$ , the collapsed state $\mid\psi\rangle^{\prime}$ would also be pure .
correct . the state $\psi$ you have described is a superposition of two momentum eigenstates , with momentum $p$ and $-p$ . so $\psi$ does not itself have a definite momentum - if you measured the momentum of a particle in this state you had get either $p$ or $-p$ with a probability distribution that depends on the $c_1 , c_2$ . the energy of a free particle depends only on its speed , not its direction of movement , so the energy basis is degenerate . here your state has a definite value of energy because in all cases , the particle is moving with the same speed . however , momentum is a vector quantity and does depend on the direction of movement . in general , a linear combination of two eigenstates of a given operator is not going to be another eigenstate of that operator - unless the two eigenstates also have the same eigenvalue . for example , here $\psi_1$ and $\psi_2$ are both eigenstates of $h$ with the same eigenvalue ( energy ) $e$ , so $\psi$ is still an eigenvalue of $h$ . but since $\psi_1$ and $\psi_2$ are eigenstates of $p$ with different eigenvalues ( momenta ) $p$ and $-p$ , then $\psi$ is not an eigenstate of $p$ . ( by the way , you wrote : "$h$ can only be used in the time-independent case right ? " it requires a time-independent potential . it is fine to use it with time-dependent wavefunctions . )
an important discrimination in the integrals is by looking at the zero values of the $e 's$ in the table $i$ for particular values of the $ ( l , m , n ) $ , because the $ ( l , m , n ) $ of table $i$ are proportionnal to the $ ( p , q , r ) $ of table $ii$ ( see page $1504$ of the ref ) : a ) if you compare $ ( s/s ) $ and $ ( s , x ) $ in table $ii$ , the difference is that you have not the integral $e_{s , x} ( p=0 , q=0 , r=0 ) $ in the development of $ ( s , x ) $ but , looking at the table $i$ , we have $e_{s , x} ( l , m , n ) = l ~ ( sp\sigma ) $ , so $e_{s , x} ( p=0 , q=0 , r=0 ) = e_{s , x} ( l=0 , m=0 , n=0 ) =0$ . b ) if you look at $ ( x , y ) $ , you have not $e_{x , y} ( p=0 , q=0 , r=0 ) $ , $e_{x , y} ( p=1 , q=0 , r=0 ) $ , $e_{x , y} ( p=0 , q=1 , r=0 ) $ terms . but we see in table $i$ , that : $e_{x , y} ( l , m , n ) = lm ( ( pp\sigma ) - ( pp\pi ) ) $ . so , for instance you have $e_{x , y} ( p=1 , q=0 , r=0 ) = e_{x , y} ( l=\lambda , m=0 , n=0 ) =0$ , and idem for the other terms . this is not the full story , of course . . . [ edit ] i am using the ideas given in this paper , ( see for instance , example $2$ , lithium ) . the tight-binding wave-function : $$\psi_{j , \vec k} ( \vec r ) = \sum_{p , q , r} e^{i ( p \vec k . \vec a_1 + q \vec k . \vec a_2 +r \vec k . \vec a_3 ) } \phi_j ( \vec r - p \vec a_1 - q \vec a_2 - r \vec a_3 ) $$ where $i , j$ are orbital indices . we have $ e_{ij} = \langle \phi_j|\hat h| \psi_{i}\rangle $ in the tight binding approximation , only the on-site and nearest neighbor matrix elements are retained . $e_{ij} = \sum_{ ( on-site ~+~ nearest ~~neighbours ) ~~p , q , r} e^{i ( p \vec k . \vec a_1 + q \vec k . \vec a_2 +r \vec k . \vec a_3 ) } \langle \phi_j| \hat h|\phi_i ( p , q , r ) \rangle \tag{1}$ ( s , x ) and its symmetries now , considering $ ( s , x ) $ , we are going to use symmetries , that is : $\langle \phi_s| \hat h|\phi_x ( \epsilon_1 p , \epsilon_2 q , \epsilon_3 r ) \rangle = ( -1 ) ^{\epsilon_1} \langle \phi_s| \hat h|\phi_x ( p , q , r ) \rangle \tag{2a}$ $\langle \phi_s| \hat h|\phi_x ( p , r , q ) \rangle = \langle \phi_s| \hat h|\phi_x ( p , q , r ) \rangle \tag{2b}$ $\langle \phi_s| \hat h|\phi_x ( 0 , q , r ) \rangle = 0\rangle \tag{2c}$ note that $ ( 2c ) $ is a consequence of $ ( 2a ) $ nearest neighbours we only consider $p , q , r = ( 1,0,0 ) , ( -1,0,0 ) $ . call $e_{s , x} ( 1,0,0 ) = \langle \phi_s| \hat h|\phi_x ( 1,0 , 0 ) \rangle$ . from formula $ ( 1 ) $ and the symmetries relations $ ( 2 ) $ , we get : $ ( s , x ) _1 = e^{ik_1 a_1} e_{s , x} ( 1,0,0 ) + e^{-ik_1 a_1} e_{s , x} ( -1,0,0 ) \\ = e_{s , x} ( 1,0,0 ) ( e^{ik_1 a_1} - e^{-ik_1 a_1} ) \\ = 2i e_{s , x} ( 1,0,0 ) sin ( k_1 a_1 ) \\ = 2i e_{s , x} ( 1,0,0 ) sin ( \xi ) $ second nearest neighbours we only consider $ ( p , q , r ) = { ( 1,1,0 ) , ( 1 , -1,0 ) , ( -1,1,0 ) , ( -1 , -1,0 ) } + {q \leftrightarrow r}$ . so , we have : $ ( s , x ) _2 = e^{ik_1 a_1 +ik_2 a_2} e_{s , x} ( 1,1,0 ) + e^{ik_1 a_1 - ik_2 a_2} e_{s , x} ( 1 , -1,0 ) + e^{-ik_1 a_1 +ik_2 a_2} e_{s , x} ( -1,1,0 ) + e^{-ik_1 a_1 - ik_2 a_2} e_{s , x} ( -1 , -1,0 ) + \{q \leftrightarrow r , k_2 a_2 \leftrightarrow k_3 a_3 \}$ from formula $ ( 1 ) $ and the symmetries relations $ ( 2 ) $ , we get : $ ( s , x ) _2 = e_{s , x} ( 1,1,0 ) [ e^{ik_1 a_1 +ik_2 a_2} + e^{ik_1 a_1 - ik_2 a_2} - e^{-ik_1 a_1 +ik_2 a_2} - e^{-ik_1 a_1 - ik_2 a_2} ] + \{ k_2 a_2 \leftrightarrow k_3 a_3 \} \\ = 4i e_{s , x} ( 1,1,0 ) ( \sin k_1 a_1 \cos k_2 a_2 + \{ k_2 a_2 \leftrightarrow k_3 a_3 \} ) \\ = 4i e_{s , x} ( 1,1,0 ) ( \sin \xi \cos \eta + \sin \xi \cos \zeta ) $
the ( anti ) symmetrization simply acts on all the enclosed indices ( at the same " height " which are really enclosed between the brackets ) , regardless of their belonging to the same tensor or different tensors . for example , $$ \delta^{ [ \alpha}{}_{ [ \gamma} r^{\beta ] }{}_{\delta ] } = \frac 12 \left ( \delta^{ [ \alpha}{}_{\gamma} r^{\beta ] }{}_{\delta} - \delta^{ [ \alpha}{}_{\delta} r^{\beta ] }{}_{\gamma} \right ) = \dots $$ and each term on the right hand side may still be expanded because it is an antisymmetrization of the $\alpha\beta$ indices : $$\dots = \frac 14 \left ( \delta^{\alpha}{}_{\gamma} r^{\beta}{}_{\delta} - \delta^{\alpha}{}_{\delta} r^{\beta}{}_{\gamma} - \delta^{\beta}{}_{\gamma} r^{\alpha}{}_{\delta} + \delta^{\beta}{}_{\delta} r^{\alpha}{}_{\gamma} \right ) $$ similarly for all your other terms ( the $\delta-\delta$ term is computed in the other answer that i endorse ) , or any other tensors . the number of terms in these expansions is increasing " exponentially " if there are many ( anti ) symmetrizations so it becomes really tiresome to write all of the terms  this tiresomeness is the reason why use the ( anti ) symmetrization symbols , after all . as long as the several groups of indices that are ( anti ) symmetrized are disjoint ( and groups of upper indices never have any intersection with a group of lower indices ) , it does not matter in which order you expand the ( anti ) symmetrization . you are guaranteed to get the same result .
your body is warmer than the surrounding air and as such when heat escapes from your body it warms up that air ; if it did not , you would have overheated years ago . however , if the air is not moving , that air around you begins to warm up . heat transfer is faster if the temperature difference is greater . what wind does is move that warm air away , and replace it with other , cooler air . this new patch of air will wick off heat from you more efficiently , which makes you feel cooler . the process is continuous so you start to feel " cold " and hence , wind chill . it is also why blowing on food makes it cooler and easier to eat . incidentally , you feel cold because you lose heat , and the faster you lose heat the colder you feel . it is why touching cold metal feels colder than touching wood at the same temperature .
you can take the newtonian expression for the orbital speed as a function of orbital radius and see what radius corresponds to an orbital speed of $c$ , but this is not physically relevant because you need to take general relativity into account . this does give you an orbital radius for light , though it is an unstable orbit . if the mass of your planet is $m$ then the radius of the orbit is : $$ r = \frac{3gm}{c^2} $$ where $g$ is newton 's constant . the mass of the earth is about $5.97 \times 10^{24}$ kg , so the radius at which light will orbit works out to be about $13$ mm . obviously this is far less than the radius of the earth , so there is no orbit for light round the earth . to get light to orbit an object with the mass of the earth you would have to compress it to a radius of less than $13$ mm . you might think compressing the mass of the earth this much would form a black hole , and you had be thinking on the right lines . if $r_m$ is the radius of a black hole with a mass $m$ then the radius of the light orbit is $1.5 r_m$ . so you can only get light to orbit if you have an object that is either a black hole or very close to one , but actually it is even harder than that . the orbit at $1.5r_m$ is unstable , that is the slightest deviation from an exactly circular orbit will cause the light to either fly off into space or spiral down into the object/black hole . if you are interested in finding out more about this , the light orbit round a black hole is called the photon sphere , and googling or this will find you lots of articles on the subject .
i am just guessing on this based on the reference you posted in the comments . sometimes db 's are referenced to a max level , like with audio at 120db . in this case they look at the difference between the maximum . in your case , 10,000 is the max , or 40 db . based on this scale comparison : i would guess apostilibs to db is done like this : $db = 40 - 10*log ( apostilibs ) $ and the other way would be $apostilbs = 10^{ ( 4-db_{value}/10 ) }$
there are no known applications and no imaginable applications of the higgs boson which is not surprising given the fact that its lifetime is a zeptosecond . the higgs boson is a particular particle  state in the hilbert space  in a correct quantum mechanical theory describing nature . i mean the standard model or its extensions . any discussion of the higgs boson would be totally impossible without quantum mechanics . all properties of the higgs boson crucially depend on principles and special effects of quantum mechanics . the higgs boson is a particle associated with the higgs field . to see the emergence of particles from fields , one has to discuss physics at the level of quantum mechanics ; see the previous point . however , even in classical physics , one may add the higgs field to the general theory of relativity , much like the electromagnetic fields . the higgs field is a source of gravity and other things . but it is just " another added player" ; the main field in the general theory of relativity is the metric tensor , i.e. the spacetime geometry , not the higgs field . all realistic models of string theory that are or were trying to describe the universe around us contain a higgs field and a higgs boson ; string theory seems incompatible with all the alternatives . in this sense , the discovery of the higgs is a victory for string theory and a confirmation of one of its predictions ( a relatively uncontroversial prediction ) . scalar fields  a broader family of fields similar to the higgs field ( fields without spin )  are also generic and important in string theory , see http://motls.blogspot.cz/2012/06/higgs-boson-scalar-fields-and-victory.html i think that none of the points you asked answers the question in the title , " what is next " . we do not know what will be the next discovery . physics researcher is not stalin 's five-year plan . if we knew what the next breakthrough would be , we would already to it tonight . one may only discuss what ideas people are studying in their effort to make the big breakthrough . when it comes to physics that is as analogous to the higgs boson as possible , supersymmetry would probably be the top pick .
first of all the very name is misleading , as real green houses get heated by obstructing convection and not by the gases in the green house . the question can be reformed as : does the existence of co2 in the atmosphere contribute to the heat retention capacity of the atmosphere which is what the " greenhouse effect " is about . the answer is yes , it has been measured and it is small . in one can see the gases which can absorb the incoming solar radiation , and the main contribution is from h2o , water vapor . the co2 contribution , though small , is not negligible . these are old measurements as you can see from the reference on the figure . so the answer is yes to co2 being a " greenhouse gas " among others . now the importance given by the community of climate change advocates to co2 is completely dependent on computer modelling of the earth 's climate . the importance comes from an assumed in the programs feedback process , where a small increase in atmospheric co2 induces a large increase in atmospheric h2o in a run-a-way mode . this is another story though .
" different values of n represent different modes of buckling " ( http://emweb.unl.edu/negahban/em325/21-buckling%20of%20columns/buckling%20of%20columns.htm )
due to general relativity the universe started 13.7gyr ago . it was finite then and since there was no phase of infinite expansion , it is still finite .
if $g$ is the constant proper acceleration of the rocket , and $t$ the coordinate time measured by an observer on earth , then the velocity of the rocket in the earth frame is ( during the first half of the trip ) $$ v = \frac{gt}{\sqrt{1 + g^2t^2/c^2}} , $$ assuming that the initial velocity is zero . see this post for a derivation , and this post for more info about proper acceleration . integrating this gives us the travelled distance $x$ , measured by an observer on earth : $$ x = \frac{c^2}{g}\left ( \sqrt{1 + g^2t^2/c^2}-1\right ) . $$ insert $x=d/2$ and you find the time needed for half the trip , and the velocity at that time . the corresponding proper time $\tau$ on board the rocket is found by integrating $$ \text{d}\tau = \sqrt{1 - v^2/c^2}\text{d}t = \frac{\text{d}t}{\sqrt{1 + g^2t^2/c^2}} , $$ so that , for the first half of the trip , $$ \tau = \frac{c}{g}\ln\left ( gt/c + \sqrt{1 + g^2t^2/c^2}\right ) . $$ the equations need some adjustments for the second half of the trip ( see also the first link ) , but the situation is symmetrical .
these are two different effects . satellites do not fall down because they are moving on a circular orbit . actually , they are falling down all the time , since circular motion is accelerated ( though the velocity does not change absolute value , it changes direction ! ) , so it is kind of " falling around the earth " . the second question is , why does not an astronaut in a space station fall down to the floor ? that is because , both the station and the astronaut feel the same force which holds both of them on a circular orbit . but for falling down to the floor the astronaut should feel a stronger force than the space station , otherwise there is no net force driving the astronaut and the stations floor into each other .
nope . gravitational radiation is a kind of radiation and it has a completely different equation of state than the cosmological constant . the cosmological constant has pressure equal to the energy density with a minus sign , $p=-\rho$: the stress-energy tensor is proportional to the metric tensor so the spatial and temporal diagonal components only differ by the sign . radiation has $p=+\rho/3$ , much like for photons . most of the energy density of the universe has $p/\rho = -1$ ; that is what we know from observations because the expansion accelerates . a radiation-dominated universe would not accelerate ( and did not accelerate : our universe was indeed radiation-dominated when it was much younger than today ) . the ratio $p/\rho$ must be between $-1$ and $+1$ because of the energy conditions ( or because the speed of sound can not exceed the speed of light ) . the $-1$ bound is saturated by the cosmological constant , the canonical realization of " dark energy" ; $-2/3$ and $-1/3$ comes from hypothetical cosmic domain walls and cosmic strings , respectively ; $0$ is the dust , i.e. static particles ; $+1/3$ is radiation ; and higher ratios may be obtained for " somewhat unrealistic " types of matter such as the dense black hole gas for which it is $+1$ . this ratio determines the acceleration rate as a function of the hubble constant .
it is not the most stable structure it will in fact decay to graphite ( theoretically ) it is the hardest natural substance , which is due to the arrangement of it is atoms
moving at a constant velocity , no matter how close to the speed of light , has absolutely no effect on the person moving . in fact , it has no effect on the laws of physics . this is the fundamental tenet of special relativity - you cannot tell absolute motion , only relative motion between different things . the changes you are referring to are what someone moving at a different velocity would see from his or her perspective . if you are moving very fast with respect to me , i might say that your " mass " ( in the resistance to further acceleration sense ) is large , and that you appear squashed in the direction of your motion , but for you it is just a normal day and i am the one who appears to be having problems . now two catches . first , if two people start at rest with respect to one another ( e . g . sitting on the earth ) and you want to get one of them moving relativistically with respect to the other , there will necessarily be acceleration involved . human bodies cannot withstand acceleration much beyond the $9.8~\mathrm{m}/\mathrm{s}^2$ or so we feel from earth 's gravity . in fact , a fundamental tenet of general relativity is that acceleration is indistinguishable from gravity , so a large acceleration for a long time would be like living on a planet with a larger gravitational field at its surface , which would probably not agree with your body too well . second , if you are thinking of space travel , keep in mind that space is not empty . if you are traveling at near-light speeds compared to your surroundings ( e . g . the solar system ) , you are likely to encounter some piece of dust that , from your perspective , is heading toward you at nearly the speed of light . not a pleasant experience . furthermore , if you go fast enough relative to the cosmic microwave background radiation that permeates the universe , the stuff in front of you will appear blueshifted to the point of being rather harmful ( and nearly impossible to shield ) gamma rays .
the helicopter in your example would have some velocity given to it by the earth . i believe atmospheric drag would play a significant role in this , but let 's ignore that for now . you may have heard the process of an orbit described as continuous free fall , where you fall " towards " the other body just as fast as you move along the orbit . if this hypothetical helicopter lifted off , it would just be orbiting the planet !
alright , here 's the skinny . the universe is much much larger than 92 billion light years . but the region we can see is around that number . also , you have it all wrong for the expansion rate of the universe . the expansion is more like stretching . no matter where you are ( that is an approximation ) , you would see a length x expand by the same amount after a time t . you seem to be thinking of it like there is some border expanding away from us , when really it is the space everywhere that is just getting larger . this is a cumulative effect ; the farther away things are , the faster they seem to expand away from us because there is more space in between to be stretched . the current rate of expansion is called the hubble rate . it is estimated at around 500 km/s/mpc ( mpc is megaparsec ) . or , alternatively , 160km/s per million light years . that means that every second a distance that was $1000000ly$ becomes a distance of $ ( 1000000+1.691\times10^{-11} ) ly$ . now , you say the observable universe is 92 billion light years across . then the farthest reaches are 46 billion light years away . this means that every second , somewhere around 7.4 million kilometers are added to that space . technically , that means that objects at the edge of the visible universe are receding faster than the speed of light , but they are not moving faster than light . if i were moving faster than light , you could stand next to me as i pass and it would look like i am going faster than light . however , these objects at the edge of the observable universe are not moving faster than light . if i positioned myself next to them , they would not be moving at all . their motion with respect to space , to me , to anything near them is not above the speed of light , it is simply that more distance is added between them and us each second than light could traverse . place a beam of light at that distance travelling away from us . to us , the beam of light would be travelling at $c$ plus the speed due to expansion . so no matter what , the objects out that far are not moving faster than light . the point here is twofold . first , the expansion rate is not a definite speed ; it is a speed per set distance length . and second , nothing is actually moving faster than light ever ; space makes things recede faster than light , but space is not really moving either .
typically sheets of laser light are usually achieved by bouncing the laser off an oscillating mirror , though you could achieve a similar effect by passing the light through an oscillating or rotating prism . if the frequency of oscillation of the mirror is fast enough persistence of vision makes it look like a steady sheet of light . prisms broaden a beam of multichromatic light because the refractive index varies with wavelength so each colour bends by a different amount . with monochromatic light there is just a single refractive index and single angle of bending .
slip and stick the friction between your shoes and the floor is quite non-linear . as you put your foot down , your shoe comes under elastic strain as your foot moves forward but friction holds the bottom in place . this is the stick phase . as your foot moves forward , the strain becomes too big , and the friction between floor and shoe can not overcome it anymore . the bottom of the shoe starts to move . the dynamic friction now is lower than the static friction before , which means the acceleration is rather large . however , the strain also dissipates quickly as the shoe flexes back to its original form . this slip phase is therefore also time-limited . so , when you hear a high-pitched squeak , you are hearing your shoe stick and slip several thousand times per second , moving micrometers at a time . water definitely affects the static and dynamic friction , so that explains why it can matter . but note that there is no simple rule about the exact change in friction , and therefore adding water may also prevent squeaky noises in other situations ( when it acts as a lubricant )
yes , the newtonian theory says that we would instantly notice a difference in the gravitational field but that contradicts special relativity . to resolve this we use general relativity . in gr a massive body will still give a gravitational field that only vanishes at infinity . to resolve the point about escape velocities , this model is idealized but it works to a good approximation . the gravity of other stars has very very very little influence on the trajectory of rockets we launch from earth , because they are so far away that we can basically neglect their gravitational fields .
the cosmic microwave background does not originate with the big bang itself . it originates roughly 380,000 years after the big bang , when the temperature dropped far enough to allow electrons and protons to form atoms . when it was released , the cosmic microwave background was not microwave at all- the photons had higher energies . since that time , they have been redshifted due to the expansion of the universe , and are presently in the microwave band . the universe is opaque from 380,000 years and earlier . the galaxies that we can see only formed after that time . before that , all that is observable is the cmb .
what i have been telling the students in the intro mechanics class i ta is that each force corresponds to some amount of work . the individual contributions of work add up to the net ( or total ) work , just as the individual forces add up to the net force . so , for instance , when you lift a box up from the floor to a table , there are two forces acting on that box : the force $\vec{f}_\text{lift}$ you apply to lift it , and the force of gravity , $\vec{f}_g = -mg\hat{y}$ . you can calculate the amount of work done by the lifting force as $$w_\text{lift} = \int_0^h \vec{f}_\text{lift}\cdot\mathrm{d}\vec{x}$$ and you can calculate the amount of work done by the gravitational force as $$w_g = \int_0^h \vec{f}_g\cdot\mathrm{d}\vec{x}$$ in a typical physics-problem scenario , you would be lifting the box at constant velocity ( except for the brief moments when you start and stop its motion , which i will ignore for now ) . in this case , you could draw a free-body diagram and use newton 's second law $\vec{f} = m\vec{a}$ to discover that your lifting force has to be equal in magnitude and opposite in direction to the gravitational force : $$\vec{f}_\text{lift} = mg\hat{y}$$ armed with this result , you can calculate the work done by each individual force ( try it ! ) : $w_\text{lift} = mgh$ and $w_g = -mgh$ . the net work done is zero . you can just as well calculate the net work from the net force , $$w_\text{net} = \int_0^h \vec{f}_\text{net}\cdot\mathrm{d}\vec{x}$$ in this case , the net force is $\vec{f}_\text{lift} + \vec{f}_g$ , which is zero ( because the box is moving at constant velocity , remember $\vec{f}_\text{net} = m\vec{a}$ ) . so , again , the net work is zero . " but wait , " you say , " i thought the work done was $mgh$ ! " well , it is , if you only consider the work you did to lift the box . this is what you would measure if you used a crane to lift the box , for example : the amount of work that needs to be done by the crane is $mgh$ . however , gravity did the opposite amount of work , which canceled out the work you did ( except for those brief moments at the beginning and the end , but those two contributions cancel each other out anyway ) . so the total work done by all forces involved is zero . this might seem a little confusing - perhaps you are wondering , if the total work done is zero , how did the gravitational potential energy increase from $0$ to $mgh$ ? the trick there is that the " work done by gravity " $w_g$ is actually the gravitational potential energy under a different name . for conservative forces , we define $\delta u_i = -w_i$: the change in a certain kind of potential energy is the negative of the work done by the force corresponding to that potential energy . in this case , the gravitational force does work $-mgh$ , so the change in gravitational potential energy should be $mgh$ . but once you start calling it a change in gravitational potential energy , you can no longer call it a contribution to the work - and that leaves you with just the $w_\text{lift} = mgh$ contribution . so in that case , if you label the " action " of gravity as a change in potential energy rather than a work , the work done is $mgh$ .
higher order derivatives ( usually ? ) correspond to non-renormalizable terms in the lagrangian . every term in the lagrangian density needs to have units of $ [ m^4 ] $ , so that the action is unitless ( in units where $\hbar = 1$ ) . for example , take a scalar field $\phi$ . this has units $ [ m ] $ , or equivalently $ [ 1/d ] $ . each derivative adds a factor of $ [ m ] = [ 1/d ] $ ( think of it as $\delta / \delta x$ ) . with no derivatives , you can form the standard mass term $$ \frac{1}{2} m^2 \phi^2 $$ the coefficient $m$ must thus have units $ [ m ] $ , making it a mass . you can not form a lorentz scalar with only one derivative , since $\partial_\mu \phi$ is a one-form and without breaking rotational invariance there is no vector to take an inner product with . the next possibility is two derivatives . the only allowable term is $$ \frac{1}{2} \left ( \partial_\mu \phi \right ) \left ( \partial^\mu \phi \right ) \underset{\text{parts}}{\sim} - \frac{1}{2} \phi \partial^2 \phi $$ this term does not have a unitful coefficient , because the two $\phi$ 's and two $\partial$ 's make it have units $ [ m^4 ] $ already . a term $m \partial^2 \phi$ is equivalent to $0$ by integration by parts , so with no boundary and fields that die off at infinity they do not matter . a term like $m ( x ) \partial^2 \phi$ would contribute , but breaks translational invariance . what about three derivatives ? same as with one - there is no four-vector to contract the third derivative with , so this is impossible without breaking rotational ( and possible translational ) invariance with some preferred four-vector field $v^\mu ( x ) $ . the next possibility is four derivatives . a generic possibility is $$ \frac{1}{\lambda^2} \left ( \partial^2 \phi \right ) ^2 $$ because of the four derivatives and two appearances of $\phi$ , the term needs a coefficient with units $ [ m^{-2} ] $ , which is why i have added $1/\lambda^2$ where $\lambda \sim [ m ] $ . an inclusion of this term requires a mass scale $\lambda$ . like the fermi four-fermion theory , this would violate unitarity at energies $\sqrt{s} \gg \lambda$ . you can go through the same exercise for spin-1/2 and spin-1 fields and reach much the same conclusion , although the details will be different .
absorption is not an instant event . at the level of simple quantum mechanics , this system can be described as follows . evolution of electron in crystal is governed by schrdinger 's equation . external electromagnetic field , namely the light which we shine on the crystal , is a periodic addition to the hamiltonian . when you start shining light at the crystal , electron in state $|e_1\rangle$ is quasiperiodically changing its state : basically it oscillates near $|e_1\rangle$ . but if there is an energy level $e_2$ such that $h\nu=|e_2-e_1|$ , probability increases that the electron will appear at that level when measured , i.e. its state after some time $t$ appears $\alpha ( t ) |e_1\rangle+\beta ( t ) |e_2\rangle$ . why ? this is very similar to mechanical resonance . suppose you have a very heavy ball hanging on a rope of negligible mass . applying some arbitrary frequency force on it will not make it oscillate much . but if you apply force at the eigenfrequency of this pendulum , then even a tiny force , applied for long enough time can make the oscillation amplitude quite high . " somehow " the pendulum knows its eigenfrequency : ) when you have bands of allowed energies , then the role of $|e_2\rangle$ is played by any of the levels in the band satisfying $h\nu=|e_1-e_2|$ . if $h\nu&lt ; e_g$ , then there're no such levels , thus such radiation can not be absorbed . note that there're some differences from mechanical resonance : for instance , probability can not exceed $1$ , so instead of unbounded increase it in fact oscillates when the radiation is supplied long enough time  it is so called rabi cycle .
if i understand you correctly , then yes . this page , for example , has an image showing a typical velocity profile . the water moves slower near the bottom because of friction .
not really . a magnetic field alone does not create electricity . a changing magnetic field does . the earth 's magnetic field does change a tiny bit but not enough to really generate much . the other option is to move the inductor in the magnetic field . the earth 's magnetic field is quite homogeneous over short distances though so the coil would need to move fast and very far to generate much . this would use more energy than it creates ( at least on the surface of the earth ) . several years back there was an experiment ( the space tether experiment ) to drag a conductor through the earth 's magnetic field with the space shuttle . i do not know how viable this is though because i think it saps orbital energy .
you are thinking that the big bang happened in a particular point in space and then expanded outwards from that point . this is not true . the big bang happened at all points in space . this is because space itself expanded in the actual bang . therefore each point in space has its own " horizon " of 13.7 billion light years across . this edge is due to light simply not having enough time to reach that point , therefore nothing can be observed beyond that point . it is not a literal edge .
the wikipedia article is quite good on this subject . for any discharge in the air the molecules of the air must be ionized . this ionization happens during thunderstorms because of the high static electric fields carried by the clouds which generate " streamers " , i.e. paths for the electrons to flow downwards . corresponding streamers are formed by conductors and high points on the ground with positive charge again generated by the high fields of the storm cloud , the positive ions flow upward and the path for a discharge is set . a lightning flash terminates [ and discharge occurs ] on a tree while an un-attached streamer is visible on the earth surface projection to the left . when the electric field of the storm passes over the ground , high points that are also grounded have higher fields then the ground and can form streamers . lightning rods work , by generating upward streamers more efficiently since they are highly conducting and the field at the top is very high due to the geometry . [ upward streamers ] when a stepped leader approaches the ground , the presence of opposite charges on the ground enhances the strength of the electric field . the electric field is strongest on grounded objects whose tops are closest to the base of the thundercloud , such as trees and tall buildings . if the electric field is strong enough , a positively charged ionic channel , called a positive or upward streamer , can develop from these points . this was first theorized by heinz kasemir . as negatively charged leaders approach , increasing the localized electric field strength , grounded objects already experiencing corona discharge exceed a threshold and form upward streamers . once any downward leader connects to any upward leader available , a process referred to as " attachment " , a circuit is formed and discharge may occur . photographs have been taken on which unattached streamers are clearly visible . the unattached downward leaders are also visible in branched lightning , none of which are connected to the earth , although it may appear they are . return stroke once a channel of ionized air is established between the cloud and ground this becomes a path of least resistance and allows for a much greater current to propagate from the earth back up the leader into the cloud . this is the return stroke and it is the most luminous and noticeable part of the lightning discharge . discharge when the electric field becomes strong enough , an electrical discharge ( the bolt of lightning ) occurs within clouds or between clouds and the ground . during the strike , successive portions of air become a conductive discharge channel as the electrons and positive ions of air molecules are pulled away from each other and forced to flow in opposite directions . the electrical discharge ( averaging 30 ka for negative or 300 ka for positive lightning , and travelling at around 1108 m/s ) rapidly superheats the discharge channel , causing the air to expand rapidly and produce a shock wave heard as thunder . the rolling and gradually dissipating rumble of thunder is caused by the time delay of sound coming from different portions of a long stroke it is wise not to be swimming during a thunderstorm because the water being flat the most discontinuous conductive object will be the head . also if during a thunderstorm in the open air one 's hair becomes electrified it is wise to fall on the ground and keep away from high objects like trees .
this going to be a rather approximate answer because it involves lots of estimated quantities like the current density of matter and the value of the cosmological constant . the second friedmann equation tells us : $$ \frac{\ddot{a}}{a} = \frac{-4\pi g}{3}\left ( \rho + \frac{3p}{c^2} \right ) + \frac{\lambda c^2}{3} $$ it is conventional to take $a = 1$ at the current time , and i will assume the pressure of the stuff in the universe is negligable , so the equation simplifies to : $$ \ddot{a} = \frac{-4\pi g}{3} \rho + \frac{\lambda c^2}{3} $$ the value of $\ddot{a}$ is negative if the expansion of the universe is slowing , and positive if the expansion of the universe is accelerating . from a quick look at the equation it should be obvious that the density of matter ( baryonic and dark ) , $\rho$ , makes a negative contribution to $\ddot{a}$ , while the dark energy makes a positive contribution to $\ddot{a}$ . to answer your question we need to feed in the current values for $\rho$ and $\lambda$ and see how the two terms in the equation compare . the current density of baryonic matter is about 0.25 hydrogen atoms per cubic metre , and the density of all matter ( baryonic + dark ) is about 1.6 hydrogen atoms per cubic metre . let 's take the density of all matter and use it to calculate the first term in the equation above . the result is : $$ \frac{4\pi g}{3} \rho = 7.5 \times 10^{-37} s^{-2} $$ wikipedia assures me that the current value of the cosmological constant is $10^{-52}$ m$^{-2}$ , and this makes the second term : $$ \frac{\lambda c^2}{3} = 3 \times 10^{-36} s^{-2} $$ so we end up concluding that $\ddot{a}$ is positive i.e. the expansion of the universe is accelerating , just as the supernovae observations have confirmed . now you have asked what happens if we double the amount of matter i.e. double $\rho$ . normal ( i.e. . baryonic ) matter is only about one sixth of the total matter , so doubling the amount of normal matter only increases the first term to $8.7 \times 10^{-37} s^{-2}$ , and $\ddot{a}$ is still positive so the expansion still accelerates . doubling the amount of all matter doubles the first term to $1.5 \times 10^{-36} s^{-2}$ and $\ddot{a}$ is still positive so the expansion still accelerates . doubling the amount of matter makes no difference to the future fate of the universe . let me end with a caveat : the figures i have used above are very rough so i would not place too much emphasis on the exact values . however the overall conclusion still applies i.e. that the effect of the cosmological constant currently dominates . i estimate you had need to increase the total matter content of the universe by about a factor of 7 for matter to become dominant again .
short answer - no . if field lines are curved and parallel , then the path integral along one line or the other will either give a different potential difference , or require different field strength . in either case you cannot call the field " uniform " . a little picture to clarify : the dotted lines represent equipotential lines ( at right angles to the electric field ) . now when you start at point a and integrate the field around the path indicated with arrows , you can see that the first and third elements of the integral are zero because they are along an equipotential line ; that leaves us with the two curved segments of the path . now if the path lengths are $s_1$ and $s_2$ , and fields $e_1$ and $e_2$ are in general different , then the total loop integral is $e_1 s_1 - e_2 s_2$ where i use the minus sign since the path is in the opposite direction for the last segment . we know this integral should be zero because the electric field is conservative - no work done by a charge after returning to the starting point . i can rewrite that as an equality : $$\frac{e_1}{e_2}= \frac{s_2}{s_1}$$ in other words , the only way that $e_1=e_2$ is if $s_1 = s_2$ . and if the two paths are the same length , and they connect two equipotential surfaces ( which are at right angles to the field lines ) , then the two paths must be straight and parallel . of course the above is using the fundamental result that for a static electric field , $$\underline{\nabla} \times \underline{e} = 0$$ since the electric field is conservative ( maxwell 's equation ) .
i believe that is covered by this answer i posted some time ago to a related ( but not quite the same ) question . adapting it to your notation , $$t = \frac{1}{\sqrt{2g ( m + m ) }}\biggl ( \sqrt{r_0 r ( r_0 - r ) } + r_0^{3/2}\cos^{-1}\sqrt{\frac{r}{r_0}}\biggr ) $$ the same formula is given in the wikipedia article qmechanic mentioned in a comment .
let 's have a go at doing the calculation for the schwarzschild metric . i am going to assume the particle is moving radially because otherwise the algebra gets hairy , but i will not make any restriction on the velocity . because $d\omega = 0$ the metric simplifies to : $$ c^2d\tau^2 = c^2 dt^2 ( 1 - \frac{r_s}{r} ) - \frac{dr^2}{1 - r_s/r} $$ when we say the velocity of the particle is $v$ we mean that in our coordinates $dr/dt = v$ so we can substitute $dr = vdt$: $$ c^2d\tau^2 = c^2 dt^2 ( 1 - \frac{r_s}{r} ) - \frac{v^2 dt^2}{1 - r_s/r} $$ or : $$ d\tau^2 = dt^2 \left ( 1 - \frac{r_s}{r} - \frac{v^2}{c^2}\frac{1}{1 - r_s/r} \right ) $$ if we graph the ratio of $d\tau/dt$ as a function of $v/c$ we get ( this is at $r = 4r_s$ ) : the magenta line shows the expression i have derived above , and the blue line shows what you get by multiplying the schwarzschild time dilation factor by the lorentz factor . the two curves are obviously different apart from the stationary case when $v = 0$ . however there is something odd because the time dilation goes to infinity when $v/c = 0.75$ , and remember this is at $r = 4r_s$ so we are well outside the event horizon . the reason for this is that for the schwarzschild observer the speed of light reduces as you approach the event horizon . for a radially travelling light ray the velocity in the schwarzschild coordinates is : $$ v_{light} = c \left ( 1 - \frac{r_s}{r} \right ) $$ so at $r = 4r_s$ an object travelling at $0.75c$ is travelling at the same speed as light and therefore the time dilation is infinite . response to comment : i will do the calculation at the earth 's surface where the curvature is greatest . the schwarzschild radius of the earth is a shade under 9 mm , so the ratio $r_s/r$ at the surface is $1.39 \times 10^{-9}$ . using this value of $r_s/r$ the graph looks like : there is basically no difference between the two calculations , but this is because the gravitational time dilation is negligable so only the lorentz factor matters . still , it is reassuring to note that my calculation derived from the schwarzschild metric does match the lorentz time dilation :- )
schiff and sakurai are graduate level books . a more " doable " textbook would be shankar 's book . griffiths is the standard textbook for undergraduate qm . it is very nice book but , like most of qm textbooks , it must be supplemented by solved problems . your best choice is zettili 's book . it contains solved problems on all topics including bra-ket notation . that is the reason basically why it has such high rating on amazon . it bridged a needed gap in qm textbooks . you can check also landau 's book . as far as i remember , it contains problems with insightful short answers spread throughout the book .
maybe too simple , and certainly not a general solution , but anyway here goes . the simplest case is a spherical charge distribution : $$\rho= \left\{ \begin{array}{cc} \rho_0 and r \le a \\ 0 and r&gt ; a \end{array} \right . $$ then $$ v ( 0 ) = \rho_0 \int_0^a \frac{4 \pi r^2}{r^n} dr = 4 \pi \rho_0 \int_0^a r^{2-n} dr $$ cases : $$ 0&lt ; n&lt ; 3 \qquad v ( 0 ) = \frac{4 \pi \rho_0}{3-n} a^{3-n} \text{ , finite}$$ $$ n=3 \qquad v ( 0 ) = 4 \pi \rho_0 \lim_{\epsilon\rightarrow 0} \ln \left ( \frac{r}{\epsilon} \right ) \rightarrow \infty $$ $$ n&gt ; 3 \qquad v ( 0 ) = \frac{4 \pi \rho_0}{n-3} \lim_{\epsilon\rightarrow 0} \left ( \frac{1}{\epsilon^{n-3}} - \frac{1}{a^{n-3}} \right ) \rightarrow \infty $$ in all these cases , the denominator of the original integral , $|r-r'|^n$ , grows without bound for large $r$ , and the charge is localized around the origin , so $$ \lim_{r \rightarrow \infty} v ( r ) =0 $$ so , $n=3$ divides the finite from the infinite cases . ( technically , i suppose this calc constitutes a counter-example for $n \ge 3$ by exhibiting an unbounded result for those cases ) .
the answer is negative . there is no action of the free particle invariant under the galilean group . in the following , a heuristic explanation will be given and in addition a reference where a more detailed proof is provided . the basic reason is that the galilean group cannot be realized on the poisson algebra of functions on the phase space of the free particle $t^{*}\mathbb{r}^3$ ( equipped with the canonical symplectic form ) . it is only its central extention ( please , see the following wikipedia page ) which can be realized in terms of poisson brackets . for this central extension , the poisson brackets between the generators of the boosts $b_i$ and the translations ( i.e. . , the components of the momentum ) $p_i$ no longer vanishes but depends on the mass of the particle : $\{b_i , p_j\} = m\delta_{ij}$ . since boosts must generate the transformation : $ p_i \rightarrow p_i + m v_i $ on the momentum coordinates via the canonical poisson brackets , the boost generators have to be realized as multiples of the position coordinates $q_i$ . $b_i = m q_i$ the transformation law of the boosts on the phase space ( which is the manifold of the initial data , thus this realization does not involve time ) : $ q_i \rightarrow q_i $ $ p_i \rightarrow p_i + m v_i $ $ h ( \vec{p} ) \rightarrow h ( \vec{p}+m\vec{v} ) - \vec{p} . \vec{v}-\frac{1}{2}m v^2 $ it is easy to verify that the free particle hamiltonian is invariant and its transformation satisfies a group law . but this realization still does not make the lagrangian $ l = \vec{p} . \dot{\vec{q}} - h$ invariant , because the cartan-poincare form : $ \vec{p} . d\vec{q} $ is not invariant and changes by a total derivative : $m d \vec{v} . \vec{q}$ . thus the existence of mass prevents the action from being invariant , because of the canonical poisson bracket and not because of the choice of the dynamics through the particular choice of the hamiltonian . the noninvariance of the cartan-poincare form under the boosts is referred to as non-equivariance of the momentum maps associated to the boosts , which indicates that we cannot redefine the group generators so the poisson bracket between the boosts and translation generators vanishes . please see pages 430-433 and exercise 12.4.6 in " introduction to mechanics and symmetry " , by marsden and ratiu for a rigorous proof .
to be simple , consider an object hitting a heavy plane . if there is coefficient of friction $c_f$ ( and restitution $c_r$ ) between the bodies , parallel velocity is modified as well : $\delta p_{\parallel} = f_{\parallel} \delta t = -c_f f_{\perp} \delta t = -c_f \delta p_{\perp} = -c_f ( 1+c_r ) p_{\perp}$ thus obtaining $$ \left [ \begin{array}{c} v_{\parallel}^{out} \\ v_{\perp}^{out} \end{array} \right ] = \underbrace{ \left [ \begin{array}{cc} 1 and - c_f ( 1+c_r ) \\ 0 and - c_r \end{array} \right ] }_{a} \left [ \begin{array}{c} v_{\parallel}^{in} \\ v_{\perp}^{in} \end{array} \right ] . $$ note that it works as long as $v_{\parallel}^{in}\geq c_f ( 1+c_r ) v_{\perp}^{in}$ . if there were a general ( working for all input velocities ) matrix $a$ , it should allow dissipation , but not creation , of energy i.e. $||a||_2\leq 1$ ( the condition is not fulfilled in the above example ) . i means that either the matrix need to have other entries as well or that the problem intrinsically needs restriction of the initial conditions . however , bear in mind coefficient of restitution is only a effective parameter ( as it has been already mentioned by jalexiou ) . anyway , it may be a nice experimental problem for an undergraduate student to find the all coefficients ( and check for which conditions they work ) . i am curious of the results .
retarded propagators are those with $g ( \dots , t , t&#39 ; ) =0$ for all $t&lt ; t&#39 ; $ . they are vanishing before $t=t&#39 ; $ , the delta-function " stimulates " the field at $t=t&#39 ; $ , and the green 's function for positive $t-t&#39 ; $ measures the response of the field . one may view this description as a construction of the green 's function which also proves that it is unique . advanced propagators are the time reversal of the retarded ones , with the sign of $t-t&#39 ; $ interchanged in the description above . ( recheck whether i have not permuted the two propagators by a mistake : beware of sign errors . ) feynman propagators are the arithmetic averages of the corresponding retarded and advanced ones . the ( non-thermal ) green 's functions are defined as correlation functions in the vacuum state . this is useful because the ground state is the simplest state and all other multiparticle states may be built from the ground state by adding linear combinations of fields ( check the lsz formula etc . ) . for this reason , the structure of the vacuum including all the correlations in it " knows " about all dynamical questions . in thermal field theory , the expectation values in the vacuum are replaced by ${\rm tr} ( \rho \dots ) $ where $\rho=\exp ( -\beta h ) $ is the thermal density matrix . the latter point also clarifies why the vacuum statet acts like a " boundary condition": the $t=0$ ( non-thermal ) correlators may be calculated for $t=\infty ( 1+i\epsilon ) $ which has an infinite real part but a relatively smaller imaginary part , too . by analyticity , the small imaginary " angle " does not change the green 's function much . however , it simplifies the calculation because the evolution operator will contain an extra factor of $\exp ( -\epsilon \infty h ) $ which is still enough to exponentially suppress all excited states relatively to the ground state . when $\infty$ is really sent to infinity , only the matrix elements evaluated relatively to the ground state survive . the same comments and choices apply to the infinite future . for this reason , the vacuum matrix elements are automatically picked from the simplest boundary conditions one may assume at $t=\pm\infty$ . the small but nonzero value of $\epsilon$ gets translated to the $i\epsilon$ in feynman 's propagator : a procedure like this one also shows why feynman 's propagators are the correct ones to use in the feynman diagrams .
the results from the experiment does not differ significantly from the " true 2d " system , in fact , this is why experiments and theory agree so well ! consider a semiconductor heterostructure gaas/gaasal . at the interface , alignement of the fermi level at both sides of the semiconductor crystals creates a triangular potential well at the gaas side of the interface . this potential well is quite narrow so that quantization in one direction can be effectively assumed . in fact , putting numbers for gaas , $m^{\ast}=0.067m_e$ , $n_{2\text{d}} \simeq 10^{15}m^{-2}$ , $\epsilon^\ast=13\epsilon_0$ you get that the typical quantization energy is $\delta e \simeq 20 $mev . due to the triangular well , the $3\text{d}$ electron wavefunction [ consider nearly-free electrons in the effective mass approximation ] is modified as $\psi_{k_x , k_y , n\sigma} ( \mathbf{r} ) =\dfrac{1}{a^{1/2}}{\rm e}^{i k_x x}{\rm e}^{i k_y y}\zeta_{n} ( z ) \chi_\sigma$ with $\zeta_n ( z ) $ the $n$-th eigenfunction of the triangular well with energy $\varepsilon_n^z$ . the total energy can be written as $\varepsilon_{k_x , k_y , n} = \dfrac{\hbar^2}{2m^\ast} ( k_x^2+k_y^2 ) + \varepsilon_n^z$ the fermi energy can be obtained using that $k_f^2=2 \pi n$ , $\varepsilon_f\simeq 10$ mev . the difference between the highest occupied energy is $\delta e -\varepsilon_f \simeq 10$ mev . this yields $t \simeq 100$ k . conclusion a quick conclusion of this calculation is the following : at temperatures $t \ll 100$k all the occupied electron states have the same orbital in the $z$ direction and promotion to other orbital requires an excitation energy of at least $10$ mev . if this is not provided , the system has indeed lost one degree of freedom and it is dynamically a true $2\text{d}$ system . thus $2\text{d}$ systems can exist in nature ! concerning the second question , anyon statistics exists only in two-dimensional systems . it cannot exist in $3\text{d}$ , and the reason is topological : in two dimensions the configuration space of $n$ particles is multiple connected and closed path of a particle which encloses another particle cannot be " shrinked " to a point [ mathematically this is called " compatification" ] . on the other hand , for higher dimensions , the configuration space is simply connected and we lose the possibility of distinguish between the interior and the exterior of a closed path . hence , in the transition from $2\text{d}$ to $3\text{d}$ you simply lose the possibility of interpolate between bose and fermi statistics .
gluons are bosons , they have spin one and are mediators of the strong force . they have mass 0 and there is no weak interaction vertex with a gluon . the extract is talking of the higgs field , not the higgs boson . a field in physics is a field is a physical quantity that has a value for each point in space and time at that point . the higgs field permeates all space and is distinct from the higgs boson . it is the field that gives the mass to the z and w when the electroweak symmetry is broken . symmetry is broken at every point is space and time ( the mexican hat ) that describes the field at each point . the higgs boson is a particle that appears because of the group structure of the standard model , and is on par with all the other particles . it is the higgs field that gives mass to the particles of the theory which have electroweak vertices ( interactions ) and to the higgs boson itself . . edit after comments . note that the higgs field gives masses to elementary particles . if you see that they are massive in the table it means they have weak interaction vertices and the higgs breaking of symmetry raises them from the zero mass they would have had before breaking . these are the particles that build up all matter the electrons appear in the atoms , the quarks are always bound and appear according to the rules of su ( 3 ) color . the three quarks that compose a proton , for example , exist in a tight " bag " exchanging gluons with each other , and the gluons exchange gluons with each other etc . in a " soup " of contained interactions that can be computed or approximated . this means that there are binding energies and invariant masses between all these constituents that give the mass we measure for the proton , while the central three quarks have very small masses as you can see in the table ( or go to the link for enlargement ) . this is due to special relativity that reigns at these dimensions and energies . the quarks do have electroweak vertices in interacting with leptons , a simple example neutron decay :
as a clarification the energy per nucleon is always lower : for example , currently in the lhc the proton top energy is 3.5 tev . now the pb energy is 3.5 tev times z so the energy per nucleon is 3.5*z/a and a is greater than z for every nucleus ( except the proton where it is equal to one ) . but the goal of ion-ion collision is not to increase the total energy or the energy per nucleon : it is to obtain a different type of collision . it should be noted than in a proton-proton collision , the energy involved in the real collision process is variable : each quark and gluon carry a fraction of the energy of the proton , and hard collision involve a collision between a quark/gluon of one proton against a quark/gluon of the other . in the case of ion-ion collision you have the same process : the energy is shared by the protons/neutrons and they can have different energies . the goal of such collision is also to obtain a volume ( bigger than in a p-p collision ) with a very high energy density . in such a volume , a " state of matter " called quark-gluon-plasma is believed to be possibly created . the study of this qgp is one of the main goal of the alice experiment at the lhc . a few references : the alice experiment at cern live control screen of the lhc now running as a pb-pb collider at 574 tev in the center of mass
yes . in george 's frame gracie passes him at some velocity $v$ , so george can calculate the distance to gracie as a function of time . that means when george receives a time signal from gracie he can calculate how long the signal took to reach him ( travelling at $c$ ) . if george adds this travel time to the time in gracie 's time signal the result will be a time that does not match the time on george 's clock . from this george will know that gracie 's clock must be running at a different rate to his .
scanning tunneling microscopes can " see " atoms . here is a 2008 issue of nature magazine : what you are really seeing here is an electrical equipotential surface , which is close enough to being a solid surface for most purposes . a scanning tunneling microscope ( or the similar atomic force microscope ) is like a very precise phonograph ; it traces over a surface and senses the atomic-scale bumps in a surface . it is not too much inference to assume that the atomic-scale bumps are really atoms . the colors are added though ; any atom is smaller than a wavelength of visible light . how did we know atoms existed before that ? well , that is quite a story , and wikipedia probably tells it better than i can .
i think that your initial intutiion is right--before the point particle is removed , you had ( an infinite set of ) two $\frac{g\ , m\ , m}{r^{2}}$ forces balancing each other , and then you remove one of them in one element of the set . so initially , every point particle will feel a force of $\frac{g\ , m\ , m}{r^{2}}$ away from the hole , where $r$ is the distance to the hole . an instant after that , however , all of the particles will move , and in fact , will move in such a way that the particles closest to the hole will be closer together than the particles farther from the hole . the consequence is that the particles would start to clump in a complicated way ( that i would expect to depend on the initial spacing , since that determines how much initial potential energy density there is in the system )
we consider infinitesimal transformations of a field in the form , $$\phi\to\phi ' = \phi ( x ) +\alpha \delta \phi ( x ) $$ for an infinitesimal parameter $\alpha$ . the system is said to be invariant under such a transformation if it changes up to a total derivative or surface term , i.e. $$\mathcal{l}\to\mathcal{l}'=\mathcal{l} ( x ) +\alpha \ , \partial_\mu f^{\mu} ( x ) $$ by varying with respect to the fields , $$\alpha \delta \mathcal{l} = \frac{\partial \mathcal{l}}{\partial \phi}\alpha \delta \phi + \frac{\partial \mathcal{l}}{\partial ( \partial_\mu \phi ) }\partial_\mu ( \alpha\delta \phi ) $$ $$= \alpha \underbrace{\partial_\mu \left ( \frac{\partial \mathcal{l}}{\partial ( \partial_\mu \phi ) }\delta \phi\right ) }_{f^\mu ( x ) } + \alpha \left [ \frac{\partial \mathcal{l}}{\partial \phi}-\partial_\mu \left ( \frac{\partial \mathcal{l}}{\partial ( \partial_\mu \phi ) }\right ) \right ] \delta \phi$$ where in the last line we employed the equations of motion which arise by demanding $\delta s=0$ . notice the second term is zero for that reason , and hence we can declare , $$j^{\mu} ( x ) =\frac{\partial \mathcal{l}}{\partial ( \partial_\mu \phi ) }\delta \phi - f^{\mu} ( x ) $$ which satisfies the continuity equation , $\partial_\mu j^{\mu}=0$ , or in vector calculus language , $$\frac{\partial j^{0}}{\partial t}+\nabla \cdot \vec{j}=0$$ the corresponding noether charge is given by , $$q=\int \mathrm{d}^{d-1} x \ , j^{0}$$ which one can verify via the continuity equation and stokes ' theorem that $q$ is conserved locally . useful resources : peskin and schroeder 's introduction to quantum field theory
the short answer is " observations " . in the case of the gravitational law , the orbits of the planets around the sun , the moon around the earth fit mathematically a force with an inverse square law for the distance . an inverse law does not . in the case of electricity this article points out the observational history : early investigators who suspected that the electrical force diminished with distance as the gravitational force did ( i.e. . , as the inverse square of the distance ) included daniel bernoulli 1 and alessandro volta , both of whom measured the force between plates of a capacitor , and aepinus who supposed the inverse-square law in 1758 . and then others took it from there to end up with the comprehensive publications of coulomb , based on measurements . finally , in 1785 , the french physicist charles augustin de coulomb published his first three reports of electricity and magnetism where he stated his law . this publication was essential to the development of the theory of electromagnetism . [ 8 ] he used a torsion balance to study the repulsion and attraction forces of charged particles and determined that the magnitude of the electric force between two point charges is directly proportional to the product of the charges and inversely proportional to the square of the distance between them .
you are on the money , and dumpsterdoofus 's got the right answer in the comments . you can solve this problem quickly straight out using the potential energy for a gravity field formula ( which is the one your book obtains on line 5 ) : $$u = -g\frac{mm}{r}$$ $r$ is distance from center of earth note that this potential energy is negative -- this does not matter at all , just use it as usual since $g = \frac{gm}{r^2}$ , we can use $u = -g\frac{r^2}{r}$ ( compare with line 5 , keeping in mind that your book is using $x$ instead of $r$ ) . so , simple conservation of energy gives us : $$\frac{1}{2}v_0^2 - g\frac{r^2}{r} = \frac{1}{2}\frac{gr}{4} - g\frac{r^2}{4r}$$ work it out and you will find $v_0 = \sqrt{\frac{7}{4}gr}$ .
the " resources " linked in the post are bad . but there was a time when serious people were interested in the possibility that quarks have integer charges . han and nambu introduced the idea , pati and salam made a gauge theory of it , witten suggested how to test it , and this was done at cern in the 1980s ( see page 11 ) . there would be several ways in which the charge of the quarks can show up experimentally ( but i do not know a good review ) , and to maintain the integer-charge theory now , you would need extra new physical effects to explain all the measurements consistent with fractional charge . psychological resistance to the idea of fractionally charged quarks probably has two sources : 1 ) fractions just feel unnatural 2 ) the quarks always combine into integer-charged hadrons . for 1 ) , the pattern of charges in the standard model can be explained in grand unified theories . for 2 ) , given these charges , the allowed color singlet states in qcd all have integer charge , since they are made of quark-antiquark and/or quark-quark-quark combinations , and those elements only have integer charge . but i feel there ought to be a deeper explanation for 2 ) .
there are three parts to this : in mechanical equilibrium , things go to their lowest energy state . a straight line is the shortest distance between two points . whenever you have minimized something , it means that small deviations do not change its value ( to first order ) . let 's start with the third point , which is mathematical , and then look at the physics of the situation . take a look at this hill : at the bottom , it is flat , which the middle red line shows . when you look on the sides away from the bottom , it is sloped . so if you want to be at the bottom , you need to be somewhere where small steps in any direction do not change your height . when you minimize a quantity , small deviations do not change it . on to the physics . the black things are pulleys on supports . the brown thing is a rope . the grey things are weights . how heavy do the weights on the sides have to be to pull the rope straight ? the physics of a system like this is that the weights will try to fall as much as they can . another way to say this is that at equilibrium ( i.e. . nothing is moving ) , systems go to their lowest possible energy state . ( or a local minimum , at least . ) there is a tradeoff here in terms of energy . you could pull the middle weight down further , lowering its energy , but that would yank the end weights up some , raising their energy . the system will have to find the right angle of the ropes so that the energy is minimized . a straight string will never minimize the energy , though . it is the shortest possible path between the two supports . since it is the shortest path , small deviations to that path do not change its length to first order . ( that is the math point from the beginning of the answer . ) that means that you could always lower the energy a little bit moving the middle weight down . the side weights do not go up because the distance between the middle weight and the posts is not changing . meanwhile , the middle weight is going lower down , so the total energy of the system goes down . that means the straight line is never the lowest energy state , and so can not be the equilibrium configuration .
the reason is the mass of the earth . its mass creates a gravitational field around it so anything that is inside the gravitational field of the earth , would feel the gravitational force the earth is exerting on the object . in the same way because of newton 's third law , the earth would feel a gravitational force towards the object of the same magnitude it exerts over it . the core of the earth contains convecting currents due to highly conductive fluids , this creates electric currents which are the cause of the earth`s magnetic field . the force of gravity acts between two objects of any mass , and since the mass of the earth is bigger compared to a simple object as a car , then the earth attracts any object . the reason is that any body creates a gravitational field around it , so the massive the object , the stronger the gravitational force it exerts . also since the gravitational force is inversely proportional to the square of the distance , then the closer it is the strongest is the gravitation force
in a simplistic model , you can view destructive interference for a two-slit situation as arising from one of two possible events : either light from a single slit is destructively interfering ( and hence light from the other slit will as well , since the off-set is usually ignored ) , or light leaving both slits interfere with each other . the smaller " inner " interference pattern is caused by interference between the slits , the second option above . this is contrast to the diffraction envelope which , as you stated , is caused by interference for a single slit . for example , if light leaving the left-most edge of the left slit has a path length difference of $\lambda/2$ with respect tho the corresponding edge of the right slit , then light from these paths will completely destructively interfere . furthermore , every point in one slit has a pair in the other slit that causes destructive interference ( with the same path length difference ) . you will notice the first inner minimum occurs at a smaller angle than first single-slit minimum . this is consistent with $d\sin\theta=\lambda/2$ producing a smaller angle than $ ( a/2 ) \sin\theta=\lambda/2$ since the slit separation $d$ is larger than the slit width $a$ . these equations yield the first " interference " and first " diffraction " minima for two slits , respectively .
if you work with a smaller number of coordinates ( usually " curved ones " in a sense ) and no lagrangian multipliers , you are simply considering a configuration space that is a submanifold of the full configuration space in the calculation that does include lagrange multipliers . extremizing the action $s_{full}$ with lagrange multipliers $$\delta s_{full} = 0 , \quad s_{full} = s_{orig}+\sum\int \lambda ( g ( x^i ) -c ) $$ may be seen to imply $g ( x^i ) =c$  that is the derivative of the full action with respect to the lagrange multiplier ( s ) $\lambda$ . because $\delta s_{full} = 0$ implies $g ( x^i ) =c$ , among other things , we may assume this relationship while extremizing $s_{full}$ on the subspace of the configuration space that obeys the conditions $g ( x^i ) =c$ . but on this submanifold , $s_{full}=s_{orig}$ , so the two extremization conditions are equivalent .
per this article on the subject : http://theenergycollective.com/nathantemple/53384/how-shutdown-and-core-cooling-japanese-reactors-likely-functions even with rods inserted , the reactor continues to produce heat equivalent to about 3% of its full power level . this is not the same as taking a pot off the stove and letting it cool . there are still some atoms splitting and fission products decaying that produce heat . this drops off slowly and is why there needs to be layers of redundant cooling with backup power . during such an earthquake , power from outside the plant would not be expected to be available .
you seem to be doing dimensional analysis in si units . the paper seems to using gaussian units . the magnetic field differs between these units by a factor $c$ . in si units we have $$\mathbf f = q ( \mathbf e + \mathbf v \times \mathbf b \tag{si} ) $$ but in gaussian units $$\mathbf f = q ( \mathbf e + \frac{\mathbf{v}}{c} \times \mathbf b \tag{g} ) . $$ the latter definition makes electric and magnetic fields have the same unit and is the form used in the paper you linked .
the flaw is that you are trying to mix classical with relativistic concepts . gravitational lensing ( this is the phenomenon you are referring to ) is best described in terms of general relativity . massive bodies bend spacetime , inducing a curvature , which is described by einstein 's equations : $$g_{\mu\nu}=8\pi t_{\mu\nu} , $$ where on the left hand side is the einstein tensor which contains information about curvature and on the right hand side there is the energy-momentum tensor , containing information about energy and matter . from this formalism , it is possible to derive so-called geodesics , which are the paths objects will take through curved spacetime . photons feel this curvature and have to move according to it , resulting in the phenomenon we see as " bending " . below , you can find a visualization of the effect :
changing the charge density at optical frequencies is hard to do , but suppose we wind the frequency down a bit to fm radio frequencies . the question is then whether changing the charge density at fm frequencies would cause radio waves to be emitted , and of course the answer is yes because that is what a radio aerial does . well , em waves would only be emitted if there was a dipolar component of the charge density that was changing , but in practice unless your aerial is a perfect sphere i think there will always be some oscillating dipole present and hence your bar of metal will glow ( at radio frequencies )
the hamiltonian for the spin-spin interaction is : $\delta h_{ss} = \frac{\gamma_p e^2}{m m_p c^2 r^3} \big ( \frac{1}{r^3} \big ( 3 ( \vec{s}_p \cdot \hat{r} ) ( \vec{s}_e \cdot \hat{r} ) - ( \vec{s}_p \cdot \vec{s}_p ) \big ) +\frac{8 \pi}{3} ( \vec{s}_p \cdot \vec{s}_p ) \delta^{ ( 3 ) } ( \vec{r} ) \big ) $ for the cases where $l \neq 0$ the term with the delta function cancels , and the wavefunction is proportional to $r^l$ for small $r$ values . thus , when $l&gt ; 0$ we get that $\psi ( 0 ) =0$ , and then the correction to the energy will be : $\delta e_{hf} = \frac{\gamma_p e^2}{m m_p c^2} \langle \frac{1}{r^3} \big ( ( \vec{l} \cdot \vec{s}_p ) +3 ( \vec{s}_p \cdot \hat{r} ) ( \vec{s}_e \cdot \hat{r} ) - ( \vec{s}_p \cdot \vec{s}_p ) \big ) \rangle $ this expectation value was calculated by bethe and salpeter , and the result is : $\delta e_{hf} = \frac{m}{m_p} \alpha^4 mc^2 \frac{\gamma_p}{2 n^3} \big ( \frac{ f ( f+1 ) -j ( j+1 ) -\frac{3}{4}}{j ( j+1 ) ( l+\frac{1}{2} ) } \big ) $ , this result coincide with the $l=0$ case , since then $j=\frac{1}{2}$ and the proton has spin 1/2 so $f=j \pm \frac{1}{2}$ and the expression above can then be simplified to the result you already have for the $l=0$ case . . . ( next time try older qm books like this one http://adsabs.harvard.edu/abs/1957qmot.book.....b )
a very loose answer would be that the time period actually depends upon the angular acceleration and not the torque . just like the time taken for a object to fall through a height of $h$ , depends on the gravitational acceleration and not the mass , i.e. if you drop a sponge ball or you jump yourself , you both will cover height $h$ in the same time ( of course neglecting air resistance ) . similarly , the time period of a pendulum does not depend upon the mass , or rather the inertia of the pendulum , but only on the angular acceleration due to gravity . now you might ask that in this case , it should also not depend upon the length , but the term of length comes when you calculate the angular acceleration due to the acceleration of gravity .
your understanding is correct . a destructive partial measurement will result in a deterministic bit . but it will never collapse to the state with zero probability as in your example . i have tried the example in their about page of the MeasureBit(b) . however , after the partial measurement , the state always corresponds to the measurement result of 0 , no matter whether the actual measurement result give 0 of 1 . so it is a bug .
the answer is no , op 's argument ( v1 ) is not enough to derive the lagrangian for a non-relativistic free particle . it is true that the constant of motion mentioned by op $$\vec{c}~:=~\frac{\partial l}{\partial \vec{v}}~=~2\vec{v}~\ell^{\prime}$$ does not depend on time $t$ . ( it is in fact the canonical/conjugate momentum , which in general is different from the mechanical/kinetic momentum $m\vec{v}$ . ) however , $\vec{c}$ could still depend on e.g. the initial velocity of the particle ( which is also the velocity $\vec{v}$ , since we know that the velocity is constant for a free particle , cf . e.g. the first part of the linked answer ) . this is perhaps best illustrated by taking a simple example , say $$l~=~\ell ( v^2 ) ~=~ v^4 . $$ then $$\vec{c}~=~2\vec{v}~\ell^{\prime} ~=~4\vec{v}~v^2 , $$ which is a constant of motion , as it should be .
is it due to this similar triangle relation , derived from the fact light rays will always be seen at the same speed , so we draw them at $\pi/4$ ( larger link http://i.stack.imgur.com/zavod.jpg )
the situation in which a observes some object moving at a velocity $c+u$ cannot occur within the framework of special relativity , therefore your scenario is meaningless . the theory puts an upper speed limit on observable motion , which cannot even be reached by massive bodies , but only massless ones . this limit is given precisely by $c$ . hence , assuming that $u$ is positive , nothing can move at $c+u$ .
so i understand the electromagnetic spectrum -- electromagnetic radiation is mediated by photons briefly , electromagnetic radiation is due to real ( observable ) photons ; electric and magnetic force are due to virtual photon exchange . the macroscopic electromagnetic wave phenomena we observe are due to an almost unimaginable number of photons , electromagnetic " quanta " , coherently adding together . this is where i get lost ; i can not visualize how it works . this topic is not something that one " groks " overnight or , if you are like me ( an ee ) , over some period of years . it is a continuous process . just today , while driving to lowe 's , something i had been thinking a long time about in quantum field theory suddenly became very clear . the fact is , no matter how many classes you take or books you read , much of the material must , like a great chili , " stew " for awhile before it is ready .
your proof is right ( and i voted it up accordingly ) . but this is a result that is worth proving a few different ways , because the different ways lead to different insights . so i will give some alternative proofs . proof 2 ( yours is proof 1 ) : taking linear combinations of the equations we are trying to solve , we get an equivalent pair of equations . $$ h_x+ih_y=\left ( {\partial\over\partial x}+i{\partial\over\partial y}\right ) ( \phi-i\psi ) $$ $$ h_x-ih_y=\left ( {\partial\over\partial x}-i{\partial\over\partial y}\right ) ( \phi+i\psi ) . $$ if we define some shorthand , $$ \partial_\pm={\partial\over\partial x}\pm i{\partial\over\partial y} , $$ $$ f_{\pm}=\phi\pm i\psi , $$ $$ h_{\pm}=h_x\pm ih_y , $$ we can write these as $$ h_+=\partial_+f_- , $$ $$ h_-=\partial_-f_+ . $$ now we can think of these as two uncoupled equations for the two unknowns $f_\pm$ . once we have solved for them , we can get $\phi , \psi$ from them . so we have to prove that each of these two equations has a solution . to solve for $f_-$ , we start by finding a solution $g$ to $$ \nabla^2g=h_+ . $$ ( this is poisson 's equation , so it has a solution . ) then set $f_-=\partial_-g$ . because of the identity $$ \nabla^2=\partial_+\partial_- , $$ we have $$\partial_+f_-=\partial_+\partial_-g=\nabla^2g=h_+ , $$ which is what we wanted . a similar construction works for $f_-$ . in fact , every step of the argument for $f_-$ is just the complex conjugate of the corresponding step for $f_+$ ( as long as the $h$ 's are real ) . that is the way it has to be for $\phi , \psi$ to end up real . i have a personal reason i like this argument . i work a lot with maps of linear polarization , which are spin-2 fields rather than spin-1 ( vector ) fields . the equivalent of the helmholtz theorem for spin-2 fields is called the $e$-$b$ decomposition ( in the cosmology literature anyway ) . the above argument generalizes in a nice way to spin-2 ( and presumably higher spins ) . proof 3 : intuitively , it seems like we ought to be able to get the 2-d result from the 3-d helmholtz theorem , and it turns out that we can . here 's one way to think about it . extend the vector field ${\bf h}$ to be a function of $ ( x , y , z ) $: $$ {\bf h}_{3d} ( x , y , z ) ={\bf h} ( x , y ) . $$ for simplicity , imagine that $z$ extends only over a finite interval , say 0 to $2\pi$ , and has periodic boundary conditions ( i.e. . , points with $z=0$ are identified with those with $z=2\pi$ ) . it is not necessary to do this , but it makes things cleaner . helmholtz 's theorem says that there are functions $\phi , {\bf g}$ such that $$ {\bf h}_{3d}=\nabla\phi+\nabla\times{\bf g} . $$ if we knew that $\phi , {\bf g}$ were independent of $z$ and that ${\bf g}$ pointed in the $z$ direction , we had be done . but we do not ( yet ) know that . here 's the trick . both $\phi$ and ${\bf g}$ are nice , smooth functions , so they can be expanded in fourier series in $z$: $$ \phi ( x , y , z ) =\sum_{n=-\infty}^\infty \phi_n ( x , y ) e^{inz} , $$ and similarly for ${\bf g}$ . substituting these into the previous equation , we get $$ {\bf h}_{3d}=\sum_n\left ( \nabla ( \phi_ne^{inz} ) +\nabla\times ( {\bf g}_ne^{inz} ) \right ) . $$ each term on the right has $z$-dependence of the form $e^{inz}$ ( even after writing out the derivatives ) . but the left side is independent of $z$ . by the uniqueness of fourier series , it follows that all the terms with $n\ne 0$ on the right vanish ! so $$ {\bf h}_{3d}=\nabla\phi_0+\nabla\times{\bf g}_0 . $$ we know that $\phi_0 , {\bf g}_0$ depend only on $ ( x , y ) $ , not $z$ . setting $\phi ( x , y ) =\phi_0 ( x , y ) $ and $\psi ( x , y ) =g_z ( x , y ) $ gives the desired result . this one could almost certainly be phrased more compactly in more formal mathematical language , probably involving symmetry groups . the basic idea is that the problem we are solving is invariant under translations in $z$ , and the operations we are performing ( in some sense ) " commute " with these translations . that means it is possible to find a solution that respects that symmetry . " proof " 4: the mathematicians will not like this one , but i think it is a nice way to think about it anyway . take 2-d fourier transforms of everything in sight : $$ {\bf h} ( x , y ) =\int \tilde{\bf h} ( k_x , k_y ) e^{i{\bf k}\cdot{\bf r}}d^2k , $$ and similarly for $\phi , \psi$ . when you do , the equations you are trying to solve decouple -- that is , each value of ${\bf k}$ can be solved independently : $$ \tilde{\bf h} ( {\bf k} ) =i{\bf k}\tilde{\phi} ( {\bf k} ) +i\hat{\bf z}\times{\bf k}\tilde\psi ( {\bf k} ) . $$ ( there may be a sign error in the above , but it is " morally " true . ) this equation can be solved algebraically for each ${\bf k}$ . in fact , the solution has a nice physical meaning : decompose $\tilde{\bf h}$ into components parallel and perpendicular to ${\bf k}$ . $\tilde\phi$ is the parallel component , and $\tilde\psi$ is the perpendicular component . i like this one because it too generalizes to the spin-2 case nicely , and provides better intuition than anything else i can think of about the " meaning " of the $e$-$b$ decomposition for spin-2 fields . the reason i say the mathematicians will not like it is because not everything has a fourier transform , and the convergence of fourier transforms is not normal pointwise convergence . but for physics applications , it is generally a fine way to think about things .
objects do not accelerate because they are inside other objects . objects accelerate because other objects make forces on them . the chain of cause and effect here is that the box can affect the air , then the air can affect the helicopter . the answer to the question depends on how rapidly the box is accelerated . to pick an extreme case , suppose that the box is accelerated so rapidly that the wall hits the helicopter at greater than the speed of sound . until the wall hits it , the helicopter will not have felt any " disturbance in the force " from the air , because any disturbance in the air can only travel at the speed of sound . at the opposite extreme , let the box be anchored to the earth 's surface . the earth 's surface is accelerating due to the earth 's rotation . clearly the helicopter will not collide with the walls when the acceleration is this small .
the neutron star crust is a solid and there are indeed elastic waves for which the speed of sound is controlled by the shear modulus . i am not sure where you got your estimate of the shear modulus from ( there is some literature on the subject , see for example http://arxiv.org/abs/1104.0173 ) . most of the neutron stat is a liquid , and the speed of sound is given by the usual hydrodynamic result $$c_s^2=\left ( \frac{\partial p}{\partial\rho}\right ) _{s} . $$ in dilute , weakly interacting neutron matter the speed of sound ( in units of the speed of light $c$ ) is $$ c_s^2 = \frac{1}{3}\frac{k_f}{\sqrt{k_f^2+m^2}}$$ where the fermi momentum $k_f$ is determined by the density , $$ n = \frac{k_f^3}{3\pi^2} . $$ you can see that in the high density ( relativistic ) limit the speed of sound approaches $c/\sqrt{3}$ . in the center of a neutron star you get quite close to this . interaction change this result by factors of order one , but as an order of magnitude estimate these simple results are quite good .
i am afraid to say that unless we are considering incredibly cold superconductors ( which have been known to sustain currents for years ) , none of these devices work . the first video offers instruction on building all manner of green devices including solar power generators and i am not sure if it even claims to be a perpetual motion machine . i mention the other devices because if it was perpetual energy generation , you would not need anything else ! the second video makes all manner of strange claims , including room temperature superconductivity which would be defying quite a few laws of physics and regardless , is not something proven by a one and a half minute video of something spinning . as well as saying that the earth 's orbit around the sun is perpetual which is not true as the orbit is continually changing - it just lasts a long time . as for electrons orbiting an atom , this just is not true either . it is an old idea that was used in teasing out conceptually what was happening , but nearly all of the predictions based on this are wrong . hope that helps , please feel free to ask any more questions .
assuming energy conservation is not " bodged " because at the most fundamental level , energy is defined as the quantity that is conserved as the result of the time-translational symmetry . all specific formulae for energy , such as $mv^2/2$ in nonrelativistic mechanics , are just solutions to the problem " find a conserved quantity linked to that symmetry " . still , you can try to achieve what you have defined . first , you must realize that $k=mv^2/2$ only holds if $v\ll c$: it is just not a valid formula in relativity for large velocities . it seems that you believe that $e=mv^2/2$ is correct in some frames even in relativity but it is not . your formula is just an approximation , via taylor expansions , $$ \frac{mc^2}{\sqrt{1-v^2/c^2}} = mc^2 + \frac{mv^2}{2} + \frac{3mv^4}{8c^2} + \dots $$ if you want to use $e=mv^2$ for small $v$ and deduce what is $e$ for an arbitrary $v$ comparable to the speed of light $c$ , you must use infinitely many interpolating inertial systems . in this se question how to deduce the theorem of addition of velocities ? ron maimon explained how velocities add . so if you want to switch to an inertial system moving by velocity $v$ , you may calculate a rapidity from $$\tanh a = \frac vc$$ these rapidities behave as angles so if you are boosting by some incremental speeds many times , the rapidities just add up ( much like angles for rotations ) . the total energy is then $mc^2\cdot \cosh a$ which is equal to the usual relativistic formula but the derivation of this fact will have to use some conservation of energy argument similar to one you know . the relativistic formula is the only one that reduces to $mv^2/2$ for infinitesimal $v$ and that conserves the energy while the object is boosted .
i think the fundamental object is quantum mechanics is not the hilbert space and operators on it but the c*-algebra of observables . in this picture the hilbert space appears as a representation of the algebra . different irreducible representations are different superselection sectors . the answer to " which operator is observable " is thus simple : the observable operators are those that come from the algebra . indeed its better to think of observables as self-adjoint elements of the algebra rather than as operators you might ask where do we get the algebra from . well , this already should be supplied by the particular model . for a quantum mechanical particle moving on a manifold $m$ , the c*-algebra consists of all bounded operators on $l^2 ( \hat{m} ) $ commuting with $\pi_1 ( m ) $ where $\hat{m}$ is the universal cover of $m$ . the superselection sectors correspond to irreducible representations of $\pi_1 ( m ) $ . for qfts the problem of constructing the algebra of observables is in general open however certain cases ( such as free qft and i believe rational cft as well ) were solved . an approach emphasizing the algebra point of view is haag-kastler axiomatic qft from the point of view of deformation quantization the quantum observable algebra is a non-commutative deformation of the algebra of continuous ( say ) functions on the classical phase space . this point of view is not fundamental but it is useful . for example it allows to understand different values of spin and different quantum statistics as superselection sectors
the keywords here are rayleigh scattering . see also diffuse sky radiation . but much more simply , it has to do with the way that sunlight interacts with air molecules . blue light is scattered more than red light , so during the day when we look at parts of the sky that are away from the sun , we see more blue than red . during sunset or sunrise , most of the light from the sun comes towards the earth at a sharp angle , so now the blue light is mostly scattered away , and we see mostly red light .
i have answered a series of related questions recently . do a search of the site for " flrw metric " to find relevant questions . anyhow , the idea of the big bang arose from a solution to the einstein equations called the flrw metric . if you assume that the universe is homogenous and isotropic ( i.e. . that it is basically the same everywhere ) then you can solve the einstein equation to give an equation that describes how the universe expands . this solution is called the flrw metric , and it does seem to be a very good description of the universe as we see it today . if you take the flrw description of the universe and you wind time backwards , then as you approach 13.7 billion years ago the density becomes greater and greater and eventually becomes infinite . this point is known as the big bang . the big bang happens at the same time everywhere . actually , at the moment of the big bang the flrw metric predicts that the spacing between any two points falls to zero , but the universe remains infinite . this paradoxical situation is why most of think that some theory of quantum gravity will cut in before we reach the big bang . so , to answer your question , the universe today seems to be well described by the flrw metric , and the flrw metric predicts that the big bang happened everywhere at the same time . to answer your specific question about the red shift , the red shift appears to be the same in all directions so this supports an isotropic universe and a single big bang everywhere . it is possible that our interpretation of the experimental evidence might change as we learn more . for example you might like to have a look at this question , which discusses eternal inflation and the possibility that the big bang is different in different parts of the universe . at the moment there is no experimental evidence to support the idea of eternal inflation .
turns out michael brown was right after all . the calculations are correct for the curvature terms in the tetrad basis .
in order to use lagrangians in qm , one has to use the path integral formalism . this is usually not covered in a undergrad qm course and therefore only hamiltonians are used . in current research , lagrangians are used a lot in non-relativistic qm . in relativistic qm , one uses both hamiltonians and lagrangians . the reason lagrangians are more popular is that it sets time and spacial coordinates on the same footing , which makes it possible to write down relativistic theories in a covariant way . using hamiltonians , relativistic invariance is not explicit and it can complicate many things . so both formalism are used in both relativistic and non-relativistic quantum physics . this is the very short answer .
if you do not consider atmospheric drag and other dissipative effects , assuming that the earth is non-rotating , and ignore general relativity ( which is a lot of assumptions to make , and later i can try to edit this to comment on how they might be relaxed ) , a ballistic trajectory is just a segment of an orbit . so finding the launch parameters just entails finding an orbit that intersects both the launch point and the destination . orbital trajectories are given by the equation $$r_o = \frac{\rho}{1 + \epsilon\cos\phi}$$ where $\rho$ is a length parameter related to the size of the orbit and $\epsilon$ is the eccentricity . at the points where the orbit intersects the earth 's surface , you will have $r_o$ equaling the radius of the earth at that point ; let 's say $r_o = r_l$ for the launch point and $r_o = r_d$ for the destination . this gives you the following two equations , $$r_l = \frac{\rho}{1 + \epsilon\cos\phi_l}$$ $$r_d = \frac{\rho}{1 + \epsilon\cos\phi_d}$$ along with the constraint that $\phi_d - \phi_l$ ( or vice-versa , depending on how you define coordinates ) has to be equal to the angular separation $\delta$ between the source and destination . that angular separation can be calculated using the law of haversines . this system of equations can be solved for the orbital parameters as follows , assuming $r_d \neq r_l$ ( see below for the other case ) , $$\epsilon = \frac{r_l - r_d}{r_d\cos ( \phi_l + \delta ) - r_l\cos\phi_l}$$ $$\rho = \frac{r_d r_l [ \cos ( \phi_l + \delta ) - \cos\phi_l ] }{r_d\cos ( \phi_l + \delta ) - r_l\cos\phi_l}$$ notice that they are dependent on $\phi_l$ ; this is because only the difference $\phi_d - \phi_l = \delta$ is physically relevant , so you are free to choose the actual values to be whatever you like . your choice of $\phi_l$ will influence the shape of the trajectory . having calculated $\epsilon$ and $\rho$ , you can determine the launch angle $\alpha$ above the horizon by calculating the slope of the orbit at the launch point : $$\tan\alpha = \left . \frac{1}{r_o}\frac{\mathrm{d}r_o}{\mathrm{d}\phi}\right|_{\phi_l} = \frac{\epsilon\sin\phi_l}{1 + \epsilon\cos\phi_l} = \frac{ ( r_l - r_d ) \sin\phi_l}{r_d [ \cos ( \phi_l + \delta ) - \cos\phi_l ] }$$ to determine the speed , you can use the fact that ( according to my notes ) the total energy of the projectile is given by $$e = \frac{gmm ( \epsilon^2 - 1 ) }{2\rho}$$ which is equal to the sum of kinetic and potential energies , $\frac{1}{2}mv^2 - \frac{gmm}{r}$ . ( $m$ is the mass of the projectile , $m$ is that of the earth ) plugging in $r = r_l$ , i get $$v = \sqrt{\frac{2gm}{r_l} + \frac{gm ( \epsilon^2 - 1 ) }{\rho}}$$ so the bottom line is that you plug $r_d$ , $r_l$ , $\delta$ , and some choice of $\phi_l$ into the formulas for $v$ and $\alpha$ to get the launch parameters . i mentioned that the procedure above hits a snag if $r_d = r_l$ . you had wind up calculating $\epsilon = 0$ , which on a spherical planet entails rolling your projectile along the ground ; - ) which does not make sense . if $r_d = r_l$ , you can go back to the orbital equations and find that $\cos\phi_d = \cos\phi_l$ . ( alternatively , this could come from the condition that the denominator of $\epsilon$ also be zero . ) the only way to satisfy this is by setting $\phi_l = -\phi_d = \delta/2$ . this gives you the condition $$\rho = r_l\left ( 1 + \epsilon\cos\frac{\delta}{2}\right ) $$ again , you have a degree of freedom : you can choose any value of $\epsilon$ , and plugging into this equation will give you the corresponding value of $\rho$ . once you have those , you can use the same procedures to calculate the launch angle and speed : $$\tan\alpha = \left . \frac{1}{r_o}\frac{\mathrm{d}r_o}{\mathrm{d}\phi}\right|_{\phi_l} = \frac{\epsilon\sin\frac{\delta}{2}}{1 + \epsilon\cos\frac{\delta}{2}}$$ $$v = \sqrt{\frac{2gm}{r_l} + \frac{gm ( \epsilon^2 - 1 ) }{r_l ( 1 + \epsilon\cos\frac{\delta}{2} ) }}$$
as mitchell porter said , we need more than just the absence of self-evident contradictions . we need a theory that encompasses both quantum mechanics and general relativity . the most straightforward , naive union of these frameworks produces a theory that is " nonrenormalizable "  predicts all quantities to be equal to a finite number plus infinity ( many types of infinities arise ) . string theory is the only known reconciliation of general relativity and quantum mechanics and chances are high that this status will not ever change . there is of course no contradiction between " appropriately reduced in reach " general relativity and " appropriately tamed " quantum mechanics  after all , to a certain extent , both of these frameworks have been established so there has to exist a more accurate theory that agrees with all the established insights . however , there are contradictions between quantum mechanics ( which seems perfectly exact and valid ) and classical general relativity believed literally . for example , classical general relativity paints a black hole as a perfectly determined , unique state of the spacetime . it carries no entropy because it has " no hair " according to classical general relativity . according to quantum mechanics , this can not be the case . a black hole has to carry and does carry a huge entropy  in fact , a greater entropy than any other localized object of the same mass  which is needed for the second law of thermodynamics to hold ( entropy has to increase in time ) and which is needed to " preserve the information " about the initial state , something that is required by " unitarity " in quantum mechanics . so some " tunneling " of the information from the interior has to be possible .
thermal broadening by itself does not cause coherence loss , in the sense that if you put a fermion into a free fermi gas at any temperature , it stays coherent forever , because it does not interact with the gas . what people usually mean by " loss of coherence due to thermal broadening " is two things which both act together : the distribution of thermal electrons is completely random , in that the density matrix has no off-diagonal matrix elements in the energy representation . the elecrons are strongly interacting , and the interactions do not affect the thermal distribution , but instead spread the stochasticity from the thermal electrons to the non-thermal electrons . but the reference you are citing is saying something else , which is less precise . this is talking about electrons of different energies scattering off the same time-dependent potential . you can not know which electron is which , you get a random phase for the scattering , depending on the thermally random phase of the electron coming in . so if you are trying to ask what the phase of one electron only is going to be , you get a random answer . this is not really decoherence , in principle , because you have not entangled information about the electron state in an outgoing quantum , which can be thought of as doing a measurement . but you can still lost effective coherence anyway , because you will not be able to track the coherent electrons through the scattering event , because you will not be able to extract the crazy basis in which the density matrix has good non-random off-diagonal elements . this is very closely analogous to the classical problem of a thermal gas of x atoms with one x atom that you know for sure is close to a certain point , with close to a certain velocity . in principle , you never lose this information , because the liouville theorem guarantees that the entropy is constant . but after a few scattering events , even just in an external potential , you will not be able to use this information effectively , and you will lose your extra x particle to the thermal background . if you could describe what type of decoherence calculation this type of statement is used for , it would help make the answer more precise .
your skepticism is well founded in this case . for a low orbit such as this , the leading perturbation will be from the mass concentrations ( colloquially " mascons" ) in the moon , which produce large variations in the gravitational field . the relevant timescale is probably weeks at best . the orbit of the apollo 16 subsatellite pfs-2 decayed so rapidly that the instrument impacted the moon after just 34 days . the grail mission ( currently in science phase ) has been planned to take advantage of this behavior in order to smoothly vary the orbits and study the gravitational field of the moon . the spacecrafts are planned for decommission on may 29 , 2012 and are expected to impact within one month . it is worth noting that at some inclinations , low orbits may be much more stable ( 7 , 50 , 76 , and 86 ) , but apollo 8 was inserted at 12 . at this inclination the orbit was actually observed to be perturbated by several kilometers over the course of the 20 hour mission .
i will answer your second question because it is the one with which i am more familiar . the question we are answering is : " why does current in a superconductor move with no resistance ? " to understand this we should first understand why normal metals have nonzero resistivity . imagine an electron in the metal and suppose it is traveling in some direction . if the electron never interacted with anything else then it would just go along merrily in that direction and in fact the resistance would be zero . however , in a normal metal the traveling electrons interact ( "collide" ) with the ions of the metal via the coulomb force because they are both charged . this transfers energy and momentum from the electron to the ion . since that ion is tightly connected to the other metal ions , this energy and momentum transfer causes the lattice of ions to vibrate . an excitation of this kind of vibration of the lattice is called a " phonon " . when you will hear people say that , in a normal metal , electrons scatter off of phonons , this is what they are talking about . by the way , excitation of phonons is precisely heating of the metal , so we see that as the electron loses energy the metal heats up . this is joule heating . so , we can say that in a normal metal the resistivity is non-zero because the electrons scatter off of phonons . in a superconductor you had think the situation would be the same . there are still vibrational modes in the metal , and the electrons and ions are still charged so of course they still interact via the coulomb force . to understand why this does not happen we have to more carefully consider what is going on in a normal metal . when an electron scatters off of a phonon the electron 's state ( ie . it is direction and speed of travel ) changes . electrons are in a class of particles called fermions . fermions are subject to the pauli exclusion principle , which is that it is impossible to have two fermions occupying the same state . that means , for example , that you can not have two electrons in the same momentum state . that means that in order for an electron to scatter off of a phonon , we have to have a situation where there is an unoccupied final state into which the electron can scatter . otherwise , the scattering process simply cannot happen . in a normal metal at finite temperature , the electrons occupy a set of momentum states up to a certain level . think of it as follows . when you put the first electron in the system it goes in the lowest energy state ( which is at zero momentum ) . because of the exclusion principle the next electron goes in a higher energy state , which also has more momentum . as you continue to add electrons you fill of a 3d ball of momentum states . this is called the " fermi sea " and is illustrated in part a of the attached diagram . at zero temperature the electrons would fill the ball up to a cutoff energy . this is shown in dark red in the diagram . however , at finite temperature there is a bit of thermal energy that allows the electrons to jump around to slightly higher energy ( and momentum ) states . this little band is shown in pink in the diagram . in part b we zoom in on the pink band . the dark circles indicate filled electron states and the open circles indicate empty electron states . when an electron interacts with a phonon , it can only scatter and change state if there is a final state available . the green arrow indicates a possible scattering process , while the gray arrow indicates one forbidden by the exclusion principle . in a superconductor , something really interesting happens . because of interactions with the phonons , it turns out that there is an effective attractive interaction between the electrons ! basically what happens is that a traveling electron with its negative charge causes the positively charged ions of the metal to get sucked together a little bit . the electron leaves really fast , but the ions take longer to move around , so after the electron is gone there is still a region of increased density of positively charged ions . this causes other electrons to get attracted to that point , because of the higher positive charge . in this way , there is a weird time dependent attraction between electrons . the attraction between electrons makes it turn out that there is a lower energy state than the filled red region shown in part a of the figure . this lower energy state is such that the usual single electron excitations are at an energy level that is a significant distance away from this new ground state . the gap in energy , shown in part c of the figure , is the reason that superconductivity happens . now if electrons were to scatter off of a phonon , the only available states they have to scatter into are a large distance away in energy . that means that you need really high energy phonons ( or something else ) to disturb this ground state . so , as long as you do not whack your superconductor too hard , there is is no scattering , and thus no resistance . this argument also explains why superconductors have zero resistance at dc , but nonzero resistance at ac . if you put in high frequency perturbations , you can introduce enough energy to actually kick a superconducting electron out of the gap into those available states in the pink region . remember , the energy of a perturbation is related to frequency by $e=h f$ . summary : superconducting metals have zero resistance because there are not any available states for electrons to scatter into . no scattering means no resistance .
for spin measurements the original experiment was the stern-gerlach experiment in which you will see that a prior unpolarized beam will split up in two ( spin up and down ) orientations . see : http://en.wikipedia.org/wiki/stern%e2%80%93gerlach_experiment for helicity , a very ingenious and fascinating experiment is the famours goldhaber experiment that uses a very peculiar set of elements and also the mbauer effect to measure the helicity of neutrinos . the helicity is the projection of the spin onto the momentum direction , and thus if you measure spin and momentum , you can compute helicity . link to original paper here : http://www.bnl.gov/nh50/ people often confuse helicity and chirality they are only the same in the case of massless particles . this is also the reason why we know that ( at least interacting ) neutrinos are always left-handed ( the famous $$su ( 2 ) _l$$ ) , at least if we assume that neutrinos are massless ( which is almost the case , but not strictly ) . in contrast to helicity for massive particles ( where you can always boost into a frame where you change the sign of the momentum direction and thus change helicity ) , chirality is a lorentz-invariant property of the particle .
( i have a suggestion to make this question a cw . ) general physics : ( early undergrad and advanced high school ) problems in physics i.e. irodov - ( highly recommended ) problems in physics s s krotov - ( once again , highly recommended but out of print ) physics olympiad books - ( have not read but saw some olympiad problems back in the day ) physics by example ( like this book a lot , lower undergrad ) feynman 's tips on physics ( exercises to accompany the famous lectures ) general qualifying exam books : the following books are a part of a series dedicated to the qualifying exams in american universities and has a large compilation of problems of all levels . others in the series include mechanics , electromagnetism , quantum mechanics , thermodynamics , optics and solid state physics . unlike other compilation of exercises for qualifiers ( such as princeton or chicago problems , or the one mentioned below ) , they make no excuse for economy and include as many problems from all levels for each subtopic . another good book that i read recently for my exam is the two volume series : a guide to physics problems ( part 2 has some relatively easy but interesting problems . i have not gone through the first part , which is much much more challenging . )
dear rajesh , in reality , physics sometimes works with continuous functions that are not infinitely differentiable - for example look at the energy of the beam at atlas . ch ( click at the " status " button in the middle ) when they ramp it up - there are all kinds of discontinuities . but an arbitrary function that is smooth almost everywhere - and this is a description of functions that really covers everything that a physicist would use - may be approximated by infinitely differentiable functions with an arbitrary accuracy . so it does not really hurt if your theorems assume that all the spacetime fields including the metric tensor are infinitely differentiable ; you may solve the situations involving functions that are not infinitely differentiable by taking a limit of the infinitely differentiable ones . may we ask a physics question whether the fields in electromagnetism are infinitely differentiable ? well , we may but it is a meaningless question because the real world is not described by classical physics . so classical physics itself is just an approximation , so both " classical physics with all differentiable functions " and " classical physics with infinitely differentiable functions only " are just approximations of the reality , with none of them being more " physically real " . in quantum physics , we do not use classical fields but we may use wave functions . the time evolution is dictated by schrdinger 's equations - but may we ask whether $\psi ( x , y , z ) $ should be an infinitely differentiable function of $x , y , z$ ? well , in this case , we usually do not make this assumption . instead , we use all $l^2$ functions which are much more natural at the level of the hilbert space , fourier transforms , and so on . the $l^2$ condition surely does not require infinite differentiability . on the other hand , a finite expectation value of the kinetic energy does force $\psi ( x , y , z ) $ to be a continuous function . nevertheless , the infinite differentiability condition is not ever natural in quantum mechanics . i want to emphasize that all these extra conditions - whether something should be smooth or infinitely smooth etc . - are only a matter of mathematical taste . there can not exist an operational " physics " way to determine whether the world allows functions that are not infinitely differentiable . it is because the infinitely smooth and not-infinitely smooth functions can mimic each other with an arbitrary accuracy but the accuracy of any measurement in physics is always limited . so the restrictions on the " nice behavior " of the mathematical functions is always a matter of mathematical taste . exotic differentiable structures could be counterexamples to what i just wrote - because they are " qualitatively " different and their pathological behavior is the very point of their existence ( so in some important sense , the ability of them to mimic the normal functions disappears ) - but their role in physics remains very confusing and limited as of today .
i am not doing any of your calculations . but consider a to b . since you know the temp change you know the energy change . since this is a isobaric process , the volume is increasing . therefore you can calculate the work done that the helium does on its environment . since you know energy change and work done you can calculate the heat exchange . for b to c the process is isothermal . since there is no energy exchange you know that the work done is equal and opposite to the heat exchange . since $$w = \int p dv $$ and $$p = \frac{c}{v} $$ where c depends on your units ( boltzmann constant or ideal gas constant ) . you can use this to integrate from v1 to v2 and solve for the work done . you dont need any more information than this for the last two processes .
you are looking for this : http://en.wikipedia.org/wiki/entropy_in_thermodynamics_and_information_theory#information_is_physical so , one bit of information allows an amount of work equal to $kt\ln2$ . where $k$ is boltzmann 's constant and $t$ is the thermodynamic temperature . i have seen the article in nature and i think it is horrible . they do a very bad service in explaining what is happening . the way they do it , it sounds like they invented a perpetuum mobile , which is not true . how was the information uncovered in the first place ? that involved energy . and as thermodynamics will have it , more energy than the information provided you to perform work with the bead .
first of all , i would like to recommend the following book . it is the best introduction to distributions -and many other topics- i have ever seen , insisting on intuition and applications in physics , but the mathematical presentation should be rigorous enough to give you an entry point into the literature . as for your questions : the simplest example is that of a problem with two very different length scales , e.g. the electric field created by a number of charged objects , much smaller than the typical distance between them ( and hence than the distance over which the electric field varies ) . we can then treat these objects as point charges . one can justify this approach a posteriori , by going to a more detailed description of the system and showing that the solutions are close enough . this is indeed a good way of seeing the dirac delta . the trick with differential operators is to shift them from the distribution onto the test function by partial integration . if you are uncomfortable with this , you can also go back to the limiting process that you allude to and perform the partial integration with " nice " functions that approach the distribution . discontinuous wave functions would have infinite momentum ( although they do seem relevant for some class of peculiarly " spiky " potentials ) . in complement h1 , cohen-tannoudji and al . show that the wave function remains continuous at a step potential by taking the limit of smooth potentials . i cannot find the place where they invoke measurement precision . i think a good idea would be to start from a physicist 's point of view , hence the reference above . the final questions : as to $\frac{1}{x}$ , see the answer by @5891user ; you can define it using cauchy 's principal value and show that it has the expected propeties , e.g. that $\text{pv} \frac{1}{x} \cdot x = \mathbb{1}$ ( the identity ) . thus , your last three questions pose no particular difficulty apart from acquiring an intuitive understanding of these equations ( which can be a challenge ) .
the quantity that tells you what time an observer travelling along a path $\gamma : [ t_0 , t_1 ] \rightarrow \mathbb{r}^4$ experiences is the proper time $$ \tau = \int_\gamma \sqrt{\mathrm{d}x_\mu\mathrm{d}x^\mu}$$ assuming flat spacetime , i.e. minkowski metric/special relativity , this reduces to $$ \tau = \int_\gamma\sqrt{\mathrm{d}t^2 - \frac{1}{c^2}\mathrm{d}x^i\mathrm{d}x_i} =\int_{t_0}^{t_1} \sqrt{1 - \frac{\vec{v} ( t ) ^2}{c^2}}\mathrm{d}t $$ for $\vec{v} = c$ , i.e. an object travelling with the speed of light , this is $\int_{t_0}^{t_1} 0 \mathrm{d}t = 0$ , so anyone hypothetically travelling with the speed of light will indeed not experience any time at all . by plugging in other values for the travelling speed $\vec{v}$ , you are able to calculate the experienced time for arbitary travellers .
this is effectively the same as the following question : how do $\pi^0$ particles exist ? i hope this helps .
there is an approach called weak measurement that can be used to probe the properties of a superposition without destroying it . there is a reasonable detailed article on it on wikipedia , or a more accessible article on the nature web site .
ok , the inside of the sphere is perfectly-reflecting , and there is an ideal optical diode to let light in but keep it inside . as you keep the light turned on , the photon density in the sphere goes up and up , of course . it " looks " brighter and brighter , but you do not see that because the light can not escape . after turning the light off , it stays bright , the photons just keep bouncing around . if you " stick your head in " to look , you see a bright uniform glow that quickly dies away because your head and eyes are absorbing all the photons . but do the photons bounce around forever ? no ! ! even a perfectly-reflective sphere will still interact with the light , because of radiation pressure . each time a photon bounces off a wall , the wall gets kicked backwards , gaining energy at the expense of the photon ( on average ) . light can not produce a smooth force , only a series of kicks with shot noise statistics , because one photon hits the wall at a time . these kicks eventually heat up the walls , and cool down the photons . ( from the photon 's point of view , the photon frequency is going down because of doppler-shifts during reflection off the moving walls . ) eventually everything equilibrates to a uniform temperature , hotter than the sphere started out . i do not know how long that would take . [ in any realistic circumstance this radiation pressure effect can be ignored , because it is much less important than the " reflection is not 100% perfect " effect . ]
$k$ in thermodynamics ( and quantum mechanics , and some other disciplines touched by either ) is boltzmann 's constant . thermal conductivity ( units $\frac{\mathrm w}{\mathrm m^2 \cdot \mathrm k}$ ) is also named $k$ , but that is not what you have there either . refer to that article for help in deriving what your incomplete equation calls $k$ .
the interaction is not a measurement because the probability that it will produce a measurable change in the momentum of the reflecting object is extremely small . the reflector is in a mixed state in which its momentum has a range of values that is large compared to the momentum of the photon . so the shift in the mirror 's momentum as a result of the reflection will be so small that it will not be detectable with very high probability . i do not know the exact numbers but let 's say it is -1,000,000 to +1,000,000 in units of the momentum the photon will impart upon reflection . the photon is incident on the mirror and changes its momentum by +1 so that the range is now -999,999 to +1,000,001 . so the probability is large that if you measure its state you will not detect any difference .
in general , when a physical system is free to change , the system will tend to change its state so as to minimize the free energy . it is a principle . for example , remove a dam from a river and the water from the lake behind the dam spontaneously flows down the canyon . no need to do additional work to ' push ' the water down . the energy to flow around small obstacles on the way down comes from gravitational potential energy . temperature arises from the kinetic energy of the motion of molecules . if a gas is contained in a small container , and the container is opened into a larger volume , the gas molecules spontaneously move to occupy the larger volume . if the expanding gas does some work , the energy for this work comes from the kinetic energy . the expanding gas might do work against the side of the container ( i.e. . a piston ) or against attractive intermolecular forces ( i.e. . a real gas . ) the decrease in kinetic energy of the system is sensed as a decrease in temperature . reversible expansion of an ideal-gas which does no work is isothermal . the molecules in a liquid stick together and form a liquid because of inter-molecular forces between the molecules . work is done against these forces to increase the distance between molecules and form the gas phase . this work decreases the kinetic energy , and the temperature decreases .
assuming you mean a macroscopic potential difference , the largest i know about was in the nuclear structure facility accelerator at the daresbury laboratory in the uk , and this was 30mv . the wikipedia article on electrostatic particle accelerators claims this is about the highest possible in such devices .
it means that the ball is pushing to the wall . and , due to newton 's third law , the wall also on the ball . the forces on the ball are then equal and opposite .
answer : the problem says that the system was moving right with constant velocity . so , all the friction forces will work towards left . hence , the equations will become $$t+n_1 \mu = m_1 a$$ and $$m_2 g \sin 37- t + m_2 g \cos 37 = m_2a$$ solving these will give you the right answer ! elementary , my dear watson !
there is a lot of old threads here with the same question . here 's a link to an old , though closed thread with some good suggestions what is a good introductory book on quantum mechanics ?
i ) it depends on how abstract op wants it to be . say that we discard any reference to 1d geometry , and position and momentum operators $\hat{q}$ and $\hat{p}$ . say that we only know that $$\tag{1}\frac{\hat{h}}{\hbar\omega} ~:=~ \hat{n}+\nu{\bf 1} , \qquad\qquad \nu\in\mathbb{r} , $$ $$\tag{2} \hat{n}~:=~\hat{a}^{\dagger}\hat{a} , $$ $$\tag{3} [ \hat{a} , \hat{a}^{\dagger} ] ~=~{\bf 1} , \qquad\qquad [ {\bf 1} , \cdot ] ~=~0 . $$ ( since we have cut any reference to geometry , there is no longer any reason why $\nu$ should be a half , so we have generalized it to an arbitrary real number $\nu\in\mathbb{r}$ . ) ii ) next assume that the physical states live in an inner product space $ ( v , \langle \cdot , \cdot \rangle ) $ , and that $v$ form a non-trivial irreducible unitary representation of the heisenberg algebra , $$\tag{4} {\cal a}~:=~ \text{associative algebra generated by $\hat{a}$ , $\hat{a}^{\dagger}$ , and ${\bf 1}$} . $$ the spectrum of a semi- positive operator $\hat{n}=\hat{a}^{\dagger}\hat{a}$ is always non-negative , $$\tag{5} {\rm spec} ( \hat{n} ) ~\subseteq~ [ 0 , \infty [ . $$ in particular , the spectrum ${\rm spec} ( \hat{n} ) $ is bounded from below . since the operator $\hat{n}$ commutes with the hamiltonian $\hat{h}$ , we can use $\hat{n}$ to classify the physical states . let us sketch how the standard argument goes . say that $|n_0\rangle\neq 0$ is a normalized eigenstate for $\hat{n}$ with eigenvalue $n_0\in [ 0 , \infty [ $ . we can use the lowering ladder ( annihilation ) operator $\hat{a}$ repeatedly to define new eigenstates $$\tag{6} |n_0- 1\rangle , \quad |n_0- 2\rangle , \quad\ldots$$ which however could have zero norm . since the spectrum ${\rm spec} ( \hat{n} ) $ is bounded from below , this lowering procedure ( 6 ) must stop in finite many steps . there must exists an integer $m\in\mathbb{n}_0$ such that zero-norm occurs $$\tag{7} \hat{a}|n_0 - m\rangle~=~0 . $$ assume that $m$ is the smallest of such integers . the norm is $$\tag{8} 0 ~=~ || ~\hat{a}|n_0 - m\rangle ~||^2 ~=~ \langle n_0 - m|\hat{n}|n_0 - m\rangle ~=~ ( n_0 - m ) \underbrace{||~|n_0 - m\rangle~||^2}_{&gt ; 0} , $$ so the original eigenvalue is an integer $$\tag{9} n_0 ~=~ m\in\mathbb{n}_0 , $$ and eq . ( 7 ) becomes $$\tag{10} \hat{a}|0\rangle ~=~0 , \qquad\qquad \langle 0 |0\rangle ~\neq~0 . $$ we can next use the raising ladder ( creation ) operator $\hat{a}^{\dagger}$ repeatedly to define new eigenstates $$\tag{11} |1\rangle , \quad |2\rangle , \quad \ldots . $$ by a similar norm argument , one may see that this raising procedure ( 11 ) cannot eventually create a zero-norm state , and hence it goes on forever/does not stop . inductively , at stage $n\in\mathbb{n}_0$ , the norm remains non-zero , $$\tag{12} || ~\hat{a}^{\dagger}|n\rangle ~||^2 ~=~ \langle n|\hat{a}\hat{a}^{\dagger}|n\rangle~=~ \langle n| ( \hat{n}+1 ) |n\rangle ~=~ ( n+1 ) ~\langle n|n\rangle~&gt ; ~0 . $$ so $v$ contains at least one full copy of the standard fock space . on the other hand , by the irreducibility assumption , the vector space $v$ cannot be bigger , and $v$ is hence just a standard fock space ( up to isomorphism ) . iii ) finally , if $v$ is not irreducible , then $v$ could be a direct sum of several fock spaces . in the latter case , the ground state energy-level is degenerate .
this is one of the problems with analogies ( when not treated as analogies ) . obviously , spacetime is not a sheet and masses are not rolling on it . you encounter these kinds of problems whenever you take an analogy too far or out of its context . for example , you get in the same mess when you try to describe ( attractive ) electromagnetic forces as rubberbands between particles . because what is going on inside the rubberbands ? that is electromagnetic forces at work ! so , as with your question , this analogy gets you into trouble if you ask about the nature of the elements in the analogy .
it sounds like you already have a pretty comprehensive answer in hand , but i would mention that galactic clusters are often classified by the mass or luminosity of their nth-biggest or brightest member , where n is a smallish integer like 5 . the idea is that the biggest couple might be weird outliers , but by the time you get down to the " rank and file " galaxies in a cluster , you should have a good handle on how big the cluster is . the strength of that scheme is that it can be difficult to determine if every galaxy in your image is actually a member , so that it is difficult to arrive at a good estimate of total mass directly . . . . and you only have to take detailed data on a handful of the easiest galaxies anyway . edit : i knew there was another , more direct answer , but i could not remember the authors ' names . the tully-fisher relation and the faber-jackson relation describe empirically the relation between galactic luminosity and velocity-dispersion for spiral and elliptical galaxies , respectively . it is usually used to infer the former from the latter , but if for some reason you had luminosity in-hand already , velocity dispersion is related to the strength of the gravitational field of the galaxy and thus its mass .
hah , i just studied this a while ago with james bardeen , so i would say he is the best resource for learning this ! since you probably do not have access to the physical bardeen , you can check out : physical review d , vol 22 no 8 ( 1980 ) " gauge-invariant cosmological perturbations " and physical review d , vol 40 no 6 ( 1989 ) " designing density fluctuation spectra in inflation " there is also a set of lecture notes i have sitting on my desk by him that claim they are " to be published in particle physics and cosmology " which are dated 1988 , so presumably they were published within the next year or so . if you can find them , the talks are probably the easier of the three , and the first prd article is the second easiest . the third paper is very nice , but more technically difficult .
just to be sure , magoo is the ads/cft bible http://arxiv.org/abs/hep-th/9905111 whose authors are usually listed alphabetically . concerning your questions , conformal dimension or scaling dimension or mass dimension are meant to be the same thing in the context of conformal field theories in more than 2 dimensions . in 2 dimensions , one may distinguish the left-moving ( holomorphic ) and right-moving ( antiholomorphic ) dimensions or their sum - but the separate chiral " dimensions " are usually called " weights " , anyway . the dimension pretty much counts the exponent of " kilogram " in the unit of the corresponding operator - except that in quantum mechanics , the exponent is often fractional . the dimensions are usually positive - they get mapped to the energy which is positive as well . operators have positive dimensions of mass . if your cryptic "-ve " meant that there is a minus sign , then there is no minus sign . if you prefer to use units of length , then their powers are negative , but you should switch to masses as the base whose powers count the dimension . a ) the field $\phi$ , as opposed to $\phi_0$ , is dimensionless because it is an actual field in the ads space ( the bulk ) , unlike $\phi_0$ that is defined on the boundary . they mean that it is dimensionless under the conformal transformations of the boundary - and that is true because the conformal transformations are realized as simple isometries in the bulk , so they can not rescale the ads field $\phi$ - at most , they move it to another point . also , on that page , one deals with particular finite modified boundary conditions for $\phi$ at the boundary of the ads space so it must be finite even at $z=0$ . b ) in equation ( 3.15 ) , the left hand side is dimensionless . the right hand side has a power of $\epsilon$ whose dimension is $length^{d-\delta}$ because $\epsilon$ has units of length ( on boundary ) , and $\phi_0$ whose dimension must be $length^{\delta-d}$ as a consequence , to get a dimensionless product . in equation ( 3.13 ) , the exponent on the left hand side has $d^4x$ whose dimension is $length^d$ - note that $d=4$ for $ads_5$ ; $\phi_0$ whose dimension is $length^{\delta-d}$ as i said in the previous sentence - note that the $d$ term in the exponent cancels ; and $o$ whose dimension must therefore be $length^{-\delta}$ which means $mass^{\delta}$ . as i said , the standard base for counting dimensions is mass , so we also say that $o$ has dimension $\delta$ . so i suspect that your sign error is simply caused by the point 1 ) - namely by your incorrect assumption that dimensions refer to the powers of length . when we say " dimension " without extra specifications , we mean the power of the mass , not length . it is a healthy convention because the operators end up having non-negative dimensions . you may want to remember the dimensions of basic operators in 4 dimensions . the identity operator is always dimensionless ( and $x$-independent ) : the dimension is 0 . bosonic scalar fields $\phi_0$ and potentials $a_\mu$ have dimension 1 , their field strength has dimension $2$ , fermions like the dirac $\psi$ have dimension 3/2 , and all terms in the lagrangian density have dimension $4$ . these are classical dimensions ; each derivative adds $1$ to the dimension . products of these operators have dimensions that are sums of the dimensions of the factors - except that quantum mechanics also adds " anomalous dimensions " to this classical form of the dimension , so that the total dimensions may have fractional terms that are proportional to powers of $g$ etc .
although unitarity is necessary for conservation of the norm , indeed there is some periodicity ingraved in its form . for time-invariant bounded systems ( the hamiltonian does not depend on time and has a discrete , or at least non-dense number of eigenstates ) there is always a recurrence time $\tau_r$ after which the state of the system is again the initial state ( or as close as you can measure it ) . the simpler quantum system that you can imagine , a two-level system , can be represented as a point on the surface of a sphere , moving at constant speed along a circle ( for time-invariant hamiltonians ) . it will obviously revisit the inital state periodically . more complex systems ( but bounded and time-invariant ) described by n-level hamiltonians also revisit the initial state following richer dynamics . you can google for information under the name of revival ( and partial revival ) times . if you think carefully , there is nothing surprising about it . similar phenomena is also found in classical physics under the name of poincar recurrence time . stable systems tend to be periodic or quasi-periodic . this has helped their study and the birth of physics since the early days . however , as the complexity of the system increases , or in other words , as the number of eigenstates that participate in the dynamics is larger , so it will be the recurrence time , which eventually may be larger than the time related to the inverse of the precision of the hamiltonian eigenstates , or in other words , larger than the time disposed to follow or measured the system . in fact for most physical systems the recurrence time is clearly unphysical . this is also reflected in the fact that for increasingly complex systems it becomes harder to guarantee that they are discrete and time-invariant , or closed , or isolated .
watch a boat through binoculars as it sails away from you . much easier .
you are always allowed to introduce a new integration variable as long as its not its already being summed over . this might be more clear in discrete form : \begin{align} \int d x \ , f ( x ) and \rightarrow \delta x\sum _i \ , f ( x _i ) \\ and = \big ( n \delta y\sum _j g ( y _j ) \big ) \delta x \sum _i f ( x _i ) \\ \end{align} where $ n \delta y\sum _j g ( y _j ) = 1 $ ( note that it is very important this does not depend on $x$ ) . then , \begin{align} \int d x \ , f ( x ) and \rightarrow n \delta x \delta y \sum _{i , j} f ( x _i ) g ( y _j ) \end{align} the only difference with the fadeev poppov procedure is that now $ g ( y ) $ is also a function of a new unphysical parameter , $\xi$ . in order to not change the value of the integral over $ f ( x ) $ , the constant $ n $ needs to also change with $ \xi $ .
the stress tensor may be continuous rather than due to a discrete number of forces e.g. in fluid mechanics . for example the cauchy momentum equation is naturally written using the divergence of the stress tensor .
short answer you have hit upon the quirk that the si and cgs systems not only measure electric charge with different units , but also assign them different dimensionality . in si , the ampere is a base unit . amperes are not made out of anything else - they are primitive , like meters , kilograms , and seconds . one ampere is one coulomb per second , so the unit of electric charge , the coulomb , is equal to one ampere-second . when there is an equation that has amperes or coulombs on one side , and something without those units ( like force ) on the other side , the equation will always have a constant that has the correct units to balance things out . that job is done by $\mu_0$ and $\epsilon_0$ . in cgs , charge is measured in esu , but this is a derived unit . charge is considered to be made up out of length , mass , and time the way , for example , angular momentum is . charge has dimensions $$ [ m ] ^{1/2} [ l ] ^{3/2} [ s ] ^{-1}$$ and one esu is equal to the square root of a $\text{dyne-cm}^2$ . longer answer in si , the we start with meters , kilograms , and seconds . then we set $\mu_0$ to be $4\pi\times 10^{-7}\textrm{ohm-sec/m}$ . now take two wires very long compared to the distance between them , and run the same current through them . make the distance between them $d$ . a section of length $l$ of the wires will feel a force of attraction $f$ , which depends on the square of the current . the ampere is defined such that when the current is measured in amperes , we find that $$f = \frac{\mu_0 i^2 l}{2\pi d}$$ when $f$ is measured in newtons , $d$ and $l$ in meters . this definition requires that you can create the same current in each wire , that you can accurately measure the force per unit length , and that the force is perfectly proportional to the square of the current . in truth it will not be because the wires are not infinitely long and perfectly straight and parallel , but some equivalent operational definition might be used in practice . ( the fact that the definition is at all possible is a test of the physical hypothesis of the proportionality . ) the important point is that the ampere becomes a new basic unit . ( technically , what is been said so far would not define amperes , but give us a relationship between amperes and ohms . we could hash that out by looking at the units of $v=ir$ , for example ) . the coulomb used in coulomb 's law is then defined as one ampere-second . this would seem to allow us to experimentally measure $\epsilon_0$ because it is now the only unknown in coulomb 's law . originally , that was correct , but now we have defined the speed of light to be $c = 2.99792458 \times 10^8$meters/second , and so we are constrained by $c = 1/\sqrt{\mu_0\epsilon_0}$ . this forces $\epsilon_0$ to be $$\epsilon_0 = \frac{1}{4\pi\times 8.9875517853681764}\frac{\textrm{sec}}{\textrm{ohm-m}}$$ , meaning that we could also define the coulomb directly from coulomb 's law - take two charged bodies , measure the force between them , and define the coulomb to be the unit of charge such that $$f = \frac{q_1q_2}{4\pi\epsilon_0 r^2}$$ with $r$ in meters and $f$ in newtons . the experimental observation that these two definitions of the coulomb ( one as the ampere-second and one directly from coulomb 's law ) agree then becomes a test of the physical theory of electromagnetism . in cgs units , charge is measured in esu , a derived unit defined by coulomb 's law as you have written it . one esu is the charge such that two charged bodies feel the coulomb force $$f = \frac{q_1q_2}{r^2}$$ with $f$ in dynes and $r$ in centimeters . if both charges are one esu , this gives $$1\text{ }\textrm{dyne} = \frac{1\text{ }\textrm{esu}^2}{1\text{ }\textrm{cm}^2}$$ or $$1\text{ }\textrm{esu} = 1\text{ }\textrm{cm}\sqrt{\textrm{dyne}}$$ finally , since the dyne is a derived unit , this could be written in terms of base units as $$1\text{ }\textrm{esu} = \sqrt{\frac{\textrm{g-cm}^3}{\textrm{s}^2}}$$ as a consequence , there is one less base unit involved in electromagnetic formulas when using cgs . also , there is no need for the constant $\epsilon_0$ used in si . there is also no need for $\mu_0$ . instead , things like maxwell 's equation explicitly include the speed of light $c$ . you can see a side-by-side comparison of basic equations of electromagnetism in si and cgs units here . converting cgs centimeters and grams to si meters and kilograms , then equating the two expressions for the force in coulomb 's law , we find the conversion that one coulomb is the same amount of charge as $2.99792458 \times 10^9$esu . reference this answer is a summary of an appendix to purcell 's electricity and magnetism
original newton idea was that $$\vec{f} = m \cdot \vec{a}$$ meaning in inertial frame of reference ( net or total ) force equals product of mass of body and its acceleration . indeed , as you mentioned similar expression appears in non-inertial frame of reference $$\vec{f&#39 ; } = - m \cdot \vec{a&#39 ; }$$ meaning in non-inertial frame of reference you have to add pseudo or fictitious or inertial force $\vec{f&#39 ; }$ if you wish to use $\vec{f} = m \cdot \vec{a}$ . $\vec{a&#39 ; }$ in second expression represents the acceleration of the non-inertial frame of reference and not the acceleration of the mass . also , note minus sign in the second expression , which means that pseudo force is directed in the opposite direction to acceleration of the frame of reference ! by the way , non-inertial frame of reference is usually conveniently chosen in a way that observed body is at rest ( position of the body defines that frame of reference ) , so you end up with $\vec{a} = 0$ .
consider the following diagram : this shows a mass $m$ moving past a point $p$ in a straight line . note that the mass is not connected to $p$ in any way - it is just moving past in a straight line . the angular momentum of $m$ about $p$ is given by : $$ \vec{l} = \vec{r} \times m\vec{v} $$ so the direction of $\vec{l}$ is normal to the screen and the magnitude is : $$ l = rmv \sin\theta \tag{1} $$ and since : $$ \sin\theta = \frac{d}{r} $$ substituting this into equation ( 1 ) gives : $$ l = mvd \tag{2} $$ note that everything in equation ( 2 ) is a constant , so that tells us that $l$ is a constant as well , so angular momentum is conserved even though at first glance this is not a system that is rotating . and this provides the answer to your question . even though in your second example ball $b$ is not attached to anything it still has an angular momentum and this angular momentum is still conserved . response to comment : your distinction between angular and linear momentum is an artificial one . if you look at my working above the particle obviously has a linear momentum , but also an angular momentum . what is more the value of the angular momentum depends on where you fix the point $p$ so there is no unique value of angular momentum . when you have a ball on a rotating rod the direction of linear momentum is not conserved because there is a force acting ( through the rod ) and newton 's laws tells that force is the rate of change of momentum : $$ \vec{f} = \frac{d\vec{p}}{dt} $$ however there is a theorem ( noether 's theorem ) thats tell us angular momentum is conserved if the force is independant of angle . so if we calculate the angular momentum about the pivot point we will find that this quantity is a constant . that is why it is useful for calculating trajectories . but it is incorrect to say that the linear momentum of the ball attached to the rod is zero . at any time the linear momentum of the ball $a$ is $\vec{p} = m\vec{v}$ , but the direction of $\vec{p}$ ( though not its magnitude ) is changing continuously with time due to the force applied by the connecting rod . back to your problem : before the collision the linear momentum of $a$ is mv , but it is direction is continually changing with time . however the angular momentum is constant because the force on $a$ is central . during the collision a force acts between $a$ and $b$ . this force acts normally to the connecting rods , i.e. it is not a central force , so the angular momenta will not be constant . if we consider the collision to take an instant then the only force acting is the one between the two balls so the total linear momentum will be conserved . before the collision $p_a = mv$ and $l_b = 0$ . after the collision $p_a = 0$ and $p_b = mv$ , so the total linear momentum $p_a + p_b$ is conserved . immediately after the collision $b$ starts rotating about its pivot due to the force applied by its connecting rod . the force applied by its rod means the direction of $p_b$ now changes with time , though its magnitude does not . the angular momentum $l_b$ is constant because the force applied by the rod is central around $b$ . note that it still makes perfect physical sense to calculate $l_a$ - it is just : $$ l_a = \vec{r}_a \times m\vec{v_b} $$ but the force applied to $b$ by its rod is not centrally symmetric about $a$ , so $l_a$ is not constant ( and therefore not terribly useful ) . if $b$ is not connected to a pivot , so it moves off in a straight line , then no forces act on $b$ . that means both $\vec{p}_b$ and $\vec{l}_b$ calculated about any point are constant .
no , he does not think that gr is wrong . actually , in his article on arxiv , he gives some ideas in order to ( try to ) solve a problem associated to the information paradox for evaporating black holes . that would mean that the usual idea of black hole we have would not longer be accurate . it does not imply that gr is wrong ( at least on the domain of validity where we believe it provides a pretty good description of physics ) .
there is no fundamental difference between the gluons $ ( r\bar{r}-b\bar{b} ) /\sqrt{2}$ and $ ( r\bar{b} + b\bar{r} ) /\sqrt{2}$ . the first one is represented by the matrix $$ z = \frac{1}{\sqrt{2}}\left ( \begin{array}{rrr} 1 and 0 and 0 \\ 0 and -1 and 0 \\ 0 and 0 and 0\end{array}\right ) $$ and the second by the matrix $$ x = \frac{1}{\sqrt{2}}\left ( \begin{array}{rrr} 0 and 1 and 0 \\ 1 and 0 and 0 \\ 0 and 0 and 0\end{array}\right ) . $$ however , these two matrices are related by the change of basis $$ h = \left ( \begin{array}{rrr}1/\sqrt{2} and 1/\sqrt{2} and 0 \\ 1/\sqrt{2} and -1/\sqrt{2} and 0 \\ 0 and 0 and 1\end{array}\right ) . $$ it is easy to check this by multiplying matrices and seeing that $hzh^\dagger = x$ . thus , if you call $ ( r\bar{r}-b\bar{b} ) /\sqrt{2}$ " color-neutral " and $ ( r\bar{b} + b\bar{r} ) /\sqrt{2}$ " non-color-neutral " , it is clear that " color-neutral " is not a property that is invariant under change of basis , and thus is not a meaningful property in quantum chromodynamics . actually , neither of these gluons is color-neutral .
this is more like a maths question to me . this is just an identity , which is true and facilitates the calculation and it is valid for any vector field . the proof , using einstein summation convention would be something like : $$ ( \nabla \times \vec u ) \times \vec u = \epsilon_{ijk} ( \nabla \times u ) _j u_k = \\ \epsilon_{ijk}\epsilon_{jlm}\partial_l ( u_m ) u_k = \epsilon_{jki}\epsilon_{jlm}\partial_l ( u_m ) u_k = \\ ( \delta_{kl}\delta_{im} - \delta_{km}\delta_{il} ) \partial_l ( u_m ) u_k = \\ \partial_k ( u_i ) u_k - \partial_i ( u_k ) u_k = ( \vec u \cdot \vec \nabla ) \vec u - \frac 1 2 \nabla ( \vec u \cdot \vec u ) $$ i hope that helps .
the publishers of the feynman lectures recently released a free online edition ! see this link . this should prove an invaluable resource for physics students . one note for feynman lectures purists like myself : the content is of course the same as the original books , but the look and feel of this edition is very different from the original . the font has been changed for easier online readability , the equations have been retyped in latex , and the diagrams have been redrawn .
it is not a sufficient explanation . there are asymptotically free theories which are not strongly coupled in the ir . the rate at which the coupling gets strong is important . in qcd , it seems to get strong very quickly near the confinement scale , so that beyond a certain scale , you only see hadrons . it is not really understood how this works . the perturbation theory suggests it could happen , as you have observed , but one can not trust perturbation theory at strong coupling .
although you might not like to hear it , the answer really does lie in the definition of $\mu_0$ ( and $c$ ) . $\mu_0$ is defined to be exactly $4\pi *10^{-7}\ \text{h m}^{-1}$ . similarly , $c$ is defined as exactly $299792458\ \text{ms}^{-1}$ . it immediately follows from the relation $$\epsilon_0=\frac{1}{\mu_0 c^2}$$ that $\epsilon_0$ also has no uncertainty . maybe you do not like this because it makes explicit reference to a concept from magnetism , and you would like to see a formulation of electric effects that is separated from magnetic effects . such a thing is simply not possible , since a simple change of reference frame can turn an electric effect into a magnetic one or vice versa . electromagnetism really is a single unified framework . there is also no circularity in this argument , as far as i can tell .
this heat or energy given by the fork is given to the next layer of air and in this way , this energy makes sound . this is the crux of the misunderstanding ; sound energy is not transmitted by heat , but rather by the concerted kinetic energy of the gas movement . it is adiabatic because the heat has nowhere to go other than the gas itself . it is not isothermal because during compression , the temperature increases , and vice versa . response to comment : from which energy is heat energy evolved during compression of gas ? acc . to you , the kinetic energy of the molecules that makes the sound comes from the fork . where does the heat energy go during expansion ? i think the heat energy comes from the work of the fork on the gas during compression . but the energy goes as kinetic energy of the molecule . confused yes , heat is evolved during the compression . during expansion , the heat energy is converted into potential energy of the expanded gas . energy cycles between heat and potential energy , similar to this slow motion video of a shotgun primer firing underwater . during the compression stages , the bubble heats up , and during the expansion stages , the bubble cools down . the energy is still there in both the expanded and contracted bubbles , just in different forms . however , in this case , the adiabatic assumption is slightly incorrect , and the bubble slowly loses energy after each oscillation , causing the bubble oscillations to be damped after about 5 oscillations . here is a simple derivation of the speed of sound ; the key fact is to note that the speed is determined by $dp/d\rho$ , which in some sense represents the " stiffness " of a gas . they use the adiabaticity of the process to show $c=\sqrt{\gamma rt}$ , but do not totally explain why adiabaticity is used . once you understand that page , read this one , which is a more complicated version which explains why the adiabaticity is used . in particular , newton assumed boyle 's law held in his derivation of wave propagation in gases , but this is actually false , as explained in the paragraph which starts with " the flaw in this reasoning is the assumption that changes in pressure of an acoustic disturbance are exactly proportional to the changes in density " . but basically , the idea is that heat has nowhere to go except to heat up the gas .
considering all the possible low-order corrections to newtonian motion that there might be to a particle moving in a gravitation field of any kind , or rather , in comparing relative motions of neighboring particles , there are defined some paramaters according to the parameterized post newtonian ( ppn ) formalism . this is described nicely in the famous book by misner thorne and wheeler . ( my other three books on gtr do not mention ppn . ) gtr can be analyzed in terms of ppn parameters - beta=1 , eta=0 , etc . alternative theories work out to having different ppn values . any experimental observations , after being calibrated and curves fitted to and so on , can be judged in terms of ppn parameters , at least some of them , or combinations of some of them . so it is a matter of seeing what fits within error bars . http://en.wikipedia.org/wiki/parameterized_post-newtonian_formalism tells more detail .
any charged particle creates an electric field . if that particle accelerates then the electric field changes . this change in the electric field moves through space at the speed of light and we call this change , or disturbance , a photon . there is much more detail than i gave , but given your question i think you will find the answer suitable . leave a comment if you have any follow-up questions .
the definition of torque is force times lever arm . as you state it , force and lever arm are identical , so is the torque . there are differences . because the moment of inertia differs , the applied torque will cause a different angular velocity ( if the force is not in line with the centre of mass ) .
maxwell 's equations in curved spacetime are written in the form $$\begin{split}\nabla_a f^{ab} and = - 4\pi j^b , \\ \nabla_{ [ a} f_{bc ] } and = 0 , \end{split}$$ with $f$ the faraday two-form , $j^a$ the current four-vector , $\nabla$ the covariant derivative and $ [ ] $ denotes antisymmetrization of the indices . in terms of exterior calculus they become : $$ \begin{split} d\star f and = 4\pi \star j\\ df and =0 , \end{split}$$ with $\star$ the hodge dual , which sends p-forms to $4-p$-forms in dimension 4 . if we integrate the left side of the first equation over a space-like hypersurface of dimension 3 , $\sigma$ , with normal time-like vector $t^a$ , then stokes ' theorem yields $$\int_\sigma d\star f = \int_s \star f , $$ with $s$ the boundary of $\sigma$ with normal $n_a$ . since $\sigma$ is space-like and $ ( \star f ) _{cd} = \frac{1}{2} f^{ab} \epsilon_{abcd}$ , $s$ is also space-like and one component of $f$ must be time-like , therefore $\star f = f^{ab} t_a n_b ds$ . this is easy to see also if one takes the restriction of the dual on $s$ in local coordinates , all the 2-forms are space-like . but we know that $e_a = f_{ab} t^b$ , hence $$\int_s \star f = -\int_s f^{ba} t_a n_b d s = -\int_s e_b n^b d s . $$ now we integrate the right side , $$\int_\sigma \star j = \int_\sigma j^a \epsilon_{abcd}=\int_\sigma j^a t_a d \sigma = -q , $$ and after combining this and the previous one , we obtain : $$\int_s e_a n^a d s = 4\pi q , $$ gauss 's law applies also in curved spacetime . note that $\epsilon_{abcd}$ is the levi-civita ( volume ) tensor , not the symbol . in local coordinates its components are the product of the symbol with $\sqrt{|\det{g_{\mu\nu}}|}$ . for the case of the magnetic field , $b_a = - \frac{1}{2} \epsilon_{abcd} f^{cd} t^b$ , only the space-like components of $f_{ab}$ are used , and the magnetic field part of the faraday tensor is $$f = f_{12} dx^1\wedge dx^2 + f_{23} dx^2\wedge dx^3 + f_{31} dx^3\wedge dx^1 , $$ and the components of the field are $b^1 = - |\det g_{\mu\nu}|^{-1/2} f_{23}$ etc , therefore the integrals become $$0=\int_s f = -\int_s \sqrt{|\det g_{\mu\nu}|} ( b^1 dx^2\wedge dx^3 + b^2 dx^3\wedge dx^1 + b^3 dx^1\wedge dx^2 ) = - \int_s b^a n_a ds . $$
that is a complicated question because different frequency waves are absorbed/scattered by different mechanisms and in different media . have a look at http://en.wikipedia.org/wiki/mass_attenuation_coefficient as this contains some introductory information . actually that article describes x-ray absorption in some detail but glosses over light and radio waves . you specifically mention radio and low frequency waves and http://en.wikipedia.org/wiki/radio_propagation gives some discussion of these . low fequency em is generally too low energy to be absorbed by exciting molecular transitions and instead it is absorbed by interaction with the electrons in whatever it is passing though . generally speaking the more conducting the media the faster the radio/vlf waves are absorbed .
in landau 's theory , the order parameter $m$ should make $g ( m ) $ minimal . $$\frac{\partial g}{\partial m} = 2 b ( t ) m + 4 c ( t ) m^3=0$$ $$\frac{\partial^2 g}{\partial m^2} = 2 b ( t ) + 12 c ( t ) m^2 &gt ; 0$$ hence , $m=0$ or $m = \pm m_0 = \pm \sqrt{ - \frac{b ( t ) }{2 c ( t ) }}$ when $t$ is higher than the critical point $t_c$ , the groud state of system satisfies $m=0$ . hence , $m=0$ is the minimal point , and $\frac{\partial^2 g}{\partial m^2} |_{m=0} = 2 b ( t ) &gt ; 0$ , or $b ( t ) |_{t&gt ; t_c}&gt ; 0$ . when $t$ is lower than $t_c$ , the groud state of system satisfies $m=m_0$ . $$\frac{\partial^2 g}{\partial m^2} |_{m=\pm m_0} = 2 b ( t ) + 12 c ( t ) ( -\frac{b ( t ) }{2 c ( t ) } ) = - 4 b ( t ) &gt ; 0$$ or $b ( t ) |_{t&lt ; t_c}&lt ; 0$ . and $c ( t ) |_{t&lt ; t_c}&gt ; 0$ due to $m_0 = \sqrt{ - \frac{b ( t ) }{2 c ( t ) }}$ . now , here is a short summary , $t&gt ; t_c$ , $b ( t ) &gt ; 0$ , and $t&lt ; t_c$ , $b ( t ) &lt ; 0$ , $c ( t ) &gt ; 0$ . the simplest assumptions are , by taylor 's expansion , $b ( t ) =b ( t-t_c ) $ and $c ( t ) =c$ .
systems of plates are not typically considered capacitors unless they are globally neutral . nevertheless , capacitance is a geometric property that is to do with the system more than the actual voltages and charges you apply to it , so that your question still makes sense : the capacitance is the same as it would be with symmetric charges . more specifically , the ( mutual ) capacitance of two conducting surfaces is defined as the charge that must be applied to one surface so that the potential in the other one will rise by one unit ; by energy considerations it must be symmetric . if the charges on both surfaces are antisymmetric ( i.e. . $+q$ and $-q$ ) then there will be a potential difference between the plates of $v=q/c$ . if they are asymmetric , a similar statement holds : if plate 1 has charge $q_1$ and plate 2 has charge $q_2$ , then there will still be a potential difference between them of $v= ( q_1-q_2 ) /2c$ . the problem with this , though , is that you are no longer seeing the whole picture , and you will also have to deal with the self capacitance of the plates , which was not a problem before : if you put 100 c of charge on one plate and 99 c on the other , there will still be some potential difference between the plates , but they are also at a very high potential with respect to anything else you might consider , and you will have a host of other problems . this is why the situation is hardly ever considered . in the general case , then , you have not one but two independent voltages to consider ; that is , the mean and the difference , or the two voltages of the plates separately . to deal with this appropriately , you need to use a general capacitance matrix as suresh describes in his answer . finally , you also need to worry about what other charged systems you must consider , and where they are . you can not just conjure a large charge on one plate without taking it from somewhere , and depending on where you are grounding there may also be some significant energy of interaction there .
interestingly enough , aristotle proposed that $\vec f=m\vec v$ , for , by intuition , it is reasonable to assume that how hard something hits you depends upon how heavy something is , and how hard something is going . however , this is because of newton 's first law , inertia , which is analogous to momentum ( $\vec p=m\vec v$ ) . yet , mass does have to have some effect on it , for it is not the same to have a truck hit you than to have a bee hit you . thus , we reach the conclusion that a push or pull ( a force ) must be affected by the mass , and some factor dependent upon the derivatives of displacement with respect to time . yet , what makes something move is not the speed with which it is hit , as we have discussed , but the change in speed , that is to say , the acceleration . hence , $\vec f=m\vec a$ . i understand this is not the most theoretical derivation of a formula , it is rather an intuitive one , and it is very poorly written , for i am writing it on my phone and cannot even use latex . so please , do not hesitate to ask if you have any other questions or want a more theoretical approach . hope it helped !
this is interesting . you would definitely have to ' nail down ' the magnets to the sphere , because it will be an unstable configuration . also in the real world , edge-effects will destroy any chance of perfect radial field lines , so let 's assume we are in an ideal scenario . outside the sphere , the magnetic field would be that of a source monopole placed at the sphere 's center . but we need $\nabla\cdot b=0$ , so as a result there is no b-field on the outside . inside the sphere , there is nowhere the magnetic field lines can end , especially when they are all pointed towards the center . . . in fact , such a magnetic field would have divergence less than zero ( the center of the sphere being a ' sink' ) , and this is a property that magnetic fields cannot have ( since $\nabla\cdot b=0$ ) . as a result , my answer is that there is no $b$-field on the inside either . the real reason the b-field must have zero divergence : if there are no physical source monopoles in the vicinity , then any configuration is made of dipoles , and there is no way mathematically ( i think ) for a collection of dipoles to produce a monopole .
for the history on the unit of time have a look at this link . the second is the duration of 9 192 631 770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the cesium 133 atom . in the same click on meter to get the definition of a meter . have a look at the paragraph experimental measurements of the elementary charge . the oil drop experiment has been repeated in innumerable under graduate lab courses . the kilogram is the unit of mass ; it is equal to the mass of the international prototype of the kilogram once the charge of an electron is known , a simple experiment with a magnetic field and a measurement of the radius will give its mass . in this image of a bubble chamber materialization of an electron positron pair there is energy loss due to electromagnetic collisions of the electron . when the experiment is carried out in vacuum with electrons the mass can be measured . it is very seldom in physics experiments that two quantities can be measured at the same time . the measurements are done sequentially building logically on previous knowledge . here is an experiment measuring h . the experiment used a black body approach to experimentally measure plancks reduced constant , by using the planck radiation law , which relates the intensity and wavelength of light to the temperature of the emitting black body . the measurement was accomplished by using a simplified detection system involving a photodiode and diffraction grating to measure the intensity of emitted light as a function of the wavelength . from the experiment it was found that the value h_bar of was 2.81.5x10^-35 js .
the rule i mentioned in another question , that the time dilation factor is $1+\delta\phi/c^2$ , applies here . the derivation ( found in various textbooks ) depends only on the assumptions that fields are weak and matter is nonrelativistic , both of which are true for the earth . modeling the earth as a uniform-density sphere ( not true , of course , but i do not care ) , we find that $g ( r ) =gmr/r^3$ where $r$ is the radius of the earth . so $$ \delta\phi={gm\over r^3}\int_0^rr\ , dr={gm\over 2r} . $$ that means that $$ {\delta\phi\over c^2}={gm\over 2rc^2}={1\over 4}{r_s\over r} . $$ here $r_s=2gm/c^2$ is the schwarzschild radius corresponding to the earth 's mass . numerically , $r_s$ is about 9 mm , and $r$ is about 6400 km , so $\delta\phi/c^2=3\times 10^{-10}$ . the sign of the effect is that clocks tick slower when they are deeper in the potential well . that is , a clock at the earth 's surface ticks 1.0000000003 times faster than one at the center .
the equivalence principle tells us that to some one sitting in the train , it looks like the acceleration due to gravity has a new term which points opposite to the direction of acceleration of the train . this new gravity force would cause the butterfly to accelerate to the back of the train until it reaches its terminal velocity ( i think the terminal velocity would be reached quickly ) . notice that there will also be a buoyant force acting on the butterfly which points to the front of the train . in fact if the butterfly were instead a balloon , so that it was less dense than the air , then the net horizontal force would be in the direction of acceleration .
well in principle , it is pretty easy . the example of the sun is often taught in introductory astronomy laboratory courses . the sun is a nice clean example because it is so bright compared to anything that might contaminate its spectrum . you just put some sunlight through a diffraction grating ( or slit , or prism ) that you have previously calibrated so that you can measure the wavelength of each line , then compare the resulting absorption spectrum to the spectra of various elements , which you could look up in a table or measure from clean samples in your lab . schematically you do something like this : from there it is just a simple matter of matching up solar spectrum lines with known element lines - usually starting with the most obvious ones . in this case the one that stands out the most is the sodium ( na ) doublet . not every line from a particular element may be visible - the conditions on the sun are such that some electronic transitions ( corresponding to particular lines ) just do not happen often enough to produce a visible line . things get more complicated for fainter stars , because their spectrum is easily contaminated . for instance , there are both emission and absorption features in the earth 's atmosphere which will be measured simultaneously with a stellar spectrum when using a ground-based telescope . fortunately , people have gone to great lengths to measure and model the atmospheric lines so that they can be removed as cleanly as possible from a stellar spectrum . another complication is red/blueshift . the wavelength at which a given line occurs is fixed , but the detected position of the line can change if the source is moving ( doppler shift ) , is in a strong gravitational field ( gravitational redshift , such as near a black hole ) , or is in an expanding universe ( cosmological redshift ) . fortunately all lines from a source are shifted by the same factor - for instance a source with a cosmological redshift of 1 would have the measured wavelengths of all its lines doubled from what you would measure in the lab ( redshift $z=\lambda_{\rm obs}/\lambda_{\rm emit}-1$ ) . because the change is uniform for all lines , we can still use the spacing between the lines to figure out which one is which . this still involves a bit of guesswork , usually you need to start with a strong line and make an educated guess about what physical process is causing it , then start trying to identify more lines consistently with your guess . finally , i happen to have an example of a spectrum lying around ( it is from this paper ) . in this case the source is moving at more than $-1000 \rm km/s$ relative to the telescope , so there is a slight doppler shift ( about . 3% change in wavelength ) . the source is close enough that cosmological redshift is unimportant . here there are a number of absorption lines . the most prominent are some of the hydrogen balmer series ( $n\rightarrow2$ transitions ) , $\rm h\alpha$ , $\rm h\beta$ , $\rm h\gamma$ , $\rm h\delta$ , $\rm h\epsilon$ . these are a good starting point because if one in the series appears , often others do as well , and they are all in a part of the spectrum that is easy to observe from earth ( not blocked by the atmosphere ) . there is also the $\rm h$ and $\rm k$ lines of singly ionized $\rm ca$ . the g-band is actually a set of closely spaced absorption lines of the $\rm ch$ molecule . finally , the lines marked $\oplus$ are emission lines from the earth 's atmosphere . most of the atmospheric lines were removed from this spectrum , but these left some residuals because the atmospheric model was imperfect . the paper shows the spectrum , but does not mark the individual lines ( though it does say which lines are present ) . i was giving a talk and wanted to label the individual lines , so i went through trying to identify them all . initially i made a couple of wrong guesse , but once i figured out a couple of the balmer lines the rest fell into place .
the efficiency of large hydroelectric generators can be very high - up to 95% in ideal cases - however the efficiency of small installations is a lot lower and in particular it is hard to get efficient electricity generation if the flow rate is low , which is likely to be the case for your friend . but lets see what the potential is . you do not say what the water flow rate is , so let 's just call it $f$ kg/sec . 12 feet is about 3.7m so the power is : $$ p = fgh \approx 36f \space\text{watts} $$ let 's guesstimate the efficiency of electricity generation at 10% , then the electrical power your friend can generate is : $$ p \approx 3.6f \space\text{watts} $$ i would guess most houses use a few kw of electricity . if we take 1kw and work out what flow rate is needed to produce this we get : $$ f \approx \frac{p}{3.6} \approx 280 \text{kg}/\text{sec} \approx 0.28 \text{m}^3/\text{sec} $$ i would guess this flow rate is more than the typical stream , but it is not all that great .
the only way that a change of interpretation can effect quantum computing is due to the different mathematical representation of the dynamics ( which , strictly speaking , is not even part of the interpretation ) . there may be advantages to the " programming " of quantum computers by going into a pilot-wave representation . other than that " interpretations " and " ontologies " have no consequences in physics . some people feel more comfortable using a different language for the same phenomenon , and that is perfectly valid . in case of programming languages i prefer python for most of my work , even though i also " talk " c , c++ , vba , vhdl and verilog ( and can pick up the basics of pretty much any other language that i have to adopt for whatever reason ) . all of these languages are essentially describing the same computational domain . for the context of quantum computing the basic representations of qm ( including all of its interpretations ) are all low level assembly languages . to the best of my knowledge there is , so far , little progress in defining a formalism that allows non-experts to do quantum computations . that , however , is one of the major theoretical hurdles for quantum computing : how do we let ordinary programmers write software for these systems ? after all , programmers of classical computers are not required to be computer architects and to write their own compilers either . if they had to , classical computing would still be in the stone ages of the 1950s .
i can only recommend textbooks because that is what i have used , but here are some suggestions : gravity : an introduction to general relativity by james hartle is reasonably good as an introduction , although in order to make the content accessible , he does skip over a lot of mathematical detail . for your purposes , you might consider reading the first few chapters just to get the " big picture " if you find other books to be a bit too much at first . a first course in general relativity by bernard schutz is one that i have heard similar things about , but i have not read it myself . spacetime and geometry : an introduction to general relativity by sean carroll is one that i have used a bit , and which goes into a slightly higher level of mathematical detail than hartle . it introduces the basics of differential geometry and uses them to discuss the formulation of tensors , connections , and the metric ( and then of course it goes on into the theory itself and applications ) . it is based on these notes which are available for free . general relativity by robert m . wald is a classic , though i am a little embarrassed to admit that i have not read much of it . from what i know , though , there is certainly no shortage of mathematical detail , and it derives/explains certain principles in different ways from other books , so it can either be a good reference on its own ( if you are up for the detail ) or a good companion to whatever else you are reading . however it was published back in 1984 and thus does not cover a lot of recent developments , e.g. the accelerating expansion of the universe , cosmic censorship , various results in semiclassical gravity and numerical relativity , and so on . gravitation by charles misner , kip thorne , and john wheeler , is pretty much the authoritative reference on general relativity ( to the extent that one exists ) . it discusses many aspects and applications of the theory in far more mathematical and logical detail than any other book i have seen . ( consequently , it is very thick . ) i would recommend having a copy of this around as a reference to go to about specific topics , when you have questions about the explanations in other books , but it is not the kind of thing you had sit down and read large chunks of at once . it is also worth noting that this dates back to 1973 , so it is out of date in the same ways as wald 's book ( and more ) . gravitation and cosmology : principles and applications of the general theory of relativity by steven weinberg is another one that i have read a bit of . honestly i find it a bit hard to follow - just like some of weinberg 's other books , actually - since he gets into such detailed explanations , and it is easy to get bogged down in trying to understand the details and forget about the main point of the argument . still , this might be another one to go to if you are wondering about the details omitted by other books . this is not as comprehensive as the misner/thorne/wheeler book , though . a relativist 's toolkit : the mathematics of black-hole mechanics by eric poisson is a bit beyond the purely introductory level , but it does provide practical guidance on doing certain calculations which is missing from a lot of other books .
the statement that photons are massless means that photons do not have rest mass . in particular , this means that , in units where $c=1$ , the magnitude of the photon 3-momentum must be equal to the total energy of the photons , rather than the standard relationship where $m^{2} = e^{2}-p^{2}$ . but , you can create multi-photon systems where the net momentum is zero , since momentum adds as a vector . when you do this , however , since the energy of a non-bound state is always non-negative , the energies just add . so , this system looks just like the rest frame of a massive particle , which has energy associated with its mass and nothing else . the statement about gravity is a little bit more subtle , but all photon states will interact with the gravitational field , thanks to the positive results of the light-bending observations that have been made over the past century . so you do not even need a construction like this to get photons " falling " in a gravitational field .
it depends on the domain of $p$ . if we take the domain of $p$ to be the schwartz space on $\mathbb{r}$ , then , by symmetry of $p$ , $$ \langle p^2\rangle =\langle p\psi |p\psi \rangle =\left\| p\psi \right\| ^2 $$ this is $0$ iff $p\psi =0$ iff $\psi$ is constant . however , the only constant schwartz function is $0$ . hence , $p^2$ is positive-definite . this will work for any domain in which the only constant function is the $0$ function . by the way , this is just one manifestation of the fact that , for unbounded operators , the domain is a crucial part of the definition of the operator .
yes , it is possible to have pockets of air underwater as long as there is something there to contain the pocket . you can easily demonstrate this by turning a cup upside-down and submerging it in water . if you put a napkin in the bottom of the cup before you do this , the napkin stays dry . if you do the above experiment , but dive down in a swimming pool with the cup , you will notice that the air pocket decreases in size as you dive , and returns to normal size as you surface . this indicates that no air is being lost . instead , the air is being compressed . these observations indicate that the air pocket does fill part way up with water , but does not disappear . this is because as the air is submerged , the water pressure on it increases . this shrinks the air pocket 's volume until the pressure in the air is the same as the pressure in the water surrounding the air . yes , you can breathe air in an air pocket - it is normal air . however , you will exhaust the oxygen supply quickly if the air pocket is small .
what is confusing you seems to be the rope-pulley component itself . when you have a set-up like this , it is equivalent to draw it without the pulley . imagine instead that the same masses were attached to each other via a massless horizontal adamantium bar . then imagine there is a force pulling $m_1$ to the left by $m_1g$ and pulling $m_2$ to the right by $m_2g$ . now how does the system react ? clearly , this would be the same as if the two masses were combined and you had the same forces : $$f_{net}= ( m_2-m_1 ) g\\ a=\frac{m_2-m_1}{m_2+m_1}g$$ when using the rope and pulley , tension does the same thing . to keep the two masses the same distance apart and to uphold newton 's third law , the rope must apply the same tension force on both ends . let 's get that force from the horizontal bar model : we know the net acceleration of the system , which means the left side of the bar must pull $m_1$ with a force strong enough to overcome the leftward gravity and accelerate it rightwards $$f_{net}=t-f_g\rightarrow t=m_1g+m_1a$$ we also know the right side of the bar must apply a force to the left to cancel the rightward force on $m_2$ just enough so it accelerates at $a$ $$f_{net}=f_g-t\rightarrow t=m_2g-m_2a$$ plug in the value for $a$ we found above and lo you find the t 's are equal ! not surprising since that was a necessary outcome . but this is the same as the atwood machine . note though , that the tensions are not equal to the force of gravity on each mass because the system is accelerating . in this case , the true scope of newton 's third law is upheld because while the masses accelerate toward earth , earth imperceptibly accelerates towards the masses .
there is a commonly used analogy for electric circuits called the hydraulic analogy . this imagines the electrons as water and the wires as pipes . the voltage is equivalent to the water pressure and the current is equivalent to the water flow rate . start with a dc current and imagine the water is doing work by flowing through a water wheel : this is all very straightforward : the water/current flows steadily through pipe/wire and turns the waterwheel/electric motor , which can then do work . but now suppose we pump the water from left to right , then reverse the pump and pump the water from right to left . this is exactly what an alternating voltage does . the water just moves to and fro within the pipe so there is no net flow . however the water can still move the waterwheel , but it moves it to and fro rather than at a steady rotation speed . we can still get the waterwheel to do work - if you are using it to grind corn it does not matter whether the wheel is rotating steadily or moving to and fro . the corn still gets ground . likewise , with an alternating voltage the electrons oscillate to and fro in the wire so there is no net flow of electrons . but the electrons can still do work e.g. heat a light bulb or run a ( suitably wired ) electric motor .
i believe one method is to search for antinuclei in cosmic rays emitted from an antimatter region of the universe . it is a bit tricky , however , to distinguish these from antimatter that is been indirectly produced ( e . g . from the interaction of " normal " cosmic rays with matter in the atmosphere ) . unfortunately , looking for antiphotons rather than photons does not help since the photon is its own antiparticle . however , there are some possibilities to look for the signature of photons that would be produced in large matter/antimatter annihilations ( see here for example ) . the paper also mentions tentative methods for using the photon polarization as an indicator of an antimatter source .
they are almost identical . if the incoming energy is the same then all you care about is how much of the input electrical energy can escape from the room - everything else will become heat . even the fan noise becomes heat when the sound energy hits the walls . the only part that potentially can escape is microwave emissions from the cpu clock . computers are just slightly more expensive than electrical heaters
yes , this is exactly what a van der graaff generator does . the metal sphere at the top accumulates a net charge ( actually i think the charge is generally positive in a vdgg but the principle still applies ) . the electrons in a conductor are delocalised into bands , so the conduction electrons are not localised onto specific atoms but rather their wavefunction spreads out throughout the conductor . adding a negative charge just means adding more electrons to the conduction band . these extra electrons are delocalised throughout the conductor just like the electrons already there . the extra electrons raise the energy , which is why if you earth the charged conductor the extra electrons flow off again .
you have to consider where the infalling matter has come from . a neutron star will normally form at the centre of a planetary system , and the whole system has a non-zero angular momentum not just the star . since the infalling matter is coming from the planetary system it will be rotating in the same direction as the star . in principle an interstellar dust cloud , or indeed anything from outside the planetary system , could hit the neutron star with a retrograde motion . however only in exceptional circumstances will this be the dominant source of infalling matter . a neutron star does have a tendancy to rotate matter in a prograde direction due to frame dragging . however this effect is only strong very near the event horizon of a rotating black hole . it will not have much effect on infalling matter until the matter gets very close .
since energy and speed depend on the reference frame you adopt , i will choose the center of mass ( or center of momentum ) frame to give a first idea of the problem . first of all let us consider two particles of proper mass $m_1$ and $m_2$ respectively . thus our equations are : $$ e_1 + e_2 = e_1'+e_2' , \\ p_1 = p_2 = p , \ \ p_1' = p_2' = p ' $$ where the un-primed quantities refer to the system before the collision , and the primed ones to the situation afterwards . $$ e_1 + e_2- e_1' =e_2'\\ ( e_1 + e_2- e_1' ) ^2 =e_2'^2\\ e_1^2 + e_2^2+ e_1'^2 + 2e_1e_2 -2e_1e_1' -2e_2e_1'=e_2'^2 $$ now , using the relativistic shell-mass relation $e^2 - p^2 = m^2$ ( where $c=1$ ) , we get $$ e_1^2 + e_2^2+ e_1'^2 + 2e_1e_2 -2e_1e_1' -2e_2e_1'=p'^2+m_2^2 = e_1'^2-m_1^2+m_2^2\\ e_1^2 +p^2 + 2e_1e_2 -2e_1e_1' -2e_2e_1' + m_1^2 = 0\\ 2e_1^2 + 2e_1e_2 -2e_1e_1' -2e_2e_1' = 0 $$ which is easily recognized as $e_1 = e_1'$ . this means there is no energy transfer at all , in the cm reference frame , but only momentum transfer , depending on the angle at which the particles are scattered : $$ |\mathbf{p}_1 - \mathbf{p}_1'| = \sqrt{p_1^2 + p_1'^2 - 2p_1p_1'\cos\theta} . $$
as with any phenomenon occuring in nature , we know it because we can measure it ( well , actually what we can measure are phenomena like quantum nonlocality and quantum steering , but both are only possible due to quantum entanglement , and therefore those measurements prove that there is quantum entanglement ) . the oldest example is the violation of bell 's inequality . bell 's inequality is an equality about expectation values of observables which holds for all separable ( that is , unentangled ) states , yet we can measure that real systems do violate bell 's inequality . states which violate bell 's inequality are called non-local because they are exactly the states which cannot be modelled with a local hidden variable theory ( that is actually the original purpose of bell 's inequality ) . another simple example of an effect only possible with entanglement is dense coding ( which also has been demonstrated experimentally ) . imagine alice wants to send bob a message composed of bits . however , all she can send to bob are systems which , when you measure them in any way , only give one of two results , and after that measurement , any further information which might have been in the sent system is gone . now it seems obvious that you can send at most one bit using such a system . however it turns out that if this is a quantum system ( a qubit ) , and actually part of a specific two-qubit state ( called bell state ) where bob already has the other qubit ( that is , the transmission of that qubit happened long before alice had the bits she wants to send ; indeed , it might even be that bob created the pair and sent one of the qubits to alice ) , then alice can encode two bits in this single qubit and send it to bob , and then bob can reliably read off both bits with the help of his part of the bell state qubit pair . again , that this can be experimentally demonstrated proves that entanglement exists .
indeed , nothing is wrong with noether theorem here , $j^\mu = f^{\mu \nu} \partial_\nu \lambda$ is a conserved current for every choice of the smooth scalar function $\lambda$ . it can be proved by direct inspection , since $$\partial_\mu j^\mu = \partial_\mu ( f^{\mu \nu} \partial_\nu \lambda ) = ( \partial_\mu f^{\mu \nu} ) \partial_\nu \lambda+ f^{\mu \nu} \partial_\mu\partial_\nu \lambda = 0 + 0 =0\: . $$ above , $\partial_\mu f^{\mu \nu}=0 $ due to field equations and $f^{\mu \nu} \partial_\mu\partial_\nu \lambda=0$ because $f^{\mu \nu}=-f^{\nu \mu}$ whereas $\partial_\mu\partial_\nu \lambda =\partial_\nu\partial_\mu \lambda$ . addendum . i show here that $j^\mu$ arises from the standard noether theorem . the relevant symmetry transformation , for every fixed $\lambda$ , is $$a_\mu \to a'_\mu = a_\mu + \epsilon \partial_\mu \lambda\: . $$ one immediately sees that $$\int_\omega {\cal l} ( a ' , \partial a' ) d^4x = \int_\omega {\cal l} ( a , \partial a ) d^4x\tag{0}$$ since even ${\cal l}$ is invariant . hence , $$\frac{d}{d\epsilon}|_{\epsilon=0} \int_\omega {\cal l} ( a , \partial a ) d^4x=0\: . \tag{1}$$ swapping the symbol of derivative and that of integral ( assuming $\omega$ bounded ) and exploiting euler-lagrange equations , ( 1 ) can be re-written as : $$\int_\omega \partial_\nu \left ( \frac{\partial {\cal l}}{\partial \partial_\nu a_\mu} \partial_\mu \lambda\right ) \: d^4 x =0\: . \tag{2}$$ since the integrand is continuous and $\omega$ arbitrary , ( 2 ) is equivalent to $$\partial_\nu \left ( \frac{\partial {\cal l}}{\partial \partial_\nu a_\mu} \partial_\mu \lambda\right ) =0\: , $$ which is the identity discussed by the op ( i omit a constant factor ) : $$\partial_\mu ( f^{\mu \nu} \partial_\nu \lambda ) =0\: . $$ addendum2 . the charge associated to any of these currents is related with the electrical flux at spatial infinity . indeed one has : $$q = \int_{t=t_0} j^0 d^3x = \int_{t=t_0} \sum_{i=1}^3 f^{0i}\partial_i \lambda d^3x = \int_{t=t_0} \partial_i\sum_{i=1}^3 f^{0i} \lambda d^3x - \int_{t=t_0} ( \sum_{i=1}^3 \partial_i f^{0i} ) \lambda d^3x \: . $$ as $\sum_{i=1}^3 \partial_i f^{0i} = -\partial_\mu f^{\mu 0}=0$ , the last integral does not give any contribution and we have $$q = \int_{t=t_0} \partial_i\left ( \lambda \sum_{i=1}^3 f^{0i} \right ) d^3x = \lim_{r\to +\infty}\oint_{t=t_0 , |\vec{x}| =r} \lambda \vec{e} \cdot \vec{n} \: ds\: . $$ if $\lambda$ becomes constant in space outside a bounded region $\omega_0$ and if , for instance , that constant does not vanish , $q$ is just the flux of $\vec{e}$ at infinity up to a constant factor . in this case $q$ is the electric charge up to a constant factor ( as stressed by ramanujan_dirac in a comment below ) . in that case , however , $q=0$ since we are dealing with the free em field .
yes , that is how physics is done ! aside from what i assume is a typo in your final summary , your equations ( 1 ) and ( 2 ) are both correct . you should note , however , that this is the newtonian way of answering your questions . real-life experiments will show some variation in time and distance traveled , a quicker slow-down time , and a shorter path . this is due to air resistance . you will need a more complex model if you want super-accurate answers , but these should work for rough estimations and low-level physics classes .
how does string theory differ from philosophy or religion ? as the field of mathematics , string theory differs from philosophy and religion not by its experimental verifiability but by its methods . it is mostly based on mathematical reasoning . it uses basically the same tools that are used in other areas of quantum physics . it has many points of contact with other areas of physics and mathematics . it is studied in physics and math departments . some of those working on string theories are also highly regarded in other areas of theoretical physics or mathematics . ( e . g . witten got the fields medal , the highest distinction in mathematics . ) some concepts first discussed in string theory found later use in particle physics . ( e . g . , ads/cft , hep-th/9905111 ; hep-ph/0702210 ) it may make one day testable predictions of previously unknown effects , and can then be checked for its validity .
there are fictitious forces when you define a non-inertial frame . for more information , see these newtonian dynamics notes by richard fitzpatrick . there are two core parts of the answer : one corollary of newton 's third law is that an object cannot exert a force on itself . another corollary is that all forces in the universe have corresponding reactions . the only exceptions to this rule are the fictitious forces which arise in non-inertial reference frames ( e . g . , the centrifugal and coriolis forces which appear in rotating reference frames ) . fictitious forces do not possess reactions . it should be noted that newton 's third law implies action at a distance . in other words , if the force that object exerts on object suddenly changes then newton 's third law demands that there must be an immediate change in the force that object exerts on object . moreover , this must be true irrespective of the distance between the two objects . however , we now know that einstein 's theory of relativity forbids information from traveling through the universe faster than the velocity of light in vacuum . hence , action at a distance is also forbidden . in other words , if the force that object exerts on object suddenly changes then there must be a time delay , which is at least as long as it takes a light ray to propagate between the two objects , before the force that object exerts on object can respond . of course , this means that newton 's third law is not , strictly speaking , correct . however , as long as we restrict our investigations to the motions of dynamical systems on time-scales that are long compared to the time required for light-rays to traverse these systems , newton 's third law can be regarded as being approximately correct .
the equation $$\displaystyle\int_{v}\frac{\rho}{\epsilon_0}d\tau=\int_{v} ( \nabla\cdot e ) ~d\tau$$ is true for all region $v$ in space the integration is performed over . that is why it follows that the integrands are equal . your counterexample is invalid , because the integrals are equal only when the domain of integration is of the form $ [ -a , a ] $ .
i think you are exercising an incorrect picture of statistics here - mixing the inputs and outputs . you are recording the result of a measurement , and the spread of these measurement values ( we will say they are normally distributed ) is theoretically a consequence of all of the variation from all different sources . that is , every time you do it , the length of the string might be a little different , the air temperature might be a little different . of course , all of these are fairly small and i am just listing them for the sake of argument . the point is that the ultimate standard deviation of the measured value $\sigma$ should be the result of all individual sources ( we will index by $i$ ) , under the assumption that all sources of variation are also normally distributed . $$\sigma^2 = \sum_i^n{\sigma_i^2}$$ when we account for individual sources of variation in an experiment , we exercise some model that formalizes our expectation about the consistency of the experiment . your particular model is that the length of the string ( for instance ) changes very little trial after trial compared to the error introduced by your stopwatch timing . unless we introduce other errors , this is claiming $n=1$ , and if the standard deviation of your reaction timing contributes $0.1 s$ to the standard deviation of the measurement , then theoretically the measurement should have that standard deviation as well . if this conflicts with the statistics of the time you actually recorded , then the possible ways to account for this include : your reaction time is not as good as you thought it was there are other sources of experimental error i would favor the latter , although it could be a combination of both of them .
books galileo and einstein very interesting book , 200 pages , by michael fowler , text for physics 109 , fall 2009 ( from babylonians and greeks to einstein ) physics made easy karura notes classical and quantum mechanics via lie algebras by arnold neumaier , dennis westra , 502 pages , ( arxiv ) by hans de vries : ' physics quest ' understanding relativistic quantum field theory - i love this ' book in progress ' to understand special relativity , and beyond . to see how a real lorentz contraction do happen ( ch . 4 ) and how magnetic field is induced by electrostactic field and non-simultaneity ( it is like a coriollis effect ) by benjamin crowell : ' light and matter ' - general relativity explore other physics topics here http://www.lightandmatter.com/ by bo thid: electromagnetic field theory - advanced electrodynamics textbook elecromagnetic fields and energy mit hypermedia teaching facility , by herman a . haus and james r . melcher ( with media ) hyperphysics - everything , in short . the physics hypertextbook detailed online book , very interesting work in progress . relativity - the special and general theory by albert einstein ( 1920 ) feynman lectures ( pdfs ) ( final index ) wikipedia physics a portal to start digging . a colaborative gigantic work . wikibooks -sr a textbook on relativity . wikisource - relativity portal find here " the measure of time " by henri poincar and many other original sources . stanford encyclopedia of philosophy a plethora of info related to physics ( for ex . singularities ) exploring the biofluiddynamics of swimming and flight david lentink the physics of waves by howard georgi of harvard the physics of ocean waves ( for physicists and surfers ) , by michael twardos at uci photonics - the basics and applications 92 pages , university of pennsylvania photonic crystals : molding the flow of light 305 pages , joannopoulos et al , princeton univ press computational genomics algorithms in molecular biology , lecture notes by ron shamir ( pdfs ) motion mountain by christoph schiller journals open access and online collections pse-list-open-access-journals directory of open access journals free , full text , quality controlled scientific and scholarly journals ( 6286 , been 2735 searchable at article level ) mathpages - lectures on various subjects in physics and mathematics . livingreviews journal articles by invitaton on relativity and beyond livingreviews blog about the journal articles calphysics research on the electromagnetic quantum vacuum ( with care , controversial material ) mit - opencourseware several courses available mit ocw fundamentals-of-photonics-quantum-electronics download pdfs sources to use with precaution preprints arxiv door to papers that i cannot afford ( sometimes good ideas ) -- cornell univ controlled i follow this archive thru this mit is blog the physics arxiv blog vixra free to post the ugly , the bad , the crazy , and sometimes good ideas independent researchers can publish here . the arxiv is usually closed to authors without academic affiliation . portals archive . org access to a world of original digitized books , and much more . nasa ads absctract data service search scribd - a generic social publishing site where i find books ( scientific/technical ) with full or partial access . scholar . google . com from the giant that is changing the observable universe of human beings cosmos portal from digital library encyclopedia of earth from digital library nanohub - a resource for nanoscience and technology multimedia youtube berkeley chanel with courses richard feynman - science videos - 4 original videos ( recorded at auckland ) arguably the greatest science lecturer ever . videos for shiraz 's lectures on string theory leonard susskind - modern theoretical physics from his " physics for everyone " blog fundamentals of nanoelectronics nanohub - lectures ( purdue univ ref . ece 495n ) including lecture 10: shrdinger 's equation in 3-d ( mp4 ) math multivariable calculus and vector analysis a set of on-line readings ( interactive click and drag with livegraphics3d ) ; explore tab topics khanacademy ( videos ) mission : to deliver a world-class education to anyone anywhere math and physics online tools : online latex equation editor ( right click the result and apply anywhere ) wolframalpha - computational knowledge engine , do you want to calculate or know about ? sage online - support a viable open source alternative to magma , maple , mathematica , and matlab . euler math toolbox free software for numerical and for symbolic computations geogebra - free mathematics software for learning and teaching modeling and simulation openmodelica physical modeling and simulation environment elmer open source finite element software for multiphysical problems ( examples ) mason multiagent based simulation ( ia ) ecj evolutionary computation ( ia ) breve a 3d simulation environment for multi-agent simulations and artificial life ( ia ) nanohub-periodic potential lab solves the time independent schroedinger equation in a 1-d spatial potential variation demonstrations . wolfram 7050 applets astronomy and astrophysics books and reviews cosmology today-a brief review ( arxiv 2011 ) this is a brief review of the standard model of cosmology . we first introduce the frw models and their flat solutions for energy fluids playing an important role in the dynamics at different epochs . we then introduce different cosmological lengths and some of their applications . the later part is dedicated to the physical processes and concepts necessary to understand the early and very early universe and observations of it . review of big bang nucleosynthesis ( bbn ) ( arxiv 2008 ) portals astro-canada introduction to astronomy , light , instruments , etc . data simbad search data on celestial bodies with the proper tools . nasa pds : the planetary data system data related to nasa missions sky viewers skyview , nasa skyview is a virtual observatory on the net wwt world wide telescope , microsoft simulation and presentations celestia free space simulation that lets you explore our universe in three dimensions . astrolab presentations_astronomiques ( fr ) other resources kirk mcdonald page at princeton . edu a handful of resources on em , qed , qm ( +-5gb ; - ) springerlink 's latexsearch you can search articles by using latex formulas input
the universe is not expanding " into " anything , it is the space itself that is expanding , or " stretching " if you will ! besides , it is the space between the galaxies that is expanding mostly . inside them , gravity is overcoming the expansion , so it is really only on the larger scales .
the flipping of a half dollar has been investigated ( theoretically and experimentally ) by the great persi diaconis along with susan holmes , and richard montgomery : see http://www-stat.stanford.edu/~cgates/persi/papers/headswithj.pdf it is rather impressive . they conclude that " vigorously-ipped coins are biased to come up the same way they started " in that a coin has about a 51% chance of landing on the side that started up when the coin was flipped . they call this a " dynamical bias . " it has nothing to do with the asymmetry of the coin ( which they take into account - they even give the principle moments of inertia of an american half dollar ) . if the initial condition ( heads up or tails up ) is chosen randomly ( and fairly ! ) , then according to diaconis et al . the result is essentially a fair coin toss . this of course raises the question of how this initial state should be generated in practice . but in any case this dynamical bias , in practice , favors heads and tails equally .
first of all , the statement is by design that perturbative string theory reproduces perturbative quantum-gravity+yang-mills at low energy , for perturbation about any solution to the supergravity equations of motion ( what user " dimension10" mentions is one part of the statement that perturbative string theory around such backgrounds is consistent to start with ) . notice that this perturbative nature is not some secret bug , but is so by the very nature of what perturbation theory is , in whichever context . ( see also http://ncatlab.org/nlab/show/string+theory+faq#backgrounddependence ) . moreover , the way in which this works in not new to string theory but is the time-honored process of effective quantum field theory ( see there for the historic examples ) : you write down some scattering amplitudes that you are interested in for one reason or another , and then you look for a quantum field theory that reproduces these scattering amplitudes in some low energy regime . once found , this is the given effective quantum field theory which approximates whatever theory your scattering amplitudes describe at possible high energy . next you play this game with the string scattering amplitudes which are defined by summing up correlation functions of some 2d super-conformal field theory of central charge -15 over all possible riemann surfaces with given insertions ( your asymptotically in- and outgoing states ) . next you ask if there is an ordinary quantum field theory such that it is perturbative scattering amplitudes coincide with these at low energy . turns out that this is a higher dimensional locally supersymmetric einstein-yang-mills theory , which is hence the effective field theory that describes the perturbative dynamics of strings at low energy . see on the nlab at string theory faq -- how is string theory related to the theory of gravity ?
i will try to adress the misunderstandings first , then answer the question . particle exchange force model is not causal there is a flaw in your thinking , in that you are formulating the electromagnetic interaction in terms of photon emission and absorption and at the same time telling a story forward in time . these two ideas are both ok separately , but not together . the particle emission/absorption picture is not a causal picture--- it requires that the particles go back and forth in time--- so you can not use causal language , like an electron emits a photon which kicks an electron etc . that is part of the story , but another part of the story is : an electron emits a photons which had already kicked an electron earlier , which emitted a different photon earlier than the first , etc , etc . if you go to a unitary hamitonian causal picture , you renounce the idea that the field is due to particle emission and absorption ( i only said unitary for a technical reason : it is conceivable somebody could make a nonunitary hamiltonian formulation with unphysical photon polarizations which contain the coulomb force , but then these unphysical photons would only be intermediate states , since the physical photons are not responsible for the coulomb interaction anyway ) . the acausality in the feynman description is not a problem with consistency , because there are causal formulations of qed , one of which is dirac 's . here the electrostatic repulsion is not due to photon exchange , but is instantaneous action-at-a-distance , while photons travel with only the physical transverse polarization . in feynman 's particle push picture , the electrostatic interaction is due to unphysically polarized photons travelling much faster than the speed of light , and these photons just are not present in dirac 's equivalent formulation . anyway , the best way to understand electron motion is using the classical electric and magnetic fields produced by the electrons . it is not the electrons pushing the electrons in a wire are not pushed by other electrons . they are pushed by the external voltage applied to the wire . the voltage is a real thing , it is a material field , it has a source somewhere at the power plant , and the power plant transmits the power through electric and magnetic fields , not by electron pushes . the electron repulsion in a metal is strongly screened , meaning that an electron travelling along at a certain speed will not repel an electron 100 atomic radii away . in many cases , it will even attract that electron due to weak phonon exchange ( this weak attraction gives superconductivity , and essentially all ordinary metals become superconducting at some low enough temperature ) . you can completely neglect the interelectron repulsion for the problem of conduction , and just ask about external fields rearranging charges in the wire . fermi surface , not wire surface the only electrons that carry current are those near the fermi surface . the fermi surface is in momentum space , it is not a surface in physical space . the electrons which carry the current are distributed everywhere throughtout the wire . but they all have nearly the same momentum magnitude ( if the fermi surface is spherical , which i will assume without comment in the remainder ) . the behavior of a fermi gas is neither like a particle nor a wave . it is not a wave , because the occupation number is 0 or 1 , so that there is no coherent superposition of a large number of particles in the same state , but it is also not like a particle , because the particle is not allowed to have momentum states lower than the fermi momentum , by pauli exclusion . the particle is traveling through a fluid of identical particles that jam up all the states with momentum smaller than the fermi momentum . this strange new thing ( new in the 1930s , at least ) , is the fermi quasiparticle . it is the excitation of a cold quantum gas , and to picture it in some reasonable terms , you have to think of a single particle which is always required to move at faster than a certain speed , it cannot slow down below this speed , because all these states are already occupied , but its can vary its direction . it has an energy which is proportional to the difference in speed from the lower bound . this lower bound on the speed in the fermi velocity , which in metals is the velocity of an electron with wavelength a few angstroms , which is about the orbital velocity in the bohr model , or a few thousand meters per second . the fermi liquid model of dense metals is the correct model , and it supersedes all previous models . the speed of the current carrying electrons is this few thousand meters per second , but at longer distances , there are impurities and phonons which scatter the electrons , and this can reduce the propagation to a diffusion process . the electronic diffusion does not have a speed , because distance in diffusion is not propotional to time . so the only reasonable answer to the question " what is the speed of an electron in a metal ? " is the fermi velocity , although one must emphasize that an injected electron will not travel a macroscopic distance at this speed in a metal with impurities . 1 . presumably at this level , electrons are acting more like waves and less like particles , but is there any classical component in the picture , ie are electrons coming in imparting other electrons with kinetic energy through repulsion , or does it not work that way ? in order to using a time-ordered causal langauge ( this does that , then this does that ) , you need electric and magnetic fields , not photons . the electrons are not what is coming into the wire to make it conduct , the thing that is coming in is an electric field . when you switch on a light , you touch a high voltage metal to a neutral metal , instantly raising the voltage , and making an electric field along the metal . this field accelerates the electrons near the fermi surface ( not on the wire surface , those near the fermi momentum ) to travel faster in the direction of ( minus ) the electric field e . it can only accelerate those electrons which can be sped up into new states , so it only speeds up electrons which are already running around at the fermi velocity . these electrons keep moving until they build up enough charge on the surface of the metal to cancel out the electric field , and to bend the electric field direction to follow the wire wherever the wire curves . this causal propagation is field-electrons-field , and the only electrons which serve to shunt the field are those which are building up charges on the surface of the wire ( and the protons on the surface which also redirect the field where there needs to be positive charge ) when you apply a constant voltage , the electrons come to a steady state where they are carrying the current from the negative voltage to the positive voltage , making the voltage drops line up in space along the direction of the wire , no matter what the shape , and bouncing off impurities and phonons to dissipate the energy they get from the field into phonons ( heat ) . the local electric field drives their motion , not their mutual repulsion . in that sense , it is not like water in a pipe . it is more like a collection of independent ball bearings pushed by a magnet , except that the ball bearings shunt the magnetic field to go along the direction of their motion . 2 . if electrons momentarily have energy , then pass it on by a photon , what determines when that photon is emitted , and what frequency it will be ? i assume that electrons in this cloud are not limited by any kind of exclusion principal , and that any frequencies are possible ? the electrons in the cloud are not only limited by exclusion , they are dominated by exclusion , this is the fermi gas . it is not the electrons pushing other electrons , it is the field pushing the electrons . the photon particle-exchange picture is irrelevant to this , but if you insist on using it , then the photons are coming out of the wall socket , having followed the high-voltage wires from the power-plant in a back-and-forth zig-zag in time , and a negligible fraction of the photons are emitted by the conduction electrons , since all those photons are absorbed into phonons by the metal within a screening length . the photons coming from the wall are bounced around by surface charges on the wire ( static electrons and protons ) so that they bounce around to follow the path of the wire in steady state . 3 . why should a photon emitted by an electron be in the direction of travel ? conservation of momentum tells me that if an electron is moving , the photon should be emitted in that direction , slowing the electron , but could an electron emit a photon in the opposite direction ? if it did , i assume it would somehow have had to absorb energy from elsewhere ? that sounds possible by analogy with quantum tunneling . photons are emitted in all directions , and back in time . it just is not useful to think of feynman picture when you want to think causally . 4 . what is the mechanism by which electrons propagating increase the temperature of the material ? are they transmitting energy to the electrons in the valence shell , which tug at the nucleus , do some photons hit the nuclei directly , or is there some other way ? so far , i have been treating the electrons as a gas of free particles . but you might be upset--- there are lots nuclei around ! how can you treat them as a gas ? do not they bounce off the nuclei ? the reason you can do this is that a quantum mechanical particle which is confined to a lattice , which has amplitudes to hop to neighboring points behaves exactly the same as a free particle obeying the schrodinger equation ( at least at long distances ) . it does not dissipate at all , it just travels along obeying a discrete version of the schrodinger equation with a different mass , determined by the hopping amplitudes . in solid state physics , this type of picture called the " tight binding model " , but it is really more universal than this . in any potential , the electrons make bands , and the bands fill up to the fermi surface . but the picture is not different from a free gas of particles , except for losing rotational symmetry . if the lattice were perfect , this picture would be exact , and the metal would not have any dissipation losses at all . but at finite temperature there are phonons , defects , and a thermal skin of electrons already excited at a little more energy than the fermi surface . the phonons , defects , and thermal electrons can scatter the conducting electrons inelastically , and this is the mechanism of energy loss . the electrons can also emit phonons spontaneously , if their energy is far enough above the fermi surface so that they are no longer stable . all of these effects tend to vanish at zero temperature ( with the exception of defects , which can be frozen in , but then the defects become elastic ) . but at cold enough temperatures , you do not go to zero conduxctivity smoothly . instead , you tend to have a phase transition to a superconducting state . 5 . presumably , electricity travels slower than light , because there is some time in each exchange , and some time when electrons are moving at sublight speeds before emitting a photon . by how much is this slower than light , and what is the speed of each interaction ? this is again confusing feynman description with a causal description . but i did this experiment as an undergraduate , and along a good coaxial cable , the speed was 2/3 the speed of light . i assume that if you use an ordinary wire in a coil on the floor , its going to be significantly slower , perhaps only 1% of the speed of light , because it requires more finnagling of surface charges for the wire to set up the field to follow it curves .
this question has a great pedigree ! supposedly it is what einstein asked himself when he was first thinking about relativity . the answer is that you can not " ride on top of a light stream " -- that is , you can not go as fast as the speed of light . the speed of light is invariant -- same for all observers . so at any sub-light speed you can see yourself in a mirror just fine . no matter how fast you are going , a beam of light going past you still moves at the speed of light , relative to you . no matter how hard you fire your rocket engines , you never catch up with it .
it is been proposed , and there are a variety of possible experimental systems people are working on . i expect this to be a very useful way of simulating quantum systems , which will work well for many questions long before large-scale universal quantum computers can be built .
update : the insights and conceptual meanings advocated below in this answer are detailed and technically developed , for example , in these wonderful articles : fuchs ; peres - quantum mechanics needs no interpretation , physics today ( 2000 ) , vol . 53 , issue 3 , p . 70 . englert - on quantum theory , eur . phys . j . d ( 2013 ) 67: 238 . duvenhage - the nature of information in quantum mechanics , found . phys . 2002 , 32 1399-1417 . bub - quantum mechanics is about quantum information , found . phys . 2005 , 35-4 , p . 541-560 . rovelli - relational quantum mechanics , int . j . of theor . phys . 35 ( 1996 ) 1637 . grinbaum - on the notion of reconstruction of quantum theory . clifton ; bub ; halvorson - characterizing quantum theory in terms of information-theoretic constraints , found . phys . ( 2003 ) 33 , 1561-1591 . for intuitions and insights on the meaning of the formalism of quantum mechanics , i eagerly recommend you read carefully the following wonderful reference books ( especially feynman on intuition and examples , isham on the meaning of mathematical foundations , and strocchi or blank et al . on the $c^*$-algebras approach ) : griffiths - " consistent quantum theory " ( freely available ! ) . isham - " lectures on quantum theory , mathematical and structural foundations " . strocchi - " an introduction to the mathematical structure of quantum mechanics : a short course for mathematicians " . takhtajan - " quantum mechanics for mathematicians " . blank ; exner ; havlcek - " hilbert space operators in quantum physics " . as feynman said ( something similar ) : " if you think you understand quantum mechanics then you do not understand it at all " . the whole issue of understanding its hilbert space formalism , aside from the interpretation of the physical theory itself , can be dealt with more easily ( in fact , that is what most physicists do : understand the mathematical formalism with " nave " empirical intuitions of its meaning so that the theory is predictive and useful , but the issue of its real ontology and epistemology is not at all settled yet ) . the best approach to grasp the quantum formalism may be to give a parallel interpretation of its classical mechanics analogue , so i will try to elaborate this a bit regarding your different points you mention . note that quantum mechanics and its postulates can be formulated in different equivalent forms : dirac 's bra-ket formulation of schrdinger 's picture ( wave mechanics ) , heisenberg 's operator formulation ( matrix mechanics ) and the density operator formulation of states ( in heisenberg 's picture ) . since your different points are very intertwined , i will try to explain a little bit of everything all at once . in classical mechanics one measures empirical quantities like the position and speed ( so linear momentum ) of bodies and idealized particles , thus defining a configuration space and phase space of all physical possible states . any other observable property must be a function of the system intrinsic parameters ( usually constants like rest mass , charge . . . ) and those dynamical variables , so the algebra of classical observables ( like energy ) is the commutative ring of typical functions on phase space . given a measurement one constrains the system to a localized region of phase space within the instrumental precision available , thus obtaining the inicial conditions of the system of interests ( where the rest of the universe is usually either ignored or put into an effective/statistical external action over the relevant degrees of freedom ) . after much experimental observation , physicists obtained the " dynamic laws of classical mechanics " by which given that initial observed state , the system shall evolve with respect to an external clock variable , so at a later time its new observed state can be predicted within the restrictions of precision and chaos theory . quantum mechanics is the experimental realization that at the microscopic level , the degrees of freedom of any system behave differently . classical observables , like position and linear momentum arise as a large scale statistical result of their quantum mechanical counterparts . concretely , phase space is not commutative ( heisenberg 's uncertainty bound ) so classical position and momentum define a minimum " quantum chunk " of phase space . thus , as functions of these noncommutative basic observables , the rest of quantum observables ( with classical counterpart ) must come from generically noncommuting operators , and these act naturally on hilbert spaces . points 1 and 3 have really the same justification . one assumes that any real experiment , i.e. any empirical observation made by any instrument and read by any sentient being , measures real numbers : position is located by distances to reference systems , timing is kept track with periodic movements also so speed and then momentum all reduce to movement measurements in the end ( masses are measured for example with the distance of stretching of a spring by which an object is hung ) . so observables must give real values upon measurement , which because of precision errors are actually approximated by rationals , or even computable reals , in practice . now , one can think of an observable as the set of possible values of it , i.e. the set of different possible states of a measurable property . if our quantum mechanical observables must be in general noncommutative operators and be totally specified by a spectrum of real values , then the natural choice is to consider self-adjoint operators since operators have a real eigenvalue spectrum in a suitable basis if and only if they are self-adjoint . so quantum mechanics is " just " the transition to operators which are not simultaneously diagonal in the same basis . this is the significance of the spectral decomposition theorem for the whole formalism : a quantum observable is just the set of its possible classical empirical values codified as an operator by a spectral decomposition with these values as eigenvalues . thus , all the quantum observables must be self-adjoint operators , with the quantumness manifesting itself by the general noncommutativity of them . self-adjoint operators which are diagonalizable in the same eigenbasis are said to be " compatible " , and they are so if and only if they commute with each other ( that is why commutators $ [ a , b ] $ play such an important role in the formalism ) . the fact of existing noncommuting sets of operators , like position and linear momentum , was called " complementarity " by bohr . point 2 and 4 is just the mathematical realization of all the previous discussion . since classical observables form a commutative $c^*$-algebra , and quantum observables a noncommutative $c^*$-algebra , by the gel'fand-naimark theorem any of the latter is isometrically isomorphic to an algebra of ( bounded ) linear operators in some hilbert space . this is a practical realization of the abstract concept of observables as being sets of values and having general algebraic relations between them , i.e. a kind of calculational representation . once the hilbert space is introduced , the gel'fand-naimark-segal construction shows that pure states correspond then to rays in the hilbert space ( i.e. . to vectors/point of the projective hilbert space ) . this corresponds to the quantum case of the classical situation were the abelian version of gel'fand-naimark states that every commutative $c^*$-algebra ( with unity ) is isometrically isomorphic to the algebra of continuous functions for some compact hausdorff space , thus recovering phase space . this establishes the kinematics of the theory , after which dynamics can be introduced and studied by either evolving states with operators fixed ( schrdinger 's picture ) or evolving operators with states fixed ( heisenberg 's picture ) , and these are dual to each other . in the statistical physics point of view , when a system is in a given state , all one really measures about an observable is its expectation value with respect to the probability distribution of that state ; thus , a state can be viewed as a positive linear functional on the $c^*$-algebra of observables ( not to be confused with the functionals given by dirac 's " bras " discussed below ) , establishing the duality between states and operators : a " state " is a probability distribution on the algebra , determining the probability of possible values of any observable in the next measurement , now by gleason 's theorem any such distribution implies the existence of a density operator which represents a pure ( or more generally mixed ) state of the system . thus , you can either see a pure state as an eigenvector of a complete set of compatible observables , or either a positive linear functional on their algebra . this is very deep and important because removes any ontological weight to the state vectors beyond the mere fact of being " the collection of probabilities of possible actualization of values " of the system . point 4 can now be understood naturally . when measuring observables , the instruments are localizing " where in the spectrum of eigenvalues " the system has each property . if measuring a particular observable has no effect on the value of another , then they are compatible , and by exhausting the measurements of a complete set of compatible observables for a system , one specifies the state of the system at that moment : since our system is characterized by the properties we observe , particular defined properties for each of its features characterizes the system completely . since by measuring compatible observables we are selecting eigenvalues in the spectrum of the operators , we are actually projecting from the complete hilbert space down to a particular vector labeled by the eigenvalues , as a complete set of commuting self-adjoint operators define a common eigenbasis . thus the " string of data of observed values " is our measured observed state , so one thinks of the vector ( ket $|\psi\rangle$ ) as the pure state of the system . since predictions of the theory do not depend on the norm of the vector , the state of the system is actually a ray , i.e. a vector in the projective hilbert space . ( in fact since this must be done also for continuous spectrum of unbounded self-adjoint operators , the right formalism is that of rigged hilbert spaces ) . therefore , each ( ray ) vector basis of the hilbert space corresponds to a choice of which set of compatible observables one is measuring , with each vector of each basis being a possible state to get in observations , i.e. a possible array of ( eigen ) values of our chosen properties to measure and characterize the system . in between observations , the isolated system evolves unitarily , so the " hidden " state of the system gets into a general superposition of eigenvectors in any chosen eigenbasis . besides the duality on the operator-state level , there is the other dual notion of " bra " $\langle\psi|$ which are the linear functionals on the vectors of hilbert space . do not confuse hilbert vector states , their vectorial duals as final states , and the state seen as a positive linear functional on the algebra of observables : the pure measurable states $|\psi\rangle$ are given by eigenstates in a common eigen-basis of commuting self-adjoint operators , and general states as a superposition of those ; now since most of the prediction of the formalism are given by the hilbert scalar products of vector states , $ ( |\psi\rangle , \ , |\chi\rangle ) $ , the riesz representation theorem guarantees that there is a functional $\phi_\psi:\mathcal h\rightarrow\mathcal c$ such that $\langle\psi|\chi\rangle :=\phi_\psi ( |\chi\rangle ) = ( |\psi\rangle , \ , |\chi\rangle ) $ , so dirac rewrote the whole formalism in terms of bras $\langle\chi|$ and kets $|\chi\rangle$ to denote things like the probability amplitud to observe the sate $|\psi\rangle$ after having observed state $|\chi\rangle$ , so : $\mathcal p ( \chi\rightarrow\psi ) =|\langle\psi|\chi\rangle |^2$ . since probabilities are scalar products squared , if $\chi$ or $\psi$ are linear superpositions of other eigenstates , the transition probability is not the sum of individual possibilities but there are also cross terms which are responsible for the interference quantum effects and the wave-particle duality . so you can think of bras , the functionals , as " final states " in a calculation . the fundamental experimental fact is that there are properties which cannot be measured simultaneously ( not even in perfect ideal conditions ) . if one measures the position of an atom , one gets a region in $\mathbb r^3$ more localized/small as more precise is the measurement device , but then the measurement of its linear momentum spreads in size over the possible values . of course each measured value is as precise and definite as possible , but if you repeat the position measurement after the momentum one , the old value is not conserved and position randomly takes a new value within its spectrum , with a probabilistic distribution of dispersion/variance bigger as smaller is the uncertainty in momentum . what is happening is not a mysterious magic , but the fact that position and momentum operators do not commute , so they do not have a common eigenbasis , thus the system cannot be at the same time in a defined position and defined momentum . it is an old philosophical misconception that the act of observing by perturbing the system alters the value and makes simultaneous measurements impossible , complementarity is one of the core features of the quantum world regardless of who or what makes a measurement . since " a state " is a set of defined properties of our system , there is no meaning to talking about simultaneously defined incompatible observables , in the same sense as there is no meaning in talking about the color of a music note ( putting aside synesthesia ) . the confusion appears for trying to conceptualize the quantum world within classical realism ( structural empiricism is much better in this regard ) . after the observation was made , if the system is allowed to evolve in isolation again , the theory predicts the probability of observing another possible eigenstate , maybe from another basis , at a later time . the formalism does this by evolving the initial observed eigenstate into a general linear superposition by schrdinger 's equation , so at a later time the complex components of the evolved vector over every eigenstate of any chosen basis have changed , with the square of the modulus of each component being the probability for observing the eigenvalues of that eigenvector . this is schrdinger 's picture , which is misleading philosophically ( as dirac himself claimed ! ) , from an empiricist stance the more meaningful picture is heisenberg 's : the state is only the observed state , the ket characterized by the string of observed ( eigen ) values of a chosen set of compatible observables ; when the system is isolated , its observables/operators evolve unitarily , the state does not change until observed again , but the new observation gives randomly new values as the operators have changed . thus the whole formalism can be cast into operator algebras , with observables and states characterized by specific types of operators ( as the states themselves can be seen as the projections of the spectral decomposition in a common eigenbasis , so if you just measure some of the compatible observables your state is a projector not into an eigenvector but into a common eigen-subspace of your chosen set of compatible observables ) . summary : systems are completely described by the observable properties they have , which may not be simultaneously defined . this is because having defined properties of certain kinds makes impossible to have defined properties of another kinds , so observing some aspects of a system destroys/"undefines " the previous properties which are incompatible with the new ones . this forces the algebra of observables to be noncommutative : the order of which observables are measured in succession does not necessarily commute . since any physical measurement is numerical , in general real-valued , the noncommutative algebra fits nicely among self-adjoint operators , as they are the only unitarily equivalent to a real spectrum . thus , we determine experimentally which sets of observables commute , so we can talk of complete sets of compatible operators , which define the state of the system completely by a string of eigenvalues ( the actual values of the parameters/properties measured ) . such a set defines a basis of a hilbert space upon which the operators act , so different sets of compatible operators define different basis , so that if our system is given by a common eigenvector of one basis , it will be generally a linear superposition of the eigenvectors of another basis ; since the eigenvalues are the observable properties of the system , the superposition in the other basis has no uniquely defined eigenvalues and thus those properties of the system in that state are not well-defined at that time . since common eigenstates can be given by projection operators which project onto the successive eigensubspaces , a ( pure ) state of the system can be given either by a ray , by a suitable projection operator on the hilbert space , or by a positive linear functional on the noncommutative $c^*$-algebra of quantum observables . one can work only with operator algebras and characterize which are observables and which are states and interrelate them by linear evaluations to get the predictions of the theory ( mostly expectation values ) .
as said by john rennie , it has to do with the shadows ' fuzzyness . however , that alone does not quite explain it . let 's do this with actual fuzzyness : i have simulated shadow by blurring each shape and multiplying the brightness values 1 . here 's the gimp file , so you can see how exactly and move the shapes around yourself . i do not think you had say there is any bending going on , at least to me the book 's edge still looks perfectly straight . so what is happening in your experiment , then ? nonlinear response is the answer . in particular in your video , the directly-sunlit wall is overexposed , i.e. regardless of the " exact brightness " , the pixel-value is pure white . for dark shades , the camera 's noise surpression clips the values to black . we can simulate this for the above picture : now that looks a lot like your video , does not it ? with bare eyes , you will normally not notice this , because our eyes are kind of trained to compensate for the effect , which is why nothing looks bent in the unprocessed picture . this only fails at rather extreme light conditions : probably , most of your room is dark , with a rather narrow beam of light making for a very large luminocity range . then , the eyes also behave too non-linear , and the brain cannot reconstruct how the shapes would have looked without the fuzzyness anymore . actually of course , the brightness topography is always the same , as seen by quantising the colour palette : 1 to simulate shadows properly , you need to use convolution of the whole aperture , with the sun 's shape as a kernel . as ilmari karonen remarks , this does make a relevant difference : the convolution of a product of two sharp shadows $a$ and $b$ with blurring kernel $k$ is $$\begin{aligned} c ( \mathbf{x} ) = and \int_{\mathbb{r}^2}\ ! \mathrm{d}{\mathbf{x'}}\: \bigl ( a ( \mathbf{x} - \mathbf{x}' ) \cdot b ( \mathbf{x} - \mathbf{x'} ) \bigr ) \cdot k ( \mathbf{x}' ) \\ = and \mathrm{ift}\left ( \backslash{\mathbf{k}} \to \mathrm{ft}\bigl ( \backslash\mathbf{x}' \to a ( \mathbf{x}' ) \cdot b ( \mathbf{x}' ) \bigr ) ( \mathbf{k} ) \cdot \tilde{k} ( \mathbf{k} ) \right ) ( \mathbf{x} ) \end{aligned} $$ whereas seperate blurring yields $$\begin{aligned} d ( \mathbf{x} ) = and \left ( \int_{\mathbb{r}^2}\ ! \mathrm{d}{\mathbf{x'}}\: a ( \mathbf{x} - \mathbf{x}' ) \cdot k ( \mathbf{x}' ) \right ) \cdot \int_{\mathbb{r}^2}\ ! \mathrm{d}{\mathbf{x'}}\: b ( \mathbf{x} - \mathbf{x'} ) \cdot k ( \mathbf{x}' ) \\ = and \mathrm{ift}\left ( \backslash{\mathbf{k}} \to \tilde{a} ( \mathbf{k} ) \cdot \tilde{k} ( \mathbf{k} ) \right ) ( \mathbf{x} ) \cdot \mathrm{ift}\left ( \backslash{\mathbf{k}} \to \tilde{b} ( \mathbf{k} ) \cdot \tilde{k} ( \mathbf{k} ) \right ) ( \mathbf{x} ) . \end{aligned} $$ if we carry this out for a narrow slit of width $w$ between two shadows ( almost a dirac peak ) , the product 's fourier transform can be approximated by a constant proportional to $w$ , while the $\mathrm{ft}$ of each shadow remains $\mathrm{sinc}$-shaped , so if we take the taylor-series for the narrow overlap it shows the brightness will only decay as $\sqrt{w}$ , i.e. stay brighter at close distances , which of course surpresses the bulging . and indeed , if we properly blur both shadows together , even without any nonlinearity , we get much more of a " bridging-effect": but that still looks nowhere as " bulgy " as what is seen in your video .
from what i have read , the force on a wing is calculated by integrating the sheer stress and pressure distribution around the surface of the wing . the net force is broken down into two components : the component perpendicular to the relative wind is called lift , and the component parallel to the relative wind is called drag . edit : this is the book you want : http://www.amazon.com/fundamentals-aerodynamics-mcgraw-hill-aeronautical-engineering/dp/0072950463 you can find an old edition here : http://www.scribd.com/doc/6681202/fundamentals-of-aerodynamics
maxwell 's equations do not contain any information about the effect of fields on charges . one can imagine an alternate universe where electric and magnetic fields create no forces on any charges , yet maxwell 's equations still hold . ( e and b would be unobservable and totally pointless to calculate in this universe , but you could still calculate them ! ) so you can not derive the lorentz force law from maxwell 's equations alone . it is a separate law . however . . . --> some people count a broad version of " faraday 's law " as part of " maxwell 's equations " . the broad version of faraday 's law is " emf = derivative of flux " ( as opposed to the narrow version " curl e = derivative of b" ) . emf is defined as the energy gain of charges traveling through a circuit , so this law gives information about forces on charges , and i think you can derive the lorentz force starting from here . ( by comparison , " curl e = db/dt " talks about electric and magnetic fields , but does not explicitly say how or whether those fields affect charges . ) --> some people take the lorentz force law to be essentially the definition of electric and magnetic fields , in which case it is part of the foundation on which maxwell 's equations are built . --> if you assume the electric force part of the lorentz force law ( f=qe ) , and you assume special relativity , you can derive the magnetic force part ( f=qv x b ) from maxwell 's equations , because an electric force in one frame is magnetic in other frames . the reverse is also true : if you assume the magnetic force formula and you assume special relativity , then you can derive the electric force formula . --> if you assume the formulas for the energy and/or momentum of electromagnetic fields , then conservation of energy and/or momentum implies that the fields have to generate forces on charges , and presumably you can derive the exact lorentz force law .
actually , you are right : the hubble constant is not really constant . at least , it is not constant in time . the reason it is called a constant is that , when edwin hubble originally compared the recession velocities of galaxies with their distances in 1929 , there was no reason to expect any particular pattern . after all , just a few years prior , people had thought there were no other galaxies . but what hubble found was that , except for a small amount of random variation , the velocities of galaxies were proportional to their distances ; in other words , the ratio $v/d$ was roughly the same for all galaxies he observed . the value of this ratio came to be known as hubble 's constant , $h_0 \equiv \frac{v}{d}$ , because it was constant from one galaxy to the next , rather than varying randomly as one might have guessed at the time . of course , it was not long before people realized that if the recessional velocity of each galaxy was proportional to its distance , you could extrapolate back to some point in the past at which $d = 0$: all the galaxies would have started out in the same place . this gives you an effective age for the universe . if you use a simple linear extrapolation , from basic kinematics you get $$t = \frac{d}{v} = \frac{1}{h_0}$$ so the hubble " constant " is not constant in time , but rather is inversely related to the age of the universe . as the universe gets older , the hubble constant gets smaller , as you would expect . this happens because the distance $d$ to any given galaxy increases with time . however , the fact that the universe has an age does not create an absolute time . sure , different observers at different points in spacetime will measure different values for the age of the universe . and sure , you could define a time coordinate system by specifying that the time coordinate for any observer is the age of the universe as measured by that observer . this is called the comoving time , and it is a useful and sensible way to set up a coordinate system in time . but it is not the only possibility , and there is certainly nothing so special about it that it deserves to be called " absolute . " any observer who is moving with respect to the universe as a whole ( i.e. . relative to the hubble flow ) would not measure time at the same rate as this comoving time .
the water near the heating element turns into water vapor . this vapor then rises up to the surface but as it meets colder water upwards it turns back into water . as the water/steam transition is not smooth ( e . g . the volume changes rather rapidly during phase change ) , the constant transition between vapor and liquid states produces noise . as the water gets to +100c ( boiling point ) , the vapor bubble will not turn back into liquid before reaching the surface thus making less noise . for more on this click this .
textbook solution the first thing to do is to define the center of mass and relative coordinates : $$ r ( t ) = {m_1 r_1 + m_2 r_2 \over m_1 + m_2} $$ $$ r ( t ) = r_2 - r_1 $$ you invert this to find $$ r_1= r - {m_2\over m} r$$ $$r_2=r+ {m_1\over m} r$$ the equation of motion for r is trivial , since center of mass is a conservation law : $$ {d^2 r \over dt^2 } = 0 $$ and it is solved by $$ r ( t ) = v_0 t + r_0 $$ where $v_0$ and $r_0$ are the initial center of mass velocity and position respectively ( which are calculated from the given initial conditions ) . the nontrivial equation is for the relative coordinate : $$ m {d^2 r \over dt^2} = - {m r\over |r|^3}$$ where $m=m_1 + m_2$ is the total mass , and m is the reduced mass : ${1\over m} ={1\over m_1} + {1\over m_2}$ the problem is reduced to solving the kepler motion in a 1/r potential . from now on , i will rescale time to make the mass parameter in the r equation 1 . you can choose the x axis to lie along the initial r , and the y-axis to lie along the component of the initial $\dot{r}$ perpendicular to the initial r . another way of saying this is that you rotate the coordinates to make the angular momentum vector $r\times p$ where $p=m\dot{r}$ to lie along the z-axis . this rotation reduces the problem to a plane , and the rotation matrix columns are given by the normalized initial r ( now along the x-axis ) , the component of the initial velocity perpendicular to r , normalized ( along the y-axis ) , and normalized l along the z-axis . you then use units to set the total over reduced mass to 1 , and use polar coordinates in the x-y plane of the motion , and note that the angular momentum is constant : $$ r^2 {d\theta\over dt} = l $$ this tells you that equal areas are swept out by the r-vector in equal times . the equation of motion for r ( t ) ( no longer a vector , now a scalar radial coordinate ) is : $$ {d^2 r \over dt^2} = {l^2 \over r^3} - {1 \over r^2} $$ then you change time out for $\theta$ , expressing everything in terms of $r ( \theta ) $ , which you can do using the equal area law , whenever the anguar momentum is nonzero ( if the initial angular momentum is zero , or very close to zero , this is a one-dimensional two-body problem which can be solved directly by more elementary means ) . the equation of motion for $r ( \theta ) $ simplifies when you make a coordinate transformation to $u={1\over r}$: $$ {d^2 u \over d\theta^2} = c - u $$ where c is some unimportant constant , and this is solved by $$u ( \theta ) = {1\over a} ( 1 + a \cos ( \theta -\theta_0 ) ) $$ where a is the semi-major axis of the ellipse ( if the orbit is an ellipse ) , $\theta_0$ determines the orientation in the x-y plane , and a is the eccentricity of the ellipse ( if a&lt ; 1 ) , or determines the angle of the hyperbola ( if a> 1 ) or tells you the orbit is a parabola ( a=1 ) . the only result you need is that $$ r ( \theta ) = {a\over 1+ a\cos ( \theta-\theta_0 ) } $$ this gives you the solution of r as a function of $\theta$ , which gives the shape of the orbit . this is where textbooks stop . finding $\theta$ as a function of $t$ but you then want the solution for $\theta$ as a function of time , to get the r and $\theta$ as functions of time . this is determined from the area law , conceptually : $$ r^2 {d\theta\over dt} = l$$ $$ {d\theta \over ( 1+ a \cos ( \theta-\theta_0 ) ) ^2 }= {l \over a^2} dt $$ and integrating this from time 0 to time t , tells you in principle what $\theta ( t ) $ is . the result can be written as : $$ f ( a , \theta ) = f ( a , \theta_0 ) + {l\over a^2} t $$ where $f ( a , \theta ) $ is the special function that gives you the area of a conic section of parameter a , in a wedge from the focus where one half-line is along the major axis , and the other half-line makes an angle $\theta$ with the first . this special function is not expressible in terms of elementary functions . this function is defined by the integral above , and you can calculate it numerically using any numerical integration method . finding this function , and inverting it , is the only difficult part of this problem . there are three limits which are necessary for perturbations : for a=0 , $ f ( 0 , \theta ) = \theta$ for a=1 , $ f ( 1 , \theta ) = {y\over 4} + {y^3\over 12} $ where $y=r\sin{\theta} = {\sin ( \theta ) \over 1+\cos ( \theta ) }$ for $a=\infty$ , $ f ( a , \theta ) \approx {1\over a} \tan ( \theta ) $ each of these are elementary degenerations : the first is the circle , the second is the parabola , and the third is a straight line . the important thing is that each of these degenerations gives you x ( t ) and y ( t ) which are simple , and further , you can perturb around each of these three limits in a nice way . in the following , the parameter t is rescaled to absorb ${l\over a^2}$ circle : $x ( t ) = \cos ( t ) $ $y ( t ) = \sin ( t ) $ parabola : $y ( t ) = ( \sqrt{1+36t^2} + 6t ) ^{1\over 3} - ( \sqrt{1+36t^2} - 6t ) ^{1\over 3} $ $x ( t ) = {1\over 2} - {y^2\over 2} $ line : $x ( t ) = {1\over a}$ , $y ( t ) = t$ the line and circle are obvious , the parabola is found by inverting the cubic for y as a function of t using the cubic equation . near the circle , time is periodic with the orbital period , which is the area inside the ellipse $aa^2$ divided by the area sweep-rate $l/2$ . so you have once-winding function from a circle to a circle , which can always be written as a fourier series with a linear term , which is found from the power series of the integrand in a , integrated term by term . near the straight-line hyperbola , you can similarly perturb in a series , and the only interesting degeneration is the parabola . near the parabola , the perturbation theory is a little more complicated .
just to get that out of the way , " straight line " here means geodesic on a curved surface/manifold , but i guess you understand that . " if the earth travelling around the sun is just moving along a straight line in the curved space , should not light also be trapped in orbit around the sun ? " as you have guessed , they do not follow the same geodesic because of their velocity . and the velocity of a body is automatically taken into account , because you do not just compute these geodesics in curved space , but in curved spacetime , where this makes a difference : imagine a $t$-$x$-diagram and how the earth or a ray of light go away from the origin . the earths path will be close to the time axis , while the light path will be leaning towards the space axis ( depending on your units ) . both paths are similarly affected by the curvature of spacetime but they clearly start out in different directions ( in spacetime , not in space ) and so the geodesics will be quite different . there is some specific angle for which the object will be orbiting ( notice that this now requires at least two spatial dimensions ) . smaller angles will fall towards the earth while bigger angles will boldly go where no man has gone before . this is a very geometric notion of escape velocity . " i expect the actual equations in the theory also take into account a body 's velocity , not just the curvature . " since you have 1000 rep on se mathematics i can formulate it this way : the geodesic equations are , like newtons second law , second order and so they require two initial values . one is a position vector and the other one is a velocity vector , i.e. the direction in spacetime . the curvature ( and that is another business ) is taken into account by the coefficients of $\gamma$ in the differential equations . these coefficients are basically derivatives of the spacetime metric , which itself must be a solution of the einstein equations . also , if you take a look at the geodesic equation , you see that for $\gamma = 0$ ( flat spacetime ) , you get the easy case $x&#39 ; &#39 ; ( t ) =0$ , or $x ( t ) =x_0+v_0t$ , which represent actual straight lines . here $x_0$ and $v_0$ are initial data . furthermore , due to the equivalence principle in general relativity , the geodesics do not depend on the mass of the objects . all things fall equally , once they have the same starting position and velocity . but if two different masses are initially at rest ( or since this is a relative statement let 's better say : not moving relative to each other ) , then because of the relation between acceleration and mass , it is more difficult to get the heavier mass to have a certain starting velocity , i.e. direction in spacetime . and therefore you personally will never be able to get a chair on the same trajectory as a pen , which you forcefully throw in a certain direction in space . the chair is too heavy for you to make it follow the same path you could make a small pen follow . if two objects with different masses are initially at rest ( not moving relative to each other ) and then get pushed equally hard by some force , they will not both end up orbiting the same thing . so even if the geodesics of spacetime do not depend on the masses of the objects , which would follow them , you will never see a ray of light follow the same trajectory as a flashlight , because , by the laws of relativity , the flashlight can not move at the speed of light . this is the extreme example : massive objects never follow light-like geodesics and the other way around .
the " is simply an excitation " might be a bit overstated , as it if it were that simple you probably would not have needed to ask this question . it might be better stated as " can simply be modeled mathematically as an excitation " . the " fields " are part of a mathematical model that attempts to explain our observations . saying " where does a field come from " is a bit like saying " where does everything in the universe come from " or " what happened before the big bang " , which may seem intuitively sensible to humans , but are not necessarily well-founded questions . some " fields " may have emerged shortly after the big bang due to symmetry breaking , so it may be possible to unify some of the " fields " to a simpler state with fewer " fields " , if that makes sense . evidence suggests ( as well as a scientific bias toward simplicity ) that the closer you get to the big bang , the simpler the models can be . unfortunately , this also makes the process of modeling things more complicated in part since we can not make direct observations of what the universe might have looked like back then to test theory .
i do not see why there could't be asymmetries for r> c , considering the fact that the charge distribution is not symmetric . so what is the argument for this symmetry ? look at the figure below , in the initial stage , there is an electric field within the conductor ( electric field created by the the charge $q$ ) which lasts for an $infinitesimal$ $period$ $of$ $time$ . that is because , the free electrons within the conductor move under the influence of the electric field . under electrostatic conditions , the electric field inside the conductor becomes zero due to the redistribution of these free electrons . the charge distribution is shown in the diagram above ( look at $final$ ) . you clearly see that $-q$ appears on the inner surface of the conductor as a result of the redistribution ( non-uniformly distributed ) . since charge is conserved , a charge of $3q$ appears on the outer surface of the conductor . the charge $3q$ distributes uniformly on the outer surface of the conductor . this result is consistent with gauss 's law . consider the $gaussian surface$ as shown above . the net charge enclosed by this surface is $zero$ and the $flux$ associated with this surface is also $zero$ which tells us that the $electric field$ inside the conductor is $zero$ . mathematically , $$\iint \vec{e} \cdot \vec{\mathrm{d}a} = \frac{q}{\epsilon_0}$$ since $q$ = 0 , $\vec{e} = 0$ . now , if i move the charge $q$ anywhere inside the cavity , the charge distribution on the outer surface will not be affected . although the charge distribution on the inner portion of the conductor will change as result of the change in position of $q$ ( remember that there is an electric field in the cavity ) , the charge distribution on the outer surface will remain uniform . why ? it is because there is no electric field within the conductor and we know that static charges respond to electric fields . for the charges present on the inner walls of the conductor to interact with the charges present on the outer surface , an electric has to be there which in this case is $zero$ . so , charge $3q$ remains uniformly distributed on the outer surface of the conductor . this phenomenon is called $electrostatic$ $shielding$ . for $r &gt ; c$ , $\mathbf{v} = \frac{3q}{4\pi \epsilon_0 ( r ) }$ for $r = c$ , $\mathbf{v} = \frac{3q}{4\pi \epsilon_0 ( c ) }$ for $b &lt ; r &lt ; c$ , $\mathbf{v} = \frac{3q}{4\pi \epsilon_0 ( c ) }$ ( why ? ) that is because , there is no electric field inside the conductor which implies $\delta\boldsymbol{v} = 0$ . so , under electrostatic conditions , the potential at the surface of the conductor is equal to the potential anywhere inside the conductor . also , the surface of the conductor is an $equipotential$ $surface$ .
i think you may have to include the weight ( $mg$ ) of the ball using the force vector along y-axis ( $\vec{f_y}$ ) along with the force vector along x-axis ( which is the electrostatic attraction , which here is $\vec{f_x}$ ) . the magnitude of the resultant of the two forces may end up with the answer . there should be a reason why they have given mass and inclined angle . both are used for projecting the force along the respective axes . dot product should help for projecting $f_n$ as $fcos \theta$ .
how do gravitons impact on general relativity ? [ . . . ] i would guess that this kinda question was asked by some scientists and answered . . . it is been asked but never answered satisfactorily . the full impact on general relativity would be that it would become a theory of quantum gravity . nobody has ever been able to construct a satisfactory theory of quantum gravity . we have various guesses , and we can reason by analogy with other fields such as the electromagnetic field , but basically not much is known .
is it possible for a star to have the same mass and radius as e.g. the moon and orbit a planet like earth at the same distance ( at which moon orbits earth in actuality ) ? no . the lowest mass type of star is a brown dwarf , which still has a mass greater than that of jupiter . even brown dwarfs have too little mass to fuse light hydrogen . neutron stars can have much smaller radii than the moon , and white dwarfs can be about the same size of the moon , but they have much greater masses .
it sounds like you do not want the normal rotating spaceship like in "2001" because you get motion sickness . no one really gets " motion sickness " just from moving , though . that is impossible because moving with constant velocity is physically the same as being stationary . what you get is " acceleration sickness " . you feel the bumpiness of a car ride . even then , it might more-accurately be called " jerk sickness " or " tidal sickness " . " jerk sickness " would be when your acceleration is changing over time , as in going over a bump . " tidal sickness " is when different parts of your body experience different accelerations at the same time . inside a rotating cylinder , what you feel would be identical to normal gravity to first order , so you would not necessarily get motion sick like in a car . if you get motion sick because you see the stars rotate when you look out the window , do not build any windows and you will never know . a few effects break this illusion of perfect gravity , including tidal forces , coriolis forces , and non-constant angular velocity of the spaceship . the bigger your ship , the smaller these effects will be . tidal forces arise when the gravitational field is not the same everywhere . to make tidal forces small , you need the ship to be big . the strength of the artificial gravity on your ship is $\omega^2 r$ with $\omega$ the angular frequency ( how fast the ship is rotating ) and $r$ the distance from the rotation axis . tidal accelerations are then on the order of $\omega^2\delta r$ , with $\delta r$ the size of the object we are considering ( e . g . your body ) . tidal forces compared to gravity are $\delta r / r$ , or the size of the object compared to the size of the ship . if you are 1000 times smaller than your ship , the tidal forces you feel will be 1000 times smaller than gravity - too small to notice . ( on earth they are much smaller still , of course , because you are so small compared to earth ) . coriolis forces are forces that makes tossed projectiles appear to bend when you throw them inside the ship , even though nothing is pushing on them . the size of the coriolis acceleration is on the order of $\omega v$ , with $v$ the speed of the thing being thrown . in order to make them smaller , you need to decrease $\omega$ . remember that the strength of gravity is $g = \omega^2 r$ , so $\omega = \sqrt{g/r}$ . the make the coriolis forces small , you need a big ship again . suppose you want coriolis forces to be under some fraction $\alpha$ of $g$ when you go at a certain maximum velocity $v$ . then the formula for the minimum size of the spaceship is $r &gt ; v/ ( \alpha^2 g ) $ . if $v = 10 {\rm m/s}$ and $\alpha = . 01$ , we need a spaceship 10 kilometers across . so we are going to have to tolerate fairly high coriolis acceleration . but remember you do not feel this at all if you are stationary . if you are moving it is just a constant force pushing up/down/left/right depending on which way you are going . only if different parts of you are moving different directions does it becomes a serious problem . your free throws would all be off , too . finally , when stuff moves around in the spaceship , the rotation rate of the entire ship could change because the stuff pushes on the ship . this acceleration would be on the order of $am/m$ , with $a$ the acceleration of the moving object , $m$ the mass of the object , and $m$ the mass of the ship . most things on the ship would have a tiny $m/m$ , so this would not be a big deal . if an object did not accelerate , but moved towards the center at a constant speed $v$ , the ship would accelerate like an ice skater accelerating as they pull their arms in . this acceleration would be on the order of $v^2m/rm$ , and again would be small for small objects . another potential problem is that if the mass distribution of the ship is not symmetric , the ship would wobble . suppose that the ship is balanced at some time , but then an object of mass $m$ is moved a distance $d$ along the side of the cylinder towards the end . then the frequency of the wobble should be on the order of $\omega md/mr$ and the amplitude should be on the order of $md/m$ . the jerk you feel would then be on the order of $\omega^3 m^2d^2/m^2 r$ , which should be small as long as it is a fairly small mass moving a small distance . how small can these effects be ? that relies mostly on the size of the ship , as we saw earlier . if the ship is going to simulate one earth gravity , it can not be too big . eventually , the tension in the sides of the ship would be so great the ship would tear itself apart . if your ship were a loop , that critical radius would be $t/\lambda g$ , with $t$ the tension and $\lambda$ the linear mass density of the wall . for carbon nanotubes this gets you a ship size of up to $10^7 {\rm m}$ , a figure so large ( bigger than earth ) that all effects could be made inconsequential . for steel it is about $5*10^3 {\rm m}$ , meaning that the effects on to human-sized might be in general $ . 001$ or $ . 0001$ of gravity .
that $c$ is the specific heat for the given cycle , i.e. $$dq=ncdt$$ this is for $n$ moles of gas . ( not the $n$ you stated in question ) i will assume $$pv^z=\text{constant}$$ $$ncdt=du+pdv$$ $$\int ncdt=\int nc_vdt+\int pdv$$ we will integrate it using pranjal 's method : $$nc\delta t=nc_v \delta t+\int \frac{pv^z}{v^z}dv$$ as numerator is a constant , take it out ! also note that $$p_iv_i^z=p_fv_f^z$$ $i = \text{initial}$ $f=\text{final}$ focusing on integral only , $$pv^z\int v^{-z}dv$$ $$pv^z\left [ \frac{v^{-z+1}}{-z+1}\right ] ^{v_f}_{v_i}$$ note that the $pv^z$ is same for initial and final step . so , we write multiply it inside and do this ingenious work : $$-\frac{p_iv_i^zv_i^{-z+1}}{-z+1}+\frac{p_fv_f^zv_f^{-z+1}}{-z+1}$$ $$-\frac{p_iv_i}{-z+1}+\frac{p_fv_f}{-z+1}$$ note that $pv=nrt$ $$\frac{nr\delta t}{-z+1}$$ where $\delta t=t_f-t_i$ final equation : $$nc\delta t=nc_v \delta t+\frac{nr\delta t}{-z+1}$$ $$c=c_v+\frac{r}{1-z}$$ this will bring you the original equation , you can find $c_v$ by $$c_p/c_v=\gamma$$ $$c_p-c_v=r$$ using $c_p=\gamma c_v$ , $$c_v\left ( \gamma-1\right ) =r$$ $$c_v=\frac{r}{\gamma-1}$$ substituting in original equation , $$c=\frac{r}{\gamma-1}+\frac{r}{1-z}$$
$p$ is defined as the projector onto a particular nonzero subspace $w$ of the vector space $v$ on which the representation acts . it is shown in the proof ( see eq . 1.40 ) that $w$ is an invariant subspace of the representation $d_2$ . since , by definition , an irreducible representation has no invariant subspaces except for $v$ and $\{0\}$ , we find immediately that $w=v$ .
glue one end of the rubber band to the bottom of the bottle ( on the inside ) and the other to the inside of the cap . ( it may be easiest to cut the bottle open for this and later tape it back together , but with sufficient dexterity and suitable tools , you could do this through the neck of the bottle . ) attach a weight to the middle of the rubber band , in such a way that the weight can not rotate around the rubber band without twisting it . make sure the band is short enough that the weight hangs suspended when the bottle is horizontal . when the bottle is horizontal , gravity will keep the weight below the rubber band , and will thus prevent the middle of the band from turning . thus , when you roll the bottle on the floor , the band will twist , storing energy . when you let go , the rubber band unwinds , making the bottle rotate backwards and thus roll back to where it started . of course , there is a limit to how much energy you can store this way . if you roll the bottle far enough , the tension in the rubber band will eventually overcome gravity and set the weight spinning ( or the band will snap ) .
the sounds that you hear in the kitchen have a frequency range of ( say ) 50 - 500 hz , with smatterings of higher frequencies ( dishes banging ) into the low khz . the speed of sound is approximately 300 m/s . this means that a 300 hz wave form has a wavelength of about 1 m . this in turn means that when that ( sound ) wave bounces off an object that is locally " smooth compared to 1 m " , it maintains coherence - the bits of sound bouncing off one part of the wall will remain in phase with bits of sound bouncing off another bit of wall . another way of looking at this : at any moment , you can consider every point of the wave front ( sound , light ) to be a series of point sources , all of which are in phase . as long as the dimension of openings etc . is small ( comparable to the wavelength ) , these wave will spread out - this is diffraction . you can see a nice image at http://homepages.ius.edu/kforinas/s/pics/diffraction1.jpg by contrast , visible light has wavelengths in the 400 - 700 nm range . thus photons bouncing off one bit of the surface will have a random phase relationship relative to other photons . the net result is that these photons ( waves ) will interfere constructively in just one specific direction ( like young 's diffraction experiment , they will interfere constructively when their phase difference is a multiple of $2\pi$ . ) in practice this means that the photons end up scattered , losing their spatial coherence . that is a fancy way of saying " they get so scrambled that they no longer form an image " . recognize that imaging requires that there is a 1:1 relationship between the origin of a photon ( did it come from the pan , or from the wife ) , and the direction in which it arrives at your eye . the lens in the eye makes sure that all photons that arrived from one direction end up on the same point on your retina . as long as all these photons came from the same source , that means that source becomes a " point " in the image you see . when the directions of the photons get scrambled , all you end up seeing is an " average intensity " of the light . think " shower door " , but worse . [ note the above is slightly simplified , assuming that the object you look at is at infinity . closer up there is a very small divergence of the light reaching different parts of your eye - this requires you to " focus " with your lens . that does not affect the basic principle in play here . ] this is why you can tell whether the light is on in the kitchen at the end of the hall , but you can not see what is going on - unless the walls are made of mirrors ( which are smooth on the scale needed for specular reflection ) as an aside - sound can also travel " through the wall" ; the pressure waves will cause slight motion of the wall which in turn causes waves on the other side ( although much reduced in amplitude because of the acoustic mismatch between the air and the wall ; this is where the " glass against the wall " so beloved in 50 's movies can help ) . in principle , electromagnetic waves can also travel through walls ( which is why your radio works indoors ) - but again , the short wavelength of light has two implications . one is , that the distance to cross is " many wavelengths " so that even a tiny extinction coefficient is sufficient to cause complete absorption ; and the short wavelength means that " everything " in the wall acts as a scattering point , and the light does not stand a chance of making it through in measurable quantities .
consider the simpler system of a mass in 2d , connected to the origin by a massless rod that is free to rotate about the origin . when you exert a force perpendicular to the rod on the mass , the mass exhibits circular motion . in this case , the centripetal force needed comes from the tension exerted on the mass by the rod . a similar situation happens for the roundabout : tensions in different parts of the roundabout act on each other to give the necessary centripetal force .
it is an incorrect picture to envision the cooper pairs as existing as an isolated occurrence in a lattice , since the very existence of cooper pairs depends on a supporting cast of other electrons . in his work , cooper showed that the ground state of a metal is unstable against an arbitrarily small net attraction between two electrons of opposite momentum , with this momentum near the fermi momentum of the system . therefore , the energetically favorable state is the paired state , and the superconducting state exists when all of the conduction electrons are bound into such pairs . further , something which is under-emphasized in the " wake model " is the size of the cooper pairs in " classical " superconductors ( ie , superconductors that have the pairing mechanism described by bcs theory ) . the spatial extent of cooper pairs is quite large , on the order of thousands of angstroms ( many , many lattice spacings ) , meaning that the pair condensation results in millions ( or more ) over lapping pairs . in effect , in the superconducting state , these millions of paired electrons travel like a superfluid through the material .
the answer is yes . the de broglie wavelengths of freely propagating particles ( i.e. . forget the influence of interactions and gravity perturbations , just consider the universe as a whole ) are redshifted by the expansion of the universe . another way of saying this is that their peculiar momenta with respect to a co-moving local volume decrease as the inverse of the scale factor . neutrinos are an example of a particle with a non-zero mass ( maybe of order 0.1 ev - see http://adsabs.harvard.edu/abs/2014phrvl.112e1303b ) . they decouple from the rest of the universe at about 1 second and freely propagate . the expansion then reduces their momenta to the extent that they should have a temperatures $&lt ; 2$k in the present-day universe , typical kinetic energies of 0.2 mev ( e . g . see http://adsabs.harvard.edu/abs/2010phrvd..82f2001k ) and may have speeds of only ( depending on their actual masses ) perhaps $\sim 10^3-10^4$ km/s and so are non-relativistic . electrons would behave in the same way , if they could be considered not to be strongly ( or rather electromagnetically ! ) interacting with other particles and photons . i do not think this can be satisfied except in the very early universe and a typical free , intergalactic electron in the present day universe has an energy of $\sim 0.1$ kev due to heating by radiation from stars and galaxies .
i would not say it was a lucky guess , it is simply a prediction of their assumptions . it is based on certain requirements for the value of the higgs self coupling at the planck scale which as far as i am aware translate into pretty much the same criteria that the higgs potential remains just stable up to the planck scale under renormalisation group effects . since the higgs mass has been measured , there have been many papers which have discussed the fact that this mass lies just at the low end of stabilising the potential up to the planck scale . however , what would appear to be the most accurate determination of this running including three loop pole matching effects etc is given here ( they also include a discussion of the paper you cite ) . their conclusion is that " while $\lambda$ at the planck scale is remarkably close to zero , absolute stability of the higgs potential is excluded at 98% c.l. for $m_h &lt ; 126$ gev " . i understand this to mean that were shaposhnikov and wetterich to repeat their analysis using these more advanced calculations , they would predict a higgs mass somewhat above 126 gev . this does not yet rule out their very interesting observation , and the higgs mass measurement certainly lends weight to the asymptotic safety program ( although somewhat diminished considering my above comments ) . is the prediction taken seriously ? probably not that much , but only because the asymptotic safety program ( maybe unfairly ) does not get that much attention . it is however a growing field and the authors of the paper are certainly very well respected physicists .
first , the displacement current is a result of time varying electric field ( not magnetic field ) , that is clear to see from amperes law : one way of interpreting the above equation is to say that the conduction current density ( the first term in right hand side ) and displacement current density ( the second term in right hand side ) generate a curly magnetic field . it is true that displacement current is not described by moving charges , but it was called a current because it generates the same phenomena as conduction current does , hence it is called a current . to understand how it works physically , let us think of a capacitor driven by time varying source as shown in the following figure . the left plate is connected to the positive terminal of a time varying voltage source , the right plate is connected to negative terminal or ground if you wish . the two plates are separated by a dielectric material that doesnt allow the charges to move into it . one way of interpreting the displacement current is , when the positive voltage terminal is biased , the positive charges ( conventionally ) are driven away from it . that means they get away ( move as current ) as far as they can , until they stop in the left plate of the capacitor where they see an open circuit . as positive charges accumulate at the left plate , they give rise to field e that penetrates the dielectric and is seen by positive charges in the right plate . the positive charges in the right plate are repelled by the electric field , so they move ( as current ) until they make it to the negative terminal or ground . hence the current in the circuit is continuous without the need of positive charges to penetrate the dielectric . their electric field did the job ! ! final two remarks , the first is , we know that electrons are the ones who move so the direction of motion is reversed , still the argument i explained earlier holds for electrons . the second is , if you make the separation between the plates so big such that the electric field at one side is not seen by the charges in the other plate , that is no longer a capacitor . it will act as an open circuit with means no current will flow what so ever ( that argument doesnt apply to very high frequencies ) . hope that helped
in the idealised case , the answer to this is slightly surprising . the fact that the mass of a rocket must include the mass of its fuel is embodied in the rocket equation , $$ \delta v = v_e \ln\frac{m_i}{m_f} , $$ where $m_i$ is the initial mass of the rocket ( including fuel , payload and everything else ) , and $m_f$ is the final mass , including the payload but much less fuel . $v_e$ is the effective exhaust velocity , which we might as well assume stays fixed for a given type of rocket , and $\delta v$ is essentially the velocity change required to reach escape velocity , which we will also assume stays constant . the above equation does not include the acceleration due to gravity , which is of course an important factor . this is because ( as is usually done ) it is included in the $\delta v$ term , which includes the velocity you lose to gravitational acceleration as the rocket ascends . you can put in the gravitational acceleration explicitly and the result does not change , as i will show below . rearranging the rocket equation gives us $$ m_i = m_f e^{\delta v/v_e} , $$ which tells us the amount of fuel ( the majority of $m_i$ ) we need to lift a mass $m_f$ . you can see that this is exponential in $\delta v$ , meaning that if we want to go a little bit faster we need a much bigger rocket . this is called " the tyranny of the rocket equation . " in this case we do not want to go faster , we just want to send more stuff , i.e. we want to increase $m_f$ . but the equation is not exponential in $m_f$ , it is linear . therefore if we ignore any changes in rocket design that would be needed to increase its size , we can conclude that if you want to double the payload , you only need to double the size of the rocket , not quadruple it . if we want to do this more precisely , we should include gravitational acceleration in the rocket equation . as per this answer by asad to another question , this gives us $$ \delta v = v_e ln \frac{m_i}{m_f} - g\left ( \frac{m_f}{\dot m}\right ) , $$ where $g$ is acceleration due to gravity and $\dot m$ is the rate at which fuel is burned , which we assume is constant over time . according to the reasoning in asad 's answer , we end up with $$ m_i = m_f \left ( \exp\left ( \frac{\delta v + g\left ( \frac{m_f}{\dot m}\right ) }{v_e}\right ) -1\right ) ^{-1} , $$ where $\delta v$ is now the true escape velocity rather than the effective escape velocity . in asad 's answer , he assumes that $\dot m$ stays constant as you change $m_f$ , and he concludes that there is a strong limit to the size of a rocket . but in fact if you were going to make a rocket twice the size , it would not make sense to keep $\dot m$ the same . to take it to an extreme , imagine building something the size of a saturn v that burns fuel at the same rate as a hobby rocket . it obviously would not be able to lift itself off the launch pad , and nobody would consider building such a design . so let 's instead assume that the burn rate is proportional to the size of the rocket . this means that $\frac{m_f}{\dot m}$ is a constant , and the equation as a whole is still of the form $$ m_i = m_f \times \text{a constant} , $$ so it is linear in $m_f$ . in fact none of this is really all that surprising after all , because if you want to send twice the mass you could always just use two rockets of the original size . by just strapping those rockets next to each other you have got one of twice the size that can send twice the payload . moreover , it burns fuel at twice the rate , just as i assumed above . there is no reason that would not work in principle . ( though in practice it would be another matter of course ! ) if the equation had been exponential in $m_f$ then there would have been a point at which increasing the payload mass would require an unreasonable amount of extra fuel , and that would have imposed a strong practical limit on rocket size . but since it is linear this does not really happen . the limits on rocket size are not due to an exponential increase in propellant mass , but to the engineering challenges in building a structure of that size and complexity that will not fail under the violent conditions of a rocket launch . these include factors to do with the way the strength of a structure scales with its size and ( i imagine ) practical issues involved in getting fuel where it needs to be at the right time . in this respect the factors that limit the size of rockets are quite similar to the factors that limit the size of buildings .
hints to the sought-for formula ( 16 ) for $\hat{h}$: use integration by parts in ${\bf r}$-space to remove derivatives from the dirac delta distributions , cf . comment by user acuriousmind . work on the problem from both ends ( 15 ) and ( 16 ) . use leibniz rule $$\tag{*}\nabla^2 ( fg ) ~=~ g\nabla^2 f + f \nabla^2 g+ 2 \nabla f\cdot\nabla g , $$ so that $\nabla$ only acts on single objects everywhere . let 's call the last term in eq . ( * ) for a ' cross-term ' . change the derivative $\nabla_{\bf r}~$ to $~\pm\frac{1}{2}\nabla_{\bf r}$ . perform the $\bf r$-integration . for cross-terms that act on $\psi\psi$ or $\bar{\psi}\bar{\psi}$ , integrate by parts in ${\bf r}$-space , so that there are only cross-terms that act on $\bar{\psi}\psi$ . compare !
you do not give any indication of your physics background so i will answer in simple terms . are they saying , for example , that a source which has a flux emanating from it can be treated as a point particle , even if it is spherical ? how can spatially extended objects behave like point particles ? a simple example is the solar system . to first order the force controlling the orbits is gravity . we have a simple gravitational equation about the gravitational field from a source and the position of a point particle in this field , an inverse square law . this assumes that the force from the source comes also from a point . this is possible if the distances examined are very much larger , orders of magnitude , than the dimensions of the masses producing the field . this is the case for the planets and the solar system . more so if one is thinking of stars . it is an approximation which works very well for calculating the orbits of the planets or the moon . the " point " used is the center of mass of the objects under study . even though the inverse square law holds for all gravitational attraction , the gravitational field differs when the dimensions are small with respect to a large mass generating the field . here are the differences of the earth 's gravitational field , for example , where one can no longer assume point locality of the source and would have to model the field by the map , integrating over a distribution of point sources from the earth . the fact that gravity is a very weak force for dimension of objects around us , also allows us to calculate from the center of mass of a falling object , a brick , or a ball , except that we have to take into account kinematics of rotation and of friction and lift , ( in case of a balloon ) : this will depend on the shape of the body and the type of mass and integrations will have to take place before a trajectory can be defined with small errors . all in all the center of mass is a good first order approximation to the behavior under gravitational fields . the same is true for charge . when far away from the dimensions of the field producing mass , the center of mass is a very good approximation . when one works close to the charged bodies one has to integrate over charge distributions . electromagnetism though is a stronger force and the charge distributions are more important than the gravitational distributions for similar masses . you could say that these distribution introduce a non locality , calculable though .
there are two important concepts here that explain the influence of gravity on light ( photons ) . the theory of special relativity , proved in 1905 ( or rather the 2nd paper of that year on the subject ) gives an equation for the relativistic energy of a particle ; $$e^2 = ( m_0 c^2 ) ^2 + p^2 c^2$$ where $m_0$ is the rest mass of the particle ( 0 in the case of a photon ) . hence this reduces to $e = pc$ . einstein also introduced the concept of relativistic mass ( and the related mass-energy equivalence ) in the same paper ; we can then write $$m c^2 = pc$$ where $m$ is the relativistic mass here , hence $$m = p/c$$ in other words , a photon does have relativistic mass proportional to its momentum . de broglie 's relation , an early result of quantum theory ( specifically wave-particle duality ) , states that $$\lambda = h / p$$ where $h$ is simply planck 's constant . this gives $$p = h / \lambda$$ hence combining the two results , we get $$m = e / c^2 = h / \lambda c$$ again , paying attention to the fact that $m$ is relativistic mass . and here we have it : photons have ' mass ' inversely proportional to their wavelength ! then simply by newton 's theory of gravity , they have gravitational influence . ( to dispel a potential source of confusion , einstein specifically proved that relativistic mass is an extension/generalisation of newtonian mass , so we should conceptually be able to treat the two the same . ) there are a few different ways of thinking about this phenomenon in any case , but i hope i have provided a fairly straightforward and apparent one . ( one could go into general relativity for a full explanation , but i find this the best overview . )
it seems that such a measure for mixed states is fundamentally impossible , since you can have both entangled and separable states which have exactly the local expectation values . for pure states , monogamy of entanglement ensures that the impurity of a reduced density matrix ( which can be infered from the expectation values of local pauli operators ) is directly related to entanglement . however for mixed states , this is not the case , as the following example will homefully make clear : consider a two qubit system , in which the two reduced density matrices are maximally mixed . in this case , it is possible the system is separable , composed of two copies of the maximally mixed state , or it is maximally entangled , composed of a single epr pair , or anything in between . thus no function of local expectation values can distinguish separable from entangled states in general . however , purity ( which is a function of single site correlation functions ) , can indeed be used as a bound on the entanglement of a system , again due to monogomy of entanglement . if the local system is not maximally mixed , then it is not maximally entangled , and hence maximum amount of entanglement possible for a system is a monotonic function of its ( im ) purity .
for su ( n ) , the adjoint representation can be obtained from a tensor product of the fundamental representation with its dual and projecting out the scalar . thus we can replace the adjoint representation indices $a , . . . $ by double indices $i \bar{j}$ and write this relation as : $$w^{i \bar{j}}_{k \bar{l}} = u^i_k u^{\dagger j}_l - \frac{1}{n} \delta^i_k \delta^j_l$$ ( the factor $\frac{1}{n}$ can be obtained by taking the traces in the case $u=\mathbb{1}$ ) . on the other hand , the generators of the lie algebra are the traceless hermitian matrices , which have the following form in the double index notation : $$ t^{i\bar{j}} = \frac{1}{\sqrt{2}} ( e^{ij} - \frac{\mathbf{1}}{n} \delta^{ij} ) $$ the prefactor consists of a standard normalization and $\mathbb{1}$ is the unit matrix in the fundamental representation . substituting the second equation into the required identity gives you the first equation .
let the electron ground state have energy $e_g$ , let the first excited state have energy $e_1$ , and let the second excited state have energy $e_2$ . let the energy of the photon be given by $e_p = hf$ . now it is not the energy of the exited states that is important in transitions , but the energy differences between states . so instead i will assume you mean the following condition : $ ( e_1 - e_g ) &lt ; hf &lt ; ( e_2 - e_g ) $ . now as the atom absorbs $hf$ , it causes the electron to transition to the first excited state . there is therefore excess energy of $hf - ( e_1 - e_g ) $ . this excess energy is either converted to kinetic energy in the atom , or is reflected as a new photon with lower frequency and energy $e ' = hf'$ .
here is the actual announcement of the redshift from andrew levan . for those unfamiliar with astronomy practices , circulars are generally where simple things like discoveries , first spectra , redshifts , etc . are announced just as soon as the data is gathered and a preliminary reduction is performed , usually issued the morning after the observation . in this case , the gemini multi object spectrograph ( gmos ) , attached to the $8\ \mathrm{m}$-class gemini-north telescope , was used to get a visible/near-uv spectrum of the afterglow the same day the burst was detected . absorption lines for calcium 1 and magnesium 2 were used to get a redshift of $0.34$ . so yes , it is optical astronomy pinning down the redshift . as to how this was done so fast , grb 's ( and other interesting transients ) are followed by many observatories , which have measures in place to slew to " targets of opportunity " the moment they are reported to get data before they vanish . for grb 's in particular , they are often detected in gamma rays first , but gamma ray " telescopes " are basically blocks of scintillating material sitting in space , and so they have terrible angular resolution . here , as is often the case , the swift satellite was triggered to search the general area with its x-ray instrument , getting a somewhat more precise sky position . then other facilities operating in other em bands were able to look for the object . this happens largely automatically over the course of minutes to hours . 1 specifically , the h and k lines were used . these are very common for getting redshifts of galaxies hosting distant transient phenomena . 2 $\mathrm{mg}~\mathrm{i}$ is atomic ; $\mathrm{mg}~\mathrm{ii}$ is singly-ionized . the latter has a nice , recognizable doublet at rest wavelengths of 2796.352  and 2803.530  , as described in this astrobites article . by the way , you will often see the doublet referred to with the numbers 2796/2803 rather than 2796/2804 , which is not an effect of truncation instead of proper rounding , but rather a leftover tradition from back when spectroscopy was reported in air wavelengths rather than vacuum ones .
why , after absorbing a photon does an atom 's electron ' fall ' back to its ground state ( what causes it to immediately lose its absorbed energy ) ? the answer by @davidmh gives our observations from classical physics , where we formulated the quantities " energy " , " potential " etc . we observed that this was so , an apple falls , and brilliant mathematics organized these observations into equations that can predict what will happen . so the real answer to a " why " in physics is " because we have observed that it does so " . physics then answers " how " this happens , by referring to the mathematical models constructed to describe how . now electrons and photons are not classical particles , they are quantum mechanical entities , working with different models of nature in a different framework . still , there is continuity in physics , and the concept of energy and potential energy has been retained , and at the limit classical emerges from quantum mechanical , the underlying layer . so the answer to " why " at the electron atom photon level is really " because that is what we have observed " . but we have a very good mathematical model to explain the " how " this happens , which is predictive of future behaviors also . this is as follows : a photon hitting an atom has a probability to interact with it , be absorbed , and raise the atom to a higher energy level . this probability can be calculated with the tools of quantum mechanics which also predict that once the electron is on a higher energy level , there exists a strong probability that it will fall back to the ground level emitting a photon with the appropriate energy . one can think of it with the heisenberg uncertainty principle , too . the higher energy level has a width , delta ( e ) which means that it has a lifetime delta ( t ) of existing , it is unstable . our models are very good and successful , and we tend to think that they answer " why " . for example in this case we will say " because the energy level has a width and from the hup it will have to decay " , but in truth , we are just answering " how " within our model this happens .
this is not correct : " since the relativistic lagrangian is transitionally invariant , there is no dependence on coordinates so the last term is zero . " in fact , $$ \frac{\partial l}{\partial {\bf r}} = q\nabla\left [ \frac{1}{c} \left ( {\bf u} \cdot {\bf a}\right ) - \phi \right ] = q\left\{\frac{1}{c} \left [ \left ( {\bf u} \cdot \nabla \right ) {\bf a} + {\bf u} \times \left ( \nabla \times {\bf a}\right ) \right ] - \nabla\phi\right\} . $$ setting this equal to $$ \frac{d}{dt}\left ( \frac{\partial l}{\partial {\bf u}} \right ) = \frac{d}{dt} \left [ \frac{\partial }{\partial {\bf u}}\left ( -m c^2 \sqrt{1 - \frac{u^2}{c^2}} + \frac{q}{c} {\bf u} \cdot {\bf a}\right ) \right ] = \frac{d}{dt}\left ( \gamma m {\bf u}\right ) +\frac{q}{c} \frac{d{\bf a}}{dt} , $$ and noting that $$ \frac{d{\bf a}}{dt} = \frac{\partial {\bf a}}{\partial t} + \left ( {\bf u} \cdot \nabla\right ) {\bf a} , $$ we get $$ \frac{d}{dt}\left ( \gamma m {\bf u}\right ) = q\left [ - \nabla\phi - \frac{1}{c}\frac{\partial{\bf a}}{\partial t} + \frac{{\bf u}}{c} \times \left ( \nabla \times {\bf a}\right ) \right ] . $$
i will address your point about why the integral is lorentz invariant , as from the comments to cduston 's answer , i think this is your sticking point : you can see the relation between a manifestly lorentz invariant form like this $$\int d^4p \frac{e^{-ipx}}{ ( p^2-m^2 ) } \ \ \ ( 1 ) $$ and the not-so-obviously lorentz invariant form $$ \int d^3p \frac{1}{e_{{\bf{p}}}}{e^{-ipx}} \ \ \ ( 2 ) $$ by using the identity $$\frac{1}{ ( p^2-m^2 ) }= \frac{1}{2e_{{\bf{p}}}}\{\frac{1}{ ( e_{{\bf{p}}}+p_0 ) }-\frac{1}{ ( e_{{\bf{p}}}-p_0 ) }\} $$ here $e_{{\bf{p}}} = \sqrt{{\bf{p}}^2+m^2}$ is the on-shell time component of the momentum four vector , and $p_0$ is the " generic " time component - not necessarily on-shell . if you substitute this in ( 1 ) and do the $p_0$ integral using the appropriate contour , you will get ( 2 ) . what is actually going on is explained in the discussion near equation ( 2.40 ) , you are doing a 4 momentum integral , but just restricting it to the mass shell using a delta function . restriction to a mass shell is a lorentz invariant operation , so you are maintaining lorentz invariance throughout ( even though with the three momentum integral it does not look like it ! ) .
yes , the density matrix reconciles all quantum aspects of the probabilities with the classical aspect of the probabilities so that these two " parts " can no longer be separated in any invariant way . as the op states in the discussion , the same density matrix may be prepared in numerous ways . one of them may look more " classical "  e.g. the method following the simple diagonalization from equation 1  and another one may look more quantum , depending on states that are not orthogonal and/or that interfere with each other  like equations 2 . but all predictions may be written in terms of the density matrix . for example , the probability that we will observe the property given by the projection operator $p_b$ is $$ {\rm prob}_b = {\rm tr} ( \rho p_b ) $$ so whatever procedure produced $p_b$ will always yield the same probabilities for anything . unlike other users , i do think that this observation by the op has a nontrivial content , at least at the philosophical level . in a sense , it implies that the density matrix with its probabilistic interpretation should be interpreted exactly in the same way as the phase space distribution function in statistical physics  and the " quantum portion " of the probabilities inevitably arise out of this generalization because the matrices do not commute with each other . another way to phrase the same interpretation : in classical physics , everyone agrees that we may have an incomplete knowledge about a physical system and use the phase space probability distribution to quantify that . now , if we also agree that probabilities of different , mutually excluding states ( eigenstates of the density matrix ) may be calculated as eigenvalues of the density matrix , and if we assume that there is a smooth formula for probabilities of some properties , then it also follows that even pure states  whose density matrices have eigenvalues $1,0,0,0 , \dots$  must imply probabilistic predictions for most quantities . except for observables ' or matrices ' nonzero commutator , the interference-related quantum probabilities are no different and no " weirder " than the classical probabilities related to the incomplete knowledge .
the claim is often that there is no condensation in $d&lt ; 3$ . the other answers are correct , but let 's be clear , there are actually two assumptions present in the claim : assume you have $n$ noninteracting bosons in $d$-dimensions in a hypervolume $l^d$ assume that these bosons have an energy-momentum relationship of $e ( p ) = ap^s$ . now , the way we calculate the critical temperature ( $1/\beta_c$ ) for bec requires satisfying the equation $$\int_0^\infty \frac{\rho ( e ) de}{e^{\beta_c e}-1}=n$$ where $\rho ( e ) $ is the density of states . whether this integral is convergent or not depends on the values of both $s$ and $d$ . the details of the proof are up to you though . : )
there is one simple reason : in such scenario the physics at the string scale has supersymmetry . supersymmetry ( more technically $n=1$ supersymmetry ) has some nice phenomenological features that make it an attractive bridge between low energy physics and string theory . the existence of this symmetry translates directly to the requirement that the compactification manifold is calabi-yau .
the middle galaxy in einstein 's cross has an elliptical mass distribution which is wider in the direction of the short leg of the cross ( originally , this said long leg ) , with a center of mass where you see the galaxy . the object is slightly to the right of the center of the ellipse , in the direction of the long leg of the cross ( the original answer had the direction reversed ) . this type of lensing is acheivable in such a configuration , when the lensing object is relatively close to us , so that the rays pass the central region , where the quadrupole moment asymmetry of the gravitational field is apparent . lensing map given a light source , call the line between us and the source the z axis , and parametrize outgoing light rays by the x-y coordinates of their intesection with an x-y plane one unit distance away from the source in our direction . this is a good parametrization for the tiny angles one is dealing with . the light rays are parametrized by a two dimensional vector v . these light rays then go through a lensing region , and come out going in another direction . call their intersection point with the x-y plane going through our position v ' . the lensing problem is completely determined once you know v ' as a function of v . we can only see those rays that come to us , that is , those rays where v' ( v ) is zero . the number and type of images are entirely determined by the number and type of zeros of this vector field . this is why it is difficult to reconstruct the mass distribution from strong-lensing--- many different vector fields can have the same zeros . the only thing we can observe is the number of zeros , and the jacobian of the vector field v ' at the zero . the jacobian tells you the linear map between the source and the observed image , the shear , magnification , inversion . the lensing map is always asymptotically linear , v' ( v ) = v for large v , because far away rays are not lensed , and the scale of v is adjusted to make this constant 1 . generic strong lensing in a generic strong lensing problem , the vector field v has only simple zeros . the jacobian is a diagonalizable matrix with nonzero eigenvalues . this means that each image is perfectly well defined , not arced or smeared . the image is arced only in the infinitely improbable case that you have a singular jacobian . but we see gravitational arcs all the time ! the reason for this is that for the special case of a spherically symmetric source , the jacobian is always singular . the source , the center of symmetry , and us make a plane , and this plane includes the z-axis , and necessarily includes the direction of the image . the jacobian at a zero of v ' always has a zero eigenvalue in the direction perpendicular to this plane . this means that the spherically-symmetric far-field of any compact source will produce arcs , or smears . when the lensing object is very far , the rays that get to us are far away from the source , and we see far-field arcs and smears . when the lensing galaxy is close , the lensing field has no special symmetry , and we see points with no smearing . so despite the intuition from point sources and everyday lenses , einstein 's cross is the generic case for lensing , the arcs and smears are special cases . you can see this by holding a pen-light next to a funhouse-mirror . generically , at any distance , you will see the pen light reflected at multiple images , but only near special points do you get smearing or arcing . topological considerations there is a simple topological theorem about this vector field v ' . if you make a large circle in the v plane , and go around it counterclockwise , the value v' ( v ) along this circle makes a counterclockwise loop once around . this is the winding number of the loop . you can easily prove the following properties of the winding number : every loop has a winding number if you divide a loop in two , the winding number of the two parts add up to the winding number of the loop . the winding number of a small circle is always 0 , unless the vector field is zero at inside the circle . together they tell you what type of zeros can occur in a vector field based on its behavior at infinity . the winding number of the vector field in a small circle around a zero is called its index . the index is always +1 or -1 generically , because any other index happens only when these types of index zeros collide , so it is infinitely improbable . i will call the +1 zeros " sources " although they can be sources sinks or rotation/spiral points . the -1 zeros are called " saddles " . the images at saddles are reflected . the images at sources are not . these observations prove the zero theorem : the number of sources plus the number of saddles is equal to the winding number of a very large circle . this means that there are always an odd number of images in a generic vector field , and always one more source than saddle . a quick search reveals that this theorem is known as the " odd number theorem " in the strong lensing community . the odd number paradox this theorem is very strange , because it is exactly the opposite of what you always see ! the generic images , like einstein 's cross , almost always have an even number of images . the only time you see an odd number of images is when you see exactly one image . what is the deal ? the reason can be understood by going to one dimension less , and considering the one-dimensional vector field x' ( x ) . in two dimensions , the light-ray map is defined by a zeros of a real valued function . these zeros also obey the odd-number theorem--- the asymptotic value of x' ( x ) is negative for negative x and positive for positive x , so there are an odd number of zero crossings . but if you place a point-source between you and the object , you generically see exactly two images ! the ray above that is deflected down , and they ray below that is deflected up . you do not ever see an odd number . how does the theorem fail ? the reason is that the point source has tremendously large deflections when you get close , so that the vector field is discontinuous there . light rays that pass very close above the point are deflected very far down , and light rays that pass very close below are deflected far up . the discontinuity has a +1 index , and it fixes the theorem . if you smooth out the point source into a concentrated mass distribution , the vector field becomes continuous again , but one of the images is forced to be right behind the continuous mass distribution , with extremely small magnification . so the einstein cross has five images : there are four visible images , and one invisible images right behind the foreground galaxy . this requires no fine tuning--- the fifth image occurs where the mass distribution is most concentrated , which is also where the galaxy is . even if the galaxy were somehow transparent , the fifth image would be extremely dim , because it is where the gradient of the v field is biggest , and the smaller this gradien , the bigger the magnification . einstein 's cross after analyzing the general case , it is straightforward to work out qualitatively what is happening in einstein 's cross . there is a central mass , as in all astrophysical lenses , so there is an invisible central singularity/image with index +1 . the remaining images must have 2 sources and 2 saddles . the most likely configuration is that the two sources are the left and right points on the long leg of the cross , and the two saddles are the top and bottom points ( in my original answer , i had the orientation backwards . to justify the orientation choice , see the quantitative analysis below ) you can fill in the qualitative structure of the v' ( v ) vector field by drawing its flow lines . the image below is the result . it is only a qualitative picture , but you get to see which way the light is deflecting ( i changed the image to reflect the correct physics ) : the flow lines start at the two sources , and get deflected around the two saddles , with some lines going off to infinity and some lines going into the central singularity/sink . there is a special box going around source-saddle-source-saddle which cuts the plane in two , and inside the box , all the source flows end on the central singularity/image and outside all the source flows end at infinity . the flow shows that the apparent fourfold symmetry is not there at all . the two sources are completely different from the two saddles . the direction of light deflection is downward towards the long axis of the cross , and inward toward the center . this is the expected deflection from a source which is elliptical oriented along the long-direction of the galaxy . model ( the stuff in this section was wrong . the correct stuff is below ) general astrophysical lensing the general problem is easy to solve , and gives more insight into what you can extract from strong lensing observations . the first thing to note is that the deflection of a particle moving at the speed of light past a point mass in newton 's theory , when the deflection is small is given by the integral of the force over a straight line , divided by the nearly constant speed c , and this straightforward integral gives a deflection which is : $$ \delta\theta = -{r_s\over b}$$ where $r_s = {2gm\over c^2}$$ is the schwartschild radius , b is the impact parameter , the distance of closest approach , and everything is determined by dimensional analysis except the prefactor , which is as i gave it . the general relativity deflection is twice this , because the space-space metric components contribute an equal amount , as is easiest to see in schwartschild coodinates in the large radius region , and this is a famous prediction of gr . when the deflections are small , and they always small fractions of a degree in the actual images , the total deflection is additive over the point masses that make up the lensing mass . further , the path of the light ray from the distant light source is only near the lensing source for a very small fraction of the total transit , and this lensing region is much smaller than the distance to us , or the distance between the light source and the lensing mass . these two observations mean that you can squash all the material in the lensing mass into a single x-y plane , and get the same deflection , up to corrections which go as the ratio of the radius of a galaxy to the distance from us/source to the galaxy , both of which are safely infinitesimal . the radius of a galaxy and dark-matter cloud is a million light years , while the light source and us are a billion light years distant . you convert $\delta\theta$ to x-y plane coordinates i am using by multiplying by a unit distance . this gives the amount and direction of deflection from a given point mass . the total deflection of the light ray at distance b is given by the sum over all point masses in the galaxy and its associated dark-matter of this vector contribution , which is four time the mass ( twice the schwartschild radius ) divided by the distance , pointing directly toward the mass . this sum is $\delta v$ . what is important to note is that this sum is equal to the solution to a completely different problem , namely the 2-d gravitational field of ( four times ) the squashed planar mass . in 2d , gravity goes like 1/r . the planar gravitational field of the planar mass distribution gives $\delta v$ , and it is most important to note that this means that $\delta v$ is the gradient of the 2d gravitational potential : $$ \delta v= - \nabla \phi$$ where $$\phi ( x ) = \int \rho ( u ) \ln ( |x-u| ) d^2u$$ where the two dimensional density $\rho ( u ) $ is the integral of the three dimensional density in the z direction ( times $4g\over c^2$ ) . this is important , because you can easily determine $\phi$ from the mass distribution by well known methods for solving laplace 's equation in 2d , and there are many exact solutions . the impact parameter b is equal to $vr_1$ , the original direction the light ray goes times the distance from the light-source to the lensing object , and the position this light ray reaches when it gets to us is : $$ v' ( v ) = v ( r_1 + r_2 ) + \delta v ( vr_1 ) r_2$$ choosing a new normalization for v so that vr_1 is the new v , and choosing a normalization for v ' so that v' ( v ) is v at large distances : $$ v' ( v ) = v - {r_1\over r_1+r_2} \nabla \phi ( v ) $$ this is important , because it means that the whole thing is a gradient , the gradient of : $$ v' ( v ) = - \nabla ( \phi' ( v ) ) $$ $$\phi' ( v ) = {r_1\over r_1+r_2} \phi ( v ) - {v^2\over 2} $$ the resulting potential also has a 2d interpretation--- it is the gravitational potential of the planar squashed mass distribution in a newton-hooke background , where objects are pushed outward by a force proportional to their distance . the 2-d gravity potential is easy to calculate , often in closed form , and to find the lensing profile , you just look for the maxima , minima , and saddles of the 2-d potential plus a quadratically falling potential . this solves the problem for all practical astrophysical situations . i found it remarkable that the deflection field is integrable , but perhaps there is a simpler way of understanding this . point mass the 2d potential of a point mass is $$ \phi ( v ) = \ln ( |v| ) $$ and for an object directly behind it , you get $$ \phi' ( v ) = a \ln ( |v| ) - |v|^2$$ this gives a central singularity ( or if you spread out the mass in the center , a dim image right on top of the mass ) plus a perfect ring where $r=\sqrt{a}$ . this is the ring image . moving the light source off center just shifts the relative position of the two potential centers . the new potential is : $$ \phi' ( v ) = {a\over 2} \ln ( x^2 + y^2 ) - { ( x-a ) ^2 + y^2\over 2}$$ setting the x and y derivatives of the potential to zero , you find two critical points ( not counting the singular behavior at x=y=0 ) . the two points both have a singular jacobian , so they give very large magnifications , and smears or arcs . the two images occur at $$y=0$$ , $$x= {a\over 2} \pm \sqrt{a^2 - {a\over 2}^2}$$ so the smear to the side where the object is at is moved further , at large values of a , the second image is right on top of the lesing mass , and at small values of a , the two images are moved in the direction of the displacement by half the amount of displacement . quadrupole mass distribution consider two masses of size {1\over 2} at position $\pm a$ . this gives a potential which is a superposition of the two masses : $$ \phi ( x , y ) = {1\over 4} \ln ( ( x-a ) ^2 + y^2 ) + {1\over 4} \ln ( ( x+a ) ^2 + y^2 ) = {1\over 2} \ln ( r^2 ) + a^2 { x^2 - y^2\over 2r^2}$$ the part in addition to the ordinary $m\ln ( r ) $ potential of a point source is a quadrupole . lensing in a quadrupole has a simple algebraic solution . differentiating , and subtracting the linear part gives $$ {ax\over r^2} ( 1 + {a^2\over r^4} ( 6y^2 - 2x^2 ) - {r^2\over a} ) =0$$ $$ {ay\over r^2} ( 1 + {a^2\over r^4} ( 2y^2 - 6x^2 ) - {r^2\over a} ) =0$$ the x=0 , y=0 point is at the singular position . the real critical points are at the other simultaneous solutions : $$ x=0 , y= \pm\sqrt{a}\sqrt{{1\over 2} \pm \sqrt{{1\over 4} + {2a^2\over a}}}$$ $$ y=0 , x= \pm\sqrt{a}\sqrt{{1\over 2} \pm \sqrt{{1\over 4} - {2a^2\over a}}}$$ of these eight points , two are imaginary ( taking the minus sign inside the square root for y ) , and two are outside the domain of validity of the solution ( taking the minus sign inside the square root for x--- the point is $\sqrt{2}a$ ) , which is right by the point masses making the quadrupole ) . this leaves four points . but they are all local maxima , none of these are saddles . the saddles are found by solving the nontrivial equations in parentheses for x and y . taking the difference of the two equations reveals that $x=\pm y$ , which gives the four saddle solutions : $$ \pm x=\pm y = \sqrt{a}$$ there are eight images for a near-center source lensed by a quadrupole mass . for small values of a , the two images along the line of the two masses are pulled nearer by a fractional change which is $a^2\over a$ , the two images perpendicular to the line of the two masses are pulled apart by a fractional change of $a^2\over a$ , while the four images on the diagonals are at the location of the point-source disk . for me , this was surprising , but it is obvious in hindsight . the quadrupole field and the newton-hooke fields both point along the lines y=x on the diagonal , and their goes from inpointing near the origin to outpointing far away , so it must have a zero . the zeros are topological and stable to small deformations , so if you believe that the field of the galaxy is spherical plus quadrupole , the einstein cross light source has to be far enough off-center to change the topology of the critical points . quadrupole mass distribution/off-center source to analyze moving off center qualitatively , it helps to understand how saddles and sources respond to movement . if you move the light source , you move the newton-hooke center . the result is that the points that were previously sources and saddles now have a nonzero vector value . when the position of a source slowly gets a nonzero vector value , that means that the source is moving in the direction opposite this value . if a saddle gets a nonzero value , the saddle is moving in the direction of this value reflected in the attracting axis of the saddle . this means that if you start with a very asymmetric quadrupole , and you slide the source along the long-axis of the source-saddle-source-saddle-source-saddle-source-saddle ellipse towards one of the sources at the end of the long axis , one of the short axis sources and the short axis saddles approach each other . they annihilate when they touch , and they touch at a finite displacement , since the result must smoothly approach the spherically symmetric solution . right after the sources and saddles annihilate , you get a cross , but it does not look too much like einstein 's cross--- the surviving two saddles and two sources are more asymmetric , and the narrow arm is much narrower than the wide arm . line source for the lensing from a line source , you write the 2-d potential for a line oriented along the y-axis ( it is the same as a plane source in 3d , a point source in 1d , or a d-hyperplane source in d+1 dimensions--- a constant field pointing toward the object on either side ) : $$ \phi ( x ) = b|x|$$ and subtract off the newton-hooke source part , with a center at $x=a$ . $$\phi' ( x ) = b|x| - {1\over 2} ( ( x-a ) ^2 + y^2 ) $$ the critical points are on the y-axis by symmetry , and they are very simple to find : $$ y=0 , x= b+a$$ $$ y=0 , x= -b+a$$ these are the two images from a long dark-matter filament , or any other linear extended source . cosmic strings give the same sort of lensing , but the string-model of cosmic strings give ultra-relativistic sources which produce a conical deficit angle , and are technically not covered by the formalism here . but the result is the same--- doubled images . if you spread out the line source so that it is a uniform density between two lines parallel to the y-axis ( this would come from squashing a square beam of uniform mass density into a plane ) , the lensing outside the two lines is unaffected , by the 2-d gauss 's law . the interior is no longer singular , and you get a third image , as usual , at x=y=0 . elongated density plus point source the next model i will consider is a string plus a point . this is to model an elongated mass density with a concentration of mass in the center . the far-field is quadrupolar , and this was analyzed previously , but now i am interested in the case where the mass-density is comparable in length to the lensed image , or even longer . spreading out the string into a strip does nothing to the lensing outside the strip , and spreading out the point to a sphere also does nothing to the lensing outside the sphere , so this is a good model of many astrophysical situations , where there is an elongated dark matter cloud , perhaps a filament , with a galaxy concentrated somewhere in the middle of the filament . the 2-d potential , plus on-center newton hooke is $$ \phi' ( x ) = {a\over 2} \ln ( x^2 + y^2 ) + b|x| - {x^2 + y^2\over 2}$$ the solution to the critical point equations give images at $$ y=0 , x= {b\over 2} + \sqrt{a + ( {b\over 2} ) ^2}$$ . $$ y=0 , x= -{b\over 2} - \sqrt{a + ( {b\over 2} ) ^2}$$ where one of the two solutions of each quadratic equation is unphysical . this lensing is obvious--- it is the same as the string because the light-source is right behind the center mass . looking along the string itself , there are two more critical points : the x-direction field becomes zero ( it is singular for an infinitely narrow string , but ignore that ) , and the gradient of the potential is in the y direction , by symmetry , and for y near zero , it is inward pointing , and for large y it is outward pointing , so there is a critical point . the string potential has a minimum on the string , so in the x-direction you have a minimum , but the newton hooke potential is taking over from the point source potential at the critical point , so in the y-direction these two points are potential maxima . these are two critical points . the two critical points are at : $$ x=0 , y=\pm\sqrt{a}$$ and this is very robust to thickening the string and the point into strips/spheres , or blobs , so long as the shape is about the same . this is a generic source-saddle-source-saddle cross . in the string case , the two saddles become infinitely dim , because the jacobian blows up , but in the physical case where the thickness of the string is comparable to the lensing region , the jacobian is of the same order for the sources and the sink . moving the light-source off center towards positive x , perpendicular to the string orientation pushes the left source inwards , the right point outward , and the two saddles back and out . this is exactly the einstein cross configuration . point/strip --- best fit consider a strip of dark matter which is as wide or wider than the lensing configuration , with a point galaxy in the middle . this gives the lensing potential : $$\phi' ( x , y ) = {a\over 2} \ln ( x^2 + y^2 ) + {b\over 2} x^2 - { ( x-a ) ^2 + y^2\over 2}$$ valid inside the strip . outside the strip , instead of quadratic growth , the potential grows linearly , like for the string . the strip is more useful , because it is simultaneously the simplest elongated model to solve for an off-center object , and also the most accurate einstein cross . the parameter a tells you how far to the right of center the light-source is . the equations for the critical points are : $$ x ( {a\over r^2} - ( 1-b ) ) + a = 0$$ $$ y ( {a\over r^2} - 1 ) =0$$ there are two solutions when y=0 , at $$ x= {a\over 2 ( 1-b ) } \pm \sqrt{ { ( a/2 ) ^2\over ( 1-b ) ^2} +a^2}$$ these are the two sources , on the x axis , as in the string-point problem . there are two additional solutions when ${a\over r^2 -1}=0$ , and these are at $$ x= {-a\over b} , y = \pm \sqrt{a-{a^2\over b^2}}$$ and these are the usual saddles of line-string lensing . for a small a , the two saddles move to the right of the symmetry line , and the long-arm of the cross moves to the right . this is a perfect fit to einstein 's cross . to see how good a fit it is , look at the following plot of the lensing produced by $$ \phi ' = \ln ( x^2 + y^2 ) - { . 9* ( x- . 04 ) ^2 + y^2\over 2}$$ the black circle is the center of symmetry of the point/strip , the cross next to it is the true position of the quasar , and the four crosses are the locations of the critical points , while the contour-line density on the saddles/sources tell you the inverse brightness . this matches the data perfectly . summary the quadrupole lensing has a hard time reproducing einstein 's cross exactly , although it can get cross-like patterns . the reason is the eight images for an on-center light source . this means that to get a cross , two saddle-source pairs have to annihilate . once they do , the remaining saddles and source are not in such a nice cross , they tend to be too close together , not spread out nicely like the image . the quadrupole crosses are already approaching the asymptotic spherical limit , where the saddles and sources become the degenerate spherical arcs . the brightness of the saddles and sources are not approximately the same , the brightness of the far image on the long-leg of the cross is not approximately the same as the brightness of the near image , it is not a good model . this means that we should consider dark-matter around the galaxy extending in an elongated ellipse , the elongation is along the short-leg of the cross . the light-source is slightly to the right of center . this reproduces einstein 's cross exactly . this is almost surely the orientation of the dark-mass distribution in the galaxy , but the details of the distribution are not revealed just from the critical points , which is all strong lensing provides .
i see you edited the question from a sauna to what i assume is a insulated test chamber . steam would have very little change in the voltage required to getting a spark . you would also need more than just a bare wire , you would need a grounding plate inside your chamber . the voltage would primarily be a function of the air gap separating your wire and your ground . any ionized gases would assist in lowering that voltage threshold , whereas water vapor would probably not change the conductivity much at all . for your second question , steam in an acoustic standing wave probably would not have a noticeable effect . you really need a gas that can conduct . in plasma physics this is called an ion pressure wave . these waves are used to excite and conduct the plasma usually between two electric plates like your wire and ground . from the comments i think you want the current to flow along the direction of the wave like a wire , but it actually would flow perpendicular to it . this is where the gas gets compressed increasing the amount of free electrons available for conduction .
if by highest , you mean temperature ( proportional to mean kinetic energy of the particles ) , then the plasma state is " higher " than the other states you list . i think that there are other " higher " states of matter . for example , when it becomes energetically favorable for protons and electrons to combine into neutrons , you get a state called " neutron degenerate matter " . ( by the way , have you ever read " dragon 's egg " ? ) an even " higher " state would be qcd matter , e.g. , quark-gluon plasma .
this is a question with a large answer , so i will do my best to summarise the key points and provide links ( in-text ) to help answer it . thinking about this question and doing some reading ( to remind myself ) , the particular aspect ( in my understanding ) you are referring to are air masses , which according to this university of illinois ww2010 website is defined as : an air mass is a large body of air that has similar temperature and moisture properties throughout . the best source regions for air masses are large flat areas where air can be stagnant long enough to take on the characteristics of the surface below . further information is included in the slides of " air masses and fronts " . note , the homogeneity of temperature and moisture is true for the horizontal plane , in regards to your question - the vertical homogeneity related to the breezes that we feel at the surface is affected by many factors , explained in the chapter " chapter 12: atmospheric boundary layer " the lowest atmosphere is far from being a simple system . its physics include a diurnal component ( typically convection during the day and stratification at night ) , complications due to complex terrain ( surface elements such as buildings , forests , hills and mountains ) and larger weather events ( replacement of air masses by prevailing winds ; clouds and precipitation ) . the atmospheric boundary layer occupies about the lower 1km of the atmosphere ( it can vary depending on latitude ) where the atmosphere has the most contact with and is influenced most by the surface , through mainly thermal and mechanical means , explained by the reference above : the mechanical contact arises from the friction exerted by the wind against the ground surface ; this friction causes the wind to be sheared and creates turbulence . in the absence of thermal processes , i.e. when the abl is said to be neutral , we expect a logarithmic velocity profile . a diagram of the diurnal and dynamic nature of the atmospheric boundary layer is shown below : source ( and further information ) : " atmospheric boundary layer structure " ( piironen , 1996 ) . further information about the above can be found in " characteristics of air parcels and masses " so , the breeze you feel may only extend metres to kilometres , depending on the factors discussed above .
( 1 ) commutators are simply how two qm operators acting on $|q\rangle$ behave when acting in opposite orders ( i.e. . , is $\hat{a}\hat{b}|q\rangle=\hat{b}\hat{a}|q\rangle$ or no ) . thus , your question is largely unanswerable because it depends entirely on $|q\rangle$ , $\hat{a}$ and $\hat{b}$ . ( 2 ) uncertainty can be derived from the commutators . denoting $\langle a\rangle$ as the expectation value of the operator $\hat{a}$ on $|q\rangle$ , then your uncertainty for non-commuting operators $\hat{a}$ and $\hat{b}$ ( that is , $ [ \hat{a} , \hat{b} ] =i\hat{c}$ ) is $$ \left ( \langle a^2\rangle-\langle a\rangle^2\right ) ^{1/2}\left ( \langle b^2\rangle-\langle b\rangle^2\right ) ^{1/2}\geq \frac{1}{2}\left|\langle c\rangle\right| $$ usually one writes $\left ( \delta a\right ) ^2=\langle a^2\rangle-\langle a\rangle^2$ to get it in the normal form . so , again , this question entirely depends on $\hat{a}$ , $\hat{b}$ , and $|q\rangle$ . ( 3 ) i am not sure what you are asking here .
you might want to have a look at does light induce an electric current in a conductor ? . it is probably impossible for a radio aerial to emit visible light as the frequency of light is around the plasma frequency of the metal that the aerial is made of . we are not really supposed to address hypothetical questions , but if you could find some material with a high enough plasma frequency ( remember this applies to everything in the circuit ) then the emitted light would just be light . you had see the aerial glowing .
yes you could . since the force on an object from drag is given by $$f_d = \frac{1}{2}\rho v^2 a c_d$$ where $c_d$ is the drag coefficient , then all you would need to know are your velocity ( $v$ ) , your fluid density $\rho$ , your cross sectional area ( $a$ ) and the force of gravity on the body , which would then be equivalent to $f_d$ since the body would have no net force on it . thus , you could isolate for $c_d$ , getting $$c_d = 2mg/\rho v^2 a$$
since in the limit of weak gravitational fields , newtonian gravitation should be recovered , it is not surprising that the constant $g$ appears also in einstein 's equations . using only the tools of differential geometry we can only determine einstein 's field equations up to an unknown constant $\kappa$: $$g_{\mu\nu} = \kappa t_{\mu\nu} . $$ that this equation should reduce to the newtonian equation for the potential $\phi$ , $$\nabla^2 \phi = 4\pi g\rho \tag{1}$$ with $\rho$ the density fixes the constant $\kappa = \frac{8\pi g}{c^4} . \tag{2}$ in detail , one assumes an almost flat metric , $g_{\mu\nu} = \eta_{\mu\nu} + h_{\mu\nu}$ where $\eta_{\mu\nu}$ is flat and $h_{\mu\nu}$ is small . then from writing down the geodesic equation one finds that if $h_{00} = 2\phi/c^2$ , one obtains newton 's second law , $$\ddot{x}^i = -\partial^i \phi . \tag{3}$$ using ( 3 ) and taking $t_{\mu\nu} = \rho u_\mu u_\nu$ for a 4-velocity $u_\mu$ with small spatial components , the $00$ component of the field equations ( 2 ) is $$2\partial^i \partial_i \phi /c^2 = \kappa \rho c^2 . $$ in order to match this with ( 1 ) , we must have $\kappa = \frac{8\pi g}{c^4}$ . ( the detailed calculations here are , as is often the case in relativity , rather lengthy and boring , so they are omitted . )
as newton did in his principia mathematica with an example similar to yours , imagine what happens over a short time $dt$ to the velocity of the mass . using $f = m\ , dv/dt$ gives $dv = f\ , dt/m$ the force is at right angles to $v$ and so $dv$ is at right angles to , rather than along $v$ . do the same for the next $dt$ and so on , adding up all the contributions of $dv$ for one complete revolution and you will get $\sum dv = 0$ . the additional wobble to the circular motion becomes zero as $dv \rightarrow 0$
it seems that op just wants to trace a sign convention in a specific book ( ref . [ js ] ) . to answer this most convincingly , we should document where we read what . in the case of canonical coordinates ( also known as darboux coordinates ) , [ js ] uses a convention where the positions $q^i$ are ordered before the momenta $p_i$ , $$\xi^i~=~ ( q^1 , \ldots , q^n , p_1 , \ldots p_n ) , $$ cf . e.g. p . 215 in [ js ] . on top of p . 230 in [ js ] is written : the elements $\omega_{jk}$ of the symplectic form and the $\omega^{jk}$ of the poisson bracket [ . . . ] are the matrix elements of $\omega$ and $\omega^{-1}$ , respectively . since we are only after a sign convention , let us for simplicity restrict to canonical/darboux coordinates . then $\omega^2=-1_{2n}$ , and hence the difference between $\omega$ and $\omega^{-1}$ boils down to a sign . on p . 216 eqs . ( 5.42b ) and ( 5.43 ) read $$ \tag{5.42b} \omega_{\ell j} \dot{\xi}^j~=~ \partial_{\ell} h , $$ $$ \tag{5.43} \omega ~=~\begin{bmatrix} 0_n and -i_n \cr i_n and 0_n \end{bmatrix} . $$ on p . 218 eq . ( 5.47 ) defines the poisson bracket $$ \tag{5.47} \{f , g\} ~\equiv~ ( \partial_jf ) \omega^{jk} ( \partial_kg ) ~\equiv~ \frac{\partial f}{\partial q^{\alpha}}\frac{\partial g}{\partial p_{\alpha}}-\frac{\partial f}{\partial p_{\alpha}}\frac{\partial g}{\partial q^{\alpha}} , $$ leading to hamilton 's equation of motion ( 5.50 ) , $$ \tag{5.50} \dot{\xi}^j~=~ \{\xi^j , h\} . $$ we conclude that [ js ] has the convention that $$ \omega^{-1} ~=~\begin{bmatrix} 0_n and i_n \cr -i_n and 0_n \end{bmatrix} . $$ so far so good . on p . 228 is written $$ \tag{5.75} \omega ~=~ dq^{\alpha} \wedge dp_{\alpha} . $$ comparing with $\omega$ and $\omega_{ij}$ , we conclude that [ js ] has the convention that $$ \omega ~=~ \frac{1}{2} \omega_{ij} d\xi^j \wedge d\xi^i ~=~ -\frac{1}{2} \omega_{ij} d\xi^i \wedge d\xi^j . $$ note the opposite ordering of $i$ and $j$ ! this is probably the point where op and many others instead would have liked to defined it oppositely as $$\tag{opposite js} \omega ~=~ \frac{1}{2} \omega_{ij} d\xi^i \wedge d\xi^j , $$ and $$\tag{opposite js} \omega ~=~ dp_{\alpha} \wedge dq^{\alpha} . $$ references : [ js ] jose and saletan , classical dynamics : a contemporary approach , 1998 .
there are two misconceptions present in your explanation of the problem . $n$ is not number of dipoles , but their volumetric density $q$ is not total charge , but equivalent charge at boundaries of the dielectric . the idea is that ( a ) dielectric of the area $a$ and height $l$ polarized homogeneously along its height and ( b ) two plan-parallel plates of the area $a$ , distanced by $l$ and with charges $n a p$ and $-n a p$ produce macroscopically the same electric field ( $n$ is volumetric density and $p = q s$ is polarization of one dipole ) . this effect can be relatively simply understood if you imagine that you have charges of volumetric density $n$ homogeneously distributed all along material . initially positive and negative charges are on the same positions , all material is electrically neutral and polarization equals zero ( picture left ) . now you pull all positive charges for $s/2$ up and all negative charges for $s/2$ down ( picture right ) , so you actually get total dipole moment $p&#39 ; = n v p = n a l q s$ . figure : red = positive charge , blue = negative charge , violet = neutral . what is the effect of such movements ? the bulk of dielectric material remains neutral in terms of charge , but you do get excess charge $q = n a s q$ at the top and excess charge $-q = -n a s q$ at the bottom of the dielectric ( $a s$ is the volume at the top or bottom where only one type of charge is present ) . the point of this simple derivation is that surface charge density $\sigma = \frac{q}{a} = n s q$ equals polarization volumetric density $p = \frac{p&#39 ; }{a l} = n q s$ , i.e. $\sigma = p$ . ( polarization density is by definition total dipole moment of the dielectric divided by its volume . )
what you refer to is probably to ground state of a infinite potential wall . if this is the case , then there , in the ground state , the particles are not localized . you can find the solution of the orthogonal ground-states here : https://en.wikipedia.org/wiki/infinite_potential_well we can reduce the problem to a one dimensional case , that does not change the problem or solution to much . with orthogonal states is meant that the integral over the wave function vanishes . so in the case of a one dimensional box this is fulfilled for the given sinus functions . that does not imply that the two particles may not be found at the some position in space . if that would be the case , the electrons would have to be localized . but this is not the requirement for orthogonal states . i hope this helps to clarify this .
as everyone else is saying , if you assume newton 's law of cooling : $$ \dot q = m c_p \dot t = h a \delta t $$ the equation for how you heat or cool is an exponential $$ t ( t ) = t_\infty + \delta t e^{ -\frac{ha}{mc_p} t } $$ the rate constant for growth ( or dying ) of temperature is the same ( assuming other details of the material do not change much ) , so the half-life is the same regardless of the differences in temperatures or goal temperatures , but this does not mean you have to wait the same amount of time . as is the nature of half lives . i think the best explanation would be a visual one : here i am showing how the temperature would evolve with some made up , yet practical chosen values . if you like , ignore the actual numbers on the scales as they are not that important . what is important is you will notice that for the red curve i " took it out of the fridge " for 1 hour , and then put it back and it is not until another 6 hours later that is 41 degrees again . but , for the purple curve , which was out of the fridge long enough to nearly come to room temperature ( 8 hrs ) , it takes just as much time to cool down as it took to heat up .
the gauge covariant derivative is given by \begin{equation} d _\mu \psi = \partial _\mu \psi - i g w _\mu ^a t _a \psi \end{equation} rearranging gives , \begin{equation} i g w _\mu = \partial _\mu \psi - d _\mu \psi \end{equation} where we write $w_\mu \equiv w_\mu^a t _a $ . under a gauge transformation we have , \begin{align} i g w _\mu \psi and \rightarrow \partial _\mu \left ( l ( x ) \psi \right ) - l ( x ) d _\mu \psi \\ i g w ' _\mu l\psi and = ( \partial _\mu l ) \psi + l \partial _\mu \psi - l d _\mu \psi \\ i g w ' _\mu \psi ' and = ( \partial _\mu l ) l ^\dagger l \psi + l w_\mu l^\dagger l \psi \\ i g w ' _\mu \psi ' and = \left ( ( \partial_\mu l ) l ^\dagger + l w _\mu l ^\dagger \right ) \psi ' \end{align} which implies that a gauge field transforms as \begin{equation} i g w _\mu \rightarrow ( \partial_\mu l ) l ^\dagger + l w _\mu l ^\dagger \end{equation}
elaborating on trimok 's comment , you are mixing up $\dot{\mathbf r}$ and $\dot r$ . $\dot{\mathbf r}$ is the rate of change of the separation vector , while $\dot r$ is the rate of change of the separation distance . we can quickly see they are different . by definition , we know that $ \mathbf r = r \ , \hat{\mathbf r} $ . take the time derivative of this : $$ \dot{\mathbf r} = \dot r\ , \hat{\mathbf r} + r \ , \dot{\hat{\mathbf r}} $$ in this case , the direction between the two is changing , i.e. $\dot{\hat{\mathbf r}}\neq 0$ , so both terms on the right-hand side of this equation contribute to the value of $\dot{\mathbf r}$ . in particular , $\dot{\mathbf r}$ can remain constant while $\dot r$ changes , so long as the above equation is obeyed .
i think your question might be reformulated in some way . the general approach to solitons is to derive a differential equation with respect to a given potential . then , the solution to this equation might have soliton solution , which of course interact with the external field , can collide etc . but i think this may not cover what you are thinking off . let me give you a quick derivation of sine-gordon equation arising from a model of coupled pendulums . this might help to specify what you want to know . sin gordon equation : elastic coupled pendulums the kinetic energy of $n$ pendulums with mass $m$ and length $l$ is given by $t = \sum_{i=1}^n\frac{m}{2}l^2\dot{\varphi}_i^2$ the potential energy is a little more complicated : $u = \sum_{i=1}^nmgl\left ( 1 - \cos\varphi_i\right ) + \sum_{i=1}^{n-1}\frac{d}{2}\left ( \varphi_{i+1}-\varphi_{i}\right ) ^2$ where $d$ accounts for the strength of the band and $g$ for gravitation . the lagrange function is then given by $l = t - u$ and we find the equations of motion using lagrangian mechanics via the famous $\frac{d}{dt}\frac{\partial l}{\partial\dot{\varphi}_i}-\frac{\partial l}{\partial\varphi_i}$ leading to $\frac{d}{mgl}\left ( \varphi_{i+1} - 2\varphi_{i} + \varphi_{i-1}\right ) - \frac{l}{g}\partial_{tt}\varphi_i=\sin\varphi_i$ now , by letting $n\rightarrow\infty$ and rescaling of coordinates we arrive at the famous sine-gordon-equation for the continuous system : $\partial_{\rho\rho}\varphi - \partial_{\tau\tau}\varphi = \sin\varphi$ this equation has mono- or multisoliton solutions which are nicely described in the wikipedia article mentioned earlier . you can see that for the derivation of the equation of motion we needed to have the potential already at hand , it does not work the other way round as far as i know . the solutions are very interesting and widely used throughout physics to explain " non-dispersive " water waves , scattering of particles and so on . some further remarks for a much deeper insights you might have a look at the book " solitonen - nichtlineare strukturen " by r . meinel written in german ( and judging from your name you might understand ) or research articles like non-commutative soliton scattering by lindstrm et al . as far as i know there does not exist a strict kind of approach that will tell you if a system has soliton solutions or not ; the search for such solutions is a science on its own . an expert for solitons in optical systems is o . egorov , some of his papers on the subject can be found on google scholar . sincerely robert
i am surprised you have not found this information online because the equation is one of the most basic in all of physics : $$\mathbf{f} = m\mathbf{a} = m\frac{\mathrm{d}\mathbf{v}}{\mathrm{d}t}$$ ( bold represents a vector ) . for an application like yours , it will be more useful to rearrange this in the form of an evolution equation $$\frac{\mathrm{d}\mathbf{v}}{\mathrm{d}t} = \frac{\mathbf{f} ( \mathbf{v} , t ) }{m}$$ there are two ways to handle this . if the magnetic field configuration is known and is given by a simple function , you may be able to solve the evolution equation analytically . in that case , just program that solution into your code and have the program calculate the position of the particle at each time . in general , that is not practical , so you will have to investigate the field of numerical integration . here is a primer : as a crude method of solution , you can replace the derivative operator with a finite difference operator , e.g. for the $x$-component : $$\frac{\mathrm{d}v_x}{\mathrm{d}t} \to \frac{v_x ( t + \delta t ) - v_x ( t ) }{\delta t}$$ plugging this into the evolution equation , you can rearrange it to $$v_x ( t + \delta t ) = v_x ( t ) + \frac{1}{m}f_x ( v_x ( t ) , v_y ( t ) , v_z ( t ) , t ) \delta t$$ since you have a formula for calculating $f_x$ as a function of $\mathbf{v}$ and $t$ , you can use the $x$-velocity at any given time step to calculate an approximation to the $x$-velocity at the next time step , and similarly for the other components . this is called the euler method for numerical integration . although it is easy to understand , it is highly unstable - that is , it becomes very inaccurate very quickly , because calculating the velocities and forces only at the beginning of each time step ( as opposed to , say , the beginning and end and taking an average ) leads to errors that accumulate at each time step . there are more sophisticated methods of numerical integration which arrange for the errors to largely cancel out by calculating values of $f_x$ within the time step . whatever programming language or framework you are using , there is probably a scientific computation library that includes a numerical integrator , which you should look into using . that sort of thing is beyond the scope of this site , however . ( try searching on stack overflow )
in classical mechanics newton 's second law applies only to constant mass systems . in those cases there is no difference between $f=ma$ and $f=\mathrm{d}p/\mathrm{d}t$ . however , in special relativity the latter is valid , but the former is not . a relativistic definition of momentum is required : $p=\gamma m v$ . some details [ rework of my original answer ] some of the responses given so far answer the op , but have weaknesses that might lead to misconceptions about newton 2 . i will try to address the issue . some of the answers so far are not entirely correct if by $p$ is meant $mv$ of the rocket . newton 's second law is valid only for constant mass systems . $f=\mathrm{d}p/\mathrm{d}t$ leads to the rocket equation by accident if the propellent is exhausted in a direction opposite to the direction of motion . if you are very careful about what you mean by $p$ a correct analysis can be made , but $p=m_\mathrm{rocket}v_\mathrm{rocket}$ does not work . this wikipedia entry . is the clearest statement of that fact that i have found . to see why , consider a system comprising the rocket plus its remaining fuel and remaining propellant , which is what i think the other responders intend . ( i may be wrong , but if so , they should clarify what exactly is their system . ) imagine the system moving at constant velocity , and having two thrusters pointing perpendicular to the direction of motion , and directly opposite each other , and producing identical constant thrust by expelling exhaust gas . the thrust force of the two are equal and opposite . so the net force on the rocket is zero . then $f_{net}=0$ and $\mathrm{d}v/\mathrm{d}t = 0$ , and $v \neq 0$ and $\mathrm{d}m/\mathrm{d}t \neq 0$ . blindly applying $f=\mathrm{d}p/\mathrm{d}t$ leads to $0=v\ , \mathrm{d}m/\mathrm{d}t$ , a contradiction which can only be resolved if the mass is unchanging . comments on some comments the system is losing momentum by losing mass , but it is velocity is not changing : there is no net force on the system . momentum is conserved in the closed system consisting of the rocket and the exhausted propellant . the system consisting of the rocket plus the fuel and propellant not yet exhausted ( fuel remaining in the tank ) is an open system . conservation of momentum does not apply to open systems . careful analysis of a variable mass system lead to $$ \vec{f}_\mathrm{ext}=\vec{u}\frac{\mathrm{d}m}{\mathrm{d}t} + m\frac{\mathrm{d}\vec{v}}{\mathrm{d}t}$$ where $\vec{u}$ is the velocity of the mass leaving the system relative to the velocity of the system , and $\vec{f}_\mathrm{ext}$ is the external force on the system . for a rocket $\vec{f}_\mathrm{ext}=0$ , and $$ 0=\vec{u}\frac{\mathrm{d}m}{\mathrm{d}t} + m\frac{\mathrm{d}\vec{v}}{\mathrm{d}t}$$ this is not the same as $$ 0=\vec{v}\frac{\mathrm{d}m}{\mathrm{d}t} + m\frac{\mathrm{d}\vec{v}}{\mathrm{d}t}$$ where $\vec{v}$ is the velocity of the rocket . trying to write $f = m\ , \mathrm{d}v/\mathrm{d}t + v\ , \mathrm{d}m/\mathrm{d}t$ for a variable mass system is not correct . here 's a nice discussion whose first sentences are " in mechanics , a variable-mass system is a collection of matter whose mass varies with time . newton 's second law of motion cannot directly be applied to such a system because it is valid for constant mass systems only . " aeronautical engineers know all this . it is only physicists who are confused . i checked some books . symon 's and john r . taylor 's classical mechanics texts get it right , as does halliday , resnick , and walker .
for the first fundamental form - if you have got two vectors tangent to $\sigma$ , and $\sigma$ is embedded in $m$ , and $m$ has a metric , just use the embedding to consider the vectors as living tangent to $m$ and use $m$ 's metric to compute their inner product . for the second fundamental form , basically , if you imagine a two surface $\sigma$ embedded in $\mathbb{r}^3$ , and you imagine the normals as arrows orthogonal to $\sigma$ sticking out like a hedgehog 's spines . then if the surface is gently curved , the normals dont change much as you go from a point $x$ on $\sigma$ to another point $x'$ on $\sigma$ infinitesimally separated from $x$ by a vector tangent to $\sigma$ . conversely if the embedding is highly curved , the normals change a lot when you do this small displacement . the weingarten map , your $\chi$ , is just the map $$u\rightarrow \nabla_un $$ where $n$ is the normal to $\sigma$ and $u$ is tangent to $\sigma$ , and this encodes how much the normals are changing when you nudge by $u$ along $\sigma$ . the 2nd ff , or " extrinsic curvature " is just another way of representing the information in the weingarten map . explicitly , it is a bilinear form which maps a pair of vectors $u , v$ tangent to $\sigma$ to a number $-u . \chi ( v ) $ ( $\chi ( v ) $ is also tangent to $\sigma$ so this makes sense ) . btw , since you are studying 3+1 , this reference ( which came up in one of alex nelson 's posts ) , is really informative and steers you through the minefield of conflicting approaches .
the answer is yes , and in fact i have described how this works in my answer to another question of yours : if you shoot a light beam behind the event horizon of a black hole , what happens to the light ? . i will not repeat the working from that question here , but it might be worth a comment on exactly how the idea works . when you solve the equations of gr you get an object called the metric tensor that tells you about the geometry of spacetime . to write this down you have to choose some system of coordinates i.e. the units you use to measure time and distance . the metric itself does not depend on what coordinates you choose , but the form you write the metric does depend on the coordinates . for a static black hole the usual way the metric is written is using the schwarzschild coordinates , in which case it looks like : $$ ds^2 = -\left ( 1-\frac{r_s}{r}\right ) dt^2 + \frac{dr^2}{\left ( 1-\frac{r_s}{r}\right ) } + r^2 d\omega^2 \tag{1} $$ in this form the coordinates $t$ and $r$ are the times and distances measured by an observer far from the black hole , so it is an obvious choice . as written in this form there is no suggestion that spacetime is flowing in any sense . however the schwarzschild coordinates have a singularity at the event horizon and they are difficult to use if you are trying to work out what happens at the event horizon . to get round this we use a different set of coordinates called the gullstrand-painlev coordinates . using these coordinates the metric looks like : $$ ds^2 = -dt_{ff}^2 + \left ( dr + \frac{2m}{r} dt_{ff} \right ) ^2 + r^2 d\omega^2 \tag{2} $$ in these coordinates $dr$ is the distance measured by a distant observer , as in the schwarzschild coordinates , but the time coordinate $dt_{ff}$ is the time that would be measured by an observer falling freely into the black hole . if you compare this with the metric for flat space : $$ ds^2 = -dt^2 + dr^2 + r^2 d\omega^2 $$ it should be obvious that this looks like flat space but where the radial distance is replaced by $dr + \tfrac{2m}{r} dt_{ff}$ . in other words the distance from the black hole changes with time , and the rate of change is faster the closer you get to the centre of the black hole . the physical interpretation of this is that the spacetime is flowing inwards into the black hole , a bit like water draining in a sink . if you are falling into the black hole you are just being carried along by spacetime , just like someone being carried along in a river . but you should be careful of attaching physical significance to this . the whole point of general relativity is that it does not depend on the coordinates you use . although the two metrics ( 1 ) and ( 2 ) look different they will calculate the same value of $ds^2$ - they are just different ways of writing down the same physics . so we can write down the metric in a way that describes spacetime as flowing , just as you suggest in your question . however we can also write an equivalent equation that does not show spacetime as flowing . so you cannot say gravity is due to spacetime flowing , you can just say that the equations can be written down in that way .
dear gordon , i hope that other qg people will write their answers , but let me write mine , anyway . indeed , you need to distinguish the types of singularities because their character and fate is very different , depending on the type . you rightfully mentioned timelike , spacelike , and coordinate singularities . i will divide the text accordingly . coordinate singularities coordinate singularities depend on the choice of coordinates and they go away if one uses more well-behaved coordinates . so for example , there seems to be a singularity on the event horizon in the schwarzschild coordinates - because $g_{00}$ goes to zero , and so on . however , this singularity is fake . it is just the artifact of using coordinates that differ from the " natural ones " - where the solution is smooth - by a singular coordinate transformation . as long as the diffeomorphism symmetry is preserved , one is always allowed to perform any coordinate transformation . for a singular one , any configuration may start to look singular . this was case in classical general relativity and it is the case for any theory that respects the symmetry structure of general relativity . the conclusion is that coordinate singularities can never go away . one is always free to choose or end up with coordinate systems where these fake singularities appear . and some of these coordinate systems are useful - and will remain useful : for example , the schwarzschild coordinates are great because they make it manifest that the black hole solution is static . physics will never stop using such singularities . what about the other types of the singularities ? spacelike singularities most famously , these include the singularity inside the schwarzschild black hole and the initial big bang singularity . despite lots of efforts by quantum cosmologists ( meaning string theorists working on cosmology ) , especially since 1999 or so , the spacelike singularities remain badly understood . it is mainly because they inevitably break all supersymmetry . the existence of supersymmetry implies the existence of time-translational symmetry - generated by a hamiltonian , the anticommutator of two supercharges . however , this symmetry is brutally broken by a spacelike singularity . so physics as of 2011 does not really know what is happening near the very singular center of the schwarzschild black hole ; and near the initial big bang singularity . we do not even know whether these questions may be sharply defined - and many people guess that the answer is no . the latter problem - the initial big bang singularity - is almost certainly linked to the important topics of the vacuum selection . the eternal inflation answers that nothing special is happening near the initial point . a new universe may emerge out of its parent ; one should quickly skip the initial point because nothing interesting is going on at this singular place , and try to evolve the universe . the inflationary era will make the initial conditions " largely " irrelevant , anyway . however , no well-defined framework to calculate in what state ( the probabilities . . . ) the new universe is created is available at this moment . you mentioned the no-boundary initial conditions . i am a big fan of it but it is not a part of the mainstream description of the initial singularity as of 2011 - which is eternal inflation . in eternal inflation , the initial point is indeed as singular as it can get - surely the curvatures can get planckian and maybe arbitrarily higher - however , it is believed by the eternal inflationary cosmologists that the universe cannot really start at this point , so they think it is incorrect to imagine that the boundary conditions are smooth near this point in any sense , especially in the hartle-hawking sense . the schwarzschild singularity is different - because it is the " final " spacelike singularity , not an initial condition - and it is why no one has been talking about smooth boundary conditions over there . well , there is a paper about the " black hole final state " but even this paper has to assume that the final state is extremely convoluted , otherwise one would macroscopically violate the predictions of general relativity and the arrow of time near the singularity . while the spacelike singularities remain badly understood , there exists no solid evidence that they are completely avoided in nature . what quantum gravity really has to do is to preserve the consistency and predictivity of the physical theory . but it is not true that a " visible " suppression of the singularities is the only possible way to do so - even though this is what people used to believe in the naive times ( and people unfamiliar with theoretical physics of the last 20 years still believe so ) . timelike singularities the timelike singularities are the best understood ones because they may be viewed as " classical static objects " and many of them are compatible with supersymmetry which allowed the physicists to study them very accurately , using the protection that supersymmetry offers . and again , it is true that most of them , at least in the limit of unbroken supersymmetry and from the viewpoint of various probes , remained very real . the most accurate description of their geometry is singular - the spacetime fails to be a manifold , i.e. diffeomorphic to an open set near these singularities . however , this fact does not lead to any loss of predictivity or any inconsistency . the simplest examples are orbifold singularities . locally , the space looks like $r^d/\gamma$ where $\gamma$ is a discrete group . it is clear by now that such loci in spacetime are not only allowed in string theory but they are omnipresent and very important in the scheme of things . the very " vacuum configuration " typically makes spacetime literally equal to the $r^d/\gamma$ ( locally ) and there are no corrections to the shape , not even close to the orbifold point . again , this fact leads to no physical problems , divergences , or inconsistencies . some of the string vacua compactified on spaces with orbifold singularities are equivalent - dual - to other string/m-theory vacua on smooth manifolds . for example , type iia string theory or m-theory on a singular k3 manifold is equivalent to heterotic strings on tori with wilson lines added . the latter is non-singular throughout the moduli space - and this fact proves that the k3 compactifications are also non-singular from a physics viewpoint - they are equivalent to another well-defined theory - even at places of the moduli spaces where the spacetime becomes geometrically singular . the same discussion applies to the conifold singularities ; in fact , orbifold points are a simple special example of cones . conifolds are singular manifolds that include points whose vicinity is geometrically a cone , usually something like a cone whose base is $s^2\times s^3$ . many components of the riemann curvature tensor diverge . nevertheless , physics near this point on the moduli space that exhibits a singular spacetime manifold - and physics near the singularity on the " manifold " itself - remains totally well-defined . this fact is most strikingly seen using mirror symmetry . mirror symmetry transforms one calabi-yau manifold into another . type iia string theory on the first is equivalent to type iib string theory on the second . one of them may have a conifold singularity but the other one is smooth . the two vacua are totally equivalent , proving that there is absolutely nothing physically wrong about the geometrically singular compactification . we may be living on one . the equivalence of the singular compactifications and non-singular compactifications may be interpreted as a generalized type of a " coordinate singularity " except that we have to use new coordinates on the whole " configuration space " of the physical theory ( those related by the duality ) and not just new spacetime coordinates . it is very clear by now that some singularities will certainly stay with us and that the old notion that all singularities have to be " disappeared " from physics was just naive and wrong . singularities as a concept will survive and singular points at various moduli spaces of possibilities will remain there and will remain important . physics has many ways to keep itself consistent than to ban all points that look singular . that is surely one of the lessons physics has learned in the duality revolution started in the mid 1990s . whenever physics near/of a singularity is understood , we may interpret the singularity type as a generalization of the coordinate singularities . at this point , one should discuss lots of exciting physics that was found near singularities - especially new massless particles and extended objects ( that help to make singularities innocent while preserving their singular geometry ) or world sheet instantons wrapped on singularities ( that usually modify them and make them smooth ) . all these insights - that are cute and very important - contradict the belief that there is no " valid physics near singularities because singularities do not exist " . spacetime manifolds with singularities do exist in the configuration space of quantum gravity , they are important , and they lead to new , interesting , and internally consistent phenomena and alternative dual descriptions of other compactifications that may be geometrically non-singular .
depending on what you mean by " exist " , the answer to your question is yes . there is an $n=3$ poincar supersymmetry algebra , and there are field-theoretic realisations . in particular there is a four-dimensional $n=3$ supergravity theory . a good modern reference for the diverse flavours of supergravity theories is toine van proeyen 's structure of supergravity theories . added weinberg 's argument is essentially the following observation . take a massless unitary representation of the $n=3$ poincar superalgebra with helicity $|\lambda|\leq 1$ . this representation is not stable under cpt , so the cpt theorem says that to realise that in a supersymmetric quantum field theory , you have to add the cpt-conjugate representation . once you do that , though , the $\oplus$ representation admits in fact an action of the $n=4$ poincar superalgebra . the reason the supergravity theory exists ( and is different from $n=4$ supergravity ) is that the $n=3$ gravity multiplet , which is a massless helicity $|\lambda|=2$ unitary representation , is already cpt-self-conjugate .
have a look at http://en.wikipedia.org/wiki/barycenter#astronomy ( and the links from it to find out more about the subject ) . if you take our solar system as an example and consider just the heaviest planet jupiter , the sun attracts jupiter , but jupiter attracts the sun as well . jupiter is much lighter than the sun , but it is heavy enough to significantly move the sun as it orbits . the barycentre of the sun-jupiter system is slightly above the sun 's surface , so an anstronomer looking at the solar system from the planet zogg would see the sun orbiting ( i.e. . wobbling ) about a point just above the sun 's surface . to calculate the mass of the planet you need to know it is orbital period and how much the star is moving . you also need the mass of the star , but we can estimate this from the star 's brightness and colour . the period of the star 's wobble tells us the radius of the planet 's orbit , and from that and how much the star wobbles , i.e. how much the planet moves it , we can work out the mass of the planet . the maths is not as hard as you might think . see http://en.wikipedia.org/wiki/doppler_spectroscopy#procedure for the details . you ask about other methods of determining the mass . in the solar system we can calculate the masses of planets , asteroids etc by observing their effects on their moons , other planets etc . for exoplanets we can generally only see the star , so that is the only way we have of estimating their mass .
this paper on the arxiv claims to have measured an anisotropy at the $2\sigma$ level . i am not familiar enough with the subject to pass judgement , though i get the impression that the general view is interesting but unproven at the moment . if nothing else the paper gives a convenient literature review . various anisotropies have been suggested , such as a north-south anisotropy in the fine structure constant and the axis of evil alignment of low multipoles in the cmb data . however the various proposed anisotropies do not line up with each other , which strengthens the skeptics case .
to a very good approximation , the transit time can be estimated as the arithmetic mean of the rise and set times . however , this will not work accurately for rapidly moving objects ( e . g . moon ) . i followed the link in your question not realizing it was in the mathematica forum and i am not familiar with how mathematica computes these times . however , i am willing to bet that atmospheric refraction is accounted for in the traditional way , that is by adding 34 arc minutes to the object 's altitude as rise/set . refraction acts to increase an object 's altitude , but the effect is zero at the zenith . anyway , if refraction is accounted for , the rise/set times should be for either the center of the object 's disk or the upper limb of the object 's disk . there are two conventions used here . in the u.s. all sunrise/sunset/moonrise/moonset times are for upper limb of sun 's/moon 's disk . however , most european almanacs tabulate these times for the center of sun 's/moon 's disk . all other celestial objects are too small to show a naked eye disk , so the rise/set times refer to the disk 's center .
neutrons have spin 1/2 and therefore obey the pauli exclusion principle , meaning two neutrons cannot occupy the same space at the same time . when two neutrons ' wavefunctions overlap , they feel a strong repulsive force . see http://en.wikipedia.org/wiki/exchange_interaction .
you have to look at the actual vertices involved in the feynman diagram , not just the net flavor . here you have an up quark changing to a strange quark , which is a change of flavor . it does not matter that there is an anti-up quark changing to an anti-strange quark to balance it out . when we say " the em interaction cannot change flavor , " what that really means is that there is no elementary vertex which couples the photon ( the boson that mediates em interactions ) to two fermions of different flavor . in other words , any time a particle has a fundamental interaction with a photon , it has to come out with the same flavor it had going in . that is not the case in the diagram you have shown : the elementary vertex between the photon and the third quark of the top proton has a photon , an up quark , and a strange quark . that is a flavor-changing vertex involving a photon , exactly the kind that does not exist in the standard model .
you can use $q/\epsilon_0$ to calculate flux for both cases because that is what gauss ' law says : just look at the enclosed charge . it is amazing . flux for any closed surface is $\phi = \oint \vec{e} \cdot d\vec{a}$ . there are two ways to calculate this quantity : the hard way , which means evaluating $\vec{e}$ on every part of the surface , and integrating . this is hard because for your case 2 , the electric field strength varies on the surface and it is not normal everywhere . when computed in this manner , it is not obvious at all that your two cases should yield the same value for flux . it really is not obvious . the easy way , which is using gauss ' law $\phi = q_\text{encl}/\epsilon_0$ . to evaluate flux using gauss ' law , you only need to look at how much charge how is physically enclosed . that is what gauss ' law says . that is why gauss ' law gives the same answer in both cases . it makes no mention of the particular geometry of your closed surface and does not assume that the electric field is normal to the surface . there are proofs of gauss ' law floating around , but i do not think you are asking for that . as a side note , one often uses gauss ' law to figure out what the electric field is . case 1 is very useful for figuring out the electric field at some distance away from your charge . case 2 is not useful since you can not factor out $\vec{e}$ from the integral . nonetheless , both cases yield the same value for flux .
the " effect " is quite real , but i do not believe there is a name for it . it has to do with rotational stability and the intermediate axis theorem . i am sure you have also observed this effect when throwing a spatula , frying pan , or remote control . in terms of rotational inertia , the " easiest " axis of rotation for the racquet is straight through the handle . the " hardest " axis of rotation would be straight down ( perpendicular to the strings ) . but when there is rotation along the " intermediate " axis ( the one you describe ) , the other axes ( especially the " easiest" ) become extremely sensitive to random perturbations . if you could flip or spin the racquet so that it turned exclusively about one of its three principal axes , it would continue to spin about that axis indefinitely . that is why they are called principal axes . but in a real flip there is always some mixture of motions about all three axes . here is where the intermediate axis theorem enters the picture : while a racquet spinning mostly about either the low-rotational-inertia axis or high-rotational-inertia axis will be relatively unaffected by extraneous motion about the other two axes , a racquet that is spinning mostly about the intermediate-rotational-inertia axis is exquisitely sensitive to any accidental motion about those other two axes . even a tiny amount of unintended motion about those axes will cause the racquet to wobble significantly .
there is no known analytical way to predict this , because the equations are chaotic . so just set up the equations of motion carefully , integrate them properly , and watch what happens . there will be some near misses , and then there will be a collision . you might need to " speed up time " in your model to have it happen in your life time - in a real planetary system this might take many millions of years . . note that " chaotic " means that even a tiny change in the initial conditions will most likely lead to a completely different evolution of the motion - this is why the best you can hope for is something that " looks life like " rather than something that will predict the actual evolution of say our own planetary system - you just cannot know the initial conditions accurately enough . but even an approximate solution can be fun to make and observe - you will get a feeling for the kinds of behavior a real planetary system might exhibit . paraphrasing the stock market : " past orbits are no guarantee of future motion "
lets start with the definition of the structure factor from c and l , by com- bining eq . 2.1.9 and 2.1.10 . we have $$s ( q ) = \frac{1}{n} \langle \sum_{\alpha , \alpha'} e^{i q ( x_{\alpha}-x_{\alpha '} ) } \rangle$$ where the xs refer to the mass points and the summation is understood as a double sum . lets break this up into parts and well notice that things get simplified a bit . $$s ( q ) =\frac{1}{n} [ \langle \sum_{\alpha=\alpha'} e^{i q ( x_{\alpha}-x_{\alpha '} ) } \rangle + \langle \sum_{\alpha&gt ; \alpha'} e^{i q ( x_{\alpha}-x_{\alpha '} ) } \rangle + \langle \sum_{\alpha&lt ; \alpha'} e^{i q ( x_{\alpha}-x_{\alpha '} ) } \rangle ] $$ now the first term in brackets is just n . the third term is just the complex conjugate of the second one . so we really only need to calculate the second ( or the third ) term . lets proceed $$s ( q ) = \langle \sum_{\alpha&gt ; \alpha'} e^{i q ( x_{\alpha}-x_{\alpha '} ) } \rangle = \sum_{\alpha&gt ; \alpha'} [ p e^{iq1}+ ( 1-p ) e^{iq ( 1+\rho ) } ] ^{\alpha-\alpha'}$$ the introduction of the probabilities comes from the definition of an expected value . the power of $$ ( \alpha-\alpha' ) $$ comes from the fact that you have to sum over all of the possible combinations of $$x_{\alpha}-x_{\alpha'}$$ . since this is in the exponent , this sum of exponents can be rewritten as a product of exponentials . letting $$ ( \alpha-\alpha' ) =k$$ and evaluating the sum , we obtain $$\sum_{k=1}^{n-1} ( n-k ) [ pe^{iq}+ ( 1-p ) e^{iq ( 1+\rho ) } ] ^{k}$$ if you stare at this long enough you come to see this is a geometric sum . we can wave our hands and end up with ( look at the bounds on the sum , n is large , think of what terms dominate . etc ) $$s ( q ) \approx\frac{1}{n} ( 1+\frac{pe^{iq}+ ( 1-p ) e^{iq ( 1+\rho ) }}{1- [ pe^{iq}+ ( 1-p ) e^{iq ( 1+\rho ) } ] }+c . c . ) =\frac{1-|pe^{iq}+ ( 1-p ) e^{iq ( 1+\rho ) }|^{2}}{|1- [ pe^{iq}+ ( 1-p ) e^{iq ( 1+\rho ) } ] |^{2}}$$ the rest is just algebra from here .
we can think of a sound signal either as a function of time or as a function of frequency . the two ways are related by what is called a fourier transform , so they are often written as $f ( t ) $ and as $\tilde f ( \omega ) $ , using the same letter , but with the " tilde " above one of them indicating that one is the fourier transform of the other . from these functions it is also possible to construct other functions of both time and frequency , such as the wigner function . i would take " pitch " as more-or-less another name for frequency . audacity help says " generally synonymous with the fundamental frequency of a note , but in music , often also taken to imply a perceived measurement that can be affected by overtones above the fundamental . " the squared amplitude of a function is the square of the function , properly speaking the norm , written as $|f ( t ) |^2$ and $|\tilde f ( \omega ) |^2$ . this function of time or of frequency is always positive or zero , whereas the original function can be positive or negative ( or it can be a 2-valued function , which in the time coordinates represent both the signal and its rate of change at a given time ) . it is common to call the " squared amplitude " the amplitude , just to confuse matters . sound level sometimes refers to the logarithm of the squared amplitude , as georg says , but sometimes not . wikipedia , for example , suggests that a loudness meter reports on a logarithmic scale , whereas sound level might mean either linear or logarithmic scale . one test for what is meant is whether the signal level is followed by " db " , which will always indicate a logarithmic scale . ultimately i would say that it is the mathematical relationships between these objects that one tries to understand , but a lot can be understood just by experimenting with something like audacity ( which i think is simpler to use than the software mentioned by @whoplisp , although also less powerful ) , using the " analyze-> plot spectrum " option for various sounds . here 's a plot for me whistling , apparently at about 1300hz ( the fundamental frequency ) in the 0.026 seconds that i analyzed here , but with various higher frequencies also represented . when reading something about sound or music , there are various technical usages that are not very standardized , and one often has to figure out what is meant by the overall context . there are many different expert groups that have a strong interest in signal analysis of various kinds , and they have often developed specialized languages appropriate to their own interests .
it depends on the frequency . dc electricity travels through the bulk cross section of the wire . a changing electrical current ( ac ) experiences the skin-effect where the electricity flows more easily in the surface layers . the higher the frequency the thinner the surface layer that is usable in a wire . at normal household ac ( 50/60hz ) the skin depth is about 8-10mm but at microwave frequencies the depth of the metal that the current flows in is about the same as a wavelength of visible light edit : interesting point from navin - the individual strands have to be insulated from each other for the skin effect to apply to each individually . that is the reason for the widely separated pairs of wires in this question what are all the lines on a double circuit tower ?
in general , the forward thruswt is achieved by tilting the entire helicopter forward . this converts some of the lift produced by the main rotor into a forward component of force . the same is true for turns . yes , the swashplate may be used to create a thrust imbalance ; but this imbalance does not provide any horizontal forces . it merely creates a force imbalance which tips the helo slightly , so that a component of the upward force of the main rotor gets converted into a lateral force . take a look at any helo moving with any kind of rapid forward velocity and you will see that the entire craft is tipped noticably forward . same with sharp turns : the helo tilts noticably in the direction of the turn .
no , because the process of transferring the voltage into a useable form or device will reduce it . in other words , under perfectly idealized conditions ( which are impossible ) , yes it might spin forever , but as soon as you try to use your generator to power a device , you will slow it down .
the underlying level of nature is quantum mechanical and obeys special relativity laws , not newtonian mechanics . mass is a conserved quantity in newtonian mechanics , but not in the underlying quantum mechanical framework . the classical framework emerges as the variables become large enough where h_bar , which characterizes the quantum mechanical level , can be safely assumed zero , because it is so small with respect to the constants entering classical equations . in the quantum mechanical framework the basic building blocks are the elementary particles that have very small invariant mass . the invariant mass is the " length " of the corresponding four dimensional vector that describes a system . in a similar way that in three dimensional space , if one adds vectors the length of the resultant vector is not a linear addition of the length of the vectors but can be much smaller than the sum , in four dimensional vector additions the invariant mass is controlled by the four dimensional vector algebra of the pseudoeuclidean space that is the basis of special relativity . the masses of the quarks entering the proton when summed have mass less than 15 mev , and the proton has a mass close to 1000 . the mass of the proton comes up by the vector addition of the four dimensional vectors entering the problem , gluons , quark antiquark pairs that are holding the three basic quarks bound as a proton . these particles are called virtual because they cannot be extracted and measured , they are a part of the necessary calculations . they are characterized by the quantum numbers of the named particles , but are off mass shell . they follow the four dimensional algebra of special relativity . the invariant mass resulting by adding up these innumerable constituents ' four vectors , gives the mass of the proton . as we go to larger space dimensions , the dimensions of nuclei , the difference between adding the constituent nucleons ( protons and neutrons ) masses and the mass of the nucleus they compose is much smaller , but still exists , and we see it as the binding energy of the nuclei . this is what has been utilized in getting nuclear energy in reactors and bombs . at the sizes of crystals and solid state lattices the energy differences between adding constituent masses and the mass of the solid is even smaller since the levels are now in electron volts , and is responsible for the chemistry and solid state of matter . at the level of our every day life , newtonian physics holds and we can assume masses are conserved , unless there is a nuclear interaction . the accuracy in measurement should be quite good to see the differences due to chemical interactions .
yes , physics has learned things on both concepts , but only gradually . the value of the mass 125 gev is in the sub-130-gev region that favors supersymmetry , or makes it necessary according to some , because the pure standard model predicts a catastrophically unstable vacuum for such low higgs masses . it is also below 135 gev which is where it should be according to the general version of the minimal supersymmetric standard model . on the other hand , 125 gev is higher than what the simplest models of supersymmetry  at least according to most supersymmetry phenomenologists  want to see . a value closer to 115 if not 100 gev could be favored if one wanted to make susy models " really simple " , at least according to the sociologically prevailing ideas about the simplicity . the idea about the multiverse has strengthened because the higgs is known to be both light and the only new physical phenomenon found at the lhc so far . this seems to imply that nature does not care about our notion of " naturalness "  the lightness of the higgs boson relatively to some very high energy scales such as the planck scale is " unnatural " . the multiverse is the only concrete enough yet acceptable enough framework to " predict " unnaturally small values of parameters such as the higgs mass , so the physicists ' subjective probability ( belief ) that " the multiverse is needed " has increased . none of these results are really conclusive or qualitative . there are lots of loopholes and conceivable alternatives . but among the propositions that are on the market , we may usually identify the marginal winners and losers . ideas about low-lying , easily found supersymmetry were marginal losers ; the general idea of susy and its impact on the higgs mass has been a marginal winner ; the concept of naturalness has lost something , and the multiverse has gained a little . as anna said , susy and the multiverse are not really mutually exclusive although the " generic " multiverse explanations indeed favor no susy or supersymmetry invisible at realistic colliders .
he " only " flew at the maximum speed of 370 m/s or so which is much less than the speed of the meteoroids  the latter hit the earth by speeds between 11,000 and 70,000 m/s . so he was about 2 orders of magnitude slower . the friction is correspondingly lower for baumgartner . note that even if he jumped from " infinity " , he would only reach the escape velocity which is 11,200 m/s for the earth , just like the slowest meteoroids . i guess that a good enough ( and cooled ) suit inspired by nasa rockets might be capable of protecting a human against such relative speeds even though for generic surfaces , they would almost certainly start to burn at the surface . however , it would not be pleasant to slow down from such speeds in the atmosphere . ; - ) you see that if you uniformly slow down from 10 km/s to 0 km/s while flying through 10 km of the atmosphere , the penetration through the atmosphere takes about 2 seconds . however , getting from 10 km/s to 0 km/s in two seconds means that the deceleration is 5000 m/s/s or 500 $g$ . i guess that not even he could survive that . ; - )
observer is a special person ( or a system that contains such person ) which does not obey the usual laws of quantum mechanics . while it is much easier to define observer from a philosophical point of view , the mathematical answer is that the observer is a system which manifests subjective decoherence when observed . for the definition of subjective decoherence and precise mathematical formulations , refer to this work .
it is a visual illusion akin to the wagon wheel effect the stream of water is being waggled back and forth by a 25 hz audio signal and being filmed at 25 frames per second .
the shadow of the earth on the moon during an eclipse and the way masts of ships are visible when they are out of sight are the classical reasons .
model the air in a tire as a viscuous fluid , and you should get a pretty good answer . the viscosity is low , but so is the mass , so inertia effects will be relevant . check the reynolds number , but i suspect you can largely ignore turbulance if you model the inside of the tire as a smooth surface and ignore the deformation where the tire flattens against the road . when the tire has been at rest for a while , everything will be " motionless " . when the tire first spins , the center of the air will stay where it was . the boundary layer by the tire wall will move with the tire , which then makes the next layer move , with makes the next layer move , etc . there is a overall time constant . eventually in steady state all the air moves with the tire .
modern processors are built from cmos technology . these digital circuits consume relatively little current when sitting in one state or the other . however , there is some inevitable capacitance on every node . when the output of a digital gate changes state , that capacitance is charged or discharged , which means current has to flow . the total average current used by a digital gate is therefore largely proportional to how often it switches , which is to say it is proportional to frequency . the supply current times the supply voltage is the power used by the digital circuit , which it dissipates as heat . so yes , clocking a modern processor faster causes it to dissipate more power , which can cause it to overheat if this power is not removed properly . note the big heat sink with dedicated fan clamped to the processor chip . getting rid of the heat of such chips is a major design consideration . newer processors also draw significant power just because they are on . one way to reduce the power required for switching is to lower the voltage . that reduces the charge that is moved onto or off of the little capacitors each digital state change , which reduces the overall average current and therefore the heat dissipated . however , when the transistors are made for too low a voltage , they do not really turn off that well . this results in a little leakage current thru each gate . a few million gates here , another few million gates there , and pretty soon you have some real currents to worry about . this tradeoff between less current due to switching versus more current when not switching is a carefully balanced design decision .
one of the tricky things with general relativity is that different observers may use different coordinate systems and measure very different things . the exterior geometry of any static spherically symmetric object is described by the schwarzschild metric : $$ ds^2 = -\left ( 1-\frac{r_s}{r}\right ) dt^2 + \frac{dr^2}{\left ( 1-\frac{r_s}{r}\right ) } + r^2 d\omega^2 $$ the only difference between a black hole and your almost black hole is that in your case this metric applies only to some minimum radius $r_{min} &gt ; r_s$ while for a black hole it applies everywhere . the coordinates $t$ and $r$ correspond to times and lengths as measured by an observer at infinity , and as these will not be the same as times and lengths measured by observers nearer the star . if you bear this in mind , you can calculate the speed of light in the schwarzschild coordinates simply by setting $ds = 0$ and choosing a radial path so $d\omega = 0$ , and you get : $$ \frac{dr}{dt} = 1 - \frac{r_s}{r} $$ as you reduce $r$ towards $r_s$ the speed of light measured in schwarzschild coordinates will indeed be reduced to below $c$ , and will tend to zero as you approach the event horizon . as i said above , this happens for all static spherically symmetric objects whether or not they are black holes . but you need to think long and hard about the physical significance of this . an observer hovering just above the surface of your collapsing star will measure the speed of light locally to be $c$ , and so will a freely falling observer plunging towards the star 's surface . in fact all observers making a local measurement of the speed of light will get the value $c$ . the reason the observer at infinity measures a velocity of less than $c$ is that the different sets of observers disagree about the meaurement of times and lengths . re your last question , when light from the star 's surface reaches the observer at infinity it will be moving at $c$ ( as all locally measured light is ) but it will have lost energy on the journey from the star and will be red shifted . this is the gravitational red shift and happens in all gravitational fields , even the earth 's . the only special thing about a black hole is that the red shift tends to infinity as you approach the event horizon .
first of all , it would be preposterous to think that there was a simple recipe that newton followed and that anyone else can use to deduce the laws of a similar caliber . newton was a genius , and arguably the greatest genius in the history of science . second of all , newton was inspired by the falling apple - or , more generally , by the gravity observed on the earth . kepler understood the elliptical orbits of the planets . one of kepler 's laws , deduced by a careful testing of simple hypotheses against the accurate data accumulated by tycho brahe , said that the area drawn in a unit time remains constant . newton realized that this is equivalent to the fact that the first derivative of the velocity i.e. the second derivative of the position - something that he already understood intuitively - has to be directed radially . in modern terms , the constant-area law is known as the conservation of the angular momentum . that is how he knew the direction of the acceleration . he also calculated the dependence on the distance - by seeing that the acceleration of the apple is 3,600 times bigger than that of the moon . so he systematically thought about the second derivatives of the position - the acceleration - in various contexts he has encountered - both celestial and terrestrial bodies . and he was able to determine that the second derivative could have been computed from the coordinates of the objects . he surely conjectured very quickly that all kepler 's laws can be derived from the laws for the second derivatives - and because it was true , it was straightforward to prove him this conjecture . obviously , he had to discover the whole theory - both $f=ma$ ( or , historically more accurately , $f=dp/dt$ ) as well as a detailed prescription for the force - e.g. $f=gm_1m_2/r^2$ - at the same moment because a subset of these laws is useless without the rest . the appearance of the numerical constant in $f=ma$ or $p=mv$ is a trivial issue . the nontrivial part was of course to invent the mathematical notion of a derivative - especially because the most important one was the second derivative - and to see from the observations that the second derivative has the direction it has ( from kepler 's law ) and the dependence on the distance it has ( from comparing the acceleration of the moon and the apple falling from the tree ) . it was not a straightforward task that could have been solved by anyone but it was manifestly simple enough to be solved by newton . so he had to invent the differential calculus , $f=ma$ , as well as the formula for the gravitational force at the same moment to really appreciate what any component is good for in physics .
let us quickly run through the standard kk compactification . we start with a $d+1$ dimensional theory $$ s = \frac{1}{16\pi g_{d+1}} \int d^{d+1}x \sqrt{g} r_{d+1} $$ more general actions on the $d+1$ dimensional space can be considered , but this will suffice for our purposes . the metric $g_{mn}$ can be decomposed as $$ ds^2 = g_{mn} dx^m dx^n = e^{2\phi} \left ( dt + a_\mu dx^\mu \right ) ^2 + g_{\mu\nu} dx^\mu dx^\nu $$ we assume now that $t$ is the compactified direction with $t \sim t + 2 \pi r$ . the theory has the following symmetries $d$-dimensional diffeomorphims , $x^\mu \to x'^\mu ( x ) $ under which $a_\mu$ and $g_{\mu\nu}$ transform as rank 1 and 2 tensors respectively . gauge transformations along the compactified directions , $t \to t + \lambda ( x ) $ , $a_\mu \to a_\mu - \partial_\mu \lambda$ . this symmetry essentially describes the local choice of origin in the compactified direction . now , if the length scales of our problem are large compared to the radius of the compactified circle $r$ , we then assume that $\phi$ , $a_\mu$ , and $g_{\mu\nu}$ are only functions of $x^\mu$ and not $t$ . ( this is only done here to make things simpler . one can consider the more general case where the fields are expanded into modes in the $t$ direction . this gives us massive particles in the $d$-dimensional space . we will not consider this here ) . with this assumption , we find $$ r_{d+1} = r_d - 2 e^{-\phi} \nabla^2 e^{\phi} - \frac{1}{4} e^{2\phi} f_{\mu\nu} f^{\mu\nu} , ~~ \sqrt{g} = e^{\phi} \sqrt{g} $$ the action then takes the form $$ s = \frac{2\pi r}{16\pi g_{d+1}} \int d^d x \sqrt{g} e^{\phi} \left [ r_d - \frac{1}{4} e^{2\phi} f_{\mu\nu} f^{\mu\nu} + \nabla_\mu \phi \nabla^\mu \phi \right ] $$ thus , we note that in the $d$-dimensional space , we have a gauge field and a scalar field . the action is not quite in the einstein-hilbert form since the scalar field $\phi$ couples to $r_d$ and $f_{\mu\nu} f^{\mu\nu}$ non-trivially . also , the kinetic term for $\phi$ has the wrong sign . this field is called the dilaton . to understand why $\phi$ is called the dilaton ( related to dilation , or in other words scale ) , let us go back to the $d+1$ dimensional metric . consider the $s^1$ living at a fixed point $x^\mu$ . the induced metric on this circle is $$ ds^2_{s^1} = e^{2\phi ( x^\mu ) } dt^2 $$ the size of this circle is $$ \int ds_{s^1} = \int_0^{2\pi r} dt e^{\phi ( x^\mu ) } = 2\pi r e^{\phi ( x^\mu ) } $$ thus , we see that the effective radius of the circle at $x^\mu$ is $r e^{\phi ( x^\mu ) }$ . in other words the dilaton field controls the size of the circle . as an aside , the compactified action above is written in what is called the string frame ( name comes from string theory ) . it possible to go to the more standard einstein frame ( where the action takes the form $\sqrt{g} r$ , etc ) by doing a field redefinition $g_{\mu\nu} \to e^{2\omega} {\tilde g}_{\mu\nu}$ and appropriately choosing $\omega$ . in this frame , the scalar kinetic term has the right sign . however , we still have a non-trivial coupling to $f_{\mu\nu}f^{\mu\nu}$ .
first let me refer you to eric weinberg 's book where the instanton moduli space is described in more detail . principal bundles over 4-dimensional riemannian manifolds are classified by the second chern class = instanton number and the t ' hooft discrete abelian magnetic fluxes . please see the following lecture notes by mns henningson . t ' hooft fluxes are present only when the gauge group has a nontrivial center , thus in the case of $su ( n ) $ , the classification is according to the instanton numbers . a compactified minkowski space , can be thought of as a four dimensional ball $b^4$ , with the boundary points ( at infinity ) $s^3$ identified . thus by the stokes theorem , the instanton number is given by : $$ k = \int_{b^4}*f \wedge f =\int_{b^4}dcs ( a ) = \int_{s^3_{\infty}} cs ( a ) = \int_{s^3_{\infty}} wzw ( g ) $$ where $cs ( a ) $ and $wzw ( g ) $ are the chern-simons and the wess-zumino-novikov-witten functionals respectively and the last step is derived from the substitution of the pure gauge condition at infinity . the local geometry of the moduli space can be understood as follows : the instanton solutions are of the form:$ a_{\mu} = a_{\mu} ( x^\alpha ) $ , where $x^\alpha$ are the coordinates of the moduli space . these solutions are local minima of the action , for all constant values of the moduli but the action is not extremal when these coordinates are made to depend on time . this time dependence is introduced to study the dynamics of the moduli near the classical solution . the difference between the action with time varying moduli and time invariant ones is due to the time dependence of the moduli coordinates , assuming the time derivatives are small ( i.e. . , by substituting $x^{\alpha}= x_0^{\alpha} + t \dot{x}_0^{\alpha}$ , the leading terms have the least number of time derivatives thus the varied action must have the form : $$ i = \frac{1}{2g^2} \int d^4x f_{\mu\nu} ( a ) f^{\mu\nu} ( a ) = \frac{8\pi^2}{g^2} k + \int dt b_{\alpha} ( x ) \dot{x}^{\alpha} +g_{\alpha \beta} ( x ) \dot{x}^{\alpha}\dot{x}^{\beta} + . . . $$ where the last step is obtained after the integration of the " known " solution over the spatial coordinates . this action has just the form of a particle moving on a riemannian manifold having a metric $g$ in a magnetic field $b$ . the exterior derivative of the magnetic field can be interpreted as the symplectic structure of the moduli space . the form of the metric taken by david tong will give exactly the same metric , because , the leading terms in the moduli time derivatives of the yang mills action will include a time derivative , thus we are left with the variation of the gauge fields themselves . this is only the local structure of the moduli space . this analysis will not tell us , for example , if the symplectic structure is exact or closed . of course , the global structure of the moduli space requires deeper analysis . one global property that can be " relatively " easily computed is the moduli space dimension , even if a simple closed form of the solution is not available : the dimension is the count perturbation $a_{\mu}+a_{\mu}$ of a self dual solution $a_{\mu}$ which is also self dual modulo gauge transformations . inserting the gauge potential in the self duality equation , the following condition is obtained ( weinberg equation 10.112 ) : $$ \eta^{\mu \nu} d_{a \mu} a_{\nu} = 0$$ where $d_{a}$ is the covariant derivative of the classical solution , and $ \eta^{\mu \nu} $ is a self dual matrix defined in weinberg ( equation 10.74 ) . to remove the pure gauge direction , there exist another orthogonality condition to all pure gauge directions : $$d_{a\mu} a^{\mu} = 0$$ . these two conditions can be combined together into a single dirac equation : $$\not{d}_{a} \psi = 0$$ . where , the relation of the gauge field perturbation and the spinor $\psi$ are given by : $a_{\mu} = \sigma_{\mu}^{\alpha \dot{\alpha}} \psi_{\alpha \dot{\alpha}}$ this construction converts the problem of counting the number of moduli to counting the number of a dirac equation zero modes . for the dirac equation , the number of zero modes is given by the atiyah-singer index theorem .
a couple of reasons : first , the whole framework of parton distributions is developed to apply to high-energy hadrons , as they appear in scattering experiments . these hadrons move with a very large longitudinal momentum , typically hundreds or thousands of gev , and except at very small $x$ , the partons will also have large longitudinal momenta . if a parton is going to diverge from the hadron path by any significant angle , it'll have to have a similarly large transverse momentum , and outside of hard scattering events , there just is not enough energy around to do that . at small $x$ , however , it is conceivable that a parton could have an amount of transverse momentum comparable to its longitudinal momentum . the reason we do not care about this is that the cross section for parton splitting is roughly inversely related to the transverse momentum transfer ( $\hat\sigma \sim 1/p_t^2$ i think ) , so it exhibits a sharp peak at $p_t^2 = 0$ . this means we can consider the only non-negligible contribution to come from $\theta = 0$ , i.e. true collinear branching , and any significant deviations of partons from the original longitudinal path are exponentially suppressed .
the question is a bit more subtle than you ( probably ) think . the wavefunction you have given is for an infinite plane wave with a perfectly defined momentum . because the wave is infinite the probabilty of finding the particle is the same everywhere , i.e. the uncertainty in the particle position is infinite . this is the same result you get by setting $\sigma_p$ to zero in the uncertainty relation dmckee gave in his answer . if you want a wavefunction for a particle with a position you can can build one by adding up a number of plane waves with different values of momentum , $p$ . the example usually given in qm courses is to make the particle wavefunction a gaussian curve . you may think it is hard to make a gaussian by adding up plane waves , but actually it is very easy . there is a mathematical procedure called a fourier transform that you can use to disassemble a gaussian into it is component plane waves . if you are interested , this calculation is given in detail here . with a gaussian we normally describe it is width by the width at 1/e of the peak height , and this width is known as $\sigma$ . if you use a gaussian with a width $\sigma_x$ to describe your particle , then the fourier transform gives a distribtuion of momenta that is a gaussian of width $\sigma_p$ , and as the calculation i linked to shows , $\sigma_x$ and $\sigma_p$ are related by : $$\sigma_p \sigma_x = \frac{\hbar}{2}$$ exactly as the uncertainty relation requires .
in physics , it is often implicitly assumed that the lagrangian $l=l ( q^i , v^i , t ) $ depends smoothly on the ( generalized ) positions $q^i$ , velocities $v^i$ , and time $t$ , i.e. that the lagrangian $l$ is a differentiable function . let us now assume that the lagrangian is of the form $$l~=~\ell ( v^2 ) , \qquad\qquad v~:=~|\vec{v}| , \qquad\qquad ( 1 ) $$ where $\ell$ is a differentiable function . the equations of motion ( eom ) become $$ \vec{0}~=~\frac{\partial l}{\partial \vec{q}} ~\approx~\frac{d}{dt}\frac{\partial l}{\partial \vec{v}} ~=~\frac{d }{dt} \left ( 2\vec{v}~\ell^{\prime}\right ) ~=~2\vec{a}~\ell^{\prime}+4\vec{v}~ ( \vec{a}\cdot\vec{v} ) \ell^{\prime\prime} . \qquad\qquad ( 2 ) $$ ( here the $\approx$ symbol means equality modulo eom . ) we conclude that on-shell $$\vec{a} \parallel \vec{v} . $$ ( the words on-shell and off-shell refer to whether eom are satisfied or not . ) therefore by taking the length on both sides of the vector eq . $ ( 2 ) $ , we get $$ 0~\approx~2a ( \ell^{\prime}+2v^2\ell^{\prime\prime} ) , \qquad\qquad a~:=~|\vec{a}| . $$ this has two branches . the first branch is that there is no acceleration , $$ \qquad \vec{a}~\approx~\vec{0} , \qquad\qquad ( 3 ) $$ or equivalently , a constant velocity . the second branch imposes a condition on the speed $v$ , $$\ell^{\prime}+2v^2\ell^{\prime\prime}~\approx~0 . \qquad\qquad ( 4 ) $$ to take the second branch $ ( 4 ) $ seriously , we must demand that it works for all speeds $v$ , not just for a few isolated speeds $v$ . hence eq . $ ( 4 ) $ becomes a 2nd order ode for the $\ell$ function . the full solution is precisely op 's counterexample $$l~=~ \ell ( v^2 ) ~=~\alpha \sqrt{v^2}+\beta~=~\alpha v+\beta , $$ where $\alpha$ and $\beta$ are two integration constants . this is differentiable wrt . the speed $v=|\vec{v}|$ , but it is not differentiable wrt . the velocity $\vec{v}$ at $\vec{v}=\vec{0}$ if $\alpha\neq 0$ , and therefore the second branch $ ( 4 ) $ is discarded . thus the eom is the standard first branch $ ( 3 ) $ . firstly , the definition of form invariance is discussed in this question . concretely , landau and lifshitz mean by form invariance that if the lagrangian is $$l~=~\ell ( v^2 ) \qquad\qquad ( 5 ) $$ in the frame $k$ , it should be $$l&#39 ; ~=~\ell ( v^{\prime 2} ) \qquad\qquad ( 6 ) $$ in the frame $k^{\prime}$ . here $$\vec{v}^{\prime }~=~\vec{v}+\vec{\epsilon}$$ is a galilean transformation . secondly , op asks if adding a total time derivative to the lagrangian $$l ~\longrightarrow~ l+\frac{df}{dt}$$ is the the only thing that would not change the eom ? no , e.g. scaling the lagrangian $$l ~\longrightarrow~ \alpha l$$ with an overall factor $\alpha$ also leaves the eom unaltered . see also wikibooks . however , we already know that all lagrangians of the form $ ( 5 ) $ and $ ( 6 ) $ lead to the same eom $ ( 3 ) $ . ( recall that acceleration is an absolute notion under galilean transformations . ) instead , i interpret the argument of landau and lifshitz as that they want to manifestly implement galilean invariance via noether theorem by requiring that an ( infinitesimal ) change of the lagrangian $$ \delta l~:=~l&#39 ; -l ~=~2 ( \vec{v}\cdot\vec{\epsilon} ) \ell^{\prime} $$ is always a total time derivative even off-shell . in general , how do we know if an expression $\delta l$ is a total time derivative ? well , one way is to apply the euler-lagrange operator on the expression , and check if it is identically zero off-shell . we calculate $$ \vec{0}~=~ \frac{d}{dt}\frac{\partial \delta l}{\partial \vec{v}} -\frac{\partial \delta l}{\partial \vec{q}} $$ $$~=~4\vec{\epsilon}~ ( \vec{a}\cdot\vec{v} ) \ell^{\prime\prime} +4\vec{v}~ ( \vec{a}\cdot\vec{\epsilon} ) \ell^{\prime\prime} +4\vec{a}~ ( \vec{v}\cdot\vec{\epsilon} ) \ell^{\prime\prime} +8\vec{v}~ ( \vec{v}\cdot\vec{\epsilon} ) ( \vec{a}\cdot\vec{v} ) \ell^{\prime\prime\prime} . \qquad\qquad ( 7 ) $$ since eq . $ ( 7 ) $ should hold for any off-shell configuration , we can e.g. pick $$ \vec{a}~\parallel~\vec{v}~\perp~\vec{\epsilon} . $$ then eq . $ ( 7 ) $ reduces to $$ \vec{0}~=~ 4\vec{\epsilon} ~ ( \pm a v ) \ell^{\prime\prime} . $$ we may assume that $\vec{\epsilon}\neq\vec{0}$ . arbitrariness of $a$ and $v$ implies that $$\ell^{\prime\prime}~=~0 . \qquad\qquad ( 8 ) $$ ( conversely , it is easy to check that eq . $ ( 8 ) $ implies eq . $ ( 7 ) $ . ) the full solution to eq . $ ( 8 ) $ is the standard non-relativistic lagrangian for a free particle , $$l~=~ \ell ( v^2 ) ~=~\alpha v^2+\beta , $$ where $\alpha$ and $\beta$ are two integration constants . for more on galilean invariance , see also this question .
no , i think you are mistaken . entanglement cannot be used to transmit information . two distant experimenters each with one of two entangled electrons cannot communicate by performing measurements of their electrons . furthermore , entanglement does not imply that quantum mechanics is nonlocal , i.e. , that there is spooky , instantaneous action at a distance . if you follow the common copenhagen interpretation of the wavefuction , the wave function has no physical meaning prior to measurement . nothing passes between the electrons upon measurement . all that changes upon measurement is our state of knowledge .
background you would need a very sensitive instrument to measure the daytime vs nighttime difference in g . it is not 0.006 m/s 2 . it is much , much smaller than that , about $6\times10^{-11}$ m/s^2 . your 0.006 m/s 2 is the gravitational acceleration toward the sun at distance of 1 au . the earth as a whole is accelerating sunward at 0.006 m/s 2 . you cannot measure that acceleration with any local experiment , and a pendulum most certainly is a local experiment . what you can measure is tidal gravity , but you will need a very sensitive instrument . at noon , on object on the surface of the earth is a bit closer to the sun than is the center of the earth and thus the object experiences a slightly greater sunward acceleration than does the earth as a whole . the sun pulls the object away from the earth at noon , decreasing the sensed value of g . the difference between these two accelerations is the source of the tides caused by the sun . tidal forces are approximately a 1/r 3 force . this is why the tides raised by the moon are about twice those of the tides raised by the sun , even though the sun is much , much more massive than the moon . what about midnight ? at midnight , the object is a bit further from the sun than is the center of the earth and thus the object experiences a slightly reduced sunward acceleration than does the earth as a whole . the sun pulls the earth away from the object away at midnight , once again decreasing the sensed value of g . the difference between the daytime and nighttime is extremely small , about $6\times10^{-11}$ m/s 2 . detecting that small a change would require a superconducting gravimeter . what about sunrise or sunset ? now the tidal gravitational force points toward the center of the earth , but with about half the magnitude of the outward tidal force at noon/midnight . the difference between the solar tidal force at sunset and at noon is measurable without needing a superconducting gravimeter ; you only need to be able to measure to eight significant digits . there is a problem , however : the moon . the lunar tides are about twice as strong as the solar tides . it might be easier to conduct your experiment when the moon is new or full , making the lunar and solar affects additive and easier to measure . measurement measuring this will not be easy . a seconds pendulum nominally has a period of two seconds . the length of such a pendulum is a bit shy of one meter . ( aside : this was the definition of a meter recommended by french scientists . there was one problem : it was a " placist " definition because gravitational acceleration varies somewhat over the face of the earth . ) assuming a seconds pendulum with a nominal , non-tidal period of two seconds , a seconds pendulum will have a period of 2.000000167 seconds at noon at the sub-moon point during a solar eclipse and a period of 1.999999917 seconds at sunset of the same day . an hour as measured by the clock will be 0.45 milliseconds shorter at sunset than at noon . that is not much of a difference , and you will have to wait until oct 23 , 2014 for a solar eclipse , and you will have to go to central america to be close to the sub-moon point . fortunately , the effect is not significantly reduced at a non-eclipse new moon , and the latitudinal effects are not terrible so long as you do not live in a far northern city such as anchorage or helsinki . that 0.45 millisecond difference is measurable , and with relatively inexpensive equipment . having students start and stop a millisecond accuracy stopwatch will not work . you will need something more sophisticated than that . set your physics and engineering students to the challenge . calculations the two links above are the wolframalpha calculations that result in the 2.000000167 and 1.999999917 second periods of a nominally seconds pendulum . there are some magic numbers in those calculations . 0.99362138556613170633 meters this is the length of a seconds pendulum , with g =9.80665 meters/seconds 2 . 9.80665 meters/seconds 2 this is the defined value of g . 1.6338699e-6 meters/seconds 2 this is the combined effect of the sun and moon at a solar eclipse .
the electric field of a transmission line decreases with the distance from the line , so there is a potential difference in the radial direction . this potential difference can cause the glow in fluorescent tubes . i believe such tubes can glow even if it is a dc transmission line , as the charge can drift in the air . you may wish to consider a comparison with a corona discharge , which often takes place near transmission lines ( both dc and ac ) , even if there are no fluorescent tubes around , because the electric field strength can be so high near the conductor that causes air breakdown . corona discharge causes extra power losses , so do glowing fluorescent tubes . while there are losses even in open ac circuits due to reactive loads , extra load leads to extra losses . so those fluorescent bulbs do cause extra losses , they do not use just " wasted energy " . furthermore , i am not sure it is safe to fool around transmission lines .
i want to try to give a solution to your equation using a separation ansatz of the form $$x ( y , t ) = t ( t ) \cdot y ( y ) $$ we find $$\frac{t&#39 ; &#39 ; }{t} = \frac{1}{y}\left ( gy&#39 ; + \left ( \frac{t_0 + \lambda g y}{\lambda}\right ) y&#39 ; &#39 ; \right ) $$ both sides do depend on different variables , hence the given equation must equal a constant , say $-\omega_n^2$ . we see directly that the fundamental system for $t$ can be stated as oscillations ( if this notation is convenient for you ) : $$eig ( t ) = \{ \sum_n \left [ e^{\mathrm{i}\omega_n t} , e^{-\mathrm{i}\omega_n t} \right ] \}$$ new solution for y the differential equation for $y$ now takes the form $$ \frac{\omega_n^2}{g} y + y&#39 ; + \left ( \frac{t_0}{\lambda} + y\right ) y&#39 ; &#39 ; = 0$$ and it is obvious that one should first of all make a substitution of the form $$z = \frac{t_0}{\lambda} + y$$ such that we find with $y ( y ) = z ( z ) $ $$\frac{\omega_n^2}{g} z + z&#39 ; + z z&#39 ; &#39 ; = 0$$ the solution to this equation are indeed bessel functions $$eig ( z ) = \{j_0\left ( 2\omega_n \sqrt{z/g} \right ) , y_0\left ( 2\omega_n \sqrt{z/g}\right ) \}$$ so , finally $$x{ ( t , y ) }=\sum_n e^{\{+ , -\}\mathrm{i}\omega_n t}\left [ a_n j_0 \left ( 2\omega_n \sqrt{\frac{t_0 + \lambda y}{\lambda g}} \right ) + b_n y_0 \left ( 2\omega_n \sqrt{\frac{t_0 + \lambda y}{\lambda g}} \right ) \right ] $$ the overall solution is then given due to the starting and boundary conditions you specified . since this is a discussion on its own , i will leave this for the other thread : ) sincerely robert
so , is there any reason for overruling the idea of fixed orbit ? or is there any thing wrong in my opinion about the concept , if so please explain , so that i would not proceed with that wrong thinking ? an insurmountable problem with the bohr atom is that one has two charged particles orbiting around each other . electromagnetism was an exact science at that time . a charged particle moving in an electric field ( one moving in the field of the other ) would have to radiate energy away , finally falling into the nucleus , if there were no laws that did not allow it to radiate . thus within the laws of classical physics fixed orbits were an impossibility . postulating that x electrons orbiting around y protons can orbit without radiating was necessary , but not general enough to be called a theory . the theory that emerged from the plethora of experimental data studying the small dimensions of the atoms was quantum mechanics . it is now understood by physicists that the underlying stratum of nature behaves according to the rules of quantum mechanics , and that classical mechanics and classical electrodynamics are emergent theories from this basis . generally , quantum mechanics does not assign definite values . instead , it makes a prediction using a probability distribution ; that is , it describes the probability of obtaining the possible outcomes from measuring an observable . often these results are skewed by many causes , such as dense probability clouds . probability clouds are approximate , but better than the bohr model , whereby electron location is given by a probability function , the wave function eigenvalue , such that the probability is the squared modulus of the complex amplitude , or quantum state nuclear attraction . naturally , these probabilities will depend on the quantum state at the " instant " of the measurement . hence , uncertainty is involved in the value . there are , however , certain states that are associated with a definite value of a particular observable . these are known as eigenstates of the observable ( "eigen " can be translated from german as meaning " inherent " or " characteristic" ) . the simplest mathematical formulation for solving for an electron around a nucleus is the schrodinger equation with the appropriate potential . the solutions reproduce the experimental observations and allow for predictions for other potentials and situations . what is calculated instead of an orbit , is an orbital , a locus in space and time where the electron can be found if measured with a probability given by the square of the wave function .
suppose you move a small distance $\vec{dr}$ = ( $dx$ , $dy$ , $dz$ ) and you take a time $dt$ to do it . pre-special relativity you could say three things . firstly the distance moved is given by : $$ dr^2 = dx^2 + dy^2 + dz^2 $$ ( i.e. . just pythagorus ' theorem ) and secondly the time $dt$ was not related to the distance i.e. you could move at any velocity . lastly the quantities $dr$ and $dt$ are invarients , that is all observers will agree they have the same value . special relativity differs by saying that $dr$ and $dt$ are no longer invarients if you take them separately . instead the only invarient is the proper time , $d\tau$ , defined by : $$ c^2d\tau^2 = c^2dt^2 - dx^2 - dy^2 - dz^2 $$ in special relativity all observers will agree that $d\tau$ has the same value , but they will not agree on the values of $dt$ , $dx$ , $dy$ and $dz$ . this is why we have to talk about spacetime rather than space and time . the only way to construct laws that apply to everyone is to combine space and time into a single equation . you say : i am having a hard time understanding how changing space means changing time well suppose we try to do this . let 's change space by moving a distance ( $dx$ , $dy$ , $dz$ ) but not change time i.e. $dt$ = 0 . if we use the equation above to calculate the proper time , $d\tau$ , we get : $$ d\tau^2 = \frac{0 - dx^2 - dy^2 - dz^2}{c^2} $$ do you see the problem ? $d\tau^2$ is going to be negative so $d\tau$ is imaginary and has no physical meaning . that means we can not move in zero time . well what is the smallest time $dt$ that we need to take to move ( $dx$ , $dy$ , $dz$ ) ? the smallest value of $dt$ that gives a non-negative value of $d\tau^2$ is when $d\tau^2$ = 0 so : $$ c^2d\tau^2 = 0 = c^2dt^2 - dx^2 - dy^2 - dz^2 $$ or : $$ dt^2 = \frac{dx^2 + dy^2 + dz^2}{c^2} $$ if we have moved a distance $dr = \sqrt{dx^2 + dy^2 + dz^2}$ in a time $dt$ , the we can find the velocity we have moved at the dividing $dr$ by $dt$ , and if we do this we find : $$ v^2 = \frac{dr^2}{dt^2} = \frac{dx^2 + dy^2 + dz^2}{\frac{dx^2 + dy^2 + dz^2}{c^2}} = c^2 $$ so we find that the maximum possible speed is $v = c$ , or in other words we can not move faster than the speed of light . and all from that one equation combining the space and time co-ordinates into the proper time !
check out the following 3 articles and 2 books : regularization renormalization and dimensional analysis : dimensional regularization meets freshman e and m published in the american journal of physics ( can be found also on hep-ph , but slightly different with less references ) regularization , from murayama 's course of qft at berkeley a hint of renormalization a more general detailed , still introductory , treatment including renormalization would be the book renormalization methods : a guide for beginners a . zee 's book qft in a nutshell anyway , i hope that was useful revo
imagine an electron a great distance from an atom , with nothing else around . the electron does not " know " about the atom . we declare it to have zero energy . nothing interesting is going on . this is our reference point . if the electron is moving , but still far from the atom , it has kinetic energy . this is always positive . the electron , still not interacting with the atom , may move as it pleases . it has positive energy , and in any amount possible . its wave function is a simple running plane wave , or some linear combination of them to make , for example , a spherical wave . its wavelength , relating to the kinetic energy , may be any value . when the electron is close to the atom , opposite charges attract , and the electron is said to be stuck in a potential well . it is moving , so has positive ( always ) kinetic energy , but the coulomb potential energy is negative and in a greater amount . the electron must slow down if it moves away from the atom , to maintain a constant total energy for the system . it reaches zero velocity ( zero kinetic energy ) at some finite distance away , although quantum mechanics allows a bit of cheating with an exponentially decreasing wavefunction beyond that distance . the electron is confined to a small space , a spherical region around the nucleus . that being so , the wavelength of its wavefunction must in a sense " fit " into that space - exactly one , or two , or three , or n , nodes must fit radially and circumferentially . we use the familiar quantum number n , l , m . there are discrete energy levels and distinct wavefunctions for each quantum state . note that the free positive-energy electron has all of space to roam about in , and therefore does not need to fit any particular number of wavelengths into anything , so has a continuous spectrum of energy levels and three real numbers ( the wavevector ) to describe its state . when the atom absorbs a photon , the electron jumps from let 's say for example from the 2s to a 3p orbital , the electron is not in any orbital during that time . its wave function can be written as a time-varying mix of the normal orbitals . a long time before the absorption , which for an atom is a few femtoseconds or so , this mix is 100% of the 2s state , and a few femtoseconds or so after the absorption , it is 100% the 3p state . between , during the absorption process , it is a mix of many orbitals with wildly changing coefficients . there was a paper in physical review a back around 1980 or 1981 , iirc , that shows some plots and pictures and went into this in some detail . maybe it was reviews of modern physics . anyway , keep in mind that this mixture is just a mathematical description . what we really have is a wavefunction changing from a steady 2s , to a wildly boinging-about wobblemess , settling to a steady 3p . a more energetic photon can kick the electron out of the atom , from one of its discrete-state negative energy orbital states , to a free-running positive state - generally an expanding spherical wave - it is the same as before , but instead of settling to a steady 3p , the electron wavefunction ends as a spherical expanding shell . i wish i could show some pictures , but that would take time to find or make . . .
the rate of expansion of the universe ( the fact that all objects are receding from each other and more so if they are farther away from each other ) is given by the hubble constant $h_0= 69.32  0.80 ( km/s ) /mpc$ 1 check out this plot from wikipedia on the y axis you have the velocity with which the object is receding from us and on the x axis the current distance in a common astrophysical unit called megaparsec ( parallax of one arc-second $1\text{pc} \approx 3.26$ light yrs ) . edit : the discrepancy circled in blue is due to the galaxies having additional internal motions on top of their receding due to expansion . the galaxies measured there are ( as the label says ) part of the virgo cluster . the internal motion will induce a doppler shift that will influence the overall redshift of the galaxy 1: according 20th dec 2012 the hubble constant , as measured by nasa 's wilkinson microwave anisotropy probe ( wmap )
i assume your objective is to minimize resistance to flow , in which case the dominating influence would be the length of the 1-inch pipes and the shape of their entry points where they enter the plenum . if you want the fluid velocity at the entrance to the plenum to be the same as it is in the 1-inch pipes , then the plenum should have the same cross-sectional area as the small pipes , and a diameter of sqrt ( 10 ) = 3.16 inches would be sufficient . however , you could use a smaller diameter plenum , with a higher fluid velocity , without introducing too much drag since its ratio of volume to surface area is larger . on the other hand , a larger plenum would have less drag , at the cost of its containing a larger amount of fluid .
jonas 's answer shows one way wherein complex numbers are useful in representing sinusoidally varying with time quantities . the quantity $e^{i\ , ( \omega\ , t + \delta}$ when it replaces $\cos ( \omega\ , t+\delta ) $ in a linear equation ( or $\sin ( \omega\ , t+\delta ) $ if one " favours the imaginary part " in jonas 's words ) is called a phasor . the phasor method is applied widely throughout physics , not only to electric fields . however , in the particular case of maxwell 's equations , there is a radically different way to bring in complex equivalents of the electromagnetic field that has a neat interpretation in terms of polarization . in practice , it ends up being used in a way very like the phasor method , even though its grounding is altogether different . this is the idea of diagonalising the maxwell curl equations ( faraday and ampre laws ) with the riemann-silberstein fields which are : $$\vec{f}_\pm = \sqrt{\epsilon_0} \ , \vec{e} \pm i\ , \sqrt{\mu_0} \ , \vec{h}\quad\quad\quad ( 1 ) $$ and which decouple the maxwell curl equations into the following form : $$i\ , \partial_t \vec{f}_\pm = \pm c\ , \nabla \wedge \vec{f}_\pm\quad\quad\quad ( 2 ) $$ note that by taking the divergence of both sides of ( 2 ) we get $i\ , \partial_t \vec{f}_\pm =0$ , so that if the fields are time varying and have no dc ( zero frequency ) component ( i.e. $\partial_t$ is invertible ) , ( 2 ) also implies the gauss laws $\nabla\cdot\vec{f}_\pm=0$ too . now one could simply sit with real electric and magnetic fields and one would need only one complex riemann-silberstein vector ( either of $\vec{f}_\pm$ will do just as well as the other ) to stand in the stead of two real fields and then the real valued curl equations are replaced by one complex-valued one . so one would interpret the real part as the field $\sqrt{\epsilon_0} \ , \vec{e}$ and the imaginary part as $\pm\sqrt{\mu_0} \ , \vec{h}$ ( depending on whether $\vec{f}_\pm$ were used ) at the end of the calculation . however , it turns out to be more physically meaningful to keep both vectors , but to throw away their negative frequency parts and keep the positive frequency parts alone of both vectors . what is really neat about this second approach is that if the light is right circularly polarized , only $\vec{f}_+$ is nonzero ; if left , only $\vec{f}_-$ is nonzero . so the positive frequency parts of the electromagnetic fields are decoupled precisely by splitting them into left and right circularly polarized components . now to restore a field 's negative frequency part from the positive frequency part alone , one adds the complex conjugate , i.e. we are still effectively taking the real part of the $\vec{f}_\pm$ fields at the end of the calculation , so the practicalities are rather like the phasor method . but now we take : $$\begin{array}{lcl} \vec{e} and = and \operatorname{re}\left ( \frac{\vec{f}_+ + \vec{f}_-}{2\ , \epsilon_0}\right ) \\ \vec{h} and = and \operatorname{re}\left ( \frac{\vec{f}_+ - \vec{f}_-}{2\ , i\ , \ , \mu_0}\right ) =\operatorname{im}\left ( \frac{\vec{f}_+ - \vec{f}_-}{2\ , \mu_0}\right ) \end{array} \quad\quad\quad ( 3 ) $$ to get our " physical " fields at the end of the calculation . but , given the physical , manifestly lorentz covariant interpretation of the riemann-silberstein vectors i talk about below ( see " more advanced material " below ) , one might just as well say that $\vec{f}_\pm$ are the physical fields ( even though they are not what you would measure with a vector voltmeter or magnetometer ) . in this framework of thought , a quantity 's being real or imaginary has a geometric meaning as whether it is bivector or a hodge dual thereof in the clifford algebra $c\ell_3 ( \mathbb{r} ) $ wherein the now " spinor " $\mathbf{f}_\pm$ live and the entity $i$ is now the unit pseudoscalar in this algebra . bivectors and their hodge duals mix and transform differently under the lorentz transformation ( 8 ) , so , if you like , you can very soundly take this difference as the meaning of real and imaginary parts . lastly , since now ( 2 ) is confined to two equations in positive frequency ( therefore positive energy ) we can now interpret ( 2 ) as the time evolution , i.e. schrdinger equation for the quantum state of a first quantized photon . see : i . bialynicki-birula , " photon wave function " in progress in optics 36 v ( 1996 ) , pp . 245-294 also downloadable from arxiv:quant-ph/0508202 * for more details . more advanced material the riemann-silbertein vectors are actually the electromagnetic ( maxwell ) tensor $f^{\mu\nu}$ in disguise . we can write maxwell 's equations in a quaternion form : $$\begin{array}{lcl} \left ( c^{-1}\partial_t + \sigma_1 \partial_x + \sigma_2 \partial_y + \sigma_3 \partial_z\right ) \ , \mathbf{f}_+ and = and {\bf 0}\\ \left ( c^{-1}\partial_t - \sigma_1 \partial_x - \sigma_2 \partial_y - \sigma_3 \partial_z\right ) \ , \mathbf{f}_- and = and {\bf 0}\end{array}\quad\quad\quad ( 4 ) $$ where $\sigma_j$ are the pauli spin matrices and the electromagnetic field components are : $$\begin{array}{lcl}\frac{1}{\sqrt{\epsilon_0}}\mathbf{f}_\pm and = and \left ( \begin{array}{cc}e_z and e_x - i e_y\\e_x + i e_y and -e_z\end{array}\right ) \pm i \ , c\ , \left ( \begin{array}{cc}b_z and b_x - i b_y\\b_x + i b_y and -b_z\end{array}\right ) \\ and = and e_x \sigma_1 + e_y \sigma_2+e_z\sigma_3 + i\ , c\ , \left ( b_x \sigma_1 + b_y \sigma_2+b_z\sigma_3\right ) \end{array}\quad\quad\quad ( 5 ) $$ the pauli spin matrices are simply hamilton 's imaginary quaternion units reordered and where $i=\sigma_1\ , \sigma_2\ , \sigma_3$ so that $i^2 = -1$ . when inertial reference frames are shifted by a proper lorentz transformation : $$l = \exp\left ( \frac{1}{2}w\right ) \quad\quad\quad ( 6 ) $$ where : $$w = \left ( \eta^1 + i\theta \chi^1\right ) \sigma_1 + \left ( \eta^2 + i\theta \chi^2\right ) \sigma_2 + \left ( \eta^3 + i\theta \chi^3\right ) \sigma_3\quad\quad\quad ( 7 ) $$ encodes the transformation 's rotation angle $\theta$ , the direction cosines of $\chi^j$ of its rotation axes and its rapidities $\eta^j$ , the entities $\mathbf{f}_\pm$ undergo the spinor map : $${\bf f} \mapsto l {\bf f} l^\dagger\quad\quad\quad ( 8 ) $$ here , we are actually dealing with the double cover $psl ( 2 , \mathbb{c} ) $ of the identity-connected component of the lorentz group $so ( 3,1 ) $ , so we have spinor maps representing lorentz transformations , just as we must use spinor maps to make a quaternion impart its represented rotation on a vector .
luminosity is necessary in order to turn number of interactions to crossections , because theories provide crossections to compare with experiments . experiments measure number of interactions . a well known crossection , as is bhabha scattering , substituted on the right will give the luminosity to be used in the other observed interactions in the experiment . of course it works for electron colliders , but the logic of calibration is the same for other colliders too . from the formula one sees that the higher the luminosity the more number of interactions per unit time . as the statistical error on the values extracted from the data come from the number n , the larger the number the better the accuracy of all measurements .
with vision depth is determined by parallax . this largely works for objects out to 100m or less . depth beyond that distance is assessed by familiarity with these objects , say large mountains or vistas , and experience with them . depth with hearing is determined by two means . a tone which is perceived louder in one ear than the other is usually perceived as having a direction based on that . for lower frequency sounds phase differences between left and right auditory perception can be used by the brain to detect angular direction relative to the facial direction . some species of whales are thought to generate a mental map of the ocean , where a blue whale in the atlantic maps the outlay of the ocean bottom and coasts through echo location . clearly this indicates a huge amount of neural processing which uses these acoustical data . dogs generate maps of their immediate world through olfactory means . life forms generate maps of their world by a variety of means .
the short answer is no . regarding the paper , i can not understand the logic . as far as i can tell , the author writes down some version of the time-energy uncertainty relation , then says " hence , it is quite natural to investigate x " where x has little or nothing to do with the time-energy uncertainty relation . the language and formulation of the paper are also not clear , for example the fundamental inequality eq . 3 is not even precisely formulated . in my opinion , you should not take the paper seriously . you said in your question that the author convinced you of their premise , but i am not even sure what the premise is . if you could state precisely what you are convinced of , i could try to address it more directly . here is a physical counter-example showing that quantum computation need not take excessive ( exponential ) physical resources . one proposal for quantum computation involves adiabatically moving topological excitations around each other in a two dimensional piece of quantum matter . this is called topological quantum computation . we already have experimental examples , such as fractional quantum hall fluids , which support these kind of topological excitations . by slowly braiding these excitations around each other in a suitable piece of quantum matter we can produce any unitary transformation we want and hence do computation . however , this braiding does not take exponential time , nor does it require exponentially large energy ( in fact if the braiding is down adiabatically and interactions are short ranged , the energy of the piece of quantum matter may not even change during the computation ! ) hope this helps .
i agree with ron maimon that large scale structure of space-time by hawking and ellis is actually fairly rigorous mathematically already . if you insist on somehow supplementing that : for the purely differential/pseudo-riemannian geometric aspects , i recommend semi-riemannian geometry by b . o'neill . for the analytic aspects , especially the initial value problem in general relativity , you can also consult the cauchy problem in general relativity by hans ringstrm . for a focus on singularities , i have heard some good things about analysis of space-time singularities by c.j.s. clarke , but i have not yet read that book in much detail myself . for issues involved in the no-hair theorem , markus heusler 's black hole uniqueness theorems is fairly comprehensive and self-contained . one other option is to look at mme . choquet-bruhat 's general relativity and einstein 's equations . the book is not really suitable as a textbook to learn from . but as a supplementary source book it is quite good . if you are interested in learning about the mathematical tools used in modern classical gr and less on the actual theorems , the first dozen or so chapters of exact solutions of einstein 's field equations ( by stephani et al ) does a pretty good job .
you are correct that the standard for naming exoplanets is normally the lower-case letter after the star name in the order of discovery . so in our system , earth would be sol b . if there are multiple stars in the system , like 16 cyg ( which has 16 cyg a and b ) , then the planet 's lower-case letter would be appended to the star 's , such as 16 cyg bb . so the space is normally there unless it is a binary star system in which case the space is not there because there already is one between the star 's a/b/c/etc . designation and the name . ( different systems have been proposed , such as by hessman et al . ( 2010 . ) however , as you note , it appears as though the kepler team , does not include a space . their papers , for example , would list a planet as kepler-22b . or kepler-16b ( which is confusingly short for kepler-16 ( ab ) -b ) . newspaper/internet editors often in these cases will go to an established grammar guide or set of rules for their own publications instead of following what the team does . just looking at the external links on the wikipedia page for kepler-22b shows the bbc using " kepler 22b , " nasa properly using " kepler-22b , " and planetary habitability lab using " kepler-22 b . "
let 's use si units so mass is in kilograms and length is in meters . let the original mass be $m_1 = 25 \ , \mathrm{kg}$ and the final mass be $m_2 = m_1 +\delta m$ where $\delta m = 6 \ , \mathrm{kg}$ . let the original extension of the spring be $x_1$ and the final extension be $x_1 + \delta x_1$ where $\delta x = 2.5\ , \mathrm{cm}$ . we can determine the spring constant by noting that in each case , the gravitational force of the mass on the spring must balance the spring force pulling up ; this gives two equations : $m_1g = k ( x_1 - x_\mathrm{eq} ) $ $m_2g = k ( x_2 - x_\mathrm{eq} ) $ so that subtracting the first equation from the second gives $ ( m_2 - m_1 ) g = k ( x_2-x_1 ) $ which given the notation above gives $\delta m g = k \delta x$ so the spring constant is $ k = \frac{\delta m}{\delta x} g = \frac{6\ , \mathrm{kg}}{2.5\ , \mathrm{cm}} g$ with this in hand , you can indeed compute the angular frequency for the original load $m_1$ using the formula you wrote . this in turn will give you the period , which is what i assume part ( a ) is asking for . i will let you think about part ( b ) since this sounds like a homework question to me . let me know of any typos . cheers !
yes nuclear batteries are reasonably common , it is generally easier to just use the heat given off to drive either a stirling cycle motor or a thermo-electric generator rather than use the energy of the emitted particle directly . they are very useful when you need power for many years without being able to recharge or replace the batteries eg . in space probes or remote monitoring sites . but they do involve large lumps of highly radioactive materials so are not for everyone ! ps . i am not sure what you mean by the energy of a " single event in a battery "
if we know wavelength and intensity of emission spectra for an element , is not what we " see " as it burns simply a superposition of these ? yes . if i try to reconstruct the apparent color of copper from its spectrum as $\sum f_n \cdot i_n /\sum i_n$ in which $f_n$ and $i_n$ are the frequencies and relative intensities respectively , it seems to work . but trying this with , say , helium , using approximations because there are a lot of lines , i consistently get something in the green range . actually , that is not how to interpret it . what the above is doing is adding together frequencies , but that is not what it means to have a " superposition of emission spectra " . the flaw becomes obvious when you consider a flashlight . if you shine red and green light on a wall , is that the same as shining ultraviolet light on the wall ? no , you just get a mixture of red and green photons hitting a wall . similarly , emission spectral lines do not add their frequencies to form a single frequency , but rather they coexist independently . helium glows reddish because the dominant visible lines are red and yellow .
i do not see anything wrong with what you have done . conservation of momentum , and conservation of total angular momentum will hold exactly ( as you assumed ) . but what you are doing is an " inelastic collision " so energy is not typically conserved . so the " unaccounted for " energy appears as heat .
short answer : yes , the rate at which we would receive photons from the emitter would slow down , and the photons be redshifted . no , this could not account for dark matter in the universe . rate of photons if you think about relativistic doppler shift of photons as a slowing down of frequency due to relativistic time dilation , rather than a change in wavelength , it is quite easy to see that the redshift of the photons and the slower rate at which they will be received by the observer is the exact same effect . remember that $$ \delta t = \gamma \delta \tau , \\ \gamma = \frac{1}{\sqrt{1-v^2/c^2}} , $$ with $\tau$ being the time as measured on a clock in the emitter 's frame and $t$ the time on a clock in the observer 's frame . the graph for $\gamma$ as a function of $v$ looks like this ( from wikipedia ) : ( the nice thing about $\gamma$ is that it is often just a multiplicative factor , so remembering its shape gives a good intuition for how many things in sr work . ) so , the time between two wave tops in a photon will be $\tau$ in the emitting frame , and $\gamma \tau$ in the observing frame , and looking at the figure we get a good grasp how that time - and thus the redshift of the photon - will develop as $v \rightarrow c$ . but the exact same effect will of course happen to the time span between the emission of two subsequent photons/light pulses , so yes , the frequency with which they arrive will approach zero as $v$ approaches $c$ . explanation for dark matter ? it is true that particles moving at relativistic speeds would have a larger mass than the same particles at rest relative to us . but we should remember that from symmetry arguments , there would be about as many emitting particles moving towards us as away from us at those speeds . particles approaching us at relativistic speed will blueshift their photons emitted in our direction ( remember that $v$ is negative at motion towards us and thus $\gamma$ gets smaller with higher speed of approaching ) , and so we would definitely observe if there was a lot of matter moving at relativistic speeds around us . there is of course the cosmologically redshifted photons emitted from distant parts of the universe that are receding from us due to cosmic expansion , but the redshift is gradual as a function of distance ( and we should also remember that sr does not apply on these scales ) . another problem with the dark matter scenario is that we can observe the effects of dark matter everywhere , even where we still observe lots of ordinary , luminous matter , e.g. in the halos of nearby galaxies . these haloes are making up the bulk ( $\sim 90\%$ ) of the galactic mass , and can definitely not be receding from us at relativistic speed , without taking the luminous matter of the galaxy with it , which we can easily observe that it does not . similarly , if we look at e.g. the bullet cluster , we can observe that dark matter also on a larger scale is holding together systems of luminous matter which are definitely not receding from us at anything resembling light speed .
in the first case , the vertex is a vertex in the common sense ( used to construct diagrams ) . in the second case , the gauge field is not dynamic ( in a path integral formulation , you do not integrate over ) , it is a background field that is fixed . in that case , we are interested on the effect of this non-dynamical field on the electron field . this is useful to study , for example , the probability to create electron-positron pairs from the vacuum when there is a ( very ) strong electric field ( imposed by the outside , say , the experimentalists in the lab ) . edit : to be more technical on the second case ( the non-dynamical field ) : let 's have a look at the partition function $$z [ a ] =\int d\psi e^{i s_0 [ \psi ] +s_a [ \psi ] } , $$ where $s_0$ is the standard free fermion action , and $$s_a [ \psi ] =\int d^4 x \tilde a_\mu\bar\psi\gamma^\mu \psi . $$ notice that we do not integrate over $\tilde a_\mu$ in the functional integral . however , introducing $s_a [ \psi ] $ implies that a new vertex has to be used to compute the partition function , denoted by this wiggly line with crossed circle in the op 's question . what is the point ? first , we see that $\tilde a_\mu$ couples to the fermions as a usual e and m field . therefore , if the system we want to described is given by some fermions in a classical e and m field , we can modelize that by using this $\tilde a_\mu$ ( the assumption here is that the effects of the fermions on the e and m is negligible ) . second , by deriving $\ln z$ with respect to $\tilde a_\mu$ , we can compute the the correlation functions of the current . in this case , $\tilde a_\mu$ plays the role of a source term . if the e and m field is dynamical , we have to integrate over and now $$z=\int d\psi da e^{is [ \psi , a ] }$$ where $$s [ \psi , a ] =\int d^4x\left ( \bar\psi \gamma^\mu ( \partial_\mu-a_\mu ) \psi+\frac{1}{4}f_{\mu , \nu}f^{\mu , \nu}\right ) . $$ now we integrate over $\psi$ and $a$ ( with no ~ ) and the partition function does not depend on any sources . $a$ plays the role of a dynamical photon , with its own propagator and there is now the standard vertex interaction .
the charge of an atom is defined by its constituent number of protons/electrons and local fluctuations in their density distribution which cause instantaneous dipoles , unless we are talking about ions which have a permanent charge . charge is a classical concept that has real meaning in classical physics and can be described in various fields ( magnetic/electric ) using electrodynamics and maxwells laws . even simple dipole-dipole interactions between non-ideal gases are accounted for albeit in a limited way ) by classical van der waals theory . at a very simple level : you do not require a quantum description to account for the charge of an atom . of course in order to obtain any real data one would need to use ab-initio quantum calculations such as hatree-fock or various variational methods and other assorted computational methods such as density functional theory but this focuses on the density operator while the former uses the wavefunction . the concept of charge is as a parameter which can be fine tuned to obtain the useful detailing of the system under investigation i.e. coupling interactions/repulsion terms/correlations . . . etc . . . . spin on the other hand is a little different . there are two types of spin : -the intrinsic spin of a particle ( is defined by what type of particle we have ) and , -the orbital spin . these two types of spin are crudely described by classical celestial physics and are likened to planetary motion , the earth orbiting the sun yet rotating about its own axis . in so much as an electron can be modelled by a planet ( bohr theory ) then yes spin does have a classical interpretation . in order to properly describe the spin-orbit interaction you will need relativistic qm as schrodinger wave mechanics does not account for the intrinsic coupling . this is evident in the singlet/triplet states of helium for instance . the dirac equation is the equation of choice in canonical quantum mechanics , while there are other quantum field theory approaches you could take to use a lagrange approach which i leave to someone more adept than i ( at three o'clock in the morning at least ! : ) in summary : charge is described by both , but the " true " picture short of a hard shell particle is left to qm . while spin has only tentative parallels with classical mechanics and in general cannot ( as far as i am aware ) be described to any useful detail by classical mechanics or introductory qm , instead requiring a second quantisation or relativity in some form . cheers .
first of all , this type of time reversibility is the feature not of laminar flows per se , but of a special subclass of them : stokes flows ( also known as creeping flows ) . creeping flow is described by linear system of equations ( stokes equations ) that have the property of instantaneity , that is the solution depends on time only through time-dependant boundary conditions . this means that once ( as in video ) you stop moving the handle the movement of fluid instantly stops . coupled with linearity of equations this ultimately means that the current position of fluid particles ( including colored fluid ) in the jar depends only on the angle of rotation of the handle and not on how it was achieved , that is not on speed of handle movement . this means that if the direction of handle rotation will be reversed the flow will also be reversed and once the handle resumes its original position the colored particles also should be in their original place . however , this description in terms of creeping flow is only approximate as demonstrated in video since the form of colored blobs is deformed in the end . as for thermodynamics , while it may seem that we have reversal of fluid mixing , in this case no actual fluid mixing occurs because the colored fluids in the ' mixed ' state are actually aligned in spiral-like shapes along the walls of the jar and differently coloured spirals though close , do not intersect and so no mixing occurs . and , of course , we do not need to involve quantum mechanics in explanation .
yes , absolutely . in fact , gauss 's law is generally considered to be the fundamental law , and coulomb 's law is simply a consequence of it ( and of the lorentz force law ) . you can actually simulate a 2d world by using a line charge instead of a point charge , and taking a cross section perpendicular to the line . in this case , you find that the force ( or electric field ) is proportional to 1/r , not 1/r^2 , so gauss 's law is still perfectly valid . i believe the same conclusion can be made from experiments performed in graphene sheets and the like , which are even better simulations of a true 2d universe , but i do not know of a specific reference to cite for that .
this is a consequence of the fact that there is no positive frequency function which is zero outside the light cone . if you have a particle in relativity , its dynamics require that it goes faster than light , and to restore causality , it must go back in time . this is explained in my answer here : is anti-matter matter going backwards in time ? . if you have a quantum particle with positive energy , the propagation function $g ( x-y ) $ is the amplitude to go from x to y . this propagation is said to be causal if the propagator is zero unless x is to the future of y , so that in a time-space decomposition , $g ( t , r ) $ is zero for t&lt ; 0 . in this case , the fourier transform $g ( \omega , k ) $ cannot vanish for all $\omega&lt ; 0$ , because it is impossible for a nonzero function and its fourier transform to both be exactly zero in a half-plane . to see this , the condition of vanishing of $g ( t , r ) $ for t&lt ; 0 implies the analyticity of the fourier transform for $\omega$ with a negative imaginary part , since in this region , the fourier transform of g becomes a sum of decaying exponentials . an analytic function can not be zero in a region without being zero everywhere , so the fourier transform of a future directed function is not strictly positive energy . because of this , there is no relativistic particle formalism in which the particles both have positive energies and causal propagation . you can either deal with fields , in which case the particle notion is nonlocal , or you can deal with particles , but then they go back in time . the back-in-time formalism is using the standard noncausal feynman propagator , which is $$ g ( omega , k ) = {i\over \omega^2 - k^2 - m^2 - i\epsilon}$$ up to numerator modifications for higher spin , with the $i\epsilon$ pole prescription . this has two poles in $\omega$ for any $k$ , and the pole prescription pushes one pole to have slightly positive imaginary part and the other pole to the slightly negative imaginary part . there are singularities in both directions in imaginary $\omega$ direction , which means that the propagation is noncausal . the part that goes forward in time is the positive energy part , and the part that goes back in time is the negative energy part .
i found the original text at http://babel.hathitrust.org/cgi/pt?id=uc1.b2619178 ( page 5 onwards ) . the relevant pages and key translation ( each time i place the page before the translation - and i will repeat snapshots of equations in the translation where appropriate ) : 1 ) contrary to the work by h blasius , in this work we treat the problem of a body of revolution with its axis along the direction of the current . it is convenient to use the following coordinate system for the boundary layer ( figure 2 ) 2 ) we must transform the basic hydrodynamic equations into this framework . in the cartesion framework , they are ( in vector notation ) transformation of the acceleration components $ ( \upsilon\nabla ) \upsilon$ results in components like $u\frac{du}{dx}$ , $v\frac{du}{dy}$ etc . as well as centrifugal forces like $\frac{u . v}{r}$ and $\frac{u^2}{r}$ . those components that appear in the expression for x components may be omitted since they are of order $s$ compared to other components like $u\frac{du}{cx}+v\frac{du}{dy} \sim 1$ the resulting shear force , according to figure 3 , is so thus , the following is left from the dynamic equation for the x direction : estimating the order of magnitude of the y equation yields $\frac{\partial p}{\partial y} \sim \frac{u^2}{r} \sim 1$ from this we conclude that $\frac{\partial p}{\partial x}$ in the boundary layer differs from $\frac{\partial p}{\partial x}$ in the potential flow field by an order of $s$ , which we will neglect since we only preserve components $\sim 1$ . then we can use $\frac{\partial p}{\partial x}$ that was computed in the potential flow field for the boundary layer . with that , the y equation has been fully exhausted for our use - it could only serve for more subtle calculations of $p$ in the boundary layer . we want to write the continuity equation : where for a moment we consider $\xi , \eta$ , and $\zeta$ as the rectangular coordinates . we then take the very general transformation $$\xi=\xi ( x , y , z ) ; \eta=\eta ( x , y , z ) ; \zeta=\zeta ( x , y , z ) ; $$ by elementary computation , one finds the new continuity equation , in which : the expression thus found is quite generally valid . ===== i did the best i could . it does not answer your question exactly but i hope you can take it from here . feel free to ask questions in the comments .
the best you can do without reference to kinetic theory is apply the ideal gas law $p = \frac{nrt}{v}$ where $p$ is the pressure , $n$ is the number of moles of gas , $r$ is the ideal gas constant and $v$ is the volume . once the container is closed the pressure will obey this law ( assuming and forces acting on the gas such as gravity are negligible ) it may change if there are changes in the volume or temperature , so it is independent of the atmospheric pressure outside from the moment it is sealed . if you want an explanation of the ideal gas law you need kinetic theory .
why would you assume they do not ? of course they do . but as you probably know , the gravitational pull decreases with distance ( inverse square law ) . from a safe enough distance any other object ( star , galaxy ) would feel the normal gravitational pull of an object of the black-hole 's mass at that distance ; it makes no difference to the stars if the source is a black hole or anything else .
the simplest $su ( 5 ) $ gut higgs transforms as ${\bf 10}$ under the gauge group , an antisymmetric tensor $5\times 4/2\times 1$ with two indices of the same kind ( without complex conjugation ) . the 2-dimensional representation of $su ( 2 ) $ has an antisymmetric invariant $\epsilon_{ab}$ and if you extend this antisymmetric tensor to 5-valued indices of $su ( 5 ) $ and only make the $ab=45$ component nonzero , it will break the $su ( 5 ) $ down to $su ( 2 ) $ rotating $45$ and $su ( 3 ) $ rotating the remaining $123$ . one could a priori think about other representations , for example ${\bf 15}$ , the symmetric tensor with two indices $5\times 6/2\times 1$ . it passes the basic test : you may imagine it determines a bilinear form on the 5-dimensional fundamental representation that has a different coefficient for the group of 3 basis vectors and different for the remaining 2 basis vectors among the 5 , so something that tells you $$ds^2 = a ( da^2+db^2+dc^2 ) +b ( dd^2 +de^2 ) $$ where $a , b$ are different complex coefficients and $ ( a , b , c , d , e ) $ is a complex 5-dimensional " vector " in the fundamental representation . it is easy to see that distinct values of $a , b$ break the rotational symmetry $su ( 5 ) $ between all five $ ( a , b , c , d , e ) $ to $su ( 3 ) \times su ( 2 ) $ between $a , b , c$ and $d , e$ separately . it is hard to write realistic potentials for this one  and moreover , the hypercharge $u ( 1 ) $ which should be composed of the $u ( 1 ) $ factors in the $u ( 2 ) $ , $u ( 3 ) $ subgroups  will not arise properly ( the bilinear form above is not invariant under any such $u ( 1 ) $ )  but there exist other , larger representations for the higgs in $su ( 5 ) $ that can potentially do the breaking job . in $so ( 10 ) $ gauge theories , one usually needs a 16-dimensional representation to do the higgsing to $su ( 5 ) $ . the $su ( 5 ) $ is the subgroup preserved by a single chiral spinor . there may also be a 126-dimensional higgs multiplet to do similar things ( antisymmetric , self-dual , with 5 indices ) but i do not want to list all group theory used in grand unification here . in string theory , the breaking of the gut gauge group often proceeds by non-field-theoretical mechanisms such as fluxes and the wilson lines around some cycles in the compactified dimensions . the wilson line is a monodromy , an element of the original unbroken gauge group , and the gauge subgroup that commutes with the monodromy remains unbroken . it has some advantages because the required higgs fields in gut theories ( and their potentials ) may be rather messy and moreover , the stringy approach may justify more structured yukawa couplings for various quarks and leptons which is probably needed . gut theories have their characteristic energy scale , the gut scale , so all massive things such as the $x , y$ new gauge bosons as well as the new gut higgses are naturally this heavy , near $10^{16}\ , {\rm gev}$ . there are other ways aside from the proton decay constraints to derive this energy scale - it is the scale at which properly normalized three standard model gauge couplings approximately unify ( almost exactly when supersymmetry is added ) . so before one answers your question , it must be reverted . the right question is why the other fields ( and dimensionful parameters ) are so immensely light relatively to the gut scale . because most of them are derived from the electroweak higgs mass to one way or another ( gluon is formally massless although it is confined at the qcd scale , and one explains the qcd scale as the scale at which the slowly logarithmically running qcd coupling just grows to 1 if we run from a reasonable value near the gut scale ) , this question really asks why the electroweak higgs boson is so much lighter than the gut scale . this question is known as the hierarchy problem and it is been the primary mystery that was driving much of the work in phenomenology and model building although the lhc , by its seeing nothing new , is increasingly suggesting that there may be no " nice answer " to this puzzle at all .
http://www.khanacademy.org/#physics scroll down till #145 , that is where optics starts . if you want just lenses , start with the ' virtual image ' one . if not for your online-only request , i would have suggested resnick-halliday-walker . imho , that is just about the best book for classical anything
i think it is a great question , and enjoyed it very much when i grappled with it myself . here 's a picture of some of the forces in this scenario . $^\dagger$ the ones that are the same colour as each other are pairs of equal magnitude , opposite direction forces from newton 's third law . ( w and r are of equal magnitude in opposite directions , but they are acting on the same object - that is newton 's first law in action . ) while $f_{matchbox}$ does press back on my finger with an equal magnitude to $f_{finger}$ , it is no match for $f_{muscles}$ ( even though i have not been to the gym in years ) . at the matchbox , the forward force from my finger overcomes the friction force from the table . each object has an imbalance of forces giving rise to acceleration leftwards . the point of the diagram is to make clear that the third law makes matched pairs of forces that act on different objects . equilibrium from newton 's first or second law is about the resultant force at a single object . $\dagger$ ( sorry that the finger does not actually touch the matchbox in the diagram . if it had , i would not have had space for the important safety notice on the matches . i would not want any children to be harmed because of a misplaced force arrow . come to think of it , the dagger on this footnote looks a bit sharp . )
consider the fact that $$\rho_\beta :=z^{-1}_\beta e^{-\beta h}$$ is trace class by definition in a given ( separable ) hilbert space $\cal h$ , and thus it can be expanded as : $$\rho_\beta = \sum_{j\in \mathbb n} p_j ( \beta ) |\psi_j\rangle \langle \psi_j|\quad \mbox{where } 0 \leq p_j ( \beta ) \leq 1 \mbox{ and} \sum_j p_j ( \beta ) =1\quad ( 1 ) $$ above i use the convention $p_j \leq p_{j+1}$ ( where the number of $j$ with a common fixed value $p_j$ is finite because $\rho_\beta$ is compact ) and the convergence is in the strong operator topology . in the considered case , more strongly ( it turns out to be important shortly ) : $$p_j ( \beta ) = e^{-\beta e_j}&gt ; 0 \quad \forall j\: , \forall \beta &gt ; 0\: . $$ fix another hilbert basis $\{\phi_j\}_{j\in \mathbb n} \subset {\cal h}$ and , in ${\cal k}:= {\cal h}\otimes {\cal h}$ , define the normalized vector : $$\psi_\beta := \sum_{j \in \mathbb n} \sqrt{p_j ( \beta ) } \psi_j \otimes \phi_j\: . \quad ( 2 ) $$ the triple $\left ( {\cal k} , \pi , \psi_\beta \right ) $ is a gns triple for $\rho_\beta$ if : $$\pi : b ( {\cal h} ) \ni a \mapsto a \otimes i\quad ( 3 ) $$ the only thing to be proved , because not self-evident , is that $\{ a\otimes i \psi_\beta\:|\: a \in b ( {\cal h} ) \}$ is dense in $\cal k$ . for every $j$ and $k$ , we may pick out $a_{jk} \in b ( \cal h ) $ such that $a_{jk}\psi_j = \sqrt{\frac{1}{p_j ( \beta ) }}\psi_k$ ( notice that all $p_j$ are strictly positive ) and $a_{jk}\psi_{j'}=0$ if $j\neq j'$ . therefore $\pi ( b ( \cal h ) ) \psi_\beta$ includes every product $\psi_k \otimes \phi_j$ and thus every linear combinations of these products . since $\{\psi_k\otimes \phi_j\}_{k , j \in \mathbb n}$ is a hilbert basis of ${\cal h}\otimes {\cal h}={\cal k}$ , we can infer that $\{ a\otimes i \psi_\beta\:|\: a \in b ( {\cal h} ) \}$ is dense in $\cal k$ . to conclude , notice that i never used the value of $\beta$ barring in the definition of $\psi_\beta$ . consequently , changing $\beta$ we can use the same gns hilbert space and gns representation . just the identity map is the wanted unitary operator $u : {\cal k} \to {\cal k}$ verifying $u \pi_\beta ( a ) = \pi_{\beta'} ( a ) u$ .
special relativity is based on the invariance of a quantity called the proper time , $\tau$ , which is the time measured by a freely moving ( i.e. . not accelerated ) observer . the proper time is defined by : $$ c^2d\tau^2 = c^2dt^2 - dx^2 - dy^2 - dz^2 $$ this is similar to pythagoras ' theorem as learned by generations of schoolchildren , except that it includes time ( converted to a distance by multiplying by $c$ ) and it has a mixture of plus and minus signs . the mixture of signs is responsible for all the weird effects like time dilation and length contraction , and because there is a mixture of signs the value of $d\tau^2$ can be positive , negative or zero . if $d\tau^2$ is less than zero then $d\tau$ must be imaginary , and therefore unphysical . a quick bit of maths will show you that $d\tau^2$ can only be negative if you travel faster than light , and therefore that $c$ is the fastest speed anything in the universe can travel . so $c$ is special because it determines a fundamental symmetry of the universe . footnote : i have said $c$ is special while kostya has said the opposite , but actually we are both right . kostya is right that there is nothing special about the speed 299,792,458 m/s ( though if you change it by much you will change physics enough that we may not be here :- ) . however the speed at which light travels is very special because anything travelling at this speed follows a null geodesic , i.e. $d\tau^2 = 0$ . this is the sense in i mean that $c$ is special .
important notice : my previous result was a little bit incorrect . i found the factor $1/2$ by comparison with the textbook v.v. batygin , i.n. toptygin problems in electrodynamics . let 's denote the radius of the inner sphere $s_1$ as $a$ , the radius of the outer sphere $s_2$ as $b$ and the displacement as $c$ , so that $c\ll a , b$ . we choose the origin of the coordinate system to be in the center of the inner sphere . then , up to the second order in $c$ the distance from the origin to the outer sphere has the form : $$ r\left ( \theta\right ) =b+c\cos\theta . $$ therefore , the potential in the space between them can be found as $$ \phi=\left ( \alpha_{1}+\frac{\beta_{1}}{r}\right ) +c\left ( \alpha _{2}r+\frac{\beta_{2}}{r^{2}}\right ) \cos\theta , $$ where $\alpha_{i}$ and $\beta_{i}$ are constant which should be found from the boundary conditions : $$ \left . \phi\right\vert _{s_{1}}=const , \quad\left . \phi\right\vert _{s_{2} }=0 , \quad \oint_{s_{1}} \mathrm{d}s\ , \mathbf{n}\cdot\mathbf{\nabla}\phi=-4\pi q , \ , $$ where $\mathbf{n}=\mathbf{r}/r$ . hence the potential reads as follows : $$ \phi=q\left ( \frac{1}{r}-\frac{1}{b}\right ) +\frac{qc}{b^{3}-c^{3}}\left ( r-\frac{a^{3}}{r^{2}}\right ) \cos\theta . $$ therefore the potential on the inner sphere does not depend on $c$ up to the second order : $$ \left . \phi\right\vert _{s_{1}}=q\ , \frac{b-a}{ab} . $$ the charge distribution on the inner sphere can be found as follows : $$ \sigma=-\frac{1}{4\pi}\left ( \mathbf{n}\cdot\mathbf{\nabla}\phi\right ) =-\frac{1}{4\pi}\left . \frac{\partial}{\partial r}\ , \phi\right\vert _{r=a}=\frac{q}{4\pi a^{2}}\left ( 1-\frac{3a^{2}c}{b^{3}-c^{3}}\cos \theta\right ) . $$ hence , the force acting on the inner sphere has the form : $$ \mathbf{f}=-\frac{1}{2}\oint_{s_{1}} \mathrm{d}s\ , \sigma\mathbf{\nabla}\phi , $$ $$ f =-\frac{q^{2}}{4}\int_{-1}^{1}\mathrm{d}\cos\theta\ , \left ( 1-\frac{3ca^{2}}{b^{3}-c^{3}}\cos\theta\right ) \left . \frac{\partial }{\partial z}\left [ \frac{1}{r}+\frac{c}{b^{3}-c^{3}}\left ( 1-\frac{a^{3} }{r^{3}}\right ) z\right ] \right\vert _{r=a}\\ =-\ , \frac{q^{2}c}{b^{3}-c^{3}} , $$ where i use the trivial identity : $$ \frac{\partial}{\partial z}\frac{1}{r^{n}}=-\frac{nz}{r^{3}} . $$ the capacity $c$ can be found from the potential energy : $$ u=\frac{cv^{2}}{2}\quad\rightarrow\quad f=-\frac{\delta u}{\delta c} =-\frac{\phi^{2}}{2}\frac{\delta c}{\delta c} , $$ thus $$ \frac{\delta c}{\delta c}=-\frac{2f}{\phi^{2}}=\frac{2ca^{2}b^{2}}{\left ( b^{3}-a^{3}\right ) \left ( a-b\right ) ^{2}} . $$ finally , we obtain $$ c=\frac{ab}{b-a}+\frac{a^{2}b^{2}c^{2}}{\left ( b^{3}-a^{3}\right ) \left ( a-b\right ) ^{2}}\quad\quad ( 1 ) $$ update : the comments given above give the reference to the article of capacitance bounds for geometries corresponding to an advanced simulator design by m.i. sancer and a.d. varvatsis . the article in turn contains the reference to the book : w . r . smythe , static and dynamic electricity , mcgraw-hill , new york , 1950 where the following exact result for the capacitance is presented : $$ c=ab\sinh\alpha\sum_{n=1}^{\infty}\frac{1}{b\sinh n\alpha-a\sinh\left ( n-1\right ) \alpha} , \quad\quad\left ( 2\right ) $$ so that $$ \quad\cosh\alpha=\frac{a^{2}+b^{2}-c^{2}}{2ab} . $$ sancer and varvatsis claim that they found the approximation of the exact result in the $c\rightarrow0$ limit : $$ c=\frac{ab}{b-a}\left [ \frac{1}{2}\left ( \sqrt{\frac{1-y^{2}/\left ( 1+x\right ) ^{2}}{1-y^{2}/\left ( 1-x\right ) ^{2}}}+1\right ) + \frac{x}{2}\left ( \sqrt{\frac{1-y^{2}/\left ( 1+x\right ) ^{2}}{1-y^{2}/\left ( 1-x\right ) ^{2}}}-1\right ) \right ] , \quad\quad\left ( 3\right ) $$ where $$ x=\frac{a}{b} , \quad y=\frac{c}{b} . $$ it is easy to see that the expansion of the result ( 3 ) does not coincide with mine result ( 1 ) . the numerical comparison of all three results presented in the figure below : one can see that the result ( 3 ) of sancer and varvatsis is incorrect .
the general metaphor here is that you are gambling . " you can not win . " yes , this means your profit can not be greater than zero . you go to your friend 's house to play poker , and when you go home that night , you can not end up with more money ( energy ) than you started with . " you can not break even . " to " break even " means to go home after gambling with the same amount of money you had originally . saying you can not break even means that you can not have zero profit . entropy will increase , so the amount of energy available to do mechanical work must decrease . " you can not get out of the game . " the game is the same gambling game being implied in the first two metaphors . the application of the metaphor to the third law does not completely make sense to me ; maybe it is just not an exact metaphor . various sources online give interpretations like " because absolute zero is unattainable " or " there is no way to escape rules 1 and 2 because it is impossible to reach absolute zero . " it makes sense to me that lowering an object 's temperature to absolute zero -- which is impossible by the third law -- would in some sense mean that that object was " out of the game " thermodynamically , but it still would not constitute a counterexample to the first or second laws .
the kinetic energy of the bullet is $\frac12 mv^2 \approx 1$kilojoule . if the deceleration is continuous over $x=1$meter , energy conservation gives us an acceleration $a = v_\text{initial}^2/2x \approx 65,000\ , \mathrm{m/s^2} \approx 6600\ , g$ , and the stopping time is $t = v_\text{initial}/a \approx 5.6$milliseconds . spreading the bullet 's energy over the stopping time gives an average power of 175kilowatts . if you make some hand-waving assumptions that the mechanism for stopping the bullet is inefficient you might multiply this power by a factor of 10100 . this is a lot of power ! but the time interval is very brief . and it is certainly not prima facie unphysicalafter all , the gunpowder explosion that launched the bullet involved the same energy transfer and an acceleration length of much less than a meter . after some thinking , and a silly mistake , i can make an order-of-magnitude estimate of the magnetic field that would have to be involved . i would expect that the main effect involved in rapidly stopping a bullet would not be diamagnetism , a small effect where the magnetic field strength inside a " non-magnetic " material is changed in its fourth or fifth decimal place ( and thus the energy density of the field $e\propto b^2$ is changed in its eighth or tenth decimal place ) . the predominant factor on introduction of a strong magnetic field to a bullet would be eddy currents in the material . wikipedia gives me a formula for energy loss due to eddy currents in a material , $$ p = \frac{\pi^2 b^2 d^2 f^2}{6k\rho d} $$ where $p$ is the power in watts per kilogram , $b$ is the peak field , $d$ is the thickness of the conductor , $f$ is the frequency , $k$ is a dimensionless constant which depends on the geometry , $\rho$ is the resistivity , and $d$ is the mass density . wiki gives $k=1$ for a thin plane and $k=2$ for a thin wire , so i wild-guess $k=3$ for a zero-dimensional bullet . using values for the lead core of the bullet , we find the rate of field change \begin{align*} ( bf ) ^2 and = \mathrm{ \frac{18}{\pi^2} \frac{2\times10^{-7}\ , \omega\ , m \cdot 10^4\ , kg/m^3}{ ( 10^{-2}\ , m ) ^2} \frac{2\times10^5\ , w}{8\times10^{-3}\ , kg} } = \mathrm{10^{9} \frac{n^2}{c^2\ , m^2} }\\ {}\\ bf and = \mathrm{ \pi\times10^4\ , t/s } \end{align*} the simplest assumption about the frequency is that the field is being ramped up to its maximum while the bullet stops , so we have seen a quarter-oscillation and $1/f = 20\ , \mathrm{ms}$ . this gives us a peak field of 600tesla , which is large , but not absurdly large . on the other hand , if magneto is actually an fm radio broadcaster at 100mhz , he had need only a field of $$\mathrm{ \frac{ \pi\times10^4\ , t/s }{ 10^8\ , hz } = \pi\times10^{-4}\ , t . }$$ i do not think that radio engineers ordinarily think about local peak magnetic field strengths , but this is not outrageous either . my college npr station has a 100kw transmitter . however , their antenna is not shaped correctly to put that entire power into a one-cc volume .
gauss 's law is always true ( that is , numerically ) , but it is not always useful for calculating electric fields . it is only useful for calculating a charge distribution 's electric field when certain symmetries ( e . g . cylindrical , spherical , or planar ) are present that allow the surface integral to be done very simply . it is not useful for calculating electric fields when these symmetries are not present in a problem .
you do not need to use lorenz factor to answer ( a ) . you only need to notice that light propagates at c and that firecrackers go off simultaneously ( relative to table and to stationary observer at either end of the table ) . then observer at x = 0 will notice light from the first firecracker after 1/c . ( t = s/v that is after 1/c . the second after 2/c . ) the second question is a bit more tricky . what is simultaneous to one observer ( call him stationary ) is not simultaneous to another observer moving relative to the first one . here you will need to employ special relativity .
this is not a physics answer , but perhaps it could point the way to one . although : here 's some physics . assuming that with " static " you mean " hovering " , i think an ornithopter ( from greek ornithos " bird " and pteron " wing" ) could do the trick . it looks like fun ! and . . . the hobby variety seems to be able hover . 2008 . i built this four-winged rc ornithopter for a demonstration at iit bombay . it represents a concept for a manned ornithopter at 1/10th scale . the wingspan is 36 inches . the four-winged design gives this ornithopter excellent slow-flight capabilities , and it can even be configured for hovering flight . source and so can a possibly military version . not the mean machine you had expect ! : ) because ornithopters can be made to resemble birds or insects , they could be used for military applications , such as aerial reconnaissance without alerting the enemies that they are under surveillance . several ornithopters have been flown with video cameras on board , some of which can hover and maneuver in small spaces . in 2011 , aerovironment , inc . announced a remotely piloted ornithopter resembling a large hummingbird for possible spy missions . source
using your definition of " falling , " heavier objects do fall faster , and here 's one way to justify it : consider the situation in the frame of reference of the center of mass of the two-body system ( cm of the earth and whatever you are dropping on it , for example ) . each object exerts a force on the other of $$f = \frac{g m_1 m_2}{r^2}$$ where $r = x_2 - x_1$ ( assuming $x_2 &gt ; x_1$ ) is the separation distance . so for object 1 , you have $$\frac{g m_1 m_2}{r^2} = m_1\ddot{x}_1$$ and for object 2 , $$\frac{g m_1 m_2}{r^2} = -m_2\ddot{x}_2$$ since object 2 is to the right , it gets pulled to the left , in the negative direction . canceling common factors and adding these up , you get $$\frac{g ( m_1 + m_2 ) }{r^2} = -\ddot{r}$$ so it is clear that when the total mass is larger , the magnitude of the acceleration is larger , meaning that it will take less time for the objects to come together . if you want to see this mathematically , multiply both sides of the equation by $\dot{r}\mathrm{d}t$ to get $$\frac{g ( m_1 + m_2 ) }{r^2}\mathrm{d}r = -\dot{r}\mathrm{d}\dot{r}$$ and integrate , $$g ( m_1 + m_2 ) \left ( \frac{1}{r} - \frac{1}{r_i}\right ) = \frac{\dot{r}^2 - \dot{r}_i^2}{2}$$ assuming $\dot{r}_i = 0$ ( the objects start from relative rest ) , you can rearrange this to $$\sqrt{2g ( m_1 + m_2 ) }\ \mathrm{d}t = -\sqrt{\frac{r_i r}{r_i - r}}\mathrm{d}r$$ where i have chosen the negative square root because $\dot{r} &lt ; 0$ , and integrate it again to find $$t = \frac{1}{\sqrt{2g ( m_1 + m_2 ) }}\biggl ( \sqrt{r_i r_f ( r_i - r_f ) } + r_i^{3/2}\cos^{-1}\sqrt{\frac{r_f}{r_i}}\biggr ) $$ where $r_f$ is the final center-to-center separation distance . notice that $t$ is inversely proportional to the total mass , so larger mass translates into a lower collision time . in the case of something like the earth and a bowling ball , one of the masses is much larger , $m_1 \gg m_2$ . so you can approximate the mass dependence of $t$ using a taylor series , $$\frac{1}{\sqrt{2g ( m_1 + m_2 ) }} = \frac{1}{\sqrt{2gm_1}}\biggl ( 1 - \frac{1}{2}\frac{m_2}{m_1} + \cdots\biggr ) $$ the leading term is completely independent of $m_2$ ( mass of the bowling ball or whatever ) , and this is why we can say , to a leading order approximation , that all objects fall at the same rate on the earth 's surface . for typical objects that might be dropped , the first correction term has a magnitude of a few kilograms divided by the mass of the earth , which works out to $10^{-24}$ . so the inaccuracy introduced by ignoring the motion of the earth is roughly one part in a trillion trillion , far beyond the sensitivity of any measuring device that exists ( or can even be imagined ) today .
both examples are using the same methods . in the first example , the break in the circuit removes the 6 ohm and 5 ohm resistors altogether . all of the current then flows through the 2 ohm resistor , causing the thevenin resistance to be 2 ohms . in the second example , the current first passes through $r_2$ , then is split between $r_1$ and $r_3$ in parallel . the parallel resistance between $r_1$ and $r_3$ is $\frac{r_1 r_3}{r_1 + r_3}$ , and adding on the $r_2$ which is in series to both of those , we get the thevenin resistance as $r_2 + \frac{r_1 r_3}{r_1 + r_3}$ . this value is the same as the one provided by your source . if you need any more clarification , feel free to ask in the comments .
which would be a more accurate expectation ? since this is a physics q and a forum , we should approach this question from the perspective of physics . until you have a hypothesis for the mechanism by which matter can be instantaneously moved from one point in time to a prior point in time , you have nothing on which to base predictions . so far as i know , there is a maximum speed that we can move through spacetime . if i were to move backwards in time , at the very least i would expect to gradually get smaller and younger , to know less and have my memories slowly evaporate .
since the acceleration is not constant , you need to start from the equation of motion and solve it directly . this is easiest if you consider the instantaneous balance of forces tangential to the circle . the weight has a component $mg\sin\theta$ in this direction , and the acceleration is $a=r\ddot\theta$ . ( that is not trivial to work out , by the way : have a good think about it . in fact , it is also an approximation . ) newton 's second law then reads $$\ddot\theta=-\frac gr\sin\theta . $$ now you need to solve this equation , which is a differential equation for the function $\theta=\theta ( t ) $ . if you are in a regime with large displacements , then there are some things you can do ( specifically , you can find a nice expression for the inverse function $t=t ( \theta ) $ in terms of an integral , but you can not solve that one exactly and you can not invert the relation ) but they are pretty limited . if you are in a regime with small displacements , then you can approximate $\sin \theta\approx\theta$ , and you are left with a harmonic oscillator ; $$\ddot\theta=-\frac gr\theta . $$ note that the acceleration is $r\ddot\theta=-g\theta$ and it is not constant ; it is proportional to the displacement when the latter is small . this is much easier to solve : trying with the functions $\theta ( t ) =\theta_0\sin ( \omega t ) $ and $\cos ( \omega t ) $ turns up two linearly independent solutions with $\omega=\sqrt{g/r}$ , and that is enough to solve the general problem .
because this is the most probable configuration . black-body light is the thermal equilibrium for light , so that anything that produces light which has a finite energy produces light that knocks around eventually to become blackbody light . the light we see was scrambled during the first 300,000 years by innumerable collisions with electrons and nuclei , and thermalized itself into equilibrium . there is no surprise when you see something in thermal equilibrium , the surprise in cosmology is that the temperature has small variations in different directions , which give us information about the inflationary process that gave birth to the light initially .
usually flow maps are integrals of vector fields . in other words , solutions to dr ( t ) /dt = v ( r ) . if you pick one specific value for the parameter t then you get one specific flow map that takes r ( 0 ) to r ( t ) as a ( bijective ) map on the manifold . a hamiltonian flow is one that is generated as by a symplectic form s and a hamiltonian potential h , as in v = s ( dh ) . edit : i think an example might help . consider the hamiltonian of a harmonic oscillator h = p^2 + x^2 . if we take the gradient in phase space we get ( dh/dx , dh/dp ) = ( 2x , 2p ) , which points radially away from the coordinate origin with a magnitude that is proportional to the distance from there . applying the standard symplectic form on phase space gives ( dh/dp , -dh/dx ) = ( 2p , -2x ) for the hamiltonian vector field . this field points tangentially around the origin and its integral lines are circles . the proportionality of the magnitude to the distance from the origin makes sure that the flow transports points at a constant angular velocity around the origin . the flow maps are therefore rotations , or in other words , the hamiltonian above generates rotations on phase space . generally if you look at the flow equation in the symplectic vector field dr/dt = v you get ( dx/dt , dp/dt ) = ( dh/dp , -dh/dx ) , which are just the hamilton equations of motion . so you see this is just a different way to look at the hamilton formalism .
in the 19th century , the physicists young and helmholtz proposed a trichromatic theory of color , in which the eye was modeled as three filters with overlapping ranges . this is essentially a physical model of the pigments in the eye , and it predicts the response of the nerve cells at the retina . helmholtz did related work on sound and timbre . ca . 1950 , hering , hurvich , and jameson proposed significant modifications to the trichromatic theory , called opponent processing . this models a later stage in the processing of the signals , after the retinal response but before the more sophisticated stages of processing in the brain . both the trichromatic model and opponent processing are needed in order to describe certain phenomena in human color perception . the complete theory can be modeled by two functions depending on wavelength . i will call these $rg ( \lambda ) $ and $by ( \lambda ) $ . these functions are drawn here . they both oscillate between positive and negative values . for any given pure wavelength $\lambda$ , the net result of pigment-filtering plus the later neurological processing produces these two numbers , which can be thought of as the final signals that go on to later processing in the brain . i am calling them $rg$ and $by$ for the following reasons . let 's pretend , for the sake of simplicity , that these functions oscillated between -1 and +1 . then the pair $ ( rg , by ) = ( 1,0 ) $ produces the sensation of red , ( -1,0 ) is green , ( 0,1 ) is blue , and ( 0 , -1 ) is yellow . there is various psychological evidence for this model , e.g. , no color is perceived as reddish-green or yellowish-blue . roughly speaking , what seems to be happening is that the eye-brain system is taking differences between signal levels of different cone cells . this sort of makes sense because , for example , the red and green pigments have response curves that overlap a lot , so if you want to place a pure-wavelength color on the spectrum , the difference between them is more a more direct measure of what you want to know than the individual signals . the $rg$ function actually has two different peaks , one at the red end of the spectrum and one , surprisingly , at the blue end . this implies that by mixing blue and red , you can produce an $ ( rg , by ) $ pair similar to what you would have gotten with monochromatic violet . if you look at other sources , e.g. , this one ( figure 3.3 ) , they seem to agree on the secondary short-wavelength peak of the $rg$ function , but the details of how the two functions are drawn at the short wavelengths are different and seem to make for a less convincing explanation of the observed perceptual similarity between violet and a red-blue mixture . i do not know if there is a valid reductionist explanation of the short-wavelength peak of the $rg$ function . like a lot of things produced by evolution , it may basically be an accident that got frozen in . however , it is possible that it serves the evolutionary purpose of helping us to distinguish different shades of blue and violet . if the $rg$ function was simply zero over the whole short-wavelength end of the spectrum , then the $by$ function would be the only information we had get for those wavelengths . but the $by$ function has a maximum , simply because the eye 's sensitivity to light fades out as you get into the uv . near this maximum , the ability of the $by$ function to discriminate between colors becomes zero . in the york university graph , it appears that the short-wavelength extrema of the $rg$ and $by$ functions are offset from one another , which would allow some color discrimination in this region . the physical information being preserved by the $by$ function would then be the difference in response between the blue and green cones . but the briggs graphs do not appear to show any such offset of the extrema , so it is possible that the explanation i am giving is a bogus " just-so story . " there may be a good analogy here with sound . the sound spectrum is linear , but there is a psychological phenomenon of octave identification , which makes the spectrum " wrap around , " so that frequencies $f$ and $2f$ are perceptually similar and can often be mistaken for one another even by trained musicians . similarly , the predictive power of the " color wheel " model shows that to some approximation we can think of the trichromatic/opponent process model as resulting in a wrapping around of the visible segment of the em spectrum into a circle . but in both cases , the wrap-around is only an approximation . in terms of pitch , $f$ and $2f$ are perceptually similar but not indistinguishable . for color , we have the 1976 cieluv color color diagram , which is a modification of the 1931 diagram meant to represent at least somewhat accurately the degree of perceptual similarity between different points based on the distance between them . the monochromatic spectrum constitutes part of the outer boundary of this diagram , and is more of a " v " than a circle ; there is quite a large gap between monochromatic violet and monochromatic red . it is trivially true that any such diagram has a boundary that is a closed curve . if the diagram is not constrained to give any accurate depiction of the sizes of the perceptual differences between colors , then it can be distorted arbitrarily , and we can arbitrarily define it such that its boundary is a circle . in this sense , the success of the color wheel model is guaranteed , and it follows from nothing more than the fact that humans are trichromats , so that the color space is three-dimensional , and controlling for luminance produces a two-dimensional space . but this fails to explain why there is some degree of perceptual similarity between the red and violet ends of the monochromatic spectrum ; for that you need the opponent processing model . there is also a slight variation in the absorbance of the pigment in the red cones at the blue end of the spectrum . i do not think this is sufficient to explain the perceptual similarity between violet and red , or the even closer similarity between violet and a mixture of red and blue light , i.e. , i do not think you can explain these facts using only the trichromatic theory without opponent processing . the classic direct measurements of the filter curves of cone-cell pigments were done with cone cells from carp by tomita ca . 1965 , but afaik the only direct measurement using human cone cells was bowmaker 1981 . bowmaker 's red-cell absorbance curve has a very slight rise at short wavelengths , but it is not very pronounced at all . you will see various other curves on the internet , often without any attribution or explanation of where they came from , and some of these show a much more pronounced bump rather than bowmaker 's slight rise . possibly some of these are from people using the cie 1931 curves , which were never intended to be physical models of the actual human cone-cell pigments . it should be clear , however , that the red and green pigments ' curves must have some variation near the violet end of the spectrum . if they did not , then the dimensionality of the color space would be reduced there , and the human eye would be unable to distinguish different wavelengths in this region , which is contrary to fact . bowmaker , " visual pigments and colour vision in man and monkeys , " j r soc med . 1981 may ; 74 ( 5 ) : 348 , freely accessible at http://www.ncbi.nlm.nih.gov/pmc/articles/pmc1438839/
you are right . like any physical process described by linear equations , there are limits . think of a sound so intense as to crush the cells in a sound absorbing foam , turning it into a hard surface . a reversible version of that foam is one where the bubbles do not get destroyed , they get flattened to the point that the material they are in starts to play a role , instead of the bubble compression .
for von neumann entropies you would have : $h ( ab ) _\rho\leq h ( a ) _\rho+h ( b ) _\rho$ $h ( ab ) _{\rho'}\leq h ( a ) _{\rho'} + h ( b ) _{\rho'}$ whatever happens in $b$ should not affect $a$ so : $h ( a ) _\rho=h ( a ) _{\rho'}$ for $b$ itself : $h ( b ) _\rho\leq h ( b ) _{\rho'}$ with equality if the decoherence process is in the basis that diagonalizes $b$ . and : $h ( ab ) _\rho\leq h ( ab ) _{\rho'}$ this is all textbook material , you can find it in nielsen and chuang or in wilde 's quantum shannon theory . now if you replace von neumann entropies by renyi entropies , the third to fifth relations are still valid , but subadditivity does not hold and you would have something a little bit weaker for any bipartite system : $s_\alpha ( ab ) \leq s_\alpha ( a ) + s_0 ( b ) $ this relation was proved in ( http://arxiv.org/abs/quant-ph/0204093 )
to quickly answer your question about the non-commutativity of matrices ( and more generally operators ) : the uncertainty relation is entirely a consequence of non-commutativity . the uncertainty relation that you are talking about is a consequence of a much more general statement . it goes something like this . . . let $a , b$ be operators ( thinking of the as matrices is fine ) and let $ab - ba = c$ ( operators , in general , do not commute , so this is usually nonzero ) . then after a little bit of work one can find $$ \delta a \delta b \geq \frac{1}{2} | \langle c \rangle | $$ where $\delta a = \sqrt{ \langle a^2 \rangle - \langle a \rangle^2 }$ . this follows from a famous inequality , cauchy-schwarz inequality . the uncertainty relation that you are familiar with $ \delta x \delta p \geq \frac{ \hbar }{ 2 } $ is because $ \hat{x} \hat{p}_x - \hat{p}_x \hat{x} = i \hbar $ ( the hats distinguish operators from numbers ) . on a final note , the argument in the previous answer about calculus being a fundamental basis for exploring quantum is wildly incorrect . the best language for understanding quantum mechanics and discovering deeper truths would be geometric algebra ( and , more generally , abstract algebra ) . geometric algebra 's are a extremely powerful language for doing quantum theory ( and physics for that matter ) . i am talking about clifford algebras , jordan algebras , lie algebras , and so on . there has been a massive paradigm shift in the past few decades in the setting theoretical physicists do quantum mechanics . this shift is entirely motivated by geometric algebras . there is , arguably , new physics lying within these algebraic structures , waiting to be unpackaged by the clever physicist who realizes the correct way to apply them . this is not to say calculus is not important . it will always play a role , but it is not the proper setting for understanding quantum theory and beyond . i could go much further with this argument , but it would rapidly get to mathematically sophisticated . since you are just looking for a little insight , there is no need to walk that path . edit : it has been a while since i wrote this and i would like to add something based on the other answers here . the fact that a quantum particle has a associated wave-function is not a fundamental tenet of quantum mechanics . the postulates of quantum mechanics are as follows states of a quantum system are described by normalized vectors in the associated hilbert space $h$ . for any classical observable $a$ there is a corresponding hermitian operator $\hat{a}$ acting in $h$ . conversely , any hermitian operator corresponds to some observable . a measurement of a yields one of the eigenvalues of the corresponding operator $\hat{a}$ . a measurement of $a$ on many identical copies of the system , all in the state $\left| \psi \rangle \right . $ , produces random results . the probability to find the eigenvalue $a$ of $\hat{a}$ is $\langle \psi \left| \hat{\mathbb{i}}_a \right| \psi \rangle$ , where $\hat{\mathbb{i}}_a$ is the projector onto the subspace generated by the eigenvectors associated with $a$ . equivalently , the probability density of $a$ can be written as $p ( a ) = \sum_a p ( a ) \delta ( a - a ) = \langle \psi \left| \delta ( a \hat{\mathbb{i}}_a - \hat{a} ) \right| \psi \rangle$ . a measurement affects the state of the system ( this is often referred to as the wave function collapse ) : right after the measurement the system is in the state $\left| \psi_a \rangle \right . \propto \hat{\mathbb{i}}_a \left| \psi \rangle \right . $ , a normalized eigenvector of $\hat{a}$ corresponding to the eigenvalue $a$ found in the measurement . between the measurements , the state of the system evolves according to the schrodinger equation $$ i \hbar \frac{d}{dt} \left| \psi \rangle \right . = \hat{h} \left| \psi \rangle \right . $$ where $\hat{h}$ is the hamiltonian operator of the corresponding physical system . that is it . all non-relativistic quantum theory stems from these axoims or , if you prefer , postulates . further i must add that calculus is not what is needed to probe deeper into quantum theory . if this was true physicists would have been finished as soon as they started . the entire reason quantum mechanics is so difficult to comprehend both physically and mathematically is because the the foundation of the theory is built on mathematics that is far more sophisticated that newton 's/leibniz 's calculus . today , most theories in modern physics do not stand on the shoulders of simple calculus , not even modern day classical mechanics ! for example , the mathematical model of the standard model of particle physics is group theory ( extremely important , start learning it ) , and we know that it is even limiting us ! there is an even more sophisticated foundation for the standard model called the clifford algebraic standard model , and the current hopes is that it can be the path way that help us understand the new physics scales neutrino masses are going to introduce . now of course you we will always use calculus , but my point is that it is old news . we are far far beyond this . if you want more examples of what i am talking about look up what things like lie theory have done for nuclear physics , what differential geometry and topology has done for general relativity and gravity , even look up how we formulate classical mechanics today !
the einstein field equations which relate physical quantities to curvature are : $$r_{\mu \nu} -\frac{1}{2}g_{\mu \nu} r = 8\pi g t_{\mu \nu}$$ the tensor which contributes to the curvature is the stress-energy tensor $t_{\mu \nu}$ which contains quantities such as energy density , shear stress and pressure . the tensor itself is computed from the lagrangian which governs the matter present . in field theories , we see via noether 's theorem that the stress-energy tensor is in fact the conserved current due to invariance ( up to a total derivative ) under global spacetime translations .
so we start with the definition of grand partition function , $\mathcal{z} = tr ( e^{-\beta ( \hat{h}-\mu\hat{n} ) } ) $ , with $\beta=1/k_bt$ , $\hat{h}$ the hamiltonian and $\hat{n}$ the operator of total particle number , which commutes with $\hat{h}$ . since you also want to evaluate the average occupation number for each spin , you may add to $\hat{h}$ a source term $\sum_{s_z}a_{s_z}\hat{n}_{s_z}$ , which amounts to replacing $\mu\hat{n}$ by $\sum_{s_z}\mu_{s_z}\hat{n}_{s_z}$ in $\mathcal{z}$ . see that $\hat{n}_{s_z}$ also commutes with $\hat{h}$ . next , we need the eigenvalues of $\hat{h}$ . since you are considering a non-interacting system of bosons , the eigenvalues are completely determined by the simple particle hamiltonian $h ( \vec{p} , s_z ) $ . the eigenvalues of $h ( \vec{p} , s_z ) $ is known to be landau levels , $\varepsilon_{n , s_z , p_z} = ( n+1/2 ) \omega_c - \mu_0bs_z + p^2_z/2m$ , where i assumed the magnetic field points along z-direction . note that each landau level is highly degenerate , i.e. , for each level , there is a subset of states ( in total $d$ ) . now the eigenvalues of $\hat{h}$ are just $e = \sum_{n , s_z , p_z , \nu}l^{\nu}_{n , s_z , p_z}\varepsilon_{n , s_z , p_z}$ , with $l^{\nu}_{n , s_z , p_z}$ the occupation of the $\nu$-th state of the subset $ ( n , s_z , p_z ) $ . now the $\mathcal{z}$ can be written as $\mathcal{z} = \mathcal{z}_+\cdot\mathcal{z}_-\cdot\mathcal{z}_0$ , where the factors $\mathcal{z}_{s_z}$ describes the $s_z$-sector . performing the trace in the diagonal representation of $\hat{h}$ , we find $$\mathcal{z}_{s_z} = \prod_{n , p_z , \nu} ( 1+e^{-\beta ( \varepsilon_{n , s_z , p_z}-\mu_{s_z} ) }+e^{-2\beta ( \varepsilon_{n , s_z , p_z}-\mu_{s_z} ) }+ . . . ) =\prod_{n , p_z} ( 1-e^{-\beta ( \varepsilon_{n , s_z , p_z}-\mu_{s_z} ) } ) ^{-d} . $$ from this , you can find the $\mathcal{z}$ . the average occupation for spin $s_z$ is obtained as $$\langle \hat{n}_{s_z}\rangle = \beta^{-1}\frac{\partial}{\partial \mu_{s_z}}~\ln\mathcal{z} , $$ which is understood in the limit $\mu_{s_z}\rightarrow\mu$ . the orbital effects of $\vec{a}$ refers to the fact that magnetic field quantises the motions in the normal plane of the magnetic field : the appearance of landau levels .
hamilton 's dynamics occurs on a phase space with an equal number of configuration and momentum variables $\{q_i , ~p_i\}$ , for $i~=~1 , \dots n . $ the dynamics according to the symplectic two form ${\underline{\omega}}~=~\omega_{ab}dq^a\wedge dp^b$ is a hamiltonian vector field $$ \frac{d\chi_a}{dt}~=~\omega_{ab}\partial_b h , $$ with in the configuration and momentum variables $\chi_a~=~\{q_a , ~p_a\}$ gives $$ {\dot q}_a~=~\frac{\partial h}{\partial p_a} , ~{\dot p}_a~=~-\frac{\partial h}{\partial q_a} $$ and the vector $\chi_a$ follows a unique trajectory in phase space , where that trajectory is often called a hamiltonian flow . for a system the bare action is $pdq$ ignoring sums . the hamiltonian is found with imposition of lagrangians as functions over configuration variables . this is defined then on half of the phase space , called configuration space . it is also a constraint , essentially a lagrange multiplier . the cotangent bundle $t^*m$ on the configuration space $m$ defines the phase space . once this is constructed a symplectic manifold is defined . therefore lagrangian dynamics on configuration space , or equivalently the cotangent bundle defines a symplectic manifold . this does not mean a symplectic manifold defines a cotangent bundle . the reason is that symplectic or canonical transformations mix the distinction between configuration and momentum variables . as a result there are people who study bracket structures which have non-lagrangian content . the rr sector on type iib string is non-lagrangian . the differential structure is tied to the calabi-yau three-fold , which defines a different dynamics .
a few basic checks : what size time steps are you taking ? way too big will lead to wild errors . way too small and changes in velocity and position will be incorrect due to roundoff errors ( finite precision ) . is the primary mass much larger than the orbiter ? in real life , the moon and earth orbit around a common inertial center . since the earth is much heavier , we can declare it stationary at the risk of small errors . i do not think this explains your odd results , however . but it could be a helpful clue to know who is not guilty . ( what was the famous sherlock quote ? ) what initial velocity are you giving the orbiter ? do you have a choice of integration method ? i can not imagine the wrong choice giving the result you get , but still , clues help . . . you really are telling it inverse square , right ? and it is inverse , right ? if you multiply not divide by r squared , that will explain centered ellipses .
dear john , the vacuum does not contain any physical higgs bosons ; it only contains a " higgs condensate " . so the vacuum does not break the lorentz symmetry . the potential energy energy for the higgs field looks like $$ v ( h ) = \frac{m_h^2}{2} ( h-v ) ^2 + o ( ( h-v ) ^3 ) $$ for $h$ near $v$ , so it is minimized by $h ( x , y , z , t ) =v$ . note that $v ( h ) $ is a lorentz scalar so when added to the lagrangian , it preserves the lorentz symmetry . the nonzero value of $h$ at each point is what breaks the electroweak symmetry because $h$ is a component of a doublet whose only symmetry-invariant point is $h=0$ . but because this configuration is constant , it does not pick any preferred reference frame . there are no physical particles in the vacuum state . physical higgs bosons may be added and they are energy quanta that allow $h$ to deviate away from $h=v$ - more than the uncertainty principle requires . those quanta carry energy and momentum $ ( e , p ) $ that satisfies $e^2 - p^2 = m_h^2$ . that is the energy one has to add to the energy of the vacuum state ; the latter state is free of particles and its energy may be chosen to be zero . ( let me not discuss the cosmological constant issues here because they have nothing to do with this question . ) when it comes to the difference between the vacuum , which has no particles , and states with particles ( higgs bosons ) , you may define a new field , $h&#39 ; =h-v$ , and this field will be expanded around zero just like the electromagnetic fields or other fields . the vacuum will be at $h&#39 ; =0$ and it may perhaps make it easier to understand why there is no lorentz-symmetry breaking in the vacuum . the $su ( 2 ) \times u ( 1 ) $ symmetry will act " non-linearly " on $h&#39 ; $ - in a way that can be easily deduced from $h&#39 ; =h-v$ .
i did the experiment . ( dipping wins ) h2o ice bath canning jar thermometer pot of boiling water stop watch there were four trials , each lasting 10 minutes . boiling water was poured into the canning jar , and the spoon was taken from the ice bath and placed into the jar . a temperature reading was taken once a minute . after each trial the water was poured back into the boiling pot and the spoon was placed back into the ice bath . temperature readings have an error $\pm$ 1 f ,  Red line, no Spoon Green line, Spoon in, no motion Aqua line, Stirring Blue line, Dipping 
since you say you are mostly interested in learning physics , not in a degree , there are lots of options . you say you have good knowledge of physics ; if you want to learn particle physics , you need a good undergrad physics background plus some additional knowledge . i tend to think that reading textbooks is still the best way to learn the basics ; if you want to learn particle physics , you might start with the book by griffiths . if you find that you do not have enough background knowledge for it , then you would need to read other , more basic textbooks . once you pass some threshold of background knowledge , though , you can learn a lot online . you could try searching arxiv . org for introductory lectures on various topics . in particle physics , for instance , you might search for the word " tasi " , which is a summer school for graduate students to learn more about the field ( the lectures are often written up and posted online ) . most of this will assume you already know quantum field theory , though . you can get a prepublication draft of mark srednicki 's qft textbook from his website ( pdf file ) . the perimeter institute has a one-year master 's degree program and videotapes all of the lectures . you can watch the videos from their archive .
spencer 's comment is right : we never " prove " anything in science . this may sound like a minor point , but it is worth being careful about . i might rephrase the question like this : what is the smallest size of the universe for which we have substantial observational evidence in support of the standard big-bang picture ? people can disagree about what constitutes substantial evidence , but i will nominate the epoch of nucleosynthesis as an answer to this question . this is the time when deuterium , helium , and lithium nuclei were formed via fusion in the early universe . the observed abundances of those elements match the predictions of the theory , which is evidence that the theory works all the way back to that time . the epoch of nucleosynthesis corresponds to a redshift of about $z=10^9$ . the redshift ( actually $1+z$ ) is just the factor by which the universe has expanded in linear scale since the time in question , so nucleosynthesis occurred when the universe was about a billion times smaller than it is today . the age of the universe ( according to the standard model ) at that time was about one minute . other people may nominate different epochs for the title of " earliest epoch we are reasonably sure about . " even a hardened skeptic should not go any later than the time of formation of the microwave background ( $z=1100$ , $t=400,000$ years ) . in the other direction , even the most credulous person should not go any earlier than the time of electroweak symmetry breaking ( $z=10^{15}$ , $t=10^{-12}$ s . ) i vote for the nucleosynthesis epoch because i think it is the earliest period for which we have reliable astrophysical evidence . the nucleosynthesis evidence was controversial as recently as about 10 or 15 years ago , but i do not think it is anymore . one way to think about it is that the theory of big-bang nucleosynthesis depends on essentially one parameter , namely the baryon density . if you use the nucleosynthesis observations to " measure " that parameter , you get the same answer as is obtained by quite a variety of other techniques . the argument for an earlier epoch such as electroweak symmetry breaking is that we think we have a good understanding of the fundamental physical laws up to that energy scale . that is true , but we do not have direct observational tests of the cosmological application of those laws . i would be very surprised if our standard theory turns out to be wrong on those scales , but we have not tested it back to those times as directly as we have tested things back to nucleosynthesis .
this is because the different parts of a stroke are be very different in distance . this reverberations and echoes of sound last longer than those of light . is obviously wrong . which walls or mountains are the reflectors of those echos ? which rooms do the reverberations ?
there are some recent efforts in trying to derive the mathematical structure of quantum mechanics from some reasonable and/or operational axioms . you may want to give a look , for example , at http://arxiv.org/abs/1011.6451 and references therein .
the increased ' resistance ' of an underinflated tyre is due to mechanical deformation , friction is independent of area as suggested . the simplest explanation for me is that : as area increases the applied force per unit area decreases , but there is more contact surface to resist motion . added as per zass ' suggestion below : $$\rm{friction}= \rm{material\ coefficient} \times \rm{pressure} \times \rm{contact area}$$ where the material coefficient is a measure of the ' grippiness ' of the material , the pressure applied to the surface and the area of the surfaces in contact . so we can see the area in the pressure term cancels with the third term . this is not to be confused with traction , where spreading the motive force over a larger area can help .
sound is a pressure wave : each vibration compresses the molecules and that compression propagates onwards . in order for sound to exist , we need something to be compressed . in space , the particles floating about probably are not close enough to compress and make anything audible ( i have not traveled to space to test this , and i do not think anyone who has gone to space has tried it either ) .
fission is exothermic only for heavy elements , while fusion is exothermic only for light elements . intermediate nuclei , in the iron/nickel range , are the most tightly bound , and so you generally release energy moving in that direction . fusing stable elements into uranium would consume energy , as would trying to break helium into hydrogen . for a more thorough background , see for instance this post .
well , the information does not have to escape from inside the horizon , because it is not inside . the information is on the horizon . one way to see that is from the fact that from the perspective of an observer outside the horizon of a black hole , nothing ever crosses the horizon . it asymptotically gets to the horizon in infinite time ( as it is measured from the perspective of an observer at infinity ) . an other way to see that is the fact that from the boundary conditions on the horizon you can get all the information you need to describe the space-time outside but that is something more technical . finally , since classical gr is a geometrical theory and not a quantum field theory* , gravitons is not the appropriate way to describe it . *to clarify this point , gr can admit a description in the framework of gauge theories like the theory of electromagnetism . but even though electromagnetism can admit a second quantization ( and be described as a qft ) , gr can not .
we know that for reversible processes in isolated systems the entropy remains constant . you have mentioned that the system is isolated and that the plunger is reversibly manipulated . so considering the 2 gases as one isolated system we get that the entropy of the system is a constant . so $\delta$$s$ = $0$ now since there is a diathermic piston separating the gases , it means that one gas in itself is not an isolated system , because heat transfer can take place through diathermic piston . thus considering each gas , there will be a non-zero change in entropy for each . so $\delta$$s ( 1 ) $ + $\delta$$s ( 2 ) $ = $\delta$$s$ = $0$ thus you get your result .
two media can have equal indices of refraction . for example , you could pick the densities of two different gases so as to make their indices of refraction equal . you could do the same thing with different liquids containing properly chosen concentrations of solutes .
free nonabelian gauge theory is the limit of zero coupling . so it is true , in a sense , that free $su ( n_c ) $ gauge theory is a theory of $n_c^2 -1$ free " photons . " there is a crucial subtlety , though : it is a gauge theory , so we only consider gauge-invariant states to be physical . thus , already in the free theory , there is a " gauss law " constraint that renders calculations ( e . g . of the partition function ) nontrivial and the physics of nonabelian free theories different from that of abelian ones ( at least in finite volume ) . see , for instance , this beautiful work of aharony , marsano , minwalla , papadodimas , and van raamsdonk , which shows that many thermodynamic aspects of confinement appear already in free theories at finite volume .
as long as you do not have any mountains above the straight line between your eye and the ' ideal ' horizon , you will be able to see ' the full extent ' , constrained only by the extent of your peripheral vision . so is not this question much more around what angle your peripheral vision covers ? as soon as you can see to the horizon all round , when looking down , that surely meets the requirements ? quick calculation done : remembered my geometry from 25 years ago : sohcahtoa . right angled triangle . one side 6371 miles . assuming peripheral vision = 135 degrees , acute angle 28 degrees . cos 28 = 6371/hypoteneuse so hyp = 7215 . subtract 6371 = 844 miles up this does assume peripheral vision is 135 degrees all round ( left , right , up and down ) - so feel free to update if that assumption is false
in current cosmological models , the milky way resides in a ' halo ' of dark matter . halo is a technical term - in this case , it means a spherically symmetric collection of dark matter . since dark matter is not self-interacting and does not interact with other matter , it does not experience any sort of collisions or friction , and therefore never flattens out into a disk the way normal ( baryonic ) matter does . so , dark matter does not trace out a disk and does not follow spiral arms .
well , the argument is something like this . matter and radiation are tightly coupled until the time of last scattering . the photons are in thermal equilibrium with the plasma surrounding them . this means the state of the universe at the time of last scattering is very precisely imprinted on the photons , and the photons travel from last scattering to us more or less unchanged , except for being redshifted by the expansion of space ( and a large number of smaller corrections ) . at the time of last scattering , and at earlier times , the universe was in what we call the " linear regime " . this means the evolution of the state of the universe ( e . g . the density at each point , temperature at each point , pressure , bulk velocity , etc . ) is well described by linear approximations to the usual non-linear equations . linear equations can be solved exactly , so if you know the state of the universe at one time in the linear regime , you can easily solve for the state at another time in the linear regime . broadly speaking , if you look at any quantity evolving in the linear regime , how it is distributed does not change , only the amplitude of fluctuations in the distribution does . if we think the cmb accurately tells us the state of the universe at last scattering ( at the very least , it gives us a lot of information about this state ! ) , and that the universe was in the linear regime at the time of last scattering , then the cmb accurately tells us about all earlier times up to the end of inflation ( not to be confused with the expansion of the universe ) , in other words all the way back to shortly after the big bang .
physical problems also have boundary conditions . in the case of waves , we usually want a well-defined flux of energy or current or whatnot going to or coming from the boundary of the region , or infinity . the wave function you gave diverges as $z \to \infty$ so it might not be physically realistic . placed inside a box , you might want to demand that no energy or current flow through the walls of the box , which is probably not satisfiable using your example . the equations for these fluxes are given by conservation laws , noether currents . a note on terminology , an equation is something that equates , and therefore has an equal sign in it . a wave equation usualy contains operators that act on functions , and so can be satisfied by a wave function .
i was working at cern bubble chamber experiments back in the early seventies . the accelerators required a vacuum so as to be able to sustain the beam which makes many turns in the circle ( practically velocity of light ) , so the best possible vacuum is and was a necessity . after the generation of the beams , the beam lines did not need a vacuum because the probability of scattering in air is very small , and mainly a bit of ionization can happen . even in the dense liquid of the bubble chamber maybe one in ten incoming particles interact ; the intensity was low , controlled to be ten or so particles at a time , for clear pictures : this is an antiproton beam entering on the left . when we got a glitch in data taking we would complain that the cat had entered the beam line ! yes , a person could get in the beam . there were physicists in the early years who would center the beam by the cerenkov light it made passing trough the eye . i know of one who died of cancer of the retina . in those early times a proton beam could be a primary beam steered towards an electronic detector . in this case the beam would have many more particles and would present a greater danger . still it could go through the air , depending on the experimental setup , may be for special checks with low intensity , but one should be careful of the radiation induced and certainly not to enter the beam line .
when the magnet is moving , the electric field of the magnet is doing the work , pushing the current carriers around the wire . when the magnet is still and the wire is moving , the magnetic field produces a force in the current carriers , but this force does no work , it is the constraint force that keeps the electrons in the wire that is doing the work . the paradox is resolved by noting the the wire is moving , so the constraint is not time-independent . the constraint force is perpendicular to the surface of the wire pushing on the charge carriers in the direction of motion ( because the whole thing is moving ) . this force is doing the work on the charge carriers in this frame ( although it is somewhat strange to think of a constraint force doing work ) . the push of the current carriers against the wire 's constraint force gives the breaking force on the wire , which slows it down so as to conserve energy , as the resistance gives off heat .
as an addition to already posted answers and while realising that experiments on mythbusters do not really have the required rigour of physics experiments , the mythbusters have tested this theory and concluded that : the jumping power of a human being cannot cancel out the falling velocity of the elevator . the best speculative advice from an elevator expert would be to lie on the elevator floor instead of jumping . adam and jamie speculated the attendant survived because the tight elevator shaft created an air cushion . this together with spring action from slack elevator cable could have slowed the car to survivable speeds . ( this myth is fueled by the story of an elevator attendant found alive but badly injured in an elevator car that had fallen down a shaft in the empire state building after a b-25 medium bomber crashed into it in 1945 . )
here are some online lecture notes by chetan nayak which look pretty good : introductory : http://www.physics.ucla.edu/~nayak/solid_state.pdf more advanced : http://www.physics.ucla.edu/~nayak/many_body.pdf
a fairly good schematic of an air-powered venturi pomp : the geometry ( alignment of nozzle and mixer ) is crucial , it is inertia that directs the compressed air toward the mixer . the difference between the pressure of the compressed air and the ambient pressure is of course important , as it is driving the flow . in turn , this flow is the cause of the suction that can generate vacuum  in your case , though you do not describe in which conditions it is used , the object seems simply to be moved by entrainment in the air flow .
on the curved side near the equator it would be similar to earth ( or any other spherical planet ) then as you head toward the poles you would be going uphill - since the gravity vector would be pointing to near the middle of the hemisphere and so at 45deg to the ' ground ' on the flat side the area near the ' equator ' would not be too different and then again as you go toward the pole it becomes steeper until - until you can stand on the edge . but to do all this you would need oxygen since the atmosphere would form a ring around the ' lower ' tropics . there is a similar interesting puzzle of what would happen on a cubic planet . essentially the corners would form peaks extending miles out of the atmosphere which would form circular skies over the center of each face
your quotation of the equation is actually wrong . the correct version is given by $\rho^{ind} ( r ) =-e [ n_0 ( \mu+e\phi ( r ) ) -n_0 ( \mu ) ] . $ now you have to expand this expression around $\phi=0$ $\rho^{ind} ( r ) =\rho^{ind} ( r ) \mid_{\phi=0}+\frac{\partial}{\partial \phi}\rho^{ind}\mid_{\phi=0}\phi+\ldots$ now we have to evaluate the zero and first order terms : $\rho^{ind} ( r ) \mid_{\phi=0}=e [ n_0 ( \mu ) -n_0 ( \mu ) ] =0 , $ $\frac{\partial}{\partial \phi}\rho^{ind}\mid_{\phi=0}\phi ( r ) =-e\frac{\partial n_0}{\partial ( \mu+e\phi ( r ) ) }\frac{\partial}{\partial\phi} ( \mu+e\phi ( r ) ) \mid_{\phi=0}\phi=-e^2\frac{\partial n_0}{\partial\mu}\phi ( r ) . $ as you can see , the last line yields the desired expression by application of the chain rule .
let 's say the black hole has mass and charge $q$ and $m$ , and the electron has $m$ and $q$ . an extremal black hole has $|q|=2m$ ( in the appropriate units ) . an electron has $|q| \gg 2m$ in the same units . if the electron fell into a negatively charged extremal r-n black hole , then the black hole would have $|q| &gt ; 2m$ , which would make it more than extremal . this would cause it to be a naked singularity , which would be exciting , since it would be a counterexample to the cosmic censorship hypothesis ; but i am pretty sure there is no such trivial counterexample , since cosmic censorship is still alive and kicking , decades after being conjectured . another way of seeing that it will be repelled is that extremal r-n black holes with like charges do not interact ; their gravitational attraction exactly cancels their electrostatic repulsion . since the electron has a greater $|q|/m$ , it will definitely be repelled . one thing to watch out for in this kind of situation is that different observers can disagree on the direction of the forces . for example , a light ray that falls radially into a black hole experiences a repulsion at certain points as measured in schwarzschild coordinates . this is the famous " hilbert repulsion " beloved of kooks like angelo loinger . the light ray nevertheless passes through the horizon , and local observers never see a repulsion -- mcgruder , gravitational repulsion in the schwarzschild field , phys . rev . d 25 , 31913194 ( 1982 ) .
lowest gravity on earth surface is near sri lanka based on the articles below . http://www.expertcore.org/viewtopic.php?f=13t=3537start=0sid=4f7757b9c5105a7ba859ddc46c616102 http://www.timeseye.com/2011/12/sri-lanka-best-place-for-space-port.html
an elegant proof of the converse is obtained by using the action formulation . recall that for a given lagrangian $l$ , the action is a functional of paths defined as follows : \begin{align} s_l [ q ] = \int_{t_a}^{t_b} dt\ , l_q ( t ) \end{align} now suppose that two lagrangians differ by a total time derivative ; \begin{align} l'_q ( t ) -l_q ( t ) = \frac{da_q}{dt} ( t ) , \end{align} then we immediately find that \begin{align} s_{l'} [ q ] - s_{l} [ q ] = a_q ( t_b ) - a_q ( t_a ) \end{align} if , in addition , $a$ is a local function ( see definition of local function ) of paths $q$ , then this implies that \begin{align} \delta s_{l'} [ q ] - \delta s_l [ q ] = 0 \end{align} for all variations keeping the endpoints of paths fixed . therefore , for all such variations , $q$ is a stationary point of $s_{l'}$ if and only if $q$ is a stationary point of $s_l$ . now , if $l$ and $l'$ are local functions that depend only on $q$ , it is first derivative , and time , namely if \begin{align} l_q ( t ) and = l ( q ( t ) , \dot q ( t ) , t ) \\ l'_q ( t ) and = l' ( q ( t ) , \dot q ( t ) , t ) , \end{align} then a path $q$ is a stationary point of the action for all such variations if and only if it satisfies the corresponding euler-lagrange equations . combining these facts , we see that if $q$ satisfies the el equations for $l$ , then it is a stationary point of $s_l$ , so it is also a stationary point of $s_{l'}$ and therefore satisfies the el equations for $l'$ , as desired . addendum . we show that the variation of a function $a$ that is local in paths $q : [ t_a , t_b ] \to \mathbb r$ and their first derivatives vanishes at the endpoints $t_a$ and $t_b$ for all variations that keep path endpoints fixed . let a function $a$ that is local in paths $q$ and their time derivatives be given . namely , $a$ is such that there exists a function $\alpha$ for which \begin{align} a_q ( t ) = \alpha ( q ( t ) , \dot q ( t ) , t ) \end{align} for all paths $q$ . now let a path $q$ defined on $ [ t_a , t_b ] $ be given , and let $\delta q$ be a variation of $q$ , then if $\delta a_q$ denotes the variation of $a_q$ induced by $\delta q$ , then we have \begin{align} \delta a_q ( t ) = \frac{\partial \alpha}{\partial q} ( q ( t ) , \dot q ( t ) , t ) \delta q ( t ) + \frac{\partial\alpha}{\partial\dot q} ( q ( t ) , \dot q ( t ) , t ) \delta\dot q ( t ) \end{align} now , for a sufficiently smooth variation that keeps the endpoints $q ( t_a ) , q ( t_b ) $ of the path fixed , one has \begin{align} \delta q ( t_a ) and = \delta q ( t_b ) =0\\ \delta \dot q ( t_a ) and = \delta \dot q ( t_b ) = 0 , \end{align} which you should be able to convince yourself of ( see stackexchange-url for mathematical details of variations that will help ) from which it follows that \begin{align} \delta a_q ( t_a ) = \delta a_q ( t_b ) = 0 . \end{align} as desired .
firstly , i refer you to prof . binney 's textbook ( see below ) which covers perturbation theory in quantum mechanics in explicit detail . when doing perturbation theory , we perturb the hamiltonian $h^{ ( 0 ) }$ of a system which has been solved analytically , i.e. the eigenstates and eigenvalues are known . specifically , $$h^{ ( 0 ) }\to h^{ ( 0 ) } + \lambda h'$$ where $h'$ is the perturbation , and $\lambda$ is a coupling constant . why include such a constant ? as binney says , it provides us a ' slider ' which when gradually increased to unity increases the strength of the perturbation . when $\lambda = 0$ , the system is unperturbed , and when $\lambda=1$ we ' fully perturb the system . ' introducing a coupling constant $\lambda$ also provides us with a manner to refer to a particular order of perturbation theory ; $\mathcal{o} ( \lambda ) $ is first order , $\mathcal{o} ( \lambda^2 ) $ is second order , etc . as we increase in powers of the coupling constant , we hope the corrections decrease . ( the series may not even converge . ) a caveat : the demand that a coupling $\lambda \ll1$ may not be sufficient or correct to ensure that the coupling is small ; this is only the case when the coupling is dimensionless . for example , if the coupling , in units where $c=\hbar=1$ , had a mass ( or equivalently energy ) dimension of $+1$ , then to ensure a weak coupling we would need to demand , $\lambda/e \ll 1$ , where $e$ had dimensions of energy . such couplings are known as relevant as at low energies they are high , and at high energies the coupling is low . http://www-thphys.physics.ox.ac.uk/people/jamesbinney/qbhome.htm
in a consistent theory of gravity , there can not exist any objects that can shield the gravitational field in the same way as conductors shield the electric field . it follows from the positive-energy theorems and/or energy conditions ( roughly saying that the energy density cannot be negative ) . to see why , just use the conductor to shield an ordinary electric field - which is what your problem reduces to temporarily for very low frequencies of the electromagnetic waves . the basic courses of electromagnetism allow one to calculate the electric field of a point-like charged source and a planar conductor : the electric field is identical to the original charge plus a " mirror charge " on the opposite side from the conductor 's boundary . importantly , the mirror charge has the opposite sign . in this way , one may guarantee that the electric field is transverse to the plane of the conductor . this fact makes the electromagnetic waves bounce off the mirror if you consider time-dependent fields . if you wanted to do an " analogous " thing for gravity , you would first have to decide what boundary conditions you want to be imposed for the gravitational field by the mysterious new object - what is your gravitational analogy of "$\mathbf e$ is orthogonal to the conductor " . the metric field has many more components . most of them will require you to consider a negative " mirror mass " - but the mass in a region can not be negative , otherwise the vacuum would be unstable ( one could produce regions of negative and positive energy in pairs out of vacuum , without violating any conservation laws ) . microscopically , one may also see why there can not be any counterpart of the conductor . the conductor allows to change the electric field discontinuously because it can support charges distributed over the boundary . the profile of the charge density goes like $\sigma\delta ( z ) $ if the conductor boundary sits at $z=0$ . however , you would need a similar singular mass distribution to construct a gravitational counterpart . if the distribution failed to be positively definite , it would violate the positive-energy theorem or energy conditions , if you wish . if it were positively definite , it would create a huge gravitational field . locally , the boundary of the gravitational " conductor " would have to look like an event horizon . but we know that the event horizons cannot shield the interior from the gravitational waves - the waves as well as everything else falls inside the black hole . moreover , the price for such " non-shielding " is that you have to be killed by the singularity in a finite proper time . one may probably write some formal solutions that have some properties but they can not really work when all the behavior of gravity and the related consistency conditions are taken into account . one can not fundamentally construct a " gravitational conductor " because gravity means that the space itself remains dynamical and this fact can not be undone . in particular , in a consistent theory of gravity - in string/m-theory - you will not find any objects generalizing conductors to gravity . by the way , there is one object that comes close to a " gravitational conductor " in m-theory - the hoava-witten domain wall , a possible boundary of the 11-dimensional spacetime of m-theory . it can be placed at $x_{10}=0$ and about $1/2$ of the components of the metric tensor become unphysical near this domain wall ( boundary of the world that carries the $e_8$ gauge multiplet ) because the domain wall also acts as an orientifold plane . but such an orientifold plane is not just some object ( like a conductor ) that is " inserted " into a pre-existing space that does not change . quite on the contrary , the character of the underlying spacetime is changed qualitatively : the world behind the orientifold plane is literally just a mirror copy of the world in front of the plane . so there is a lot of fundamentally incorrect thinking about gravity in all the answers that try to say " yes " . gravity is not another force that is inserted into a pre-existing geometrical space ; gravity is the curvature and dynamics of the spacetime itself . once we say that the space is dynamical , we can not find any objects that would " undo " this fundamental assumption of general relativity . there are also some more detailed confusions in the other solutions . first , a white hole corresponds to a time-reversed black hole with the same ( positive ) mass but it is an unphysical time reversal because it violates the second law of thermodynamics : entropy has to increase which means that the ( large entropy ) black hole can be formed , but it cannot be " unformed " . but a white hole , which is forbidden thermodynamically ( and microscopically , it corresponds to the same microstates as black hole microstates , they just never behave in any " white hole " way ) , is still something else than a negative-mass black hole . a negative mass black hole is not related to a positive-mass black hole by any " reflection " . it is a solution without horizons , with a naked singularity , and can not occur in a consistent theory of gravity because it would cause instability of the vacuum . ( also , naked singularities can not be " produced " by any generic evolution in 3+1 dimensions because of penrose 's cosmic censorship conjecture . ) so white holes and negative-mass black holes are forbidden for different reasons . however , even if you considered them , it would still fail to be enough to create a " gravitational conductor " which is nothing else than the denial of the fact that the geometry of the spacetime is dynamical , and one can not freely construct infinite , delta-function-like mass densities without completely changing the shape of spacetime .
an easy example of another entropic force would be that exerted by , say , a box of ideal gas with a wall that can move in or out . push on this movable wall and you increase the pressure of this gas . conversely , the force which would try to reduce the pressure by moving the wall outwards is " entropic " in the same sense as the force which seeks to contract a stretched rubber band .
if i ruled the world , i would ban the phrase " pure energy " in contexts like this . there is no such thing as pure energy ! when particles and antiparticles annihilate , the resulting energy can take many different forms -- one of the basic principles of quantum physics is that any process that is not forbidden ( say , because of violation of some sort of conservation law ) has some probability of happening . so when a proton and an antiproton annihilate , they can produce photons , or pairs of other particles and antiparticles , such as a neutrino-antineutrino pair , or a positron-electron pair . although all sorts of things are possible , by far the most common product of matter-antimatter annihilation is photons , especially if the collision occurs at low energy . one reason is that lower-mass particles are easier to create than high-mass particles , and nothing has less mass than a photon . ( other particles , particularly neutrinos , have so little mass that they are " effectively massless , " but neutrinos are weakly interacting particles , which means that the probability of producing them is lower . )
for almost all detectors , it is actually the energy of the photon that is the attribute that is detected and the energy is not changed by a refractive medium . so the " color " is unchanged by the medium . . .
i will ignore the confused statement " the metal fermi level is shown as the top of the conduction band , with the entire band filled " and focus on the main part of the question . first , i will answer within the framework of schottky-mott theory , which is where this diagram comes from . in any material , the difference between vacuum level and a particular electron wavefunction state is constant ( a function of the electronic state and of the material , but independent of everything else ) . for example , in a semiconductor , the difference between vacuum level and the bottom of the conduction band is constant , and the difference between vacuum level and the top of the valence band is constant . in a metal ( unlike a semiconductor ) , the fermi level is fixed relative to the band structure ( i.e. . , an electron with a certain wavefunction may always be at the metal 's fermi level ) , because the metal is always charge-neutral ( except within a debye length ( few angstroms ) of the surface ) . so if there are x protons per cm^3 , then there are x electrons per cm^3 , and the metal fermi level always sits close to the x'th lowest-energy electron state . so again , the metal and semiconductor are treated the same : the states of both stay unchanged relative to the local vacuum level . the difference is that in a semiconductor , but not a metal , the fermi level can shift relative to the electronic states . sometimes people ask " does the vacuum level of the metal shift or does the vacuum level of the bulk semiconductor shift ? " this question has no answer . only the difference in vacuum level between somewhere and somewhere else is meaningful . a whole separate subject is " is the framework of schottky-mott theory actually correct for understanding these things ? " in fact , its predictions for schottky barrier heights are pretty bad . it by-and-large captures the trend--there are systematic differences between low and high workfunction metals--but the quantitative schottky barrier heights can be pretty far off . on a related note , the question " what is the workfunction of metal xyz ? " does not have a single answer . it depends on the detailed atomic structure of the surface . for example the ( 111 ) surface of tungsten will have a different workfunction than the ( 100 ) surface of tungsten .
of course you can have configurations which do not respect the underlying symmetry of they dynamics , the issue is that those will not be eigenstates of the hamiltonian . typically when talking solid state people are interested in energy bands which are eigenstates of the hamiltonian , and these must also be eigenstates of the lattice translations . this follows from elementary operator algebra . the non-symmetric configurations would correspond to superpositions of different energy bands and would not be stationary in time .
but then the speed of light is universal constant regardless of the motion of its frame of reference , so should not their relative velocity be $c$ ? what is their relative velocity and how ? edit upon further review ( special thanks to alfred 's comment ) , i think my original answer is incorrect . it turns out that the question of relative velocities of photons moving in the same direction is a meaningless question . the reason is as follows . for two objects A and B moving as such ,  v u -------&gt; A -------&gt; B ------------------ (ground)  the velocity of B in A 's frame is then $$ u'=\frac{u-v}{1-\frac{uv}{c^2}} $$ notice the denominator ? for $u=v=c$ , this is zero and we get 0/0 which is an undefined operation , hence the meaningless question . do objects moving at the speed of light obey law of addition of velocities ? not exactly . the galilean velocity addition , $s=u+v$ does not hold for large-velocity objects . we use the " composition law " , $$ s=\frac{u\pm v}{1\pm\frac{uv}{c^2}} $$ where $\pm$ depends on directions/frames . if $uv\ll c^2$ , then this does reduce to the galilean transformation .
when you say " we see that if the current doubles then the potential difference is halved , " you are assuming that $p$ is fixed , whereas when you say " doubling the current should increase the potential difference " you are assuming $r$ is fixed . but in fact , it is not possible to change the current while keeping both of these things constant . let us assume that we are actually keeping $r$ fixed , i.e. we are working with a normal resistor . let 's try to work out what happens to the power if we increase the current . then we have $v=ir$ , and we also have $p=iv$ . substituting the first into the second , we have that $p=i^2r$ . so if you double the current , the power will increase by a factor of four . so for a fixed resistor , the power does not stay constant , and that is why the potential difference is not halved when you double the current .
update i finally got it right . my thanks goes to @neurofuzzy , who pointed me in the right direction . according to wiki 's legendre polynomial solution for the elliptic integral , " an exact solution to the period of a pendulum is : " $$t=2\pi\sqrt\frac{\ell}{g}\sum\limits_{n=0}^\infty \left [ \left ( \frac{ ( 2n ) ! }{ ( 2^nn ! ) ^2}\right ) ^2sin^{2n}\left ( \frac{\alpha}{2}\right ) \right ] $$ which after solving for g gives $$g=\ell\left ( 2\pi \sum\limits_{n=0}^\infty \left [ \left ( \frac{ ( 2n ) ! }{ ( 2^nn ! ) ^2}\right ) ^2sin^{2n}\left ( \frac{\alpha}{2}\right ) \right ] t^{-1} \right ) ^2$$ although it is probably not the most beautiful wonder of the world , it seems to work for me . i finally get acceptable results ( $\overline g = 9.402$ ) . the difference between real and this g can easily be explained , therefore i mark this post as the real answer . in case somebody is interested in this , here is python2 code for finding g from measured data . original answer after reading your answers , suggestions and after going through simplification of the formula once again ( with the very same result ) , i have to conclude that , assuming $$t_m = 2\pi\sqrt{\frac{\ell}{g}} ( 1+\frac{1}{4}\sin^2 ( \frac{\alpha}{2} ) ) $$ is correct ( taken from two independent sources ) , following formula is correct as well $$g=\frac{\ell\pi^2 ( \cos ( \alpha ) -9 ) ^2}{16t_m^2}$$ this suggests my experiment alone was not very accurate , thus resulting in the error seen in my original question . additional info i have tried to come up with the formula for $t_m$ but with no luck . i got $$t=2\pi\sqrt{\frac{\ell\cos ( \alpha ) }{g}}$$ from which one can easily get $$g=\frac{4\ell\pi^2\cos ( \alpha ) }{t^2}$$ although it looks little bit similar , it is not quite there . it also returns unacceptable results ( $\overline g = 4.94$ )
in schwarzschild coordinates , the change of sign of the $g_{00}$ and $g_{11}$ components of the metric means that , in some sense , $t$ becomes a " spatial " coordinate and $r$ a " temporal " one : the " future " points towards decreasing $r$ instead of increasing $t$ , you can see that looking at the light cones in schwarzschild coordinates , see for example this figure schwarzschild light cones in schwarzschild coordinates ( from mtw , page 848 ) in addition , inside the surface $r = r_\text{s} = 2m$ you cannot have a positive $\mathrm{d}s^2$ without a non-null $\mathrm{d}r^2$ due to the sign change , so inside a schwarzschild black hole you have to move . this , again , can be seen using the above light cones : the word line cannot keep constant $r$ . the fact that after having crossed the event horizon light cones point towards the $r = 0$ singularity is true also using other coordinates , such as kruskal-szekeres coordinates schwarzschild metric in kruskal-szekeres coordinates ( see the full definition of the coordinates in the wikipedia article ) : \begin{equation}\mathrm{d}s^{2} = \frac{4r_{\text{s}}^{3}}{r}\mathrm{e}^{-r/r_{\text{s}}} ( \mathrm{d}v^{2} - \mathrm{d}u^{2} ) - r^{2}\mathrm{d}\theta^{2} - r^{2}\sin^{2}\theta\mathrm{d}\phi^{2}\end{equation} schwarzschild light cones in kruskal-szekeres coordinates . the $r = 0$ region is the one with the inward toothed border ( from mtw , page 848 ) and eddington-finkelstein coordinates schwarzschild metric in eddington-finkelstein coordinates ( see the full definition of the coordinates in the wikipedia article ) : \begin{equation}\mathrm{d}s^{2} = \biggl ( 1 - \frac{r_{\text{s}}}{r}\biggr ) \mathrm{d}\tilde{v}^{2} - 2\mathrm{d}\tilde{v}\mathrm{d}r - r^{2}\mathrm{d}\theta^{2} - r^{2}\sin^{2}\theta\mathrm{d}\phi^{2}\end{equation} schwarzschild light cones in eddington-finkelstein coordinates ( from mtw , page 849 ) the sign change has a physical meaning in schwarzschild coordinates because schwarzschild $t$ and $r$ coordinates bear physical meanings ( $t$ is the far-away time , $r$ the reduced circumference ) , while i am not aware of any simple physical meaning of the kruskal-szekeres $u$ and $v$ coordinates or of the eddington-finkelstein $\tilde{v}$ coordinate . note that $u$ , $v$ and $\tilde{v}$ coordinates mix the original schwarzschild $t$ and $r$ coordinates . depending on the coordinates used , there is not always a sign change in the metric components ( in kruskal-szekeres coordinates there is no sign change at all ) , so do not take that change as a general rule . the kerr metric tensor , with boyer-lindquist coordinates ( briefly described in this introduction to the kerr spacetime by matt visser ) , reads \begin{equation} g_{\mu\nu} = \begin{pmatrix} ( \delta - a^{2}\sin^{2}\theta ) \sigma^{-1} and 0 and 0 and a\sigma^{-1}r_{\text{s}}r\sin^{2}\theta \\ 0 and -\delta^{-1}\sigma and 0 and 0 \\ 0 and 0 and -\sigma and 0 \\ a\sigma^{-1}r_{\text{s}}r\sin^{2}\theta and 0 and 0 and -\bigl ( ( r^{2} + a^{2} ) + a^{2}\sigma^{-1}r_{\text{s}}r\sin^{2}\theta\bigr ) \sin^{2}\theta \end{pmatrix} \end{equation} with \begin{align} \delta and = r^{2} - r_{\text{s}}r + a^{2} , \\ \sigma and = r^{2} + a^{2}\cos^{2}\theta . \end{align} the $g_{00}$ component changes its sign on the surfaces $$r_{\text{e}}^{\pm} = m \pm \sqrt{m^{2} - a^{2}\cos^{2}\theta}$$ instead , $g_{11}$ changes its sign on the surfaces $$r_{\pm} = m \pm \sqrt{m^{2} - a^{2}}$$ which determine the outer ( with $+$ sign ) and the inner ( with $-$ sign ) event horizons . so they change their sign on two different surfaces . as jerry schirmer pointed out in a comment , this would occur also in the schwarzschild geometry with kerr-like non-diagonal coordinates ( e . g . , it occurs with eddington-finkelstein coordinates ) . this does not mean that the signature of the metric will change : there always will be a negative ( positive ) eigenvalue , and three other positive ( negative ) eigenvalues . in a non-diagonal metric tensor ( like schwarzschild one in eddington-finkelstein coordinates or the kerr one in boyer-lindquist coordinates ) , the diagonal components of the metric tensor are not necessarily the eigenvalues .
the equation describes parabolic motion , if $a\neq 0$ is a non-zero constant acceleration , which i will assume from now on . if you think about it , your solution provides an answer to the question : at what time does the object is in the position $s$ ? [ a note on notation : traditionally , the letter $s$ denotes distance ( i guess from the german word " strecke" ) , which by definition is a non-negative quantity , but your formula makes more sense , if we interpret $s$ as a position $x$ , which can also be negative . ] $\frac{1}{2}at^2+ut-x=0$ $t_1 = \frac{-u-\sqrt{d}}{a}$ $t_2 = \frac{-u+\sqrt{d}}{a}$ where $d := u^2+2ax$ . let 's think about it for a moment , and see what answers we get by varying $x$ . case $d&lt ; 0$: the discriminant is negative , there are no solutions , therefore at no time your object will have that position . case $d=0 \leftrightarrow x=-\frac{u^2}{2a}$: the discriminant is zero , there is only one solution which is the " top " ( "bottom" ) point reached by the object , if $a&lt ; 0$ ( $a> 0$ ) , respectively . case $d&gt ; 0$: the discriminant is positive and there are two solutions . this means that the object will reach that position twice , once going " up " and once going " down " the parabola . now will one of them always be negative ? not necessarily . case $ax &gt ; 0$: one positive and one negative . case $ax &lt ; 0$ and $\frac{u}{a}&gt ; 0$: two negative . case $ax &lt ; 0$ and $\frac{u}{a}&lt ; 0$: two positive . case $x = 0$ and $\frac{u}{a}&gt ; 0$: one zero and one negative . case $x = 0$ and $\frac{u}{a}&lt ; 0$: one zero and one positive .
the energy is borrowed from the heisenberg uncertainty principle to create virtual particles and has to be paid back in a very short time . $\delta{t} \geq \frac{\hbar}{2\delta{e}}$ this is why virtual particles live for very short times ( i.e. pop in and out of existence ) . we cannot manipulate this energy .
i think this is the baker-campbell-haussdorff formula , and the notation means to iterate the commutator . that is , $$ [ l , m ] _1 = [ l , m ] $$ and $$ [ l , m ] _{n+1} = [ l , [ l , m ] _{n} ] . $$
( this answer addresses the new question ) as a consequence of the singularity theorems , it is not only possible but ( arguably ) inevitable for singularities to form in a finite amount of " time " in a physically reasonable spacetime . the word " time " in this context means " proper time along a specific timelike geodesic " . for example , if there is a trapped surface* in spacetime , then a singularity will appear within a finite amount of proper time ( along a timelike geodesic ) in the future of that surface ; so , an observer sitting in a collapsing star will reach the singularity in finite time . thus , the collapse of matter is one possible way to create a singularity " out of nothing " . if your spacetime is globally hyperbolic and you foliate it by cauchy surfaces you can say in a much more " universal " way that the singularity did not exist at time [ ; t_0 ; ] and came to exist at time [ ; t_1 ; ] . i should point out that the singularities are a generic feature of physically reasonable spacetimes ; take a look at the hawking-penrose theorem - it applies in very general situations . also , as the original question was about black holes and not singularities , i should advise you to make a clear distinction between the two concepts . trapped surfaces form due to the condensation of matter ( this is the famous schoen-yau theorem ) , and under a certain extra hypothesis , these surfaces will be hidden inside black holes . this extra hypothesis is the well-known ( weak ) cosmic censorship conjecture ( ccc ) . if it does not hold , gravitational collapse can create naked singularities , that is , singularities not " causally hidden " by the event horizon of a black hole . much of what is known in general about black holes depend crucially on the ccc . *a trapped surface is a two-dimensional spacelike compact surface such that the null geodesics departing from it are accelerating towards each other - mathematically , we say that the expansion of the congruence of future-directed null geodesics orthogonal to the surface is negative .
what is supersymmetry ( in a big nutshell ) ? to explain supersymmetry , let us consider the simplest supersymmetric model which is the wess-zumino model with an action , $$s=-\int d^4x \left ( \frac{1}{2}\partial^\mu \phi^\star \partial_\mu \phi + i\psi^\dagger \bar{\sigma}^\mu \partial_\mu \psi + |f|^2 \right ) $$ where $\phi$ is a scalar field , $\psi$ a majorana spinor field and $f$ is known as an auxiliary field , a pseudo-scalar , which arises due to a subtlety explained later . a supersymmetry is a continuous symmetry relating bosons to fermions and vice versa . most importantly , for it to be a symmetry of the system , we must require that the lagrangian changes only up to a total derivative . naively , we might propose : $$\delta\phi = \epsilon_\alpha \psi^\alpha$$ $$\delta\psi^\alpha = \epsilon^\alpha\phi$$ where we have introduced an infinitesimal parameter $\epsilon^\alpha$ which is required in order to balance the indices on both sides of the transformation . the parameter is fermionic ; to deduce the dimensionality , we can do some dimensional analysis . $$ [ \partial_\mu\phi^\star\partial_\mu\phi ] = 4 , \ ; [ \partial_\mu ] = 1$$ hence $ [ \phi ] = 1$ . similar reasoning for the fermionic field shows $ [ \psi ] = 3/2$ , as expected . now from the transformation , we may deduce the dimension of the arbitrary parameter : $$ [ \epsilon^\alpha ] = [ \psi ] - [ \phi ] = 1/2$$ and $ [ \epsilon_\alpha ] = -1/2$ . plugging the transformation in , and deducing $\delta\mathcal{l}$ , we will find it does not quite cancel to leave only a total derivative left . after some manipulation , we can find : $$\delta\psi_\alpha = -i ( \sigma^\nu\epsilon^\dagger ) _\alpha \partial_\nu \phi$$ now to address the auxiliary field , $f$ . if we compute the equations of motion , we find $$\frac{\partial \mathcal{l}}{\partial f} \sim f \quad \frac{\partial \mathcal{l}}{\partial ( \partial f ) } = 0$$ which implies $f=0$ . although it may seem adding the field has no effect , remember that $f=0$ is only true on-shell . the purpose of $f$ then is actually off-shell , which you will notice balances the required degrees of freedom of the system for consistency . a historical note in 1967 , coleman and mandula published a paper , all possible symmetry of the s-matrix , which aptly presented a theorem restricting s-matrix symmetries . however , a key assumption was a symmetry transformation is said to be an internal symmetry transformation if it commutes with p . this implies that it acts only on particle-type indices , and has no matrix elements between particles of different four-momentum or different spin . a group composed of such transformations is called an internal symmetry group . in other words , the symmetries were bosonic ; notice that supersymmetry is in fact fermionic , and hence not subject to the coleman-mandula theorem . in some ways , one may attribute the initial development of supersymmetry to a desire to find a loophole in the coleman-mandula theorem . resources for a simple introduction , see the beyond the standard model course by professor veronica sanz , available here : http://www.perimeterscholars.org/338.html , which includes videos . for a modern introduction , complete with superfields , supermanifolds and more , see the text string theory and m-theory : a modern introduction , by becker , becker and schwarz where it is presented in the context of string theory .
suppose i have a body that ( on some measurement scale ) has an energy of $e$ in the rest frame . it will have energy $h=\gamma e$ in the moving frame , so the difference is $h-e = e ( \gamma - 1 ) $ . now suppose i want to change to a different energy scale with the same units but a different additive constant , so that i now consider the rest energy to be $e'=e+e_0$ , with $e_0$ being an arbitrary constant . now the energy in the moving frame will be $h'= \gamma e ' = \gamma ( e+e_0 ) = h + \gamma e_0$ , and the difference will be $h'-e ' = ( e+e_0 ) ( \gamma - 1 ) = ( h-e ) + e_0 ( \gamma - 1 ) $ . i think that einstein 's $c$ is the $e_0 ( \gamma-1 ) $ term here .
the article is pretty poorly written . as siva said it does not even link to the original paper . so i just looked up the name mentioned in the article and found this which is probably what they are talking about ( though this is just a guess ) . they measured the magnetic dipole moment of protons and antiprotons to ~4 parts per million ( and verified the cpt theorem at this level ) . and yes , magnetic monopoles still have not been observed .
you might find the yahoo " home_transistor " group a useful resource . there is also a series of videos on youtube by jeri ellsworth including some where she makes transistors . in one , in particular , she takes the crystal out of a germanium point-contact diode and turns the crystal into a point-contact transistor ( much like the bell labs transistor . ) there are a couple of semiconductor materials that are easy ( and relatively safe ) to make at home . one is cuprous oxide . h.p. friedrichs has a page where he shows how to make a copper/cuprous oxide diode . ( the copper/cuprous oxide junction is a schottky barrier ) . there is also a page at the science cupboard where they make cuprous oxide diodes . another easy home semiconductor is titanium dioxide . here is a page at university of wisconsin talking about how to make titanium dioxide solar diodes . transistors are hard to make because they require two rectifying junctions , one of which , the emitter , needs to be injecting . ( the emitter needs to send minority carriers into the base . ) thus the emitter/base junction needs to be either p+/n or n+/p . if you are using schottky junctions , as the first bell labs transistor did , then only certain metals form injecting barriers while most do not . additionally the base on a transistor needs to be narrower than the recombination distance for the material you are using . for most materials this is less than 10 microns ( which is a distance you can work with at home , but only with a great deal of care . ) it might be easier to start out with a jfet or a mesfet . these will exhibit switching behavior but require only a single rectifying junction . ( a p/n semiconductor junction in the case of a jfet and a metal/semiconductor schottky junction in the case of a mesfet . ) i have been trying to make a cuprous oxide mesfet , but have not had any luck yet . ( honestly , i have been doing more " research " than " trying " , so it might not actually be that hard . )
current is defined as the total amount of charge that passes through a surface per unit time , and voltage is defined as the electrical potential energy per unit charge . so , for the usual case in which electrons are the charge carriers , the amount of current corresponds to the number of electrons , and the voltage corresponds to the potential energy that each electron has . when voltage is lost ( by which i presume you are talking about the voltage decrease as you move along a circuit ) , it just means that the electrons lose potential energy as they move through the circuit . by the time they get to the end of the circuit ( the positive terminal of the battery ) , they have lost all their potential energy to collisions with atoms in the wire . then they go through the battery and get their energy replenished , so they can start the cycle over . keep in mind that the same electrons keep cycling through the circuit over and over again - they do not go anywhere when they reach the end .
it is actually quite easy to perform the experiment in the comfort of your own home . the simplest setup i have seen ( as depicted in this , and other youtube videos ) is to use a laser pointer and pencil lead , but you can certainly be more systematic and cut slits in some opaque material as well . i would encourage you to experiment to answer the question of how far apart the slits need to be etc . , but some basic math behind this is as follows : if the slits are a distance $d$ apart , if the light has wavelength $\lambda$ , and if the distance between the slits and the screen is $l$ , then the spacing $\delta y$ between successive fringes on the wall will approximately be $$ \delta y \approx \frac{\lambda l}{d} $$ so let 's say the laser is red so that $\lambda\approx 700 \mathrm{nm}$ , the slits are $1\ , \mathrm{mm}$ apart , and the screen is $1.5\ , \mathrm m$ away from the slits , then we have $$ \delta y \approx \frac{ ( 700\ , \mathrm{nm} ) ( 1.5\ , \mathrm{m} ) }{1\ , \mathrm{mm}} = 1.05\ , \mathrm{mm} $$ so you can actually try this and see if your results agree ! ( i might actually try this myself come to think of it ; thanks for the question ! ) cheers !
you have clearly read the paper of hayden and preskill : black holes as mirrors : quantum information in random subsystems , since you mention it in the comments . another quantum information phenomenon related to your question is " information locking " ( see this and other papers ) , where you can arrange things so that all the information in a quantum system of $n$ bits is essentially inaccessible until you get the last $\log n$ bits . there are several papers about papers about " locking " information or entanglement , and i suspect some of your questions may be answered in one of these .
the idea is very easy ( computations are not so easy if you are not familiar with curvilinear coordinates ) : write down $\vec{f}=m\vec{a}$ using the basis $\hat{r} , \hat{\theta} , \hat{\varphi}$ associated to spherical coordinates and next project that vectorial equation along the basis of vectors tangent to the sphere , $\hat{\theta} , \hat{\varphi}$ . the two equations you find this way do not contain the unknown reaction due to the sphere , as it is normal to it by hypotheses . this pair of equations is the same as the one obtained within the lagrangian approach . you have two degrees of freedom and a system of two differential equations which can be proved to satisfy the theorem of existence and uniqueness and thus it determines the motion of the pendulum . as a matter of fact this procedure is nothing but what lagrangian mechanics automatically does .
everything that passes the event horizon of a black hole falls into the singularity , including photons . that is why it is a singularity . there is a particular radius outside the event horizon where a photon will orbit , but the orbit is unstable- if the photon gets perturbed a little closer or the black hole 's mass increases at all , it will fall in , and if the black hole 's mass decreases due to hawking radiation or the photon gets perturbed away from the black hole , it will escape .
it is not needed in a rocket , however if you are going to the effort of sending something up outside the atmosphere ( or even just high up within the atmosphere ) you might as well try and get some useful data out of it . this might even help you get sponsorship for your rocket , as data from climbs through altitude is useful to a number of academic institutions . you will also find many rockets taking thermometers and other devices for exactly the same reason .
i am going to dare to give a very brief answer that is likely not what most folks would expect , but is deeply rooted in experiment : the speed of time is just the speed of a clock -- that is , of how fast some kind of a repeated cycle can be done . clocks thus only have meaning relative to each other . you can set one as a standard , then measure other by it , but you can never really define " the " time standard . that is actually a very einstein way of defining time -- which is to say , it is a very mach way of defining time , since einstein got much of his insistence on hyper-realism in defining physics quantities from mach . now , most likely you thought i was going to answer that there is some kind of velocity of an object along a time axis $t$ that has " length " in much the same fashion as x or y or z , not in terms of cycles . that is certainly what comes to mind for me , in fact ! while viewing $t$ as having ordinary xyz style length turns out to be an incredibly useful abstraction , it is difficult experimentally to make $t$ to behave fully like a length . the main reason is that the clock with its cycles keeps sticking in its nose and requiring that at some point , you sort of " borrow " a space-like axis from xyz space and use that to write out a sequence of clock cycles ( called proper time or $\tau$ ) on paper . as a result , it is not really $t$ you are drawing in those diagrams . you are instead borrowing a bit of ordinary space and mapping clock cycles onto it , making them seem like a length more through the way you represent order them than in how they actually work . fortunately , there is a different and more satisfying approach to the question of whether time has length , one that is suggested by special relativity , or sr . sr says in effect that xyz space and $t$ are interchangeable , and in a very specific way . so , even though there is always a need to write out some cycles in diagrams -- proper time happens ! -- you can argue that there is nonetheless a limit at which objects traveling closer and closer to the speed of light look more and more as if their time axis has been changed into a static length along some regular xyz direction of travel . so , by this take-it-to-the-limit kind of thinking , you can construct a more explicit concept of $t$ as an axis with xyz-style length . it also provides a pretty good answer to you question . since proper time comes to an almost complete stop as an object nears the speed of light , you can say that you have in effect " stolen " the velocity of that object or spaceship through time ( from your perspective or frame , not hers ! ) and converted it fully into a velocity through space ( from your perspective ) . so there is your answer : that " stolen " velocity along $t$ appears to correspond most closely with the velocity of light $c$ in ordinary space , since that is the real-space velocity at which proper time $\tau$ comes ( at the limit ) to a complete halt . this idea that objects " move " at the speed of light along the $t$ axis is in fact a very common assumption in relativity diagrams . it shows up for example whenever you see a light-cone diagram whose cone angle is $45^\circ$ . why $45^\circ$ ? because that is the angle you get if you assume that the " velocity " of light along the $t$ axis is identical to its velocity $c$ in ordinary xyz space . now , is there some slop in how that could be interpreted ? you bet there is ! the idea of a " velocity " in time is for example problematic in a number of ways -- just try to write it out as a derivative and you will see what i mean . but taking such a perspective at least in terms of how to think of the issue gives a really nice simplicity to the units involved , as well as that conceptual simplicity in how to think of it . more importantly , where such simplicity keeps popping up in the representations of something in physics , it is almost certainly reflecting some kind of deeper reality that really is there .
suppose first that $c_{5}$ is absent , that the voltage at the connection point of $c_{2}$ and $c_{3}$ is zero and that of the connection point of $% c_{1}$ and $c_{4}$ is $v ( \omega ) $ . the impedance of a capacitor $c$ at the angular frequency $\omega $ is $$z ( c ) =\frac{1}{i\omega c} . $$ then \begin{eqnarray*} v ( a ) and = and \frac{z ( c_{2} ) }{z ( c_{1} ) +z ( c_{2} ) }v ( \omega ) =\frac{1}{\frac{z ( c_{1} ) % }{z ( c_{2} ) }+1}v ( \omega ) , \\ v ( b ) and = and \frac{z ( c_{3} ) }{z ( c_{3} ) +z ( c_{4} ) }v ( \omega ) =\frac{1}{\frac{z ( c_{4} ) % }{z ( c_{3} ) }+1}v ( \omega ) . \end{eqnarray*} if $v ( a ) =v ( b ) $ then attaching $c_{5}$ does not change the situation since there is no voltage across it . but $$ v ( a ) =v ( b ) \rightarrow \frac{z ( c_{1} ) }{z ( c_{2} ) }=\frac{z ( c_{4} ) }{z ( c_{3} ) }% \rightarrow z ( c_{1} ) z ( c_{3} ) =z ( c_{2} ) z ( c_{4} ) \rightarrow c_{1}c_{3}=c_{2}c_{4} . $$
the way to answer your question is to take the friedman equation and put in the components you want to . in the standard model of cosmology you had put in radiation , matter and lamdba . you then solve the equation for the scale factor $a ( t ) $ . ( this can be automated with a program like mathematica . ) you will get an explicit $a ( t ) $ function that you can plot and see how the universe expands . a very nice pedagogical introduction is in barbara ryden 's " introduction to cosmology " , there is a pdf version online : http://www.astro.caltech.edu/~george/ay21/readings/ryden_introcosmo.pdf pg . 119 figure 6.5 is just what you are looking for .
nonlocal resistance is the ratio of the current in a material to the voltage between some other two points . it is a much less useful quantity , because this depends on the details of induced changes on the conductor and elsewhere to determine , it is not something that is determined only by local material quantities . the only advantage is that you can measure it away from the material , if you can not stick a probe in . it can be measured using the electric field far away from the material . the disadvantage is that you then have to break your head to figure out what is going on inside the conducting material itself , which is what you usually care about . the ratio of voltage to current , where voltage is between to other points . in the linear regime , with only materials with linear response and conductors around , the answer is always yes , with one important caveat--- if the resistance is negative ( meaning you measured the voltage between two points which for some reason have the opposite voltage than two points at successive position in the wire ) , it gets more negative as you increase the current , so it technically goes down . the precise statement is that if you multiply the current by a factor of k , you also multiply the additional voltage elsewhere by a factor of k , so it is a linear relationship .
it goes the other way , actually . in the lagrangian and hamiltonian approaches to classical dynamics ( on which the quantum theory is based ) , you learn about " generalized coordinates " or " degrees of freedom . " most often this is used to reduce the complexity of a problem . the canonical example is a clock pendulum , constrained to move in a plane . the motion in $x$ and $y$ is rather complicated , but it becomes simpler once you realize that the only degree of freedom is the angle $\theta$ that the pendulum makes with the vertical direction . a more sophisticated example is the description of the motion of many coupled oscillators in terms of their " normal modes . " there is a very close connection between the normal modes of a classical oscillator and the energy eigenstates of a quantum system . the various internal state variables that describe a quantum system are simply degrees of freedom in the hamiltonian which do not happen to correspond to spatial position . in quantum theory there is nothing special about the degrees of freedom corresponding to position in space apart from our macroscopic biases . this is where you get some of the counterintuitive , non-local phenomena : qm treats position in space as no more or less special than any other degree of freedom .
based on your comments ( that the exam question said " switch a is pressed" ) , the question can be answered - and the tutor was correct . the key is to look closely at the diagram , and observe that the lower halves of the compartments are connected together , as are the upper halves . in this diagram , $p_1$ represents the driving pressure . now across the piston on the left there is a pressure drop equal to the weight of the 50 kg mass divided by the area of the piston - let 's call the pressure above the piston $p_2$ . on the right , the same thing would lead to a pressure $p_3&lt ; p_2$ if the 100 kg object was getting lifted . but this pressure differential cannot exist while the two compartments are connected , so the right hand weight stays at the bottom - where it exerts a force of $50 g\ n$ . this restores the balance of pressure and force . once the 50 kg weight reaches the top , the pressure against the end stop will result in a force of $50 g\ n$ , and as that force increases , the force of the piston against the bottom of the right hand piston decreases until it , in turn , is lifted up . all this assumes that the pressures at the top of both pistons are equalized by the tube joining them . it is hard to know from the diagram whether that is the case . if the exit pipe represents essentially no resistance to fluid flow , then $p_2=p_3=1 atm$ , and both pistons could be lifted at the same time . however i am pretty sure , given the way the diagram above is drawn , that this is not the case , and that my explanation above is the one your tutor had in mind .
there is a book titled " group theory and physics " by sternberg that covers the basics , including crystal groups , lie groups , representations . i think it is a good introduction to the topic . to quote a review on amazon ( albeit the only one ) " this book is an excellent introduction to the use of group theory in physics , especially in crystallography , special relativity and particle physics . perhaps most importantly , sternberg includes a highly accessible introduction to representation theory near the beginning of the book . all together , this book is an excellent place to get started in learning to use groups and representations in physics . "
the field of the accelerated charge cannot change instantaneously . from " gravitation " by misner , wheeler , and thorne . so it does have an electric field-but why does accelerating a field ( which is in the answer below ) cause light to be made ? or rather , why would not a constant velocity just do it ? the field of a uniformly moving charge does not have a propagating disturbance as does the field in the diagram above . note that , as time progresses , the shell of " kinked " field lines expands away from the acceleration event at the speed of light leaving behind the field of a uniformly moving charge . try this applet and see ( 1 ) how the field looks when the charge is uniformly moving and ( 2 ) how a disturbance propagates in the field of an accelerated charge . i find the " user controlled " option useful . moving charge applet
the idea is this . you will start from an action in the form $$s_i=\int d^4x j_\mu a^\mu$$ and for a system of charge you will write down $$j_\mu= ( \rho ( x ) , \rho ( x ) \dot{\bf x} ) . $$ an integration by part in time will do the job producing the contribution $$s&#39 ; _i=-\int d^4x \rho ( x ) {\bf x}\cdot {\bf e}$$ using the standard relation ${\bf e}=-\partial{\bf a}/\partial t-\nabla\phi$ and using is made of the continuity equation . you can see classically that the contribution is the standard dipolar interaction . with a set of pointlike charges , $\rho ( x ) =\sum_aq_a\delta^3 ( x-x_a ) $ , you can integrate in the volume and you will be left with the interaction term $$v=-\sum_aq_a{\bf x}_a\cdot{\bf e ( x_a , t ) } . $$ this translate as is into quantum mechanics and finally $${\hat\mu}_{mn}=\sum_aq_a\langle\psi_m|{\bf x}_a|\psi_n\rangle . $$ in this case we are assuming that the e.m. field is an external field .
the general solution to the classical wave equation is $$y ( x , t ) =f ( \mathbf{k}\cdot\mathbf{x}-wt ) $$ where $f$ is an arbitrary function . in this case , $f ( x ) = ( 0.35m ) \sin ( -x+\frac{pi}{4} ) $ . basically , whatever multiplies $t$ is $-\omega$ , and whatever multiplies ( or is dotted into ) $\mathbf{x}$ is $\mathbf{k}$ . anything else is part of the arbitrary function .
first of all , dark matter and dark energy , despite their naming , are two very different concepts . we do not really have any good reason to group them together , other than the fact that both represent things we do not understand . thus they are not necessarily backed by the same sets of evidence . why we believe these things exist as it happens though , some of the strongest evidence for both comes from the cosmic microwave background . basically , whatever " stuff " there is in the universe will have an effect on the temperature and polarization fluctuations in this radiation , which was emitted some few hundred thousand years after the big bang . the best all-sky map of this radiation is made by the wmap satellite , and every couple years they release several papers with the analysis of the data . for instance , here is the 2011 paper focusing on the cosmological parameters . they basically just feed all the data into an enormous statistical program to find the most likely values for a large set of parameters , including the amount of " normal " baryonic matter $\omega_\mathrm{b}$ , the amount of cold dark matter $\omega_\mathrm{c}$ , the amount of dark energy $\omega_\lambda$ , and the dark energy equation of state parameter $w$ . there is a lot that can be said as to what the effects of these parameters are , but ultimately you just cannot explain the cmb without having dark energy and dark matter . alternate theories for dark matter now " cold dark matter " ( the cdm of the $\lambda\text{cdm}$ model ) means massive particles interacting via gravity and the weak force but not via electromagnetism and that were non-relativistic even at the time of recombination when the cmb was released . just plain old " dark matter " refers to any gravitating mass that does not have much of an electromagnetic signal . the galaxy rotation curves you refer to were some of the first dark matter evidence , and indeed they could be explained by assuming a large number of quiescent black holes or star-less planets or dust that we just missed for one reason or another . these alternate theories could also , with enough manipulation , explain the bullet cluster , where the gravitational mass found with lensing maps is clearly not collocated with the baryonic mass in hot , x-ray emitting intracluster gas . however , microlensing surveys tend to rule out the first two , and we think we have a good handle on dust dynamics . something more exotic is called for . there is a diminishing community in support of mond - modified newtonian dynamics - which postulates long-range deviations from the inverse-square law in gravity . however , the bullet cluster , together with very precise solar system data , makes this theory difficult to get working . add to this the very nice " wimp miracle " ( no good wiki page there - sorry ) , which is suggestive of dark matter being new types of particles . " wimp " stands for " weakly interacting massive particle , " and the " miracle " is this : if you assume there is a species of particle $\text{x}$ whose only appreciable interaction is annihilation with its antiparticle $\overline{\text{x}}$ , with a cross section typical of weak interactions and a mass typical of , well , particles , you can easily calculate the abundance of these things in the present universe . they are in equilibrium with other species when the universe is energetic enough for them to be pair-created , and they " freeze out " when the universe cools enough . the end result is right around what we infer from other means . alternate theories for dark energy dark energy is a little trickier . there really is not a good explanation of " what " it even is - the name is more a catch-all for describing the observed accelerated expansion of the universe . you might believe it is a cosmological constant . in this case it is a nonzero scalar $\lambda$ in einstein 's equation $$ g_{\mu\nu} + \lambda g_{\mu\nu} = 8\pi t_{\mu\nu} , $$ where $g$ is the metric containing all the information about the curvature of spacetime , $g$ is a known function of $g$ and its first two derivatives , and $t$ is the stress-energy tensor containing all the information about matter , energy , and momentum in the universe . this is equivalent to saying there is some substance in the universe with equation of state $p = -\rho c^2$ ( $p$ is pressure , $\rho$ is mass density ) . [ for comparison , non-relativistic diffuse matter is well approximated by $p = 0$ , and relativistic matter has $p = \rho c^2/3$ . ] others are open to the idea that $w = p/ ( \rho c^2 ) $ is not exactly $-1$ for this new " stuff , " and so that can be a free parameter in your modeling . all the evidence points to $w$ being consistent with $-1$ , but the uncertainties are still somewhat large . you can get more exotic and note that the accelerated expansion phase the universe is currently undergoing is not entirely unlike the inflation many believe happened in the very early universe . inflation has all sorts of theories proposed for it , many of which are variations on " there is an abstract quantum field $\phi$ with these certain properties . . . " others take place in the realm of strings and branes . many of these theories can be boiled down to a $w$ that changes over time . future research these fields of study are very much alive and well . dark matter calls for both astrophysicists to study its role in large-scale dynamics and particle physicists to try to nail down its properties . the astrophysics side involves observers placing better constraints on the evidence so far , which has a lot to do with galaxy clusters and large-scale structure , as well as theorists to predict how dark matter influences e.g. galaxy formation , usually by using massive simulations . ( and those simulations , such as the millennium simulation , often lead to cool movies . ) the physics side involves experiments to try to detect the stuff directly ( there are literally dozens of these - far too many to list - and though there are some claimed detections , none are really accepted by the community at large ) . it also involves finding a place for such particles in an extended version of the standard model . dark energy is begging for physicists to come up with testable models that mesh with the rest of physics without seeming fine-tuned . it is still very much new , given that it was only last year 's nobel prize that recognized the teams that provided undeniable proof of its existence around the turn of the millennium . so by all means feel free to be inspired by these problems to work in some field . there is plenty to be done . i would even say the prospects are better than e.g. string theory alone , since we have solid , incontrovertible evidence saying there are gaping holes in our knowledge when it comes to dark matter and energy , and these holes are right where we can easily probe them via astronomy .
it travels forwards instead of backwards in an accelerating car for the same reason that a helium balloon travels upwards instead of downwards under the influence of gravity . why is that ? in an accelerating car , for all intents and purposes the acceleration can be considered a change in the amount and direction of gravity , from pointing straight down to pointing downwards and backwards . the balloon does not know and does not care if the acceleration is from gravity or from the acceleration of the car ; it just tries to move in the direction it naturally moves , namely , against the direction of the acceleration . thus , it moves forwards when you accelerate . hopefully you find this explanation intuitively satisfying . another more rigorous way to view the problem is through lagrangian minimization . the balloon can be considered a low-density object embedded in a higher-density fluid constrained within the confines of the car . under the influence of gravity pointing sideways , the total system potential energy decreases the farther forward the balloon is situated . since the force is the gradient of the potential , the balloon will try to move forward .
the nuclear forces are a complex amalgam of primarily quantum chromodynamics forces and electromagnetic ones , but to deal with the diagramatic way of calculating in quantum field theory , is not possible . the weak force responsible for beta decays should also be in the calculations . too many diagrams and too convoluted . quantum mechanical models with a potential well to estimate the collective forces are used for this . nuclear physics has been using various models successfully , like the shell model . to predict energy levels in nuclei . ways of estimating lifetimes are taught in nuclear engineering , for example : course outcomes : students must be able to . . . calculate the consequences of radioactive growth and decay and nuclear reactions . calculate estimates of nuclear masses and energetics based on empirical data and nuclear models . calculate estimates of the lifetimes of nuclear states that are unstable to alpha- , beta- and gamma decay and internal conversion based on the theory of simple nuclear models . use nuclear models to predict low-energy level structure and level energies .
which will go negative if $h&gt ; 10m$ . how is it possible ? it is not possible that $p_{up}$ will go negative . it will experience cavitation . this is one of those cases where you have to realize that the equation you are using is a special case of a larger theory , step back , and employ the larger theory . if you step back from the concept of water as an absolute liquid , then it is clear that it has a state diagram . going from some liquid place on the state diagram to $p=0$ will cross a phase transition line . probably just once , and probably the liquid to vapor line . that means the water boils . as you hit this threshold of $10 m$ between atmosphere and the lowest pressure point in the system , that point will turn to gas . there will possibly be some significant change in temperature , but we can imagine it happens slowly so everything is held at room temperature . so draw a line on the pt diagram of water of constant t , decreasing pressure . i have to make some adjustments to your diagram now . firstly , you imagined an open tube pointing down . i will have to revise this to a tube that snakes around so that it has a defined level . take a pipe filled with water and turn the open end down - the water is likely to fall out . we can often ignore this in small ( common aquarium ) pipes because of surface tension effects . in addition to that , let 's imagine the cavitated area that is now filled with water vapor . i will , of course , assume your pipe is strong enough . not to worry , plenty of pipes are . there is some detail that i did not include . but let 's start making observations : the pump 's exit is greater than atmosphere pressure , because the head above it is $&gt ; 10m$ the white areas in the pipe are filled with water vapor , in fact , saturated water vapor at room temperature the pressure of the vapor region will be the saturation pressure at room temperature . this is not zero , but it is very low . as the pump pushes more liquid through the system , that liquid will overflow through the vapor region and fall down , joining the water in the outlet pipe .
barring whatever fantastic energies would be required to stop the mass of the earth from rotating and then changing the direction of the rotation , one of the major things i can see changing would be the expectations of weather patterns . part of what affects our weather is known as the coriolis effect . while there would certainly be effects from the change of the weather from the coriolis effect how this would change the weather could only be guessed . it is certain to be transformative , and if it were an abrupt change , possibly catastrophic as deserts might move , food crops might be affected , and likely expected storm patterns would be changed . edit : i neglected to think about the effect this would have on our structures which would collapse due to shearing action unless the stopping action included a planetary stasis field . objects in motion tending to stay in motion and rotation speeds of over 1000 mph and the inconvenient theory of conservation of momentum . i forgot to think about our oceans as well , since the coriolis effect would change them , too . i also neglected to consider the molten core of the earth ( pesky spinning , geo-magnetic iron ) . stopping the rotation of the molten core might cause the magnetic field of the earth to collapse , allowing the world to be bathed in cosmic radiation ( killing every living thing ) . my assumption was when they stopped the rotation of the earth and reset it , they would consider having a magnetic field a good thing and would be sure to stop reset the whole thing . granted , the earth of the year 3000 may have other advantages which might offset any changes from the altered rotation of the earth . as a funny idea , it has potential , but the serious ramifications of such a feat boggle the imagination .
no , but it does not matter . the theories that approximate things using spheres are ones in which the final result ( the number you measure , the reading on your meter , whatever ) depends continuously in some sense on the deviations from sphericity . more symbolically , for any $\varepsilon$ tolerance you allow in your measurement ( none of our measurements are infinitely precise ) , there exists a $\delta$ such that any real object " within $\delta$" of being a sphere will give the same measurement to within $\varepsilon$ . it is not that theories are invalid because they assume something " wrong " about nature . instead , you have to understand that there is always an implicit statement about how " real " behavior approaches the model as deviations from the model 's assumptions get smaller .
in the short term , no , nothing bad will happen . the milky way is not particularly important to us in terms of keeping the solar system together , protecting us from anything dangerous , and so on . in fact it is probably slightly safer to leave : if the solar system is leaving the galaxy by natural causes , chances are we are a few billion years in the future and the milky way is colliding with andromeda , and our star is one of those that happened to get gravitationally scattered/ejected from the system . this is actually probably for the best . at present there are no known nearby stars that are likely to explode as supernova any time soon . a galaxy collision typically causes the formation of quite a large number of new stars , many of which will be the short lived hot type that go boom . if one happened to form fairly close to the sun and we waited maybe 10 million years , this could be very bad for earth . on the other hand the sun is expected to burn out around the same time as the milky way collides with andromeda , so being in the galaxy might be advantageous if we are still around . . . we might want to find a new home around another star . there are many more options in the galaxy than in the void of intergalactic space . other consequences of leaving the milky way : some astronomers will be very happy . the milky way will no longer be in the way of a big chunk of the sky , making life much easier for them ( i fall roughly into this category ) . some other astronomers will be very annoyed , since they were trying to study the milky way and now it is getting more difficult to do so . the night sky will get a lot less spectacular , especially in the southern hemisphere ( where you can look out toward the centre of the milky way ) . on the bright side , the milky way-andromeda collision will be quite the firework show , so at least we had get to see that before the view gets a bit more boring . in response to comment on potential harmful radiation sources : much of the astronomically sourced radiation that is potentially dangerous to humans is actually from our own galaxy . all galaxies are dumping radiation into space , but ours being nearby is the dominant source . for instance , here is the sky in the ultraviolet ( galex satellite ) ; the orange is higher intensity ( more info/maps here ) : in x-rays ( rosat satellite ) ; most of the energy is coming from the blue/green regions : and $\gamma$-rays ( fermi satellite ) ; red/yellow = higher intensity : as you can see , there is one feature that is qualitatively similar in every map - there is a big band of emission horizontally across the middle . the images are oriented so that the galactic plane lies across the middle horizontally ; our galaxy ( and all galaxies ) put out a lot of radiation . rather than shielding us , being in a galaxy actually irradiates us more . it is a good thing our atmosphere does a pretty good job of attenuating radiation in most harmful wavelengths ! of course in many wavelengths , the night sky is of secondary concern beside the sun - which definitely puts out way more in the ir , optical and uv ( you do not get a sunburn at night , after all ) , and has significant x-ray emission . i think the rest of the sky wins in gamma rays since the sun does not have processes energetic enough to produce any . for the longer wavelengths ( sub-mm/microwave/radio ) i am not sure off the top of my head whether most of the energy we receive is solar , galactic or cosmic in origin .
if you think of this in terms of quantum field theory , which is really required to give meaning to the photon , then all you are able to say is that the photon can take any of all possible paths from where it is emitted to where it is absorbed . these paths will contain paths where the photon momentarily splits into an electron positron pair , where the interactions with the electrons in the mirror involve all sorts of virtual particles , where the photon travels in directions which are far from the classical trajectory etc . the total amplitude is given by the sum of all these possibilities and they can all occur . in the classical limit this sum over all paths gets dominated by the contributions closest to the classical straight line path of the photon with velocity $c$ , so classically we see light travel in a straight line at velocity $c$ , and obey the laws of optics . however if you really wanted to follow the path of an individual photon you would see that it could do any of a spectacular number of things ( and unfortunately our attempts to observe the photon would interfere with its path ) . if you want to understand this better , i highly recommend feynman 's description of it all in his lectures here or in his book taken from the lectures : " qed , the strange theory of light and matter " .
temperature is an scalar function in this case . it remains invariant at every point . the heat flow forms a vector field . the values and components do transform under change of under change of co-ordinate system : qa = -kat in new system of co-ordinates ( x' ) : q'a = -ka't = -k ( x'a/xb ) bt = -k ( x'a/xb ) qb now , if these are co-ordinates in your frame of reference . sitting in your frame you might see some guy moving . you may decide to model how this guy views the heat flow . to do this first look at how this ' guy'/'thing ' is moving in your frame . obviously he will move with a velocity as a function solely of time or constant ( otherwise we would be discussing a whole family of observers ) . let the components of his position in your frame be ra in your frame , which is function only of time or a constant . now assuming he uses 3d cartesian co-ordinates his frame co-ordinates will be : x'a = xa - ra so , ( x'a/xb ) =ab as ra is a function of time , q'a = qa in this case . however if your observer decides to use some other co-ordinate system ( like circular ) then naturally the components of heat flow will be different . i hope this answers your question . please excuse my lack of proper formatting .
first , avoid the very cheapest binoculars . binoculars typically get much rougher handling than most optics , so must be robust , able to survive being banged around a bit without losing alignment ( collimation ) . look for even dark green coatings , preferable multicoated ( coating on all glass-air surfaces ) . prisms should be bak-4 glass , not cheaper bk-7 ; you can tell by looking at the exit pupils ( bak-4 has round pupils , bk-7 appear cut off ) . for astronomy , look for sharp images all across the field of view . after owning and using many different binoculars , the size i find the most useful for astronomy is 10x50 . the first number is magnification , the second aperture ( diameter of front lenses ) . 50mm aperture is the minimum for astronomy . 10x is about the highest magnification which most people can hand-hold . binoculars are at their best when hand-held . binoculars to avoid are those with zoom magnification ( their optical quality is generally poor ) and " universal focus " ( means fixed focus at an average distance , making everything in the sky slightly out of focus ) . stick to major brands , particularly camera manufacturers like canon , nikon , and pentax . orion has a particularly complete and generally good quality line . http://www.telescope.com/binoculars/5.uts
there are a large number of programs called planetarium programs , which animate the solar system , so that you can view the positions of any objects , such as the galilean moons , at any point in time . these are so accurate that you can enter the times galileo himself observed the moons in 1609 and 1610 and see them plotted accurately . my personal favourite for many years has been starry night , so much so that i now work for the company that produces it as a technical writer . if you just want to identify the moons , my friend akkana peck has written a little aplet which plots them online for any date and time : http://www.shallowsky.com/jupiter/
nerd sniping ! the answer is $\frac{4}{\pi} - \frac{1}{2}$ . simple explanation : http://www.mbeckler.org/resistor_grid/ mathematical derivation : http://www.mathpages.com/home/kmath668/kmath668.htm
if you want a function that has a certain graph , it is easy . first you digitize the graph , or ask the person who made the graph to send you the raw data . then you define an interpolating function from this data-set . that is it ! now you have your function . i do not know why you would want to do this , but it is very easy to do . usually in physics , we do not want to find any old function that happens to resemble an experimental curve . we would only do that kind of thing in very rare , specialized circumstances . * instead , we usually want a function that matches a curve for a deeper reason , i.e. because the function is describing or simulating the same real-world phenomenon which the experiment is measuring . therefore you would learn about the real world by reading the function definition , and you would learn interesting things about physics by seeing how well the function fits the experimental data . -- *an example where you might search for a function that happens to match a curve by coincidence , rather than for a deeper reason : if you are creating a large computer simulation , sometimes you want to increase the simulation 's speed by replacing a subroutine calculation with a different , faster-to-compute function ( or lookup table + interpolation ) , that happens to give approximately the same answer .
a common procedure to determine the spin of the excitations of a quantum field is to first determine the conserved currents arising from quasi-symmetries via noether 's theorem . for example , in the case of the dirac field , described by the lagrangian , $$\mathcal{l}=\bar{\psi} ( i\gamma^\mu \partial_\mu -m ) \psi $$ the associated conserved currents under a translation are , $$t^{\mu \nu} = i \bar{\psi}\gamma^\mu \partial^\nu \psi - \eta^{\mu \nu} \mathcal{l}$$ and the currents corresponding to lorentz symmetries are given by , $$ ( \mathcal{j}^\mu ) ^{\rho \sigma} = x^\rho t^{\mu \sigma} - x^\sigma t^{\mu \rho}-i\bar{\psi}\gamma^\mu s^{\rho \sigma} \psi$$ where the matrices $s^{\mu \nu}$ form the appropriate representation of the lorentz algebra . after canonical quantization , the currents $\mathcal{j}$ become operators , and acting on the states will confirm that , in this case , the excitations carry spin $1/2$ . in gravity , we proceed similarly . the metric can be expanded as , $$g_{\mu \nu} = \eta_{\mu \nu} + f_{\mu \nu}$$ and we expand the field $f_{\mu \nu}$ as a plane wave with operator-valued fourier coefficients , i.e. $$f_{\mu \nu} \sim \int \frac{\mathrm{d}^3 p}{ ( 2\pi ) ^3} \frac{1}{\sqrt{\dots}} \left\{ \epsilon_{\mu \nu} a_p e^{ipx} + \dots\right\}$$ we only keep terms of linear order $\mathcal{o} ( f_{\mu \nu} ) $ , compute the conserved currents analogously to other quantum field theories , and once promoted to operators as well act on the states to determine the excitations indeed have spin $2$ . an argument as to why a massless spin $2$ particle must be a graviton is given by feynman et al . , and i present verbatim a paraphrasing by professor d . tong : to summarize , theories of massless spin 2 fields only make sense if there is a gauge symmetry to remove the negative norm states . in general relativity , this gauge symmetry descends from diffeomorphism invariance . the argument of feynman and weinberg now runs this logic in reverse . it goes as follows : suppose that we have a massless , spin 2 particle . then , at the linearized level , it must be invariant under the gauge symmetry $f_{\mu \nu} \to f_{\mu \nu} + \partial_\mu \xi_\nu + \partial_\nu \xi_\mu$ in order to eliminate the negative norm states . moreover , this symmetry must survive when interaction terms are introduced . but the only way to do this is to ensure that the resulting theory obeys diffeomorpism invariance . that means the theory of any interacting , massless spin 2 particle is einstein gravity , perhaps supplemented by higher derivative terms . supplementary information the negative norm states that tong refers to are ghost states which the polyakov action initially suffers from . they arise because of the commutation relations , $$ [ a_{m}^\mu , a^\nu_n ] = m\delta_{m+n , 0} \eta^{\mu \nu}$$ regardless of the metric convention , either timelike or spacelike oscillators give rise to the negative norms . they are unphysical , violate unitarity , and hence a consistent theory requires their removal . counting physical degrees of freedom the graviton has spin $2$ , and as it is massless only two degrees of freedom . we can verify this in gravitational perturbation theory . we know $h^{ab}$ is a symmetric matrix , and only $d ( d+1 ) /2$ distinct components . in de donder gauge , $$\nabla^{a}\bar{h}^{ab} = \nabla^a\left ( h^{ab}-\frac{1}{2}h g^{ab}\right ) = 0$$ which provides us $d$ gauge constraints . there is also a residual gauge freedom , providing that infinitesimally , we shift by a vector field , i.e. $$x^\mu \to x^\mu + \xi^\mu$$ providing $\square \xi^\mu + r^\mu_\nu \xi^\nu = 0$ , which restricts us by $d$ as well . therefore the total physical degrees of freedom are , $$\frac{d ( d+1 ) }{2}-2d = \frac{d ( d-3 ) }{2}$$ if $d=4$ , the graviton indeed has only two degrees of freedom .
theoretically it would be feasible , as the law of conservation of energy is a universal principle , regardless of how large the system is . i think is would be difficult to realize a very large model like shown in the ad . first of all , the balls are not exactly rigid . the larger the balls get , the larger the area of contact becomes , thereby increasing dissipation of energy . coming to the ad , it is clearly a product of modern day computer graphics . if we assume a length of 50 m for the hanging chain , the time period of the pendulum , $2\pi\sqrt{\frac{l}{g}}$ ~ 14 s . clearly the time period of the pendulum in the video is much much smaller . further in reality , the balls would initially break-up due the elastic response of the balls ( the balls are a little ' springy ' and obey hooke 's law ) . then each ball would oscillate about a mean position ( yes all the balls move ! we do not notice this as the time period of the individual oscillations is quite small compared to the time period of the main balls ) . and finally due to dissipation of energy , the balls would settle down . a very realistic simulation has been done on newton 's cradle by stefan hutzler et al . a video of the simulation can be found here .
by a coincidence , the radius of a " newtonian black hole " is the same as the radius of the schwarzschild black hole in general relativity . we demand the escape velocity $v$ to be the speed of light $c$ , so the potential energy $gmm/r = mc^2/2$ , i.e. $$ r = \frac{2gm}{c^2} $$ the agreement , especially when it comes to the numerical factor of $2$ , is a coincidence . but one must appreciate that these are totally different theories . in particular , there is nothing special about the speed $c$ in the newtonian ( nonrelativistic ) gravity . to be specific , objects are always allowed to move faster than $c$ which means that they may always escape the would-be black hole . there are no real black holes ( object from which nothing can escape ) in newton 's gravity .
put a scale between your hand and the wall . the reason you have no acceleration is that the sum of force vectors is zero . you are pushing , and the wall is pushing back in the opposite direction , adding up to zero .
sure , that is what the wavy motion over a hot car is , and also the cause of mirror-like mirages on deserts and hot roads . stars also twinkle due to the same effect . not sure about street lights per se , but i am sure there are combinations where air refraction of light would show up there also .
the book that you are following makes the following simplification : the object instantly comes to rest the moment it hits the ground . this is perhaps not completely physically reasonable , but i think you can also understand that the point of the relevant section is to discuss how far one can throw objects , not to provide a detailed discussion of the relevant effects when rolling a ball ; )
the topic of " energy conservation " really depends on the particular " theory " , paradigm , that you are considering  and it can vary quite a lot . a good hammer to use to hit this nail is nther 's theorem : see , e.g. , how it is applied in classical mechanics . the same principle can be applied to all other theories in physics , from thermodynamics and statistical mechanics all the way up to general relativity and quantum field theory ( and gauge theories ) . thus , the lesson to learn is that energy is only conserved if there is translational time symmetry in the problem . which brings us to general relativity : in several interesting cases in gr , it is simply impossible to properly define a " time " direction ! technically speaking , this would imply a certain global property ( called " global hyperbolicity" ) which not all 4-dimensional spacetimes have . so , in general , energy is not conserved in gr . as for quantum effects , energy is conserved in quantum field theory ( which is a superset of quantum mechanics , so to speak ) : although it is true that there can be fluctuations , these are bounded by the " uncertainty principle " , and do not affect the application of nther 's theorem in qft . so , the bottom line is that , even though energy is not conserved always , we can always understand what this non-conservation mean via nther 's theorem . ; - )
a short summary of the paper mentioned in another answer and another good site . basically planes fly because they push enough air downwards and receive an upwards lift thanks to newton 's third law . they do so in a variety of manners , but the most significant contributions are : the angle of attack of the wings , which uses drag to push the air down . this is typical during take off ( think of airplanes going upwards with the nose up ) and landing ( flaps ) . this is also how planes fly upside down . the asymmetrical shape of the wings that directs the air passing over them downwards instead of straight behind . this allows planes to fly level to the ground without having a permanent angle on the wings .
you have a few basic formulas for solving this kind of stuff : $$\vec{x}_f=\vec{x}_i+\vec{v}_it+\frac{1}{2}\vec{a}t^2 , $$ $$\vec{v}_f=\vec{v}_i+at . $$ these are vector formulas , but all you are doing with the vectors is adding/subtracting them . when you add vectors you add the individual components , i.e. $$\vec{a}= ( a_1 , a_2 , a_3 ) $$ $$\vec{b}= ( b_1 , b_2 , b_3 ) $$ $$\vec{a}+\vec{b} = ( a_1+b_1 , a_2+b_2 , a_3+b_3 ) . $$ so for your first question , you want to find what time your $r_y$ component is 0 , as that is when the particle is on the x-axis . for your second question , your particle will be moving parallel to the y-axis when it is velocity is completely in the y direction , in other words your $v_x=0$ .
the experiments have measured the number of events in which the higgs boson was produced and decayed into various final particles such as a pair of photons ; pair of z bosons ; pair of tau leptons ; bottom quark-antiquark pair , and a few others . all these numbers are compatible , within the current error margin , with the prediction of the standard model higgs boson ( the diphoton decay rate may be currently higher by 70% or so and it has not been fully updated , but all the others are currently " totally well-behaved" ) . there are specific polarization-based experiments that show that the particle is almost certainly a scalar , not a pseudoscalar , and it must have spin $j=0$ . if the particle were a " completely , qualitatively different " particle than the higgs boson , the numbers of events mentioned above would differ from the standard model higgs values ( theoretically calculable and experimentally measured ) by a lot , typically by many orders of magnitude . so it is clearly " something like the higgs boson " . moreover , a higgs boson is needed for the standard model to be consistent . at this moment , it remarkably barks like the sm higgs boson and does everything else like one , too . at the current accuracy , we may see it is the standard model higgs boson . in fact , our accuracy and certainty that it is so is no longer too different from the certainty with which we may say that what we call the w-bosons are w-bosons and what we call the z-bosons are z-bosons . if you were impartial , you should have asked the same doubtful questions about the w-bosons or z-bosons or other particles , too . the higgs boson is a newer particle but it is already a part of the standard collection that agrees with the standard model so well that it can not be quite a coincidence . the fact that the discovered july 4th higgs boson behaves just like the standard model higgs boson is increasingly clear and increasingly frustrating because it gives us no hints how to go beyond the standard model so far . see stories such as http://articles.timesofindia.indiatimes.com/2012-11-19/science/35204514_1_higgs-boson-standard-model-higgs-british-physicist-peter-higgs sometime in the future , perhaps distant future , new physics will be discovered , e.g. new particles , and they will modify the whole theory including the identity of the higgs sector . but they will slightly modify the z-bosons , w-bosons , and all other particles as well . in some sense , it will be proved that they are not the standard model z-bosons or w-bosons , either . it makes no sense to single out the higgs boson ( although when it comes to the higgs , there are best reasons to think it has " siblings" ) . the whole theory will ultimately be shown to differ from the standard model . at this moment , however , once again , all the observations at the lhc agree with the standard model which is why it is legitimate to use the standard model terminology for all the particles and interactions we are observing .
the atmosphere rotates along with the earth for the same reason you do . force is not needed to make something go . that is a basic law of physics - that a thing that is moving will just keep moving if there is no force on it . force is needed either to make something change its speed , or to make its motion point in a new direction . a force can do both or just one of these . most forces do both , but a force that pushes in the exactly the same direction you are already going only changes your speed , and does not change your direction . a force that pushes at a right angle to the direction you are already going only changes your direction , and does not add any speed . a force at "10 o'clock " , for example , will change both your speed and your direction . as you stand still on earth , you continue going the same speed , but your direction changes ; between day and night you move opposite directions . so the forces on you must be at a right angle to your direction of motion . indeed , they are . your motion is from west to east along the surface of the earth , and the force of gravity pulls you down towards the center of the earth - the force and your motion are at right angles . similarly for the atmosphere . it is moving along with the earth , and moving at a constant speed . it does not need anything to push it along with the earth . since only its direction of motion is changing , it only needs a force at a right angle to its motion , the same as you , and the force that does the job is again gravity . that is not the whole picture , because the amount that your direction of motion changes depends on how strong the right-angle force is . it turns out gravity is much too strong for how much our direction of motion changes as the earth spins . there must be some other force on us and on the atmosphere canceling out most of the gravity . there is . for me it is the force of the chair on my butt . for the atmosphere , it is the air pressure . so gravity does not " make the air rotate " . the air is already going , and gravity simply changes its direction to pull it in a circle . you may be wondering why the air does not just sit there and have the earth spin underneath it . one answer to that is that from our point of view that would mean incredibly strong wind all the time . that wind would run into stuff and eventually get slowed down to zero ( that is from our point of view - the air would " speed up " to our speed of rotation from a point of view out in space watching everything happen ) . even the air high up would eventually rotate with the earth because although it can not slam into mountains or buildings and get stopped from blowing , it can essentially " slam into " the air beneath it due to friction in the air . ( this is a little redundant with dmckee 's answer ; i was half way done when he beat me to the punch )
the vacuum state is the thermal state for $t=0k$ . how to compare if a state is close enough to the vacuum state ? by counting photons ( for vacuum it is zero ) . the occupation for photons is given by bose-einstein distribution : $$n = \frac{1}{\exp ( e/ ( kt ) ) - 1} , $$ where $e$ is the photon energy ( $e = \hbar \omega = h \nu$ ) and $k$ is the boltzmann constant . for room temperature ( $t\approx 300k \rightarrow kt \approx 0.025ev$ ) and visible light ( $e\approx 2.5ev$ ) it gives $$n\approx 10^{-44}$$ that is , very very few ( and in practice - the vacuum state ) . see a gallery of wigner functions ( the so-called winger function illustrates fluctuation of electrical field around 0 ; note that even fore $t=0$ there are some fluctuations ( zero-point energy fluctuations ) , but for $t> 0$ there are higher ) .
look up the " wigner 's friend " thought experiment : your scenario is the one at the point when the friend knows the state of the cat but before wigner comes back and asks his friend whether his friend 's cat is dead . generally such thought experiments involving " conscious observers " are not tackled much anymore as a conscious observer is a hugely complicated , uncharacterised system . instead we replace wigner and his friend by quantum observables : simple operators that take the quantum state as an input , return a real valued measurement and somehow ( the answer to this somehow is the quantum measurement problem ) straight after the " observable 's " application , the quantum system is in the eigenstate of the observable 's operator that corresponds to the value measured . the quantum measurement problem need not worry us here . we simply replace the two people by observables and assume that an eigenstate of an observable prevails straight afterward its application , whether or not the quantum state is " collapsed " there . ( as an aside , i sense a hope in some writings that the quantum measurement problem is indeed resolvable and may be so even in my lifetime - look up einselection for example ) . so now , observable $\hat{w}_1$ acts on a pure quantum state $\psi$ ( a pure " superposition " , as you call it ) and forces it into one of its eigenstates . now , from the standpoint of the second person ( who , as you say , knows the first observable $\hat{w}_1$ has been applied , but does know the eigenstate prevailing after application of $\hat{w}_1$ ) , the quantum state is now in a mixed quantum state . the question of consciousness does not enter : the quantum state has been acted on by $\hat{w}_1$ , it is simply that the second person does not know the outcome . so now , the accepted way to think of the state from the second person 's standpoint is as a classical statistical mixture of pure quantum states . suppose the second observer wants to impart observable $\hat{w}_2$ some time after $\hat{w}_1$ is applied . in principle ( this is not done in practice , rather the density matrix formalism is used ) , to foretell the probability distributions of outcomes , the second observer must calculate the pure quantum state 's evolution ( by , say the schrdinger equation ) for each possible outcome of the observable $\hat{w}_1$ ( i.e. . for each possible eigenstate $\left|\left . \psi_{1 , j}\right&gt ; \right . $ output by the observable $\hat{w}_1$ ) , work out the the probability distibutions arising from the application of $\hat{w}_2$ on each of these evolved eigenstates and combine these distributions following the rules of classical probability . as i said , there is a much simpler way of doing all this through the density matrix formalism : the density matrix is : $$\rho = \sum_j p_{1 , j} \ , \left|\left . \psi_{1 , j}\right&gt ; \right . \left&lt ; \left . \psi_{1 , j}\right|\right . $$ where $\left|\left . \psi_{1 , j}\right&gt ; \right . $ are the eigenstates of the first observable $\hat{w}_1$ and $p_{1 , j}$ their probabilities given the pure quantum state that was input to the first observable . instead of evolving the pure state by the schrdinger equation , the density matrix evolves following the liouville-von neumann equation : $$i \hbar \frac{\partial \rho}{\partial t} = [ \hat{h} , \rho ] $$ where $\hat{h}$ is the quantum system 's hamiltonian , and when we get to the time where the second person does their experiment , i.e. imparts observable $\hat{w}_2$ , we calculate the $n^{th}$ moment of the statistical distribution of the measurement outcomes as : $$m_n={\rm tr}\left ( \rho\ , \hat{w}_2^n\right ) $$ whence we can derive the full probability density function for the measurement outcomes from $\hat{w}_2$
i think you are asking how much damage would be done to the driver in the two cases you described . if that is your question , then the single car that is driven at a speed of $50\frac{km}{hr}$ into an infinite mass wall would experience the same damage as two identical cars being driven exactly head on at a speed relative to the ground of $50\frac{km}{hr}$ into each other . you can easily understand this if you imagine an infinitely thin wall between the two colliding cars at exactly the plane where they collide . assuming no shards or other pieces breaking off and going through this imaginary wall you can see that this is exactly the same as having an infinitely massive wall in place of either car since everything is exactly symmetric about this infinitely thin imaginary wall . to be absolutely correct , this answer would actually require that the cars be left-right symmetric so that the centers of mass exactly line up perpendicular to the plane of the collision . if they were asymmetric it would be similar to symmetric cars hitting slightly off center - so there would be some torque around the point on the collision plane where the line determined by centers of mass intersect the plane . clear ? edit : a situation which is equivalent to 2 cars hitting head on at 50kmph is the following : one car sitting stationary ( with brakes off ) while the other car hits it head on at 100kmph . this assumes a perfectly inelastic collision so that the two cars will then proceed ( joined together ) in the original direction of the 100kmph car but they will both be going at 50kmph . in both of these cases the change in speed of each car is 50kmph in a short time so the damages will be equivalent ( either 50-> 0 , 100-> 50 or 0-> -50 ) . however , if a car that is traveling at 100kmph hits an infinite mass wall the change in speed in a short time will be 100kmph so it is not equivalent to the two cases .
you ask : how can astronomers see light from shortly after the big bang ? how did we get " here " before the light that emanated from our " creation " ? the very idea of " big bang " assumes general relativity holds . an analogue of this big bang can be a balloon that starts from a point and starts expanding . our three space dimension universe has as analogue the two dimensional surface of this balloon . everything we observe , the light going from point to point on the balloon is on that surface . the inside of the balloon is not a space where matter or photons propagate . everything happens at the expanding surface . thus the light the astronomers see from the big bang , the cosmic microwave background radiation is the light coming from everywhere on our point on this surface . we hypothesize a time 0 for light coming to us because we see how cold the photons are , and calculate the red shift from the initial hot production of all those photons . but they are coming from everywhere ( the balloon analogue ) . it is very anthropic in a sense to claim that we each of us are the center from where the big bang started : ) .
nuclei have energy levels just like atoms do , but while the energy level spacing in atoms is around the energy of visible light , the energy level spacing in nuclei is around the energy of gamma rays . so while an atom may relax from an excited state to the ground state by emitting visible light , when a nucleus relaxes from an excited state to a ground state it emits gamma rays . when a nucleus undergoes radioactive decay the daughter nuclei are often created in an excited state , and they subsequently relax to the ground state and emit gamma rays . see the wikipedia article on gamma decay for more details .
from wikipedia , the mean sun-planet distances are ( in au ) : Mercury 0.39 Venus 0.72 Earth 1.00 Mars 1.52 Jupiter 5.20 Saturn 9.54 Uranus 19.22 Neptune 30.06  to get the maximum and minimum distances from earth , add or subtract 1 au . to get those distances in light-minutes , multiply by 8.317 light-minutes per au . to save you the trouble : these are maximuma and minima presuming circular orbits for all the planets . calculating the distance the planets right now is more difficult but you can get the data from solar system live . for the record , i have answered this question because it is really easy . but the op should take note that an easy calculation with easily accessible data was all the he needed . . .
it is difficult to know what happens on the other side of a black hole , since no information can cross back through the event horizon ( the radius at which light and therefore any information can no longer escape ) . the leading idea is that near the center of every black hole lies a singularity , or a point where the density ( and therefore the curvature of space-time ) reaches infinity ( that is , some amount of mass contained in zero volume ) . any mass which crosses the event horizon will accelerate inwards toward the singularity . when it starts to get close , it will experience tremendous tidal stress . because the singularity contains the mass of anywhere from one supergiant star to several million of them ( or possibly more ) and is infinitely dense , the end of the object which is closer to the singularity will experience significantly more gravitational force than the end facing away . this will manifest itself as a gradually increasing stretching of the mass , something colloquially referred to as " spaghettification . " the falling mass will contact the singularity and become part of it in finite time in the reference frame of the mass . to an outside observer , things get a little funny due to the relativistic effects of such a strong gravitational field . beyond the event horizon , time is essentially frozen , so to anyone watching the black hole , nothing can ever happen inside it . so if an object starts to get close to it and fall in , it will gradually slow down and turn red ( the light is red-shifted ) and never cross the horizon . the light will get more and more red-shifted until it is infrared , microwaves , radio waves , etc--until it essentially disappears . but you will never observe it cross the horizon . the object itself will cross the horizon normally , however it will still always look like it has not crossed the horizon--because all light is rushing down towards the singularity , and none can come back the other way , it will always appear that the horizon is just beyond reach . looking the other way though , the outside world would appear extremely weird once inside the horizon .
a way to do this is using regularization by substracting a continuous integral , , with the help of the euler-maclaurin formula : you can write : $$ \sum_{regularized} = ( \sum_{n=0}^{+\infty}f ( n ) - \int_0^{+\infty} f ( t ) \ , dt ) = \frac{1}{2} ( f ( \infty ) + f ( 0 ) ) + \sum_{k=1}^{+\infty} \frac{b_k}{k ! } ( f^{ ( k - 1 ) } ( \infty ) - f^{ ( k - 1 ) } ( 0 ) ) $$ where $b_k$ are the bernoulli numbers . with the function $f ( t ) = te^{-\epsilon t}$ , with $\epsilon &gt ; 0$ , you have $f^{ ( k ) } ( \infty ) = 0$ and $f ( 0 ) = 0$ , so with the limit $\epsilon \rightarrow 0$ , you will find : $$\sum_{regularized} = - \frac{b_1}{1 ! } f ( 0 ) - \frac{b_2}{2 ! } f ' ( 0 ) = - \frac{1}{12}$$ because $f ( 0 ) = 0$ and $b_2 = \frac{1}{6}$
just on dimensional grounds i would expect that $$\text{angular size of fluctuation in radians} \times \text{time since recombination}$$ is the time scale for significant changes in the cmb ( where you choose what makes a fluctuation " significant " and select the angular size on that basis ) . why ? because the size of the regions defined by the fluctuations is the distance to the observed shell , and that distance is given by the time since recombination and the speed of light ; and information can travel across those regions no faster than light . the whole spectrum will cool a bit faster than that , but the cooling can be expected to be uniform . the rate of cooling is given by the hubble ( non- ) constant . note that the time since recombination is roughly 13.5 billion years , so even though the domains are pretty small it is still going to be a long wait .
i am not sure i will be able to post all the links i would like to ( not enough ' reputation points ' yet ) , but i will try to point to the major refs i know . matilde marcolli has a nice paper entitled " number theory in physics " explaining the several places in physics where number theory shows up . [ tangentially , there is a paper by christopher deninger entitled " some analogies between number theory and dynamical systems on foliated spaces " that may open some windows in this theme : after all , local systems are in the basis of much of modern physics ( bundle formulations , etc ) . ] there is a website called " number theory and physics archive " that contains a vast collection of links to works in this interface . sir michael atiyah just gave a talk ( last week ) at the simons center inaugural conference , talking about the recent interplay between physics and math . and he capped his talk speculating about the connection between quantum gravity and the riemann hypothesis . he was supposed to give a talk at the ias on this last topic , but it was canceled . to finish it off , let me bring the langlands duality to the table : it is related to modular forms and , a such , number theory . ( cavalier version : think of the qft path integral as having a mbius symmetry with respect to the coupling constants in the lagrangian . ) with that out of the way , i think the better angle to see the connection between number theory and physics is to think about the physics problem in a different way : think of the critical points in the potential and what they mean in phase space ( hamiltonian and/or geodesic flow : jacobi converted one into another ; think of jacobi fields in differential geometry ) , think about how this plays out in qft , think about moduli spaces and its connection to the above . this is sort of how i view this framework . . . ; - )
yes sam , there definitely is electric field reshaping in the wire . strangely , it is not talked about in hardly any physics texts , but there are surface charge accumulations along the wire which maintain the electric field in the direction of the wire . ( note : it is a surface charge distribution since any extra charge on a conductor will reside on the surface . ) it is the change in , or gradient of , the surface charge distribution on the wire that creates , and determines the direction of , the electric field within a wire or resistor . for instance , the surface charge density on the wire near the negative terminal of the battery will be more negative than the surface charge density on the wire near the positive terminal . the surface charge density , as you go around the circuit , will change only slightly along a good conducting wire ( hence the gradient is small , and there is only a small electric field ) . corners or bends in the wire will also cause surface charge accumulations that make the electrons flow around in the direction of the wire instead of flowing into a dead end . resistors inserted into the circuit will have a more negative surface charge density on one side of the resistor as compared to the other side of the resistor . this larger gradient in surface charge distribution near the resistor causes the relatively larger electric field in the resistor ( as compared to the wire ) . the direction of the gradients for all the aforementioned surface charge densities determine the direction of the electric fields . this question is very fundamental , and is often misinterpreted or disregarded by people . we are all indoctrinated to just assume that a battery creates an electric field in the wire . however , when someone asks " how does the field get into the wire and how does the field know which way to go ? " they are rarely given a straight answer . a follow up question might be , " if nonzero surface charge accumulations are responsible for the size and direction of the electric field in a wire , why does not a normal circuit with a resistor exert an electric force on a nearby pith ball from all the built up charge in the circuit ? " the answer is that it does exert a force , but the surface charge and force are so small for normal voltages and operating conditions that you do not notice it . if you hook up a 100,000v source to a resistor you would be able to measure the surface charge accumulation and the force it could exert . here 's one more way to think about all this ( excuse the length of this post , but there is so much confusion on this question it deserves appropriate detail ) . we all know there is an electric field in a wire connected to a battery . but the wire could be as long as desired , and so as far away from the battery terminals as desired . the charge on the battery terminals can not be directly and solely responsible for the size and direction of the electric field in the part of the wire miles away since the field would have died off and become too small there . ( yes , an infinite plane of charge , or other suitably exotic configurations , can create a field that does not decrease with distance , but we are not talking about anything like that . ) if the charge near the terminals does not directly and solely determine the size and direction of the electric field in the part of the wire miles away , some other charge must be creating the field there ( yes , you can create an electric field with a changing magnetic field instead of a charge , but we can assume we have a steady current and non-varying magnetic field ) . the physical mechanism that creates the electric field in the part of the wire miles away is a small gradient of the nonzero surface charge distribution on the wire . and the direction of the gradient of that charge distribution is what determines the direction of the electric field there . for a rare and absolutely beautiful description of how and why surface charge creates and shapes the electric field in a wire refer to the textbook : " matter and interactions : volume 2 electric and magnetic interactions " by chabay and sherwood , chapter 18 " a microscopic view of electric circuits " pg 631-640 .
if you were a completely empty shell you had likely be crushed immediately on finding yourself in the earth 's atmosphere . but you are filled with stuff ( blood , flesh , bones ) which is also at approximately atmospheric pressure . if you consider a point on your skin , the pressure of the air on the outside pushing it in is exactly matched by the pressure of the contents of your body pushing it out . so the net force is zero .
yes , the bulk modulus $b$ is the inverse of the isothermal compressibility $c$ , $$ b = \frac {1}{c} . $$ see e.g. wikipedia . the " bulk modulus " is more typical terminology in mechanics where we do not care about heat much and where the typical assumption is that the temperature is kept fixed ( because mechanical engines start to malfunction if their temperature goes awry ) ; the bulk modulus is " isothermal " because of the choice of the discipline , mechanics . in thermodynamics , one speaks about compressibility  which is terminology reminiscent of gases which are " easy " in thermodynamics  and the adjective " isothermal " is very important in thermodynamics because thermodynamics is all about the differences between different ways how the heat may propagate or not propagate ( in thermodynamics , we really want the temperature to change etc . , it is pretty much the point of the discipline , so things are often non-isothermal ) .
disclaimer : this is answer is given from a mathematical physics point of view , and it is a little bit technical . any comment or additional answer from other points of view is welcome . the classical limit of quantum theories and quantum field theories is not straightforward . it is now a very active research topic in mathematical physics and analysis . the idea is simple : by its own construction , quantum mechanics should reduce to classical mechanics in the limit $\hslash\to 0$ . i do not think it is necessary to go into details , however for qm this procedure is now well understood and rigorous form a mathematical standpoint . for qfts , such as qed , the situation is similar , although more complicated , and it can be mathematically handled only in few situations . although it has not been proved yet , i think it is possible to prove convergence to classical dynamics for a ( simple ) model of qed , describing rigid charges interacting with the quantized em field . the hilbert space is $\mathscr{h}=l^2 ( \mathbb{r}^{3} ) \otimes \gamma_s ( \mathbb{c}^2\otimes l^2 ( \mathbb{r}^3 ) ) $ ( $\gamma_s$ is the symmetric fock space ) . the hamiltonian describes an extended charge ( with charge/mass ratio $1$ ) coupled with a quantized em field in the coulomb gauge : \begin{equation*} \hat{h}= ( \hat{p} - c^{-1} \hat{a} ( \hat{x} ) ) ^2+\sum_{\lambda=1,2}\hslash\int dk\ ; \omega ( k ) a^* ( k , \lambda ) a ( k , \lambda ) \ ; , \end{equation*} where $\hat{p}=-i\sqrt{\hslash}\nabla$ and $\hat{x}=i\sqrt{\hslash}x$ are the momentum and position operators of the particle , $a^{\#} ( k , \lambda ) $ are the annihilation/creation operators of the em field ( in the two polarizations ) and $\hat{a} ( x ) $ is the quantized vector potential \begin{equation*} \hat{a} ( x ) =\sum_{\lambda=1,2}\int \frac{dk}{ ( 2\pi ) ^{-3/2}}\ ; c\sqrt{\hslash/2\lvert k\rvert}\ ; e_\lambda ( k ) \chi ( k ) ( a ( k , \lambda ) e^{ik\cdot x}+a^* ( k , \lambda ) e^{-ik\cdot x} ) \ ; ; \end{equation*} with $e_\lambda ( k ) $ orthonormal vectors such that $k\cdot e_\lambda ( k ) =0$ ( they implement the coulomb gauge ) and $\chi$ is the fourier transform of the charge distribution of the particle . the magnetic field operator is $\hat{b} ( x ) =\nabla\times \hat{a} ( x ) $ and the ( perpendicular ) electric field is $$\hat{e} ( x ) =\sum_{\lambda=1,2}\int \frac{dk}{ ( 2\pi ) ^{-3/2}}\ ; \sqrt{\hslash\lvert k\rvert/2}\ ; e_\lambda ( k ) \chi ( k ) i ( a ( k , \lambda ) e^{ik\cdot x}-a^* ( k , \lambda ) e^{-ik\cdot x} ) \ ; $$ $\hat{h}$ is a self adjoint operator on $\mathscr{h}$ , if $\chi ( k ) /\sqrt{\lvert k\rvert}\in l^2 ( \mathbb{r}^3 ) $ , so there is a well defined quantum dynamics $u ( t ) =e^{-it\hat{h}/\hslash}$ . consider now the $\hslash$-dependent coherent states \begin{equation*} \lvert c_\hslash ( \xi , \pi , \alpha_1 , \alpha_2 ) \rangle=\exp\bigl ( i\hslash^{-1/2} ( \pi\cdot x+i\xi \cdot\nabla ) \bigr ) \otimes\exp\bigl ( \hslash^{-1/2}\sum_{\lambda=1,2} ( a^*_\lambda ( \alpha_\lambda ) -a_\lambda ( \bar{\alpha}_\lambda ) ) \bigr ) \omega\ ; , \end{equation*} where $\omega=\omega_1\otimes\omega_2$ with $\omega_1\in c_0^\infty ( \mathbb{r}^3 ) $ ( or in general regular enough , and with norm one ) and $\omega_2$ the fock space vacuum . what it should be at least possible to prove is that ( $\alpha_\lambda$ is the classical correspondent of $\sqrt{\hslash}a_\lambda$ , and it appears inside $e ( t , x ) $ and $b ( t , x ) $ below ) : \begin{gather*} \lim_{\hslash\to 0}\langle c_\hslash ( \xi , \pi , \alpha_1 , \alpha_2 ) , u^* ( t ) \hat{p}u ( t ) c_\hslash ( \xi , \pi , \alpha_1 , \alpha_2 ) \rangle=\pi ( t ) \\ \lim_{\hslash\to 0}\langle c_\hslash ( \xi , \pi , \alpha_1 , \alpha_2 ) , u^* ( t ) \hat{x}u ( t ) c_\hslash ( \xi , \pi , \alpha_1 , \alpha_2 ) \rangle=\xi ( t ) \\ \lim_{\hslash\to 0}\langle c_\hslash ( \xi , \pi , \alpha_1 , \alpha_2 ) , u^* ( t ) \hat{e} ( x ) u ( t ) c_\hslash ( \xi , \pi , \alpha_1 , \alpha_2 ) \rangle=e ( t , x ) \\ \lim_{\hslash\to 0}\langle c_\hslash ( \xi , \pi , \alpha_1 , \alpha_2 ) , u^* ( t ) \hat{b} ( x ) u ( t ) c_\hslash ( \xi , \pi , \alpha_1 , \alpha_2 ) \rangle=b ( t , x ) \ ; ; \end{gather*} where $ ( \pi ( t ) , \xi ( t ) , e ( t , x ) , b ( t , x ) ) $ is the solution of the classical equation of motion of a rigid charge coupled to the electromagnetic field : \begin{equation*} % \left\{ \begin{aligned} and \left\{\begin{aligned} \partial_t and b + \nabla\times e=0\\ \partial_t and e - \nabla\times b=-j \end{aligned}\right . \mspace{20mu} \left\{\begin{aligned} \nabla\cdot and e=\rho\\ \nabla\cdot and b=0 \end{aligned}\right . \\ and \left\{\begin{aligned} \dot{\xi} and = 2\pi\\ \dot{\pi} and = \frac{1}{2} [ ( \check{\chi}*e ) ( \xi ) +2\pi\times ( \check{\chi}*b ) ( \xi ) ] \end{aligned}\right . \end{aligned} % \right . \end{equation*} with $j=2\pi\check{\chi} ( \xi-x ) $ , and $\rho=\check{\chi} ( \xi-x ) $ ( charge density and current ) . to sum up : the time evolved quantum observables averaged over $\hslash$-dependent coherent states converge in the limit $\hslash\to 0$ to the corresponding classical quantities , evolved by the classical dynamics . hoping this is not too technical , this picture gives a precise idea of the correspondence between the classical and quantum dynamics for an em field coupled to a charge with extended distribution ( point charges cannot be treated mathematically on a completely rigorous level both classically and quantum mechanically ) .
i get the physical significance of vector addition and subtraction . but i do not understand what do dot and cross products mean ? perhaps you would find the geometric interpretations of the dot and cross products more intuitive : the dot product of a and b is the length of the projection of a onto b multiplied by the length of b ( or the other way around--it is commutative ) . the magnitude of the cross product is the area of the parallelogram with two sides a and b . the orientation of the cross product is orthogonal to the plane containing this parallelogram . why can not vectors be divided ? how would you define the inverse of a vector such that $\mathbf{v} \times \mathbf{v}^{-1} = \mathbf{1}$ ? what would be the " identity vector " $\mathbf{1}$ ? in fact , the answer is sometimes you can . in particular , in two dimensions , you can make a correspondence between vectors and complex numbers , where the real and imaginary parts of the complex number give the ( x , y ) coordinates of the vector . division is well-defined for the complex numbers . the cross-product only exists in 3d . division is defined in some higher-dimensional spaces too ( such as the quaternions ) , but only if you give up commutativity and/or associativity . here 's an illustration of the geometric meanings of dot and cross product , from the wikipedia article for dot product and wikipedia article for cross product :
different spin liquids are extremely rich , and they cannot be described by $z_2$ topological index . so there is no $z_2$ topological index for generic spin liquids .
by special relativity , the energy needed to accelerate a particle ( with mass ) grow super-quadratically when the speed is close to c , and is &infin ; when it is c . $$ e = \gamma mc^2 = \frac{mc^2}{\sqrt{1 - ( \text{percent of speed of light} ) ^2}} $$ since you can not supply infinite energy to the particle , it is not possible to get to 100% c . edit : suppose you have got an electron ( m = 9.1 &times ; 10 -31 kg ) to 99.99% of speed of light . this is equivalent to providing 36 mev of kinetic energy . now suppose you accelerate " a little more " by providing yet another 36 mev of energy . you will find this this only boosts the electron to 99.9975% c . say you accelerate " a lot more " by providing 36,000,000 mev instead of 36 mev . that will still make you reach 99.99999999999999% c instead of 100% . the energy increase explodes as you approach c , and your input will exhaust eventually no matter how large it is . the difference between 99.99% and 100% is infinite amount of energy .
both ignacio and anna 's answers are correct , but let me see if i can expand on them a little . you ask : why does a gas inside a closed container experience atmosperic pressure when the gas itself is not in contact with the atmosphere ? but actually a gas inside a closed container does not necessarily experience atmospheric pressure . if you have ever shaken a bottle of pop then opened it you will know that the gas pressure inside is a lot higher than atmospheric pressure . this is an example where the pressure inside the closed container is higher than atmospheric pressure , but it can be lower as well . what happens to the gas in a container depends on how flexible the container is . if i take a glass bottle and pump half the air out , the gas inside will stay at half atmospheric pressure . that is because the glass walls of the bottle are rigid and they take some of the pressure . on the other hand if i fill a plastic bag with gas then pump some out , the pressure inside the bag stays at atmospheric pressure . that is because the plastic bag is flexible . the atmosphere presses on the bag and the bag transmits the pressure to the gas inside it .
this must have been asked before , but the only near duplicate i can find is how to deduce the theorem of addition of velocities ? and this is not an exact duplicate . the point is that relativistic velocities can not just be added . in your example let u be the plane 's speed , 0.8c , and v be your speed , 0.4c , then the speed a stationary observer sees , w , is given by : $$ w~=~\dfrac{u+v}{1+uv/c^2} $$ which is about 0.91c . the other passengers in the plane will see you running at 0.4c , but remember that the aeroplane 's time is running more slowly than a stationary observers time . so what looks like 0.4c to the passengers on the plane looks slower to me standing still on the ground .
in both of your solutions , you attempted to use newton 's 3rd law : $$\vec{f}_{1\rightarrow2}=-\vec{f}_{2\rightarrow1} . \tag{newton 's 3rd law}$$ you did this correctly in your first method ( "newton 's law method" ) but incorrectly in your second method ( "kinetic energy method" ) . in your first method , you explicitly set the magnitude of the forces equal to each other . is is the way to use newton 's 3rd law . in your second method , you assumed that " equal and opposite reaction " applies to other quantities other than force . it does not . it only applies to forces . not kinetic energy . in addition , it seems you want to assign a direction for kinetic energy . it does not have one . i also want to make the comment that the phrase for every action there is an equal and opposite reaction in newtonian mechanics is extremely easy to misinterpret . i think that is what happened in your second method . i always prefer the labeled equation above .
before introducing any additional scales , assuming that the only relevant scale is given by $m^2$ , power counting gives us the following relation for the integral in question : $$\delta m\sim\frac{am}{m^2}\int \frac{d^4k_e}{k_e^2+m^2}\sim \frac{am^3}{m^2} , $$ which is a small mass correction . next , we see what happens when we introduce a regulator in the form of a uv-cutoff . in this case , the evaluated integral contains a term of the form $$am\frac{\lambda_{uv}^2}{m^2} , $$ which is not a small correction as in the case without the regulator . therefore , power counting fails as soon as we introduce a uv-cutoff . introducing an additonal scale $\lambda$ , which we assume to be small , solves this problem . within the renormalization procedure we can absorb the $\lambda_{uv}$-dependent terms into counterterms $\delta m ( \lambda , \lambda_{uv} ) $ while still having some other terms left , i.e. something proportional to $$am\frac{\lambda^2}{m^2} , $$ which is again a small correction . we have therefore " restored " the validity of power counting .
the previous answers all restate the problem as " work is force dot/times distance " . but this is not really satisfying , because you could then ask " why is work force dot distance ? " and the mystery is the same . the only way to answer questions like this is to rely on symmetry principles , since these are more fundamental than the laws of motion . using galilean invariance , the symmetry that says that the laws of physics look the same to you on a moving train , you can explain why energy must be proportional to the mass times the velocity squared . first , you need to define kinetic energy . i will define it as follows : the kinetic energy e ( m , v ) of a ball of clay of mass m moving with velocity v is the amount of calories of heat that it makes when it smacks into a wall . this definition does not make reference to any mechanical quantity , and it can be determined using thermometers . i will show that , assuming galilean invariance , e ( v ) must be the square of the velocity . e ( m , v ) , if it is invariant , must be proportional to the mass , because you can smack two clay balls side by side and get twice the heating , so $$ e ( m , v ) = m e ( v ) $$ further , if you smack two identical clay balls of mass m moving with velocity v head-on into each other , both balls stop , by symmetry . the result is that each acts as a wall for the other , and you must get an amount of heating equal to 2m e ( v ) . but now look at this in a train which is moving along with one of the balls before the collision . in this frame of reference , the first ball starts out stopped , the second ball hits it at 2v , and the two-ball stuck system ends up moving with velocity v . the kinetic energy of the first ball is me ( 2v ) at the start , and after the collision , you have 2me ( v ) kinetic energy . but the heating is the same , so $$ me ( 2v ) = 2me ( v ) + 2me ( v ) $$ $$ e ( 2v ) = 4 e ( v ) $$ which implies that e is quadratic . noncircular force-times-distance here is the noncircular version of the force-times-distance argument that everyone seems to love so much , but is never done correctly . in order to argue that energy is quadratic in velocity , it is enough to establish two things : potential energy on the earth 's surface is linear in height objects falling on the earth 's surface have constant acceleration the result then follows . that the energy in a constant gravitational field is proportional to the height is established by statics . if you believe the law of the lever , an object will be in equilibrium with another object on a lever when the distances are inversely proportional to the masses ( there are simple geometric demonstrations of this that require nothing more than the fact that equal mass objects balance at equal center-of-mass distances ) . then if you tilt the lever a little bit , the mass-times-height gained by 1 is equal to the mass-times-height gained by the other . this allows you to lift objects and lower them with very little effort , so long as the mass-times-height added over all the objects is constant before and after . this is archimedes ' principle . another way of saying the same thing uses an elevator , consisting of two platforms connected by a chain through a pulley , so that when one goes up , the other goes down . you can lift an object up , if you lower an equal amount of mass down the same amount . you can lift two objects a certain distance in two steps , if you drop an object twice as far . this establishes that for all reversible motions of the elevator , the ones that do not require you to do any work ( in both the colloquial sense and the physics sense--- the two notions coincide here ) , the mass-times-height summed over all the objects is conserved . the " energy " can now be defined as that quantity of motion which is conserved when these objects are allowed to move with a non-infinitesimal velocity . this is feynman 's version of archimedes . so the mass-times-height is a measure of the effort required to lift something , and it is a conserved quantity in statics . this quantity should be conserved even if there is dynamics in intermediate stages . by this i mean that if you let two weights drop while suspended on a string , let them do an elastic collision , and catch the two objects when they stop moving again , you did no work . the objects should then go up to the same total mass-times-height . this is the original demonstration of the laws of elastic collisions by christian huygens , who argued that if you drop two masses on pendulums , and let them collide , their center of mass has to go up to the same height , if you catch the balls at their maximum point . from this , huygens generalized the law of conservation of potential energy implicit in archimedes to derive the law of conservation of square-velocity in elastic collisions . his principle that the center of mass cannot be raised by dynamic collisions is the first statement of conservation of energy . for completeness , the fact that an object accelerates in a constant gravitational field with uniform acceleration is a consequence of galilean invariance , and the assumption that a gravitational field is frame invariant to uniform motions up and down with a steady velocity . once you know that motion in constant gravity is constant acceleration , you know that $$ mv^2/2 + mgh = c $$ so that huygens dynamical quantity which is additively conserved along with archimedes mass times height is the velocity squared .
an orbit in three dimensions is generally specified by giving how three quantities depend on time , or by giving how two coordinates depend on the third one . you have however omittied saying how $\phi$ depends on $\theta$ ; i will thus simply assume that $\phi$ is a constant , hence $v_\phi = 0$ . in this case the total angular momentum $$ \mathbf l = m\mathbf r\times\mathbf v $$ is given by , in spherical coordinates : $$ \mathbf l = mr \hat e_r\times ( v_\theta\hat e_\theta + v_\phi\hat e_\phi ) = -mrv_\theta\hat e_\phi $$ if this turns out to be a constant , then we shall know that this is a central force . however , we do not have $v_\theta$ yet , only the modulus $v$ . for our orbit , we already know that $v_\phi = 0$ . also , $$ \dot r = 2a \cos\theta \dot \theta $$ while $$ v_\theta = r\dot\theta = 2 a \sin\theta\dot\theta\ ; . $$ putting all of this together , we find that $$ |\mathbf v| = 2 a\dot\theta $$ but we also know that $$ |\mathbf v| = \frac{k}{\sin^2\theta} $$ hence $$ \dot\theta = \frac{k}{2a}\frac{1}{\sin^2\theta} $$ which allows us to find that $$ v_\theta = r\dot\theta = 2a\sin\theta \frac{k}{2a}\frac{1}{\sin^2\theta} = \frac{k}{\sin\theta} $$ and the only non-zero component of the angular momentum becomes $$ \mathbf l = -mr v_\theta \hat e_\phi = - 2 a k m \hat e_\phi $$which remains constant along the orbit , i.e. as $\theta$ changes . the force is central . a central force has a potential that depends only on $r$ , not on $\theta$ or $\phi$ . the total energy , which is conserved , is $$ e = \frac{m}{2}|\mathbf v|^2 + v ( r ) $$ can now be rewritten as $$ e = \frac{16 a^4 k^2 m}{r^4} + v ( r ) \ ; . $$ this quantity must remain constant as $r$ varies along the orbit . the only way to obtain this is to have $$ v ( r ) = -\frac{16 a^4 k^2 m}{r^4} + v_0 $$ with $v_0$ a constant .
you are correct : both halves of the wire will still support the same weight . look at it this way : the tension in each bit of the wire is the same regardless of whether the weight is hanging directly from it or suspended by another bit of wire in the middle . to put it in terms of stress , the force remains the same and the area remains the same , so the applied stress is also constant .
yes , this is correct . the reason for photons being our only means of long-range communication lies in the fact that at very low energies ( where we live today ) the unbroken subgroup of the standard model is $$su ( 3 ) _c \times u ( 1 ) _\mathrm{em}$$ the $su ( 3 ) $ color part is not useful , as it is confining at low energies . the $u ( 1 ) $ electromagnetic part gives rise to a chargeless , massless boson that interacts with everyday matter , so it is perfect for that purpose . you are correct in assuming that gravitons would in principle work as well , but their coupling to matter is just too weak for any practical applications . same goes for graviphotons . physics beyond the standard model always has to have the standard model as a low-energy limit . therefore , even if new bosons arise , they either have to have tiny couplings ( e . g . axions ) or large masses ( e . g . gut gauge bosons ) in order to explain why we have not seen them yet . if by now we have not had a chance to see the damn thing , how would we use it to communicate ?
i have heard that a spacecraft could never exceed the speed of light because it is ( relativistic ) mass quickly approaches infinity and therefore there could never create a big enough rocket to propel it faster and faster . in fact , the spacecraft could never even reach , much less exceed the speed of light . i think that you will agree that the spacecraft , no matter what speed it may have relative to some other object , is at rest with respect to itself . think carefully about that ! the spacecraft ( or any material object ) is not moving with respect to itself . this seems so intuitive , so unquestionably true that you might think that there is no reason to even mention it . but , according to special relativity , something that moves with speed c ( the speed of light in vacuum ) relative to some other object , moves with speed c relative to any object ( c is an invariant speed ) . in other words , if it were the case that the spacecraft could obtain the speed of light , it would be moving at the speed of light with respect to itself . this is so plainly incomprehensible that it is , in fact , a relief to know that the spacecraft can never attain the speed of c . more precisely , according to the lorentz transformations , there is no frame of reference that moves with speed c relative to another frame of reference . more generally , this fact " shows up " as nonsense statements like " the mass is infinite at the speed of light " or " infinite force is required to get to the speed of light " . there is no such thing as infinite mass or infinite force which is to say , you can not get to c from here .
multiple sheets of steel work nicely to attenuate magnetic fields with higher frequencies but they are not great if you want to shield against small and constant fields . to shield the earth 's magnetic field the best material has a high permeability with almost zero hysteresis . there are some metallic glasses , such as ultraperm , vitrovac or metglas that i have tested . mu-metal is still somewhat exotic , as it needs to be shaped into it is final form and then heat treated and protected against shocks afterwards . it can be magnetized easily and then needs to be demagnetized again ( if you need very low dc fields ) .
the principle of stationary action is what you are looking for . you can construct a quantity called the lagrangian , which is kinetic energy of the system , minus the potential energy of the system , namely : $$\mathcal{l} = t-v$$ it is a function of position and velocity and for example , for a particle on a line , with a force acting on it , such that $f = -\frac{dv}{dx}$ , you have $$\mathcal{l} ( x , \dot{x} ) = \frac{1}{2}m\dot{x}^2 - v ( x ) $$ if this was not already abstract enough , the lagrangian is important , because we are interested in its integral from time $t_1$ to $t_2$ , namely : $$\mathcal{a} = \int\limits_{t_1}^{t_2} \mathcal{l} ( x , \dot{x} ) dt$$ it is called the action , and it is the " thing " nature tries to minimize or , more precisely , to make stationary ( meaning maximize or minimize ) . what does that mean ? well , it means that , for a particular system , nature chooses such a lagrangian , which will give a stationary value when integrated between two fixed points . so , as you might have guessed , the goal of the game is to find the lagrangian which will minimize ( stationarize ? ) the action . the lagrangian for a particle on a line is an extremely simple case , in general it does not have to be kinetic minus potential energy and , generally , it is also an explicit function of time , which means that some terms can depend on time not just through position and velocity depending on time . how to get the equations of motion out of a lagrangian ? you use the euler-lagrange equations : $$\frac{d}{dt} \frac{\partial \mathcal{l}}{\partial \dot{q_i}} = \frac{\partial \mathcal{l}}{\partial q_i}$$ what is that $q$ ? those are generalized coordinates , they can be cartesian coordinates but they can be all sorts of different coordinates , whatever works the best . try out the equations on my example of a particle on a line . you might be thinking , why the lagrangian , what does that have to do with anything and how do we even get them ? well . . . mostly quantum mechanics and guesswork . after all , classical mechanics is only a limit of quantum mechanics and therefore it has to obey its underlying principles . although the lagrangian is also used in quantum mechanics , there is an even more elegant concept , the hamiltonian and hamiltonian mechanics formalism , which basically sets the rules . bottom line , you can view it as this : $$\text{constructing a theory} \longleftrightarrow \text{finding the lagrangian}$$ if you want a classical intuition for why is it kinetic energy minus the potential energy , you might want to read the article " gravity , time , and lagrangians " , huggins , elisha , physics teacher , v48 n8 p512-515 nov 2010 .
if you take , for example , a perfect metal sphere then it has a work function that is the energy required to remove an electron from the metal to infinity . if you start charging the sphere by adding electrons to it then the work function decreases , and above some limiting charge the work function falls to zero . this means any more electrons you add to the sphere immediately escape again . this is an example of a phenomenon is called field emission . i have chosen the example of a metal sphere since it is nice and simple , but this will apply to any object , and it means that there is a maximum charge that can be sustained on any object regardless of how close to perfectly it has been made . if you keep the charge below the level where field emission occurs , and keep the object in a vacuum , and mask it from any light with energy of greater than the work function , and keep it at a temperature below which thermionic emission occurs , then the sphere will stay charged forever .
boltzmann 's formula states that for an isolated system in thermal equilibrium $s=k_b \log \mathrm{deg}$ , where deg is the degeneracy ( i.e. . , number of ) accessible microstates . a semi-classical argument ( not reproduced here ) shows that this degeneracy can be estimated as $m:=\frac{\omega}{h^n}$ where $\omega$ is the volume of the accessible phase space and $n$ is the number of degrees of freedom ( i.e. . , the phase space is $2n$ dimensional ) . an example that clearly illustrates the counting in a simple example is here . edit : people usually drop the $h^n$ factor ( it is needed , at the very least , on grounds of dimensionality ) . however , since it appears inside a log and leads to an additive constant , it can and is dropped . famously , boltzmann 's tomb has s = k log w written on it . so the original reference is boltzmann himself . if you want to cite someone , might as well cite the great man himself . the connection with shannon information entropy $s_{info}=-\sum_{i=1}^m p_i \log p_i$ can be seen if you realise that for $m$ accessible microstates , the maximum value of the information entropy is $s_{info}=\log m$ which is boltzmann 's answer up to the multiplicative factor of $k_b$ . ( i have nothing to say about kolmogorov entropy . ) edit2: i do not think there is a proof for boltzmann 's formula . it is usually a fundamental postulate in statisical mechanics .
in the " weak field limit " where the graviational forces are small ( such as anything in the solar system , and basically anything not right next to a black hole ) , the time dilation relative to a distant observer is : $\delta t/\delta t_0 = 1 - \phi/c^2$ . here $\delta t_0$ is the time elapsed for an observer at infinity , $\delta t$ is that time elapsed at some point in the system , and $\phi$ is the gravitational potential at that point . as @leftaroundabout stated correctly the important factor is the gravitational potential not gravitational forces . since potentials add , instead of cancelling like the forces , we get twice the time dilation with two planets than we get with one , as @jim graber said .
ignoring that the effect is very small if the height difference between both containers is small , strictly speaking the one that is lower will experience a stronger gravitational pull , and as such , the sand should accelerate faster . this can be understood in terms of newton 's inverse square law for gravity , which basically states that the force experienced within a gravitational field grows as the distance is reduced . since the lower container is closer to earth , its content will experience a greater force . as a consequence , it will drain faster .
well , there is no reason to believe in supersymmetry , beyond some theoretical niceness to it , so if they see that at the lhc , then string theory gets a big boost , as there is no way other than supersymmetry to produce fermions in string theory . the other thing that might be relevant to quantum gravity is that if there are large extra dimensions ( as in , large compared to the planck length , but smaller than detectable by things like the cavendish experiment ) . if that is the case , then the ' fundamental ' gravitational constant may be much larger than newton 's constant ( they differ by a factor of the volume of the large extra dimensions ) , and quantum gravitational effects would be accessible at the lhc .
there is a distinction between points and vectors . points are positions in space , and vectors are directions . one can easily mix up the two , because in euklidean space they look rather similar . $\theta$ in this case is a coordinate , i.e. part of the description of a point . the vector associated to that coordinate could be called $\hat{e}_\theta$ , and point in the direction , in which $\theta$ changes . so , in polar coordinates , the force depends on the position at which it is evaluated .
i think you are correct , and i think whatever source you have found quoting the factor of 2.2 is wrong . in fact a quick google found this book on aircraft design that gives the conversion factor as 0.283 $\times$ 10$^{-4}$ , which is just 1/ ( 9.81*3600 ) without any rogue factors of 2.2 .
most likely the balloon will not go anywhere as there is no net force acting on it . the pressure throughout the tank of water is the same ( it is the pressure caused by the surface tension of the water , so rather low ) . archimedes ' law says the force on the submerged object equals the weight of the displaced liquid . no gravity , no weight , no force . the answer changes the moment you add ( micro ) gravity - due to acceleration of the space ship , rotation , etc . in that case the balloon will move in the direction of the acceleration ( opposite the direction of the " apparent gravity" )
given a spring with spring constant $k$ whose extension is in the direction $x$ , the magnitude of the force that the spring exerts is given by $$ |f| = k|l-l| $$ where $l$ is its length and $l$ is its equilibrium length . now , imagine that the two masses are at positions $x_1$ and $x_2$ with $x_2&gt ; x_1$ , then the length of the spring is given by $l = x_2 - x_1$ so that the magnitude of the force exerted by the spring is given by $$ |f| = k| ( x_2-x_1 ) - l| $$ now , if $x_2 - x_1&gt ; l$ , then the spring is stretched in which case the mass on the right feels a force of this magnitude to the left , and the mass on the left feels a force of the same mangitude to the right ; \begin{align} f_1 = k ( x_2-x_1 - l ) \\ f_2 = -k ( x_2 - x_1 - l ) \end{align} this results in the following two equations of motion \begin{align} m\ddot x_1 = -k ( x_1-x_2 + l ) \\ m \ddot x_2 = -k ( x_2 - x_1 - l ) \end{align} essentially , the difference in the sign of $l$ can be attributed to newton 's third law ; the forces on each mass must be equal and magnitude , but opposite in direction . if friction were included on the surface , say for the sake of concreteness that the coefficient of kinetic friction is $\mu_k$ , then each object experiences a force equal in magnitude to $\mu_k m_1g$ for mass 1 and $\mu_k m_2 g$ for mass $2$ . the sign of the friction force term must be chosen so as to always give a friction force that is opposite the direction of motion . one way to do this is to multiply the force 's magntiude by $-\dot x/|\dot x|$ which is $-1$ when the object is moving to the right , and $+1$ when the object is moving to the left . thus , with friction the equations of motion can be written as \begin{align} m\ddot x_1 = -k ( x_1-x_2 + l ) - \mu_k m_1g\frac{\dot x_1}{|\dot x_1|} \\ m \ddot x_2 = -k ( x_2 - x_1 - l ) - \mu_k m_2g\frac{\dot x_2}{|\dot x_2|} \end{align} as you can tell , these differential equations are considerably harder to solve in general .
i think the pithy answer is that our eyes adapted to see the subset of the electromagnetic spectrum where air has no absorption peaks . if we saw in different frequency ranges , then air would scatter the light we saw , and our eyes would be less useful .
in basic electrodynamics it is stated that light cannot " get inside " a metal . therefore one just thinks of piece of metal as of boundary conditions for any light-related problems . i do not see any difference with that approach when it comes to superconductors . you can just think of superconductor as of ordinary metal with absolutiely the same conclusion about reflection of light off it is surface . on the other hand it is obvious that some atoms or the metal must somehow interact with the fields . when we talk about electrodynamics of continuous media we deal with scales that are much larger than atomic scale . the statement about non-penetration of magnetic field inside of superconductor is valid for large scales as well , while it actually gets inside the media to the depth around $10^{-5}$ centimetres . in comparison to inter-atomic scales this is quite large . the same holds for " light not getting inside metal " . when it comes to x-rays , i do not think that one can use classical electrodynamics at all , because wavelengths are starting to be comparable to the atomic sizes ( 1 nm for soft x-ray and 0.01 for hard x-ray against 0.05nm for bohr radius ) .
the easiest thing for this exercise is to use levi-civita symbol for the vector product : $$\vec{a} \times \vec{b} = a_i b_j e_k \varepsilon_{ijk} , $$ where i denote by $e_i$ the canonical basis of $\mathbb{r}^3$ . using this notation , we have : $$ [ l_j , p_i ] = [ r_k p_l \varepsilon_{klj} , p_i ] = i \hbar p_l \varepsilon_{ilj} . $$ and $$ [ l^2 , \vec{p} ] =e_i [ l_j l_j , p_i ] =e_i ( [ l_j , p_i ] l_j +l_j [ l_j , p_i ] ) =i\hbar ( \vec{p}\times\vec{l}-\vec{l}\times\vec{p} ) . $$ be careful to use correctly the anti-symmetry of the levi-civita symbol in the last step .
unfortunately i cannot comment due to insufficient reputation , so here a comment on the question . there are three cases : $\frac{1}{2}mv_a^2&gt ; 2mgr$ in this case the pearl has a velocity $v&gt ; 0$ in the top point and will continue its movement . $\frac{1}{2}mv_a^2&lt ; 2mgr$ in this case the pearl will not reach the top and will oscillate around point $a$ . $\frac{1}{2}mv_a^2=2mgr$ in this case the pearl will indeed reach the top with zero velocity and remain there forever in an unstable equilibrium . however , if you write a simulation , many things can happen that prevent this case to occur . a ) the rounding errors cause the velocity at the top to be non-equal zero . b ) the positions of the pearl are calculated in small steps ( that is how numerical calculations work ) and the top is not amongst them . then the pearl springs from a point right before the top to a point right after the top . c ) the initial conditions are given with insufficient accuracy ( again rounding problem ) . in short , the problem is not a physical but a computational one . so , as mentioned by awesome , we need the code to give you the reason for the inaccuracy .
as far as i remember viscosity of the gases , unlike liquids , increases with increasing temperature . so in order to decrease viscosity , you would have to cool the air ( which is already cold at some 10 km ) . when it comes to it is effect on planes , viscosity is responsible for creating pressure gradient between top and bottom side of an airfoil or wing . that pressure gradient creates lift . on the other hand viscosity affects drag - generally the more viscous fluid , the greater drag . there is no easy answer to your question , but if you would like to improve flight conditions , i think it would be easier to start with the plane construction and materials , instead of the air viscosity .
the website is clearly supported by lots of money which does not guarantee that it reflects the most accurate scientific information . the very page you quoted says under the picture : the material is graphene , also known as graphite . . . well , no . graphene is not the same thing as graphite . graphite is a 3-dimensional material used to produce pencils - and graphene is just one layer of graphite . so you can not expect everything to be " quite right " . the honeycomb lattice is useful to get a " denser packing " which , under some circumstances , could mean a higher density of the molecules that may transform light to electromagnetic energy . however , it is very subtle to compare " different lattices " of the " same material " because the same material usually can not form " all lattices " . so i suspect that the statement is just phenomenological in character - some of the most efficient solar cells that people have found had a honeycomb lattice but as far as i know , there is no " universal proof " that it has to be so . the situation would be much less ambiguous if one asked a well-defined geometric question - what is the best close packing of spheres , for example . however , the efficiency of solar cells also depends on more complicated physical properties of the material than just its lattice , so one should not expect universal yet simple answers .
this is really just an expansion of graham 's answer . it is a commonly made mistake that gravity , and therefore a black hole , is caused by matter . in fact the spacetime curvature is related to a quantity called the stress-energy tensor . this is usually represented by a matrix with ten independant values in it ( it is a 4x4 matrix but it is symmetric so six of the elements in it are duplicated ) . only one of the elements in the matrix , $t_{00}$ , depends directly on the mass , and actually that element gives the energy density , where mass is counted as energy using einstein 's equation $e = mc^2$ . so photons affect spacetime curvature because they contribute to the energy density even though they have no mass . actually photons contribute to other elements of the matrix as well because they have a non-zero momentum and this too affects the spacetime curvature . re your question : when photons are taken irreversibly into a black hole does the mass of the bh increase ? yes , the mass of the black hole will increase by the photon energy divided by $c^2$ . re your comment to graham 's question , yes , provided you add more energy than the black hole is radiating you will maintain or increase the black hole . you could add the energy using lots of low energy photons or a few high energy photons . it is the total energy added that matters .
the thing is , the moon 's orbital plane is slightly tilted ( about 5$^\circ$ ) with respect to earth 's which means from the earth 's perspective that the moon 's motion oscillates around the sun 's trajectory . on most new moons , then , the moon is either north or south of the sun and we do not see an eclipse . for eclipses to happen , new and full moons must occur when the moon crosses the ecliptic . equivalently , the intersection of the moon and earth orbital planes have to align with both the sun-earth and the earth-moon directions . no wonder they do not happen that often ! edit : i do not know any good 3d simulator , but i found my solar system ( which i got from mike 's answer to this question ) good fun .
what you probably want to do is deconvolution . for that you need the impulse response function . what you have is the response to a step function . you need to deconvolve the respnse function you have with a step function to get the impulse response function of your instrument and then deconvolve your data with that . one problem with deconvolution is that it can amplify noise , but i do not know if that will be a problem in your case . edit : details on request . let $t ( t ) $ be the true temperature and $m ( t ) $ your measured temperature . they are related like this : $$ m ( t ) =\int t ( \tau ) f ( t-\tau ) d\tau $$ where $f ( t ) $ is the impulse response function . what you have is $f ( t ) $ given by : $$ f ( t ) =\int h ( \tau ) f ( t-\tau ) d\tau $$ where $h ( t ) $ is a heaviside step function . fortunately you do not need to deconvolute $f ( t ) $ as i said above . simply differentiating $f ( t ) $ should do : $$ f ( t ) =f' ( t ) $$ now , you have $m ( t ) $ sampled in a number of points $t_i$ , let 's call the the samples $m_i=m ( t_i ) $ . similarly , the true values at another set of time points $\tau_j$ can be called $t_j=t ( \tau_j ) $ . the integral can now be approximated with a sum : $$ m_i=\sum_j t_jf ( t_i-\tau_j ) \delta\tau_j $$ where $\delta\tau_j$ is the distances between the $\tau_j$:s . this is now a linear equation system : $$ m_i=\sum_j t_ja_{ij} $$ with the coefficients given by : $$ a_{ij}=f ( t_i-\tau_j ) \delta\tau_j $$ solve it to for $t_j$ to get the true temperatures . the trickier parts might be to make sure $f ( t ) $ is correctly normalized and choosing your set of $\tau_j$ . it should probably be similar to the set of $t_i$ , but maybe they should be a bit sparser to make it more robust . this is the basics . you might have do some tinkering . also , there are probably more advanced algorithms for deconvolution that are less noise sensitive .
assuming the wooden ball and the steel ball were both 100% elastic , the only difference between the earth and the moon would be the time it takes . if you used a slow-motion camera on the earth , it could look just like it does on the moon . ( btw , " hang time " is not a physical quantity - it is just something you perceive : )
you have basically indicated the two natural ways to solve this problem : integrate the electric field due to small segments along the filament to find the total electric field at a specified point . use gauss 's law with an appropriate gaussian surface . for method 1 , you are basically almost there given your manipulations ; you simply need to integrate $$ de = k_e\frac{\lambda\ , d\ell}{r^2} $$ but what are the limits of integration if we do not know the length of the filament ? well , the problem states that the wire is " long , " which is just physics-speak for infinitely long . so you need to integrate from $-\infty$ to $\infty$ . just make sure to be careful that $r$ denotes the distance from the point where you want to find the field , to the point where $dq$ is located on the filament , so you need to use the pythagorean theorem to related $r$ to the perpendicular distance to the filament , say $d$ , and the length along the filament , say $\ell$ . for method 2 , you simply pick a cylindrical gaussian cylinder whose axis of symmetry is along the filament . i will leave out the details for this method and let you try it . let me know if you need more help , but this is something you can easily find by googling . hope that helps ! physics rocks .
a state like $ \frac{1}{\sqrt{2}} ( a^\dagger_+ ( \vec p ) a^\dagger_+ ( -\vec p ) + a^\dagger_- ( \vec p ) a^\dagger_- ( -\vec p ) ) |0\rangle$ would be an example . it is both entangled in spin and entangled in momenta .
to do this , the man needs to build a particle accelerator and measure kaon decays , or some other process involving higher quark flavors . everything else is cp invariant , so he would not know for sure .
the formula $f=g \frac{m_1 \cdot m_2}{r^2}$ is valid only for point masses . however , it can be applied to non-point masses if its spherically symmetric . enter shell theorem : 1 . a spherically symmetric body affects external objects gravitationally as though all of its mass were concentrated at a point at its centre . so , when a spherically symmetric massive star attracts an object at its surface , its like its actually attracting that object from a distance equal to its radius . 2 . if the body is a spherically symmetric shell ( i.e. . , a hollow ball ) , no net gravitational force is exerted by the shell on any object inside , regardless of the object 's location within the shell . so , if you put something near center of that star , it can still escape because it is experiencing force due to only mass below it . but , when that star collapses to a smaller volume , force due to whole mass on surface increases ( because its inversely proportional to $r$ ) which makes escaping tougher ( required escape velocity increases ) . when this radius is decreased to schwarzschild radius , the escape velocity exceeeds $c$ .
yes , there are rules that depend on the quantum numbers carried by the particles under question and the energy available for the interaction . in general we label as annihilation when particle meets antiparticle because all the characterising quantum numbers are equal and opposite in sign and add and become 0 , allowing for the decay into two photons , two because you need momentum conservation . a positron meeting a proton will be repulsed by the electromagnetic interaction , unless it has very high energy and can interact with the quarks inside the proton , according to the rules of the standard model interactions . when a neutron meets an antiproton the only quantum number that is not equal and opposite is the charge , so we cannot have annihilation to just photons , but the constituent antiquarks of the antiproton will annihilate with some of the quarks in the neutron there will no longer be any baryons , just mesons and photons , and all these interactions are given by the rules and crossections of the standard model .
the rumors that you could make black holes in particle accelerators are due to an obviously impossible model due to arkani-hamed , dimopoulos , and dvali , also supported by lisa randall and raman sundrum , that became very popular and highly cited due to an unfortunate spate of wishful thinking and delusional model building among phenomenologically minded string theorists in the last decade . this idea is called " large extra dimensions " , and the rise and fall of large-extra dimensions was the physics iraqi wmd 's , it was indefensible groupthink which biased a political community . a political community is what string theory had been forced to become , just to scrape by through the dark ages of the 1980s and 1990s . during these dark ages , despite a large amount of resistance , string theory underwent a scientific revolution with hardly any precedent , a revolution so extensive , it can only be compared to galileo and copernicus . the conceptual framework of strings was extended to more general structures , the d-branes of polchinski and collaborators , and orbifolds of dixon and others ( all this work was intensely communal and collaborative , and it is difficult to pick out individuals--- the names are just handles into the literature ) . these structures acquired a full interpretation with susskind 's explicit recognition of the central place that t'hooft 's hologrphic principle plays in string theory , and the holographic principle itself revived , clarified , and modernized the chew/mandelstam s-matrix program which flourished in the 1960s , but was lost in the dark-ages . these two ideas were related , in that the holographic principle explained the mysterious structure of the world-volume theories , while the d-branes provided new examples of objects with world-volume theories to describe . within a few short years , m-theory , matrix theory , and ads/cft , solved the central unsolved problem of physics , which , by all rights , we should never have been able to solve : they gave a complete consistent non-perturbative description of quantum mechanical model universes with general relativistic gravity . it is no longer correct to say that we do not know how to unify quantum mechanics and general relativity--- we know for sure for certain ads spaces , for 11-dimensional matrix-theory in at least some domains , and many other cases where the universe is asymptotically cold ( meaning not surrounded by a thermal horizon , like desitter space is ) . unfortunately our own universe is not in this class , at least not yet , it looks like it is headed towards a desitter phase , and we know it was desitter in the past . anyway , the solution to the cold universe problem of quantum gravity solve the in-principle problem of a theory of everything correctly and persuasively . further , the consistency relations were obviously so stringent , that only 1990s string theory could possibly obey them--- it required that different theories , each formulated in a different number of world-volume dimensions ranging from 0 to 4 ( and more , if you allow little-strings ) , will each describe the same emergent local physics in a different number of spatial dimensions ! this idea is commonplace today , but one must always remember that it is , on its face , impossible . while it is not a mathematical proof , there is no way in heck that anything other than strings is going to do this . so the problem of the theory of everything , was , to a large extent , solved in the 1990s , with the modern formulation of string theory . you would think this would lead to a revolution in particle physics . unfortunately , these ideas do not directly connect with particles--- they are theoretical gravitational constructions that are inspired by and linked more closely to gravitational physics . but the mathematical structures were in a large part those of particle physics--- the world-volume theories of branes resemble the gauge theories of particle physics . so there was this tremendous pressure to make a direct link between these developments and particle physics , and there were very few direct clues to how to do this , because the scale of quantum gravity is so remote . when the planck scale is big , as it is in our universe , there are strong constraints to stuffing crap into a string vacuum , because there is not a lot of structure you can support in a tiny compactified space . but people were considering crazy constructions with brane-stacks all the time , and people wanted a way to turn this into particle physics , and they could not , because string theory is so tightly constrained . in this political situation , add proposed their model . their idea was to throw away the requirement that the planck energy is big , and instead , make the extra dimensions big . this immediately frees you up to do whatever the hell you want , because you can stuff anything into large extra dimensions , so it allowed all the string folks to start making particle models using the new tools . this was a nice exercise in using the new tools , so you get a billion citations . but it is crap as physics , because there are good reasons to believe the planck scale is big . these reasons are all based on the fact that the standard model is renormalizable , and the standard model works . if the planck scale is small , there is no reason that the effective theory should be so perfectly close to renormalizable . so each not-observed non-renormalizable correction is a strike against a large-extra dimension scenario . here are the ones everyone thought of immediately when add came out : proton decay : the proton is only stabilized by an accidental symmetry . so if the planck scale is about 1 tev , you would expect instantaneous proton decay from non-renormalizable interactions . neutrino masses : the neutrino mass scale is . 01 ev , which is correctly predicted from the non-renormalizable suppression of the dimension 5 two-higgs two-neutrino scattering . precision electroweak corrections : you would wreck the muon magnetic moment at the third decimal place , and the electrons at the sixth . these are measured much better than that . coupling unification : the strong and su ( 2 ) and u ( 1 ) couplings approximately unify at the gut scale . this fails when the running changes , as it must in extra dimension theories . but then , instead of the theory disappearing , something indefensible happened . people started writing bogus theories which claimed to explain these phenomena , and other people took them seriously . i was flabbergasted that otherwise intelligent people were taking this obvious garbage seriously . further , you could not get a job unless you were studying this garbage , so basically only clowns and frauds were getting positions in the us . this is depressing to see , and it mades one want to be a biologist . the bogus papers claimed the following : proton decay : this can be suppressed by putting quarks and leptons on different branes which are far apart in the large extra dimensions . this does not work because you have anomaly and su ( 2 ) instantons linking quarks and leptons , so you need su ( 2 ) /u ( 1 ) fields delocalized and running crazy to even pretend to make this work . neutrino masses : this can be suppressed if all the masses come from a right handed partner , which is delocalized in the extra dimensions . of course , this does not work because there is no reason to postulate a right-handed partner when you can just put in a dimension 5 nonrenormlizable term by hand without a partner . in order to actually suppress neutrino masses for real , you have to do kakushadze style fantasy models where you suppress 10 orders by hand using discrete symmetries , and only these 10 orders , because you need to keep the little bit of observed mass . the neutrino mass by itself is enough to kill this theory , and any other where the new-physics scale is not $10^8$ gev or higher , and realistically , what everyone always thought it was , around $10^{16}$gev . running couplings : people made up just-so stories about how the running can speed up and still make the couplings meet . without reading these papers ( i did not ) you can just see that this is crap--- the coupling meeting is based on logarithmic corrections which are sensitive to the details of stuff well past a tev . precision electroweak : particles past 1tev generally make tiny negligible corrections to the magnetic moment , so people just assumed that quantum gravity at a tev would do the same . this is complete nonsense , because these particles have extra constraints from their own renormalizable interactions , and their loop structure can sometimes force them to give small corrections , just because of some accidental renormalization property . a generic quantum gravity regime will generate a pauli-term for the electron and muon which is only suppressed by the ratio of their mass to 1 tev , which is not enough/ there are other obvious strikes against this theory from other nonrenormalizable corrections , these are not in the literature , i thought of them while writing the negative parts of the wikipedia page on this : strong cp : each extra non-renormalizable pure-strong interaction term will break strong cp in a different way , which can not be fixed by axions . the constraints on strong cp violation are insane , so you need a to fix this . w and z corrections : these would get nonrenormalizable condensate interactions ( they are close to the planck scale ) wrecking the standard model mass/coupling relation which is measured to at least 5 orders of precision . quark-lepton direct coupling : you could make leptons turn into quarks via a direct contact interation which is dimension 6 . besides not being observed , if i remember what i worked out years ago correctly , this would give neutrinos a pion-condensate mass , separate from a higgs mechanism mass , which would be many orders of magnitude bigger than the observed neutrino mass . this stuff , although it might seem convoluted to an outsider , is so elementary to a particle physics that i did not think it even merited a publication . this stuff is what you get on referee reports . some people tried to publish comments like this about large extra dimensions , but nobody listened to them , and when they failed to penetrate phenomenological string theorists ' cloud of delusion , this also served as a warning to others not to repeat the criticism , but to go along with the fraud . needless to say , this must never happen again . i have to point out that despite this sorry episode , the physics literature is still by far the most intellectually honest literature in academia , it is still a beacon of honesty to fields like linguistics and philosophy , where wishful thinking and fraudulent research is the rule , not the exception ( this was especially true during those recent dark ages ) .
the biot savart law is $${\bf b} = \frac{\mu_0}{4\pi} \oint \frac{ i\ , d{\bf l} \times {\bf r}}{|{\bf r}|^3}$$ in this case $d{\bf l} \times {\bf r} = dl\ , |r|$ directed along the loop axis and integrating around the closed loop leads to a b-field magnitude $ b = \mu_0\ , i/2r$ as you suggest . however , i think there is a problem with your application of ampere 's law . this is that $$ \oint {\bf b} \cdot d{\bf l} = \mu_0 i\ , , $$ where i is the current enclosed by the closed loop around which you do the line integral on the lhs . usually , to apply ampere 's law , you choose a loop to integrate over that has either a constant b-field , and/or with a direction that is either parallel or perpendicular to $d{\bf l}$ ( so that the scalar product and/or line integral are much simplified ) . what loop have you done your integral around ? is the b-field constant along this path ? i do not think so . . .
it is certainly possible for ice to sink in water under the right conditions . the diagram this section of wikipedia 's ice page will show you the conditions under which the various types of ice can form . most of the " exotic " ones such as xii will form only at pressures greater than around 200mpa . these high-pressure forms are all denser than water , so they would sink to the bottom . this means that they would displace less liquid than their weight , so melting them would result in an increase of the surface level . earth 's oceans are not deep enough for these types of ice to form . the pressure at the bottom of the mariana trench is about 100mpa . since pressure increases linearly with depth , the oceans would need to be around twice as deep in order for this to happen . however , there is a solid form of water that does sink in earth 's oceans . this is methane clathrate , which is a crystalline solid consisting of methane molecules surrounded by water ones . it can form at pressures found in the ocean , and there is rather a lot of it in sediments below the sea floor . although methane clathrates are denser than water , i do not know what the effect on the sea level would be if they melted . this is because when it melts the methane is released as gas and bubbles to the surface , and i do not know whether the volume increase due to the melting is bigger or smaller than the volume decrease due to the methane escaping into the atmosphere . ( if large quantities of clathrate did melt then the direct effect on sea level would be the least of our worries , because methane is a very powerful greenhouse gas and there is an awful lot more of it locked up in clathrates than we currently release industrially . this could actually happen . )
to recover einstein 's equations ( sourceless ) in string theory , start with the following world sheet theory ( polchinski vol 1 eq 3.7.2 ) : $$ s = \frac{1}{4\pi \alpha'} \int_m d^2\sigma\ , g^{1/2} g^{ab}g_{\mu\nu} ( x ) \partial_ax^\mu \partial_bx^\nu $$ where $g$ is the worldsheet metric , $g$ is the spacetime metric , and $x$ are the string embedding coordinates . this is an action for strings moving in a curved spacetime . this theory is classically scale-invariant , but after quantization there is a weyl anomaly measured by the non-vanishing of the beta functional . in fact , one can show that to order $\alpha'$ , one has $$ \beta^g_{\mu\nu} = \alpha ' r^g_{\mu\nu} $$ where $r^g$ is the spacetime ricci tensor . notice that now , if we enforce scale-invariance at the qauntum level , then the beta function must vanish , and we reproduce the vacuum einstein equations ; $$ r_{\mu\nu} = 0 $$ so in summary , the einstein equations can be recovered in string theory by enforcing scale-invariance of a worldsheet theory at the quantum level !
stop and actually think about it . small-value inductors and capacitors are possible , but you also need extremely small size else the lumped system approximation your equation relies on becomes invalid . let 's say the wavelength of the light to produce is 700 nm . a distance of half that ( 350 nm ) will cause a inversion ( 180&deg ; phase change ) . generally you want to keep the maximum dimension to 1/10 the wavelength to consider it a lumped system without having to worry about that further . your inductor and capacitor therefore have to all fit roughly within a 70 nm dimension . with everything that close to everything else , there will be a lot of parasitic capacitance . getting a low enough capacitance and still have the deliberate capacitance accross the ends of the inductor dominate over all the distributed stray capacitance will not be easy . even if you could do that , you still have to drive it with a 430 thz signal somehow . where are you going to get that from ?
in the following , i have replaced " photon " going through slits with " electron " , and the measuring device with a " photon " . this is the traditional heisenberg setup . the interaction of the photon and the electron entangles the photon and the electron , so that the electron cannot make the interference pattern . it has nothing to do with looking at the screen . the looking at the screen only serves to let you know what the result of the observation was . the entanglement of measuring devices and particles is what causes collapse relative to a detector state , independent of interpretation . the only question is when the collapse turns into a definite outcome , so that the detector state becomes a definite thing , as opposed to an indefinite superposition .
i am not sure what all you have read on them , but i will try to clarify at least a few things . i would certainly disagree with several of your assertions . for starters , you say " . . . they do not produce anything you could feasibly use as a source of material for nuclear weapons . " thorium reactors use thorium as a fertile fuel that transmutes into fissile u233 . while the spent fuel does not contain the same ratios of elements as a uranium fuel cycle , it does indeed contain bomb worthy isotopes as well as some longer lived fission and daughter products . in fact , the thorium cycle was used to produce some of the fuel for operation teapot in 1955 . you say " . . . they are far less prone to catastrophic failure . . . " while it may be the case that thorium reactors have traditionally had fewer catastrophic failures than uranium reactors , it is also true that the statistics are too small to make reasonable conclusions as to the reliability of such systems . to my knowledge , no commercial reactors use a thorium fuel cycle . in other words , all of the thorium reactors are one-off , uniquely designed pieces of equipment with well trained and knowledgable working staff . there are roughtly 435 commercial nuclear plants in operation with another 63 under construction . there have been on the order of 20 major nuclear accidents over the years . there are only 15 thorium reactors . statisitcally , thorium reactors might have a worse accident rate . there is certainly ongoing research into commercial applications of a thorium fuel cycle . interestingly , as that article suggests , a thorium cycle requires another isotope to get the reaction going so there will always be a need for some uranium cycle reactors . like p3trus said , even outside of india ( where the thorium reserves provide good economic incentive ) there are people considering thorium . ultimately , the preference for a uranium fuel cycle is a pragmatic one . the nuclear industry has a great deal of experience with uranium . it is true that there is more thorium than uranium , but uranium is hardly rare . it is sufficiently common , in fact , that there are not even very many estimates of the size of the reserves . with respect to public opinion , thorium does not offer a tangible difference to uranium other than a change of name . as long as public opinion is against nuclear , that will include thorium . if they turn to support nuclear , the economics still point to uranium .
from gauss 's law and using the conservative field , we have $$ \nabla^2u=-\frac\rho{\varepsilon_0} $$ in spherical coordinates and assuming isotropy in $\theta$ and $\phi$ directions , the above becomes , $$ \frac{1}{r^2}\frac{\partial}{\partial r}\left ( r^2\frac{\partial u}{\partial r}\right ) =-\frac{\rho}{\varepsilon_0}\tag{1} $$ since we are dealing with a point-charge , the charge density can be expressed in terms of the dirac delta function : $$ \rho ( \mathbf r ) =q\delta ( \mathbf r-\mathbf r' ) =q\delta ( \mathbf r ) \tag{2} $$ where we assume that $\mathbf r'=o$ ( the origin ) and ignore it in the delta function . this can be done because the volume integral of $\rho$ should give the total charge $q$: $$ \int\rho\ , dv=\int q\delta ( \mathbf r ) \ , dv=q\int\delta ( \mathbf r ) \ , dv=q $$ where use used the property $\int\delta ( x-x' ) \ , dx=1$ when $x'$ is in the integral range . inserting ( 2 ) into ( 1 ) , we get $$ \frac{1}{r^2}\frac{\partial}{\partial r}\left ( r^2\frac{\partial u}{\partial r}\right ) =-\frac{q}{\varepsilon_0}\delta ( \mathbf r ) $$
maybe this will help : q=c*v a key thing to bear in mind when using this formula is that q refers to the charge separation of the system . in other words , when i say that a capacitor is charged up to some level q , i mean that i have put +q on one part of the capacitor , -q on the other . the capacitor as a whole remains neutral ! ! i emphasize this now because i often find students are somewhat confused by this point , particularly when we start thinking about circuits that have capacitors in them . bold mine . it then depends on what is the +q or the -q in our circuit with respect to the earth , it is not a fixed nor a conserved amount . it is not something that can be saturated . read 6.2.1 for the earth as a spherical capacitor . the capacitance of the earth is so large that one can effectively remove charge or add charge without changing measurably the potential . now if one starts thinking of the basic carriers of charge , electrons and ions , in total the earth is neutral , and it is potential differences that define the separation and motion of charges , and conductivity on how fast charges are diffused .
wow , this is a very detailed question . thanks for your effort . lets ignore diffraction effects , which will scatter some small amount of extra power out of the laser beam . the loss at elements 1a to 1d will not simply sum up . this is because the power will only be lost at one of the mirrors , and will not be there to be lost at the other mirrors . so , to calculate the power lost by clipping in the whole path , you simply need to calculate the power lost at the mirror/glass where the beam is the largest relative to the mirror/glass . a more accurate calculation of the loss which includes diffraction effects would probably require a computer simulation . there is another source of loss in the system though . if you use a bare ( no optical coating ) piece of glass at elements 1a - 1c , then you will lose about 3.5% per interface . with 6 interfaces ( front and back of elements 1a - 1c ) you will transmit $$ t= ( 1-0.035 ) ^6=0.81\qquad\rightarrow\qquad l=19 \% . $$ so you will lose almost 20% of the light to reflectance at the glass interfaces . if you pay for an anti-reflective optical coating you can get reflectivities as a low as 0.1% pretty cheaply in which case you will lose almost nothing to this source of loss .
let 's discuss the harmonic oscillator first . it is actually a very special system ( one and only of its kind in whole qm ) , itself being already second quantized in a sense ( this point will be elucidated later ) . first , a general talk about ho ( skip this paragraph if you already know them inside-out ) . it is possible to express its hamiltonian as $h = \hbar \omega ( n + 1/2 ) $ where $n = a^{\dagger} a$ and $a$ is a linear combination of momentum and position operator ) . by using the commutation relations $ [ a , a^{\dagger} ] = 1$ one obtains basis $\{ \left| n \right &gt ; $ | $n \in {\mathbb n} \}$ with $n \left | n \right &gt ; = n$ . so we obtain a convenient interpretation that this basis is in fact the number of particles in the system , each carrying energy $\hbar \omega$ and that the vacuum $\left | 0 \right &gt ; $ has energy $\hbar \omega \over 2$ . now , the above construction was actually the same as yours for $x = \{0\}$ . fock 's construction ( also known as second quantization ) can be understood as introducing particles , $s^i$ corresponding to $i$ particles ( so ho is a second quantization of a particle with one degree of freedom ) . in any case , we obtain position-dependent operators $a ( x ) , a^{\dagger} ( x ) , n ( x ) $ and $h ( x ) $ which are for every $x \in x$ isomorphic to ho operators discussed previously and also obtain base $\left | n ( x ) \right &gt ; $ ( though i am actually not sure this is base in the strict sense of the word ; these affairs are not discussed much in field theory by physicists ) . the total hamiltonian $h$ will then be an integral $h = \int h ( x ) dx$ . the generic state in this system looks like a bunch of particles scattered all over and this is in fact particle description of a free bosonic field .
classically we would add the speeds to get $1.8c$ , which is obviously not allowed . in relativity you simply use the relativistic velocity addition formula : $$v = \frac{u+v}{1+uv/c^2}$$ where $u$ and $v$ are the velocities of the particles as seen from some reference frame , and $v$ is the velocity of one particle in the rest frame of the other , i.e. , the relative velocity when we consider one of the particles to be stationary . plugging in $u = v = 0.9c$ , we get $v = \frac{180}{181}c \approx 0.9945c$ . edit : as pointed out by alfred centauri , the above explanation is perhaps too simplistic . a more rigorous version would be the following : let 's take our particles to be moving in the $x$ direction , with particle 1 moving in the positive direction and particle 2 moving in the negative direction . as seen by particle 1 , our velocity is $-0.9c$ ( note the sign ! ) . as seen by us ( that is , the lab frame ) , particle 2 is moving with a velocity equal to $-0.9c$ . the velocity addition formula tells us how to find the velocity of particle 2 with respect to particle 1 . if $v$ is this velocity that we are trying to find , $u$ is our velocity with respect to particle 1 and $v$ is particle 2 's velocity with respect to us , then : $$v = \frac{u+v}{1+uv/c^2} \approx -0.9945c$$ this time we get the correct sign , since , relative to particle 1 , particle 2 is moving in the negative $x$ direction .
i have not read them , but this , this , this and this thread ( i thank a diligent qmechanic ) are related and clear up the but why -questions you might have . the transformation of the quantities in electrodynamics w.r.t. boosts are where $\gamma ( v ) $ and the derivation of the transformation is presented on this wikipedia page and is most transperent in a space-time geometrical picture , see for example here . namely , the electromagnetic field strenth tensor $f_{\mu\nu}$ incorporates both electric and magnetic field $e , b$ and the transformation is the canonical one of a tensor and therefore not as all over the place as the six lines posted above . in the non-relativistic limit $v&lt ; c$ , i.e. when physical boosts are not associated with lorentz transformations , you have for the traditional force law , the first formula confirms the prediction that the new $e$ magnitude is $vb$ . also , beware and always write down the full lorentz law when doing transformations . lastly , i am not sure if special relativity predicts equivalent electric force will acting upon charge instead is the right formulation you should use , because while the relation is convincingly natural in a special relativistic formulation , the statement itself is more a consistency requirement for the theory of electrodynamics . i would almost say the argument goes in the other direction : the terrible transformation law of $e$ and $b$ w.r.t. galilean transformations was known before 1905 and upgrading the status of the maxwell equations to be form invariant when translating between inertial frames suggests that the lorentz transformation ( and then special relativity as a whole ) is physically sensible .
in my humble experience , solving griffiths problems gets you good at solving griffiths problems , but not much more . typically , they have already done/seen the math required , so i would only work on the calculus part if they are really struggling . studying theoretical results does not seem a great idea either ; they will pick that up along the way or in class . what i prefer is to study more ' extended ' problems , i.e. a real life example for the ( classical ) literature , guide them through the development and finish with a comparison to experimental results . this way , they plough through the maths but also need to do a minimum of interpreting . however , if your goal is just to get them great at working griffiths-like problems , this might be overkill .
your answer holds in the case that $\frac{dv}{dt} = 0$ . the more general situation ( the one your professor seems to be working with ) , however , considers that $v$ may be changing as a function of time , and therefore changing as a function of $m$ . try recalculating $\frac{dv}{dm} = \frac{d}{dm} \frac{v ( m ) }{m}$ using the product/quotient rules , accounting for the fact that $v$ can be dependent on $m$ . this method should yield your professor 's result . feel free to ask in the comments if you need more help . ( by the way , a quicker way to approach the problem would be to simply calculate $\frac{d}{dt} \frac{v ( t ) }{m ( t ) }$ with the quotient rule of calculus rather than decomposing the original derivative with the chain rule as you did . nevertheless , your technique is valid if you find it more intuitive . )
the real issue is that the cup was not really full so that adding anything more would make it spill . you can clearly see the the level slowly growing above the top of the cup , as would be expected due to surface tension . eventually another coin finally exceeded the limit , and a little water spilled . there is really nothing extraordinary going on here . they could have just as well added some more water as individual drops and gotten the same effect . you say that the cup was filled so that another drop of water would cause it to spill , but that was never stated nor demonstrated in the video . once difference between adding a water drop and a coin is that , if done carefully , the coin will cause less of a wave . in fact the coin that caused the spill seemed to be added deliberately to cause a wave , like the person was tired of adding coins and wanted to see the spill already . a coin can be inserted into the water edge-on , and cause a small wave when doing so . a water drop will cause more of a wave because the surface tension of the water in the glass and that of the drop merge when they touch , which causes a sort of snap action that cause a wave . this wave will more likely stress the miniscus at the edge to the breaking point than the tiny rise in water level due to the drop alone .
definitely ! consider ohm 's law , $\vec j=\sigma\vec{e}$ , where $\vec j$ is the current density , $\sigma$ is conductivity ( the inverse of resistivity ) and $\vec e$ is the electric field . anisotropic conductivity corresponds to turning $\sigma$ into a tensor-valued quantity , a $3\times3$ matrix . ohm 's law is then given by $$j_i=\sigma_{ij}e_j , $$ where $\sigma_{ij}$ is now the conductivity matrix , where different components $i , j\in x , y , z$ may have different entries , depending on the material in question . note that restivity is now also a matrix , which can be acquired by inverting conductivity , i.e. $\rho_{ij}=\sigma_{ij}^{-1}$ . physical systems with anisotropic restivities include various metals , crystals and the concept also plays a role in geophysics , in the analysis of sediments and oil fields . an example of a system that exhibits this property would be one consisting of layers of several materials , each with a different resistivity . currents flowing along one specific layer will face different resistivity than those which flow perpendicularly .
what an awesome question ! by the way , as far as i know , the original video is here for those interested . one key to understanding this is the following fact from classical mechanics that is a version of newton 's second law for systems of particles : the net external force acting on a system of particles equals the total mass $m$ of the system times the acceleration of its center of mass $$ \mathbf f_{\mathrm{ext} , \mathrm{net}} = m\mathbf a_\mathrm{cm} $$ in the case of the slinky , which we can model as a system of many particles , the net external force on the system is simply the weight of the slinky . this is just given by its mass multiplied by $\mathbf g$ , the acceleration due to gravity , so from the statement above , we get $$ m\mathbf g = m\mathbf a_\mathrm{cm} $$ so it follows that $$ \mathbf a_\mathrm{cm} = \mathbf g $$ in other words we have shown that the center of mass of the slinky must move as if it is a particle falling under the influence of gravity . however , there is nothing requiring that the individual particles in the system must move as though they are each falling freely under influence of gravity . this is the case because there are interactions between the particles that affect their motion in addition to the force due to gravity . in particular , there is tension in the slinky , as you point out . you are absolutely correct that the bottom of the slinky does not move because the tension of the rest of the slinky pulling up balances the force due to gravity pulling down until the moment that the slinky is fully compressed and the whole thing falls with the acceleration due to gravity . regardless , the center of mass is moving as though it is freely falling the whole time . by the way , there are some nice comments about this experiment from the angle of wave propagation on physics . se user @mark eichenlaub 's blog which can be found here .
to figure out why this happens , you need to think about what boiling is , and how it works . as you would know , the water in the pot boils because its temperature was raised above the boiling point by the flame . this required a net transfer of heat from the flame , through the pot , to the water in the pot . why did the heat flow in this direction ? because the flame is hotter than the water in the pot , even when the water starts boiling ( $t_{flame} &gt ; t_{boil}$ ) now , think about the water in the bottle . the only way for it to get heat is through the water in the pot . as long as the temperature of the water in the pot , $t_{pot}$ , is less than $t_{boil}$ , it is still liquid , and it transfers some heat to the water in the bottle . the water in the pot boils off at $t_{boil}$ , and can no longer transfer heat as efficiently to the water in the bottle . this effectively means that the water in the bottle is restricted to a maximum temperature of slightly less than $t_{boil}$ , and that is why it never boils . another way to think of this is , there must be a temperature difference for a heat transfer to take place . since the maximum possible temperature of the pot water is $t_{boil}$ , the temperature of the bottle water can never exceed this . edit : another factor to consider is the low conductivity of glass , which means a high temperature difference is required to let a small heat flux through .
first expand the product in $\mathcal{l}_0$: $$\mathcal{l}_0=-\frac{1}{2}\left ( \partial_{\mu}a_{\nu}\partial^{\mu}a^{\nu}-\partial_{\nu}a_{\mu}\partial^{\mu}a^{\nu}\right ) -\frac{1}{2}m^2a_{\mu}a^\mu . $$ now in the term $\partial_\mu a_\nu\partial^\mu a^\nu$ , the vector fields $a$ have the same index , hence the metric tensor in the first term of $d_{\rho x , \sigma y}$ . also , the partial derivatives in this term have the same index , hence so do the partial derivatives in the first term of $d_{\rho x , \sigma y}$ . specifically , ( in what follows , $\partial_\mu\equiv\frac{\partial}{\partial x^{\mu}}$ and $\partial'_\mu\equiv\frac{\partial}{\partial y^{\mu}}$ ) : $$\eta_{\rho\sigma}\partial_\mu\partial'^\mu a^{\rho} ( x ) a^\sigma ( y ) =\partial_\mu a_\sigma ( x ) \partial'^\mu a^\sigma ( y ) $$ which after multiplying with the $\delta$-distribution and integrating over $y$ gives $\partial_\mu a_\nu ( x ) \partial^\mu a^\nu ( x ) $ . this explains the first term of $d_{\rho x , \sigma y}$ . the second term follows from $$\partial_\sigma\partial'_\rho a^\rho ( x ) a^\sigma ( y ) =\partial_\sigma a^\rho ( x ) \partial'_\rho a^\sigma ( y ) , $$ which gives the term $-\partial_{\nu}a_{\mu}\partial^{\mu}a^{\nu}$ after multiplying with the $\delta$-distribution and integrating over $y$ . the third term is trivial . combining all this , it follows that indeed $$i_0 [ a_\mu ] \equiv \int d^4x\: \mathcal{l_0} ( a_\mu ( x ) , \partial_\mu a_\nu ( x ) ) = -\frac{1}{2}\int d^4xd^4y\ ; d_{\rho x , \sigma y}\ ; a^\rho ( x ) a^\sigma ( y ) . $$
we indeed expect that the universe expands on the basis of the big bang theory . hence by looking at higher redshift , i.e. further in time , you should expect objects to recede faster . this is reflected in the linear relationshift that first measured by hubble $$v=h_0d , $$ where $v$ is the recession speed , $d$ the distance to the object and $h_0$ the hubble constant . however from the observation of snia , that completes the plot for higher redshifts , the linear relationshift deviates ! it suggest that the relation between velocity $v$ and the distance $d$ is no longer linear for higher $z$ . the expansion is accelerating . this acceleration depends on the geometry and energy content of the universe . take a look at the graph below : in a empty universe , as you noted , the recession velocities to deviate from the linear hubble relation and it is indicated by a straight line . the measurement from the sn ia however show that the universe deviates from this relation due to the presence of a repulsive energy component that result in an accelerated expansion of the universe to account for the observed recession velocities . this repulsive energy content is called dark energy and is still highly mysterious .
the coating has nothing to do with it - it is the metal of the pan that matters . as was mentioned in the comments , many teflon coated pans are made of aluminum ( or aluminium , depending on where you live . . . ) . the issue is skin depth : for a non-ferromagnetic material , at the frequencies of the induction heater a large fraction of the volume of the pan becomes a ( good ) conductor of the electrical current : and because the size of the ( eddy ) current is fixed by the geometry of the cooker , if you present it with a low resistance , the amount of power generated will be low , since $p=i^2 r$ . the following table of skin depths ( at 24 khz ) shows you what is going on ( adapted from http://en.wikipedia.org/wiki/induction_cooking ) : in this table as you can see , copper and aluminum make terrible materials for induction cooking pans : you need something with high electrical and high magnetic permeability to get the best heat transfer . 432 stainless ( which has high resistivity and is ferromagnetic ) is a lot better . some copper pans adapted for induction heating have a coating of another material specifically for this reason .
consider two points a and b in spacetime . if they are separated by a spacelike interval , then indeed different observers can disagree on which happened earlier . however , there are two other possibilities . a could be inside the future light cone of b , in which case all observers agree that a is in b 's future . similarly , b can unambiguously be in a 's future . things may get more complicated in general relativity , but this idea still holds . the distant galaxies we see are in our past , and no shift of reference frame can alter this . causality is preserved .
most practical materials are not 100% pure . the sentence that you pasted above is most likely referring to unintentional impurities ; their nature and concentration depend on the growth technique . here is one example : http://avspublications.org/jvst/resource/1/jvstal/v16/i3/p847_s1 . i am sure there are ways to get even purer samples . yes , impurities do have an effect on the narrowing of the bandgap . but there magnitude is significant only for very high concentrations of impurities . now , from eq . ( 15 ) of http://jap.aip.org/resource/1/japiau/v47/i2/p631_s1 you can see that the expression for the bandgap as a function of doping for p-gaas ( at 300 k ) is given by $$e_g ( ev ) = 1.424 - 1.6\times10^{-8}p^{1/3}\equiv e_{g0} - \delta e_g ( p ) $$ where $p$ is in $cm^{-3}$ . if you use $p = 10^{15} cm^{-3}$ then you get $\delta e_g=1.6 mev$ . yes , $\delta e_g$ may in principle have a temperature dependence . but considering the order of magnitude corrections that impurities make to the bandgap , impurities alone are not able to explain a variation of close to a $100mev$ from 0 k to 300 k . then a natural question to ask is : what are the most important things that change in a solid as you change temperature ? the atoms start to vibrate more vigorously ( increase in phonon energy ) and the solid expands ( increase in lattice constant ) . in eq . ( 45 ) of http://jap.aip.org/resource/1/japiau/v53/i10/pr123_s1 you can see the empirical expression : $$\epsilon_i ( t ) = 1.519 - \frac{5.405\times 10^{-4}t^2}{t+204}$$ where $t$ in obviously in kelvins . it can be noted that $\epsilon_i ( 297k ) = 1.424ev$ . the above expression holds for high-purity materials . or a better way to say this is that the impurity levels that are so low that they influence the energies by less than $1mev$ which are below our tolerance . by the way , do not consider the above formula is magically perfect . there are a couple of articles which follow the one cited above which come up with better and better microscopic models to explain the temperature dependence of the bandgap . but these are just quantitative differences in fitting . the qualitative behavior of the temperature dependence is still the same .
unless i have made a conceptual mistake ( which is very possible ) , surface tension plays essentially no role in the damping of the impact of a fast-moving object with a liquid surface . to see this , a simple way to model it is to pretend that the water is not there , but only its surface is , and see what happens when an object deforms this surface . let there be a sphere of density $\rho=1.0\text{g/cm}^3$ and radius $r=1\text{ft}$ with velocity $v=200\text{mph}$ , and let it collide with the interface and sink in halfways , stretching the interface over the surface of the sphere . before the collision , the surface energy of the patch of interface that the sphere collides with is $$e_i=\gamma a_1=\gamma\pi r^2$$ and after collision , the stretched surface has a surface energy of $$e_f=\gamma a_2=2\gamma\pi r^2$$ and so the energy loss by the sphere becomes $$\delta e=e_f-e_i=\gamma\pi r^2$$ which in the case of water becomes ( in mathematica ) : &lt;&lt; PhysicalConstants` r = 1 Foot; \[Gamma] = 72.8 Dyne/(Centi Meter); Convert[\[Pi] r^2 \[Gamma], Joule]  0.0212477 joule meanwhile , the kinetic energy of the ball is $$e_k=\frac{1}{2}\left ( \frac{4}{3}\pi r^3\rho\right ) v^2$$ which is : \[Rho] = 1.0 Gram/(Centi Meter)^3; v = 200 Mile/Hour; Convert[1/2 (4/3 \[Pi] r^3 \[Rho]) v^2, Joule]  474085 joule and hence the surface tension provides less than one millionth of the slowdown associated with the collision of the sphere with the liquid surface . thus the surface tension component is negligible . i would suspect that kinematic drag provides most of the actual energy loss ( you are basically slamming into 200 pounds of water and shoving it out of the way when you collide ) , but i have never taken fluid dynamics so i will await explanations from people with more experience .
if i remember correctly , assuming only a homogeneous and isotropic spacetime , on top of an arbitrary group structure , the only 4d spacetime symmetries that are allowed are either galileo group so ( 3,1 ) ( that is lorentz group ) so ( 4 ) ( that is euclidean 4d rotations ) . the relevant references where this was shown are ( according to link below ) j-m . levy-leblond , american journal of physics , 44 ( 1976 ) 271 . b . preziosi , n . cim . 109 b ( 1994 ) 1331 . a simpler ( 1+1 ) -dimensional version of the proof is given in the first 5 pages of these notes on general relativity and beyond : http://www.df.unipi.it/~menotti/appunti.pdf
as noted by kyle kanos : the derivation of the lagrange point heavily relies on the assumption that the object can be seen as a point-like mass . the lagrangian points are the constant-pattern solutions of the restricted three-body problem : ( wikipedia ) this assumption breaks down , when you consider an extended mass distribution like a galaxy .
the collision is inelastic . in either frame some of the initial kinetic energy is lost . where can it go ? it could be carried off by massless ejecta ( photons ) . it could remain as a binding energy of the combined system or in internal modes of the combined system . either way the mass , $m$ , of the combined system is greater than $2m$ . or some combination of the above . your calculation assumes the final mass is $2m$ , which is not true unless you have failed to tell us about some relevant momenta in the ejecta . assuming case 2 , let 's figure the kinematics in your second frame ( one mass stopped ) ; i am going to use $c=1$ units and every mass i write is a rest mass : $$ e_f = \sqrt{m^2 + p^2} = e_i = m + \sqrt{m^2 + p^2} $$ applying conservation of momentum we can set the $p = p$ , so we solve for $m$ $$ m^2 = m^2 + 2m\sqrt{m^2 + p^2} + m^2 + p^2 - p^2 $$ or $$ m^2 = 2m^2 \left [ 1 + \sqrt{1 + \left ( \frac{p}{m}\right ) ^2} \right ] $$ which is a bit more than $2m$ .
cross terms appear when the coordinate axes do not pass through the center of mass . that is if you start with a diagonal inertia matrix at the center of mass , when applying the parallel axis theorem cross terms will appear . in vector form the parallel axis theorem is $$ {\bf i} = {\bf i}_{cm} - m [ {\bf r}\times ] [ {\bf r}\times ] $$ where $ [ {\bf r}\times ] = \begin{pmatrix}x\\y\\z\end{pmatrix} \times = \begin{bmatrix} 0 and -z and y \\ z and 0 and -x \\ -y and x and 0 \end{bmatrix}$ is the cross product matrix operator . so if we start with a diagonal inertia at the center of mass , when moved to a different point $ ( x , y , z ) $ the inertia matrix is $$ {\bf i} = \begin{bmatrix} i_x+m ( y^2+z^2 ) and - m x y and -m x z \\ - m x y and i_y + m ( x^2+z^2 ) and - m y z \\ - m x z and - m y z and i_z + m ( x^2+y^2 ) \end{bmatrix} $$ so if any two of $x$ $y$ or $z$ are zero the result is still a diagonal matrix . this happens when one of the coordinate axis passes through the center of mass . in your case if the $x$ axis goes from the corner towards the center of mass ( across diagonal ) then $y=0$ and $z=0$ and the criteria is met . so the question boils down to under which conditions the inertial matrix is diagonal at the center of mass . the answer to this has to do with symmetries . for example , when a particle at a positive $z$ coordinate is matched with an particle at a negative $z$ coordinate then when the cross terms get added up the result is zero . this can be seen from the structure of the inertia matrix ( as shown above ) if you think of this as a parallel axis theorem for a lot of small little point masses added together .
some observations : in diodes , the recombination time is of order a nanosecond . it is as fast for similar materials with a high carrier density . the time will depend on the density but also other things and i am not aware of a simple universal formula . it is useful to know that the intra-band collisions that bring the bands into their well-known distributions are much faster and take as little as 0.1 picosecond . the inter-band processes ' characteristic time may go to many microseconds or longer . yes and no , the exponential determines the ratio of carriers that want to sit on the energetically disfavored side divided by those on the dominant side . however , if you talk about the probability that a particular process occurs , you have to specify " in what time interval " you want this process to occur ( and how many electrons you watch etc . ) - because the result will surely depend on $dt$ . it is about the " rate of the process " . moreover , in the equilibrium , recombination and generation are balanced . more generally , it is not true that the recombination appears primarily due to thermal noise that kicks the carrier through the whole band . quite on the contrary , recombination - and the opposite process of generation - of the carriers in the real world is dominated by the impurities which locally produce new states for the carriers in the middle of the gap , making their jump easier ( because it may be divided ) .
i would guess you have heard that an airplane in a spin or some other critical state can dive to build up speed , then when it pulls out of the dive the increased speed increases the lift and can allow the pilot to regain control . you are presumably asking if the same idea can be used for a falling person . the problem is that an aircraft wing is carefully designed to use forward velocity to achieve lift while a human body has no surfaces that can be used in this manner . the airflow around a falling person will simply be turbulent and will cause drag but no lift . skydivers can control their flight to some degree but this is mainly by moving the drag with respect to their centre of mass , and while this can effectively rotate them it provides little or no lift in the aeronautical sense .
first of all , by interchanging $\lambda_{\epsilon}^{\beta}$ with $\lambda_{\zeta}^{\alpha}$ nothing changes : $$\lambda_{\epsilon}^{\beta}\lambda_{\zeta}^{\alpha}\lambda_{\kappa}^{\gamma}\lambda_{\lambda}^{\delta}\epsilon^{\epsilon \zeta \kappa \lambda}=\lambda_{\zeta}^{\alpha}\lambda_{\epsilon}^{\beta}\lambda_{\kappa}^{\gamma}\lambda_{\lambda}^{\delta}\epsilon^{\epsilon \zeta \kappa \lambda}$$ now switch $\epsilon$ with $\zeta$ only in the levi-civita symbol : $$\lambda_{\zeta}^{\alpha}\lambda_{\epsilon}^{\beta}\lambda_{\kappa}^{\gamma}\lambda_{\lambda}^{\delta}\epsilon^{\epsilon \zeta \kappa \lambda} = -\lambda_{\zeta}^{\alpha}\lambda_{\epsilon}^{\beta}\lambda_{\kappa}^{\gamma}\lambda_{\lambda}^{\delta}\epsilon^{\zeta \epsilon \kappa \lambda}$$ and rename all the $\epsilon$ with $\zeta$ and all the $\zeta$ with $\epsilon$ ( again nothing changes ) : $$-\lambda_{\zeta}^{\alpha}\lambda_{\epsilon}^{\beta}\lambda_{\kappa}^{\gamma}\lambda_{\lambda}^{\delta}\epsilon^{\zeta \epsilon \kappa \lambda}\to-\lambda_{\epsilon}^{\alpha}\lambda_{\zeta}^{\beta}\lambda_{\kappa}^{\gamma}\lambda_{\lambda}^{\delta}\epsilon^{\epsilon \zeta \kappa \lambda}$$ so from ( 1 ) , ( 2 ) and ( 3 ) you get $$\lambda_{\epsilon}^{\beta}\lambda_{\zeta}^{\alpha}\lambda_{\kappa}^{\gamma}\lambda_{\lambda}^{\delta}\epsilon^{\epsilon \zeta \kappa \lambda} = -\lambda_{\epsilon}^{\alpha}\lambda_{\zeta}^{\beta}\lambda_{\kappa}^{\gamma}\lambda_{\lambda}^{\delta}\epsilon^{\epsilon \zeta \kappa \lambda}$$
the pressure is defined as the flow rate of x-momentum in the x-direction plus the flow-rate of the y-momentum in the y-direction plus the flow rate of the z-momentum in the z-direction divided by three . each component of momentum is conserved , and flows locally from point to point in a fluid . each component is like a charge , and has it is own current , which makes a tensor of stresses . a fluid does not sustain shear , and this is true whether it is still or moving , by the principle of relativity . this means that if you put fluid between two plates , and squeeze , the force per-unit-area with which you squeeze ( the local flow of momentum in the direction perpendicular to the plates ) is equal to the force per unit area pushing outward at the edge of the plates . the flow of momentum is the same in all directions . you can measure the pressure by putting a homogenous solid at the point in the liquid , and noting how much it compresses . you can also do it by noting the very slight change in density of the fluid with pressure . the pressure of fluids is not an exact description of the fluid when there is viscosity , but it is close to perfect , and the viscosity and the pressure are independent ideas which can be treated separately . for the purpose of your class , you can think of the pressure as the amount a tiny box-spring will compress if you place a scale model in the fluid , moving along with the water . this pressure is the same in all directions in the fluid , despite the answer you got earlier .
the definition of rotational kinetic energy is $$ e_\text{rot}{}_{ ( i ) } = \frac{1}{2} j_i \omega_i^2 = \frac{\ ; \ ; l_i^2}{2j_i} $$ where $j_i$ is moment of inertia , $\omega_i$ is angular velocity and $l_i = j_i\omega_i$ is angular momentum of the particle . if you select $\vec{r}_\text{cm}$ as the center of rotation these values can be calculated for each particle as follows : $$ j_i = \mu_i ( \vec{r}_i - \vec{r}_\text{cm} ) ^2 $$ $$ \vec{l}_i = \mu_i \bigl [ ( \vec{r}_i - \vec{r}_\text{cm} ) \times \dot{\vec{r}}_i\bigr ] $$ the rotational energy of the system is the sum of rotational energies of the particles : $$ e_\text{rot} = \sum_i \frac{\ ; \ ; l_i^2}{2j_i} $$ there are two translational energies : translational energy of the whole system $$e_\text{cm}=\frac{1}{2}\sum_i \mu_i \dot{\vec{r}}_\text{cm}^2$$ internal translational energy ( broadening and shrinking without rotation ) $$e_\text{int} = \frac{1}{2}\sum \mu_i \left ( \frac{d}{dt}\left|\vec{r}_i - \vec{r}_\text{cm}\right|\right ) ^2$$ total energy is the sum : $$ e_\text{tot} = \frac{1}{2}\sum_i \mu_i \dot{\vec{r}}_i^2 = e_\text{cm} + e_\text{int} + e_\text{rot} $$
the counting of the dimensions is the same for d1-branes and the fundamental f1-strings . there are 24 physical scalars because the embedding of a 2-dimensional world sheet ( of either f-string or d-string ) may be locally specified by 24 functions . for example , as long as the coordinates $x^0 , x^1$ are changing at least " a little bit " in a region of the world sheet , the remaining 24 coordinates $x^2$-$x^{25}$ may be written as functions of $x^0$ and $x^1$ . that is 24 functions that fully specify the embedding of the 2d world sheet to the 26d spacetime . in other words , you may have 26 functions $x^\mu ( \sigma , \tau ) $ of two world sheet coordinates but two of them may be eliminated by a reparameterization $ ( \sigma , \tau ) \to ( \sigma ' , \tau' ) $ that is chosen to respect a gauge choice . a particular gauge choice may break down in the regions where at least one of the coordinates like $x^0 , x^1$ in the example above reaches a stationary point . in these regions , one has to choose a different gauge choice but the physical content is still just 24 transverse scalars of some kind , despite the field redefinition . branes of any dimension may surely be compact or have pretty much any topology or shape you may think of . wrapped d-branes are actually very important in the scheme of string dualities . if they are wrapped on cycles ( submanifolds ) whose volumes shrink to zero , these wrapped d-branes become particles that may go massless , and therefore become very important . that is , for example , the reason why m-theory or type iia string theory on the ade $c^2/\gamma$ orbifolds singularities carries non-abelian gauge supermultiplet located at the singular locus itself . compact branes having closed and contractible ( topologically trivial ) shape also exist but they cannot be stable ( which also means that they cannot preserve any supersymmetry ) . they collapse much like a star collapses under its own gravity ( self-attraction ) . this instability may often be seen through the existence of tachyonic modes .
in precise terms , the heisenberg uncertainty relation states that the product of the expected uncertainties in position and in momentum of the same object is bounded away from zero . your entanglement example at the end of your edit does not fit this , as you measure only once , hence have no means to evaluate expectations . you may claim to know something but you have no way to check it . in other entanglement experiments , you can compare statistics on both sides , and see that they conform to the predictions of qm . in your case , there is nothing to compare , so the alleged knowledge is void . the reason why the heisenberg uncertainty relation is undoubted is that it is a simple algebraic consequence of the formalism of quantum mechanics and the fundamental relation $ [ x , p ] =i\hbar$ that stood at the beginning of an immensely successful development . its invalidity would therefore imply the invalidity of most of current physics . bell inequalities are also a simple algebraic consequence of the formalism of quantum mechanics but already in a more complex set-up . they were tested experimentally mainly because they shed light on the problem of hidden variables , not because they are believed to be violated . the heisenberg uncertainty relation is mainly checked for consistency using gedanken experiments , which show that it is very difficult to come up with a feasible way of defeating it . in the past , there have been numerous gedanken experiments along various lines , including intuitive and less intuitive settings , and none could even come close to establishing a potential violation of the hup . edit : one reaches experimental limitations long before the hup requires it . nobody has found a gedankenexperiment for how to do defeat the hup , even in principle . we do not know of any mechanism to stop an electron , thereby bringing it to rest . it is not enough to pretend such a mechanism exists ; one must show a way how to achieve it in principle . for example , electron traps only confine an electron to a small region a few atoms wide , where it will roam with a large and unpredictable momentum , due to the confinement . thus until qm is proven false , the hup is considered true . any invalidation of the foundations of qm ( and this includes the hup ) would shake the world of physicists , and nobody expects it to happen .
you may consider this question from perspective of energy view : the electric energy is consumed by the resistor and convert this energy to the thermal energy ( the source of heat that heat up the water ) . so from this point of view , if you can assume the 100% electric energy converting to thermal energy , and usually , this assumption is right for resistors since there is no other kind of energy that electric energy can convert to , since this is not a motor or a light bulb . thus you will have the following equation : $heat = i^2rt = \frac{u^2}{r}t$ so this amount of heat will be absorbed by water and heat the water up , so $heat = c_{water}\cdot m\cdot \delta t$ by solving these two equations , you can get the amount of water ( $m_{water}$ ) you need as $m_{water} = \frac{u^2 t}{r\cdot c\cdot \delta t}$ where $r$ can be easily calculated as $r=\frac{\rho l}{\pi r^2}$ for this cylindrical resistor . so the key point to connect the electricity to thermal dynamics is the conservation of energy so that energy has to convert from one form ( electric energy in your case ) to another form ( thermal energy or heat in your case ) , and little or no energy is converted into other form such as light .
general relativity tells us that the event horizon of a spinning object is given by $r = m + \sqrt{m^{2}-a^{2}-q^{2}}$ , where $m$ is the mass of the black hole , $a$ is the angular momentum per unit mass of the black hole , and $q$ is the charge of the hole${}^{1}$ . if you put in the appropriate mass , charge and angular momentum for common fundamental particles , you will find that , almost uniformly , $a^{2}+q^{2} &gt ; m^{2}$ , which means that the equation for the horizon has no real roots , and that if you were to interpret these particles as classical point particles , they would be naked singularities , not black holes . fortunately , no one takes that model too seriously , expecting quantum gravity to take over first . ${}^{1}$as always in gr , we choose units of $g=c=1$ , and choose gaussian units for our charge .
let $\vec y_i = \vec x_i + \lambda \vec e$ you have : $\frac{dv ( \vec y_1 , \vec y_2 , . . . . \vec y_n ) }{d\lambda} = \sum_i \vec \nabla_iv ( \vec y_1 , \vec y_2 , . . . . \vec y_n ) . \frac{d \vec y_i}{d \lambda}$ ( chain rule for partial derivatives ) here $\vec \nabla_iv$ is the vector which has components $ ( \vec \nabla_iv ) _j = \frac{\partial v}{\partial ( y_i ) _j}$ but : $\nabla_i v ( \vec y_1 , \vec y_2 , . . . . \vec y_n ) |_{\lambda=0} = \nabla_i v ( \vec x_1 , \vec x_2 , . . . . \vec x_n ) =- \vec f_i$ moreover , $\frac{d \vec y_i}{d \lambda} = \vec e$ , and this equality is true also for $\lambda = 0$ finally , we have : $$\frac{dv ( \vec y_1 , \vec y_2 , . . . . \vec y_n ) }{d\lambda}|_{\lambda=0} = \sum_i -\vec f_i . \vec e = - \vec f . \vec e$$
" multiplying the wavefunctions " is a pretty nebulous term . let 's work with some definite vocabulary here , shall we ? $ ( 1 ) $ the states of one qm particle are elements of some hilbert space $\mathcal{h}$ . if we care only about position on a line as completely defining the state ( which we can for a scalar boson ) , i.e. demand that the space be spanned by the orthogonal position basis $|x\rangle , x\in \mathbb{r}$ , this space is isomorphic to the square integrable functions $\psi : \mathbb{r} \rightarrow \mathbb{c}$ by sending arbitrary states $|\psi\rangle$ to the function $\psi ( x ) = \langle \psi | x \rangle$ . [ we have ignored some technicalities above , i.e. normalization , gelfand tripel , distribution . . . and we will continue to ignore them , they do not matter for this argument ] $ ( 2 ) $ the states of $n$ such particles is given by the $n$-fold tensor product space $\mathcal{h} \otimes \dots \otimes \mathcal{h}$ . it is basis is formally given by $$|x_1 , \dots , x_n\rangle :=|x_1\rangle \otimes \dots \otimes |x_n\rangle , x_i \in \mathbb{r} \forall i \in \{1 , \dots , n\}$$ we can map any state $|\psi\rangle \in \bigotimes_{i = 1}^n \mathcal{h}$ to a wavefunction again by $$ \psi : \mathbb{r}^n \rightarrow \mathbb{c} , ( x_1 , \dots , x_n ) \mapsto \langle \psi |x_1 , \dots , x_n\rangle$$ this can only be written as $\langle\psi_1|x_1\rangle\dots\langle\psi_n|x_n\rangle = \prod_i \psi_i ( x_i ) $ if the state $\psi$ is a state constructed from the single states $|\psi_i\rangle$ of the particles as $|\psi\rangle = |\psi_1\rangle \otimes \dots \otimes |\psi_n\rangle$ . not all states in $\bigotimes_{i = 1}^n \mathcal{h}$ can be written that way . so , if you indeed know all individual states , the total wavefunction is indeed a multiple of the single wavefunctions . however , for so-called entangled states , this is impossible . so , taking the tensor product is multiplying the wavefunctions , which enlarges the space of all possible states . adding the wavefunctions is just using the addition on the hilbert space $\mathcal{h}$ . it does not enlarge the space of states , it only superposes two states of the same particle/system . i have no idea what " field strength " you are talking about in the op , though .
i think the most powerful approach to the general problem of arbitrary domains is the approach detailed in reviving the method of particular solutions by timo betcke and lloyd n . trefethen from 2005 . [ doi ] [ pdf ] . in it they describe a modern modification of the historical method of numerically finding the solution to the helmhotz equation from approximations and bounds for eigenvalues of elliptic operators by fox , henrici and moler from 1967 . [ doi ] ( fun fact : moler would go on to create matlab ) for completeness , i will summarize the method . philosophically , it is very similar to the variational approaches used to find bounds on energies of eigenfunctions in quantum mechanics . the basic idea is to choose a possible basis for your eigenfunctions , and then just find a linear combination of those basis functions that satisfies the boundary conditions . that is , assume we have some basis of parametrized functions : $$ \phi_{k} ( x ; \lambda ) $$ we will look for solutions to the helmholtz problem of the form : $$ \psi ( x ) = \sum_k c_k \phi_{k} ( x ; \lambda ) $$ and we do this by choosing a set of points on the boundary , for which we will ensure that our solution vanishes , and a set of points on the interior for which the solution does not vanish . the details are worked out in the paper , but this means you choose a set of points $x_i$ on both the boundary and interior , then we define $\sigma ( \lambda ) $ in the following way : form the matrix $$ a_{ik} ( \lambda ) = \phi_k ( x_i ; \lambda ) $$ takes its qr decomposition $$ q_{il}r_{lk} = a_{ik} ( \lambda ) $$ then take the smallest singular value of the part of $q$ for those points on the boundary . then , to find your eigenvalues , simply look for minima of $\sigma ( \lambda ) $ using your favorite numeric scalar minimizer . once you have found the eigenvalues , you can find the coefficients determining your solution as the solution to $$ r c = \hat v $$ where $r$ is the $r$ from our qr decomposition above , $c$ is the vector of coefficients determining your solution and $\hat v$ is the right singular vector of the part of $q$ defined on the boundary associated with the smallest singular value . as a form of demonstration , we will consider the classic $l$ shaped domain , the solution to which was first given in the 1967 paper , and which serves as the logo for matlab . to make our method perform well , we will define our basis as : $$ \phi_k ( x ; \lambda ) = j_{\alpha k} ( \sqrt{\lambda} r ) \sin ( \alpha \theta k ) $$ defined from the interior corner of the $l$ . these are the solutions to the helmholtz equation on a semiinfinite wedge , and so they are guarenteed to satisfy our boundary conditions on the interior walls , which will help us out . so , for our trial points , i will just take a bunch of random points on the boundary and interior ( though i will not put any on the interior walls as i know those are already taken care of by my parametrization ) and then , if we calculate $\sigma ( \lambda ) $ we get : which shows us where we expect to find the eigenvalues , using a scalar minimizer , and finding the eigenfunctions as described , we find for the lowest 6 modes : where , for instance our first eigenvalue comes out accurate to 11 decimal places , using only 72 points on the interior and boundary each , and only taking ~ 3 seconds to find all 6 of the lowest energy eigenvalues and eigenfunctions on my machine . pretty good if you ask me . for what it is worth , i have also made the code i used to generate all of these figures available as an ipython notebook where you can see the method implemented .
the higher the number of derivatives the more initial data you have to provide . if you have some lagrangian that contains an infinite number of derivatives ( or derivatives appearing non-polynomially , such as one over derivative ) then you have to provide an infinite amount of initial data which amounts to non-local info , in the sense explained below . if you think in terms of taylor expansions around your initial value , then you have to provide the full function ( and thus non-local information ) if you have an infinite number of derivatives . this is to be contrasted with cases where you provide only the field and its first derivative as initial values ( and thus rather local information ) . personally , i would not call any higher-derivative lagrangian " non-local " , but only those theories where the number of derivatives is formally infinite in the lagrangian . in any discretization scheme you literally see the non-locality induced by higher derivatives : to define the first derivative you need to know the function on two adjacent lattice points , to define the second derivative on three and to define the n-th derivative on n+1 lattice points . thus , the more derivatives the more non-locality . if you have an infinite number of derivatives you need to know the function on an infinite set of lattice points .
they are not producing energy ; their movement is a result of the work you have put during setting same poles to face each other . they will eventually stop , since this energy is dissipated by friction on their fixings and at some point their momentum will decrease to the point it will be not enough to push the system through energy pick when the same poles are nearby . after this , they will relax to their energy optimum , so probably into configuration when different poles are facing each other .
the definition of fundamental frequency should be : the lowest frequency of a periodic wave satisfying some boundary conditions . for example , in the case of the vibrating string : the lowest frequency is determined by the length of the string ( top of the image ) , the tension in the string , and the mass per unit length . of course , if there are no boundary conditions , the lowest frequency would be $\nu=0\mathrm{hz}$ ( not $1\mathrm{hz}$ ) .
as far as i can tell , the two formulas look the same . in english , we write these formulas as : in the horizontal ( or x-direction ) : $x = x_0 + v_{0x}t + \frac{1}{2}at^2$ , where $x$ represents distance in the horizontal direction , $x_0$ represents the initial distance , $v_{0x}$ is the initial velocity in the x-direction ( sometimes instead written as $u_x$ ) and finally $a$ is the acceleration in the horizontal direction . similarly , we have for the vertical , or y-direction : $y = y_0 + v_{0y}t + \frac{1}{2}gt^2$ notice i have replaced $a$ with $g$ , since in most problems , the acceleration in the y-direction is due to gravity , so $a=g$ . these equations are referred to as " kinematic equations " along with these additional formulas : $v^2 = u^2 + 2as$ and $v = u + at$ .
you have enough information . a propeller is just a complicated sort of wing . assume the aircraft is moving forward at an airspeed of , say , 150 km/hr . assume the propeller is turning at an angular rate of , say , 40 revolutions per second ( hz ) . from that , if you look at the propeller surface at a particular radius from the center , such as 1 meter , you can determine the angle of the helical path followed by that part of the propeller through the air , and the speed it is travelling through the air . then assume it has a particular angle of attack , like 10 degrees or 0.18 radian , and has a particular " width " , which is the chord length of its airfoil . from that , you can figure out how much " lift " and " drag " it is generating , and what the direction of those forces are relative to the aircraft centerline . now if you just do that for a range of different radii on the propeller ( i.e. . perform an integration ) , you can figure out how much thrust the propeller generates and how much power it needs . do not forget , that is per propeller blade . typical propellers have 2 or 3 blades , but can have as many as 6 . this is still just a rough calculation because it depends on things like the shape of the tip of the propeller , just as the shape of an aircraft 's wingtips make a difference in how much drag is produced . note that if the thrust is greater than the drag on the aircraft , the aircraft will be climbing . if less , it will be descending .
let us assume that the cantilever is fixed on a wall at the rhs end , and use that end as reference for measuring the distance , $x$ , along the cantilever away from the wall . let us also assume that the force $f$ is applied at a point $a$ units away from the wall ( obviously the $a\le l$ , where $l$ is the length of the canrilever ) . the bending moment at some point p which is distance $x$ from the wall , the bending moment is the algebraic sum of the moments of all loads lying to the right of the point p , and taken with respect to the point p itself . so we can write the forllowing : for $x\le a$ the bending moment $m ( x ) =-f ( ( l-x ) ) $ for $x\gt a$ the bending moment $m ( x ) =0$ as there are no loads beyond the point $x=a . $ when you combine this with the equation for the deflection , v , you get $ei{\frac {d^2v}{dx^2}}=m ( x ) $ , where $e$ is the young modulus of the material , and $i$ is the second moment of area of the cantilever ( also known as moment of inertia , calculated in a similar way ) . you need to solve this equation using youe boundary conditions , to get the diflection at various values of $x$ . hope this helps .
the compton wavelength given by , $$\lambda=\frac{\hbar}{mc}$$ is a natural length scale associated to any particle with mass $m\neq 0$ . as professor tong , states : at distances shorter than this [ compton wavelength ] , there is a high probability that we will detect particle-anti-particle pairs swarming around the original particle that we put in . [ the wavelength ] is always smaller than the de broglie wavelength $\lambda_{db}=h/|\vec{p}|$ . if you like , the de broglie wavelength is the distance at which the wavelike nature of particles becomes apparent ; the compton wavelength is the distance at which the concept of a single pointlike particle breaks down completely .
you are probably filling your whole computation grid with your zernike polynomial . remember that zernikies are only orthogonal ( and useful as optical aberrations ) over the unit circle , so you have got to choose an appropriately sized patch of the computation grid over which you will define your beam intensity and phase . for example , i generated some plots . the grid is 512x512 pixels : here is the phase delay map for coma , computed as if the unit circle extends to the edges of the grid , computed as $\sqrt{8} ( 3 \rho^3 -2 \rho ) \sin{\theta}$ . it has a p-v of 1 wave : now if we put a gaussian intensity profile on , but we do not fill the whole grid ( as i am sure you are not , because this is a much more difficult error to not notice ) , with the $e^{-2}$ intensity contour having a radius of $\frac{1}{10}$ the grid , we get a near field intensity like this : if you use the above phase and intensity , you get a far field like this ( shown as $\sqrt{\mathrm{intensity}}$ ) : this basically looks like a perfect diffraction limited spot ! why ? well look at the phase over the region occupied by the beam : it is just tilt ! and its p-v is way below 1 wave . so it will not really do much of anything to the far field . now , lets try again , but properly scale the $\rho$ axis when we compute the coma : just what you expect .
when using this method of solving odes , we have to be careful not to use the indefinite integral . $\int\frac{dq}{\frac{k_bt}{m}-\frac{\alpha q}{m}}=\int dt$ is actually $\int\limits_{q_0}^q\frac{dq}{\frac{k_bt}{m}-\frac{\alpha q}{m}}=\int\limits_{t_0}^t dt$ this gives us $$-\frac{m}{\alpha}\ln\left ( \frac{k_bt}{m}-\frac{\alpha q}{m}\right ) + \frac{m}{\alpha}\ln\left ( \frac{k_bt}{m}-\frac{\alpha q_0}{m}\right ) =t-t_0$$ as $\ln a - \ln b = \ln \frac{a}{b}$ , this can be rewritten as $$-\frac{m}{\alpha}\ln\left ( \frac{k_bt-\alpha q}{k_bt-\alpha q_0}\right ) =t-t_0$$ which has a dimensionless argument of the logarithm . another way of looking at it is that the constant $c$ you got after taking the indefinite integral was of the form $c_0-\ln ( c_1 ) $ , where $c_1$ has the same units as that of $\frac{\alpha q}{m}$ . usually , in mechanics ( and most of physics ) , integrals are definite , even if we do not write them that way . also , while solving odes , it is a good idea to normalize everything to a dimensionless quantity before proceeding to solve . this is not such a big deal for simple odes like the one above , but when dealing with complex coupled systems losing the units leads to fewer things to keep track of .
you will see an image ( or piece of an image ) of the sun anywhere the surface of the water is oriented such that the angle between the sun and the water and the angle between the water and your eye are equal : for water with many ripples , there will be many such locations , but most will be along the line between you and the sun , or close to it . in practice , the surface is never perfectly calm , so the effect from ripples always comes into play at least a bit . i can think of another possible effect that could contribute , even if the water were perfectly calm : on its way in from the sun , light goes through the atmosphere , which has varying density/temperature , and so also varying refractive index . now the situation is not quite so simple as a bunch of parallel rays reflecting off of a mirror . curved paths through the atmosphere mean that the incident rays are not necessarily parallel , so reflections from different locations ( with different incident angles ) can meet at a point ( the observer ) . i would guess that this would be a smaller effect than the ripples , except perhaps for some very contrived weather setups . the effect would still be spreading out mostly along a line between the sun and observer , since the atmosphere is for the most part varying in the vertical direction ( so refraction in a vertical plane ) .
for two electrons your are right ! the absolute must is for the total state to be anti-symmetric . technically , this means that the total wave-function must belong to a one-dimensional representation of the permutation group such that each permutation is represented by either +1 or -1 depending on it is parity . you can ask about permutational symmetry separately for the spin part and the orbital part . if there number of electrons $n&gt ; 2$ , then the orbital part and the spin part may belong to these may belong to more complicated ( e . g . , more than one-dimensional ) representations of the permutations group , those get classified by young tableux . . there are rules how to combine two " conjugate" ( not sure if this is mathematically correct term ) representations to construct the total wave-function anti-symmetric under any odd permutation . for $n=2$ these rules reduce to a simple product as you quote . for more on this you can consult landau and lifshitz , quantum mechanics ( the course of theoretical physics vol . iii ) . there is also a systematic connection between the representations of the permutation group for the spin part and the $su ( 2 ) $ of the spin . unfortuntally , i can not find the refernce which i have learn this from ( hopefully others can help ) .
the definition of ampere is obtained by the below equation of force between two infinitely long parallel current carrying conductors . where $f$ is force , $\triangle{l}$ is small length element , $\mu_0$ is absolute permeability of vaccum or free space , $i_1 , i_2$ are current flowing through two conductors . by calculation we can obtain that $\frac{\mu_0}{4\pi}=10^{-7} t a^{-1} m$ when $\triangle{l}=1m , i_1=i_2=1a , r=1m$ . by substituting the values in the above equation given in the figure , we obtain $\frac{f}{\triangle{l}}$ to be equal to $2x10 ^{-7}n m{-1}$ . thus we have the definition of one ampere as : one ampere is that current , which when flowing through each of the two parallel conductors of infinite length and placed in free space of one metre from each other , produces between them a force of $2x10^{-7}$ newton per metre of their lengths .
if you rearrange your equation to solve for $y$ , $$ y= ( x+125 ) \frac{-30+70}{360+125}-70 $$ this reduces to $$ y=\frac{8x-5790}{97}=0.08247x+59.6907\approx0.08x-60 $$ which is about what the textbook obtains . the difference in values you are getting is completely due to the approximation that the solution manual uses . if you use the exact values , as you did , you will indeed get 1330 . if you use the approximate values as the solution manual did , then you will indeed get 1375 .
this is not the definitive answer that dumpsterdoofus was hoping for since i can not point to any scientific publications - they must exist but a quick google failed to find anything from a reputable journal though there are loads of blog articles . anyhow , although in soda the carbon dioxide solution is supersaturated there is an energy barrier to creating a bubble . this is because the energy released by forming a bubbe scales with the bubble volume , but the interfacial energy required to create the gas-water interface scales as the bubble area so the total energy change looks something like : $$ \delta e = -ar^3 + br^2 $$ where $a$ and $b$ are constants and $r$ is the bubble radius . typically the energy change will look something like this : so creating a small bubble actually costs energy and creates a barrier that you have to get over for the biubble to grow . the energy barrier can be reduced if there is a seed for the bubble to nucleate on . if you pour soda into a glass and look at where the streams of bubbles are coming from you will probably the the bubbles come from a few spots on the inside of the glass . these are where defects on the glass surface enhance the nucleation rate . glass is actually a very smooth surface even on the atomic scale because the surface anneals as the glass cools , so it does not have a high density of defects to provide nucleation points . by contrast ice is not a smooth surface . it has many small cracks in the surface , and the thermal stress of adding ice to ( relatively ) hot water will crack it further . all these defects provide many sites for nucleation and enhance bubble formation considerably . lots of other materials will do the same . allegedly cotton wool does ( though i have never tried it ) , salt does , and most spectularly mints do ( google soda bomb for details ! ) .
that is an excellent question , which has a few aspects : can you quantize any given lagrangian ? the answer is no . there are classical lagrangians which do not correspond to a valid field theory , for example those with anomalies . do you have field theories with no lagrangians ? yes , there are some field theories which have no lagrangian description . you can calculate using other methods , like solving consistency conditions relating different observables . does the quantum theory fix the lagrangian ? no , there are examples of quantum theories which could result from quantization of two ( or more ) different lagrangians , for example involving different degrees of freedom . the way to think about it is that a lagrangian is not a property of a given quantum theory , it also involves a specific classical limit of that theory . when the theory does not have a classical limit ( it is inherently strongly coupled ) it does not need to have a lagrangian . when the theory has more than one classical limit , it can have more than one lagrangian description . the prevalence of lagrangians in studying quantum field theory comes because they are easier to manipulate than other methods , and because usually you approach a quantum theory by " quantizing " - meaning you start with a classical limit and include quantum corrections systematically . it is good to keep in mind though that this approach has its limitations .
the commenters are right that this is going to depend a lot on the specifics of your powder ( particle size distribution , composition , etc . ) and on your laser processing technique . i happen to be doing my ph . d . on laser processing of materials and i can say from experience that the laser fluence ( total energy deposited per area ) , laser pulse duration , pulse frequency , and beam homogeneity can all play an important role when melting metals . so , the first part of my answer is : do an experiment ! use your process and measure the results . that is the only way you can know for sure . on the off chance that you might not have a laser sintering apparatus available ; - ) , i did a quick literature search to try to get a sense for the kind of strengths that folks are getting from laser sintered parts in rapid prototyping . i speculate that the best results are being obtained in industry and protected as trade secrets , but i did find a reasonable example study by a group in iran : http://www.sciencedirect.com/science/article/pii/s0924013603002838 . apologies for it being behind a paywall , but my university gives access so i can quote some facts from the article . most notably , at a density of 7.7 gm/cm$^3$ they obtain a fracture strength of ~500 mpa after secondary sintering at 1220-1280 c for 30-60 minutes . they used a fairly complex powder mixture that included fe ( of course ) and also cu , co , mo , ni , and c . alloying elements made up about 5% of the mixture . my impression ( from callister 's materials science and engineering and wikipedia ) is that a similarly complex mixture when cast into a steel can have a fracture strength in the 1 to 5 gpa range . this confirmed my initial suspicion that an optimized laser sintering procedure can get you within a factor of 2 to 10 of the strength of a cast steel . that said , a laser sintering process that is not optimized can give you a very weak material . one key step to get a strong final product is furnace annealing near the melting point after laser processing . it will give your particles a chance to coarsen and form stronger bonds . i recommend reading up on the tried and true methods of powder metallurgy if you want to develop your own procedure . laser sintering will be good for stablizing an initial shape , but ordinary sintering techniques are what will densify and strengthen your final part into a usable state . an important issue when discussing heat treatment of metals is oxidation . this is a particularly serious problem for powders and porous structures and is typically alleviated by control of the atmosphere whenever the metal is hot . in the case of the paper i discussed above , the laser sintering was performed under a pure nitrogen atmosphere while the furnace annealing took place in a vacuum of $10^{-2}$ mbar . note that if you are considering doing this on a budget , this level of vacuum is achievable with just a roughing pump . in my experience furnace annealing copper two tricks that have come in handy are annealing in flowing forming gas ( a mixture of h and n ) and including a large amount of metal powder or shot ( in this case use iron to avoid contamination ) in the furnace along with the material of interest to getter residual oxygen . so , in short , if you : optimize your powder and laser conditions experimentally perform a furnace anneal ( be sure to optimize the time and temperature ) after laser sintering prevent oxidation by controlling the atmosphere around the hot iron . you should be able to get strengths smaller than but of the same order as cast steels .
thare is only one node connecting elements b , d , e , and g . the dots on the diagram are just to indicate that the lines do in fact connect , so that all of the wires are part of the same node . in this kind of schematic diagram wires are considered as ideal , and all points on the wire are considered to be at an equal potential .
i think you misunderstood what the professor wanted to say . to understand this , let us evaluate the integral more thoroughly ( your expressions contain some mistakes ) . if we use the dimensional regularization prescription $d\rightarrow d-2\epsilon$ and an additional mass scale $\mu$ , we get for the integral in question the following result : $$\int \frac{d^{d-2\epsilon}p}{ ( 2\pi ) ^{d-2\epsilon}}\frac{1}{ ( p^2+\mu^2 ) ^2}=\frac{\gamma ( 2-d/2+\epsilon ) }{ ( 4\pi ) ^{d/2-\epsilon}}\mu^{-2 ( 2-d/2+\epsilon ) } . $$ for $d=4$ we get $$\int\frac{d^{4-2\epsilon}p}{ ( 2\pi ) ^{4-2\epsilon}}\frac{1}{ ( p^2+\mu^2 ) ^2}=\frac{\gamma ( \epsilon ) }{16\pi^2}\left ( \frac{\mu^2}{4\pi}\right ) ^{-\epsilon} . $$ expanding this at $\epsilon\rightarrow 0 , $ we arrive at $$\int\frac{d^{4}p}{ ( 2\pi ) ^{4}}\frac{1}{ ( p^2+\mu^2 ) ^2}\approx\frac{1}{16\pi^2}\left [ \frac{1}{\epsilon}-\gamma+\log ( 4\pi ) -\log ( \mu^2 ) \right ] . $$ in the massless limit , i.e. $\mu\rightarrow 0$ , the logarithm diverges . so what can we say about the nature about this divergence ? as can be concluded from powercounting , a positive $\epsilon$ corresponds to curing uv divergences , while a negative one cures ir divergences . first , let us assume that that we deal with uv divergences and identify $\epsilon=\epsilon_{uv} . $ what can we say about the remaining divergent term ? we can observe that the whole integral has to vanish ( which is proven earlier in the lecture ) , and this happens only when the divergent term is equal to minus the $1/\epsilon$ term , i.e. $$\frac{1}{\epsilon_{uv}}=\gamma-\log ( 4\pi ) +\log ( \mu^2 ) . $$ next , let us assume at we are dealing with divergences from the infrared , and identify $\epsilon=\epsilon_{ir} . $ we now have to observe that evaluating the integral gives us just the same result , but with $\epsilon_{uv}$ and $\epsilon_{ir}$ exchanged . the condition for vanishing of the integral is now $$\frac{1}{\epsilon_{ir}}=\gamma-\log ( 4\pi ) +\log ( \mu^2 ) . $$ but the right hand side is just the same as in the condition for the uv ! this means we actually get $$\epsilon_{uv}=\epsilon_{ir} . $$ as the lecturer has pointed out , this can be interpreted as dimensional regularization " taming " both the uv and the ir simultaneously .
you missed that to specify orientation , you not only need an axis , but how far the body rotated around the axis . the two axis angles and the angle of rotation is the euler angle parametrization , and i find it unweildy because the relation between this and position involves transcendental functions . the nicest way to give the rotation part is to specify a rotation matrix r which has the property that $r^tr=i$ . this has 3 parameters , since you have three orthogonal unit vectors inside , which is 2 components for the first ( it is unit length ) , one component for the second ( it is perpendicular to the first and unit length ) and none for the third . this is most convenient for pencil and paper and computer calculations both , this is why it is hardly ever presented in textbooks .
firstly , that wikipedia " trajectory calculation " page is pretty disappointing , it is not a very good fit to how smallarms ballistics is modelled and solved . a good book on the subject is bryan litz 's recent applied ballistics for long range shooting and a website with some first-rate on-line ballistics calculators as well as some good a very good writeups is jbm ballistics . you might also want to look at " gebc - gnu exterior ballistics calculator " to get some c code to play with . smallarms ballistics calculations suitable for most purposes are done by "1 degree of freedom " solvers . they treat the bullet as a point mass , affected by air drag and by gravity . the air drag is usually modelled by a " ballistics coefficient " , which is a single parameter that more or less successfully combines the effects of bullet size , weight and dragginess into a single number ( btw the wikipedia " ballistic coefficient " page is pretty decent ) . this simple physics model ( free flight in vacuum , plus air drag ) is given a starting velocity and position , and then integrated over time ( typically runge-kutta ) . a larger bc indicates that the bullet is less affected by air drag than a lower bc . there are two interesting points to this , one obvious , the other important but less intuitive : a bullet with a higher bc will lose speed slower , which will make it shoot flatter ( drop less with distance travelled ) since the bc measures the " degree of interaction between the bullet and the air " , it also turns out that the amount of wind drift ( how much the bullet is pushed sideways by a crosswind ) is directly affected by the bc of the bullet edit to add in response to the op 's comments : when you are looking at ( say ) the gebc code , you should probably be able to see that the physics model contains these points : the bullet has a starting position and velocity . these are usually expressed in a coordinate system that is stationary w.r.t. the shooter . one force acting on the bullet is gravity ( always down ) optionally , one can also model coriolis and other pseudo-forces one gets from this reference frame being not strictly an inertial one there is also the drag force . in a simple model , this is always directly opposite to the bullet 's speed through the air ( which will be the bullet 's speed through the shooter 's coordinate system plus the wind speed ) . more sophisticated models might consider other smaller forces ( lift on the bullet , side-force from magnus effect etc ) , but these other forces are a separate modelling exercise . the " b . c . " that you are talking about only concerns the drag force that a bullet experiences in the direction of the relative wind over the bullet . the force on the bullet is its drag coefficient times its area times the dynamic pressure ( which is 0.5 rho v^2 ) . in solving the bullet 's position you are actually interested in the acceleration due to this force , so you have this quantity divided by the bullet 's mass . you know the speed " v " , you know the atmospheric density " rho " , you need to find out the value of cd*a/m . note that a is constant , m is constant , but cd is not . cd depends on velocity ( actually bullet mach number ) , and the cd curve will be different for bullets of different shape . this is where the bc comes in . it is assumed that the " cd*a/m " curve of your bullet , is the same shape as and differs only by a multiplicative scaling parameter ( 1/bc ) of the " cd*a/m " curve of a standard reference bullet . the most common bc system is called " g1" and uses a reference bullet that is like a 1900s artillery shell . ( the " g7" system uses a reference bullet that is very similar to a modern long range rifle bullet ) . your bc program will need to model the " g1" drag curve as a function of mach number , typically this is done with lookup tables . at every iteration step where you need the acceleration on the bullet due to it is drag , you take the current mach number of the bullet , look up the " cd*a/m " value from the g1 table , divide it by your bc ( big bc means less drag and therefore a smaller acceleration due to drag ) , and that is the drag component that you feed to your flight model . ( go to the wikipedia ballistic coefficient writeup and have a look at the expression for " bc_sub_bullets " . in it , replace the " i " term with the " cb/cg " that it is defined to be . solve that expression for " cb " ( the bullet 's drag coefficient ) . now have a look at cb*a/m ( the " a/m " will draw the " m/d^2" term from the rhs ) . this will give you the cd*a/m that you want , expressed as function of the g1 drag table ) ( this question was also posted to firearms . stackexchange )
great photo ! edit : my language is " sloppy " ( i like talking physics in " lay person " terms so anybody can understand ) but @dcmkee made really nice comment clarifying my answer for the more advanced people . thanks @dcmkee ! since the plane is in a loop there is significant g 's due to centripetal acceleration . the water was being accelerated upward$^{1}$ with a high velocity then the plane keeps " looping " and the water tends to want to stay in the same direction ( upward ) and so the water gets released upward ! while the plane/start moving horizontally . its just the plane and people are moving in a way that are essentially " falling " faster than water so it seems ( or feels to the people in the plane ) that gravity is actually pulling them upwards . essentially the loop essentially created 2g 's of gravity in the upward direction so water pours upward just as it would on earth . ( 2g 's upward minus the one g downward of earth equals a net g upward ) . 1 by accelerated upward i mean it was initially traveling horizontally then changed its direction upward ( if uniform circular motion then its purely centripetal acceleration ) .
so here are the equations of motion : you have the geometry so you know the mass moment of inertia about the center of mass to be $i_c=\frac{m}{12}\ell^2$ the applied force $f$ creates a torque about the center of mass equal to $\tau=\frac{\ell}{2}\ , f$ the center of mass will accelerate by $a_c$ in the direction of $f$ with $$ m a_c = f $$ the body will accelerate rotationally by $\alpha$ with $$ \tau = i_c \alpha \bigg\} \frac{\ell}{2}\ , f = \frac{m}{12} \ell^2 \alpha $$ given the motion $$a_c = \frac{f}{m} \\ \alpha = \frac{6 f}{m \ell}$$ lets find if this corresponds to a rotation about a point . if a point exists it will lie on the other side of the rod from where the force is applied , lets assume a distance $c$ from the center of mass . if the linear acceleration of that point is zero , but the rotation is not , then the linear acceleration of the center of mass would be $a_c = c\ , \alpha$ . find the point by equating $$ \frac{f}{m} = c \ , \frac{6 f}{m \ell} \bigg\} c = \frac{\ell}{6} $$ fyi - the point of rotation is called a pole and the line of action of the force a polar . these show up in the center of percussion calculation . in your case , the point of the force $f$ is the point of percussion of the rod about the rotation point . you can reverse the situation and say that a point located at $c=\frac{\ell}{6}$ is the center of percussion of when rotating about the end point .
there are two explanations possible stemming from the fact that the definition of $m$ in the formula is ambiguous . well , perhaps einstein had only one of the two meanings in mind in his original paper but i am afraid i would not know that as i have not read it . by that as it may one should recall that special relativity mixes space and time and therefore also momentum and energy and the full formula relating all these fundamental quantities has to be $$e^2 = m_0^2c^4 + p^2c^2$$ where $m_0$ is the rest mass of the particle ( zero for photons ) whose importance lies in the fact that it is invariant w.r.t. lorentz transformations ( rotations and boosts ) . so first explanation is that the famous formula only holds in the object 's rest frame ( $p=0$ ) . but such a rest frame is not available for photons so that formula indeed is not valid for them . the other explanation is through the concept of relativistic mass . in that case the $m$ takes on the meaning of apparent mass because the faster the object goes the harder it will be to accelerate it ( because of finite speed of light ) . so formally one can still talk about the photon having a relativistic ( or effective ) mass $m = e/c^2$ . but this concept of mass runs in all kinds of problems so its usage is discouraged .
to formalize the comments as an answer : the difference between requiring $$ ( \alpha u , v ) =\alpha ( u , v ) \quad\text{ ( mathematician 's definition ) }$$ and $$\langle u , \alpha v\rangle=\alpha\langle u , v\rangle\qquad\quad\ , \ , \text{ ( physicist 's definition ) }$$ is purely one of convention , and the two definitions are equivalent as $ ( u , v ) =\langle v , u\rangle$ . there is no intrinsic reason to choose either , though if you work exclusively with one for long enough , you might come to regard the other as an abomination . in general , it is always advisable to keep an eye to which convention is being used . the physicist 's definition does have the advantage that it extends well to dirac notation , in the sense that matrix elements such as $\langle \phi|\hat{a}|\psi\rangle$ are linear in $\psi$ , so that the state $\hat{a}|\psi\rangle$ corresponds to the operator-acting-on-a-vector notation $av$ . if the bracket were linear in $\phi$ then we had have to make operators act to their left . this is again an ok convention but no one uses it .
chemistry is big . probably your interests are not that wide . for instance one may be interested in molecules/molecular processes such as chemical reactions or in soft stuff ( polymers , colloids , membranes , etc . ) one may seek for microscopic interpretations of the phenomena or just try to find how complex processes depend on macroscopic variables . still , without knowing any details , my bet is that as a physicist , if you master quantum mechanics , statistical mechanics and thermodynamics , then probably what you would like to read is a general textbook on physical chemistry ( the application of physics to chemical problems ) rather than a book on general chemistry . the topics mostly overlap except for a lot of descriptive material that you will be happy to skip , probably , but the p-chem book will be more focused and quantitative while using an understandable language . as a particular suggestion , you may try the not-very-well-known book " principles of physical chemistry " by kuhn and frsterling , rather than the best known p-chem books of atkins , levine and mcquarrie . other books that are worth of looking at in my opinion are : engel and reid , physical chemistry , and berry , rice and ross with the same title ( this one might be a bit too verbose ) .
well it has nothing to do with the higgs , but it is due to some deep facts in special relativity and quantum mechanics that are known about . unfortunately i do not know how to make the explanation really simple apart from relating some more basic facts . maybe this will help you , maybe not , but this is currently the most fundamental explanation known . it is hard to make this really compelling ( i.e. . , make it seem as inevitable as it is ) without the math : particles and forces are now understood to be the result of fields . quantum fields to be exact . a field is a mathematical object that takes a value at every point in space and at every moment of time . quantum fields are fields that carry energy and momentum and obey the rules of quantum mechanics . one consequence of quantum mechanics is that a quantum field carries energy in discrete " lumps " . we call these lumps particles . incidently this explains why all particles of the same type ( e . g . all electrons ) are identical : they are all lumps in the same field ( e . g . the electron field ) . the fields take values in different kinds of mathematical spaces that are classified by special relativity . the simplest is a scalar field . a scalar field is a simple number at every point in space and time . another possibility is a vector field : these assign to every point in space and time a vector ( an arrow with a magnitude and direction ) . there are more exotic possibilities too . the jargon term to classify them all is spin , which comes in units of one half . so you can have fields of spin $0 , \frac{1}{2} , 1 , \frac{3}{2} , 2 , \cdots$ . spin $0$ are the scalars and spin $1$ are the vectors . it turns out ( this is another consequence of relativity ) that particles with half integer spin ( $1/2 , 3/2 , \cdots$ ) obey the pauli exclusion principle . this means that no two identical particles with spin $1/2$ can occupy the same place . this means that these particles often behave like you expect classical particles to behave . we call these matter particles , and all the basic building blocks of the world ( electrons , quarks etc . ) are spin $1/2$ . on the other hand , integer spin particles obey bose-einstein statistics ( again a consequence of relativity ) . this means that these particles " like to be together , " and many of them can get together and build up large wavelike motions more analogous to classical fields than particles . these are the force fields ; the corresponding particles are the force carriers . examples : spin $0$ higgs , spin $1$ photons , weak force particles $w^\pm , z$ , and the strong force carriers the gluons , and spin $2$ the graviton , carrier of gravity . ( this fact and the previous one are called the spin-statistics theorem . ) now the interaction between two particles with " charges " $q_{1,2}$ goes like $\mp q_1 q_2$ for all the forces ( this is a consequence of quantum mechanics ) , but the sign is tricky to explain . because of special relativity , the interaction between a particle and a force carrier has to take a specific form depending on the spin of the force carrier ( this has to do with the way space and time are unified into a single thing called spacetime ) . for every unit of spin the force carrier has you have to bring in a minus sign ( this minus sign comes from a thing called the " metric " , which in relativity tells you how to compute distances in spacetime ; in particular it tells you how space and time are different and how they are similar ) . so for spin $0$ you get a $-$: like charges attract . for spin $1$ you get a $+$: like charges repel ! and for spin $2$ you get a $-$ again : like charges attract . now for gravity the " charge " is usually called mass , and all masses are positive . so you see gravity is universally attractive ! so ultimately this sign comes from the fact that photons carry one unit of spin and the fact that the interactions between photons and matter particles have to obey the rules of special relativity . notice the remarkable interplay of relativity and quantum mechanics at work . when put together these two principles are much more constraining than either of them individually ! indeed it is quite remarkable that they get along together at all . a poetic way to say it is the world is a delicate dance between these two partners . now why do atoms and molecules generally attract ? this is actually a more complicated question ! ; ) ( because many particles are involved . ) the force between atoms is the residual electrical force left over after the electrons and protons have nearly cancelled each other out . here 's how to think of it : the electrons in one atom are attracted to the nuclei of both atoms and at the same time repelled by the other electrons . so if the other electrons get pushed away a little bit there will be a slight imbalance of charge in the atom and after all the details are worked out this results in a net attractive force , called a dispersion force . there are various different kinds of dispersion forces ( london , van der waals , etc . ) depending on the details of the configuration of the atoms/molecules involved . but they are all basically due to residual electrostatic interactions . further reading : i recommend matt strassler 's pedagogical articles about particle physics and field theory . he does a great job at explaining things in an honest way with no or very little mathematics . the argument i went through above is covered in some capacity in just about every textbook on quantum field theory , but a particularly clear exposition along these lines ( with the math included ) is in zee 's quantum field theory in a nutshell . this is where i would recommend starting if you want to honestly learn this stuff , maths and all , but this is an advanced physics textbook ( despite being written in a wonderful , very accessible style ) so you need probably at least two years of an undergraduate physics major and a concerted effort to make headway in it .
the irreducible $su ( 3 ) $ representations of dynkin indices $ ( n , 0 ) $ are the $n-$ symmetric tensor powers of the fundamental representation . therefore let $e_1$ , $e_2$ , $e_3$ be an orthonormal basis of the fundamental representation space , then $ v_{ii} = e_i \otimes e_i $ $ v_{ij} = \frac{e_i \otimes e_j + e_j \otimes e_i }{\sqrt2}$ , $i \ne j$ identifying the fundamental representation basis with the weight vectors follows : $ e_1 = ( 1,0 ) $ $e_2 = ( -1,1 ) $ $e_3 = ( 0 , -1 ) $ , the weights of the $ ( 2,0 ) $ representation space can be obtained by inspection : $ v_{11} = ( 2,0 ) $ $ v_{22} = ( -2,2 ) $ $ v_{33} = ( 0 , -2 ) $ $ v_{12} = ( 0,1 ) $ $ v_{13} = ( 1 , -1 ) $ $ v_{23} = ( -1,0 ) $ there exist a lot of algorithms for the construction of group representations . one excellent reference is slansky 's seminal article : group theory for unified model building .
calling something aerodynamic is shorthand for calling it aerodynamically efficient . there is nothing wrong with referring to a system as thermodynamically efficient , it is just that this is not usually shortened to thermodynamic .
consider a potential , which approximately can be described by two harmonic oscillators with different base frequencies , for example ( working in dimensionless units ) $$u=1-e^{- ( x-4 ) ^2}-e^{-\left ( \frac{x+4}2\right ) ^2}$$ it will look like now let 's look at two lowest energy states of the hamiltonian $$h=-\frac1m \frac{\partial^2}{\partial x^2}+u , $$ taking for definiteness $m=50$ , so that the lowest energy states are sufficiently deep . now , at the origin of oscillator at left it can be shown to be $$u_l=\frac14 ( x+4 ) ^2+o ( ( x+4 ) ^4 ) , $$ and the for right one we will have $$u_r= ( x-4 ) ^2+o ( ( x-4 ) ^4 ) $$ if two lowest levels are sufficiently deep that their wavefunction do not overlap , then we can approximate them as eigenstates of each of the harmonic oscillators $u_l$ and $u_r$ . see how these two states look : you wanted to remove zero of the total energy by shifting the potential . of course , you could do this for a single oscillator . but now you have to select , which one to use . and if you select some , you will still get zero-point energy for another . thus , this trick is not really useful . it tries to just hide an essential feature of quantum harmonic oscillator and quantum states in general : in bound states there is lowest bound on energy , which can not be overcome by the quantum system , although classically the energy could be lower . zero-point energy is the difference between minimum total energy and infimum of potential energy . it can not be " dropped " by shifting the potential energy .
as photons have energy , gravity affects light rays , turning their path from straight to curved , and changing their energy ( frequency/color ) . in classical relativity light always travels in a straight zero-lenght line , with phase speed = $c$ . if you include gravity in your relativistic model you this is not true anymore . as this is an important property of the model that helps verifying that the theory is sound , instead of using a model where you cannot trust light going straight , you change your definition of " straight " so that it stays coherent with the fact that light moves in a straight line . that is where the " curved spacetime " comes from . so the logical process is not : i observe some strange physical phenomenon from which i conclude that the spacetime is curved i know that light goes straight , but the space is curved the light curves but instead : i observe light going around big masses but i know that light goes in straight lines i either have to change the idea that light goes in straight lines , or that the space is plain i prefer keeping in my model the property that light goes in straight lines , and from my biased point of view the space is curved this is consistent to the idea beyond special relativity , in which einstein decided that instead of preserving the rule that speed adds up while changing the system of reference preferred keeping the rule that the speed of light is the same in all ( inertial ) system of reference .
string theory should come with a proposal for an experiment , and make some predictions about the results of the experiment ; then we could check against the real results . if a theory cannot come with any predictions , then it will disprove itself little by little . . . the problem is that , with string theory , this is extremely difficult to do , and string theorists have year in front of them to go in that direction ; but if in 100 years we are still at the same status , then it would be a proof that string theory is unfruitful . . .
as best i can tell , what you are confused about is the fact that cup a ( weighing 100&nbsp ; g ) is floating in 50&nbsp ; g of water , while archimedes principle states that cup a ought to be displacing 100&nbsp ; g of water , which seems to contradict the fact that there is only 50&nbsp ; g of water available to displace . how can that be possible ? there is a subtle reason ; just because you have 50&nbsp ; g of water does not mean you can not effectively displace more than 50&nbsp ; g of water . this is probably best illustrated with a picture . here 's what the system looks like before cup b is dropped in : here 's what it looks like when you drop in cup b : the tricky thing is : cup b effectively displaced 100&nbsp ; g of water , even though there was only 50&nbsp ; g of water available to displace ! if it is not immediately obvious how it is that cup b is displacing 100&nbsp ; g of cup a 's water ( even though cup a only has 50&nbsp ; g of water ) , stare at diagram 2 for a while .
the point is that during the ordinary phase of the big bang expansion , the difference $|\omega-1|$ was rather dramatically increasing with time . today , we know that $|\omega-1|\lt 0.01$ or so . if we use the cosmological equations to reconstruct what $|\omega-1|$ had to be when the universe was a second old , or very young , we find out that the following inequality had to hold $$|\omega ( t=1\ , {\rm s} ) -1 |\lt 10^{-62} $$ or so . for some extended discussion what the flatness problem is and how the cosmic inflation solves it , see e.g. http://motls.blogspot.com/2014/03/alan-guth-and-inflation.html?m=1 now , if the universe did not have this fine-tuned $\omega$ when it was one second old , the density would immediately begin to diverge away from $\omega=1$ to either hopelessly high values ( implying big crunch ) or hopelessly low values ( implying sparse space without stars ) billions of years after the big bang . that is what the frw and similar equations  effectively , einstein 's equations ( the field equations from the general theory of relativity ) specialized for a uniform universe  imply . here everywhere , the cosmological principle is assumed , so the density $\omega$ is assumed to be basically uniform across the universe . even if you insist that it remains a uniform , a tiny change  by " one star or much less "  would have led to consequences incompatible with life . if you changed the average $\omega$ by changing $\omega$ " more dramatically " but only in a smaller region of the universe ( if you literally tried to remove a " seed " of what is a star today ) , the consequences for that region of the universe would be even more dramatic  it would have crashed or diluted or became incompatible with life much earlier . so indeed , the mass density one second after the big bang had to be adjusted with a much greater accuracy than " one missing star per visible universe " . i sort of vaguely understand why it may sound counterintuitive but there is really no paradox here . if the early universe were a consequence of the present , then it would mean that someone had to adjust the number of stars etc . with the precision better than "1 star per universe " , and it would be extremely unlikely or unnatural if not a downright inconsistency . but the point is that the events of the early universe are not a consequence of the present . it is exactly the other way around . the present and the stars around ( the " future" ) are a consequence of events in the past ( the " past" ) , including the events in the early universe . so it makes sense to ask what events or observations would be different if the early cosmology were not this exactly flat  the answer is that ( almost ) no stars would exist 14 billion years after the big bang , it does not make sense to ask how the early universe were different if the number of stars today were smaller by one  because the history of the early universe in no way depends on the number of stars today ( causality : future depends on the past , not the other way around ) . we do not have to fine-tune the number of stars in the visible universe today with an unrealistic accuracy in order to allow the young universe to exist . the young universe did whatever it did long before we were here and we can no longer change that !
we know how it would appear without any scattering medium , the experiment has been done : the large distance to the sun creates almost parallel rays of light and without scattering everything that is not directly hit by sunlight will be pitch black , such as the shadow of this astronaut . only the little bit of surface reflections from the dust are lighting up the front side of him .
i guess the simplest answer is just to carefully read you own words again . a reversible process is the one that can be made flow backwards . it is intuitive to think that it can be made flow backwards at any time we wish . but if the system were in a non-equilibrium state , one would need to wait a bit until it goes to equilibrium before trying to drive it back . so , it does not satisfy our desire to have the system under the control at any time .
collisions can be elastic or inelastic . elastic collisions are collisions where the incoming and outgoing kinetic energies are the same and only the angles change the same holds true classically , example : billiard balls ; and in the elementary particle framework . example : an electron hitting an electron has a probability of scattering elastically . billiard balls will turn some of the kinetic energy from linear to rotational when scattering , so completely elastic scatters are rare . inelastic scattering of billiard balls would involve deformation of the balls . at the quantum level if a scatter is not elastic , the energy changes form , example : atoms in a gas scattering off the electric field of each other , will lose kinetic energy into radiation , which generates the black body radiation and the eventual cooling of the gas . when elementary particles scatter inelastically new particles and radiation will be formed .
the photon cannot be split as one can split a nucleus . as it has zero mass it cannot decay . but it can interact with another particle lose part of its energy and thus change wavelength . it can be transmuted . have a look at the compton scattering entry in wikipedia . edit : intrigued by the other answers i searched and found that within special crystals " splits " can happen , if one defines as a split that there can come out two photons whose energy adds up to the original energy of the photon . so in a collective crystal photon interaction there exists such a probability .
let us denote by $\langle\cdot\rangle \equiv \int_0^t \cdot \frac{dt}{t}$ the averaging over time . please notice that : $$\frac{1}{l}\int_0^lv\mathrm{d}y=\frac{1}{\langle v\rangle t}\int_0^tv\frac{\mathrm{d}y}{\mathrm{d}t}\mathrm{d}t = \frac{1}{\langle v\rangle}\int_0^tv^2\frac{\mathrm{d}t}{t} = \frac{\langle v^2\rangle}{\langle v\rangle}$$ thus the inequality is given by : $\langle v\rangle \le \frac{\langle v^2\rangle}{\langle v\rangle}$ , or $\langle v\rangle^2 \le \langle v^2\rangle$ , which is indeed the the cauchy-schwarz inequality . thus averaging over time is not the same as averaging over the position . averaging with respect to two variables is the same if and only if these two variables are linearly dependent ( more generally related by an affine transformation ) . in our case , in general , the position is a nonlinear function of time , thus one obtains different results in averaging over time and position . the only case that these two averages are equal is the case of uniform motion with a constant velocity .
the point your book is trying to make is that the magnitude squared of the momentum four vector is invariant under a lorentz transformation . in other words , it has the same value no matter what frame you are in . i am writing the momentum four vector as $\left ( e , pc\right ) $ , in this case since the y and z momenta are zero in both frames . keeping in mind that minkowski space time is hyperbolic we have to write down the squared magnitude of the vector as $\left\vert\mathbf p^2 \right\vert = e^2-p^2c^2$ instead of as $\left\vert\mathbf p^2 \right\vert = e^2+p^2c^2$ . in other words , the pythagorean theorem for calculating magnitudes in hyperbolic space contains a difference of the square of the sides instead of the sum of the square of the sides it contains in euclidean space . this geometrically explains the minus in your formula above . now that we know the quantity is invariant with respect to the velocity of the frame , we choose the easiest two frames to calculate the necessary energy of the photon . in the frame at rest with respect to the produced particles , we get $\left\vert\mathbf p^2 \right\vert = \left ( 3mc^2\right ) ^2$ . in the frame where the initial electron was at rest , we get , as you noted above , $\left\vert\mathbf p^2 \right\vert = \left ( e_\gamma + mc^2\right ) ^2 - \left ( \frac{e_\gamma c}{c}\right ) ^2$ . setting these two terms equal to each other and solving for $e_\gamma$ gives $e_\gamma = 4mc^2$ . i do not have the book , but i believe the point of the exercise was to show that using the invariance of the momentum four vector results in an easier calculation , both conceptually and mathematically , compared to using the associated lorentz transforms between frames .
i complete analogy with classical mechanics : we define the proper velocity : $$ \eta ^\mu :=\frac{dx^\mu}{d\tau} , $$ where $\tau$ is proper time . we likewise define ( relativistic ) momentum : $$ p^\mu :=m\eta ^\mu . $$ and finally we define the ( relativistic ) energy ( up to multiples of $c$ ) as the time-component of $p^\mu$ . this happens to be $$ \frac{mc^2}{\sqrt{1- ( v/c ) ^2}} , $$ which obviously must be positive . thus , in order to be consistent with our relativistic definition of energy , we can not have particles with negative energy . this almost makes it tautological , but it is straightforward and precise .
as @georg puts it , the force is $mg\sin{\theta}$ , but then for small $\theta$ one can assume $\sin{\theta} = \theta$ . so , the force becomes , $mg\theta$ . and regarding the dimensions , $\sin{\theta}$ and $\theta$ are dimension less quantities . so dimensionally , $mg$ and $mg\sin{\theta}$ are same $\left [ kg \cdot m \cdot s^{-2} \right ] $ .
how triangle law**and **parallelogram law of addition of vectors are different ? ain't they . they are not different . they are the same thing . there is only one real law which is the head to tail rule . when adding any number vectors put head to tail , head to tail , head to tail . . . until all the vectors are used up and then draw a line from the tail to the head and that is your resultant vector . the parallelogram law is essentially just using the triangle law twice in a different order , and they both get the same answer because order does not matter i.e. $\vec a+ \vec b = \vec b + \vec a$ . here shows the addition of three vectors , the " triangle " rule has been used twice in a row to get $\vec a+ \vec b + \vec c = \vec r$ . if yo tried to use the parallelogram to get this answer the resulting diagram would be a mess of repeatedly writing the same vector and over .
it seems you are using " can " in two different ways here , one as it is intended and one in the sense of " may " . we may use the energy being constant to calculate frequency because energy is conserved . we are using that observed fact/postulate in our calculation . we can use energy to calculate frequency because it works . it is a productive way to solve the equation .
as an alternate answer using parallel axis theorem , note that the inertia tensor of a rod pointing in the $y$-direction rotating about its center is $$\mathbf{i}_\text{rod , y}=\left ( \begin{array}{ccc} \frac{b^3 \rho }{12} and 0 and 0 \\ 0 and 0 and 0 \\ 0 and 0 and \frac{b^3 \rho }{12} \\ \end{array} \right ) $$ and similar for the $x$ and $z$-pointing rods , so the unshifted inertia tensor for the whole system becomes $\mathbf{i}_c=\mathbf{i}_\text{rod , x}+\mathbf{i}_\text{rod , y}+\mathbf{i}_\text{rod , z}=\frac{1}{6}\rho b^3\mathbf{e}$ where $\mathbf{e}$ is the identity matrix . meanwhile the translation component is $$\mathbf{i}_t=\sum_{j=1}^3b\rho ( ( \mathbf{r}_j\cdot\mathbf{r}_j ) \mathbf{e}-\mathbf{r}_j\otimes\mathbf{r}_j ) $$ where $\mathbf{r}_j$ are the centers of the rods , giving the inertia tensor $$\mathbf{i}=\mathbf{i}_c+\mathbf{i}_t=\left ( \begin{array}{ccc} \frac{11 b^3 \rho }{3} and -\frac{b^3 \rho }{2} and -\frac{b^3 \rho }{2} \\ -\frac{b^3 \rho }{2} and \frac{5 b^3 \rho }{3} and -\frac{3 b^3 \rho }{2} \\ -\frac{b^3 \rho }{2} and -\frac{3 b^3 \rho }{2} and \frac{8 b^3 \rho }{3} \\ \end{array} \right ) $$ which was obtained using the following code : from this , the angular momentum and energy can be computed via $\mathbf{l}=\mathbf{i}\boldsymbol{\omega}$ and $t=\frac{1}{2}\boldsymbol{\omega}\mathbf{i}\boldsymbol{\omega}$ .
looks like you are already familiar with the classical explanation but are still curious about the quantum version of it . 2 . phase difference between absorbed and emitted light yeah , this is essentially the lowest order contribution to the phase shift in the photon-electron scattering . here is the sloppy way to visualize it continuously ( this is basically the ' classical em wave scattering ' point of view ) : you can imagine that the " kinetic energy " ( -> frequency ) of the " photon " increases as it approaches the atom 's potential well and then it goes back to its normal frequency upon leaving the atom . this translates to a net increase in the phase ( $ ( n-1 ) \omega/c$ ) . " drift velocity " of photons ( they are not the same photons , they are re-emitted all the time ) by " drift velocity " do you mean a pinball-like , zigzag motion of the photon ? this will not contribute that much because it requires more scattering ( basically it is a higher order process ) . and also , i still do not really understand about the detail of the absorption-emission process . yes the absorption will still occur in all range of the frequency . the hamiltonian of the atom will be modified by the field ( by $- p \cdot e$ where p is the dipole moment of the atom and e is the electric field component of the light ) . this will give us the required energy level to absorb the photon momentarily , which will be re-emitted again by stimulated+spontaneous emission . edit : clarification , the term ' energy level ' is misleading , since the temporarily ' excited ' atom is not in an actual energy eigenstate . see the diagram here : http://en.wikipedia.org/wiki/raman_scattering
in qft any massive particle with a spin $\ge$ 1 will be non-renormalisable . even a w boson would be non-renormalisable except of course that it acquires mass through the higgs mechanism and is massless above the weak transition energy . and this is the point . a massive particle of spin $\ge$ 1 ( or a massless particle of spin > 1 ) will be non-renormalisable unless some other physics intervenes before the interactions become infinite . in the case of the w boson the higgs mechanism intervenes . for a massive spin 2 composite particle there will be some energy at which the particle breaks up . for an elementary spin 2 particle ( massive or otherwise ) a qft description will never be renormalisable .
i think you mean to ask whether a photon can accelerate . the answer is yes . for instance when travelling from one medium to another light changes speeds . in fact , the actual photon is abosrbed and a new photon is reradiated during this process . however , you could call this effectively light acceleration
there is a lorentz transformation that maps a spacelike vector $u$ to $-u$ . if $a ( x ) $ is a field of spin 1 with $\langle u \cdot a ( 0 ) \rangle = c$ then applying the lorentz transform we find $-c=c$ and hence $c=0$ . doing this for all spacelike vectors implies $\langle a ( 0 ) \rangle = 0$ , and translation invariance then gives $\langle a ( x ) \rangle =0$ for all $x$ . for other spins the argument is similar . you are welcome to try the spinor case as an exercise .
a frame of reference means a co-ordinate system and an observer is someone using that co-ordinate system . for example i could define cartesian co-ordinates ( t , x , y , z ) and define myself to be at the point ( 0 , 0 , 0 , 0 ) and since i am always at the origin that means i am at rest . so when i measure your position and speed i am the observer and i get some value in my frame i.e. my co-ordinate system . the distinction between frame of reference and co-ordinate system may seem rather minor , but when you get on to general relativity you will realise the importance of considering co-ordinate systems . for example when calculating the properties of black holes we commonly use three co-ordinate systems , schwarzschild co-ordinates , shell co-ordinates and freely falling co-ordinates . none of these would normally be described as a " frame " . in physics a gauge has a rather special meaning . it is a reference point used for some set of co-ordinates . for example suppose i want to measure the height of mount everest , the question is what should the height be relative to ? generally we had measure the height above sea level , but you might just as well measure the height relative to the centre of the earth . the reference point we choose defines our gauge , and in choosing the reference point we are fixing our gauge . gauge invariance is a very important part of modern physics . the height of mountains is a rather trivial example , but it is hard to find an area of physics that gauge invariance does not affect . see http://en.wikipedia.org/wiki/gauge_fixing for more info on gauges , but be warned that article is a bit technical .
i think that singularities can occur in the electromagnetic fields ( other than the ones occurring at the location of particles ) , if one allows the particles to move at speed $c$ . that is , one can arrange for sonic booms or cherenkov radiation . this raises the question " is cherenkov radiation smooth ? " i suspect that the non smoothness only occurs at distances larger than the distance between the individual atoms ; that is , that the lack of smoothness is in the math approximation rather than the physics . in any case , if one does not allow the particles to approach the speed of light , i believe that the sum of all their electromagnetic fields will be a sum of smooth functions and so will be mathematically smooth .
as you lift the pulley , the masses stay on the ground until the string is taut . as the pulley is massless , you accelerate upward infinitely fast . it is better to start the simulation with the string taut . now you have twice the tension in the string pulling down on the pulley and $f$ pulling up on the pulley . physically , one of three things can happen , depending on $f$ . if $f$ is small , you will not lift either mass off the ground , the tension in the string will be $\frac f2$ and masses will be motionless . if $f$ is somewhat larger , the heavy mass will stay on the ground , the pulley will accelerate upward , and the light mass will accelerate upward twice as fast as the pulley . if $f$ is larger yet , both masses will lift off the ground . the pulley will accelerate upward at $\frac f{m_a+m_b}$ . the lighter mass will accelerate upward faster than the heavy one .
in a dc motor , when the armature rotates its coils cut the magnetic and induce a voltage in the coils . this voltage is of opposite polarity of the voltage that is powering the motor ( the battery ) and is called the back emf . it is modeled as a voltage source that is proportional to the speed of the motor times a constant . the faster the motor is rotating the higher the back emf . the power of the motor is not constant and neither is the energy . the power into a motor is just the voltage measured at the lead wires times the current in the lead wires ( p=v*i ) . the output power of the motor is just the speed of the motor times the torque . the power will be zero when there is zero torque and it will be zero when there is zero speed . in between those two points , the power will increase , peak , and then return to zero . in general , on a dc motor , speed is proportional to voltage and torque is proportional to current . as your load increases and you need more torque , the motor will draw more current . if you need to speed your motor up or slow it down , you need to raise or lower your voltage .
you must have learnt about the principle that : pressure exerted by a fluid is equal for all points at a certain depth . that is where you have gone a bit wrong . if you consider two points in the fluid just below the surface - one inside the straw , and one outside , it is the pressure at these two points which will be equal , not the force ! secondly , you have considered the total force acting on the outside area , and then said that this total force will act upwards on the tiny portion inside the straw . is this true ? no ! this is where your mistake lies . that whole force will not act on the straw ! even though that is the total force acting on the outside-area , it is not going to be translated throughout the liquid . it is the pressure which is going to be equal at those points . so you can not simply subtract the forces and say that since one is substantially greater than the other , there will not be equilibrium . if you equate the pressures , you will see that it is completely in sync . hope it is clear ! :d
i am sure emilio pisanty 's answer is fine , +1 , but it goes a little over my head . it also appeals to specific properties of electromagnetic waves , whereas the ray approximation is much more general than that . here 's a simpler plausiblity argument that may be more at the level that the op can understand . if you diffract a wave through a slit of width $w$ , you get a diffraction pattern with an angular width $\theta$ of order $\lambda/w$ ( in radians ) . when $\lambda$ is small compared to $w$ , $\theta$ gets small . in the limit where $\lambda/w\rightarrow0$ , $\theta\rightarrow0$ , and you have a ray coming through the slit . different types of waves ( water waves , sound waves , light waves , . . . ) will have different details relating to things like polarization , but none of that affects the above argument . if it is so , it should obey to the wave equation and this does not seem to me to describe a straight line ray . right . a perfectly collimated , parallel wave train can never be a solution of the wave equation . however , a diffraction pattern with a very small angular width can be a solution , and if the width is small enough , it is indistinguishable from a parallel wave train .
the trajectories are worldlines in 4d spacetime , not just paths in 3d space . they maximise proper time along the trajectory . they do not minimise distance along the curve traced through space . an object tracing a fixed path in space at different speeds would trace different worldlines in space-time .
water and air are both weakly magnetic , with magnetic permeabilities on the order of $10^{-6}$ h/m whereas iron , a strongly magnetic material , has a permeability of about 0.25 h/m . thus , if you could create the supercavitation in water , a magnetic field would do just about nothing to maintain the cavity .
yup . just a special instance of gravitational redshift , which has been tested with atomic clocks at different elevations or flown on airplanes , precision clocks in orbit , etc . relativistic effects of anomalies due to lower rock density or underground voids are not any different from the effects of just being at a higher elevation . just one thing to clarify , though , since it is commonly misunderstood , even if it is clear to you . time still passes at one second per second over there just as it does here or anywhere . if someone is going to live for exactly 100.00000 years , they will live for 100.00000 years on mt . everest if they are born there and stay there all the time , and they will live for 100.00000 years at the bottom of the ocean if they spend their whole life there . it is only in the comparison of clocks that start together , travel apart , then come together again , that we find 100.00000 years on mt . everest does not match 100.00000 in the deep . this is the famous twin " paradox " . alternatively , an observer by one clock could watch the other clock with binoculars , and see it ticking faster or slower . this is redshift , and can be due to motion or due to differences in gravitational potential . so yes , someone near hudson bay area can tell the boss " i will get it done in an hour " , rush over to where the anomaly is strongest , and have a leisurely 1.0000000003 hours to finish up that report . by the precision stopwatch the boss uses , 1.0000000000 hours have gone by when the report lands on his desk . this is assuming miraculous travel times . ( made up numbers . exact calculation = exercise for the student ! ) there is plenty of reading on the web about gravimetry and the geoid - an accurate detailed shape of the earth based on gravitational potential , for background to understanding the funny business at hudson bay .
actually , the quark and antiquark do annihilate with each other . it just takes some amount of time for them to do so . the actual time that it takes for any given pion is random , and follows an exponential distribution , but the average time it takes is $8.4\times 10^{-17}\ , \mathrm{s}$ according to wikipedia , which we call the lifetime of the neutral pion . what you have learned is a simplification , in fact ( it pretty much always is in physics ) . the actual state of a pion is a linear combination of the up state and the down state , $$\frac{1}{\sqrt{2}} ( u\bar{u} - d\bar{d} ) $$ this is how it is able to be its own antiparticle : there are not separate up and down versions of the neutral pion . each one is a combination of both flavors . the orthogonal linear combination , $$\frac{1}{\sqrt{2}} ( u\bar{u} + d\bar{d} ) $$ does not correspond to a real particle . ( in a sense it " contributes " to the $\eta$ and $\eta&#39 ; $ mesons , but i will not go into detail on that . )
the general definition for a current ( electrical , matter , energy , probability ) is as follows : consider a small area $\def\ds{\mathrm d\boldsymbol s}\ds$ through which a quantity $q$ flows . we call $\def\j{\boldsymbol j}\j_q$ the $q$-current density , i.e. the current density of $q$ . during a small time $\delta t$ the total of the quantity $q$ flowing through $\ds$ is $$\mathrm \delta q=\j_q\cdot\ds\ , \delta t . \tag{1}$$ now we consider a wire of section $\ds=\mathrm d s\ , \mathbf e_x$ . for charges $+q$ moving with velocity $\def\v{\boldsymbol v}\v=v\mathbf e_x$ the current density multiplied by the wire 's section is the current $$i=\frac{\delta q}{\delta t}=\j_q\cdot\ds=qv . $$ let us examine the three cases you mention . two charges of opposite sign $$i=\j_q\cdot\ds=\bigl ( q\v+ ( -q ) \v\bigr ) \ds=0 . $$ the total charge is zero : no current . this means that opposite charges in same directions create opposite currents . same charges with opposite velocity $$i=\j_q\cdot\ds=\bigl ( q\v+q ( -\v ) \bigr ) \cdot\ds=0 . $$ the charges moving in opposite direction create opposite currents as well . two opposite charges in opposite directions $$i=\j_q\cdot\ds=\bigl ( q\v+ ( -q ) ( -\v ) \bigr ) \cdot\ds=2qv . $$ opposite charges in opposite directions create the same current , because of $ ( -1 ) \times ( -1 ) =1$ .
fire is neither . fire is a process involving both . fire is the energetic combination of various substances with oxygen to release light and heat . in a gas fire , such as might be found on a stove or in a heater , a light hydrocarbon such propane is broken down into components of hydrogen and carbon which unite with oxygen from the atmosphere to form water and carbon dioxide . in an ordinary wood fire , heat causes the wood to break down and release hydrocarbons which then burn as propane would .
a field is usually stated as " force per mass " , where mass is weight , charge or ( in the old days ) , a magnetic mass . so one could set field = force / charge = newtons per coulomb . field = gradient of potential = volt per metre . in the first case , one gets a force when a test charge of q coulombs is placed at a point in the field , then f = eq . the equation value f = eq , applied to 10^-19 c * 10^-5 n/c = 10^-24 c , if you want to get an answer of 10^-14 , then you really need to get 10+5 v/m . typos are not unknown in texts .
when you look at a surface like sand , bricks , etc , the light you are seeing is reflected by diffuse reflection . with a flat surface like a mirror , light falling on the surface is reflected back at the same angle it hit the surface ( specular reflection ) and you see a mirror image of the light falling on the surface . however a material like sand is basically lots of small grains of glass , and light is reflected at all the surfaces of the grains . the result is that the light falling on the sand gets reflected back in effectively random directions and the reflected light just looks white . the reflection comes from the refractive index mismatch at the boundary between between air ( n = 1.004 ) and sand ( n $\approx$ 1.54 ) . light is reflected from any refractive index change . so suppose you filled the spaces between the sand grains with a liquid of refractive index 1.54 . if you did this there would no longer be a refractive index change when light crossed the boundary between the liquid and the oil , so no light would be reflected . the result would be that the sand/liquid would be transparent . and this is the reason behind the darkening you see when you add water to sand . the refractive index of water ( n = 1.33 ) is less than sand , so you still get some reflection . however the reflection from a water/sand boundary is a lot less than from an air/sand boundary because the refractive index change is less . the reason that sand gets darker when you add water to it is simply that there is a lot less light reflected . the same applies to brick , cloth , etc . if you look at a lot of material close up you find they are actually transparent . for example cloth is made from cotton or man made fibres , and if you look at a single fibre under a microscope you will find you can see through it . the reason the materials are opaque is purely down to reflection at the air/material boundaries .
an absorption grating is a grating , where the parallel bars are absorbing . this is in contrast to a reflection grating , where the bars would be reflecting , and a phase grating , where the bars are transmissive , but will change the phase of the incident waves . in general , physical gratings can ( and usually will ) introduce combinations of these three effects .
wrt the second paragraph : the inertial observer will see an object ( the walker ) moving in a fixed radius circle with a lower tangential ( and thus lower angular ) velocity , than was seen when the walker was not walking . . . this in turn would reduce the centripetal force required to keep the walker moving in his slower , fixed radius circle , and this would be reflected in the lower radial inward friction force between the walker 's shoes and the floor of the merry-go-round . the walker , who , while not walking , had been leaning towards the center of the circle , could walk more upright . this leads to a novel way of commuting in a large , wheel-type space habitat . the commuter faces against the rotation of the wheel and takes enough running steps to cancel the wheel 's tangential velocity . then he simply lifts his feet and floats in space until his destination comes along ,
how about a description in words instead ? i think the mathematics is not the point so much . other answers lay out the mechanics in detail . the short answer is that the theory has infinitely many divergences which cannot be absorbed by fixing finitely many couplings to their observed values . this means it is not predictive , or in more fancy words does not provide algorithmic compression : the amount of input it requires equals the amount of output it produces . but the talk about divergences masks the physics of the situation , which i will try to describe : any theory has couplings which describe the possible interactions of the system . when you probe the system at different length scales , or energy scales , those couplings change in a calculable way . this is the process of renormalization , which fundamentally has nothing to do with infinities . renormalizable theory is one that can be continued to high energies , or short distances , without encountering any difficulties . this means all couplings remain small , and all calculations are reliable . in non-renormalizable theories , the higher energies you probe the system , the stronger the couplings become , at some stage they become infinite . this means that you cannot do reliable calculations , which is normally an indication that there are some physical effects that you are missing . there are many examples where such physical effects are well-understood , for example new degrees of freedom ( which are not visible at low energies ) become important at those energy scales . in gravity , when you calculate in perturbation theory , for small perturbations around flat space , it looks like the coupling becomes strong and something is missing . the mechanics of seeing this is straightforward and not that illuminating . perhaps one intuitive way to understand this is that in gravity the interaction coupling is governed by the energy , so almost by definition it grows with energy . . . most likely scenario is that we need new degrees of freedom to complete the theory at short distances . one suggestion for " uv completion " is string theory . there is another scenario which may apply to gravity . once the couplings become strong , you cannot any more reliably calculate anything , and that includes their scale dependence . so it is a logical possibility that they will stabilize at some finite value , and do not go all the way to infinity . this scenario is called asymptotic safety . in my mind this is very unlikely , and there is no indication* this scenario applies to gravity , but it remains a logical possibility . ( *i know about the various claims , this is a statement of my personal opinion . )
it is not a gauge transformation , it is a field redefinition . srednicki gives an example of this in exercise 10.5 . in this exercise , a free field theory is turned into what looks like an interacting field theory by a field redefinition , however in perturbation theory , the scattering amplitudes vanish , confirming that the physics has not changed . i suspect you will find the same here ( though i have not done it ! ) - if you compute scattering amplitudes for the 3-way vertices represented by the cubic terms resulting from this field redefinition , they should cancel .
how in general can we identify whether something is a primitive concept , a fundamental law of physics , a definition or a theorem ? you can not , because it depends completely on context and on the taste of whoever has written the presentation you are using . for example , some people like to consider gauss 's law to be a definition of charge , some people call it a law of physics , and some people call it a theorem . none of these choices are right or wrong in and of themselves . here 's an example from mathematics . you can take euclid 's postulates and prove the pythagorean theorem . but you can also take euclid 's first four postulates , get rid of the parallel postulate , and add in the pythagorean " theorem " as a postulate . then the parallel postulate becomes a theorem . so applying this to physics , nobody can say whether the pythagorean theorem is a law ( in the sense that it is an empirical fact about how space behaves ) or a theorem . the same thing happens with primitive notions . for example , a line can be a primitive notion ( as in euclid ) or a defined one ( in the cartesian approach where you define it as a set of points ) .
it is because the force at work here ( gravity ) is also dependent on the mass gravity acts on a body with mass m with $$f = mg$$ you will plug this in to $$f=ma$$ and you get $$ma = mg$$ $$a = g$$ and this is true for all bodies no matter what the mass is . since they are accelerated the same and start with the same initial conditions ( at rest and dropped from a height h ) they will hit the floor at the same time . this is a peculiar aspect of gravity and underlying this is the equality of inertial mass and gravitational mass ( here only the ratio must be the same for this to be true but einstein later showed that they are really the same , i.e. the ratio is 1 )
you are quite correct that two isolated black holes would simply orbit each other until over a very very long timescale gravity wave radiation would cause them to coalesce . but galactic black holes can change their momentum by flinging stars about . the net result is not that different to the way energy is dissipated in a viscous liquid . the black holes will interact with the stars around them and lose energy in the process until they merge . there are lots of videos modelling galaxy mergers on youtube . i particularly like this one . note how stars are flung away from the galaxies as they collide .
both seem a little strange . so let us see where they are coming from . what we know is from the laws of thermodynamics that the total differential of the internal energy $u$ can be written as $$du = \delta q + dw$$ here $\delta q$ is an infinitesimal amound of thermal energy and $dw$ an infinitesimal amount of work . the work is the product of a generalized displacement times a generalized force , which are together a conjugated pair of variables . one such pair is pressure and volume , $p\ , dv$ . for a reversible process $\delta q = t ds$ , so that we get $$du = t\ , ds - p\ , dv$$ from here we easily arrive at your second equation : $$ \frac{du}{t} + \frac{p}{t} dv = ds$$ now we compare with your first equation , which can only be true if$$\frac{p}{t} = \left ( \frac{\partial p}{\partial t}\right ) _v$$ which certainly does not hold for all thermodynamic systems . so your first equation is more general and the second one only correct in specific circumstances . we rearrange the condition $$p ( t , v ) = \left ( \frac{\partial p}{\partial t}\right ) _v t$$ which can only be satisfied by a general function $f ( v ) $ and $$p ( t , v ) = f ( v ) \ , t$$
this article on triboelectricity gives a triboelectric series , listing materials according to whether they will expel electrons or acquire them , when rubbed or in contact . towel is not in there : ) but cotton is , with 0 but it is more positive than rubber . so the answer is that the electrons will move to the balloon , if you manage to get cotton to give up its electrons : ) . did you do the experiment ? a woolen towel would be much better . maybe it was your hand that gave up the electrons after all . this is what wikipedia has to say on the causes : after coming into contact , a chemical bond is formed between some parts of the two surfaces , called adhesion , and charges move from one material to the other to equalize their electrochemical potential . this is what creates the net charge imbalance between the objects . when separated , some of the bonded atoms have a tendency to keep extra electrons , and some a tendency to give them away , though the imbalance will be partially destroyed by tunneling or electrical breakdown ( usually corona discharge ) . in addition , some materials may exchange ions of differing mobility , or exchange charged fragments of larger molecules . hence the need of a table .
you have taken the right approach by summing the forces and setting that equal to mass times acceleration . for the y-component you have included a $k$ $$ $\delta{l}$ which should not be there . in the x-direction , this term represents the force contribution from the bottom spring ( the horizontal one ) , however this spring does not contribute at all to forces in the y-direction ( because it lies completely in the x-direction ) thus has no effect on the y-axis equation . also , in the diagram you provided , the y-axis points down . you wrote your equations as though the y-axis points up , and this has caused a sign error . gravity points down along the positive y-direction , thus the $mg$ term should be positive . an extension of the spring by $\delta$$l$ leads to a restoring force that will pull point a in the negative y-direction , thus the $k$ $$ $\delta{l}$ $$ $sin ( 60 ) $ term must have a minus sign .
just look , in bardeen-horowitz , how you get $2.9$ from $2.6$ ( these correspond to your 2 expressions ) , from $2.7$ and $2.8$
reading your question and the comments beneath , your use of the term " particle function " has no real meaning . i think i understand what you are asking , something like . . . we have proved that waves ( light ) can act as a particles through the double slit experiment . is there a method to prove that matter ( particles ) can act an a wavelike manner ? the answer of course is yes ! we have used the particle equivalent of the ds expt . which is electron diffraction . we can shoot a beam of electrons through a diffracting crystal ( the lattice of the crystal acts as the slits ) and we are able to see a diffraction pattern : apologies if this is not the question you are asking .
kinetic energy need not be carried by any special carrier particle . kinetic energy is inherent in any moving body or particle . if you look closely at the conservation of energy/work energy theorem , all you get is that $\delta ( \frac12mv^2 ) =\int \vec f\cdot d\vec x$ the left hand side is ( change in ) " kinetic energy " . the right hand side is change is potential energy . calling both of these terms as energy is a convenience that makes it cleaner to state the conservation of energy in terms of one quantity ( $e$ ) . fundamentally , kinetic energy is inherent in any moving body . on the other hand , forces are caused by exchange of gauge bosons , so potential energy is in the end " mediated " by various particles ( like the photon ) . i would not go so afar as to say that it is " carried " by particles , though . the forces are mediated by the particles ; but the potential energy is still a property of the system . springs work on the basis of electromagnetic energy . ( all forces like the stress reaction force , forces between materials when you push two together , etc come from electrostatic repulsion between electrons in atoms . ) photons carrying heat energy and photons mediating the em force are very , very different . when you transfer heat , the kinetic ( vibrational ) energy of the body is manifested in the form of an emitted photon . all that energy is now the kinetic energy of the photon . the photon finally bumps into some other material and is absorbed , turning into electrostatic or kinetic energy . all photons here are real photons . on the other hand , the electromagnetic force ( and the other fundamental forces ) is generally mediated by virtual particles ( photons ) . these live on borrowed energy , and do not contribute to the potential energy of an interaction . the energy of the interaction is part of the system , not really tied to any individual entity .
thrust reversers are designed to reflect the air that hits them . the air does not simply leave the engine and stop when it hits the reverser . instead the air is reflected and travels forward past the engine . the end result is equivalent to turning the engine round so it blows air forwards rather than backwards . to use your example of the fan and boat : in this example you normally assume the air starts stationary , is accelerated by the fan , stopped by the sail and ends up stationary again . that is why there is no net force on the boat . however if the sail reflected the air from the fan instead of stopping it , the air would start stationary and end up moving backwards and there would be a net force on the boat .
strings are not described very accurately in popular science , because much of the physics of strings was only understood long after the mathematical theory was somewhat advanced , and an accurate classical analog for the string was not available until relatively recently . the classical analog people often use is a vibrating band of energy , but this is mostly wrong , because strings do not interact by bumping into each other , like rubber bands do . they can only interact in a strange way determined by the consistency of having infinitely many different particles , all conspiring to make a consistent theory . this conspiracy makes it that when strings merge , they do a complicated thing , and the only correct classical analog to this complicated thing is a black hole merger . this was not understood very well until at least the late 1990s , so pop-sci has not caught up . ordinary astrophysical black holes are big , and complicated , and black , and gooey . they are electrically resistive , their oscillations decay quickly , they are very irreversible in the statistical sense . the string describes the case that the black hole is extended and charged so much that it is in the non-viscous extremal limit , so that strings are not gooey and lossy like ordinary black holes , but shiny and reversible . the one dimensional shiny black holes are the strings , in the limit that these black holes are weakly interacting , so that the lightest ones are light , and consequently weakly charged ( because extremality relates charge and mass ) . when black holes collide , the oscillations do not combine in a simple way , they combine the way black holes combine , in a strange acausal way that only makes sense teleologically . so you can not say " this pattern made this pattern " , at least not in a causal way precisely , you need to describe the whole thing at once . the ripples running along the string does not add to the ripple running along another string when they combine , but if the two combine at high energies to make a long string with many ripples running in a statistical way , so that it has a classical interpretation , they combine to an object which is gooey because it has a lot of junk running around on it , and this is no different than a viscous liquid . when the strings cool down again by shooting out other cold strings , the oscillations die down . the laws of combination are not like superposing waves on a pond , but they are described over the entire space-time world-sheet . strings are more elementary than black holes , they are small and simple . you should not be intimidated by the above to thinking that the laws of string collisions require you to understand the classical collisions of black holes ! this is no more true than saying that describing a collision in the born approximation in quantum mechanics requires you to solve the much more difficult classical problem of particles scattering around in that potential . the quantum behavior is simpler than the classical behavior in many cases . the laws of string collisions , for those strings which are lightest at the lowest energies , that is for those distance scales which reproduce our experience , are extremely simple : they just reproduce the laws of feynman diagrams , so that the particles combine into other particles the same way they do in the standard model . on the string world sheet , these laws of particle combination are the laws of algebraic products of operators , they describe how to expand a product of two operators in an infinite series of a third operator . this process takes a limit where the points of merger of the incoming particles are smooshed close together by a conformal transformation , so that each of their oscillations is no longer distinct , but merged into a combined oscillation a long long time ago ( the collision limit is not really a short-distance limit , but a long-time limit ) . the combined oscillation just looks like an infinite series of particles in the theory , an infinite series of operators on the world sheet which create the oscillation corresponding to sending in one of these particles from infinity . this is just like a taylor expansion , except for fluctuating quantities , and the operator product laws are the laws of string merger and vibration-adding . you can not make nothing , because you always have a world sheet , and the addition law is strange , not by the laws of superposition ( like water waves , or oscillations on rubber bands ) but by the rules of operator product expansion .
there is a simple general argument for why you get small-scale motion from large-scale motion in any nonlinear nonintegrable continuous mechanical system , whether it is fluids , or electromagnetic waves interacting with charged plasmas , or surface waves on water , or anything nonlinear at all . this argument must break down for those special cases where the usual turbulence does not occur , like 2d fluids . the reason is the ultraviolet catastrophe--- the idea that to get to thermal equilibrium , all modes have to have the same amount of energy . any mechanical system is only in statistical equilibrium when all its modes have about the same amount of energy . this boltzmann equilibrium is therefore unattainable for smooth motions of continuous fields , because it requires that you divide a finite energy between infinitely many modes , most of which involve very short wavelengths . the finite-energy statistical equilibrium for any continuous field is then a zero temperature state where all modes contain an infinitesimal amount of energy , which is the partition of the initial energy . the energy cascade is the method by which a fluid tries to do the paritition , by sending energy down into short wavelength modes in a random looking way , to get closer to the statistical equilibrium state . since this is impossible , you just get a continuous draining of energy from long-wavelength motion to short wavelength motion , and when this draining process reaches a scale-invariant steady state , we call the situation isotropic homogenous turbulence . there is always damping in a physical system , and damping drains energy into molecular motion in one step , not by a nonlinear cascade , just by thermodynamically converting the energy to heat . this one-step process is only relevant in fluids at short wavelengths , because it goes like the gradient of the velocity . a uniform velocity carries energy , but by galilean invariance , it has no dissipative damping . because of galilean invariance , in fluids there is an arbitrarily large separation of scales between the distance scales where nonlinearity is important and the much smaller scales where damping is important . inbetween , you get a regular nonlinear mixing which drains energy from long wavelength modes to short wavelength modes , without significant damping , and in a statistically random way , because any tiny perturbation to the long-wavelength modes will produces a completely different short wavelength modes , because the process is spreading out into the much large phase space of the short wavelength modes , in an unstable , chaotic , way . one dimension this argument only fails in certain special cases . it fails generically in 1+1 dimensions , when space is a line , because in a line there are only two modes at any wavenumber k . you still have an inaccessible boltzmann state , because there are infinitely many k , but there is no growth in the number of modes with k , as there is in 2d and higher . so energy is just as likely to move to smaller k as to larger k , and if you have turbulence , it is more like energy diffusion , where the energy random walks from smaller to larger k , without any particular reason to get to larger k except if it randomly happens to get there . this means that you can easily have energy in a certain number of k modes bound up in a closed motion , and this is reflected in the fact that thermalization in homogenous 1d systems with local nonlinear interactions is difficult . you run into a lot of soliton solutions , and other special states , where the energy just refuses to get random , but is nonlinearly shared in a non-thermal way between a bunch of low-k modes . this was discovered by fermi pasta and ulam , when they tried to simulate the approach to statistical equilibrium in a 1d system using an early computer . instead of thermalization , they discovered that their model never reached equilibrium , and this was a major motivation for the study of one dimensional integrable systems . but 1d systems , as interesting as they are , are rare . this sort of nonsense does not happen often in 2d and above , because there are just so many more modes at high k . the number of modes grows as $k^{d-1}$ . extra conserved quantities nevertheless , there is still no downward cascade for the special case of 2d fluid turbulence . the reason there is that there is a second continuous integral conserved quantity in this special case , the enstropy , which is the square of the curl of the 2d velocity . $$ s = \int ( \partial_y v_x - \partial_x v_y ) ^2 dx dy$$ this quantity has more derivatives than the energy , which is just $e=\int |v|^2$ . the conservation laws require that the total $v_k$ and the total $k^2 |v_k|^2$ are both conserved , and if you just try to equipartition energy naively , you will increase the enstrophy by a huge amount , because the enstrophy of a high-k motion is just so much bigger . so you can not equipartition energy , you have to equipartition energy and enstropy together . the law of enstropy partition requires paradoxically that the energy in the short modes is small . the enstropy equipartition is the important dynamics , and the energy ends up cascading the wrong way , from short wavelength to long wavelength modes , so that 2d fluids in a periodic box will cascade up to a single flow of two large counterrotating vortices . the inverse cascade phenomenon was discovered in the 1960s , and it is most often attributed to kraichnan . but several people noted the enstrophy cascade would wreck the traditional ideas for why turbulence occurs generic nonlinear systems the generic pdes all have a turbulence , when they have a dissipation free regime , in a regime where the nonlinearity is important , but not the damping . this is studied today in models of preheating , in inflationary cosmology , and it is studied withing mathematics here and there . the number of example systems is too large to list , it consists of any nonlinear nonintegrable equation without extra conserved quantities . scale-invariant scalar field theory is a simple example , the one relevant for preheating : $$ \partial_t^2 \phi_k - \nabla^2 \phi_k + \lambda_{ijlk}\phi_i\phi_j\phi_l = f ( x , t ) $$ with the appropriate choice of coefficients $\lambda$ . you can also add mass scales , by adding a linear term in $\phi_k$ , or additional quadratic terms which also come with an explicit scale . one is generally interested in the cascade at scales smaller than those defined by the low-order terms , so that the scale invariant cubic nonlinearity is the only important thing . you should also add a damping , to give a short distance cut-off analogous to viscosity in turbulence , and you can do this by adding a $\partial_t \nabla^2\phi $ term with the appropriate coefficient , for example . i did not do that , because in a numerical simulation , you can just do damping by artificially zeroing out very small k modes without an explicit local term to do this for you .
the problem is radially symmetric both in and outside the sphere . because of the radial symmetry only the component with $l=0$ and $m=0$ survives ; so that the potential is only a function of $r$ . as you correctly noted , the potential thus satisfies the equation $$ \frac{1}{r^2}\frac{d}{d r}\left ( r^2 \frac{d v ( r ) }{d r}\right ) =-\rho ( r ) $$ with $$\rho ( r ) = \begin{cases} \rho_0 r , and r&lt ; r , \\ 0 , and r&gt ; 0 . \end{cases}$$ solving the potential outside the sphere $$\frac{1}{r^2}\frac{d}{d r}\left ( r^2 \frac{d v ( r ) }{d r}\right ) = 0 , $$ we find $v ( r&gt ; r ) = \frac{c}{r}$ with $c$ a constant ; we set the integration constant such that $v$ approaches $0$ for $r\to\infty$ . the equation for the potential inside the sphere fulfils the equation $$\frac{1}{r^2}\frac{d}{d r}\left ( r^2 \frac{d v ( r ) }{d r}\right ) = -\rho_0 r$$ with the solution $$v ( r&lt ; r ) = -\frac{\rho_0 r^3}{12} + c $$ where we took into account that the potential should remain finite for $r=0$ . from the condition that $v ( r ) $ and $v' ( r ) $ are continuous at $r=r$ , we obtain the relations $$ c- \frac{\rho_0 r^3}{12} = \frac{c}r , \qquad -\frac{\rho_0 r^2}4 = -\frac{c}{r^2}$$ with the solution $c= \rho_0 r^4/4$ and $c= \rho_0 r^3/3$ . thus the solution has the form $$ v ( r ) = \begin{cases} \frac{\rho_0}{12} ( 4 r^3 -r^3 ) , and r&lt ; r , \\ \frac{\rho_0 r^4}r , and r&lt ; r . \end{cases}$$
well , that equation for the force due to electric charges is only true for a very special choice for the unit of the electric charge . typically , you would write down coulomb 's law as $k\frac{q_{1}q_{2}}{r^{2}}$ , where $k$ is a constant of proportionality chosen to make the units work out . in the si system , the unit of charge is the coulomb ( c ) and the value of $k$ is approximately $8.99 \cdot 10^{9} \frac{n\cdot m^{2}}{c^{2}}$ . if you choose to express your charge in gauss units , defining one gauss to be numerically equal to $\sqrt{8.99\cdot 10^{9}} \ , \ , c$ , then the value of the coulomb constant becomes 1 in this new system of units . now , you could do the same thing for gravity , but your unit of mass would be really small : $\sqrt{6.67 \cdot 10^{-11}} \ , \ , kg$ . it would also mess up the definitions for any unit that had dimensionality of mass , like the newton , joule , etc . but you could do that . also , there is a scheme called planck units that attempts to set as many fundamental constants of nature as possible equal to one .
it is surprisingly hard to explain in simple terms why nothing , not even light , can escape from a black hole once it has passed the event horizon . i will try and explain with the minimum of maths , but it will be hard going . the first point to make is that nothing can travel faster than light , so if light can not escape then nothing can . so far so good . now , we normally describe the spacetime around a black hole using the scharwzschild metric : $$ds^2 = -\left ( 1-\frac{2m}{r}\right ) dt^2 + \left ( 1-\frac{2m}{r}\right ) ^{-1}dr^2 + r^2 d\omega^2$$ but the trouble is that the schwarzschild time , $t$ , is not a good co-ordinate to use at the event horizon because there is infinite time dilation . you might want to look at my recent post why is matter drawn into a black hole not condensed into a single point within the singularity ? for some background on this . now , we are free to express the metric in any co-ordinates we want , because it is co-ordinate independant , and it turns out the best ( well , simplest anyway ! ) co-ordinates to use for this problem are the gullstrandpainlev coordinates . in these co-ordinates $r$ is still the good old radial distance , but $t$ is now the time measured by an observer falling towards the black hole from infinity . this free falling co-ordinate system is known as the " rainfall " co-ordinates and we call the time $t_r$ to distinguish it from the schwarzschild time . anyhow , i am going to gloss over how we convert the schwarzschild metric to gullstrandpainlev coordinates and just quote the result : $$ds^2 = \left ( 1-\frac{2m}{r}\right ) dt_r^2 - 2\sqrt{\frac{2m}{r}}dt_rdr - dr^2 -r^2d\theta^2 - r^2sin^2\theta d\phi^2$$ this looks utterly hideous , but we can simplify it a lot . we are going to consider the motion of light rays , and we know that for light rays $ds^2$ is always zero . also we are only going to consider light moving radially outwards so $d\theta$ and $d\phi$ are zero . so we are left with a much simpler equation : $$0 = \left ( 1-\frac{2m}{r}\right ) dt_r^2 - 2\sqrt{\frac{2m}{r}}dt_rdr - dr^2$$ you may think this is a funny definition of simple , but actually the equation is just a quadratic . i can make this clear by dividing through by $dt_r^2$ and rearranging slightly to give : $$ - \left ( \frac{dr}{dt_r}\right ) ^2 - 2\sqrt{\frac{2m}{r}}\frac{dr}{dt_r} + \left ( 1-\frac{2m}{r}\right ) = 0$$ and just using the equation for solving a quadratic gives : $$ \frac{dr}{dt_r} = -\sqrt{\frac{2m}{r}} \pm 1 $$ and we are there ! the quantity $dr/dt_r$ is the radial velocity ( in these slightly odd co-ordinates ) . there is a $\pm$ in the equation , as there is for all quadratics , and the -1 gives us the velocity of the inbound light beam while the +1 gives us the outbound velocity . if we are at the event horizon $r = 2m$ , so just substituting this into the equation above for the outbound light beam gives us : $$ \frac{dr}{dt_r} = 0 $$ tada ! at the event horizon the velocity of the outbound light beam is zero so light can not escape from the black hole . in fact for $r &lt ; 2m$ the outbound velocity is negative , so not only can light not escape but the best it can do is move towards the singularity .
the formula given is horribly unenlightening because it does not seem to use the fundamental fact about differential forms that they are alternating and thus adds $r$ equal terms , it also does not provide the connection to index notation that it supposedly tries to . let us first understand the idea of the interior product . an $r$-form is something with $r$ slots for vectors that is linear in each slot and changes sign if you interchange any two slots . if you put a vector field $x$ into the first slot , these properties continue to hold for the $r-1$ slots that are left ; this is $i_x \omega$ . this is a coordinate-free definition , so let us see how we can get nakahara 's formula . for computations , given any basis of one-forms $dx^\mu$ , an $r$-form $\omega$ has the expansion $$\omega = \sum \omega_{\mu_1 \ldots \mu_r} dx^{\mu_1} \otimes \cdots \otimes dx^{\mu_r} \tag{1}$$ where the $\omega_{\mu_1 \ldots \mu_r}$ are completely antisymmetric . clearly the interior product is just contraction on the first index . since $dx^{\mu_1} \otimes \cdots \otimes dx^{\mu_r}$ is related to $ dx^{\mu_1} \wedge \cdots \wedge dx^{\mu_r}$ by an antisymmetrization and a normalization , $$\omega = \frac{1}{r ! } \sum \omega_{\mu_1 \ldots \mu_r} dx^{\mu_1} \wedge \cdots \wedge dx^{\mu_r}$$ where the $r ! $ accounts for the normalization . since forms are usually constructed with wedge products this is how you would find the components of a form . now , using ( 1 ) and the interior product as contraction on the first index , \begin{align}i_x \omega and = \sum x^{\alpha}\omega_{\alpha \mu_1 \ldots \mu_{r-1}} dx^{\mu_1} \otimes \cdots \otimes dx^{\mu_{r-1}} \\ and = \frac{1}{ ( r-1 ) ! } \sum x^\alpha \omega_{\alpha\mu_1 \ldots \mu_{r-1}} dx^{\mu_1} \wedge \cdots \wedge dx^{\mu_{r-1}} . \end{align} but since $\omega$ is alternating , we could contract over all of the $r$ indices and get the same thing , provided we keep track of the sign , yielding a sum with $r$ equal terms . compensating for that we have $r$ equal terms , we get $$i_x \omega = \frac{1}{r ( r-1 ) ! } \sum_s x^{\mu_s} \omega_{\mu_{1} \ldots \mu_{s} \ldots \mu_{r}} ( -1 ) ^{s-1}dx^{\mu_{1}} \wedge \cdots \wedge \widehat{dx^{\mu_s}} \wedge \cdots \wedge dx^{\mu_{r}} . $$ the omission of the $s$:th factor in the wedge product is the contraction over the $s$:th index . the $ ( -1 ) ^{s-1}$ comes from that moving the $s$:th factor to the front gives $s-1$ minus signs , one for each factor it has to move past . now for the example of $i_{e^x} ( dx\wedge dy ) $ . we have $$dx\wedge dy = \frac{1}{2} \big ( \omega_{12} dx\wedge dy + \omega_{21} dy \wedge dy \big ) = \frac{2}{2} \omega_{12} dx\wedge dy$$ ( using antisymmetry ) so obviously $\omega_{12} = 1$ . the components of $e^x$ are $ ( 1 , 0 , \ldots , 0 ) $ , so contracting on the first index we get $dy$ .
recall that in this scenario the electric field is some given value - probably a constant - established as a parameter of the problem . if you have been given $e$ , you can solve for $a$ ( and therefore $v$ ) . or , having been given $a$ , you can compute the necessary $e$ to account for such an acceleration . however , they are not independent in the sense that they cannot both be free parameters of the problem . in other words , when you change $a$ you are not increasing or decreasing the affect of $e$ on the particle , you are merely recalculating the mathematically necessary electric field for this new kinematical problem .
the maxwell lagrangian is given by , $$\mathcal{l}=-\frac{1}{4}f_{\mu\nu}f^{\mu\nu}$$ where $f_{\mu\nu}$ is the field-strength of the gauge field , or alternatively may be interpreted as the curvature of a $u ( 1 ) $ lie algebra valued connection , $a_{\mu}$ . by applying the variational principle we obtain , $$\partial_\mu f^{\mu\nu}=0$$ in vacuum . in terms of the electric and magnetic fields , $$\nabla \cdot \vec{e}=0 \quad \quad \partial_t \vec{e}=-\nabla \times \vec{b}$$ we recover two of maxwell 's equations . notice , in differential form language , $f=da$ , i.e. the curvature is an exact form , and all exact forms are also closed under the operation of exterior differentiation , i.e. $$df=d^2 a=0$$ converting the above expression to a tensor equation , using the standard definition , $$d\omega^{ ( n ) }_{a_1 \dots a_n}=\frac{1}{n ! } \left ( \partial_{ [ a_1} \omega_{\dots a_n ] }\right ) $$ recovers the tensor form of the bianchi identity , $$\partial_\lambda f_{\mu\nu}+\partial_\mu f_{\nu\lambda}+\partial_\nu f_{\lambda\mu}=0$$ from which the two remaining maxwell equations follow : $$\nabla \cdot \vec{b}=0\quad \quad \partial_t \vec{b}=-\nabla\times \vec{e}$$ recall , given the spin connection $\omega$ , by cartan 's second structure equation , the curvature form is , $$\mathcal{r}=d\omega + \omega \wedge \omega$$ however , the lie group $u ( 1 ) $ is abelian , and the structure constants vanish , hence the above simplifies , $$\mathcal{r}=d\omega$$ which is completely analogous to the definition of the electromagnetic field strength . other gauge groups may not possess the same field-strength . for example , in quantum chromodynamics , $su ( 3 ) $ is non-abelian , and the extra term does not vanish ; in tensor form : $$g_{\mu\nu}=\partial_\mu a^a_\nu - \partial_\nu a^a_\mu + gf^{a}_{bc}a^{b}_\mu a^{c}_\nu$$
this is an excellent set of questions . the basic thing to realize here is that a wave function described by $ae^{i ( kx - \omega t ) }$ , where here $\omega \equiv e/\hbar = \hbar k^2 \ 2m$ , extends with equal weight through all of the entire universe . these waves are called " plane waves " . because they are of infinite extent these wave functions are not normalizable in the usual sense , and not possible to create in any experiment . already we can answer your first question q1 : what does one see when they measure the energy of a free particle ? a1 : you can not ever get a wave function that is actually proportional to $e^{i ( kx - \omega t ) }$ . whatever the physical size of your experiment is limits the boundary in which the particles live . there are always boundaries . if the boundaries are large compared to the wave length of the particle then you can get wave functions that behave almost as if they were plane waves . you could have a wave function like $\psi ( x , t ) = \phi ( x ) e^{i ( kx-\omega t ) }$ where $\phi ( x ) $ is an envelope with a large extent . if $\phi ( x ) $ extends over a large region of space , then this wave function is almost a plane wave . in this case , the fourier transform of $\phi ( k ) $ will be a very narrow function in $k$-space . the values of energy you might get as the result of a measurement are just given by the probability weight $\left|\phi \left ( k=\sqrt{\frac{2me}{\hbar^2}} \right ) \right|^2$ , so if $\phi ( k ) $ is a narrow function , then the probability distribution of energies you can observe is narrow . your second question actually is not really a quantum mechanics question . to answer it , we have to make an observation about the relationship between energy and time . at the beginning of your question you wrote $\psi ( x , t ) =\psi ( x ) e^{iet/\hbar}$ which is quite correct for a wave function of definite energy . what you can see is that just as with our discussion of how definite $k$ means your wave functions extends over all space , definite $e$ means your wave function extends over all time . a result of this is that any measurement you construct to measure $e$ for any finite amount of time will have an uncertainty bounded by $\delta e\delta t \gtrsim \hbar$ . this is entirely similar to the classical relation for waves $\delta \omega \delta t \gtrsim 1$ through the quantum relation $e=\hbar \omega$ . q2 : if a measurement returns $\hbar^2 k^2/2m$ for some $k$ , then surely the measurement has collapsed $\psi ( x , t ) $ onto $\psi_k ( x , t ) $ , but would this not contradict the observation that $\psi_k ( x , t ) $ is not an allowed state ? a2 : any measurement lasting a finite amount of time has an intrinsic uncertainty in the measurement of the energy . this is not a quantum thing , it is a classical thing , basically coming from the fact that you can not measure a frequency to infinite precision with a finite time measurement . q3 : do repeated measurements return the same energy ? if not , do the results differ with mean given by $\langle\psi|\hat{h}|\psi\rangle$ and spread given by $\sigma_h$ ? a3 : repeated measurements in the case you have described will not give the same result . indeed , you will be drawing from a distribution with mean $\langle \psi| \hat{h} | \psi \rangle$ and width $\sigma_h$ . that width comes precisely from the uncertainty relation we discussed leading up to question 2 . q4 : can we even measure the energy of a free particle ? a4 : at this point i think you get the picture . now , after all that strictly theoretical business we need to look at some numbers so you get a practical feel for how these constraints actually come into play . suppose we have a particle in a one-dimensional box . the energy levels in that case are $e_n = n^2 h^2 / 8ml^2$ where $m$ is the mass and $l$ is the length of the box . the energy difference between the first and second levels is $\delta e_{1,2} = 3h^2 / 8ml^2$ . let 's suppose this particle is an electron and see how much time we need to measure in order to distinguish these two levels . we get $t \gtrsim \hbar / \delta e_{1,2} = \frac{\hbar 8 m l^2}{3h^2} = \frac{4ml^2}{\pi3h}$ now we have to pick a length of the box . let 's just put in a meter . taking that and the mass of the electron we wind up with $t ( l=1\ , \textrm{meter} ) \gtrsim 10\ , \textrm{minutes}$ . this is a very long time for typical experiments with single electrons . the reason is that with a box that big the energy levels are very close together , so distinguishing them takes a long time . for a more realistic length scale of $l=1\mu\textrm{m}$ you get $t ( l=1\mu\textrm{m} ) \gtrsim 0.5\ , \textrm{nanoseconds}$ . that is pretty darn fast ! that means that for the right set of parameters where the energy levels are well-separated , this energy-time uncertainty relation imposes only pretty weak constraints . on the other hand , there are real life experiments going on these days where time scales of nanoseconds are commonplace and this sort of thing really matters . i hope that was helpful . if something is not clear please just ask .
i vaguely remember doing a millikan oil drop experiment in a lab in college , 60+ years ago . here is an example . with enough measurements the statistical error can become 1% , or as precise as one wants . so your question is basically about the systematic errors . most of them would also follow the addition in quadrature rule , since most systematic errors themselves follow a gaussian . i tried to find laboratory examples from students ' efforts but was unsuccessful . i do not see though why the various systematic factors you list could not be within less than 1% at that time . classical physics measurements were quite sophisticated .
a . use kinematic equation for constant acceleration : v^2=u^2+2as here v , the final velocity is 0 because the car comes to a stop , u is 60 mph , s is 1.5 m . therefore a= u^2/2s . b . just substitute u=90 mph and evaluate a in the same way . also to get the answers in g , just divide a by 9.8 m/s^2 note : a will be negative
yes , so far , 20 synthetic elements have been created , with atomic numbers 99 ( einsteinium ) to 118 ( ununoctium ) . all these elements are unstable , with half-lives ranging from a year to a few milliseconds . you can find a list on wikipedia . these elements are produced in specialized nuclear reactors , by bombarding heavy elements like uranium and plutonium with neutrons or other elements .
i will just go ahead and write this down even though it is already been covered in the other thread . . but i did not post there so : ) first , if you think of light as a ( scalar ) wave ( which is really a semi-classical way of thinking but might be enough to answer your question ) you can invoke the huygen-fresnel principle which in this case boils down to considering every point on the reflecting surface as an origin of a re-emitted spherical wave with a starting phase directly related to the phase the point got from the incident wavefront . the superposition of these wavefronts , after you let them destructively interfere with each other , will amount to a new combined wavefront which propagates according to snells law ( angle of incidence = angle of reflectance ) . see this image for the corresponding illustration of refraction ( which is very similar , i could not quickly find a good image of reflection ) : now , light really does not behave " sometimes as a particle , sometimes as a wave " . it is always detected as quanta ( particles ) but the probability amplitudes ( phases ) propagate in a wavelike fashion . one way of expressing the propagation is to say that a photon is sort of split and takes every possible path between a and b ( or , in the case of a reflector , from a to any point on the reflector and then from there to point b by any means ) . every path gets a phase contribution , and all the indistinguishable paths are summed . most paths are simply cancelling each other but some constructively interfere , creating a large contribution ( in case you do not know qm , the probability amplitude squared is the probability of the described event so a large contribution means this result will most probably occur ) . there is a very good image and description of this process in feynman - qed the strange theory of light and matter ( as i wrote in the comment above ) . in the case of the reflector , the large contribution occurs at the classical angle of reflectance ( snell 's law again ) . notice the similarity between this formulation ( called the path integral approach ) and the semi-classical principle outlined above ; this is not a coincidence of course . also to briefly digress on your implied question on the per-atom non-zero " reflectance " time - saying that an electron-orbit absorbs the photon energy for a while the re-emits it a non-zero time later is of course also a slight simplification . in reality the electron interacts with the a photon , changes its momentum a bit , the re-emits ( interacts ) with the new photon and changes its momentum again . this scattering process occurs at all allowed momenta and intermediate times , which are then all superpositioned like above and hence i am not sure it is meaningful to talk about any appreciable time of reflectance . notice that this scattering is in practice very different from scattering which can excite the electron to another orbit .
your question , can one say that all renormalizable theories are scale invariant but the converse , that every scale invariant theory is renormalizable too is not true ? has a sharp answer : no , one cannot say so . renormalizable theories typically have running coupling constants with non-vanishing beta functions . the second part ( what you called the ' converse' ) is false too . the first example that come to my mind is a theory with a spontaneously broken cft that delivers a dilaton : the low-energy lagrangian for the dilaton is scale invariant and still is non-renormalizable having an infinite series of terms organized by the number of derivatives envolved . the only relations i can see between scale invariance and renormalization are well known : a ) renormalization typically spoils classical scale-invariance ; b ) a theory with strictly renormalizable terms ( i.e. . dimension 4 only ) is classically scale invariant and it has a chance to be scale invariant at the quantum level as well ; c ) a non-scale invariant theory may run and approach a scale invariant theory at the end of the rg flow , either ir or uv , depending where you are heading to . this last point may be violated in very special non-unitary qft , though .
my chair usually creates a lot of static , so the same happens to me . what i usually do is to touch any big metal object before i get up ( my desk is metallic , so i use that ) to discharge myself . another popular trick is to touch the doorknob , faucet , etc with a metallic object , so that the discharge goes from that instead of your fingers . or you can use an antistatic strap , if it really bothers you . regarding the physical processes , when you rub against other materials ( e . g . a carpet ) you can get electrically charged . that is called triboelectric effect . if you then touch a metallic object , there will be an electrical discharge , like a tiny ray , that goes from your skin to the object and can be painful but not dangerous . if the air is dry ( like in an office with air conditioning ) , you will build more charge because the humidity makes the air ore conducting and you will slowly discharge with no sparks .
in order to have the hot side remain hot , and the cold remain cold , while transmitting the electricity , ideally a substance is desired that conducts electricity well , but heat poorly . no . the two ends are assumed to be connected to heat sources or sinks that maintain a constant temperature at those nodes . heat does flow well from one end to the other , which causes the thermoelectric effects . hot electrons and cold electrons travel through the material at different rates , leading to a net flow of electric charge towards one end . https://en.wikipedia.org/wiki/seebeck_effect#charge-carrier_diffusion a series connection of thermocouples is called a " thermopile " . it is connected like this : not sure what you are asking . you want to connect wires made of lithium , cobalt , and copper in parallel ?
as far as i could understand , it seems that you want to know whether timelike geodesics can reach the conformal boundary of ads . if that is the case ( please do confirm ) , the answer is no - no timelike geodesic can reach conformal infinity , it rather gets constantly refocused back into the bulk in a periodic fashion . you need timelike curves which have some acceleration in order to avoid this . maximally extended null geodesics ( i.e. . light rays ) , on the other hand , always reach conformal infinity , both in the past and in the future . an illustration of these facts using penrose diagrams can be found , for instance , in section 5.2 , pp . 131-134 of the book by s . w . hawking and g . f . r . ellis , " the large scale structure of space-time " ( cambridge , 1973 ) . the detailed reasoning behind the above paragraph can be seen in a global , geometric way . in what follows , i will largely follow the argument presented in the book by b . o'neill , " semi-riemannian geometry - with applications to relativity " ( academic press , 1983 ) , specially proposition 4.28 and subsequent remarks , pp . 112-113 . for the benefit of those with no access to o'neill 's book , i will present the self-contained argument in full detail . i will make use of the fact that $ads_4$ is the universal covering of the embedded hyperboloid $h_m$ ( $m&gt ; 0$ ) in $\mathbb{r}^{2,3}= ( \mathbb{r}^5 , \eta ) $ $$ h_m=\{x\in\mathbb{r}^5\ |\ \eta ( x , x ) \doteq -x_0^2+x_1^2+x_2^2+x_3^2-x_4^2=-m\}\ . $$ the covering map $\phi:ads_4\ni ( t , r , \theta , \phi ) \mapsto ( x_0 , x_1 , x_2 , x_3 , x_4 ) \in h_m\subset\mathbb{r}^{2,3}$ through the global coordinates $ ( t\in\mathbb{r} , r\geq 0,0\leq\theta\leq\pi , 0\leq\phi&lt ; 2\pi ) $ is given by $$ x_0=\sqrt{m ( 1+r^2 ) }\sin t\ ; $$ $$ x_1=\sqrt{m}r\sin\theta\cos\phi\ ; $$ $$ x_2=\sqrt{m}r\sin\theta\sin\phi\ ; $$ $$ x_3=\sqrt{m}r\cos\theta\ ; $$ $$ x_4=\sqrt{m ( 1+r^2 ) }\cos t\ . $$ the pullback of the ambient , flat pseudo-riemannian metric $\eta$ defined above ( with signature $ ( -+++- ) $ ) by $\phi$ after restriction to $h_m$ yields the $ads_4$ metric in the form appearing in the question and in pedro figueroa 's nice answer up to a constant , positive factor : $$ds^2= m\left [ - ( m+r^2 ) dt^2+ ( m+r^2 ) ^{-1}dr^2+r^2 ( d\theta^2+\sin^2\theta d\phi^2 ) \right ] \ . $$ the conformal completion of $ads_4$ , on its turn , is obtained by means of the change of radial variable $u=\sqrt{m+r^2}-r$ , so that $r=\frac{m-u^2}{2u}$ , $dr=-\frac{1}{u} ( \frac{m+u^2}{2u} ) du$ and $m+r^2= ( \frac{m+u^2}{2u} ) ^2$ , yielding $$ds^2=\frac{m}{u^2}\left [ -\left ( \frac{m+u^2}{2}\right ) ^2dt^2+du^2+\left ( \frac{m-u^2}{2}\right ) ^2 ( d\theta^2+\sin^2\theta d\phi^2 ) \right ] \ . $$ conformal infinity is reached by taking $r\rightarrow+\infty$ , which is the same as $u\searrow 0$ . the rescaled metric $\omega^2 ds^2$ , $\omega=m^{-\frac{1}{2}}u$ yields the three-dimensional einstein static universe as the conformal boundary ( i.e. . $u=0$ ) . it is clear that $h_m$ is a level set of the function $f:\mathbb{r}^5\rightarrow\mathbb{r}$ given by $f ( x ) =\eta ( x , x ) $ . therefore , the vector field $x_x=\frac{1}{2}\mathrm{grad}_\eta f ( x ) =x$ ( where $\mathrm{grad}_\eta$ is the gradient operator defined with respect to $\eta$ ) is everywhere normal to $h_m$ - that is , any tangent vector $x_x\in t_x h_m$ satisfies $\eta ( x_x , t_x ) =0$ . given two vector fields $t , s$ tangent to $h_m$ , the intrinsic covariant derivative $\nabla_t s$ on $h_m$ is simply given by the tangential component of the ambient ( flat ) covariant derivative $ ( \partial_t s ) ^a=t^b\partial_b s^a$: $$ \nabla_t s=\partial_t s-\frac{\eta ( x , \partial_t s ) }{\eta ( x , x ) }x=\partial_t s+\frac{\eta ( x , \partial_t s ) }{m}x\ . $$ the normal component of $\partial_t s$ , on its turn , has a special form due to the nature of $h_m$ ( notice that $\partial_a x^b=\partial_a x^b=\delta^b_a$ ) : $$ \eta ( x , \partial_t s ) =\underbrace{\partial_t ( \eta ( x , s ) ) }_{=0\ ; }-\eta ( s , \partial_t x ) =-\eta ( s , t ) \ \rightarrow\ \frac{\eta ( x , \partial_t s ) }{\eta ( x , x ) }x=\frac{\eta ( s , t ) }{m}x\ . $$ as such , we conclude that a curve $\gamma:i\ni\lambda\mapsto\gamma ( \lambda ) \in h_m$ ( $i\subset\mathbb{r}$ is an interval with nonvoid interior ) is a geodesic of $h_m$ if and only if $\frac{d^2\gamma ( \lambda ) }{d\lambda^2} ( \lambda ) \doteq\ddot{\gamma} ( \lambda ) $ is everywhere normal to $h_m$ , that is , $$\ddot{\gamma} ( \lambda ) =-\frac{1}{m}\eta ( \ddot{\gamma} ( \lambda ) , x_{\gamma ( \lambda ) } ) x_{\gamma ( \lambda ) }=\frac{1}{m}\eta ( \dot{\gamma} ( \lambda ) , \dot{\gamma} ( \lambda ) ) x_{\gamma ( \lambda ) }=\frac{1}{m}\eta ( \dot{\gamma} ( \lambda ) , \dot{\gamma} ( \lambda ) ) \gamma ( \lambda ) \ . $$ in particular , if $\eta ( \dot{\gamma} ( \lambda ) , \dot{\gamma} ( \lambda ) ) =0$ , then $\gamma$ is also a ( null ) geodesic in the ambient space $\mathbb{r}^{2,3}$ . given $x\in h_m$ , the linear span of $x_x=x$ and any tangent vector $t_x\neq 0$ to $h_m$ at $x$ defines a 2-plane $p ( t_x ) $ through the origin of $\mathbb{r}^5$ and containing $x$ . in other words , $$ p ( t_x ) =\{\alpha x_x +\beta t_x\ |\ \alpha , \beta\in\mathbb{r}\}\ , $$ and therefore $$ p ( t_x ) \cap h_m=\{y=\alpha x_x+\beta t_x\ |\ \eta ( y , y ) =-\alpha^2 m+\beta^2\eta ( t_x , t_x ) =-m\}\ . $$ this allows us already to classify $p ( t_x ) \cap h_m$ according to the causal character of $t_x$: $t_x$ timelike ( i.e. . $-k=\eta ( t_x , t_x ) &lt ; 0$ ) : we have that $m\alpha^2+k\beta^2=m$ with $k , m&gt ; 0$ , hence $p ( t_m ) \cap h_m$ is an ellipse ; $t_x$ spacelike ( i.e. . $k=\eta ( t_x , t_x ) &gt ; 0$ ) : we have that $m\alpha^2-k\beta^2=m$ with $k , m&gt ; 0$ , hence $p ( t_m ) \cap h_m$ is a pair of hyperbolae , one with $\alpha&gt ; 0$ and the other with $\alpha&lt ; 0$ . the point $x=x_x$ belongs to the first hyperbola ; $t_x$ lightlike ( i.e. . $\eta ( t_x , t_x ) =0$ ) : we have that $\alpha^2=1$ with $\beta$ arbitrary , hence $p ( t_m ) \cap h_m$ is a pair of straight lines , one given by $\alpha=1$ and the other by $\alpha=-1$ . the point $x=x_x$ belongs to the first line . notice that each of these lines is a null geodesic both in $h_m$ and in $\mathbb{r}^{2,3}$ ! moreover , $x=\gamma ( 0 ) $ and $t_x=\dot{\gamma} ( 0 ) $ define a general initial condition for a geodesic $\gamma$ starting at $x$ . it remains to show that any curve that stays in $p ( t_x ) \cap h_m$ is a geodesic in $h_m$ . this is clearly true for $t_x$ lightlike , since in this case we have already concluded that $\gamma ( \lambda ) =x+\lambda t_x$ for all $\lambda\in\mathbb{r}$ . for the remaining cases ( i.e. . $\eta ( t_x , t_x ) \neq 0$ ) , consider a $\mathscr{c}^2$ curve $\gamma$ in $p ( t_x ) \cap h_m$ beginning at $\gamma ( 0 ) =x$ with $\dot{\gamma} ( 0 ) =\dot{\beta} ( 0 ) t_x$ ( we assume that $\dot{\gamma} ( \lambda ) \neq 0$ for all $\lambda$ ) . writing $\gamma ( \lambda ) =\alpha ( \lambda ) x_x+\beta ( \lambda ) t_x$ , we conclude from the above classification of $p ( t_x ) \cap h_m$ that we can choose the parameter $\lambda$ so that $t_x$ timelike : $\alpha ( \lambda ) =\cos\lambda$ , $\beta ( \lambda ) =\sqrt{-\frac{m}{\eta ( t_x , t_x ) }}\sin\lambda$ , so that $\eta ( \dot{\gamma} ( \lambda ) , \dot{\gamma} ( \lambda ) ) =-m$ with $\dot{\beta} ( 0 ) =\sqrt{-\frac{m}{\eta ( t_x , t_x ) }}$ ; $t_x$ spacelike : $\alpha ( \lambda ) =\cosh\lambda$ , $\beta ( \lambda ) =\sqrt{\frac{m}{\eta ( t_x , t_x ) }}\sinh\lambda$ , so that $\eta ( \dot{\gamma} ( \lambda ) , \dot{\gamma} ( \lambda ) ) =+m$ with $\dot{\beta} ( 0 ) =\sqrt{\frac{m}{\eta ( t_x , t_x ) }}$ . in both cases , we conclude that $$ \ddot{\gamma} ( \lambda ) =\frac{\eta ( \dot{\gamma} ( \lambda ) , \dot{\gamma} ( \lambda ) ) }{m}\gamma ( \lambda ) \ , $$ i.e. $\gamma$ must satisfy the geodesic equation in $h_m$ with the chosen parametrization , as wished . since any pair of initial conditions for a geodesic determines a 2-plane through the origin in the above fashion , we conclude that the resulting geodesic in $h_m$ will remain forever in that 2-plane . for later use , i remark that all geodesics of $h_m$ cross at least once the 2-plane $p_0=\{x\in\mathbb{r}^5\ |\ x_1=x_2=x_3=0\}$ - this can be easily seen from the classification of the sets $p ( t_x ) \cap h_m$ . this allows us to prescribe initial conditions in $p_0$ for all geodesics in $h_m$ . now we have complete knowledge of the geodesics in the fundamental domain $h_m$ of $ads_4$ . what happens when we go back to the universal covering ? what happens is that the lifts of spacelike and lightlike geodesics stay confined to a single copy of the fundamental domain , whereas the lifts of timelike geodesics do not . to see this , we exploit the fact that translations in the time coordinate $t$ are isometries and the remark at the end of the previous paragraph to set $$\gamma ( 0 ) =x_x=x= ( 0,0,0,0 , \sqrt{m} ) $$ in $h_m$ ( i.e. . $\gamma$ is made to start at $p_0$ with $t=0$ ) , so that $$\dot{\gamma} ( 0 ) =t_x= ( y_0 , y_1 , y_2 , y_3,0 ) \ . $$ we also normalize $\eta ( t_x , t_x ) $ to $-m$ , $+m$ or zero depending on whether $t_x$ is respectively timelike , spacelike or lightlike . writing once more $\gamma ( \lambda ) =\alpha ( \lambda ) x_x+\beta ( \lambda ) t_x$ , we use the classification of geodesics in $h_m$ by their causal character to write explicit formulae for $\gamma$: $t_x$ timelike $\rightarrow$ $\gamma ( \lambda ) = ( \cos\lambda ) x_x+ ( \sin\lambda ) t_x$ ; $t_x$ spacelike $\rightarrow$ $\gamma ( \lambda ) = ( \cosh\lambda ) x_x+ ( \sinh\lambda ) t_x$ ; $t_x$ lightlike $\rightarrow$ $\gamma ( \lambda ) =x_x+\lambda t_x$ . the above expressions show that , in the spacelike and lightlike cases , the last component $\gamma ( \lambda ) _4$ of $\gamma ( \lambda ) $ never goes to zero , which implies by continuity that the time coordinate $t$ stays within the interval $ ( -\frac{\pi}{2} , \frac{\pi}{2} ) $ , hence the lift of $\gamma$ to $ads_4$ stays within a single copy of its fundamental domain . one also sees that the spatial components ( 1,2,3 ) of $\gamma ( \lambda ) $ go to infinity as $\lambda\rightarrow\pm\infty$ , hence $u\rightarrow 0$ along these geodesics as $\lambda\rightarrow\pm\infty$ . in the timelike case , the whole time interval $ [ 0,2\pi ] $ is spanned by $\gamma ( \lambda ) $ as $\lambda$ spans the interval $ [ 0,2\pi ] $ . since the curve is closed , its lift to $ads_4$ spans the whole time line $\mathbb{r}$ as $\lambda$ does so . on the other hand , it is clear that in this case the spatial components of $\gamma ( \lambda ) $ just keep oscillating within a bounded interval of the coordinate $r$ - hence , the coordinate $u$ stays bounded away from zero . therefore , a timelike geodesic $\gamma$ never escapes to conformal infinity .
at the physics 101 level , you pretty much just have to accept this as an experimental fact . at the upper division or early grad school level , you will be introduced to noether 's theorem , and we can talk about the invariance of physical law under displacements in time . really this just replaces one experimental fact ( energy is conserved ) with another ( the character of physical law is independent of time ) , but at least it seems like a deeper understanding . when you study general relativity and/or cosmology in depth , you may encounter claims that under the right circumstances it is hard to define a unique time to use for " invariance under translation in time " , leaving energy conservation in question . even on physics . se you will find rather a lot of disagreement on the matter . it is far enough beyond my understanding that i will not venture an opinion . this may ( or may not ) overturn what you have been told , but not in a way that you care about . an education in physics is often like that . people tell you about firm , unbreakable rules and then later they say " well , that was just an approximation valid when such and such conditions are met and the real rule is this other thing " . then eventually you more or less catch up with some part of the leading edge of science and you get to participate in learning the new rules .
the bulk of physics obeys the time reversal symmetry . in qm that means that a process which is allowed in one direction is allowed in the other direction and that the total probability for it is the same in both directions ( assuming you can set up the time-reversed final state as an initial state ) . so a way to understand the delicacy of trying to assemble a non-trivial nucleus from individual nucleons is to look for the occurrence of non-trivial nuclei ( or their excited states ) coming apart into individual nucleons . alas , as far as i know this simply does not happen . which implies that doing it by intent is not merely beyond current technology , but that it is hard to envision what improvements might lead to a way to start thinking about how to finesse it . the time , space and energy scales involved are all extremely demanding . it may help you to understand that even in light nuclei like carbon the protons have momenta up to the neighborhood of 100 mev ( which is to say they are mildly relativistic ) , but are confined to a volume a few femtometers in diameter . if you were to shoot a free proton through that volume with that kind of momentum , it would pass through on a time scale on the order of $$ t \approx \frac{10^{-14}\ \mathrm{m}}{10^{7}\ \frac{\mathrm{m}}{\mathrm{s}}} = 10^{-21}\ \mathrm{s} . $$ that is a even shorter time scale than the attosecond physics guys use !
1 ) the combined mass of the proton and electron are not equal to the mass of the neturon , though it happens to be close . 2 ) c14 and n14 do not have masses of exactly 14 amu . these are rounded numbers . 3 ) it seems like you might be aware of this and are just using different terminology , but just to clarify - mass is not conserved in nuclear processes . conservation of mass is not a universal law ; it just seemed like that for a while before we could access / study high energy processes . what is a universal law is conservation of energy . since a particle 's mass is part of its energy ( $e_{mass} = mc^2$ ) , the sum of the masses of particles before / after a nuclear or particle process may be different as long as the total energy is the same .
the phenomenon is called mirage ( edit : i called it fata morgana earlier , but a fata morgana is a special case of mirage that is a bit more complex ) . the responsible effect is the dependence of the refractive index of air on the density of air , which , in turn , depends on the temperature of the air ( hot air being less dense than cold air ) . a non-constant density leads to refraction of light . if there is a continuous gradient in the density , you get a bent curve ( i ) as opposed to light coming straight at you ( d ) . your eye does not know , of course , that the light ( i ) coming at it was bent , so your eye/brain continues the incoming light in a straight line ( v ) . this mirroring of the car ( or other objects ) then tricks you into thinking the road is wet , because a wet street would also lead to a reflection . in addition , the air wobbles ( i.e. . density fluctuations ) , causing the mirror image to wobble as well , which adds to the illusion of water .
let your carrier signal be $a_0 \cdot \cos ( \omega_c t ) $ with amplitude $a_0$ and carrier frequency $\omega_c$ . let your signal be a simple wave , $\phi ( t ) = a_s \cdot \cos ( \omega_s t ) $ . then the modulated signal becomes $$a_0 a_s \cdot \cos ( \omega_c t ) \cdot \cos ( \omega_s t ) $$ . in addition , as pointed out by george in the comments , the carrier also gets transmitted . using the trigonometric identity $\cos ( u ) \cdot \cos ( v ) = \frac{1}{2} [ \cos ( u-v ) + \cos ( u+v ) ] $ , you get the final signal : $$\frac{1}{2} a_0 a_s \cdot ( \cos ( ( \omega_c - \omega_s ) t ) + \cos ( ( \omega_c + \omega_s ) t ) ) + a_0 \cdot \cos ( \omega_c t ) $$ hence , the frequency becomes changed , you get the carrier frequency in the middle ( at $\omega$ ) and two side-bands at $\omega_c \pm \omega_s$ . now , in reality your signal is not a simple cosine , but you could do a fourier decomposition of the signal and treat each frequency independently . the two frequencies then get smeared out and you get the two sidebands .
this has as much to do with biology as with physics . the long answer on the biology is here . the biology in summary : the human eye has only three different " color-sensitive " elements , and uses a complex combination of the amount of response it sees from each of these to assign a " color " to the image . because there are only three sensitivities in the human eye , there are a variety of different techniques using only three basic ( in some schemes called primary ) colors to represent colors to humans . the actual frequency of the light emitted from the part of the rainbow we call green may have the same effect on the human eye as something we get by mixing our blue crayon with our yellow crayon on a piece of paper . but it is easy to build a detector which will trivially differentiate between monochromatic green from one slice of a rainbow , and a mixture of the light reflected from blue and yellow crayon pigments . in principle , humans might have evolved a different eye with four or five different " color " detectors , in which case the schemes needed to make color images would probably need to have four or five basic colors , and the images we see from our current three color representations would seem to be washed out , missing something important . but the eye did not develop that way .
if you have multiple masses the total momentum is $$ p = \sum_i m_i v_i $$ if you have a continuum distribution then you can proceed like follows . you divide the continuum into small boxes where each part of the box has approximately same mass and velocity ( this assumes some kind of smoothness of the distribution ) . then you can obtain whole momentum with the above formula . now letting the box sizes go to zero you obtain integral $$ p ( t ) = \int \rho ( r , t ) v ( r , t ) dr $$
you will not get a shock unless you complete the circuit to ground . this is why power lines can be worked on while live , from a helicopter : helicopter power line maintenance
because spacetime includes both multiple points in space and multiple moments in time , you have to think of a particle as a line ( or a 1d curve ) through spacetime , not a point . the line is called the world line . it is made up of all the $ ( x , t ) $ points at which the particle exists : in other words , if you , as an external observer , measure the particle 's position $x$ ( using your own rulers ) at a bunch of different times $t$ ( using your own clock ) , and plot all those points on a graph , and connect them , you get a world line . you could say that the world line itself is fixed in spacetime . but that is not a particularly useful statement to make , because there is no separate time outside of spacetime , so it is not like the world line can actually move . besides , even if you did say the world line is fixed in spacetime , that applies equally well to massive particles and to photons . you can pick any two points on a world line and figure out how much time elapses , by your clock , between those two points . it is just $t_2 - t_1$ . you can also figure out how much time elapses , by some other observer 's clock , between those two points : it is $\frac{t_2 - t_1}{\sqrt{1 - v^2/c^2}}$ , where $v$ is the relative velocity between you and the other observer . and you can even try to apply this calculation treating the particle itself as the observer , which lets you figure out how much time passes for the particle between those two points . it works out to $\sqrt{ ( t_2 - t_1 ) ^2 - ( x_2 - x_1 ) ^2/c^2}$ . this works just fine for a massive particle . but for a photon , no matter which two points you pick , the answer you get is zero . this is why we say that photons do not experience time . i do not think it is accurate in general to say that photons are fixed in time , though . that might be a useful thing to say to make a particular point in a particular argument , but in most cases , it is probably more misleading than not .
and using the acceleration equation for the scale factor , aaah ! you have committed a mortal sin ! but it is ok , it is a very common one : ) . never ever ever ever ever plug equations of motion back into the lagrangian . it is the moral analogue of setting x=3 into f ( x ) =x^2+x before differentiating--you will get 0 , whereas what you meant to do was differentiate f ( x ) and then set x=3 , which gives you 7 . ( you can get away with plugging equations of motion into the lagrangian if you are careful--you can do if you are you used an equation of motion that was purely algebraic--no derivatives--or if you used a greens function to integrate out a dynamical field . however you need to be very careful and you should not do it unless you are 100% sure you know what you are doing ) . ok , onto how to fix your dilemma . first off you want to include the $\sqrt{-g}=a^3$ factor in your definition of $\mathcal{l}$ , so that the expression in the middle of your answer becomes \begin{equation} \mathcal{l}_{you} = 3a \left ( k + \dot{a}^2 + a \ddot{a} \right ) + a^3 \left ( \frac{1}{2} \dot{\phi}^2 - v ( \phi ) \right ) \end{equation} now carroll 's equation 11 is ( with n=1 and also setting $m_{pl}$ = 1 ) \begin{equation} \mathcal{l}_{carroll} = 3a \left ( k - \dot{a}^2 \right ) + a^3 \left ( \frac{1}{2} \dot{\phi}^2 - v ( \phi ) \right ) \end{equation} this is almost your boxed expression at the top of your answer , but the $a \dot{a}^2$ has minus sign instead of a plus sign . now your worry is essentially that \begin{equation} \mathcal{l}_{you} - \mathcal{l}_{carroll} = 6 a \dot{a}^2 + 3 a^2 \ddot{a} \end{equation} is apparently nonzero . however , it this is actually a total derivative \begin{equation} \mathcal{l}_{you} - \mathcal{l}_{carroll} = 3 \frac{d}{dt}\left ( a^2 \dot{a} \right ) \end{equation} so $\mathcal{l}_{you}$ and $\mathcal{l}_{carroll}$ agree up to a total derivative , which is all that is required . incidentally , based on the title of your question i take it you are interested in studying minisuperspace . if so i think you can learn a lot by not setting $n=1$ , life is much more interesting if you keep in it . ( it is consistent to set $n=1$ because i can absorb $n$ into a redefinition of $t$ , but you do not have to do it and it is interesting to see what happens if you do not do that ) . basically $n$ parameterizes the ' gauge freedom ' associated with the ability to reparameterize your time coordinate . you will always see $n$ appear in the combination $n dt$: either as $ndt$ in the measure of the integral , or as $\frac{d}{n dt}$ in time derivatives . this is why you can absorb $n$ into $t$ if you want . if you pass from the lagrangian to the hamiltonian form , $n$ ends up playing the role of a lagrange multiplier in the hamiltonian . actually $n$ will actually multiply everything else in the hamiltonian , which is an example of the general principle that hamiltonian for diffeomorphism invariant theories are always pure constraint . you can see this explicitly in carroll 's equation 13 . however even if you do not like hamiltonians , there is another way to see the same thing : you can start with the action and $s= \int n dt \mathcal{l}_{you}$ and vary wrt $n$ , you will see that this gives you the friedman equation directly . if you set $n=1$ in the action first [ which you are allowed to do ] , you can only get the friedmann equation from the action by doing more work . you have to combine the equations of motion you get by varying wrt $a$ ( the acceleration equation ) and by varying wrt $\phi$ ( the conservation of energy equation ) .
dark energy is not really an energy in the common sense and there absolutely is no project to produce electricity via dark energy . dark energy is most probably a mistake in our calculations . even if it is not so , the nature of dark energy is currently so far from our grasp that there is no way , even theoretically , to generate energy from dark energy yet .
the question of the theoretical limit to classical measurement is interesting , and it is contained in the general thermodynamic formalism . to get a position measurement of n-bits when the environment is at temperature t , you need to dump at least $nk\log2$ entropy in the environment . the argument is by liouville 's theorem . in order to measure the position of a particle to n-bit accuracy using other particles , you need to reduce the phase space volume of the measured particle by a factor of $2^n$ , which means you need to increase the phase space volume of everthing else by the same factor ( to make the volume of the phase space occupied by possible microstate at least stable--- it can only grow ) . this is the restriction . at zero temperature , there is no limit to an arbitrarily accurate classical measurement--- you can take some little tiny particles that are moving really fast and bounce them off the moving object ( modeling light bouncing off the object ) and detect the bounced objects and calculate the position from their incoming direction . if there is no thermal jitter , you can do this classically completely accurately without any limit . the thermodynamic bound only says that you need to generate a certain irreducible amount of heat in order to get the bouncing particles ' positions accurately recorded in your device .
no . unless you consider time as the fourth dimension . also current research suggests that spacetime becomes two dimensional at high energies and very small distances . so , yes , dimensionality of spacetime is a mutable concept in modern physics . but no experimental evidence , indirect or direct , has been found for a fourth spacelike or a second timelike direction . someone is bound to question my statement that " spacetime becomes two dimensional at high energies and very small distances " . my backup reference for this is carlip 's recent paper the small scale structure of spacetime , where he considers inputs from five or six different theoretical lines of research . of course , there is bound to be a spectrum of opinion on this issue .
the main problem with being near a large planet are the tidal forces produced by its gravity . the moon is pretty small compared to the earth , and it is a quarter of a million miles away , but it still produces large movements in sea level i.e. the tides . if you imagine the moon getting bigger and closer the tidal effects would increase . the land feels the same tidal forces as the sea , though being solid it moves much less . however tidal deformations of the land generate heat due to viscous losses . the classic example of this is the moon io . this orbits jupiter at about the same distance as the moon orbits the earth , but of course jupiter is much bigger than the earth . the tidal effects on io are so big that they heat the body of io , and it is the most volcanically active body in the solar system . if earth orbited jupiter at the same distance as io we would not be around to admire the view :- ) there is a distance , called the roche limit , within which the tidal forces are so great that they pull the moon to bits . the roche limit for jupiter is about 50,000 miles so io is safe ( if rather hot ) .
yes , any notion of pressure at least normally assumes that there are many , $n\to\infty$ , particles that are the sources of the pressure , so in this sense it is a statistical term . but just like with the temperature , one may assign pressure or temperature to a single particle , too , although for a single particle , we would usually say that it is an accident that it has one momentum or energy or another , so there is still some reason to say that it is a statistical term .
superfluid helium-4 has a very well studied excitation structure -- at very low momenta , there is a low energy excitation , the phonon , that corresponds to a periodic density fluctuation in the superfluid with well defined wave-number and an energy $e = c \hbar k$ ( c being the speed of sound in the superfluid ) . though others might quibble with me over vocabulary , i prefer to call phonons " collective excitations " and reserve " quasiparticle " for excitations that correspond to renormalized single-particle excitations ( like an electron in a fermi liquid ) . in either case though , what is meant is simply that the excitation is long-lived , or , by the uncertainty principle , that it has a sharply defined energy , and here the phonon does . this linear relation breaks down at higher momenta , where the $e , k$ curve turns down , then turns back up . near the local maximum , there are sharp excitations with an inverse-parabolic dispersion . these are the maxons . near the local minimum we have a sharp excitation with a parabolic dispersion , and these are the rotons . historically rotons were introduced by landau , with a guess for their dispersion ( $e=\delta+ ( p-p_0 ) ^2/2m$ ) , not merely as a mathematical device , but because a superfluid described only by the phonon dispersion fails to capture the actual thermodynamics observed in helium-4 . on the other hand , landau was not concerned about a qualitative , microscopic picture of what rotons are , in the sense that we know what phonons mean for the local density of the system . so , i am actually not too sure what rotons are in that same microscopic sense , and the same for maxons , although it is apparent that a gas of phonons and a gas of rotons are very different things . on the other hand , this is somewhat paradoxical since all three of these excitations are just different parts of the same dispersion curve , so to say they are fundamentally different is a phrase i would not use without a great deal of caution . . . lastly , knowing all of the excitations present in superfluid helium is very important for calculating its thermodynamic properties , etc . , however i think ( and hopefully someone will correct me if i am wrong ! ) the only excitation that is intrinsic to superfluidity is the phonon mode - this is because the superfluid state breaks a continuous gauge symmetry of the normal fluid , and thus phonons are the massless goldstone modes of the superfluid state .
if you already have a dslr then a t-mount is pretty cheap and will give you steadier photos . the first issue you may hit is that you can only do short exposures unless you have a motorized mount . i use a lens mount adapter on my pentax k-x to attach a 1.25" barlow ( http://www.amazon.com/gp/product/b00009x3uv/ ) and it works quit well for me . but i also have a friend to does wonderful stuff using a canon 20x zoom ( non slr ) connected to a motorized mount . one of the tricks to learn is image stacking . registax ( http://www.astronomie.be/registax/ ) for windows does a very good job and is free . there are other apps for macos . this lets you combine multiple exposures to cancel out noise and to enhance your images .
ampre 's law does not say that the field is zero if there is no enclosed current . it says that if you integrate $b . d\ell$ round a loop then the result will be zero if there is no enclosed current . for example consider this loop in a constant magnetic field $b$: even though the magnetic field is non-zero , for every $d\ell$ in the loop there is a matching $d\ell$ pointing in the other direction and the two values of $b . d\ell$ will cancel out . this means the integral round the lop is zero even though $b$ is non-zero . as rijul says , ampre 's law only applies in some circumstances i.e. when the current density is not changing with time . however it does apply in this case .
see http://en.wikipedia.org/wiki/thermal_conductivity in metals , i think it generally has to do with the higher valence band electron mobility , but it gets more interesting elsewhere . in metals , thermal conductivity approximately tracks electrical conductivity according to the wiedemann-franz law , as freely moving valence electrons transfer not only electric current but also heat energy . however , the general correlation between electrical and thermal conductance does not hold for other materials , due to the increased importance of phonon carriers for heat in non-metals . as shown in the table below , highly electrically conductive silver is less thermally conductive than diamond , which is an electrical insulator .
a standart way of computation is to use jones calculus . in this formalism complex amplitude of the electric field is expressed as a two-component vector : $$\left|\mathcal{e}\right&gt ; =\left ( \begin{array}{c} \mathcal{e_x}\\ \mathcal{e_y}\\ \end{array} \right ) = \left|\mathcal{e}\right|\left ( \begin{array}{c} \mathcal{a}\\ \mathcal{\sqrt{1-a^2}e^{i\varphi}}\\ \end{array} \right ) , $$ whith $0\leq a\leq1$ ( usually one uses normalized jones vectors , but let 's leave it as is ) . any polarization-sensitive optical element is described by some operator acting on jones vectors . a polarizer , rotated at angle $\alpha$ with respect to $x$ axis is essintially a projector on $\left|p_\alpha\right&gt ; =\left ( \cos{\alpha} , \sin{\alpha}\right ) ^t$ , and the intensity is modulus squared of the inner product $i=\left|\left&lt ; p_\alpha|\mathcal{e}\right&gt ; \right|^2$ . simple calculation leads to the following result : $$i ( \alpha ) =\left|\mathcal{e}\right|^2\left ( a^2\cos^2\alpha+ ( 1-a^2 ) \sin^2\alpha+a\sqrt{1-a^2}\sin 2\alpha \cos \varphi\right ) , $$ which is in general not an ellipse . as an example , that is how it looks like for $a=1/\sqrt{2} , \varphi=\pi/3$:
first equation is wrong , it should say $\rho ( \vec{r} ) = \delta ( \vec{r} - \vec{r}' ) q$ . ( note that you had two errors ) . you treat it like a normal charge density $\rho ( \vec{r} ) $ , if you integrate the density over any volume you get the total charge within that volume .
i read this question but i did not understand the physics equations used in the answer . let me offer a simplified explanation . the origional big bang cosmology asserts that the universe is expanding , which if true means that the proper distance $d$ between any 2 points is increasing . since you can think of this expansion as " space itself expanding , " then the rate of at which 2 points separated by $d$ are changing is proportional to $d$ itself and thus the " velocity " $v$ ( or rate of change of the proper distance $d$ ) is $$v\propto d\rightarrow v= h d . $$ this is hubble 's law , with the constant $h$ relating $v$ and $d$ known as hubble 's constant due to it is empirical discovery/verification by edwin hubble . we detect a higher redshift in galaxies which are further away , and we infer that the expansion of the universe is accelerating . the redshift is related to the velocity $v$ between the source of the light and and receiver/observer and can be computed by the ( relativistic ) doppler shift given knowledge of the value of $v$ . if we lived in a universe that had a constant expansion rate than the velocity ( as observed via the redshift ) would simply be proportional to the distance $d$ by hubble 's law . the problem is hubble 's constant $h$ , is not really a constant but depends on time i.e. $h\rightarrow h ( t ) $ . in addition , because it takes time for light to travel , the further away an object is the further back in time we are effectively looking . therefore looking at the redshift of objects at very large distance $d$ from us will give us information about what $h$ was in the past . what we find when we look at far objects is that $h$ was smaller in the past , i.e. that the expansion of the universe is accelerating with time .
first , field strength . this calculation is strictly an electric potential calculation ; radiation and induction are safely ignored at 50hz . * for a 200kv transmission line 20m above ground , the max electric field at ground level is about 1.2 kv/m . ** this number is reduced from the naive 200kv/20m=10 kv/m calculation by two effects : 1 ) the ~1/r variation in the electric field ( reduction to 3 kv/m ) . i used the method of images to calculate this field , with a 10 cm conductor diameter to keep the peak field below the 1mv/m breakdown field . 2 ) cancellation from the other two power lines in this 3-phase system , which are at +/-120 degree electrical phases with respect to the first , and are physically offset in a horizontal line per the photo . i estimated 7m spacings between adjacent lines . the maximum e-field actually occurs roughly twice as far out as the outermost line ; the field under the center conductor is lower . next , can you feel it ? 1 ) the human body circuit model for electrostatic discharge is 100pf+1.5kohm ; that is a gross simplification but better than nothing . if one imagined a 2m high network , the applied voltage results in a 50hz current of about 70ua ( $c \omega v$ ) . very small . 2 ) there will be an ac voltage difference between the ( insulated ) human and ( insulated ) bicycle . a 1m vertical separation between their centers of gravity would yield roughly 1200v . this voltage is rather small compared to some car-door-type static discharges , but it would still be sufficient to break down a short air gap ( but not a couple cm ) , and would repeat at 100hz . i imagine it would be noticeable in a sensitive part of the anatomy . if the transmission voltage is actually 400 kv , all the field strengths and voltages would of course double . ( * ) in response to a comment , here 's an estimate of the neglected induction and radiation effects , courtesy of maxwell 4 and 3: induction : suppose a power line is carrying a healthy 1000a ac current ( f=50 hz ) . then by ampere 's law , there is a circumferential ac magnetic field ; at the wire-to-ground distance of 20 meters that field 's amplitude is $10 \mu t$ . ( compare with the earth 's dc field of approximately 0.5 gauss , or $50 \mu t$ . ) the flux of this magnetic field through a $1 m^2$ area loop ( with normal parallel to the ground and perpendicular to the wire ) is $\phi = 10 \mu wb$ ac . then from faraday 's law , the voltage around the loop is $d \phi /dt = 2 \pi f \phi = 3 mv$ ( millivolts ) . so much for induction . one can also estimate the magnetic field resulting from the $1200 v/m$ ground-level ac electric field , which has an electric flux density $d =\epsilon_0 e = 10.6 nc/m^2$ and a displacement current density $\partial d / \partial t = 2 \pi f d = 3.3 \mu a/m^2$ . the flux of this field through a $1 m$ square loop ( parallel to the ground ) is $3.3 \mu a$ , so the average magnetic field around the square is $0.8 \mu a/m$ , for a ridiculously small magnetic flux density of $1 pt$ . ( ** ) 1 sep 2014 update . dmytry very astutely points out in a comment that there will be local electric field intensification effects from conductive irregularities in the otherwise flat ground surface , such as our cyclist ( who , being somewhat sweaty , will have a conductive surface ) . the same principle applies to lightning rods . for the proverbial spherical cyclist , the local field will be increased by a factor of 3 , independent of the sphere 's size , as long as it is much less than the distance to the power line . it turns out that it does not matter whether the sphere is grounded or insulated , since its total charge remains 0 . for more elongated shapes the intensification can be much higher : for a grounded prolate spheroid with 10:1 dimensions , the multiplication factor is 50 . this intensification of course enhances any sensation one might feel .
this particle with have an unphysical wave function which blows up ( as can be quite easily derived ) . therefore , in quantum mechanics , we do not have any particles with $e&lt ; v_\text{min}$ .
when you bend a plastic ruler you are stressing the molecular/atomic structure of the material ( polymer ) . if you put something into mechanical stress you are putting such system into a higher energy state that when it is not . if you stress it enough it will break and liberate most of that energy " at once " in the form of mechanical vibrations ( a . k.a. sound ) .
your $m$ should be $m1+m2$ . . . . . . .
when working with audio signals , you must look at the signals as phasors rather than plain numbers . as such , the signals will be represented as complex numbers . for example , a given sinusoidal signal $se^{j\phi}$ has an amplitude of s , and a phase angle of $\phi$ . inverting the signal means increasing its phase angle by 180 degrees , or $\pi$ radians . so when adding the two signals together what you are doing is represented by the equation ( omitting conjugate pairs ) : $$s_{sum}=se^{j\phi}+se^{j ( \phi+\pi ) }$$ $$s_{sum}=se^{j\phi}+se^{j\phi}e^{j\pi}$$ $$e^{j\pi}=-1$$ $$s_{sum}=se^{j\phi}-se^{j\phi}=0$$ in terms of the intensity going up by 6db when the intensity is doubled , this can be calculated by using the equation : $$ 10log ( i_2/i_1 ) db$$ intensity is directly proportional to the square of the amplitude , so doubling the amplitude will result in the equation : $$ 10log ( 4s^2/s^2 ) db=6db$$ doing the other calculations you mentioned is a simple matter of trigonometric arithmetic . for example , in the case of squaring the sinusoidal signal , the result is a sinusoid of twice the frequency and half the phase . this is demonstrated by the identity : $$cos^2 ( x ) =\frac{1}{2} ( cos ( 2x ) +1 ) $$
suppose that for all $z$ in some open set $z$ of complex numbers containing $z_0$ , the hamiltonian $h ( z ) $ is a compact perturbation of the self-adjoint $h ( z_0 ) $ depending analytically on $z$ . then , for every simple eigenvalue $e_0$ of $h ( z_0 ) $ and associated normalized eigenstate $\psi_0$ , there exist a complex neighborhood $n$ of $z_0$ and unique functions $e ( z ) $ and $\psi ( z ) $ , defined and analytic on $n$ , such that $e ( z_0 ) =e_0$ , $\psi ( z_0 ) =\psi_0$ , and $h ( z ) \psi ( z ) =e ( z ) \psi ( z ) $ and $\psi_0^*\psi ( z ) =1$ for all $z\in n$ . the proof is essentially the inverse function theorem in a banach space for the resulting nonlinear system , combined with the spectral theorem applied to $h ( z_0 ) $ . i guess you can find the relevant background results ( if not a perturbation statement similar to the above ) in the old book by kato . no assumption is needed that $h ( z ) $ is self-adjoint ( would not be the case for all $z\in z$ unless $h ( z ) $ is constant ) . of course the eigenvalues will generally move into the complex domain if $z_0$ was real but $z$ is complex . weakening the assumptions will require stronger ( so-called ''hard'' ) forms of the inverse function theorem , which generally take a lot of technicality to state and verify .
the string one-loop cosmological constant is given by $$\lambda=\int_{\mathcal{f}}\frac{d^2\tau}{ ( im \tau ) ^2}z ( \tau ) $$ where the integration is over the teichmuller space of inequivalent tori and $z ( \tau ) $ is the partition function of the theory . the partition function keeps track of how many particles with any given mass appear in the spectrum of the theory , but not of their interactions . so , it can happen that two different vacua have the same partition function and therefore give the same cosmological constant . this was also discussed in the post " same partition functions , different theories " . nb : that being said , note that supersymmetric theories formally have $z ( \tau ) =0$ since the contributions of bosons and fermions exactly cancel each other , so if you are only interested in such vacua i see nothing wrong with treating them as if all give the same cosmological constant ( zero ) .
if i am not misunderstood , the relation k = 2/ ( na ) tells that when k goes to zero , we are very very far away from the reference atom and when k = 2/a , we are one lattice constant ( a ) away in real space from reference atom . i think you are a little misunderstood . the momentum space vector $\bf{k}$ has little to do with where an object is physically positioned . rather , it tells you the direction and spatial frequency of a plane wave . so $|\mathbf{k}|=\frac{2\pi}{a}$ is the spatial wavevector of a plane wave which makes one full oscillation every interatomic distance . likewise , when $|\mathbf{k}|\ll\frac{\pi}{a}$ , you should visualize a plane wave which has a very long wavelength , on the order of several hundred interatomic distances or more . with respect to k = 0 eigenvalue , how do i make the connection in real space picture . can we say that when we are infinitely far away from our reference atom , the energy is e-2t ? not quite . the intuition of the tight binding model is that when individual atoms are brought together , the wavefunctions one would obtain for the crystal by a brute-force computation would look very similar to what you had see if you just placed a single-atom wavefunction at every lattice point ( giving a function which i will denote $\psi ( \mathbf{x} ) $ ) and then multiplied the whole assembly by a spatial plane wave , $e^{i\mathbf{k\cdot x}}$ . there is also a simple intuition as to why the $\mathbf{k}=0$ assembly has a lower energy than the $|\mathbf{k}|=\pi/a$ assembly . recall from freshman physics that things with short wavelength ( electromagnetic , de broglie , quantum plane waves , etc . ) have higher energy than things with long wavelength . try to visualize $\psi ( \mathbf{x} ) e^{i\mathbf{0\cdot x}}$ . now try to visualize $\psi ( \mathbf{x} ) e^{i\mathbf{k\cdot x}}$ where $|\mathbf{k}|&gt ; 0$ . which has a shorter wavelength ? the extra factor of $e^{i\mathbf{k\cdot x}}$ adds oscillations , which you had intuitively expect to boost the energy . so to summarize , the crystal has a low-energy wavefunction which looks very similar to $\psi ( \mathbf{x} ) e^{i\mathbf{0\cdot x}}$ and which has energy $e-2t$ , it has a high-energy highly-oscillatory wavefunction which looks very similar to $\psi ( \mathbf{x} ) e^{i\mathbf{k\cdot x}}$ where $|\mathbf{k}|=\pi /a$ and which has energy $e+2t$ , and it has a bunch of modes in between . and just to reiterate , it does not really have anything to do with how far you are from any particular reference atom . additionally , have you heard of " bonding " and " anti-bonding " orbitals ? note that when $|\mathbf{k}|=\pi /a$ , the wavefunction flips sign at each lattice point , whereas when $|\mathbf{k}|=0$ , the wavefunction is the same sign at each lattice point . thus the zero-momentum state can be considered a low-energy " bonding " state , and the states near the edges of the brillouin zone can be considered high-energy " anti-bonding " states .
your mistake is to assume that the cosmic microwave background constitutes a universal rest frame , because it does not . in an flrw universe there is a frame called the comoving frame or proper frame that is particularly mathematically convenient . this is the frame in which the comoving distance between all inhabitants of that frame is constant , so all the " stuff " in the universe is mutually stationary ( the comoving frame factors out the hubble expansion ) . given that we expect all the " stuff " in the universe to be created in a similar way we would expect it to be approximately stationary ( in a comoving sense ) so the sum total of everything , matter and energy , acts as a reference point for the comoving frame . so there is nothing special about the cmb . if you ignored the cmb and measured the earth 's velocity to all the galaxies we can see then you had expect to get the same result as measuring the earth 's velocity relative to the cmb . the cmb occupies the same frame as everything else because it was created in basically the same way . the only special thing about the cmb is that gravitational interactions have not given it various peculiar velocities as has happened for large aggregations of matter .
you should be able to use energy conservation to write down the velocities of the bodies as a function of time . $$ \textrm{energy conservation ( ke = pe ) : } \frac{p^2}{2}\left ( \frac{1}{m} + \frac{1}{m} \right ) = gmm\left ( \frac{1}{r} - \frac{1}{r_0}\right ) $$ and $$ \frac{dr}{dt} = - ( v + v ) = -p\left ( \frac{1}{m} + \frac{1}{m} \right ) $$ momentum conservation ensures that the magnitude of the momenta of both masses is the same . does this help ? substituting into the second equation from the first you should be able to solve for : $$ \int_0^t dt = -\int_{r_0}^0 dr \sqrt{\frac{rr_0}{2g ( m+m ) ( r_0-r ) }} = \frac{\pi}{2\sqrt{2}}\frac{r_0^{3/2}}{\sqrt{g ( m+m ) }} $$
we can be pretty sure that the sun was not part of a binary system , because in a binary system there is no mechanism for the stars to separate . in a system of three or more stars it is possible for the energy to be divided in a way that ejects one star while leaving the others bound . but this will not work for a system of only two stars . i suppose it is conceivable that the sun might have been ejected from a system of three stars , but again this is very unlikely . firstly the ejection of the sun would almost certainly have stripped it of all it is planets , or at least left them in wildly eccentric orbits . secondly if the sun had been ejected from another system you had expect it to be moving fast relative to the stars around it , and this is not the case .
a constitutive law is generally an algebraic relation which tells you the coefficients of a differential equation , while the governing equations are the differential equations themselves . for example , if i have a metal piston on top of a gas , i can write down the equation of motion for the piston $$m \ddot x - pa = 0$$ where p is the pressure in the gas and a is the area of the piston . without knowing how the pressure depends on the piston position , this is not a closed equation--- it refers to an undetermined quantity , the pressure . but the ideal gas law , that the pressure $p=c/ ( v-ax ) $ where c , v are constants , determines the pressure in terms of x , and gives $$ m \ddot x -{ ac\over ( v - ax ) } =0$$ now the equation is closed--- it tells you the future behavior of x knowing x alone . the ideal gas law is the constitutive relation in this case , while the differential equation is the governing equation . constitutive equations are algebraic , governing equations are differential .
the result you have got would be better known as this : $$\int_0^t\biggl ( \int_0^{t'} a\mathrm{d}t''\biggr ) \mathrm{d}t ' = \frac{1}{2}at^2$$ in other words , it is a derivation of the formula for uniformly accelerated motion . this derivation , or something like it , is one of the first things students in a good calculus-based introductory physics class learn . the only difference is that you have done it explicitly , using limits , rather than using the rules for integrating polynomials . that is a good thing ! it will help you understand where the formula comes from and what it means , and if you continue to do more with numerical integration ( as in your simulations ) , it is going to be very useful to know the details of how this stuff works . now , considering that this has been known for about 350 years , its applications have been pretty thoroughly explored . it is a part of classical kinematics , which is a branch of physics that analyzes simple motion without any quantum effects , so there is no special significance to the planck time with respect to this equation .
time is not a variable in quantum mechanics ( qm ) , it is a parameter much in the same way as it is in classical ( newtonian ) mechanics . so , if you have a hamiltonian , e.g. , for the harmonic oscillator , you have $\omega$ as a parameter , as well as the masses of the particle ( s ) involved , say $m$ , and you also have time even though it is not something that shows up explicitly in the hamiltonian ( remember explicit time dependency from classical mechanics : poisson brackets , canonical transformations , etc  in fact , you could get your answer straight from these kinds of arguments ) . in this sense , just like you do not have a ' transformation pair ' between $m$ and $\omega$ , you also do not have one between time and energy . what do you say to convince yourself that $\omega \neq -i\ , \partial_m$ ? why can not you use this same argument to justify $e \neq -i\ , \partial_t$ ? ; - ) i think roger penrose makes a nice illustration of how this whole framework works in his book the road to reality : a complete guide to the laws of the universe : check chapter 17 .
1 ) op wrote ( v1 ) : [ . . . ] and thus this leads me to believe that ${\bf a}$ should be somehow connected to momentum , [ . . . ] . yes , in fact the magnetic vector potential ${\bf a}$ ( times the electric charge ) is the difference between the canonical and the kinetic momentum , cf . e.g. this phys . se answer . 2 ) another argument is that the scalar electric potential $\phi$ times the charge $$\tag{1} q\phi$$ does not constitute a lorentz invariant potential energy . if one recalls the lorentz transformations for the $\phi$ and ${\bf a}$ potentials , and one goes to a boosted coordinate frame , it is not difficult to deduce the correct lorentz invariant generalization $$\tag{2} u ~=~ q ( \phi - {\bf v}\cdot {\bf a} ) $$ that replaces $q\phi$ . the caveat of eq . ( 2 ) is that $u$ is a velocity-dependent potential , so that the force is not merely ( minus ) a gradient , but rather on the form of ( minus ) a euler-lagrange derivative $$\tag{3}{\bf f}~=~\frac{d}{dt} \frac{\partial u}{\partial {\bf v}} - \frac{\partial u}{\partial {\bf r}} . $$ one may show that eq . ( 3 ) reproduces the lorentz force $$\tag{4}{\bf f}~=~q ( {\bf e}+{\bf v}\times {\bf b} ) , $$ see e.g. ref . 1 . references : herbert goldstein , classical mechanics , chapter 1 .
if you assume that no torque is imparted to the object during flight then you are entirely correct about the fact that the spear direction can not match the velocity direction . it may cross , perhaps cross twice , and perhaps tangent , but it will be a very different behavior . a front-loaded weight in significant atmosphere , however , has a strong tendency to orient itself along the direction of motion . given a shape with some symmetry , if the center of mass is offset from the center point of the object , then air resistance will tend to point the cm to the front of the object . in the case of a stick , you can imagine a constant force per unit length from the air , meaning that the effective point of action is in the geometric middle . if the cm is not in that location , an effective torque will be present . my favorite example of this is lawn darts . the cm is intentionally place very far in the front and the object has a very significant air resistance to mass ratio . it orients itself with the tail fin trailing behind no matter how you throw it .
the simplest way to think of it is to note that the nucleons in a heavy nucleus are arranged in a shell structure not unlike the electrons in an atom , and when you want to form that alpha particle you have to grab two protons and two neutrons to do it . they do not have to be the highest energy ones . if you pull some of them out of low lying shells you leave a hole in the remnant nucleus which means that it is in an excited state . this process is not unlike what happens in the production of characteristic atomic x-rays . now , there are some limits . in particular energy conservation set an absolute limit on how excited the remnant can be . there are also correlations between the nucleons in the parent nucleus that contribute to the selection process ( imprecisely some pairs of nucleons are " near " each other more often than others ) .
yes , you can . and you do not even need to leave the earth to do it . you are always viewing things in the past , just as you are always hearing things in the past . if you see someone do something , who is 30 meters away , you are seeing what happened $ ( 30\ ; \mathrm{m} ) / ( 3\times10^8\ ; \mathrm{m}/\mathrm{s} ) = 0.1\ ; \mu\mathrm{s}$ in the past . if you had a mirror on the moon ( about 238k miles away ) , you could see about 2.5 seconds into earth 's past . if that mirror was on pluto , you could see about 13.4 hours into earth 's past . if you are relying on hearing , you hear an event at 30 m away about 0.1 s after it occurs . that is why runners often watch the starting pistol at an event , because they can see a more recent picture of the past than they can hear . to more directly answer the intent of your question : yes , if you could magically be transported 27 lightyears away , or had a mirror strategically placed 13.5 lightyears away , you could see yourself being born .
first the math : given the electric potential $v ( \mathbf a ) $ at a point $\mathbf a$ in space and the potential $v ( \mathbf b ) $ at another point $\mathbf b$ , the electric potential energy of a charge moving from point $\mathbf a$ to point $\mathbf b$ will change by $$ u ( \mathbf b ) - u ( \mathbf a ) = q [ v ( \mathbf b ) - v ( \mathbf a ) ] $$ if the speed of an electron decreases in moving from a point $\mathbf a$ to a point $\mathbf b$ , then that must mean that its potential energy has increased : $u ( \mathbf b ) - u ( \mathbf a ) &gt ; 0$ , so that since the electron has charge $-e$ , we get $$ ( -e ) [ v ( \mathbf b ) - v ( \mathbf a ) ] &gt ; 0 $$ which , upon dividing both sides by $-e$ ( and noting to change the direction of the inequality when we do so ) implies $$ v ( \mathbf b ) &lt ; v ( \mathbf a ) $$ so we see that the starting position $\mathbf a$ is at a higher potential than the final position $\mathbf b$ as the second paragraph says . the intuition here is that there are some other charges around ( they could be positive or negative depending on where we place them ) that setup the potential " landscape " v in such a way that the potential at the origin , where the charge began , is higher than the potential at the final point , and a negative charge that rolls " down this potential hill " will be going slower when it gets to the bottom of the hill . hope that was not confusing ! cheers !
ok i will answer my own question . i asked my qft professor , he said different methods of regularization will give different answers . but at the end of the day it does not matter because you are going to cancel whatever divergences from that integral from whatever method you use , with the mass counterterm anyway , to impose desired renormalization conditions . so that integral is not physical and all is good .
an excellent book which does more or less what you ask for is asher peres ' " quantum theory:concepts and methods " . it starts from the stern-gerlach experiments and logical reasoning to develop the basic principles of quantum mechanics . from there , it develops the necessary algebra . another interesting book for an approach of the conceptual side of quantum mechanics is " quantum paradoxes " by aharonov and rohrlich . but to fully appreciate this one , i think you will need to go through a standard curriculum first . then , there is " quantum computation and quantum information " by nielsen and chuang , which is meant as an introduction to the ideas of qm as applied to information theory for people with an informatics background mostly . so it also starts from an algebraic and conceptual approach .
i am not sure it is the very first , but an early paper is artificial radioactivity produced by neutron bombardment , fermi et al . , proc . roy . soc . london a 146 , 483500 ( 1934 ) . first footnote on the first page refers to about eight " preliminary results announced in short communications , " mostly with fermi as an author .
the wave equation can be written as $\nabla_i\nabla^i\varphi = 0$ where $\nabla$ is the levi-civita connection on minkowski space , $\varphi$ does not need to be lorentz invariant . $\partial_i\partial^i\varphi = 0$ is the laplace equation on both minkowski and euclidean space since ordinary partial derivatives do not respect the metric . in ( pseudo ) riemannian geometry the covariant derivative $\nabla_i$ replaces partials $\partial_i$ . the laplace-beltrami operator $$\delta \triangleq \nabla^i\nabla_i$$ is a common generalization of both the d'alembertian and the laplacian , $$\delta \varphi = 0$$ is the laplace-beltrami equation . your observation then generalizes to say that , for scalar field , a vanishing laplace-beltrami operator is the same as a divergence free contravariant gradient . this is universally valid since $$\delta \varphi = \nabla^i\nabla_i\varphi = g^{ij}\nabla_i\nabla_j\varphi= \nabla_i\nabla^i\varphi$$ it does not depend on the metric or coordinate system . however , the precise geometric meaning of this depends on the metric . in euclidean spaces it means a function is harmonic iff it has a divergence free gradient since the laplace-beltrami operator is just the laplacian . in minkowski space the laplace-beltrami operator is the d'alembertian , and the laplace-beltrami equation becomes the wave equation . in minkowski space covariant derivatives are just ordinary partial derivatives , as in euclidean space , since there is no curvature , making christoffel symbols vanish . furthermore for scalars $$\nabla_i \varphi = ( d\varphi ) _i = \partial_i\varphi$$ is true in all metrics . however the contravariant derivative $\nabla^i\varphi$ is not the usual gradient vector since index raising depends on the the metric . the gradient is therefore more naturally thought of as a covector or a 1-form . geometrically the minkowski gradient vectors are the time reflections of what the euclidean gradient vectors would be . i do not have a good intuitive picture of contravariant derivatives and their divergences in general non euclidean spaces though . note that for $$\varphi \triangleq t^2 + x^2$$ in euclidean space $$\nabla_i\nabla^i\varphi = 4$$ while in minkowski space $$\nabla_i\nabla^i\varphi = 0$$ making it a solution to the equation even though it is not lorentz invariant . a general vector field $j^i$ may satisfy $\nabla_i j^i = 0$ , but still have non zero curl or in more generally $\nabla_{ [ i} j_{j ] } \neq 0$ . such a field is not a gradient of a scalar field . so at least $\nabla_{ [ i} j_{j ] } = 0$ is required . the poincare lemma says this is sufficient for fields on contractible subsets of euclidean space .
the dividing line meets at $ ( \omega_m , \omega_\lambda ) = ( 0,1 ) $ . from the friedmann equations , it follows that the scale factor $a ( t ) $ satisfies the relation $$ \frac{\dot{a}^2}{h_0^2} = \omega_m a^{-1} + ( 1 - \omega_m - \omega_\lambda ) + \omega_\lambda a^2 . $$ the universe has no big bang singularity if the above expression is negative ( or zero ) for some ( small ) values of $a$ . so we have to examine under which conditions $$ \frac{\dot{a}^2}{h_0^2} \leqslant 0 . $$ let us first assume $\omega_m = 0$ . then the condition is $$ ( 1 - \omega_\lambda ) + \omega_\lambda a^2 \leqslant 0 , $$ which implies $\omega_\lambda\geqslant 1$ . the value $ ( \omega_m , \omega_\lambda ) = ( 0,1 ) $ is in fact a special case , because then we have $$ \frac{\dot{a}^2}{h_0^2} = \omega_\lambda a^2 , $$ with solution $a ( t ) \sim \exp ( th_0\sqrt{\omega_\lambda} ) $ , which has no big bang , since $a\rightarrow 0$ if $t\rightarrow-\infty$ , i.e. the singularity lies in the infinite past . if $\omega_m &gt ; 0$ , then we require even higher values of $\omega_\lambda$ to get a universe without a big bang . this follows from the fact that , for small values of $a$ , we have $$ \begin{multline} \omega_m a^{-1} + ( 1 - \omega_m - \omega_\lambda ) + \omega_\lambda a^2 &gt ; \\ \omega_m + ( 1 - \omega_m - \omega_\lambda ) + \omega_\lambda a^2 = ( 1 - \omega_\lambda ) + \omega_\lambda a^2 , \end{multline} $$ so for a given $a$ , the values of $\dot{a}^2/h_0^2$ in the general case are larger than the $ ( \omega_m = 0 ) $ case .
broadly speaking , fire is a fast exothermic oxidation reaction . the flame is composed of hot , glowing gases , much like a metal that is heated sufficiently that it begins to glow . the atoms in the flame are a vapor , which is why it has the characteristic wispy quality we associate with fire , as opposed to the more rigid structure we associate with hot metal . now , to be fair , it is possible for a fire to burn sufficiently hot that it can ionize atoms . however , when we talk about common examples of fire , such as a candle flame , a campfire , or something of that kind , we are not dealing with anything sufficiently energetic to ionize atoms . so , when it comes to using something as an example of a plasma for kids , i am afraid fire would not be an accurate choice .
suppose we assume that an object 's statistics depend only on its spin and not on whether the object is composite or fundamental . this assumption seems natural , since if it failed , it would be too good to be true -- it would give us a way of finding out about the internal structure of any particle , at all scales , without having to build particle accelerators . given that assumption , the full theorem follows directly from the spin-1/2 case . any spin can be realized by coupling spin 1/2 's . given that spin 1/2 has an eigenvalue of $-1$ under particle exchange , coupling $n$ of them produces a composite system that has an eigenvalue of $ ( -1 ) ^n$ .
it holds that $\sin ( x ) = - \sin ( -x ) $ and therefore $\sin ( \omega t - kx ) = - \sin ( kx - \omega t ) $ .
the composition law for quantum systems is always a tensor product . your problem arises from a confusion over what the tensor product is applied to : you are trying to tensor product the spatial coordinates together , when it is in fact the basis vectors of the hilbert space you should be tensoring together . more formally , take two quantum systems a and b , with corresponding hilbert spaces $\mathcal{h}_a$ and $\mathcal{h}_b$ . the state of the joint system ( a and b ) lives in the tensor product space $\mathcal{h}_{ab} = \mathcal{h}_a \otimes \mathcal{h}_b$ . this is a fancy-pants way of saying that the basis vectors of the combined hilbert space look like $$|\phi\rangle = |a\rangle\otimes|b\rangle . $$ a general state can therefore be written as $$|\psi\rangle = \sum\limits_{jk} c_{jk} |a_j\rangle \otimes |b_k\rangle , $$ where the $\{|a_j\rangle\}$ and $\{|b_k\rangle\}$ span hilbert spaces $\mathcal{h}_a$ and $\mathcal{h}_b$ respectively , and $c_{jk}$ are complex numbers . as you would expect , if the two hilbert spaces have dimension $d$ ( if they are spanned by $d$ basis vectors each ) then the combined hilbert space has dimension $d^2$ . a general $n$-body hilbert space has dimension $d^n$ . if the two quantum systems are the position degrees of freedom of two particles , then the total wavefunction looks like $$\psi = \sum\limits_{jk} c_{jk} \psi_j^{ ( 1 ) } ( x_1 , y_1 , z_1 , t ) \psi^{ ( 2 ) }_k ( x_2 , y_2 , z_2 , t ) , $$ which clearly depends on 6 spatial coordinates plus time .
not quite . the higgs mechanism actually applies at low energies . do not think of it as an event that happens once and bestows mass upon all particles for the rest of time ; instead , the higgs mechanism is a continuous effect that explains how particles are able to have mass at low energies . for a full ( er ) explanation , i will point you to another answer of mine , but the gist of it is that the broken symmetry that produces the higgs mechanism is a symmetry of the higgs field . if the field is $\phi$ , it has an associated potential energy density $-a|\phi|^2 + b|\phi|^4$ , which has a maximum at $\phi = 0$ and a continuum of minima along a ring at $|\phi| = \sqrt{\frac{a}{2b}}$ . early in the universe , the field would have been excited to much higher energies than either the central maximum or the ring minima , but as the universe cools and the energy density drops , the field " settles " into some minimum along the ring .
there is a conversion factor , but it is not just a multiplication because the two scales do not have their zero at the same point . the celcius scale is defined as having its zero at the freezing point of water . the fahrenheit scale , on the other hand , appears simply arbitrary to convert from celcius to fahrenheit , you multiply by $9/5$ ( the ratio to account for the difference in slope ) and then add $32$ to account for the difference in zeros .
for string theory : if the lhc sees no higgs , then the standard model has to change ( a bit or a lot ) , so string phenomenology will have a slightly different target to aim at . for loop quantum gravity : they still can not even describe a particle moving through space , so they have bigger things to worry about than the higgs .
amazingly , not only has someone recently done an analysis of the tarzan swing , but they have posted it on the arxiv . see http://arxiv.org/abs/1208.4355 for the article or the arxiv blog for a summary . to quote the arxiv blog article : in fact there is no simple rule for maximising the horizontal flight distance . it turns out this depends on a number of factors , such as rope swing 's distance off the ground , the length of the rope and the angle of the rope when tarzan begins his swing as well as the angle of the rope at the point of release . however the optimal angle of release is always less than 45 degrees .
the solve the first part with just kinematics , use the chain rule : $$ a ( x ) = \frac{dv}{dt} = \frac{dv}{dx}\frac{dx}{dt} = \frac{dv}{dx}v , $$ and then $a ( x ) dx = vdv$ . integrating both sides ( $x_0$ to infinity on the left and $v_0$ to $v_f$ on the right ) , we get $$\frac{k}{x_0} = \frac{v_f^2 - v_0^2}{2} , $$ or $$v_f = \sqrt{\frac{2k}{x_0} + v_0^2} . $$ solving the two particle scenario is no more complicated than the single particle version as long as you pay attention to signs for particle 2 .
if it is a light bulb or heater , it is just a resistor . first , forget that it is alternating current , just to simplify things . think of the power source as a really big 220 volt battery . if it is drawing 1500 watts , divide that by 220 , and that will tell you the current i in amperes . ( that just measures how many electrons per second are flowing . an ampere is about 6x10^23 electrons per second . ) to get the resistance r in ohms , just divide the voltage v ( 220 volts ) by the current i that you got above . ( an ohm is just the number of volts it takes to push one ampere through the resistor . ) i hope you can see that the smaller the resistance is , the bigger the current is , and when you multiply that by the voltage , you get the power . so the way you make a bigger heater or light bulb is by giving it less electrical resistance . if you want to go back to alternating current ( ac ) the power is a time-average , and it swings up and down at twice the ac frequency . i will let you figure out why , if you want .
each observer has their own proper time measured by the clock in their rest frame . however , one man 's proper time is not another man 's proper time . time dilation means that each observer will see the other observer 's clock running slower ( compared to their own proper time measuring clock ) . but everything is perfectly symmetric from either observer 's point of view as long as the relative motion is uniform . you measure your clock ticking at the " normal rate " ( your proper time ) and you see the other person 's clock ticking at a slower rate . similarly the other person measures their clock ticking at the " normal rate " ( their proper time ) and they see your clock ticking at a slower rate . this is all well and nice , but it gets interesting when the two compare their clocks after one of them does a round trip . this means that one of them necessarily had to accelerate and decelerate and was not in uniform motion ( technically , was not on a geodesic ) . now you have an opportunity to actually compare those two clocks and you will always find that the person in uniform motion ( in this case , the observer at rest on earth ) was the one whose clock has ticked the most , and hence aged the most . the best way to understand this is to realize that the length of paths in spacetime is measured by the total proper time along that path ( measured by that path-traveller 's clock in their rest frame ) . one can show that the paths of uniform motion ( geodesics ) have that length maximized , so any path that deviates from a geodesic ( because of accelerations ) , will necessarily measure a shorter total proper time after a round trip . edit after first comment : time dilation is not the appropriate effect to consider in this particular problem -- length contraction is . in nick 's frame , a length contracted ship passes by at speed $v$ . in molly 's frame , a point-object ( heh ) nick passes by an uncontracted ship at speed $v$ . clearly , this should happen quicker in nick 's frame because of the length contraction . thinking in terms of time dilation simply does not help here . think from the point of view of each observer and it will be quickly obvious which effect to use .
the dirac equation for a particle with charge $e$ is $$ \left [ \gamma^\mu ( i\partial_\mu - e a_\mu ) - m \right ] \psi = 0 $$ we want to know if we can construct a spinor $\psi^c$ with the opposite charge from $\psi$ . this would obey the equation $$ \left [ \gamma^\mu ( i\partial_\mu + e a_\mu ) - m \right ] \psi^c = 0 $$ if you know about gauge transformations $$ \psi \rightarrow \exp\left ( i e \phi\right ) \psi $$ ( together with the compensating transformation for $a_\mu$ , which we do not need here ) , this suggests that complex conjugation is the thing to do : $$ \psi^\star \rightarrow \exp\left ( i ( -e ) \phi\right ) \psi^\star $$ so it looks like $\psi^\star$ has the opposite charge . let 's take the complex conjugate of the dirac equation : $$ \left [ -\gamma^{\mu\star} ( i\partial_\mu + e a_\mu ) - m \right ] \psi^\star = 0 $$ unfortunately this is not what we want . but remember that spinors and $\gamma$ matrices are only defined up to a change of basis $\psi \rightarrow s \psi$ and $\gamma^\mu \rightarrow s \gamma^\mu s^{-1}$ . possibly we can find a change of basis that brings the dirac equation into the form we want . introduce an invertible matrix $c$ by multiplying on the left and inserting $ 1 = c^{-1}c $ ( note that $c$ is the more common notation for your $\tilde{u}$ ) : $$ \begin{array}{lcl} 0 and = and c \left [ -\gamma^{\mu\star} ( i\partial_\mu + e a_\mu ) - m \right ] c^{-1} c\psi^\star \\ and = and \left [ -c\gamma^{\mu\star}c^{-1} ( i\partial_\mu + e a_\mu ) - m \right ] c\psi^\star \end{array}$$ note that if we can find a $c$ which obeys $-c\gamma^{\mu\star}c^{-1} = \gamma^\mu$ then $c\psi^\star$ makes a perfectly good candidate for $\psi^c$ ! it turns out that one can indeed construct $c$ satisfying the condition and define charge conjugation as $$ \psi \rightarrow \psi^c = c\psi^\star $$ you can see this more explicitly in terms of two component spinors in the weyl basis : $$ \psi = \left ( \begin{matrix} \chi_\alpha \\ \eta^{\dagger}_{\dot{\alpha}} \end{matrix} \right ) $$ ( the notation follows the tome on the subject ) . the charge conjugate spinor in this representation is $$ \psi^c = \left ( \begin{matrix} \eta_\alpha \\ \chi^{\dagger}_{\dot{\alpha}} \end{matrix} \right ) $$ so charge conjugation is $$ \eta \leftrightarrow \chi $$ this representation explicitly brings out the two oppositely charged components of the dirac spinor , $\eta$ and $\chi$ , and shows that charge conjugation acts by swapping them . to recap : we want to define a charge conjugation operation so that given a $\psi$ with some electric charge $e$ , we can get a $\psi^c$ with charge $-e$ . complex conjugating the dirac equation gets us there , but the resulting spinor $\psi^\star$ is in a different spinor basis so the dirac equation is not in standard form . we introduce a change of basis $c$ to get the dirac equation back in standard form . the necessary conditions for this to work are that $c$ is invertible ( otherwise it would not be a change of basis and bad things would happen ) and $-c\gamma^{\mu\star}c^{-1} = \gamma^\mu$ .
i ) assume that $$ \mathbb{r}_{\geq 0}~\ni~ r \quad\stackrel{v_{\rm eff}}{\mapsto} \quad v_{\rm eff} ( r ) ~\in~ \mathbb{r} $$ is a continuous function . then energy conservation reads $$e~=~ \frac{m}{2} \dot{r}^2+ v_{\rm eff} ( r ) . $$ a necessary and sufficient condition for the existence of a radially bounded orbit ( that is not necessarily periodic/closed or stable ) is that there exists an energy level $e$ such that at least one of the connected components ( which are necessarily intervals or points ) of the preimage $$v_{\rm eff}^{-1} ( ] -\infty , e ] ) ~\subseteq~ \mathbb{r}_{\geq 0}$$ is a radially bounded set . ii ) it seems that the only place where goldstein ( in his book classical mechanics ) considers the concavity condition $$v_{\rm eff}^{\prime\prime} ( r ) ~&gt ; ~0$$ is in the beginning of section 3.6 , where he discusses stable circular orbits . he uses concavity to conclude the relatively mild inequality $p&gt ; -3$ for a power-law force $f ( r ) \propto r^p$ .
consider the projectile at an initial position $ ( x_0 , y_0 ) $ , given an initial velocity of $u$ making an angle $\theta$ above the horizontal . \begin{align} u_x and = u \cos ( \theta ) ; \\ u_y and = u \sin ( \theta ) ; \\ \end{align} the velocity remains constant in the $x$ direction , if you neglect dissipative effects like drag . the velocity in the $y$ direction changes due to gravity : \begin{align} v_x and = u_x ; \\ v_y and = u_y - gt ; \end{align} the x and y displacements can be given as \begin{align} s_x and = u_x t ; \\ s_y and = y_y t - \frac{1}{2}gt^2 ; \end{align} the position of the projectile , hence , is : \begin{align} x and = x_0 + s_x = x_0 + u_x t ; \\ y and = y_0 + s_y = y_0 + u_y t - \frac{1}{2}gt^2 ; \end{align} suppose the projectile is launched from a hill 100m above ground level . you want to find the angle of launch which will allow you to hit an object on the ground , 1000m away . this gives you : \begin{align} x_0 and =0 ; \\ y_0 and =100 ; \\ x_{final} and =1000 ; \\ y_{final} and =0 ; \end{align} putting these values in the equations for $x$ and $y$ , \begin{align} 1000 and = 0 + u \cos ( \theta ) \times t ; \\ 0 and = 100 + u \sin ( \theta ) \times t - \frac{1}{2}gt^2 ; \end{align} you now have 2 equations , with 2 variables ( $t$ and $\theta$ ) , which you can solve to get the answer . note : the equation is quadratic in $t$ , meaning you will get 2 values for $t$ . one of these can be eliminated ( you will see why when you solve it )
at your level of understanding , the bohr atom will do . atoms are neutral , composed of orbiting electrons ( negatively charged ) around a nucleus ( positively charged ) . the very basic question coming out of this fact , that an atom is composed of orbiting electrons around a positive nucleus is : how can it be possible when we know that accelerating charges makes them radiate electromagnetic waves away and lose momentum . the electrons should fall into the nucleus since a circulating charge has a continuous acceleration , and matter as we know it could not exist . enter the bohr model : it postulated that there were some orbits where the electrons could run around without losing any energy , quantized orbits . enter the absorption lines : the electrons could only change orbits if kicked up by an electromagnetic wave of an energy specific to that particular orbit and discrete . there fore if one shone that specific frequency of light ( e=h*nu ) on a specific atom there was a probability to kick an electron up to a higher , called excited , orbit . enter the emission lines : once excited there was a probability for the electron to fall back emitting the specific energy it had absorbed before . this could be observed . different experiments will show the different behaviors , even though absorption and emission will be happening continuously in the material . an experiment shining light on the material and looking at the reflected spectrum will see absorption lines at those frequencies , because the relaxation of the excited electrons will emit back radiation all around randomly , whereas the reflected spectrum is at a specific angle . an experiment out of the line of the exciting photons will see the emission spectrum . absorption spectra are useful for identifying elements in stars , the absorption happening in the star 's atmosphere and appearing as dark lines in the black body spectrum . it goes without saying that physics has moved to new horizons from the time that the bohr atom was news . it has been supereceded by quantum mechanics which gives tools to accurately predict and classify all spectra as the result of a coherent theory of the way the universe behaves ( i.e. quantum mechanically ) .
assuming that you mean root , 1 and that you fit to a TGraphErrors , then you are done . 1 first time i have heard it described as " a statistics tool " though it will do a passable job in that context .
well , probably , you should use momentum conservation for the system spacecraft+photons at two points : 1 ) photons fly towards the spacecraft at rest , 2 ) photons fly away from the accelerated spacecraft . remember that the spacecraft reflects , rather than emits photons . edit : the relation $p=\gamma^2 m_0$ for the spacecraft is incorrect , it should be $\gamma m_0$
a singularity is a condition in which geodesics are incomplete . for example , if you drop yourself into a black hole , your world-line terminates at the singularity . it is not just that you are destroyed . you ( and the subatomic particles you are made of ) have no future world-lines . a careful definition of geodesic incompleteness is a little tricky , because we want to talk about geodesics that can not be extended past a certain length , but length is measured by the metric , and the metric goes crazy at a singularity so that length becomes undefined . the way to get around this is to use an affine parameter , which can be defined without a metric . geodesic incompleteness means that there exists a geodesic that can not be extended past a certain affine parameter . ( this also covers lightlike geodesics , which have zero metric length . ) there are two types of singularities , curvature singularities and conical singularities . a black hole singularity is an example of a curvature singularity ; as you approach the singularity , the curvature of spacetime diverges to infinity , as measured by a curvature invariant such as the ricci scalar . another example of a curvature singularity is the big bang singularity . a conical singularity is like the one at the tip of a cone . geodesics are incomplete there basically because there is no way to say which way the geodesic should go once it hits the tip . in 2+1-dimensional gr , curvature vanishes identically , and the only kind of gravity that exists is conical singularities . i do not think conical singularities are expected to be important in our universe , e.g. , i do not think they can form by gravitational collapse . actual singularities involving geodesic incompleteness are to be distinguished from coordinate singularities , which are not really singularities at all . in the schwarzschild spacetime , as described in schwarzschild 's original coordinates , some components of the metric blow up at the event horizon , but this is not an actual singularity . this coordinate system can be replaced with a different one in which the metric is well behaved . the reason curvature scalars are useful as tests for an actual curvature singularity is that since they are scalars , they can not diverge in one coordinate system but stay finite in another . however , they are not definitive tests , for several reasons : ( 1 ) a curvature scalar can diverge at a point that is at an infinite affine distance , so it does not cause geodesic incompleteness ; ( 2 ) curvature scalars will not detect conical singularities ; ( 3 ) there are infinitely many curvature scalars that can be constructed , and some could blow up while others do not . a good treatment of singularities is given in the online book by winitzki , section 4.1.1 . the definition of a singularity is covered in wp and in all standard gr textbooks . i assume the real issue you were struggling with was the definition of timelike versus spacelike . in gr , a singularity is not a point in a spacetime ; it is like a hole in the topology of the manifold . for example , the big bang did not occur at a point . because a singularity is not a point or a point-set , you can not define its timelike or spacelike character in quite the way you would with , say , a curve . a timelike singularity is one that is in the future light cone of some point a but in the past light cone of some other point b , such that a timelike world-line can connect a to b . black hole and big bang singularities are not timelike , they are spacelike , and that is how they are shown on a penrose diagram . ( note that in the schwarzschild metric , the schwarzschild r and t coordinates swap their timelike and spacelike characters inside the event horizon . ) there is some variety in the definitions , but a timelike singularity is essentially what people mean by a naked singularity . it is a singularity that you can have sitting on your desk , where you can look at it and poke it with a stick . for more detail , see penrose 1973 . in addition to the local definition i gave , there is also a global notion , rudnicki , 2006 , which is essentially that it is not hidden behind an event horizon ( hence the term " naked" ) . what is being formalized is the notion of a singularity that can form by gravitational collapse from nonsingular initial conditions ( unlike a big bang singularity ) , and from which signals can escape to infinity ( unlike a black hole singularity ) . penrose , gravitational radiation and gravitational collapse ; proceedings of the symposium , warsaw , 1973 . dordrecht , d . reidel publishing co . pp . 82-91 , free online at http://adsabs.harvard.edu/full/1974iaus...64...82p rudnicki , generalized strong curvature singularities and weak cosmic censorship in cosmological space-times , http://arxiv.org/abs/gr-qc/0606007 winitzki , topics in general relativity , https://sites.google.com/site/winitzki/index/topics-in-general-relativity
you do not need to invoke friction . the magnetic forces are in equilibrium by themselves so if you place the magnets in that configuration , they will not spontaneously begin to move . the reason is that there is a corresponding force on the magnets when they are vertical that matches the ones you have already drawn . let me make a simple model . first of all , start by upping the game and including two big magnets , which can only make it better : if red is a north pole , then each rotating magnet , when horizontal , has a north pole repelling its backside north pole and a north pole attracting its front south pole . focusing on the big magnets for the moment , the fact that their north poles face each other suggests that we can trade them for a pair of anti-helmholtz coils . this means the important character of their field is its quadrupolar nature , and we can approximate the magnetic field as $$\mathbf b=\frac{b_0}a\begin{pmatrix}x\\y\\-2z\end{pmatrix} , $$ where the $z$ axis goes from one big magnet to the other , $a$ is some characteristic length , and $b_0$ is some characteristic field strength . now , for the little magnets , i think it is uncontroversial to model them as point dipoles . if $\theta$ is the angle the wheel spoke makes with the $x$ axis ( with the wheel in the $x , z$ plane ) then each magnet is a dipole with moment $$\mathbf m=m \begin{pmatrix}-\sin ( \theta ) \\0\\\cos ( \theta ) \end{pmatrix} \text{ at } \mathbf r=r \begin{pmatrix}\cos ( \theta ) \\0\\\sin ( \theta ) \end{pmatrix} . $$ with this , the potential energy of each spoke magnet is $$u=-\mathbf m\cdot\mathbf b=3\frac r a mb_0\sin ( \theta ) \cos ( \theta ) =\frac32\frac ra mb_0\sin ( 2\theta ) . $$ to see how this behaves , here is a colour plot of the energy , with negative energy in red and positive energy in blue . you can see there is a gradient pointing up on the right and down on the left . however , these are matched by clockwise gradients when the spokes are vertical . a single magnet will settle on the lower left or the upper right ; a pair of magnets will settle on that diagonal . for a symmetrical wheel with three or more magnets , the total potential energy is flat at zero , $$u=\sum_{k=1}^n\frac32\frac ra mb_0\sin\left ( 2\left ( \theta_0+\frac{2\pi}{n}k\right ) \right ) =\frac3{4}\frac ra mb_0\text{im}\left [ e^{2i\theta}\sum_{k=1}^n ( e^{2\pi i/n} ) ^k\right ] =\frac3{4}\frac ra mb_0\text{im}\left [ e^{2i\theta+2\pi i/n}\frac{1- ( e^{2\pi i/n} ) ^n}{1-e^{2\pi i/n}}\right ] =0 $$ and there is no resultant magnetic force .
it does not seem to have anything to do with phasors but it does since this is the way one adds phasors . assume you have two series connected circuit elements with phasor voltages that differ in phase by the angle $\phi$ . $$\vec v_1 = v_1$$ $$\vec v_2 = v_2e^{j\phi}$$ since the voltage across the series combination is the sum of the individual phasor voltages , the magnitude of the phasor voltage sum is given by : $$|\vec v_{sum}| = \sqrt{v^2_1 + v^2_2 + 2v_1v_2\cos\phi}$$ in the case that $\phi = 90^\circ$ , as for the series connected resistor and inductor , we have $$|\vec v_{sum}| = \sqrt{v^2_1 + v^2_2 + 2v_1v_2\cos90^\circ}= \sqrt{v^2_1 + v^2_2} $$ how do you know that they are 90 degrees out of phase ? the resistor and inductor are in series thus they have identical current $$\vec i_r = \vec i_l =\vec i $$ the voltage across the resistor is $$\vec v_r = r \vec i_r = r \vec i $$ and the voltage across the inductor is $$\vec v_l = j\omega l \vec i_l = j\omega l \vec i$$ we can express the voltage across the inductor in terms of the voltage across the resistor $$\vec v_l = j\frac{\omega l}{r}\vec v_r = \frac{\omega l}{r}\vec v_r\ , e^{j90^\circ}$$ thus , see that the inductor voltage and resistor voltage are in quadrature - they differ in phase by $90^{\circ}$
$i\hbar$ is simply a number , or if you must regard it as an operator , a multiple of the identity . so $\langle i\hbar \rangle=i\hbar$ , and so is $\langle -i\hbar \rangle$ . by the way , anticommutator of $\hat{x}$ and $\hat{p}$ is not $ [ \hat{p} , \hat{x} ] $ , but $\{\hat{x} , \hat{p}\}=\hat{x}\hat{p}+\hat{p}\hat{x}$ .
you did the rotation incorrectly--- rotating to add 90 degrees to $\theta$ does not add a constant to $\theta$ , it rotates the x-y plane by 90 degrees . the angle which used to be $\phi$ is now $\theta$-like in that it rotates in a plane including the z axis , and the $\phi$ dependence is completely changed . the easiest way to rotate this angular wavefunction is to note that it is the angular part of the quadrtic polynomial 2xy . rotating by 90 degrees around the y axis takes z to x and x to minus z , and so gives the polynomial - 2yz . subsituting the angular form of z and y , you get $$ a ( \theta , \phi ) = - \cos ( \theta ) \sin ( \theta ) \sin ( \phi ) = - \cos ( 2\theta ) \sin ( \phi ) $$ in general , do not rotate angular wavefunctions in polar coordinates . write them out as polynomials and rotate their rectangular coordinate form . there is a tabulated way to write the rotation for angular wavefunctions in terms of themselves , but this is usually more trouble than converting in the way i did above .
lorentz boost is simply a lorentz transformation which does not involve rotation . for example , lorentz boost in the x direction looks like this : \begin{equation} \left [ \begin{array}{cccc} \gamma and -\beta \gamma and 0 and 0 \newline -\beta \gamma and \gamma and 0 and 0 \newline 0 and 0 and 1 and 0 \newline 0 and 0 and 0 and 1 \end{array} \right ] \end{equation} where coordinates are written as ( t , x , y , z ) and \begin{equation} \beta = \frac{v}{c} \end{equation} \begin{equation} \gamma = \frac{1}{\sqrt{1 - \frac{v^2}{c^2}}} \end{equation} this is a linear transformation which given coordinates of an event in one reference frame allows one to determine the coordinates in a frame of reference moving with respect to the first reference frame at velocity v in the x direction . the ones on the diagonal mean that the transformation does not change the y and z coordinates ( i.e. . it only affects time t and distance along the x direction ) . for comparison , lorentz boost in the y direction looks like this : \begin{equation} \left [ \begin{array}{cccc} \gamma and 0 and -\beta \gamma and 0 \newline 0 and 1 and 0 and 0 \newline -\beta \gamma and 0 and \gamma and 0 \newline 0 and 0 and 0 and 1 \end{array} \right ] \end{equation} which means that the transformation does not affect the x and z directions ( i.e. . it only affects time and the y direction ) . in order to calculate lorentz boost for any direction one starts by determining the following values : \begin{equation} \gamma = \frac{1}{\sqrt{1 - \frac{v_x^2+v_y^2+v_z^2}{c^2}}} \end{equation} \begin{equation} \beta_x = \frac{v_x}{c} , \beta_y = \frac{v_y}{c} , \beta_z = \frac{v_z}{c} \end{equation} then the matrix form of the lorentz boost for velocity v= ( v x , v y , v z ) is this : \begin{equation} \left [ \begin{array}{cccc} l_{tt} and l_{tx} and l_{ty} and l_{tz} \newline l_{xt} and l_{xx} and l_{xy} and l_{xz} \newline l_{yt} and l_{yx} and l_{yy} and l_{yz} \newline l_{zt} and l_{zx} and l_{zy} and l_{zz} \newline \end{array} \right ] \end{equation} where \begin{equation} l_{tt} = \gamma \end{equation} \begin{equation} l_{ta} = l_{at} = -\beta_a \gamma \end{equation} \begin{equation} l_{ab} = l_{ba} = ( \gamma - 1 ) \frac{\beta_a \beta_b}{\beta_x^2 + \beta_y^2 + \beta_z^2} + \delta_{ab} = ( \gamma - 1 ) \frac{v_a v_b}{v^2} + \delta_{ab} \end{equation} where a and b are x , y or z and  ab is the kronecker delta .
this does not violate conservation of momentum , because the momentum behaves in a counter-intuitive manner in a negative refractive index medium . in most media , the refractive index is positive , and the poynting vector $\vec{s}$ and the wave vector $\vec{k}$ point in the same direction . in a medium with a negative refractive index , the poynting vector points in the opposite direction from the wave vector . the wave vector of the light is what you control when you set up the light source . if you point the source towards the positive $x$ axis , for example , then $\vec{k} = |\vec{k}|\hat x$ . on the other hand , the poynting vector is what controls the light wave 's momentum , and therefore the radiation pressure . the result is ( from the aps meeting paper you cite ) : a perfect mirror illuminated with a plane wave would experience a negative radiation pressure ( pull ) when immersed in a left-handed medium [ emphasis added ] i have emphasized that the mirror must be immersed in a left-handed medium , rather that simply made from a left-handed material , because the light wave must actually be in the negative refractive index medium for this effect to occur .
seems to me there is a confusion between various concepts , let me try to clear it up : virtual particle is one that does not live forever , at some stage it gets converted to something else . as jeff points out , none of us lives long enough to tell the difference , so the distinction between virtual and non-virtual is a matter of degree . particles that live for a long time are declared " real " , and particles that decay quickly are called " virtual " . these are just names , there is no implication that " virtual " particles do not really exist , like white unicorns and other mythical creatures , those are all real measurable effects you can see with your own eyes . . . any particle can be either real or virtual , whether or not it is massive , whether or not it is bosonic force carrier , or fermionic matter . there is a sense in which massive particles tend to live shorter life ( because they have more opportunities to decay ) , but this is just a rule of thumb . off-shell can be taken here to be synonymous with " virtual " . hope that helps .
this is a great question , and the answer relates intimately to why turbofan engines equipped with afterburners require variable geometry exhaust nozzles . without increasing the throat area to accommodate the larger volumetric flow rate , lighting the afterburner would back-pressure the fan and very possibly lead to a compressor stall . similarly , mechanically reducing the downstream area ( all other things being equal ) will require the flow to have a higher upstream stagnation pressure , which means the fan/pump will be required to work harder . now , as to your question about why the flowrate decreases when the exit area is closed , we need to expound a bit on how fans and compressors operate . the fan speed , massflow rate , and pressure ratio are related in a complex way and are usually represented graphically by a fan map . for a given rotational speed , there is a single steady-state characteristic relating pressure ratio and massflow rate . the shape of this curve can vary ( e . g . compare the 40% nf line with the 100% nf line above ) , but generally speaking the higher the pressure ratio , the lower the massflow rate for a given engine rpm . this makes some intuitive sense because the faster the bladetip velocity compared to the axial velocity , the higher will be the flow turning within the bladerow . work done and pressure rise are proportional to flow turning within the rotor , so higher pressure ratios are positively correlated with lower massflow rates/axial velocities ( up to a point ) . to truly understand the causal relationship between massflow rate and back-pressure requires that we abandon steady-state thinking altogether . if the exit area is reduced , unsteady compression waves propagate upstream at the speed of sound , incrementally increasing the static pressure at the exit of the fan . this increased back-pressure means that the entering flow now encounters an adverse streamwise pressure gradient and slows down . this slower flow is then worked harder by the spinning bladerow , which results in larger stagnation pressure and temperature rises . remember that the flow always exits the device at atmospheric pressure so long as it is subsonic , precisely because of the information propagated upstream by unsteady pressure waves . thus , if reducing the exit area means a higher exit mach number is required to conserve mass , the total-to-static pressure ratio must increase . a fixed exit static pressure and increased total-to-static pressure ratio demands that the upstream stagnation pressure increase , and so the upstream turbomachinery will be affected . if you are looking to put numbers on things , the isentropic flow function is a useful and straightforward way to determine the massflow rate of a compressible fluid if other of the fluid 's basic properties are known . in general , the massflow rate of a fluid through a cross-sectional area $a$ is equal to $\dot{m}=\rho va$ . now , if the fluid is compressible and the ideal gas law applies , then $\dot{m}=\rho va=\left ( \frac{p}{rt}\right ) ( m\sqrt{\gamma rt} ) a=pam\sqrt{\frac{\gamma}{rt}}$ . both the stagnation temperature and stagnation pressure are preferred flow variables to their static counterparts , so the above equation can be rewritten as $\dot{m}=p_0 \left ( \frac{p}{p_0}\right ) am\sqrt{\frac{\gamma ( t_0/t ) }{r ( t_0 ) }}$ , and the stagnation properties ( as well as the through-flow area ) can be moved to the lhs of the equation : $\frac{\dot{m}\sqrt{t_0}}{p_0 a}=\left ( \frac{p}{p_0}\right ) m\sqrt{\frac{\gamma}{r}\left ( \frac{t_0}{t}\right ) }$ if the flow is isentropic ( as we are assuming ) , we know that $\frac{p}{p_0}=\left ( \frac{p_0}{p}\right ) ^{-1}=\left ( \frac{t_0}{t}\right ) ^\frac{\gamma}{1-\gamma}$ , which gives us $\frac{\dot{m}\sqrt{t_0}}{p_0 a}=m\sqrt{\frac{\gamma}{r}}\left ( \frac{t_0}{t}\right ) ^{\frac{1}{2}+\frac{\gamma}{1-\gamma}}=m\sqrt{\frac{\gamma}{r}}\left ( \frac{t_0}{t}\right ) ^{\frac{1+\gamma}{2 ( 1-\gamma ) }}$ . again invoking our assumption of isentropic flow , we know that the stagnation temperature ratio is related to the local mach number by the following equation : $\frac{t_0}{t}=1+\frac{\gamma-1}{2}m^2$ which , when plugged into the previously derived expression gives us the isentropic flow function $ff_t$: $ff_t=\frac{\dot{m}\sqrt{t_0}}{p_0 a}=m\sqrt{\frac{\gamma}{r}}\left ( 1+\frac{\gamma-1}{2}m^2\right ) ^{\frac{1+\gamma}{2 ( 1-\gamma ) }}$ to compute the massflow rate we simply rearrange the isentropic flow function relation . . . $\boxed{\dot{m}=p_0 am\sqrt{\frac{\gamma}{rt_0}}\left ( 1+\frac{\gamma-1}{2}m^2\right ) ^{\frac{1+\gamma}{2 ( 1-\gamma ) }}}$ . **note : the above equation is true at any given section within a compressible flow , but the stagnation properties may change from location to location ( or over time ) based on the specifics of the exact flow the equation is being applied to .
this is a cross section of the rod : snell 's law tells us that the angle $r$ is given by : $$ \sin r = n \sin i $$ where $n$ is the refractive index of the rod ( $n \gt 1$ ) . no matter how far down the rod $d$ is there will be a value for $i$ that allows the light to reach $d$ . the total internal reflection angle is the limiting value for $i$ as the distance to $d$ goes to infinity and $r \rightarrow \pi/2$ .
it is also worth noting that there is nothing special about atoms . if you have any system where in every period of time an event has a certain chance of happening which only depends on internal effects of the object and no memory or communications with others - you will get the same decay curve . it is purely a matter of the statistics . if you have a handful of coins and every minute toss them all and remove all the heads into a separate pile - the number of coins remaining in the hand will decay with a half-life of 1 minute . what is special about carbon14 - and why it is useful for archeaology is that new carbon14 is being made all the time in the atmosphere , and while you are alive you take in this new carbon so the decays do not have any effect until you die . it is like tossing the coins , but while you are alive adding new random coins after each toss - but then when you die have somebody else start to remove the heads . if you assume you died with an equal number of heads and tails , you can work out how many tosses have happened since you died - and so how long ago the sample died .
the reason is because the time taken for the two trips are different , so the average speed is not simply $\frac{v_1 + v_2}{2}$ we should go back to the definition . the average speed is always ( total length ) &divide ; ( total time ) . in your case , the total time can be calculated as \begin{align} \text{time}_1 and = \frac{120 \mathrm{miles}}{40 \mathrm{mph}} \\\\ \text{time}_2 and = \frac{120 \mathrm{miles}}{60 \mathrm{mph}} \end{align} so the total time is $120\mathrm{miles} \times \left ( \frac1{40\mathrm{mph}} + \frac1{60\mathrm{mph}}\right ) $ . the average speed is therefore : \begin{align} \text{average speed} and = \frac{2 \times 120\mathrm{miles}}{120\mathrm{miles} \times \left ( \frac1{40\mathrm{mph}} + \frac1{60\mathrm{mph}}\right ) } \\\\ and = \frac{2 }{ \left ( \frac1{40\mathrm{mph}} + \frac1{60\mathrm{mph}}\right ) } \\\\ and = 48 \mathrm{mph} \end{align} in general , when the length of the trips are the same , the average speed will be the harmonic mean of the respective speeds . $$ \text{average speed} = \frac2{\frac1{v_1} + \frac1{v_2}} $$
human color vision is based on four types of receptors in the retina : rods , and three types of cones . their response to different wavelengths is shown in this graph : . it shows clearly how certain wavelenghts , mostly around the yellow-green portion of the spectrum , are absorbed more strongly , and by more types of cells , than the rest . so it is normal that , even with equal powers , some colors are seen brighter than others . actually , digital cameras often filter their ccd array with a bayer mask , which has twice as many pixels filtered green , as red or blue , to better simulate the eye 's color sensitivity .
there is no unique way to answer this question . the problem is the following . you are asking if , at a specific proper time , a black hole of some radius exists . however , to do this you have to say something like " the black hole has a 10m radius at the same time that the observer 's proper time is 5sec " . such simultaneous events are not well defined in gr unless you make an arbitrary choice of a coordinate system with a time coordinate . nonetheless , if you make such a choice , it is true that the black hole will start with a radius of 0 inside of the infalling matter and then get larger . eventually it will reach a maximum radius and then get smaller through hawking radiation . so what will an observer literally see if she observes the star collapsing ? the answer is that the observer will see infalling matter slow down and thermalize . the matter will never appear to " fall in " as far as this observer is concerned . however , the particles/light emitted will be redshifted more and more ( and before long the only outgoing particles of reasonably not-low energy will be from hawking radiation ) .
after i commented on the question i started wondering what an observer inside a collapsing shell would experience . if you construct a spherical shell then an observer inside it feels no gravity . this is true in newtonian gravity , and is also true in general relativity as a consequence of birkhoff 's theorem i.e. the metric inside the shell is the minkowski metric . in principle we can take the shell and compress it until it is external radius falls below the schwarzschild radius $r = 2gm/c^2$ , at which point the shell will start collapsing inwards and form a singularity in a finite time . in fact it is a very short time indeed . calculating the lapsed time to fall from the horizon to the singularity of an existing black hole is a standard exercise in gr , and the result is : $$ \tau \approx 6.57 \frac{m}{m_{sun}} \mu s $$ that is , for a black hole of 10 solar masses the fall takes 65.7 microseconds ! i would have to indulge in some head scratching to work out if the same time would be measured by an observer riding on the collapsing shell , but if the time is not the same it will be of a similar order of magnitude . this means much of the question does not apply , since the shell cannot be stable long enough for the black hole to evaporate . however it leaves open the interesting question of what the observer inside the shell experiences . curious as it seems , birkhoff 's theorem implies the observer experiences absolutely nothing until the collapsing shell hits them and sweeps them , along with the shell , to an untimely end ( a few microseconds later ! ) . response to comment : time dilation the infall time i calculated above is the proper time , that is the time measured by the freely falling observer on their wristwatch . you need to tread carefully when talking about time in relativity , but the proper time is usually easy to understand . re time dilation : again we need to be careful to define exactly what we mean . in the context of black holes we usually take an observer far from the black hole ( strictly speaking at an infinite distance ) as a reference and compare their clock to a clock near the black hole . by time dilation we mean that the observer at infinity sees the clock near the black hole running slowly . a clock in a gravitational potential well runs slowly compared to the clock at infinity . this was discussed in the higher you go the slower is ageing ( and also in gravitational time dilation at the earth&#39 ; s center ) . it is important to understand that it is the potential that matters , not the gravitational acceleration , so even though the observer inside the shell feels no gravitational acceleration they are still time dilated compared to the observer at infinity . note that the time dilation relative to the observer at infinity goes to infinity at the event horizon , so it makes no sense to compare times inside the event horizon to anything outside .
the separation of the rods does affect the acceleration even for constant separation , because ( assuming a rolling motion ) the relative amount of energy that goes into linear and angular motion is affected by the spacing . if the rolling is assumed to be without slipping , we can solve the problem by conservation of energy : $$ mg \delta h = \frac{1}{2}mv^2 + \frac{1}{2} i \omega^2 \tag{*} \ , , $$ with the relationship between $v$ and $\omega$ set by the spacing of the rods through the radius of contact during rolling . 1 you seek to determine the effective radius $r_{eff}$ of the rolling . for rods of diameter much less than the radius of the ball and spacing $d$ you can use $r_{eff}^2 = r^2 - ( d/2 ) ^2$ . if you allow that the diameter of the rods is non-trivial , the geometry is a little more complicated . in either case use $v = r_{eff}\omega$ in ( * ) and insert the appropriate moment of inertia ( $\frac{2}{5}mr^2$ for a solid sphere ) , and you are home and dry . a further complication is that if the separation of the rods vary in space the center of gravity of the ball can drop ( for increasing separation ) or rise ( for decreasing separation ) relative the plane of the rods . there is a toy that employs this effect to let you make a bearing roll " uphill " by gently and carefully controlling the separation of the ends . 1 recall that for rolling down a ramp the relationship is $v = r\omega$ .
newtonian mechanics explains that they do fall toward the object they are orbiting , the just keep missing . quick and dirty derivation for a circular orbit . let the primary have mass $m$ and the satellite mass $m$ such that $m \ll m$ ( it can also be done for other cases , but this saves on mathiness ) . assume we start with an initial circular orbit on radius $r$ , velocity $v = \sqrt{g\frac{m}{r}}$ . the acceleration of the satellite due to gravity is $a = g\frac{m}{r^2}$ which means we can also write $v = \sqrt{\frac{a}{r}}$ . the period of the orbit is $t = \frac{2\pi r}{v} = 2\pi \sqrt{\frac{r}{a}}$ . chose a coordinate system in which the initial position is $r\hat{i} + 0\hat{j}$ and the initial velocity points in the $+\hat{j}$ direction . chose a short time $t \ll t$ and lets see how far from the primary the satellite ends up after that time . if we have chosen $t$ short enough , we can approximate gravity as having uniform strength through the time period ( and we shall show later that that is justified ) . the new position is $ ( r - \frac{1}{2}at^2 ) \hat{i} + vt\hat{j}$ which lies at a distance $$ r_2 = \sqrt{r^2 - r a t^2 + \frac{1}{4}a^2 t^4 + v^2 t^2} $$ pulling our at factor of $r$ we get $$ r_2 = r \sqrt{1 - \frac{a}{r} t^2 + \frac{1}{4}\frac{a^2}{r^2} t^4 + \frac{v^2}{r^2} t^2} $$ and converting all the $\frac{a}{r}$ and $\frac{v}{r}$ terms into expressions of the period we get $$ r_2 = r \sqrt{1 - ( 2\pi\frac{t}{t} ) ^2 + \frac{1}{4} ( 2\pi\frac{t}{t} ) ^4 + ( 2\pi\frac{t}{t} ) ^2}$$ finally , we drop the $ ( t/t ) ^4$ term as negligible and note that the $ ( t/t ) ^2$ terms cancel so the result is $$r_2 = r$$ or the radius never changed ( which justified the constant magnitude for acceleration , and a small enough $t$ justifies both the constant direction and the dropping of the fourth degree term ) .
your mechanical vibration expression can be rewritten ( k subscript added by me ) as the real part of : $ae^{j ( k_1x-\omega t+\phi ) }$ this describes a standing wave , or one component of a plane wave , created by a source which is constantly on , in a lossless medium . notice that the entire exponent is multiplied by the imaginary factor ' j ' . ( edit : scratch the ' standing wave ' part : that would take two wave components , one with a -jkx factor in the exponent and another with a +jkx factor ( traveling in the opposite direction ) . ) if you have a wave set up in a lossy medium by a constantly on source , and you want to describe how it attenuates with distance , you need to add in a non-imaginary loss factor , $e^{-k_2x}$ . so then , assuming your source is at the origin , and the plane wave travels only in the positive ' x ' direction , your expression for finding the instantaneous wave height at any point in the path of the plane wave would be the real part of : $ae^{-k_2x}e^{j ( k_1x-\omega t+\phi ) } = ae^{ ( -k_2x+j ( k_1x-\omega t+\phi ) ) }$ this should be easy enough to code up . the tough part will be defining $k_2$ . and , on the huygens thing : do not worry about it too much if you are just doing plane waves or trying to analyze sound traveling down a lossy channel . where huygens really shines is explaining diffraction patterns or why light bends around a sharp corner .
suppose we have a rod of length $l$ at rest in the unprimed frame and we watch an observer in the primed frame speeding past : we will take the origins in both frames to coincide when the observer in the primed frame passes the first end of the rod , so event a is $ ( 0 , 0 ) $ in both frames . in the unprimed frame the far end of the rod is at $x = l$ , and we see the speeding observer pass it at $t = l/v$ , so event b is $ ( l/v , l ) $ . the interval between these events is therefore : $$ s^2 = \frac{c^2l^2}{v^2} - l^2 $$ in the unprimed frame the stationary observer sees the rod , of length $l'$ coming towards him at speed $v$ . the $x$ coordinate of both events is zero , and the time of event b is $t = l'/v$ , so the interval is : $$ s'^2 = \frac{c^2 l'^2}{v^2} $$ the intervals must be the same , $s^2 = s'^2$ , so : $$ \frac{c^2 l'^2}{v^2} = \frac{c^2l^2}{v^2} - l^2 $$ and a quick rearrangement gives : $$ l'^2 = l^2 \left ( 1 - \frac{v^2}{c^2} \right ) $$ $$ l ' = l \sqrt{1 - \frac{v^2}{c^2} } = \frac{l}{\gamma} $$ response to comment : to work out the time dilation you use a different pair of events . in the unprimed frame you have a clock , ticking with period $t$ , stationary at the origin . so the events for the first and second ticks are $ ( 0 , 0 ) $ and $ ( t , 0 ) $ . the interval $s^2 = c^2 t^2$ . as usual we choose the primed frame so the origins of the frames coincide , and the first tick is at $ ( 0 , 0 ) $ . the second tick is at $t = t'$ , and because the clock is moving at velocity $v$ , the $x$ coordinate of the second tick is $x = vt'$ giving $ ( t ' , vt' ) $ . the interval is therefore $s^2 = c^2t'^2 - v^2t'^2$ . as before , we set the intervals equal so : $$ c^2 t^2 = c^2t'^2 - v^2 t'^2 $$ or : $$ t'^2 = t^2 \frac{c^2}{c^2 - v^2} $$ now just divide the top and bottom of the rhs by $c^2$ and take the square root to get : $$ t ' = t \frac{1}{\sqrt{1 - v^2/c^2}} $$
yes , harnessing light can be a good way to heat water to usable hot temperatures , i.e. 50 to 80 degrees celsius . the two factors that are important are the surface area of your collector ( which looks quite small in your design ) , and the efficiency of the system . typically , we use either flat plates ( high area ) or ( partially- ) evacuated tubes ( high efficiency ) . sunlight is around 1kw per square metre at the surface in full sun , and thermal collector efficiencies might be in the range 50-90% or so . depending on where on earth you are , you might get 1-7 full sun hours per day . external air temperatures , collector tilt , alignment , and your latitude , will all affect the result . for a home in the uk , solar thermal energy should be sufficient for a small installation to meet half the home 's annual hot water demand . if you can tap into larger-scale seasonal storage , or you are closer to the equator , you can increase that proportion . here 's an evacuated-tube diagram from wikipedia 's article on solar thermal collectors :
dear user , the equivalence between the inertial mass and gravitational mass tells us the following thing about the higgs mechanism : any inertial mass produced or modified by the higgs mechanism also has to produce or modify a source of gravity of the same magnitude . and vice versa , a gravitational mass produced by the higgs mechanism also has to produce an inertial mass of the same magnitude . the reason why others - and myself - and confused about your question is that the equivalence principle tells us the very same thing not only about the higgs mechanism but also about confinement , electrostatic attraction , spinning gyroscopes , or any other process , object , or mechanism that is taking place , has been taking place , or will take place . the equivalence principle is a totally universal law that holds for all objects and all processes in this universe - and beyond . it is not true that the higgs mechanism has a more special relationship to the equivalence principle than any other mechanism in non-gravitational physics . concerning your more specific question about the experimental tests - of course that it has been tested . the available experimental tests of the equivalence principle show that all materials have the same ratio of inertial and gravitational mass up to the precision of $10^{-15}$ or so . different materials have a different percentage of their mass coming from the electrons - the more neutrons a material has , the smaller the fraction is . so the mass stored in the electrons may go from 0.02% to 0.05% . while this is much smaller than 100% , it is surely enough to exclude the conjecture that the electron mass - as produced by the higgs mechanism - does not obey the equivalence principle . the percentages above are just 3.5 orders of magnitude below 100% . so you still have 12 orders of magnitude left that prove that the mass produced by the electrons ' interactions with the higgs is exhibited both as inertial mass and gravitational mass - with the same ratio ( one - in normal units ) - as all other objects have . so once again , yes , all methods to obtain energy/mass i.e. $e=mc^2$ obey the equivalence principle and this fact has been tested with an amazing accuracy . the equivalence principle is true for masses produced by the higgs mechanism , confinement , or anything else . this fact is a problem for some " cheap " methods to solve the cosmological constant problem . the problem with the cosmological constant is that even things such as virtual objects in atomic physics , qcd , or anywhere create sources of vacuum energy density . it is not possible to throw them away because such a procedure would ultimately contradict the equivalence principle . so the mystery is why the cosmological constant is so tiny even though we may enumerate lots of possible sources that are much bigger and that appear at any conceivable scale . cheers lm
so you want the formal answer to question 2 ? read on : lets say we have $k$ blocks , numbered $i=1 \ldots k$ with 1 on the bottom and $k$ on the top . the top block has an applied force $\mathcal{p}$ and each block has mass $m_i$ and friction coefficient with the previous block ( or the ground ) $\mu_i$ . also the movement of each block is characterized by the acceleration $\ddot{x}_i$ . in matrix form the above define $$ p=\begin{pmatrix}0\\ 0\\ \vdots\\ 0\\ \mathcal{p} \end{pmatrix} $$ $$ m=\begin{bmatrix}m_{1}\\ and m_{2}\\ and and \ddots\\ and and and m_{k-1}\\ and and and and m_{k} \end{bmatrix} $$ $$ \mu=\begin{bmatrix}\mu_{1}\\ and \mu_{2}\\ and and \ddots\\ and and and \mu_{k-1}\\ and and and and \mu_{k} \end{bmatrix} $$ $$ \ddot{x}=\begin{pmatrix}\ddot{x}_{1}\\ \ddot{x}_{2}\\ \vdots\\ \ddot{x}_{k-1}\\ \ddot{x}_{k} \end{pmatrix} $$ the weight on each block is $m_i g$ and the contact force with the previous block ( or the ground ) is $n_i$ . also the friction limit is $f_i \leq \mu_i n_i$ . in matrix form the above is $$ n=\begin{pmatrix}n_{1}\\ n_{2}\\ \vdots\\ n_{k-1}\\ n_{k} \end{pmatrix} $$ $$ f \leq \begin{bmatrix}\mu_{1}\\ and \mu_{2}\\ and and \ddots\\ and and and \mu_{k-1}\\ and and and and \mu_{k} \end{bmatrix}\begin{pmatrix}n_{1}\\ n_{2}\\ \vdots\\ n_{k-1}\\ n_{k} \end{pmatrix}=\begin{pmatrix}\mu_{1}n_{1}\\ \mu_{2}n_{2}\\ \vdots\\ \mu_{k-1}n_{k-1}\\ \mu_{k}n_{k} \end{pmatrix} $$ why do we need all this ? to to make the equation of motion for the $i$-th block , which is $p_i - f_i + f_{i+1} = m_i \ddot{x}_i $ look at the free body diagram above . by convention the i -th friction opposes the motion which is to the right . the friction from the above block is reacted upon this block and applied to the left . that is why the sum of the fores is $p_i + f_{i+1} - f_i$ . the balance in matrix form , using an adjacency matrix is $$ a=\begin{bmatrix}1 and -1\\ and 1 and -1\\ and and \ddots and \ddots\\ and and and 1 and -1\\ and and and and 1 \end{bmatrix} $$ $$ p-a\ , f=m\ddot{x} $$ which expands out to $$\begin{pmatrix}0\\ 0\\ \vdots\\ 0\\ \mathcal{p} \end{pmatrix}+\begin{pmatrix}f_{2}-f_{1}\\ f_{3}-f_{2}\\ \vdots\\ f_{k}-f_{k-1}\\ -f_{k} \end{pmatrix}=\begin{pmatrix}m_{1}\ddot{x}_{1}\\ m_{2}\ddot{x}_{2}\\ \vdots\\ m_{k-1}\ddot{x}_{k-1}\\ m_{k}\ddot{x}_{k} \end{pmatrix}$$ now the contact normal force is derived from the blocks above it with $$ a\ , n = m\ , g $$ $$ n = a^{-1} m\ , g $$ $$ \begin{pmatrix}n_{1}\\ n_{2}\\ \vdots\\ n_{k-1}\\ n_{k} \end{pmatrix}=\begin{bmatrix}1 and 1 and 1 and 1 and 1\\ and 1 and 1 and 1 and 1\\ and and \ddots and \vdots and \vdots\\ and and and 1 and 1\\ and and and and 1 \end{bmatrix}\begin{pmatrix}m_{1}g\\ m_{2}g\\ \vdots\\ m_{k-1}g\\ m_{k}g \end{pmatrix} $$ so all together $$ p - \left ( a\ , \mu a^{-1}\right ) m\ , g=m\ddot{x} $$ or with $ \mu_{sys}=a\ , \mu a^{-1} $ $$ \mu_{sys}=\begin{bmatrix}1 and -1\\ and 1 and -1\\ and and \ddots and \ddots\\ and and and 1 and -1\\ and and and and 1 \end{bmatrix}\begin{bmatrix}\mu_{1}\\ and \mu_{2}\\ and and \ddots\\ and and and \mu_{k-1}\\ and and and and \mu_{k} \end{bmatrix}\begin{bmatrix}1 and 1 and 1 and 1 and 1\\ and 1 and 1 and 1 and 1\\ and and \ddots and \vdots and \vdots\\ and and and 1 and 1\\ and and and and 1 \end{bmatrix} \\ \mu_{sys}=\begin{bmatrix}\mu_{1} and \mu_{1}-\mu_{2} and \cdots and \mu_{1}-\mu_{2} and \mu_{1}-\mu_{2}\\ and \mu_{2} and \cdots and \mu_{2}-\mu_{3} and \mu_{2}-\mu_{3}\\ and and \ddots and \vdots and \vdots\\ and and and \mu_{k-1} and \mu_{k-1}-\mu_{k}\\ and and and and \mu_{k} \end{bmatrix} $$ $$ p -\mu_{sys} m\ , g=m\ddot{x} $$ $$ \ddot{x} = m^{-1} \left ( p-\mu_{sys} m\ , g \right ) $$ so this is the motion once with have slipping . we need to reverse the equations and find the traction required when $\ddot{x}=0$ which ends up being $$ \mu_i \geq \frac{\mathcal{p}}{g ( \sum_{j=i}^k m_j ) } $$ when the above is not satisfied the contact is slipping . otherwise system will have $\ddot{x}_i=0$ for when the contact sticks . block matrix solution here are the steps needed to solve the above system stick all contacts with $\ddot{x}=0$ and find the friction needed $f^{\star}=a^{-1}p$ . for example $$f^{\star}=\begin{bmatrix}1 and 1 and \cdots and 1 and 1\\ and 1 and \cdots and 1 and 1\\ and and \ddots and \vdots and \vdots\\ and and and 1 and 1\\ and and and and 1 \end{bmatrix}\begin{pmatrix}0\\ 0\\ \vdots\\ 0\\ \mathcal{p} \end{pmatrix}=\begin{pmatrix}\mathcal{p}\\ \mathcal{p}\\ \vdots\\ \mathcal{p}\\ \mathcal{p} \end{pmatrix}$$ compose the system mass matrix $m=a^{-1}m$ such that the horizontal equations of motion are $\boxed{f^{\star}=m\ddot{x}+f}$ compare friction needed to available traction with $f^{\star}&lt ; \mu n$ . construct two projection matrices $t$ and $u$ with $k$ rows and values as follows : for each block $i$ that is sliding add a column to $u$ with the i -th row element equal to 1 and all others 0 . for each block $i$ that is sticking add a column to $t$ with the i -th row element equal to 1 and all others 0 . for example if only the last element ( top ) slides then $$ \begin{aligned} t and =\begin{bmatrix}1\\ and 1\\ and and \ddots\\ and and and 1\\ and and and 0 \end{bmatrix} and u and =\begin{bmatrix}0\\ 0\\ \vdots\\ 0\\ 1 \end{bmatrix} \end{aligned}$$ define the known motions ( sticking blocks ) with $t^{\top}\ddot{x}=0$ and the known friction ( sliding blocks ) with $f=u^{\top}f=u^{\top}\mu n$ . with the example above then $$\begin{aligned} \begin{pmatrix}0\\ 0\\ \vdots\\ 0 \end{pmatrix} and =\begin{bmatrix}1\\ and 1\\ and and \ddots\\ and and and 1\\ and and and 0 \end{bmatrix}^{\top}\begin{pmatrix}\ddot{x}_{1}\\ \ddot{x}_{2}\\ \vdots\\ \ddot{x}_{k-1}\\ \ddot{x}_{k} \end{pmatrix}=\begin{pmatrix}\ddot{x}_{1}\\ \ddot{x}_{2}\\ \vdots\\ \ddot{x}_{k-1} \end{pmatrix}\\f and =\begin{bmatrix}0\\ 0\\ \vdots\\ 0\\ 1 \end{bmatrix}^{\top}\begin{pmatrix}\mu_{1}n_{1}\\ \mu_{2}n_{2}\\ \vdots\\ \mu_{k-1}n_{k-1}\\ \mu_{k}n_{k} \end{pmatrix}=\begin{pmatrix}\mu_{k}n_{k}\end{pmatrix} \end{aligned}$$ define the unknown motions vector $a$ and unknown forces vector $r$ such that the block motion is $\ddot{x}=u\ , a$ and the block friction $f=t\ , r+m\ , u\left ( u^{\top}m\ , u\right ) ^{-1}f$ . note that $u^{\top}f=f$ and $t^{\top}m^{-1}f=\left ( t^{\top}m^{-1}t\right ) \ , r$ . the horizontal equations of motion are $\boxed{ f^{\star}=t\ , r+m\ , u\left ( a+\left ( u^{\top}m\ , u\right ) ^{-1}f\right ) }$ with $r$ and $a$ as unknowns . project to the sliding blocks with $u^{\top}f^{\star}=u^{\top}m\ , u\left ( a+\left ( u^{\top}m\ , u\right ) ^{-1}f\right ) $ } $\boxed{a=\left ( u^{\top}m\ , u\right ) ^{-1}\left ( u^{\top}f^{\star}-f\right ) }$ project to the sticking blocks with $t^{\top}m^{-1}f^{\star}=\left ( t^{\top}m^{-1}t\right ) \ , r$ } $\boxed{r=\left ( t^{\top}m^{-1}t\right ) ^{-1}t^{\top}m^{-1}f^{\star}}$ back substitute the projections to get $\ddot{x}=u\ , a$ and $f=f^\star - m \ddot{x}$ .
it will diffuse into space . space is a near-perfect vacuumits pressure is nearly zero and it has extremely little matter ( in the empty parts , at any rate ) . on the other hand , your bottle has a relatively high pressure . when you remove the barrier ( by opening the cap ) , the air naturally flows to the region of low pressure . once there , it creates a localized blob of air , with low pressurebut not as low as the surrounding space . this localised blob will spread out due to the pressure difference . theoretically , it will spread to every corner of the universe , if we neglected gravity and the fact that the universe has other stuff in it . anyway , it will very very extremely negligibly marginally increase the pressure of the local zone . that is about itthe air just spreads out . sort of like what happens when you put a drop of red food coloring in a tank of waterit first creates a localized red blob , which spreads out till the change in color is negligible . note that this process , in space , will be much faster than the red food coloring one . the two are similar phenomena but the mechanics are different .
i am only going to try to address the question of dc fields . medical mri uses uniform fields of about 0.5 to 3.0 t . in a head mri , the lorentz force on ions in the brain can cause neurological effects such as vertigo . i have heard that this shows up in particular when the patient moves his head . here is a famous picture of a frog being levitated by a 16 t magnetic field . this effect requires a nonuniform field ; a diamagnetic object is attracted to a region of lower field strength . i have always assumed the frog was unharmed , but i do not know for sure . based on this , it sounds like the result depends on whether the field is uniform or nonuniform .
let us begin with an example from electromagnetism , which you may be familiar with . gauss ' law is given in 3 dimensions by : $\int\int e . ds= \frac{q}{\epsilon}$ , where $e$ is the electric field produced , $q$ is the charge and the integral is performed over a surface that encloses the charge . in 3 dimensions , the simplest shape to enclose the charge is the sphere , so we will choose this to make the mathematics simpler . making the reasonable assumption that $e$ is constant along all points in the sphere , the above equation becomes : $4\pi r^2e =\frac{q}{\epsilon}$ and therefore , $e = \frac{q}{4\pi r^2\epsilon}$ . it is not hard to see how to generalize this approach to 2 dimensions . instead of considering a surface integral around a sphere , we simply need to consider a line integral around a circle . the modified 2-d gauss ' law will then become : $\int e . dl= \frac{q}{\epsilon}$ , where the integral is performed around a circle enclosing the charge $q$ . again , evaluating this integral gives : $2\pi re = \frac{q}{\epsilon}$ , and therefore , $e = \frac{q}{2\pi r\epsilon}$ finally we can generalize to 1-d , where a circle in 1-d becomes a line , and line integral changes to simply adding together points . if we pick a line of length 2r , enclosing the charge , gauss ' law will become : $2e = \frac{q}{\epsilon}$ , that is , $e = \frac{q}{2\epsilon}$ now we must see how to generalize this approach to gravity . in 3-dimensions , the gravitational field produced by a mass $m$ is given by : $g = \frac{gm}{r^2}$ if we introduce a new variable , $k$ , defined as $k = \frac{1}{4\pi g}$ , then we can re-write the field as : $g = \frac{m}{4\pi r^2k}$ . compare this to the electric field in 3-d , which is $e = \frac{q}{4\pi r^2\epsilon}$ one could thus construct a " gauss'" law for the gravitational field and construct the 2d and 1d fields , in the same way i did for the electric fields above . the results will be the same as that for the electric field , but with $\epsilon$ replaced with $k = \frac{1}{4g\pi}$ , and $q$ replaced with $m$ .
it is mathematically possible to create some instances in which an object goes back in time relative to some observer . for example , simply going faster than light causes such an effect , but of course , speed of light is the limit for any massive object . while it is mathematically possible , there are many paradoxes caused by time travel to past , unless you accept existence of some sorts of multiverses , which does not really qualify as a time travel to past anyway , rather travel between said multiverses . mainly , and most importantly , time travel to past violates " causality " , one of the main principles the universe is thought to have , in which the cause precedes the result , e.g. your father is born before you are born . through time travel to past , it is possible for the " result " to eliminate its " cause " before it causes the result , that is , you killing your father before you were conceived . this is not the only paradox caused by time travel to past , however this is one of the main ones . also , i think it was hawking that suggested this , we see no time-travelers around us , pointing out to the fact that humanity will not achieve the technology to go back in time as far as it wants . however , it is possible that time travel to past may be discovered , only limited to taking travelers back to the creation of the machine , as some time travel machines work on this principle , namely wormholes . needless to say , wormholes require extremely exotic conditions to form and maintain , and therefore are very far from being created deliberately , if they even exist .
the explanation is really very simple to understand intuitively , and very beautiful . imagine that a particle an uncertainity in its velocity $v$ of $\delta v$ . suppose at $t=0$ we have $x=x_{0}$ . after $t=t$ , the location of the particle will be given by the range $ ( x_{0}+tv-t\delta v , x_{0}+tv+t\delta v ) $ , because we dont know the exact velocity the particle started with . it evident that a probability of finding a particle has changed from being localised in the beginning to being diffused after some time : this is wave packet spreading . note that the range ( wave packet size ) increases with time monotonically , this means that even if we started with a diffused particle density for general case , it will just become much more diffused by extension of the arguement .
the apparent horizon is a technical term , so you are going to have to have some jargon to explain it--the distinctions between trapping , apparent , dynamical and event horizons does just require some knowledge of differential geometry and general relativity , even if you can explain them using common words , as below . freeze a moment in time in some particular coordinate system . then , if that coordinate system contains an apparent horizon , it will be closed two-dimensional surface ( think a ball , but not necessarily undistorted ) where all light rays but one are forced to move into the surface when evolved in time , and the last light ray will be ' frozen ' on the surface at that moment of time ( think of the outgoing light ray at $r=2m$ in the schwarzschild solution--it tries to get away , but is just stuck on the surface ) . it is called apparent because it : depends on the notion of constant time on that one surface is therefore coordinate dependent , and in some cases , can be eliminated entirely by choice of coordinate can have strange , unpredictable behaviour is generally inside the proper event horizon , but if the ' 'stack'' of apparent horizons forms a timelike surface , the surface will be two-way transversible , and therefore , does not correspond with what we normally think of as a ' horizon ' .
there is no meaningful way to test if the speed of light varies - that is because it is dimensionful , i.e. it is measured in units . to see why , let 's say we use units in which distance is measured in terms of multiples of the circumference of the electron 's orbit in the ground state of bohr 's hydrogen atom , and the unit of time is it is orbital period . this will give you roughly 137 , which is the inverse of the fine structure constant , which is defined as $e^2 \over \hbar c$ . so , we can see that it is not possible to determine whether the value of the speed of light was different , since one of the other constants in the fsc ( the electron charge or the reduced planck constant ) could have changed . however , it is meaningful to ask whether a dimensionless constant has changed , one that is not measured in units . some examples are the above mentioned fine structure constant , and the cosmological constant . also , particle masses are fundamental constants - changing another constant does not affect them . so , rather than asking if the speed of light varies , a better question is to ask if the fine structure constant varies ( since it is dimensionless , it has no units ) . there have been claims that the fine structure constant may vary ( here and here , among many others ) . however , this certainly is not an accepted result . for more , see the usenet faq on dimensionless constants : http://math.ucr.edu/home/baez/constants.html addition rather than varying over time , let 's think of the case in which c varies over space . so , a group of scientists ventures on a rocket to a distance part of the galaxy to determine if the speed of light is different . they will need to use the same units that the earth scientists are using - we could use the above units , the vibrations of an atom for time , whatever you want . let 's say they measure a different value using the agreed units . now , imagine that a different group of scientists was going to test if the length of some particular rod was different in that same region of the galaxy . they decide to see how many vibrations of the cesium atom it takes light to travel the rod . based on their experiment , they come to the conclusion that the length of the rod is larger in this other region , or that the cesium atom vibrates slightly faster . when both groups publish their findings , they disagree - the first group tells the second group they are wrong because they based their measurements on the speed of light , which they found varies . however , group two asserts that the first group is mistaken , since they found that the length of the measuring rod and frequency of the vibrations of the cesium atom were both different . so , you can see that asserting that a dimensionful constant has varied is meaningless - since they are ratios of other constants , it is 100 percent equally valid to say those constants varied . not only is it impossible to determine if they have changed , but the question itself does not have an answer . finding different values for dimensionful constants can be interpreted in a variety of ways . for example , you can claim that the constants in the fine structure constant had varied , not the speed of light .
having sufficient time-resolution is the key in being able to differentiate between events caused by multi-photon impacts and events caused by single-photon impacts . from the wikipedia article on x-ray fluorescence , peak length discrimination is used to eliminate events that seem to have been produced by two x-ray photons arriving almost simultaneously . given a detector response time $\tau$ and average rate of photon input $r$ , the distribution of photon counts $k$ occurring in the time interval $\tau$ is poisson-distributed , $$f ( k , r\tau ) =\frac{e^{-r \tau } ( r \tau ) ^k}{k ! }$$ and so you can make multiphoton events negligible by designing the device to operate in a regime where they are unlikely . in effect , you try to make $\tau$ as short as possible for your device , you make sure that the count rate $r$ does not exceed a certain threshold , and you try to filter out the few instances where multi-photon excitations seem to have happened .
there is no such thing as an incompressible stick . when you push on your end , a compression wave travels down the stick at the speed of sound , which is much slower than the speed of light . the other end of the stick does not move until this compression wave reaches it . i think the rest of your problems disappear without the existence of an incompressible stick . if you still have any questions remaining , i am happy to edit this answer .
coil is 1.3um compared to 10.6um for co2 so to get the same diffraction limit your optical components and telescope have to be nearly 10x large diameter . you can also route 1.3um through optical fibre ( although i do not know if this weapons system does this ) more efficiently than 10.6um and it is close enough to optical wavelengths that you can do a lot of setup and alignment with visible light . regular co2 lasers operate on continual output , you can q-switch them but even then i do not think you get the very high pulse energy of a coil .
the velocity of the system after collision is $$ v=\frac{m_b}{m_b+m_s}v_b $$ the lost of kinetic energy can be assumed as the ejected heat in question , \begin{align} \delta ke and = m_b v_b^2 /2 - ( m_b+m_s ) v^2 /2\\ and = m_b v_b^2 /2 - \frac{m_b^2}{m_b+m_s}v_b^2 /2\\ and = m_b v_b^2/2 \left ( 1-\frac{m_b}{m_b+m_s} \right ) \\ and = \frac{m_b m_s}{m_b+m_s}v_b^2/2 \\ \end{align}
" thermodynamics of continuum " is the discipline that defines the temperature as a classical field i.e. a function of the coordinates $ ( x , y , z , t ) $ that parameterize a continuum ( solid , liquid , gas , plasma ) and its evolution in time . ( temperature cannot be a quantum field because it does not really correspond to any operator on the hilbert space . in other words , whenever we take the thermodynamic limit so that the temperature is well-defined , the limiting procedure automatically removes the quantumness of the ensemble of atoms , too . ) such a field can never be " one of the elementary fields " that may fully define a theory . the reason is that in general , the temperature is not well-defined . it is only well-defined at equilibrium . so instead of thinking of temperature as a function of the configuration ( or quantum state ) , one should think in the opposite way : the configuration or the quantum state ( or its local behavior ) is a function of the temperature ! so the temperature is something that determines the state of a physical object or all local properties of the state of a medium . the probability distribution or the density matrix behave like $c\exp ( -e/kt ) $ where $e$ is either the classical energy or the hamiltonian in quantum mechanics . all these comments are compatible with the fact that the temperature cannot be quite well-defined in arbitrary small regions of space , for arbitrarily small ensembles of atoms etc . the temperature is not a well-defined function of any state ; on the contrary , some special states may be defined as functions of the temperature . but they are really very special states , only . physically , they correspond to states at equilibrium or ( local ) quasi-equilibrium . one may also visualize the temperature as the ( inverse ) periodicity of the euclidean time circle ( the dual interpretation is that the imaginary total energy takes on quantized values i.e. integers in some way , but that is useless physically because the total energy is not imaginary ) but it is still true that one should start with the temperature and get a ( special ) state , instead of imagining that one starts with a general state and calculates the temperature . if the temperature is treated via this " thermal circle " , we are abandoning the ordinary time coordinate in favor of the periodic euclidean time coordinate . the normal time disappears because the states with well-defined temperatures are equilibrium states so they can not have any well-defined dependence on time . local quasi-equilibrium states may evolve ( heat conduction described by diffusion equation etc . ) but all these equations are just derived approximate equations capturing some thermal properties of the medium " almost right " . the periodicity of the euclidean time is $\delta t_e=\hbar\beta=\hbar /kt$ so formally , if the energy could be imagined to be imaginary , it would be quantized in the units of $2\pi \hbar / \delta t_e = 2\pi\hbar / \hbar\beta = 2\pi kt$ . what this " quantization " really means is ill-defined but of course the characteristic scale of energy connected with the temperature $t$ is a number of order $kt$ where $k$ is the boltzmann constant . i suspect this is the only sensible description similar to the " high-frequency " comments by the op .
pulsar 's answer is indeed correct , but let me expand a bit more . what happens when a gas giant shrinks ? a uniform mass will have a self gravitational potential of $-\frac{3gm^2}{5r}$ . if we decrease its radius , its potential will decrease as well and the difference will be turned into thermal energy . although gas giants and stars are not uniform mass balls , their gravitational binding energy is still proportional to $\frac{gm^2}{r}$ , thus if the radius decreases it will release energy , which will raise the temperature in return . what happens when the temperature increases ? assuming the gas in those planets obey the ideal gas law $$pv=nrt$$ ( where $r$ is not the radius but the molar gas constant $r=8.314\ , \text{j k}^{1}\text{mol}^{-1}$ ) , it is obvious that when $t$ increases and $v$ decreases ( due to the shrink in the previous section ) $p$ must increase . note that most real gases behave qualitatively like an ideal gas , so this is not a crazy assumption . so what is the big picture ? the planet shrinks a little bit , the potential difference turns into thermal energy and its temperature rises . the rise in temperature will cause the pressure to rise and prevent the planet from shrinking further ( holding the planet in hydrostatic equilibrium ) . however , the planet also loses energy due to em radiation as well , so it will continuously shrink and radiate . the process is called kelvinhelmholtz mechanism . for instance , jupiter is shrinking the tiny bit of $2\ , \text{cm}$ each year . although you might think this is really nothing , the amount of heat produced is similar to the total solar radiation it receives .
equilibrium in the sense of this question means there are no net forces on the objects that make up the system : the charges contained in the conductor . note that we need a model of an ideal conductor here . a neutral ideal conductor is thought of as containing equal large amounts of unbound , infinitely small ( not electrons ) positive and negative charges . consider one of the charges , one that happens to lie on the gaussian surface . if the field there was non-zero , the charge would experience a force and accelerate : the system is not in equilibrium . thus , in equilibrium , there are no forces on charges within the conductor , and the electric field is zero everywhere within the conductor .
flip back a page ; dirac uses real to mean hermitian when talking about linear operators . so you can see that even if $a$ and $b$ are hermitian , $ab$ will not be hermitian unless they commute , whereas those linear combinations will be .
should not the energy gain be greater than this formula describes since the energy from the electric field is applied for so long ? the electron gains energy and accelerates until it encounters a collision . this is a statistical process and there is a distribution for the energy loss for many electrons . then , it can accelerate again from that point on , until it encounters another collision . this process continues . so the time for which the field is applied does not matter so much , because the electron is not continuously accelerating . bursts of acceleration followed by deceleration happen continuously . to apply your equation correctly at the microscopic level to a single electron , you had need to look at the force on the electron during the acceleration phase , and subtract from that the energy lost during a collision . you had then repeat this for as many times as the electron undergoes acceleration/loss from one end of the wire to the other , to obtain the net energy gain . it is not like there is a net force on the electrons , so it almost seems the concept of impulse needs to be invoked . granted , i am very confused in general so maybe there is some obvious answer here . i know the voltage actually describes the same thing but how do i calculate it from the electric field and wire length ? voltage is defined as the line integral over the electric field . in a uniform wire , the electric field is constant and the voltage is just $v = e l$ where $l$ is the wire length . the voltage across the wire in this context describes the net energy change of an electron as it passes from one end of the wire to the other . that energy change is just $q v$ . on the one hand , if i have a field and an object , and the object moves completely from the point where it initially is and where the source of the field is , with no resistance , it will absorb energy all the way along the distance for a fixed quantity of energy ( the potential energy ) , which can be measured as f*d . without resistance , yes , it will continue to absorb energy until it reaches the other end , and f*d tells the whole story . on the other hand , if along the path energy is taken away from the object , exactly enough to balance the accelerational energy ( making for a constant velocity ) , then if the object starts with zero initial velocity it will constantly absorb energy . if i vary the initial velocity from zero on up , the fixed amount of energy absorbed from points a to b will vary from infinity down to ever smaller quantities . if the electron starts off with zero initial velocity , the net energy it gains traversing the wire is the constant velocity ( steady-state velocity ) it reaches by the time it gets to the other end . so , since an electron has a very small net velocity , should not its actual amount of absorbed energy be greater than the energy predicted by the equation f*d ( ie w=e*e ( m ) *l ) ? no . the energy the electron would gain in the absence of any resistance is f*d . so , if there are in fact collisions , we had expect the net outcome to be lower .
circular motion in special relativity is somewhat tricky : note that for circular motion , the acceleration in the spaceship travelling in a circle is not zero , so the spaceship is not in a single frame of inertia . here is an interesting thought : distances perpendicular to the direction of motion are not subject to contraction . hence , if the observes on earth see the spaceship going on a circle with radius $r$ , then in their frame of inertia the spaceship is always a distance $r$ away from the earth . since the line from earth to spaceship is perpendicular to the direction of flight , the people on the spaceship will also believe that they are always a distance $r$ away from earth , so they will also fly on a circle . they will , nonetheless , experience a different circumference ! the best way to solve this is consider a polygon with $n$ sites and then letting $n$ go to infinity . if people on the earth measure each side of the polygon as $l_0/n$ where $l_0$ is the circumference of the polygon in the earth 's frame of inertia , then the spaceship-people will measure each side to be $l_0/ ( n\gamma ) $ . hence , for $n \rightarrow \infty$ , the polygon becomes a circle . measured from earth , it has circumference $l_0$ , but for the spaceship it has circumference $l_0/\gamma$ . this suggests that the spaceship moves through non-euclidean geometry , because it travels on a circle whose ratio between circumference and diameter is less than $2\pi$ . this is a hint that accelerating frames have non-euclidean geometry , which is excessively treated in general relativity . reference : http://abacus.bates.edu/~msemon/wortelmalinsemon.pdf
$\textbf{the method utilized to measure parallaxes from the earth frame:}$ we utilize the distance from the " us " to the sun ( as a reference point of sorts ) . here is a visual : if one were to measure the parallaxes from the moon , they would have adjust their method in accordance with the position of the moon with respect to the earthsun reference line ( note that this is a time dependent measurement , the value will , of course , be different depending on when you make the calculation ) . if you were measuring parallaxes from another planet ( x ) , you would need to use the distance from that planet ( x ) to the sun as the baseline . in addition , you would need to convert " earth years " to the " planet ( x ) years " , as the other planet ( x ) would have a different orbital period than earth .
i assume you are referring to the strong and weak nuclear forces . these are two fundamental forces ( the others being electromagnetism and gravity ) . due to the very short range on which these forces are effective , they are less directly related to the intermediate and large scale structure of the universe than electromagnetism and gravity , however they are of key importance to the stability and nuclear structure of matter , and thus changing them much would result in a markedly different universe . the strong nuclear force is mediated by particles called gluons , and is responsible for the very strong , very short range attraction between nucleons . this is the reason why atomic nuclei stick together despite protons having a positive charge . the weak nuclear force is mediated by w and z bosons , and is responsible for certain types of radioactive decay . for instance , iirc , neutron decay involves interconversion of a neutron into a proton , electron and electron neutrino via a w boson .
photons pass through glass because they are not absorbed . and they are not absorbed because there is nothing which " absorbs " light in visual frequencies in glass . you may have heard that ultraviolet photons are absorbed by glass , so glass is not transparent for them . exactly the same happens with x-rays for which our body is nearly transparent whilst a metal plate absorbs it . this is experimental evidence . any photon has certain frequency - which for visible light is related to the colour of light , whilst for lower or upper frequencies in the electromagnetic spectrum it is simply a measure of the energy transported by photon . a material 's absorption spectrum ( which frequencies are absorbed and how much so ) depends on the structure of the material at atomic scale . absorption may be from atoms which absorb photons ( remember - electrons go to upper energetic states by absorbing photons ) , from molecules , or from lattices . there are important differences in these absorption possibilities : atoms absorb well-defined discrete frequencies . usually single atoms absorb only a few frequencies - it depends on the energetic spectrum of its electrons . regarding atomic absorption , the graph of absorption ( plotted as a function of frequency of light ) contains well-defined peaks for frequencies when absorption occurs , and no absorption at all between them . molecules absorb discrete frequencies but there are many more absorption lines because even a simple molecule has many more energetic levels than any atom . so molecules absorb much more light . crystalline lattices may absorb not only discrete frequencies but also continuous bands of frequencies , mainly because of discrepancies in the crystalline structure . as glass is a non-crystalline , overcooled fluid , consisting of molecules , its absorption occurs in the 1st and 2nd ways , but because of the matter it is composed of , it absorbs outside our visible spectrum .
this type of decomposition is done all the time , and it is weird looking in the operator formalism . it is most natural in the path integral , where it is known as the background field method . the path integral is over classical values , so that you can always write the field formally as the sum of a classical background and a fluctuating quantum part . the integral over the quantum part reproduces the correct answer for the background , because the integral is translation invariant in field space--- you are allowed to shift the zero value . the background field method is usually used for quick one loop calculations in nonabelian gauge theories , but you can do the decomposition for photons too . if you are adament that you want to do it in the operator formalism , you can just declare that you redefined the operators by subtracting a multiple of the identity . it is not natural , but it is equivalent to background field .
first off , your question is phrased in terms of relativistic mass , which is an obsolete concept . but anyway , that is a side issue . the question can be posed in terms of either the earth 's force on the puck or the puck 's force on the earth . we expect these to be equal because of conservation of momentum . in general relativity , the source of gravitational fields is not the mass or the mass-energy but the stress-energy tensor , which includes pieces representing pressure , for example . the puck has some stress-energy tensor , and this stress-energy tensor is changed a lot by the puck 's highly relativistic motion . therefore the puck 's own gravitational field is definitely changed by the fact of its motion . however , the change is not simply a scaling up of its normal gravitational field . the field will also be distorted rather than spherically symmetric . yes , the effect is probably to increase its force on the earth . the earth therefore makes an increased force on the puck . here is a similar example that shows that you can not just naively use $e=mc^2$ to calculate gravitational forces . two beams of light moving parallel to each other experience no gravitational interaction , while antiparallel beams do . see tolman , r.c. , ehrenfest , p . , and podolsky , b . phys . rev . ( 1931 ) 37 , 602 , http://authors.library.caltech.edu/1544/
all those expressions are equivalent and can be used , if you simultaneously measure , the voltage , current ( and know the resistance from the temperature ) . why do you think ohm 's law does not apply ? resistance is only dependent on temperature in your case ( not current or voltage ) . in general i would use p = iv , if ohm 's law actually does not apply .
here we will only answer op 's two first question ( v1 ) . yes , newton 's shell theorem generalizes to general relativity as follows . the birkhoff 's theorem states that a spherically symmetric solution is static , and a ( not necessarily thin ) vacuum shell ( i.e. . a region with no mass/matter ) corresponds to a radial branch of the schwarzschild solution $$\tag{1} ds^2~=~-\left ( 1-\frac{r}{r}\right ) dt^2 + \left ( 1-\frac{r}{r}\right ) ^{-1}dr^2 +r^2 d\omega^2$$ in some radial interval $r \in i:= [ r_1 , r_2 ] $ . here the constant $r$ is the schwarzschild radius . since there is no mass $m$ at the center of op 's internal hollow region $r \in i:= [ 0 , r_2 ] $ , the schwarzschild radius $r=\frac{2gm}{c^2}=0$ is zero . hence the metric ( 1 ) in the hollow region is just flat minkowski space in spherical coordinates .
i do not see your problem . we are dealing in hypothetical situations that lead to paradoxes and inconsistencies , so there is no problem with postulating what would happen if . . . ? , even if the " if . . . " is impossible . he could have as easily said what would happen if the sun moved suddenly , we would see it move after 8 minutes , but gravitationally feel it move ( according to newton ) instantly . but you could continue to argue over the logistics of moving a sun sized object which is irrelevant to the central point of the argument ( that maxwell 's light travels at a finite speed and newtonian gravity does not ) . does not maxwell 's theory then also predict that the sun can not suddenly disappear ? not really , you need einstein to say that it cannot travel faster than light , and even then simply disappearing would " only " violate conservation of mass-energy , so neither maxwell nor newton . and in his previous book a brief history of time he had written that newton realized this flaw and argued that the universe will be stable if there are an infinite amount of stars . previous book ? i thought you were only talking about brief history of time . in any case having an infinite number of stars does not make the system stable or solve the lack of force to counteract gravity . why this difference ( or i should say lie ) ? i think lie is a very harsh for what is ultimately a gedanken experiment and at worst a minor slip .
please note that the following is all conjectural . i only volunteer it due to the lack of other responses after numerous days , the coolness of the question , and the probably lack of people/references who are explicitly experienced with this specific topic . basic picture as a general relation , i am sure one can correlate the sound-volume with the total energy being dissipated --- but the noise produced is going to be a ( virtually ) negligible fraction of that total energy ( in general , sound caries very little energy 1 ) . to zeroth order , i think it is safe to assume the waterfall produces white-noise , but obviously that needs to be modified to be more accurate ( i.e. . probably pink/brown to first order ) . also , by considering the transition from a small/gradual slope , to an actual waterfall , i can convince myself that there is definitely dependence on the height of the fall in addition to the water-volume 2 . how would height effect the spectrum ? generally power-spectra exhibit high and low energy power-law ( like ) cutoffs , and i would expect the same thing in this case . in the low-frequency regime , if you start with a smooth flow before the waterfall , there is not anything to source perturbations larger than the physical-size scale of the waterfall itself . so , i would expect a low-energy cutoff at a wavelength comparable to the waterfall height . in other words , the taller the waterfall , the lower the rumble . there also has to be a high energy cutoff , if for no other reason , to avoid an ultraviolet catastrophe/divergence . but physically , what would cause it ? presumably the smallest scale ( highest frequency ) perturbations come from flow turbulence 3 , and thus would be determined primarily by the viscosity and dissipation of the fluid 4 . generally such a spectrum falls off like the wavenumber ( frequency ) to the -5/3 power . but note that this high-frequency cutoff would not seem to change from waterfall to waterfall . overall , i am suggesting ( read : conjecturing ) the following : low-frequency exponential or power-law cutoff at wavelengths comparable to the height of the waterfall . high-frequency power-law cutoff from a kolmogorov turbulence spectrum , at a wavelength comparable to the viscous length-scale . these regimes would be connected by a pink/brown-noise power-law . the amplitude of the sound is directly proportional to some product of the flow-rate and waterfall height ( i would guess the former-term would dominate ) . e.g. : the following power spectrum ( power vs . frequency - both in arbitrary units ) . the answer i am sure information can be obtained from the sound . in particular , estimates of its height/size , flow-rate , and distance 5 . i am also sure this would be quite difficult in practice and , for most purposes , just listening and guessing would probably be as accurate as any quantitative analysis ; ) additional consideration ? i suppose its possible waterdrop ( let ) s could source additional sound at scales comparable to their own size . that would be pretty cool , but i have no idea how to estimate/guess if that is important or not . probably they would only contribute to sound at wavelengths comparable to their size ( and thus constrained by the max/min water-drop sizes 6 . . . ) . water , especially in a mist/spray , can be very effective at damping sound ( which they used to use for the space-shuttle ) . i would assume that this would have a significant effect on the resulting sound for heights/flow-volumes at which a mist/spray is produced . the acoustic properties of the landscape might also be important , i.e. whether the landscape is open ( with the waterfall drop-off being like a step-function ) or closed ( like the drop-off being at the end of a u-shaped valley , etc ) . finally , the additoinally surfaces involved might be important to consider : e.g. rocks , the surface of the waterfall drop-off , sand near the waterfall base , etc etc . endnotes 1: consider how much sound a 60 watt amp produces , and assume maybe a 10% efficiency ( probably optimistic ) . that is loud , and carrying a small amount of power compared to what a comparable-loudness waterfall is carrying . the vast-majority of waterfall energy will end up as heat , turbulence , and bulk-motion . 2: i would also guess that height/volume blend after some saturation point ( i.e. . 1000 m 3 /min at 20m height is about the same as 500 m 3 /min at 40m height ) . . . but lets ignore that for now . 3: turbulence tends to transfer energy from large-scales to small-scales . see : http://en.wikipedia.org/wiki/turbulence 4: figuring out the actual relation for the smallest size-scale of turbulence is both over my head and , i think , outside the scale of this ' answer ' . but it involves things like the kolmogorov spectrum , and associated length scale . 5: distance could be estimates based on a combination of the spectrum and volume level - to disentangle the degeneracy between sound-volume and distance . 6: perhaps the minimum droplet size is determined by it behaving ballistically ( instead of forming a mist ) ?
because direction cosines are , unlike sines and tans , even functions of the angle which makes the sign of the angle irrelevant and that is a good thing . more importantly , the direction cosines of a unit vector $\vec v$ end up being the coordinates $v_x , v_y , v_z$ , respectively , so the direction cosines obey $$\cos^2 a+\cos^2 b+\cos^2 c = 1$$ which is nice . at the end , the special feature of the cosine is that it appears in the inner product of two vectors : $$ |\vec u\cdot \vec v| = |u|\cdot |v| \cdot \cos\varphi$$ one may talk about " direction sines " or " tans " as well but they are calculable by more complicated formulae and/or obey other , more complicated identities , so they are less useful for direction calculations of interesting quantities . sines and tans are not really banned ; they are just not equally useful or natural .
in short , yes . but there are 2 caveats for the orbit for a massless particle around a spherical body . both can be seen from the following plot , borrowed form " spacetime and geometry " by carroll pg . 211: i ) there is only one possible orbit for $r = 3 gm = \frac{3 r_{s}}{2}$ where $r_s$ is the schwarzschild radius . this orbital radius could very well be within the radius of the body being orbited , hence the particle may not even be able to get this trajectory in the first place . ii ) the orbit is unstable . edit : as for the updated , additional questions : first of all , the above analysis was done for a classical particle , so there is no notion of waves or interference there . in this sense , the photon will cross its own path the same way my dog crosses his own path when he runs in a circle . if you really want to treat the photon quantum mechanically , i can only take a stab at it under some idealized circumstances . in one idealization we could take the photon state is very localized ( high probability of detecting it in small range , virtually zero everywhere else ) . we can send this photon once around in the orbit - in this case it will act a lot like a classical particle in that it will not interfere at all with the ' tail ' of the wavefunction from the prior pass . ( that we can set up states like this is merely an intuitive notion on my part since i know we can set up single photon states and there is little chance of detecting them at say , alpha centauri , so localized states of a single photon seem to be possible ) . now for the other idealization , we could take a state that is not very localized at all , say a plane wave directed tangentially to the orbit . now that the wave function is not localized it is going to be able to ' feel ' all around the space and i think its going to want to ' fall off ' the trajectory where it can be at a state of lower energy . that is , over a long period of time its going to sense the instability and have a very small probability of being detected at along the radius at $ 3 g m$ rendering the question sort of moot . one thing you could do is stand on one side of the body being orbited and shoot a photon towards it and have your buddy on the other side detect it , which is pretty much the double slit experiment and will result in interference ( the 2 paths are going left or right of the body being orbited ) . ( i am more posting this part of my answer as a guess to see what other people think and less out of certainty ) . i want to emphasize point ( ii ) above - something that is unstable is a bit like having a needle balanced on its tip - sure you can do it for a split second but the slightest movement will make it fall over so really no matter how many special circumstances you want to invent or questions you want to ask about the photon orbiting its just not going to stay there very long .
if $\phi$ is non-zero , fixing the phase of $\phi$ is a perfectly valid gauge condition . it is used frequently in standard model calcuations involving the higgs field , where it goes by the name unitarity gauge . this is a nice gauge in some ways , because it makes manifest the fact that there is a massive vector field in the system . edit : some caution is required with unitary gauge . it is a complete gauge when you can reasonably treat $\phi$ as non zero , because it uses every degree of freedom in the gauge transformation . this means for example that it is ok to use in perturbative calculations around a higgs condensate . but when $\phi$ can vanish , the phase function is not uniquely defined , which means the gauge transformation is not invertible . this gauge is not quite a gauge .
what you are calling the " dipolar component of the potential " is not actually that object . for something to be a ' component ' of the potential it also needs to be a scalar ; in that sense , the sum of those three terms , $$\delta \phi=\sum_j\frac{\partial \phi}{\partial x_j} ( x_j-x_{\text c , j} ) $$ could indeed be called the ' dipolar component ' , and it will work in any region where there are no charges other than test charges . you chould note in particular that this object is intimately related to the electric field , as $$\delta\phi=-\nabla \mathbf{e}\cdot ( \mathbf{r}-\mathbf{r}_\text{c} ) . $$ because of this , using the dipolar component will work as long as you evaluate it at the dipole separation . on the other hand , if you take the vector $$\mathbf{\delta\phi}=\sum_j\frac{\partial \phi}{\partial x_j} ( x_j-x_{\text c , j} ) \mathbf{e}_j$$ and then dot that with the atoms ' positions , then you will get an interaction energy $$ u =\sum_a\sum_j\frac{\partial \phi}{\partial x_j} ( x_{a , j}-x_{\text c , j} ) p_j =\frac2q\sum_j\frac{\partial \phi}{\partial x_j}p_j^2 $$ which is manifestly wrong . so , bottom line : no .
fidelity is a way of quantifying how similar 2 states are to each other . fidelity $f$ is by definition $f\in [ 0,1 ] $ , with $f=1$ meaning that 2 states are identical and $f=0$ means they are as different as physically distinct possible . fidelity is often used as a way of saying how good your source or quantum state preperation is , by comparing the state of what you measure with what you ideally want . as an example , if you wanted to create pairs of perfectly entangled particles ( as you alluded to ) and you had extra stray ( non-entangled ) photons that you measure in your ensemble/group of prepared bi-photons , then your mixed or ensemble averaged state will be different then what you want ( perfectly entangeld bi-photon pairs ) and how close you are is given by $f$ .
wow , this one has been over-answered already , i know . . . but it is such a fun question ! so , here 's an answer that has not been , um , " touched " on yet . . . : ) you sir , whatever your age may be ( anyone with kids will know what i mean ) , have asked for an answer to one of the deepest questions of quantum mechanics . in the quantum physics dialect of high nerdese , your question boils down to this : why do half-integer spin particles exhibit pauli exclusion - that is , why do they refuse to the be in the same state , including the same location in space , at the same time ? you are quite correct that matter as a whole is mostly space . however , the specific example of bound atoms is arguably not so much an example of touching as it is of bonding . it would be the equivalent of a 10 year old son not just poking his 12 year old sister , but of poking her with superglue on his hand , which is a considerably more drastic offense that i do not think anyone would be much amused by . touching , in contrast , means that you have to push - that is , exert some real energy - into making the two objects contact each other . and characteristically , after that push , the two object remain separate ( in most cases ) and even bound back a bit after the contact is made . so , i think one can argue that the real question behind " what is touching ? " is " why do solid objects not want to be compressed when you try to push them together ? " if that were not the case , the whole concept of touching sort of falls apart . we would all become at best ghostly entities who cannot make contact with each other , a bit like chihiro as she tries to push haku away during their second meeting in spirited away . now with that as the sharpened version of the query , why do objects such a people not just zip right through each other when they meet , especially since they are ( as noted ) almost entirely made of empty space ? now the reflex answer - and it is not a bad one - is likely to be electrical charge . that is because we all know that atoms are positive nuclei surrounded by negatively charged electrons , and that negative charges repel . so , stated that way , it is perhaps not too surprising that when the outer " edges " of these rather fuzzy atoms get too close that their respective sets of electrons would get close enough to repel each other . so by this answer , " touching " would simply be a matter of atoms getting so close to each other that their negatively charged clouds of electrons start bumping into each other . this repulsion requires force to over come , so the the two objects " touch " - reversibly compress each other without merging - through the electric fields that surround the electrons of their atoms . this sounds awfully right , and it even is right . . . to a limited degree . here 's one way to think of the issue : if charge was the only issue involved , then why do some atoms have exactly the opposite reaction when their electron clouds are pushed close to each other ? for example , if you push sodium atoms close to chlorine atoms , what you get is the two atoms leaping to embrace each other more closely , with a resulting release of energy that at larger scales is often described by words such as " boom ! " so clearly something more than just charge repulsion is going on here , since at least some combinations of electrons around atoms like to nuzzle up much closer to each other instead of farther away . what then guarantees that two molecules will come up to each other and instead say " howdy , nice day . . . but , er , could you please back off a bit , it is getting stuffy ? " that general resistance to getting too close turns out to result not so much from electrical charge ( which does still plays a role ) , but rather from the pauli exclusion effect i mentioned earlier . pauli exclusion is often skipped over in starting texts on chemistry , which may be why issues such a what touching means are also often left dangling a bit . without pauli exclusion , touching - the ability of two large objects to make contact without merging or joining - will always remain a bit mysterious . so what is pauli exclusion ? it is just this : very small , very simple particles that spin ( rotate ) in a very peculiar way always , always insist on being different in some way , sort of like kids in large families where everyone wants their unique role or ability or distinction . but particles , unlike people , are very simple things , so they only have a very limited set of options to choose from . when they run out of those simple options , they have only one option left : they need their own bit of space , apart from any other particle . they will then defend that bit of space very fiercely indeed . it is that defense of their own space that leads large collections of electrons to insist on taking up more an more overall space , as each tiny electron carves out its own unique and fiercely defended bit of turf . particles that have this peculiar type of spin are called fermions , and ordinary matter is made of three main types of fermions : protons , neutrons , and electrons . for the electrons there is only one identifying feature that distinguishes them from each other , and that is how they spin : counterclockwise ( called " up" ) or clockwise ( called " down" ) . you had think they had have other options , but that too is a deep mystery of physics : very small objects are so limited in the information they carry that they can not even have more than directions to choose from when spinning around . however , that one options is very important for understanding that issue of bonding that must be dealt with before atoms can engage in touching . two electrons with opposite spins , or with spins that can be made opposite of each other by turning atoms around the right way , do not repel each other : they attract . in fact , they attract so much that they are an important part of that " boom ! " i mentioned earlier for sodium and chlorine , both of which have lonely electrons without spin partners , waiting . there are other factors on how energetic the boom is , but the point is that until electrons have formed such nice , neat pairs , they do not have as much need to occupy space . once the bonding has happened , however - once the atoms are in arrangements that do not leave unhappy electrons sitting around wanting to engage in close bonds - then the territorial aspect of electrons comes to the forefront : they begin defending their turf fiercely . this defense of turf first shows itself in the ways electrons orbit around atoms , since even there the electrons insist on carving out their own unique and physically separate orbits , after that first pairing of two electrons is resolved . as you can imagine , trying to orbit around an atom while at the same time trying very hard to stay away from other electron pairs can lead to some pretty complicated geometries . and that too is a very good thing , be cause those complicated geometries lead to something called chemistry , where different numbers of electrons can exhibit very different properties due to new electrons being squeezed out into all sorts of curious and often highly exposed outside orbits . in metals it gets so bad that the outermost electrons essentially become community children that zip around the entire metal crystal instead of sticking to single atoms . that is why metal carry heat and electricity so well . in fact , when you look at a shiny metallic mirror , you are looking directly at the fastest-moving of these community-wide electrons . it is also why in outer space you have to be very careful about touching two pieces of clean metal to each other , because with all those electrons zipping around , the two pieces may very decide to bond into a single new piece of metal instead of just touching . this effect is called vacuum welding , and it is an example of why you need to be careful about assuming that solids that make contact will always remain separate . but many materials , such a you and your skin , do not have many of these community electrons , and are instead full of pairs of electrons that are very happy with the situations they already have , thank you . and when these kinds of materials and these kinds of electrons approach , the pauli exclusion effect takes hold , and the electrons become very defensive of their turf . the result at out large-scale level is what we call touching : the ability to make contact without easily pushing through or merging , a large-scale sum of all of those individual highly content electrons defending their small bits of turf . so to end , why do electrons and other fermions want so desperately to their own bits of unique state and space all to themselves ? and why , in every experiment ever done , is this resistance to merger always associated with that peculiar kind of spin i mentioned , a form of spin that is so minimal and so odd that it can not quite be described within ordinary three-dimensional space ? we have fantastically effective mathematical models of this effect . it has to do with antisymmetric wave functions . these amazing models are instrumental to things such as the semiconductor industry behind all of our modern electronic devices , as well as chemistry in general and of course research into fundamental physics . but if you ask the " why " question , that becomes a lot harder . the most honest answer is , i think , " because that is what we see : half-spin particles have antisymmetric wave functions , and that means they defend their spaces . " but linking the two together tightly - something called the spin-statistics problem - has never really been answered in a way that richard feynman would have called satisfactory . in fact , he flatly declared more than once that this ( and several other items in quantum physics ) were still basically mysteries for which we lacked really deep insights into why the universe we know works that way . and that , sir , is why your question of " what is touching ? " touches more deeply on profound mysteries of physics than you may have realized . it is a good question . 2012-07-01 addendum here is a related answer i did for s.e. chemistry . it touches on many of the same issues , but with more emphasis on why " spin pairing " of electrons allows atoms to share and steal electrons from each other -- that is , it lets them form bonds . it is not a classic textbook explanation of bonding , and i use a lot of informal english words that are not mathematically accurate . but the physics concepts are accurate . my hope is that it can provide a better intuitive feel for the rather remarkable mystery of how an uncharged atom ( e . g . chlorine ) can overcome the tremendous electrostatic attraction of a neutral atom ( e . g . sodium ) to steal one or more of its electrons .
there are three things i will try to explain in my answer . first , globular clusters actually take a long time to evolve , on the order of $10^8$ to $10^9$ years . second , many encounters between a binary and a wandering star can eject the wanderer with only a small tightening of the binary , thereby depleting the core . third , it is currently contested that some globular clusters actually do have black holes at their centres . the rough timescale for the evolution of a globular cluster is the crossing time . that is , the time an average star takes to traverse the whole cluster . the wikipedia entry for globular clusters says " the mean value is on the order of $10^9$ years " . so it would take some time to get a really big black hole to build up . it is true that there are many interactions between stars in the dense core . however , a lot of these lead to the formation of binary systems rather than collisions or mergers . when a third star interacts with a binary , a common outcome is that the stars in the binary move closer together and energy is conserved by ejecting the third star at high speed . i recently saw a seminar where the speaker did a detailed calculation of how much " radiation " this produces and it turns out quite a lot . something like , if a binary forms , it can eject 20 or so stars before the binary merges into a single star . finally , do not forget that people have already claimed to have evidence for black holes of a few thousand solar masses at the centres of m15 and $\omega$ cen . they have also suggested such black holes at the centres of globular clusters around other galaxies . mayall ii is one candidate , orbiting andromeda . the recent intermediate-mass black hole candidate hlx-1 seems to have a population of stars around it , so it might be a globular cluster ( but it might also be the stripped core of a galaxy that recently interacted with eso 243-49 ) .
i think the question is too general to give any terribly useful answer . most of what we consider taste is actually smell i.e. it is the detection of volatile chemicals from the food in the nose . the volatility of many chemicals found in food changes a lot over the 0c to body temperature range , and this would have a big effect on taste . many foods contain fat and this melts over the zero to body temp range . this probably affects texture more than taste , but it may also affect volatiles dissolved in the fat . a good example is to try and eat pasta sauce straight from the fridge . if the sauce contains any appreciable amount of fat it usually tastes disgusting :- )
math high school mathematics , including algebra , pre-calculus , and basic solid geometry single-variable calculus , both differentiation and integration calculus of functions of several variables , including volume integrals and surface integrals differential equations vector algebra , including dot products and cross products vector calculus , including line integrals , divergence , gradient , curl , and the laplacian . i will second the recommendation for " div , grad , curl " , but that is starting at step 6 . silvanus thompson 's calculus made easy is good for step 2 . james nearing 's mathematical tools for physicists , especially chapters 1,4,6,8 , 10 and 13 should be helpful for steps 3 - 6 . the various schaum 's outlines might be sufficient if you need to review the high school stuff in step 1 . physics basic mechanics - newton 's laws ( technically , you do not need this , but any book you read about electromagnetism will assume you know it ) high school electromagnetism - basics of charge , current , electric and magnetic fields , coulomb 's law , ampere 's law , lenz ' law , the right-hand rule , and displacement currents . ( optional ) special relativity - lorentz transformations , the interval , four-vectors ( optional because you could choose to learn it concurrently ) that is it - the physics prerequisites are brief . however , it takes quite a bit of time and effort to get used to basic physics if you have not done so already . in theory you could skip the physics prereqs , but you had probably have a hard time relating the equations you were learning to the real world . volume 1 of the feynman lectures on physics is a good source for the physics prereqs , although you will probably need to supplement with practice problems from another source . there is a book called a student 's guide to maxwell 's equations that i have heard recommended highly . however , i have not read it . when you are ready for electromagnetism with maxwell 's equations , i can second recommendations for purcell and griffiths .
since you say you are talking about what happens locally ( in a small volume ) , i will answer from that point of view . the usual formulation of energy conservation in such a volume is that energy is conserved in an inertial reference frame . in general relativity , there are no truly inertial frames , but in a sufficiently small volume , there are reference frames that are approximately inertial to any desired level of precision . if you restrict your attention to such a frame , there is no cosmological redshift . the photon 's energy when it enters one side of the frame is the same as the energy when it exits the other side . so there is no problem with energy conservation . the ( apparent ) failure of energy conservation arises only when you consider volumes that are too large to be encompassed by a single inertial reference frame . to be slightly more precise , in some small volume $v=l^3$ of a generic expanding universe , imagine constructing the best possible approximation to an inertial reference frame . in that frame , observers near one edge will be moving with respect to observers near the other edge , at a speed given by hubble 's law ( to leading order in $l$ ) . that is , in such a frame , the observed redshift is an ordinary doppler shift , which causes no problems with energy conservation . if you want more detail , david hogg and i wrote about this at considerable ( perhaps even excessive ! ) length in an ajp paper .
if you put $\chi_{r_0} ( r ) = \delta ( r-r_0 ) $ then $ [ \chi_{r_0} ( r ) ] $ forms a basis . in dirac 's notation : $\chi_{r_0} ( r ) \rightarrow |r_0 \rangle$ and you can verify that this set is a basis because it satisfy : orthonormality : $\langle r_0 | r'_{0} \rangle = \delta ( r_0 - r'_0 ) $ closure relation : $\int d^3 r_0 \ \ |r_0 \rangle \langle r_0|=$ 1 where 1 is the identity operator .
using the description of field theory , and it is true both in classical field theory and quantum field theory , fields interact because of specific terms in the lagrangian or the hamiltonian that depend on several fields . or , equivalently , they interact because terms depending on one field appear in the equations of motion for another field . for example , the dirac field interacts with the electromagnetic field due to the triple product , cubic term $$ e a_\mu \bar\psi \gamma^\mu \psi $$ in the lagrangian or in the hamiltonian . this has the effect of adding the dirac current $\bar\psi \gamma^\mu\psi$ on the right hand side of the maxwell 's equation for the electromagnetic field , $\partial_\nu f^{\mu\nu}=\dots$ . similarly , the dirac equation for the dirac field gets modified because the partial derivatives get " decorated " by the gauge potential $a_\mu$ to become covariant derivatives . consequently , simple wave ( or similar simple ) solutions for the non-interacting fields are no longer solutions if these interactions are present or enabled . the nontrivial values of one field imply a different evolution of another field , and so on . for example , an electromagnetic wave gets redirected when it hits a charged particle ( or a packet of a charged field ) etc . if one insists on field theory , there is no " deeper " explanation why the terms are present in the hamiltonian , lagrangian , or equations of motion  those pretty much define the theory at the deepest level . in string theory or another ( hypothetical ) underlying theory , the terms in the lagrangian may be derived from a deeper principle . virtual particles or fields enable interactions between other fields  e.g. the virtual photons ( virtual quanta of the electromagnetic field ) cause the electrostatic repulsion between two dirac or other charged fields . but that is just a special type of " composite " interaction . at the microscopic level , the elementary interaction is the interaction between the electromagnetic and charged fields , and this interaction is " direct " and does not depend on any additional virtual particles .
susy has something to do with spacetime since its generators $q$ carry spin angular momentum , so they change the spin of the state they act on , hence susy is a spacetime symmetry . and this kind of generators is called fermionic generators . ( compare with the generators of gauge symmetries , which are unphysical symmetries . they do not change the spin of the state they act on , and they are called bosonic generators . )
the action has no immediate physical interpretation , but may be understood as the generating function for a canonical transformation ; see e.g. , http://en.wikipedia.org/wiki/hamilton-jacobi_equation
start with the gravitational field of the sun . we are effectively stationary with respect to the sun , because our relative speed is much less than $c$ , and the sun is rotating at well below relativistic speeds so we expect its gravitational field to be well described by the schwarzschild metric . and indeed this is true : newton 's law of gravity is the non-relativistic limit of the schwarzschild metric . the metric tensor is invarient with respect to coordinate transformations , so if we take some observer moving at near light speed they would also find the gravity round the sun to be described by the schwarzschild metric . it will not look the same in the observer 's coordinates , that is the individual components $g_{ij}$ will be different , but it will be the same tensor . since in the observer 's frame they are stationary and the sun is moving , the conclusion is that velocity does not change the spacetime curvature . incidentally , this is why a fast moving object does not turn into a black hole .
short answer : no . explanation : many introductory text books talk about " rest mass " and " relativistic mass " and say that the " rest mass " is the mass measured in the particles rest frame . that is not wrong , you can do physics in that point of view , but that is not how people talk about and define mass anymore . in the modern view each particle has one and only one mass defined by the square of it is energy--momentum four vector ( which being a lorentz invariant you can calculate in any inertial frame ) : $$ m^2 \equiv p^2 = ( e , \vec{p} ) ^2 = e^2 - \vec{p}^2 $$ for a photon this value is zero . in any frame , and that allows people to reasonably say that the photon has zero mass without needing to define a rest frame for it .
i realized that $\vec{w}$ , $\vec{y}$ and $\vec{z}$ depend on both $\vec{r}$ and $t$ independently , anot not through a composite $t_r$ . so here is an enhanced version of the answer let us say $\mathbb{v}_s \subset \mathbb{r}^3$ is the smallest spherical volume such that : $$ \forall t , \qquad \forall \vec{r}\notin \mathbb{v}_s : \qquad\qquad \rho\left ( \vec{r} , t\right ) = \vec{j}\left ( \vec{r} , t\right ) = 0 $$ $$ \forall t , \qquad \forall \vec{r}\in \partial\mathbb{v}_s : \qquad\qquad \rho\left ( \vec{r} , t\right ) = \vec{j}\left ( \vec{r} , t\right ) = 0 $$ since $\mathbb{v}_s$ is a sphere , let us say that its center is at $\vec{r}_0$ , and its diameter id $\mathcal{d}=2\mathcal{r}_s$ . now given the definitions : $$ \vec{r}_s \in \mathbb{v}_s , \qquad \vec{r} = \vec{r} - \vec{r}_s , \qquad r=\left|\vec{r}\right| , \qquad \hat{r}=\frac {\vec{r}} {r} , \qquad t_r = t - \frac {r} {c} $$ the fields $\vec{e}$ and $\vec{b}$ are as follows : $$ \vec{e}\left ( \vec{r} , t\right ) = \frac {1} {4 \pi \epsilon_0} \iiint_{\mathbb{v}_s} {\left [ \frac {\rho ( \vec{r}_s , t_r ) } {r^2} \hat{r} + \frac {1} {r c} \frac {\partial \rho ( \vec{r}_s , t_r ) } {\partial t} \hat{r} - \frac {1} {r c^2} \frac {\partial \vec{j} ( \vec{r}_s , t_r ) } {\partial t} \right ] } \space dv\left ( \vec{r}_s\right ) $$ $$ \vec{b}\left ( \vec{r} , t\right ) = \frac {\mu_0} {4 \pi} \iiint_{\mathbb{v}_s} {\left [ \frac {\vec{j} ( \vec{r}_s , t_r ) } {r^2} \times \hat{r} + \frac {1} {r c} \frac {\partial \vec{j} ( \vec{r}_s , t_r ) } {\partial t} \times \hat{r} \right ] } \space dv\left ( \vec{r}_s\right ) $$ the paper " the relation between expressions for time-dependent electromagnetic fields given by jefimenko and by panofsky and phillips " by kirk t . mcdonald shows how the continuity equation $-\dot{\rho}=\nabla\cdot\vec{j}$ can be applied to the expression for $\vec{e}$ to transform it into : $$ \scriptsize{ \vec{e}\left ( \vec{r} , t\right ) = \frac {1} {4 \pi \epsilon_0} \iiint_{\mathbb{v}_s} {\left [ \frac {\rho ( \vec{r}_s , t_r ) } {r^2} \hat{r} + \frac { \left ( \vec{j} ( \vec{r}_s , t_r ) \cdot \hat{r}\right ) \hat{r} + \left ( \vec{j} ( \vec{r}_s , t_r ) \times\hat{r}\right ) \times \hat{r} } {r^2 c} + \frac {1} {r c^2} \left ( \frac {\partial \vec{j} ( \vec{r}_s , t_r ) } {\partial t} \times \hat{r} \right ) \times \hat{r} \right ] } \space dv\left ( \vec{r}_s\right ) } $$ now given any $\vec{r}$ , the maximum possible difference between the values of $\dfrac 1 r$ for any two points $\vec{r}_s \in \mathbb{v}_s$ is : $$\frac{1}{r} - \frac{1}{r+\mathcal{d}} = \frac{\mathcal{d}}{r\left ( r+\mathcal{d}\right ) } = \frac{1}{r}\cfrac{ \color{red}{\frac{\mathcal{d}}{r}}}{1+\color{red}{\frac{\mathcal{d}}{r}}}$$ similarly , the maximum possible angle between the values of $\hat{r}$ for any two points $\vec{r}_s \in \mathbb{v}_s$ is : $$ 2\ ; {\tan}^{-1}\left ( \frac12 \color{red} {\frac{\mathcal{d}}{r}}\right ) $$ for values of $\vec{r}$ for which $r \gg \mathcal{d}$ , we have $\dfrac {\mathcal{d}} r \approx 0$ , and each of the two expressions above are $\approx 0$ as well . so $\dfrac 1 r$ and $\hat{r}$ become essentially independent of $\vec{r}_s$ . we can , therefore , bring out all $\dfrac 1 r$ and $\hat{r}$ factors outside the integral sign . so if we define the following : $$ w\left ( \vec{r} , t\right ) = \iiint_{\mathbb{v}_s} \rho \left ( \vec{r}_s , t_r \right ) \space dv\left ( \vec{r}_s\right ) $$ $$ \vec{y}\left ( \vec{r} , t\right ) = \iiint_{\mathbb{v}_s} \vec{j} \left ( \vec{r}_s , t_r \right ) \space dv\left ( \vec{r}_s\right ) $$ $$ \vec{z}\left ( \vec{r} , t\right ) = \iiint_{\mathbb{v}_s} \frac {\partial \vec{j} \left ( \vec{r}_s , t_r \right ) } {\partial t} \space dv\left ( \vec{r}_s\right ) = \frac {\partial \vec{y}\left ( \vec{r} , t\right ) } {\partial t} $$ we can then say : $$ \vec{e}\left ( \vec{r} , t\right ) = \frac {1} {4 \pi \epsilon_0} \left [ \frac {w ( \vec{r} , t ) } {r^2} \hat{r} + \frac {\vec{y} ( \vec{r} , t ) \cdot \hat{r}} {r^2 c} \hat{r} + \frac {\left ( \vec{y} ( \vec{r} , t ) \times \hat{r}\right ) \times \hat{r}} {r^2 c} + \frac {\left ( \vec{z} ( \vec{r} , t ) \times \hat{r}\right ) \times \hat{r}} {r c^2} \right ] $$ $$ \vec{b}\left ( \vec{r} , t\right ) = \frac {\mu_0} {4 \pi} \left [ \frac {\vec{y} ( \vec{r} , t ) \times \hat{r}} {r^2} + \frac {\vec{z} ( \vec{r} , t ) \times \hat{r}} {r c} \right ] $$ applying $\color{blue}{\left ( \vec{a} \times \hat{u}\right ) \times \hat{u} \equiv \left ( \vec{a}\cdot\hat{u}\right ) \hat{u} - \vec{a}}$ , we get : $$ \vec{e}\left ( \vec{r} , t\right ) = \frac {1} {4 \pi \epsilon_0} \left [ \left ( \frac {w ( \vec{r} , t ) } {r^2} + 2\frac {\vec{y} ( \vec{r} , t ) \cdot \hat{r}} {r^2 c} + \frac {\vec{z}\left ( \vec{r} , t \right ) \cdot\hat{r}} {r c^2} \right ) \hat{r} - \frac 1 {rc} \left ( \frac {\vec{y} ( \vec{r} , t ) } {r} + \frac {\vec{z} ( \vec{r} , t ) } {c} \right ) \right ] $$ $$ \vec{b}\left ( \vec{r} , t\right ) = \frac {\mu_0} {4 \pi} \left [ \frac 1 r {\left ( \frac {\vec{y} ( \vec{r} , t ) } {r} + \frac {\vec{z} ( \vec{r} , t ) } {c} \right ) } \times \hat{r} \right ] $$ thus , if we define : $$ \mathcal{u}\left ( \vec{r} , t\right ) = \color{blue}{ \vec{w} \left ( \vec{r} , t\right ) + \frac {\vec{y} \left ( \vec{r} , t\right ) \cdot \hat{r}} {c} } = \iiint_{\mathbb{v}_s} \left [ \rho \left ( \vec{r}_s , t_r \right ) + \frac { \vec{j} \left ( \vec{r}_s , t_r \right ) \cdot \hat{r} } {c} \right ] \space dv\left ( \vec{r}_s\right ) $$ $$ \vec{\mathcal{x}} \left ( \vec{r} , t\right ) = \color{blue}{ \frac {\vec{y} \left ( \vec{r} , t\right ) } {r} + \frac {\vec{z} \left ( \vec{r} , t\right ) } {c} } = \iiint_{\mathbb{v}_s} \left [ \frac { \vec{j} \left ( \vec{r}_s , t_r \right ) } {r} + \frac 1 c \frac {\partial \vec{j} \left ( \vec{r}_s , t_r \right ) } {\partial t} \right ] \space dv\left ( \vec{r}_s\right ) $$ . . . we get : $$ \vec{e}\left ( \vec{r} , t\right ) = \frac {1} {4 \pi \epsilon_0} \left [ \frac {\mathcal{u}\left ( \vec{r} , t\right ) } {r^2} \hat{r} + \frac {\left ( \vec{\mathcal{x}}\left ( \vec{r} , t\right ) \times \hat{r}\right ) \times \hat{r}} {rc} \right ] $$ $$ \vec{b}\left ( \vec{r} , t\right ) = \frac {\mu_0} {4 \pi} \left [ \frac {\vec{\mathcal{x}} \left ( \vec{r} , t\right ) \times \hat{r}} {r} \right ] $$ finally we can compute the poynting vector $\vec{s}\left ( \vec{r} , t\right ) = \dfrac {\vec{e}\left ( \vec{r} , t\right ) \times \vec{b}\left ( \vec{r} , t\right ) } {\mu_0}$ giving us : $$ \vec{s}\left ( \vec{r} , t\right ) = \mathcal{u} \left ( \vec{r} , t\right ) \frac {\hat{r} \times \left ( \vec{\mathcal{x}} \left ( \vec{r} , t\right ) \times \hat{r} \right ) } {16 \pi^2 \epsilon_0 r^3} + \frac {\left| \vec{\mathcal{x}} \left ( \vec{r} , t\right ) \times \hat{r} \right|^2} {16 \pi^2 \epsilon_0 r^2 c} \hat{r} $$ the first term is clearly perpendicular to $\hat{r}$ , while the second term is along $\hat{r}$ . if we define a sphere $\mathbb{v}$ with its center at $\vec{r}_0$ ( the center of the source volume $\mathbb{v}_s$ ) such that point $\vec{r}$ is on its surface , then the unit vector $\hat{r}$ will actually be the unit normal ( to the surface $\partial\mathbb{v}$ , pointing outward ) at $\vec{r}$ . the outward power flow through $\vec{r}$ is given by : $$ \vec{s}\left ( \vec{r} , t\right ) \cdot \hat{r} = \frac {\left| \vec{\mathcal{x}} \left ( \vec{r} , t\right ) \times \hat{r} \right|^2} {16 \pi^2 \epsilon_0 r^2 c} $$ we can see that $\vec{\mathcal{x}} \left ( \vec{r} , t\right ) \times \hat{r}$ is perpendicular to $\hat{r}$ , and is therefore tangential to the surface $\partial\mathbb{v}$ everywhere . by the hairy ball theorem , it must be zero for at least one $\vec{r}$ . the outward power flowing through $\partial\mathbb{v}$ must therefore be zero at at least one point ( and not one axis , or two points , as was suspected in the question ) . the total power passing the surface $\partial\mathbb{v}$ will be : $$ p\left ( r , t\right ) = \oint_{\partial\mathbb{v}} \vec{s}\left ( \vec{r} , t\right ) \cdot \hat{r} \ ; ds\left ( \vec{r}\right ) = \oint_{\partial\mathbb{v}} \frac {\left| \vec{\mathcal{x}} \left ( \vec{r} , t\right ) \times \hat{r} \right|^2} {16 \pi^2 \epsilon_0 r^2 c} \ ; ds\left ( \vec{r}\right ) $$ if we now express the vectors in spherical coordinates $\langle 0\le \mathrm{r} \lt \infty , \ ; 0 \le \theta \le 2\pi , \ ; 0 \le \phi \le \pi\rangle$ with the origin at $\vec{r}_0$ , this surface integral can be parameterized to become : $$ \small{ p\left ( r , t\right ) = \int_0^{2\pi} \int_0^{\pi} \frac {\left| \vec{\mathcal{x}} \left ( \vec{r} , t\right ) \times \hat{r} \right|^2} {16 \pi^2 \epsilon_0 r^2 c} r^2 \sin \phi \ ; d\phi\ ; d\theta = \int_0^{2\pi} \int_0^{\pi} \frac {\left| \vec{\mathcal{x}} \left ( \vec{r} , t\right ) \times \hat{r} \right|^2} {16 \pi^2 \epsilon_0 c}\sin \phi \ ; d\phi\ ; d\theta } $$ . . . which expands to : $$\small{ \frac 1 {16 \pi^2 \epsilon_0 c} \int_0^{2\pi} \int_0^{\pi} \left [ \frac {\left| \vec{y} \left ( \vec{r} , t\right ) \times \hat{r} \right|^2} {r^2} + 2\frac {\left ( \vec{y} \left ( \vec{r} , t\right ) \times \hat{r} \right ) \cdot\left ( \vec{z} \left ( \vec{r} , t\right ) \times \hat{r} \right ) } {r c} + \frac {\left| \vec{z} \left ( \vec{r} , t\right ) \times \hat{r} \right|^2} {c^2} \right ] \ ; \sin \phi \ ; d\phi\ ; d\theta }$$ as $r \to\infty$ , we can see that the first two terms vanish , and the term $\mathcal{p}_{\infty}$ from the question turns out to be : $$ \small{ \mathcal{p}_{\infty}\left ( t\right ) = \lim_{r\to\infty} p\left ( r , t+ \frac r c\right ) = \frac 1 {16 \pi^2 \epsilon_0 c^3} \int_0^{2\pi} \int_0^{\pi} \left| \color{red}{ \vec{\mathcal{q}}\left ( \hat{r} , t\right ) } \times \hat{r} \right|^2\ ; \sin \phi \ ; d\phi\ ; d\theta } $$ . . . where : $$ \vec{\mathcal{q}}\left ( \hat{r} , t\right ) = \lim_{r\to\infty}\vec{z} \left ( \vec{r} , t+\frac r c\right ) = \iiint_{\mathbb{v}_s} \frac {\partial \vec{j} \left ( \vec{r}_s , t+\frac {\left ( \vec{r}_s-\vec{r}_0\right ) \cdot\hat{r}} c\right ) } {\partial t} \space dv\left ( \vec{r}_s\right ) $$ $\vec{\mathcal{q}}$ depends on the direction $\hat{r}$ , but is completely independent of the distance $r$ .
the hypercharge of a doublet cannot be " deduced " . when one builds a gauge theory , the first step is to define the particle content of your theory and to postulate the representation of all particle multiplets . in particular , if the gauge group is abelian , then we have to assign numbers usually called charges . so , i reformulate your question : why do we choose the hypercharge of the higgs doublet to be $y=1$ ? the idea is that we want to respect the gell-mannnishijima formula $y=2 ( q-t_3 ) $ . since we know the charges and the values of $t_3=\pm 1/2$ for the doublet , it is straightforward to find that $y=1$ . there are several ways to see why the formula $y=2 ( q-t_3 ) $ must be respected : one possibility is to compute the commutator of the generators $q$ and $t_i$ . you will notice that $ [ q , t_i ] = [ t_3 , t_i ] \neq0$ , but $ [ q-t_3 , t_i ] =0$ . this means that the global symmetries $su ( 2 ) $ and $u ( 1 ) _{\rm em}$ cannot be simultaneously satisfied , but we can define the hypercharge $y$ as new abelian symmetry . other possibility is to try to assign abelian charges for the fermionic multiplets of the standard model ( knowing that the charge must be the same for all members of a multiplet ) . you will conclude that $y=2 ( q-t_3 ) $ is the only possible choice up to a multiplicative constant .
in 2d you are going to get an extra factor of $1/h$ that you can not get rid of . it is because in 2d you have in some sense line charges with units of $ [ charge ] / [ length ] $ , and similarly with field units . so when you discretize ( and , by the way , your discretization is only first order accurate in space ) you are going to have this units problem for lower than three dimensions . in two dimensions , you have line charges , and in one dimension you have sheet charges . hence , your units of electrostatic potential have to change to match the change in the units of your charges .
could the coriolis effect on snowing be so dramatic . . . ? no . the coriolis effect is only noticeable for objects traveling long distances with respect to earth 's surface for significant periods of time . for example , a ballistic missile fired hundreds of miles or a hurricaine that is hundreds of miles in diameter and lasts for days . across the street is too small a distance for an noticeable effect . the rossby number can be used to determine if the coriolis effect is significant in a given situation . another way of thinking of it is : the earth only rotates once a day , how does the phenomenon being investigated compare to this fact ?
basically : no cheap , efficient , large-scale battery technology exists . this question gets asked in the world of intermittent renewable energy generation all the time , but it is even harder for lightning because of the extermely high power of the energy burst , so that is an extra problem to solve on top . also , how do you predict where it will strike ? build a massive antenna in every city ? the practical concerns outweigh the theoretical ones . without a battery , you have a sudden jolt to the grid . where does it go ? you can not shut down another plant for a milisecond to save generation elsewhere .
richard terrett 's comment gives the correct answer : richard , you should post it as an answer so people can upvote it . a battery does indeed have excess charge at it is terminals , and the charge is simply given by the usual equation q = cv , where c is the capacitance of the battery and v the voltage . however both the capacitance and the voltage of a typical domestic battery are small so the net charge is negligable . however the reason a battery will not pick up scraps of paper is that the voltage is small . if you do the usual party trick of rubbing a ballon on a pullover you can charge the balloon to several thousand volts . if you only charged the ballon to 1.5v it would not pick up small bits of paper let alone a paperclip .
if you break the electric field lines you have drawn into components parallel to the surface and components perpendicular to the surface , you will find a net field pointing to the left . that means positive charges would feel a force to the left , and since this is a conductor , they are free to move , and so they move to the left . the only way for the charges to stop moving is for all of the electric field components that are horizontal/tangential/parallel to the surface to cancel out , leaving only components that are perpendicular to the surface . maybe the picture below can help : it can get a little confusing , but keep in mind that a vector field has a vector associated with each point in space , and when you draw vectors that represent the field , the vectors you draw represent the field exactly at the tail of the vector ( the point where the vector begins ) , and nowhere else , no matter how long the arrows are . so then the length is just telling you the strength of the field at the point corresponding to the tail of the vector , and the direction is telling you the direction of the vector field at the point corresponding to the tail of the vector . in other words , we do not care whether the vectors " intersect " a charge when we draw them , because the vectors are only telling us something about the point at which they originate ( their tail ) and nowhere else . ( i.e. . in my picture , i know something about the field at each charge , but nowhere else . even though the vectors extend up and to the left , they are telling me nothing about the electric field at points above and to the left of the charges , only exactly at the charges . thus each of these charges feels an electric field to the left , as drawn ( and also a component pointed up ) . these components are shown in red in the bottom part of the figure . since they ( the charges ) are in a conductor , they are free to move to the left as long as they feel a field pointed that way . ( they will not move up though , because they would have to jump out of the conductor to do that , assuming these charges are lying on the surface of a conductor . ) ) i hope that helps to clear it up . to reiterate , these vectors tell us about the field at each of the charges . so each charge must be feeling a field ( and therefore a force ) to the left , with the vectors as drawn .
